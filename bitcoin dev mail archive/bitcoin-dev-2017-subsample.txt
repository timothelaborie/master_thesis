Jared Lee Richardson via bitcoin-dev schreef op wo 29-03-2017 om 12:10
[-0700]:

In order to evaluate this statement the definition of microtransaction
has to be defined. I guess there will also be no consensus on that...

greets,
Staf.

-------------------------------------
Tom Zander wrote:


I tend to agree, if all 32 bits were given up to grinding.

But it's worth pointing out that BIP9 is purely informational, and the top 3 bits are still reserved for other purposes. One of them could perhaps be used to signal for an extended version field somewhere else, leaving the bottom 29 as entropy?

Not a direction I prefer, but just a technical possibility perhaps.
-------------------------------------
Ok, this is good stuff.  thanks for the thoughtful reply.

Regarding anyone-can-spend:

all of the examples you gave do not satisfy isStandard.  So if our
hypothetical cryptocurrency were to restrict all transactions to
isStandard at the consensus layer, would that not effectively prevent
anyone-can-spend?

Or more generally and with our thinking caps on, what would be the best
way to prevent anyone-can-spend, if that is our goal?


Regarding soft-fork = restrict:

Your example of miners running secret soft-fork code that blacklists
satoshi's utxo's is intriguing and somewhat troubling.

I think the main takeaways are that:
  1) there are other ways to soft-fork besides anyone-can-spend.
  2) it is impossible to prevent hidden soft-forks.

Is that accurate?

Still, I would put forth the following question:  If anyone-can-spend tx
were no longer allowed according to consensus rules (assuming that is
possible/practical), then could the network still be practically
"upgraded" with new features (eg opcodes) via soft-fork, and if so, what
would be the mechanism for backwards compatibility in this scenario?


or from another angle:  even if it is impossible to prevent all
soft-forks, can you see any way at all to make it logistically
infeasible to use soft-forks as a network-wide consensus change mechanism?

and another thought:  as I understand it, bitcoin is presently able to
add new opcodes via soft-fork because Satoshi added 10 unused opcodes
via hardfork. What will happen when these run out?  Can new opcodes
still be added without a hard-fork?


note: I ask these questions with the goal/vision of creating an
immutable altcoin or sidechain, not necessarily restricting bitcoin's path.





On 09/14/2017 09:01 PM, ZmnSCPxj wrote:



-------------------------------------
If a small dissenting minority can block all forward progress then bitcoin
is no longer interesting.  What an incredibly simple attack vector...

No need to break any cryptography, find a bug to exploit, build tens of
millions of dollars in mining hardware, spend lots of bitcoin on fees to
flood the network, or be clever or expend any valuable resources in any
way, shape, or form.

Just convince(or pay, if you do want to expend some resources) a few
people(or make up a few online personas) to staunchly refuse to accept
anything at all and the entire system is stuck in 2013(when we first
started widely discussing a blocksize increase seriously).

Is that really the bitcoin that you want to be a part of?

When the 1MB cap was implemented it was stated specifically that we could
increase it when we needed it.  The white paper even talks about scaling to
huge capacity.  Not sure where you got the idea that we all agreed to stay
at 1MB forever, I certainly didn't.  It was never stated or implied that we
could change the coin cap later(please cite if I'm mistaken).


On Feb 8, 2017 12:16 PM, "alp alp" <alp.bitcoin@gmail.com> wrote:

Doing nothing is the rules we all agreed to.  If those rules are to be
changed,nearly everyone will need to consent.  The same rule applies to the
cap, we all agreed to 21m, and if someone wants to change that, nearly
everyone would need to agree.


On Feb 8, 2017 10:28 AM, "Andrew Johnson" <andrew.johnson83@gmail.com>
wrote:

It is when you're talking about making a choice and 6.3x more people prefer
something else. Doing nothing is a choice as well.

Put another way, if 10% supported increasing the 21M coin cap and 63% were
against, would you seriously consider doing it?

On Feb 8, 2017 9:57 AM, "alp alp" <alp.bitcoin@gmail.com> wrote:

-------------------------------------
I really like the idea of extending signalling capabilities to the
end-users. It gives stakeholders a voice in the decisions we take in
the network, and are a clear signal to all other involved parties. It
reminds me of a student thesis I supervised some time ago [1], in
which we explored various signalling ideas.

I think we have a number of fields that may be used for such a
signalling, e.g., OP_RETURN, locktime, and output scripts. I think
OP_RETURN is probably not the field you'd want to use though since it
adds data that needs to be transferred, stored for bootstrap, and
outputs in the UTXO would need to be tagged with additional
information. Locktime has the advantage of being mostly a freeform
field for values in the past, but it clashes with other uses that may
rely on it. Furthermore, it is the transaction creator that specifies
the locktime, hence the signal trails one hop behind the current
owner, i.e., the actual stakeholder.

I think probably the best field to signal would be the output
script. It is specified by the recipient of the funds, i.e., the
current owner, and is already stored in the UTXO, so a single pass can
tally up the votes. We could for example use the last 4 bits of the
pubkey/pubkeyhash to opt in (3 leading 0 bits) and the vote (0/1
depending on the stakeholders desired signal). We'd need to define
similar semantics for other script types, but getting the standard
scripts to be recognized should be simple.

In the spirit of full disclosure I'd like to also mention some of the
downsides of voting this way. Unlike the OP_RETURN proposal, users
that do not intend to signal will also be included in the tally. I'd
expect the signals of these users to be random with a 50% chance of
either outcome, so they should not influence the final result, but may
muddy the water depending on what part of the population is
signalling. The opt-in should make sure that the majority of votes are
actually voluntary votes, and not just users that randomly select a
pubkey/pubkeyhash, and can be adjusted as desired, though higher
values require more grinding on behalf of the users.

The grinding may also exacerbate some problems we already have with
the HD Wallet lookahead, since we now skip a number of addresses, so
we should not require too many opt-in bits.

So there are some problems we'd need to tackle, but I'm really excited
about this, as it could provide data to make informed decisions, and
should put an end to the endless speculation about the will of the
economic majority.

Cheers,
Christian

[1] http://pub.tik.ee.ethz.ch/students/2015-HS/SA-2015-30.pdf
-------------------------------------
This[1] idea from April would assist in a BIP149-like segwit
activation on November 16th.

Its goal is to be incredibly easy to test and deploy, right now, even
before a decision on revisions to BIP149 is made, and well before such
"BIP149ish" testing is itself complete.

UASFs don't need time for most legacy nodes to upgrade - that's the
point of a soft fork.  UASFs simply need to have inevitability,
which is provided by some nodes more than others.  But for the node
less instrumental in that inevitability, and more relaxed about
scheduling upgrade work, being moved to a miner-protected consensus
ruleset is not as desirable a position as the opportunity to
participate fully.  As a courtesy, the plan for soft forks has
always been to allow legacy nodes time to upgrade to full
participation.  How much time should rollouts allow for this
courtesy?

Extended BIP9 activation of segwit (for legacy nodes) separates
concerns between intending to activate segwit and its method of
deployment, allowing "semi-legacy" nodes that have upgraded to
include this proposal to participate immediately in a successful
segwit activation, without needing any courtesy time to upgrade to
the particular deployment logic.

Code for deployment is included in the original email[1].  There's
nothing missing from the logic shown.  The whole intent of the
proposal is that other deployment specifics are left to be defined
by future proposals.  In the same block that THRESHOLD_ACTIVE is
reached for segwit, require Consensus::DEPLOYMENT_SEGWIT_ALT1 to
also reach THRESHOLD_ACTIVE, and the burden of the future proposal
is fulfilled.

If the idea proves broken or of no benefit, when actually
implementing and testing future deployments, then we can avoid using
DEPLOYMENT_SEGWIT_ALT1 until it expires, and zero nodes get hurt.

Let's look at how this affects options for how to deploy segwit...

/ BIP149ish UASF, without this proposal /

roadmap:
  - BIP149ish debated (handles BIP141 success, unlike BIP149)
  - BIP149ish tested
  - BIP149ish courtesy timeout debated
  - BIP148    fails
  - BIP149ish released
  - BIP141    fails
  - BIP149ish courtesy timeout expires
  - BIP149ish activates
  - segwit activates
  - legacy nodes must upgrade before recognizing BIP149 activation

If we try to restrict ourselves to only the original service bit, we
see a tension between activating soon and leaving legacy nodes on a
miner-protected consensus.  We also see a tension between *planning*
how long it will take to debate and test UASF logic, and setting
expectations for when a reasonable activation date is.

These seemingly small tensions complicate the solution, and push it
out of developer's visions of what is feasible.  It's not clear how
soon it can happen, so it doesn't get started in an urgent manner.
It doesn't get started in an urgent manner, so it's not clear how
soon it can happen.

/ BIP149ish UASF, with this proposal /

roadmap:
  - this proposal debated
  - this proposal tested
  - this proposal released (courtesy begins)
  - BIP149ish     debated  (handles BIP141 success, unlike BIP149)
  - BIP149ish     tested
  - BIP148        fails
  - BIP149ish     released
  - BIP141        fails
  - BIP149ish     activates
  - segwit activates
  - semi-legacy nodes *immediately* use segwit, via this proposal

We can remove the courtesy timeout problem, because the courtesy
begins as soon as this proposal goes live.  This simplifies debate on
how BIP149ish should deploy, and helps make reasonable a much quicker
segwit activation should BIP141 fail.

With a clear route to quick activation for semi-legacy nodes, the
UASF can be planned for activation in as short a window as the key
nodes can upgrade.  Not even all the key nodes need to upgrade: it's
still a soft fork, and UASFs simply need to have inevitability.

These differences can help us aim for activating segwit on November
16th, if BIP141 and BIP148 do not succeed earlier.  Since BIP149 as
originally conceived is slow as molasses, BIP149ish still needs
debate, BIP141 has steadfast enemies, and the community is slow to
adapt to BIP148's complicated commitment requirements, it is prudent
to take this intermediate step allowing quicker BIP149ish activation.

[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/014160.html

-------------------------------------
I've already opened a PR almost 2 weeks ago to do this and fix the other 
issues BIP 9 has. https://github.com/bitcoin/bips/pull/550

It just needs your ACK to merge.


On Wednesday 05 July 2017 1:30:26 AM shaolinfry via bitcoin-dev wrote:

-------------------------------------
See this soft-fork proposal from Felix Weiss

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012194.html

Adam

On Apr 12, 2017 5:43 PM, "Oleg Andreev via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Hi Cameron,

Presumably the "very serious security vulnerability" posed is one of
increased centralization of hash power. Would this danger exist
without the patent risk?

e

On 05/26/2017 01:02 AM, Cameron Garnham via bitcoin-dev wrote:
wonderful upgrade for Bitcoin. It seems to me that virtually the
entire Bitcoin Ecosystem agrees with me.  Except for around 67% of the
mining hash-rate who very conspicuously refuse to signal for it’s
activation.
of the mining hash-rate holding a position that isn’t at-all
represented in the wider Bitcoin Community. My study of ASICBOOST lead
to a ‘bingo’ moment:  If one assumes that the 67% of the hash rate
that refuse to signal for SegWit are using ASICBOOST. The entire
picture of this political stalemate became much more understandable.
SegWit great, it partially mitigates a very serious security
vulnerability.
and will attract criticism of self serving motivation.”
that SegWit has not been activated yet is directly because of
CVE-2017-9230.
for the attackers who it would frustrate.
regarded as a legitimate security vulnerability.  This would suggest
that it is not contentious in the wider technical community.
is NOT contentious to regard CVE-2017-9230 as a credible security
vulnerability. Then using it as partial security fix for a security
vulnerability SHOULD NOT be contentious.
community.  Or you believe CVE-2017-9230 should not be regarded as a
credible security vulnerability. Then I would logically agree with you
that we should separate the issues so that we may gain consensus.
However, I just don’t see this as the case.
<andreas@antonopoulos.com> wrote:
my name was mentioned...
(only) with a segwit-like commitment to the coinbase which does not
obligate miners to signal Segwit or implement Segwit, thus disarming
any suspicion that the issue is being exploited only to activate Segwit.
and will attract criticism of self serving motivation.
and to its security. Not claiming that is the intent here, but the
damage is done by the mere appearance of motive.
<bitcoin-dev@lists.linuxfoundation.org> wrote:
(3) (4) and actively exploited (5) security vulnerability.
detailed report:
is dangerous:
without negative feedback, that SegWit be used as a partial-mitigation
of CVE-2017-9230.
assumption that any block that doesn’t include a witness commit in
it's coinbase transaction was mined using covert asicboost.  Making
the use of covert asicboost far more conspicuous.
strengthened via another soft-fork that makes the inclusion of witness
commits mandatory, without negative feedback.
CVE-2017-9230 quickly vs more slowly but more conservatively is under
intense debate.  The author of this post has a strong preference to
the swiftest viable option.
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014416.
html
Ryan Grant:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014352.
html
Tier Nolan:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014351.
html
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/01399
6.html


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBCAAGBQJZJ+Q1AAoJEDzYwH8LXOFOqakH/R1YCifIGjV07vnnsxeC/77x
d6w5tBmtEd5MLzrX/6VtMoI8UzgLEcDM1WfFox3jDVz/HurkTVorliyJrr14BVsc
rL2nTbfychYh1rAdTIsNwFt15Wgjcp/5eAq7Lw5TM5OJ3YbPn2zWJY19QmjEAJ+M
kGz26R+IJL1095yed5RN2JoN8O9x+HVdtIjaHJJRJzLsy+3g22zMWgN1nZN0olhX
mFQJZbvS0gQyiRGJmNku3zP5Qg2cFzWt+VBtFrzNu1QTTkbK2e1owHOmpgfygTD3
g3F4VoDfyA7pBnpMMMjjTaCaG34Am3CvYu8iYnZXy85s2ZjC+XeKgqMkBLj4+q8=
=A3ne
-----END PGP SIGNATURE-----

-------------------------------------
One interesting thing to do is to compare how much does it cost to maintain
a bank check account and how much does it cost to run a full node.

It seems that it is about 120USD/year in USA:
http://m.huffpost.com/us/entry/6219730

A 4TB hard drive ~=115USD
https://www.amazon.com/gp/aw/d/B01LQQH86A/ref=mp_s_a_1_4

And it has a warranty of 3 years.

As your calculation shows, it will take more than 19 years to reach 4TB
with a 4MB blocksize.

Em 29/03/2017 12:35, "Johnson Lau via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> escreveu:


On 29 Mar 2017, at 14:24, Emin Gün Sirer via bitcoin-dev <bitcoin-dev@lists.
linuxfoundation.org> wrote:

respect and admiration, I do not agree with some of their conclusions

I'm one of the co-authors of that study. I'd be the first to agree with
your conclusion
and argue that the 4MB size suggested in that paper should not be used
without
compensation for two important changes to the network.


Our recent measurements of the Bitcoin P2P network show that network speeds
have improved tremendously. From February 2016 to February 2017, the average
provisioned bandwidth of a reachable Bitcoin node went up by approximately
70%.
And that's just in the last year.


4 * 144 * 30 = 17.3GB per month, or 207GB per year. Full node
initialisation will become prohibitive for most users until a shortcut is
made (e.g. witness pruning and UTXO commitment but these are not trust-free)


Further, the emergence of high-speed block relay networks, like Falcon (
http://www.falcon-net.org)
and FIBRE, as well as block compression, e.g. BIP152 and xthin, change the
picture dramatically.


Also as the co-author of the selfish mining paper, you should know all
these technology assume big miners being benevolent.


So, the 4MB limit mentioned in our paper should not be used as a protocol
limit today.

Best,
- egs



On Tue, Mar 28, 2017 at 3:36 PM, Juan Garavaglia via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Hi,


I also thought about this some time ago. But note that this implies
that forks grow with the same block frequency as the main chain. Thus
the longest chain rule becomes irrelevant, since all chains will have
the same length (in expectancy). Rather, the chain with most work is
the true one.

Best
Henning


On Wed, Jul 05, 2017 at 02:02:08PM +0000, Troy Benjegerdes via bitcoin-dev wrote:

-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp

-------------------------------------
On Tue, May 23, 2017 at 3:22 PM, Paul Sztorc <truthcoin@gmail.com> wrote:


I guess I was looking for the detail you get in the code, but without
having to read the code.

My quick reading gives that the sidechain codes (critical hashes) are added
when a coinbase is processed.

Any coinbase output that has the form "OP_RETURN <32 byte push>" counts as
a potential critical hash.

When the block is processed, the key value pair (hash, block_height) is
added to a hash map.

The OP_BRIBE opcode checks that the given hash is in the hash map and
replaces the top element on the stack with the pass/fail result.

It doesn't even check that the height matches the current block, though
there is a comment that that is a TODO.

I agree with ZmnSCPxj, when updating a nop, you can't change the stack.  It
has to fail the script or do nothing.

OP_BRIBE_VERIFY would cause the script to fail if the hash wasn't in the
coinbase, or cause a script failure otherwise.

Another concern is that you could have multiple bribes for the same chain
in a single coinbase.  That isn't fair and arguably what the sidechain
miner is paying for is to get his hash exclusively into the block.

I would suggest that the output is

OP_RETURN <sidechain_id> <critical hash>

Then add the rule that only the first hash with a particular sidechain id
actually counts.

This forces the miner to only accept the bribe from 1 miner for each
sidechain for each block.  If he tries to accept 2, then only the first one
counts.

OP_BRIBE_VERIFY could then operate as follows

<block height> <sidechain_id> <critical hash> OP_BRIBE_VERIFY

This causes the script to fail if
  <block height> does not match the block height, or
  <critical hash> is not the hash for the sidechain with <sidechain_id>, or
  there is no hash for that sidechain in the block's coinbase

If you want reduce the number of drops, you could serialize the info into a
single push.

This has the advantage that a sidechain miner only has to pay if his block
is accepted in the next bitcoin block.  Since he is the only miner for that
sidechain that gets into the main bitcoin block, he is pretty much
guaranteed to form the longest chain.

Without that rule, sidechain miners could end up having to pay even though
it doesn't make their chain the longest.

How are these transactions propagated over the network?  For relaying, you
could have the rule that the opcode passes as long as <block height> is
near the current block height.  Maybe require that they are in the future.
They should be removed from the memory pool once the block height has
arrived, so losing miners can re-spend those outputs.

This opcode can be validated without needing to look at other blocks, which
is good for validating historical blocks.

I am still looking at the deposit/withdrawal code.
-------------------------------------
No I actually agree with a lot of what you said.

I feel there has been a lack of precision and consistency in talking about
these things in the past.  This is not intentional, but its just what
happens when a lot of different people are all giving their own arguments.

I tried to be a bit more clear by talking about how soft forks have
historically been done.  But certainly the technical definition of a soft
fork is a slippery slope.  I completely disagree with the idea that miners
have any right to enforce a soft fork.  Even more strongly, I don't think
they have the ability.  Censoring a certain class of transactions (such as
those invalid under a soft fork) is something they can unilaterally do, but
it is not the same thing as a soft fork.  It is necessarily transient. It
requires nodes to enforce the rules to make it a permanent soft fork.

I do think there are differences between soft forks and hard forks and
consensus requirements and safety for rolling them out.  But as mentioned
in my email, I think soft forks should still have a very high bar for
consensus.  It's an open question as to whether Core misjudged this for
segwit, although if so, I think it was a close call.  The intention is not
to let 95% of miners make the rules (although again, note that it is 95%, a
very high bar, vastly different than 75% of miners).   It seems to me that
the vast majority of the community is in favor of segwit.  I'm not sure
about your comment that it is obvious to an observer than a sizable portion
of the community is opposed.  It is certainly some, and perhaps more than
expected.  But this is exactly why the new version bits soft fork roll out
mechanism allows proposals to expire. We did do an extensive job assessing
support for segwit before roll out, and although we knew of a few loud
voices against we judged them to be a very small minority.  No business we
contacted was opposed.  If it turns out we got it wrong then the proposal
will expire harmlessly.  I for one am certainly opposed to forcing segwit
or any other fork of any kind on a significant minority that is opposed.

Despite saying that, I think you will hear some other responses about how
soft forks are opt-in and if you don't like the new features don't use
them.  There is some logic behind this idea.  But these are all subtleties
which are hard to make strong right and wrong statements about.  See some
of the past discussion on this list.




On Sun, Mar 26, 2017 at 4:13 AM, Chris Pacia <ctpacia@gmail.com> wrote:

-------------------------------------
It is a useful aspect of discussion at this level as it helps higher lever developers understand the actual tradeoffs. Clearly some do not. The market will eventually sort them out, but the discussion both gives developers the necessary information.

It also helps core development prioritize resources. I personally would not prioritize core work to facilitate zero conf. I would even spend time to discourage it, as others have done.

I think the cautions in this thread about doing privacy and system security damaging things (like checking mining pools for zero conf transactions) will prevent some wasted time, which benefits everyone.

e

-------------------------------------
Hello,

First, Drivechain has vaguely escaped vaporware status. If you've ever
thought "I'd like to take a look into Drivechain when there is code",
then now is a pretty good time. (Unfinished items include M1, and M8_V2.)

https://github.com/drivechain-project/bitcoin/tree/mainchainBMM

Also,
Site:  http://www.drivechain.info/
Blank sidechain:
https://github.com/drivechain-project/bitcoin/tree/sidechainBMM

Second, I think drivechain's documentation / BIP-Drafts are tolerably
readable.

Here they are:

1.
https://github.com/drivechain-project/docs/blob/master/bip1-hashrate-escrow.md
2.
https://github.com/drivechain-project/docs/blob/master/bip2-blind-merged-mining.md

cc: Luke, I think they are ready to be assigned formal BIP Numbers.

This is also a request for code review. The most helpful review will
probably take place on GitHub.

Regular review is also welcome. Although, please read our
recently-updated FAQ, at: http://www.drivechain.info/faq .
And also see major earlier discussions:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014364.html
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-June/014559.html

Have a nice weekend everyone,
Paul



-------------------------------------
Dear Greg,

Thank you for taking the time to review the BIP148 proposal.

I agree with much of your thoughts. I originally started working on a generalized way to deploy user activated soft forks, in a way that leveraged BIP9 to allow for optional faster MASF activation. BIP148 came about as a way to satify many people's frustrations about the current segwit activation. I have said several times in various places that the proposal requires a very high amount of consensus that needs to be present to make actual deployment feasible. BIP148 is certainly not what a normal UASF would or should look like.

I remain convinced the community very much wants segwit activated and that the UASF movement in general has gained a lot of traction. While support for BIP148 is surprisingly high, there are definitely important players who support UASF in general but do not like BIP148 approach (which you rightly point out is a UASF to force a MASF).

In any case, I have been working on various iterations for generalized deployment of soft forks. My latest iteration adds a simple flag to a BIP9 deployment so the deployment will transition to LOCKED_IN at timeout if the deployment hasnt already activated or locked in by then. This is nice because it allows for a long deployment of a soft fork, giving the ecosystem plenty time to upgrade with an effective flagday at the end of the timeout. The hash power can still optionally activate earlier under MASF.

BIP8 (was uaversionbits) can be seen here https://github.com/bitcoin/bips/blob/master/bip-0008.mediawiki

With BIP8 we could perform a UASF segwit deployment. Due to some complexities in the peering logic, I recommend a new deployment with a fresh bit that starts right after November 15th (when BIP9 segwit timesout) with a BIP8 timeout for April 2018. The code can deployed much earlier. For example if code was deployed today, it would give the economy a year to upgrade. Activation could still occur safely by MASF any time from now until April 2018 (SEGWIT until Nov, then UASEGWIT from Nov until April 2018).

I am still working on the finer implementation details, but you can see a rough draft from this diff (which includes BIP8 in the first commit, and the proposed bip-segwit-uasf in the second commit).

https://github.com/bitcoin/bitcoin/compare/master...shaolinfry:uasegwit-flagday

I believe this approach would satisfy the more measured approach expected for Bitcoin and does not have the issues you brought up about BIP148.

I do not support the BIP148 UASF for some of the same reasons that I
do support segwit: Bitcoin is valuable in part because it has high
security and stability, segwit was carefully designed to support and
amplify that engineering integrity that people can count on now and
into the future.

I do not feel the the approach proposed in BIP148 really measures up
to the standard set by segwit itself, or the existing best practices
in protocol development in this community.

The primary flaw in BIP148 is that by forcing the activation of the
existing (non-UASF segwit) nodes it almost guarantees at a minor level
of disruption.

Segwit was carefully engineered so that older unmodified miners could
continue operating _completely_ without interruption after segwit
activates.

Older nodes will not include segwit spends, and so their blocks will
not be invalid even if they do not have segwit support. They can
upgrade to it on their own schedule. The only risk non-participating
miners take after segwit activation is that if someone else mines an
invalid block they would extend it, a risk many miners already
frequently take with spy-mining.

I do not think it is a horrible proposal: it is better engineered than
many things that many altcoins do, but just not up to our normal
standards. I respect the motivations of the authors of BIP 148. If
your goal is the fastest possible segwit activation then it is very
useful to exploit the >80% of existing nodes that already support the
original version of segwit.

But the fastest support should not be our goal, as a community-- there
is always some reckless altcoin or centralized system that can support
something faster than we can-- trying to match that would only erode
our distinguishing value in being well engineered and stable.

"First do no harm." We should use the least disruptive mechanisms
available, and the BIP148 proposal does not meet that test. To hear
some people-- non-developers on reddit and such-- a few even see the
forced orphaning of 148 as a virtue, that it's punitive for
misbehaving miners. I could not not disagree with that perspective any
more strongly.

Of course, I do not oppose the general concept of a UASF but
_generally_ a soft-fork (of any kind) does not need to risk disruption
of mining, just as segwit's activation does not. UASF are the
original kind of soft-fork and were the only kind of fork practiced by
Satoshi. P2SH was activated based on a date, and all prior ones were
based on times or heights. We introduced miner based activation as
part of a process of making Bitcoin more stable in the common case
where the ecosystem is all in harmony. It's kind of weird to see UASF
portrayed as something new.

It's important the users not be at the mercy of any one part of the
ecosystem to the extent that we can avoid it-- be it developers,
exchanges, chat forums, or mining hardware makers. Ultimately the
rules of Bitcoin work because they're enforced by the users
collectively-- that is what makes Bitcoin Bitcoin, it's what makes it
something people can count on: the rules aren't easy to just change.

There have been some other UASF proposals that avoid the forced
disruption-- by just defining a new witness bit and allowing
non-upgraded-to-uasf miners and nodes to continue as non-upgraded, I
think they are vastly superior. They would be slower to deploy, but I
do not think that is a flaw.

We should have patience. Bitcoin is a system that should last for all
ages and power mankind for a long time-- ten years from now a couple
years of dispute will seem like nothing. But the reputation we earn
for stability and integrity, for being a system of money people can
count on will mean everything.

If these discussions come up, they'll come up in the form of reminding
people that Bitcoin isn't easily changed at a whim, even when the
whims are obviously good, and how that protects it from being managed
like all the competing systems of money that the world used to use
were managed. :)

So have patience, don't take short cuts. Segwit is a good improvement
and we should respect it by knowing that it's good enough to wait for,
and for however its activated to be done the best way we know how.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
I don’t think this is how the blockchain consensus works. If there is a split, it becomes 2 incompatible ledgers. Bitcoin is not a trademark, and you don’t need a permission to hardfork it. And what you suggest is also technically infeasible, as the miners on the new chain may not have a consensus only what’s happening in the old chain.




-------------------------------------
Bram Cohen,

In R&D: First its appropriate to explore all interesting ideas, and help each other improve their ideas. Last, when there is a deadline that needs to be met, we compare various options and decide on which to go with.

I'm on the First step still.

If you really want to push me to saying it, I'm not a fan of the Patricia Tree for bitcoin txos. I think its too much work for everyone to do when other options are available. But I'm not trying to say that your design is bad or wont work... I'm just personally not interested in it at this time.

Cheers,
Praxeology Guy
-------------------------------------
See thread on replay attacks for why activating regardless of threshold is a bad idea [1].

BIP91 OTOH seems perfectly reasonable. 80% instead of 95% makes it more difficult for miners to hold together in opposition to Core. It gives Core more leverage in negotiations.

If they don't activate with 80%, Core can release another BIP to reduce it to 75%.

Each threshold reduction makes it both more likely to succeed, but also increases the likelihood of harm to the ecosystem.

Cheers,
Greg

[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-June/014497.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-June/014497.html>

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
Hey all,

A lot of us familiar with the pay to contract protocol, and how it uses
cleverly the homomorphic property of elliptic curve encryption system to
achieve it.
Unfortunately, there is no standard specification on how to conduct such
transactions in the cyberspace.

We have developed a basic trade finance application that relies on the
original idea described in the Homomorphic Payment Addresses and the
Pay-to-Contract Protocol paper, yet we have generalized it and made it
BIP43 complaint.

We would like to share our method, and get your feedback about it,
hopefully this effort will result into a standard for the benefit of the
community.

Abstract idea:

We define the following levels in BIP32 path.
m / purpose' / coin_type' / contract_id' / *

contract_id is is an arbitrary number within the valid range of indices.

Then we define, contract base as following prefix:
m / purpose' / coin_type' / contract_id'

contract commitment address is computed as follows:
hash document using cryptographic hash function of your choice (e.g. blake2)
map hash to partial derivation path
Convert hash to binary array.
Partition the array into parts, each part length should be 16.
Convert each part to integer in decimal format.
Convert each integer to string.
Join all strings with slash `/`.
compute child public key by chaining the derivation path from step 2 with
contract base:
m/<contract_base>/<hash_derivation_path>
compute address
Example:

master private extended key:
xprv9s21ZrQH143K2JF8RafpqtKiTbsbaxEeUaMnNHsm5o6wCW3z8ySyH4UxFVSfZ8n7ESu7fgir8imbZKLYVBxFPND1pniTZ81vKfd45EHKX73
coin type: 0
contract id: 7777777

contract base computation :

derivation path:
m/999'/0'/7777777'
contract base public extended key:
xpub6CMCS9rY5GKdkWWyoeXEbmJmxGgDcbihofyARxucufdw7k3oc1JNnniiD5H2HynKBwhaem4KnPTue6s9R2tcroqkHv7vpLFBgbKRDwM5WEE

Contract content:
foo

Contract sha256 signature:
2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae

Contract partial derivation path:
11302/46187/26879/50831/63899/17724/7472/16692/4930/11632/25731/49056/63882/24200/25190/59310

Contract commitment pub key path:
m/999'/0'/7777777'/11302/46187/26879/50831/63899/17724/7472/16692/4930/11632/25731/49056/63882/24200/25190/59310
or
<contract_base_extended_pub_key>/11302/46187/26879/50831/63899/17724/7472/16692/4930/11632/25731/49056/63882/24200/25190/59310

Contract commitment pub key:
xpub6iQVNpbZxdf9QJC8mGmz7cd3Cswt2itcQofZbKmyka5jdvQKQCqYSDFj8KCmRm4GBvcQW8gaFmDGAfDyz887msEGqxb6Pz4YUdEH8gFuaiS

Contract commitment address:
17yTyx1gXPPkEUN1Q6Tg3gPFTK4dhvmM5R


You can find the full BIP draft in the following link:
https://github.com/commerceblock/pay-to-contract-protocol-specification/blob/master/bip-draft.mediawiki


Regards,
Omar
-------------------------------------
On Tue, Sep 12, 2017 at 09:10:18AM -0700, Simon Liu wrote:

Collecting various commentary from here and reddit, I think current de
facto policy is something like:

 * Vulnerabilities should be reported via security@bitcoincore.org [0]

 * A critical issue (that can be exploited immediately or is already
   being exploited causing large harm) will be dealt with by:
     * a released patch ASAP
     * wide notification of the need to upgrade (or to disable affected
       systems)
     * minimal disclosure of the actual problem, to delay attacks
   [1] [2]

 * A non-critical vulnerability (because it is difficult or expensive to
   exploit) will be dealt with by:
     * patch and review undertaken in the ordinary flow of development
     * backport of a fix or workaround from master to the current
       released version [2]

 * Devs will attempt to ensure that publication of the fix does not
   reveal the nature of the vulnerability by providing the proposed fix
   to experienced devs who have not been informed of the vulnerability,
   telling them that it fixes a vulnerability, and asking them to identify
   the vulnerability. [2]

 * Devs may recommend other bitcoin implementations adopt vulnerability
   fixes prior to the fix being released and widely deployed, if they
   can do so without revealing the vulnerability; eg, if the fix has
   significant performance benefits that would justify its inclusion. [3]

 * Prior to a vulnerability becoming public, devs will generally recommend
   to friendly altcoin devs that they should catch up with fixes. But this
   is only after the fixes are widely deployed in the bitcoin network. [4]

 * Devs will generally not notify altcoin developers who have behaved
   in a hostile manner (eg, using vulnerabilities to attack others, or
   who violate embargoes). [5]

 * Bitcoin devs won't disclose vulnerability details until >80% of bitcoin
   nodes have deployed the fixes. Vulnerability discovers are encouraged
   and requested to follow the same policy. [1] [6]

Those seem like pretty good policies to me, for what it's worth.

I haven't seen anything that indicates bitcoin devs will *ever* encourage
public disclosure of vulnerabilities (as opposed to tolerating other
people publishing them [6]). So I'm guessing current de facto policy is
more along the lines of:

 * Where possible, Bitcoin devs will never disclose vulnerabilities
   publically while affected code may still be in use (including by
   altcoins).

rather than something like:

 * Bitcoin devs will disclose vulnerabilities publically after 99% of the
   bitcoin network has upgraded [7], and fixes have been released for
   at least 12 months.


Instinctively, I'd say documenting this policy (or whatever it actually
is) would be good, and having all vulnerabilities get publically released
eventually would also be good; that's certainly the more "open source"
approach. But arguing the other side:

 - documenting security policy gives attackers a better handle on where
   to find weak points; this may be more harm than there is benefit to
   improving legitimate users' understanding of and confidence in the
   development process

 - the main benefit of public vulnerability disclosure is a better
   working relationship with security researchers and perhaps better
   understanding of what sort of bugs happen in practice in general;
   but if most of your security research is effectively in house [6],
   maybe those benefits aren't as great as the harm done by revealing
   even old vulnerabilities to attackers

If the first of those arguments holds, well, hopefully this message has
egregious errors that no one will correct, or it will quickly get lost
in this list's archives...

Cheers,
aj

[0] http://bitcoincore.org/en/contact
    referenced from .github/ISSUE_TEMPLATE.md in git

[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-September/014986.html

[2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-September/014990.html

[3] https://www.reddit.com/r/btc/comments/6zf1qo/peter_todd_nicely_pulled_away_attention_from_jjs/dmxcw70/

[4] https://www.reddit.com/r/btc/comments/6z827o/chris_jeffrey_jj_discloses_bitcoin_attack_vector/dmxdg83/

[5] https://www.reddit.com/r/btc/comments/6zb3lp/maxwell_admits_core_sat_on_vulnerability/dmv4y7g/

[6] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-September/014991.html 

[7] Per http://luke.dashjr.org/programs/bitcoin/files/charts/branches.html
    it seems like 1.7% of the network is running known-vulnerable versions
    0.8 and 0.9; but only 0.37% are running 0.10 or 0.11, so that might argue
    revealing any vulnerabilities fixed since 0.12.0 would be fine...
    (bitnodes.21.co doesn't seem to break down anything earlier than 0.12)


-------------------------------------
Hey Jacob!


With this proposal implemented, a 'mis-send' is fundamentally impossible. The address contains the identifier of the token that should be sent.

If anything, it's possible to 'mis-receive'.
That is, the receiving wallet was not aware of a newer chain, and the receiver actually wanted to receive the newer token, but instead his wallet created an address for the old token. It is the responsibility of the receiver to write a correct invoice. This is the case everywhere else in the world too, so this seems like a reasonable trade-off.

I would even argue that this should hold in a legal case, where the receiver cannot claim that he was expecting a payment in another token (contrary to how it is today, like when users send BTC to a BCH address, losing their funds with potentially no legal right for reimbursement). If I sent someone an invoice over 100€, I cannot later proclaim that I actually expected $100.

With this proposal, wallets are finally able to distinguish between different tokens. With this ability, I expect to see different implementations, some wallets which advertise staying conservative, following a strict ruleset, and other wallets being more experimental, following hashing rate or other metrics.

-------------------------------------
On Sun, Oct 1, 2017 at 3:27 PM, Mark Friedenbach <mark@friedenbach.org>
wrote:


Creating a Bitcoin script that does not allow malleability is difficult and
requires wasting a lot of bytes to do so, typically when handling issues
around non-0-or-1 witness values being used with OP_IF, and dealing with
non-standard-zero values, etc.  Adding a witness weight flag cuts through
the worst of all this, and makes script design enormously simpler and makes
scripts smaller and cheaper.



I'll argue that I don't want my counter-party going off and using a very
deeply nested key in order to subvert the fee rate we've agreed upon after
I've signed my part of the input.  If we are doing multi-party signing of
inputs we need to communicate anyways to construct the transaction.  I see
no problem with requiring my counter-party to choose their keys before I
sign so that I know up front what our fee rate is going to be.  If they
lose their keys and need a backup, they should have to come back to me to
resign in order that we can negotiate a new fee rate for the transaction
and who is going to be covering how much of the fee and on which inputs.
-------------------------------------
If users added a signal to OP_RETURN, might it be possible to tag all
validated input addresses with that signal.

Then a node can activate a new feature after the percentage of tagged input
addresses reaches a certain level within a certain period of time?

This could be used in addition to a flag day to trigger activation of a
feature with some reassurance of user uptake.
-------------------------------------
consensus.

That's not true unless miners are thought of as the identical to nodes,
which is has not been true for nearly 4 years now.  Nodes arbitrating a
consensus the BU theory - that nodes can restrain miners - but it doesn't
work.  If miners were forked off from nonminers, the miner network could
keep their blockchain operational under attack from the nodes far better
than nodes could keep their blockchain operational under attack from the
miners.  The miners could effectively grind the node network to a complete
halt and probably still run their own fork unimpeded at the same time.
This would continue until the the lack of faith in the network drove the
miners out of business economically, or until the node network capitulated
and followed the rules of the miner network.

The reason BU isn't a dire threat is that there's a great rift between the
miners just like there is between the average users, just as satoshi
intended, and that rift gives the user network the economic edge.

to trust and rely on other, more powerful nodes to represent them. Of
course, the more powerful nodes, simply by nature of having more power, are
going to have different opinions and objectives from the users.

I think you're conflating mining with node operation here.  Node users only
power is to block the propagation of certain things.  Since miners also
have a node endpoint, they can cut the node users out of the equation by
linking with eachother directly - something they already do out of
practicality for propagation.  Node users do not have the power to
arbitrate consensus, that is why we have blocks and PoW.

5,000,000 users. Users running full nodes is important to prevent political
hijacking of the Bitcoin protocol.  [..] that changes you are opposed to
are not introduced into the network.

This isn't true.  Non-miner nodes cannot produce blocks.  Their opinion is
not represented in the blockchain in any way, the blockchain is entirely
made up of blocks.  They can commit transactions, but the transactions must
follow an even stricter set of rules and short of a user activated PoW
change, the miners get to decide.  It might be viable for us to introduce
ways for transactions to vote on things, but that also isn't nodes voting -
that's money voting.

Bitcoin is structured such that nodes have no votes because nodes cannot be
trusted.  They don't inherently represent individuals, they don't
inherently represent value, and they don't commit work that is played
against eachother to achieve a game theory equilibrium.  That's miners.

nodes. For home users, 200 GB of bandwidth and 500 GB of bandwidth largely
have the exact same cost.

Your assumption is predicated upon the idea that users pay a fixed cost for
any volume of bandwidth.  That assertion is true for some users but not
true for others, and it is becoming exceedingly less true in recent years
with the addition of bandwidth caps by many ISP's.  Even users without a
bandwidth cap can often get a very threatening letter if they were to max
their connection 24/7.  Assuming unlimited user bandwidth in the future and
comparing that with limited datacenter bandwidth is extremely short
sighted.  Fundamentally, if market forces have established that datacenter
bandwidth costs $0.09 per GB, what makes you think that ISP's don't have to
deal with the same limitations?  They do, the difference is that $0.09 per
GB times the total usage across the ISP's customer base is far, far lower
than $80 times the number of customers.  The more that a small group of
customers deviating wildly becomes a problem for them, the more they will
add bandwidth caps or send threatening letters or even rate-limit or stop
serving those users.

Without that assumption, your math and examples fall apart - Bandwidth
costs for full archival nodes are nearly 50 times higher than storage costs
no matter whether they are at home or in a datacenter.

costs you are citing by quoting datacenter prices.

No, they really aren't without your assumption.  Yes, they are somewhat
different - If someone has a 2TB hard drive but only ever uses 40% of it,
the remaining hard drive space would have a cost of zero.  Those specific
examples break down when you average over several years and fifty thousand
users.  If that same user was running a bitcoin node and hard drive space
was indeed a concern, they would factor that desire into the purchase of
their next computer, preferring those with larger hard drives.  That
reintroduces the cost with the same individual who had no cost before.  The
cost difference doesn't work out to the exact same numbers as the
datacenter costs, who have a better economy of scale but also have profit
and business overhead, but all of the math I've done indicates that over
thousands of individuals and several years of time, the costs land in the
same ballpark.  For example - Comcast bandwidth cap = 1000gb @ ~$80/month.
 $0.08/GB.  Amazon's first tier is currently $0.09.  Much closer than I
even expected before I worked out the math.  I'm open to being proven wrong.


I'm running 0.13.2 and only see 300 mb of ram.  Why is 0.14 using three
times the ram?

seeing users choose not to run nodes simply

Again, while I sympathize with the concept, I don't believe holding the
growth of the entire currency back based on minimum specs is a fair
tradeoff.  The impact on usecases that depend on a given fee level is total
obliteration.  That's unavoidable for things like microtransactions, but a
fee level of $1/tx allows for hundreds of opportunities that a fee level of
$100/tx does not.  That difference may be the deciding factor in the
network effect between Bitcoin and a competitor altcoin.  Bitcoin dying out
because a better-operated coin steals its first-mover advantage is just as
bad as bitcoin dying out because an attacker halted tx propagation and
killed the network.  Probably even worse - First mover advantages are
almost never retaken, but the network could recover from a peering attack
with software changes and community/miner responses.

start to be the case.

I calculated this out.  If blocksizes aren't increased, but price increases
continue as they have in the last 3-5 years, per-node operational costs for
one month drop from roughly $10-15ish (using datacenter numbers, which you
said would be higher than home user numbers and might very well be when
amortized thoroughly) down to $5-8 in less than 8 years.  If transaction
fees don't rise at all due to blockspace competition (i.e., they offset
only the minimum required for miners to economically protect Bitcoin),
they'll be above $10 in less than 4 years.  I believe that comparing
1-month of node operational costs versus 1 transaction fee is a reasonable,
albeit imperfect, comparison of when users will stop caring.

That's not very far in the future at all, and fee-market competition will
probably be much, much worse for us and better for miners.

your government defaults, hyperinflates, seizes citizen assets, etc. etc.
(situations that many Bitcoin users today have to legitimately worry about),

So I don't mean to be rude here, but this kind of thinking is very poor
logic when applied to anyone who isn't already a libertarian Bitcoin
supporter.  By anyone outside the Bitcoin world's estimation, Bitcoin is an
extremely high risk, unreliable store of value.  We like to compare it to
"digital gold" because of the parameters that Satoshi chose, but saying it
does not make it true.  For someone not already a believer, Bitcoin is a
risky, speculative investment into a promising future technology, and gold
is a stable physical asset with 4,000 years of acceptance history that has
the same value in nearly every city on the planet.  Bitcoin is difficult to
purchase and difficult to find someone to exchange for goods or services.

Could Bitcoin become more like what you described in the future?  A lot of
us hope so or we wouldn't be here right now.  But in the meantime, any
other crypto currency that choses parameters similar to gold could eclipse
Bitcoin if we falter.  If their currency is more usable because they
balance the ratio of node operational costs/security versus transaction
fees/usability, they have a pretty reasonable chance of doing so.  And then
you won't store your $10k+ in bitcoin, you'll store in $altcoin.  The
market doesn't really care who wins.

think it continues to make sense to be considering the home nodes as the
target that we want to hit.

That's nothing, we've never had any fee competition at all until basically
November of last year.  From December to March transaction fees went up by
250%, and they doubled from May to December before that.  Transactions per
year are up 80% per year for the last 4 years.  Things are about to get
screwed.


On Wed, Mar 29, 2017 at 1:28 PM, David Vorick via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Tue, Dec 05, 2017 at 07:39:32PM +0000, Luke Dashjr via bitcoin-dev wrote:

I'll second the objection to a no-RBF flag.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
An alternative consensus algorithm to both proof-of-work and proof-of-stake, proof-of-loss addresses all their deficiencies, including the lack of an organic block size limit, the risks of mining centralization, and the "nothing at stake" problem:

https://proof-of-loss.money/
-------------------------------------

Is 49 days particularly useful? Would it be a problem to instead leave both-
bits undefined? I'm thinking this might be better as a way to indicate "7 
days, plus a deterministically chosen set of historical blocks"...


This is technically true right now, but as soon as segwit activates, it will 
no longer be... Therefore, I suggest striking it from the BIP, expounding on 
it in greater detail, or making it true for the longer term.


This isn't entirely clear whether it refers to peers downloading blocks, or 
peers serving them. (I assume the former, but it should be clarified.)


Wouldn't this already be a problem, without the BIP?

Luke

-------------------------------------
On Wed, 2017-09-27 at 17:20 -0400, Cory Fields via bitcoin-dev wrote:

This is somewhat weird. Back in 2014, most of icons were listed as
"CC BY-SA" (which is the correct license according to the original
source):
https://github.com/bitcoin/bitcoin/blob/31aac02446472ec5bfc4676ab190ec9d37056503/doc/assets-attribution.md

However the current docs list them as "Expat". A mistake?
https://github.com/bitcoin/bitcoin/blob/master/contrib/debian/copyright

Also, even the old version lists some icons "based on Stephan Hutchings
Typicons" as "License: MIT", which could be a violation of CC BY-SA if
I'm not mistaken.

Best,
Tim



-------------------------------------

visible and acknowledged. Blocks without the applicable bit set are invalid
during this period

Luke, it seems like the amendments to BIP8 make it drastically different to
how it first was designed to work.
It now looks more akin to BIP148, which was AFAICT not how BIP8 was
originally intended to work.
Perhaps this should be made into its own BIP instead, or make it so it's
possible to decide how the LOCKED_IN state should work when designing the
softfork.

Hampus

2017-07-05 6:10 GMT+02:00 Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
OP_2DUP?  Why not OP_3DUP?

On Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Good morning list,

I would like to speculate on the addition of an opcode which would provide replay protection and allow chain-backed trustless creation of hardfork futures payment channels.

Note however that in order to work, the hardfork must "cooperate" by changing the operation of OP_CHECKHARDFORKVERIFY in the hardfork.

The opcode is simple.  If the top stack is not the exact value 1, it fails.

The intent is that a hardfork must "cooperate" by also changing OP_CHECKHARDFORKVERIFY so that the top stack must be some other, non-1 value after the hardfork date.  This is a consensus break, but as a hardfork is defined by the fact that it is a consensus break, this is deliberate.

--

In the below, I assume the creation of a future hardfork with a new "consensus version" value (<hardforkVersion>, the value required by OP_CHECKHARDFORKVERIFY) and a fork height (<hardforkHeight>, the block height at which the hardfork will diverge from legacy).

It should be noted, that an "uncooperative" hardfork would not change its consensus version, and in that regard, OP_CHECKBLOCKATHEIGHTVERIFY is superior.

--

In order to prepare my funds for splitting.  I pay to the below P2SH/P2WSH script:

OP_IF
  1 OP_CHECKHARDFORKVERIFY OP_DROP <myPubKey1> OP_CHECKSIG
OP_ELSE
  <hardforkVersion> OP_CHECKHARDFORKVERIFY OP_DROP <myPubKey2> OP_CHECKSIG
OP_END

In the above, I can then create transactions that spend the first branch on legacy chain post fork, and create transactions that spend the second branch on the hardfork chain post fork.  In addition, if I suddenly need to access the fund before the fork date, I can use the first branch to recover my funds before the fork date.

--

Suppose I wish to make a bet with another randomly generated Internet person, MX06fRH.  I am of the opinion, that the hardfork will not have economic consensus, whereas MX06fRH is of the opinion that it will.  We resolve to each bet 1 BTC.

We create a transaction spending our funds and paying a single combined output to the below P2SH/P2WSH:

OP_DUP
OP_IF
  1 OP_EQUAL
  OP_IF
    <hardforkHeight> OP_CHECKLOCKTIMEVERIFY OP_DROP
    1 OP_CHECKHARDFORKVERIFY OP_DROP
    <ZmnSCPxjWinPubKey> OP_CHECKSIG
  OP_ELSE
    <hardforkVersion> OP_CHECKHARDFORKVERIFY OP_DROP
    <MRX06fRHWinPubKey> OP_CHECKSIG
  OP_ENDIF
OP_ELSE
  OP_DROP
  2 <ZmnSCPxjExitPubKey> <MX06fRHExitPubKey> 2 OP_CHECKMULTISIG
OP_ENDIF

In the above, I can use <ZmnSCPxjWinPubKey> and an nLockTime transaction after the fork date to claim my legacy coin, if the legacy coin has value, wheres MRX06fRH can use <MRX06fRHWinPubKey> on the hardfork chain if it has value.  Alternatively, we can both agree to cancel the bet.

--

While the above is workable, it does not form a market where price discovery of legacy and hardfork future coins can be performed to determine consensus.  For that, we will need to use payment channel techniques.

I and MRX06fRH can form a payment channel that can trade fork futures by creating an anchor transaction paying to:

OP_DUP
OP_IF
  1 OP_EQUAL
  OP_IF
    <hardforkHeight> OP_CHECKLOCKTIMEVERIFY OP_DROP
    1 OP_CHECKHARDFORKVERIFY OP_DROP
    2 <ZmnSCPxjLegacyPubKey> <MX06fRHLegacyPubKey> 2 OP_CHECKMULTISIG
  OP_ELSE
    <hardforkVersion> OP_CHECKHARDFORKVERIFY OP_DROP
    2 <ZmnSCPxjHardforkPubKey> <MX06fRHHardforkPubKey> 2 OP_CHECKMULTISIG
  OP_ENDIF
OP_ELSE
  OP_DROP
  2 <ZmnSCPxjExitPubKey> <MX06fRHExitPubKey> 2 OP_CHECKMULTISIG
OP_ENDIF

Of course, before the anchor transaction is signed, we should create commitment transactions first.  For a Poon-Drjya channel, we should create two commitment transactions, one for me and one for MX06fRH, but in fact we should create two versions of both, one for the legacy token and one for the hardfork token (four commitment transactions).  The contracts are differentiated by which branch of the above we activate.  Commitment transactions can only be claimed after the fork, but we can update them continuously using normal Poon-Dryja channel operations.

If I have the same amount of legacy and hardfork tokens, then MX06fRH also has equal legacy and hardfork tokens, so we can agree to exit.

--

Unfortunately this can only reach up to payment channels.  To create a futures market we should have a payment channel network.  For Lightning we use HTLCs to create atomic swaps of coins in different payment channels.  However, the difference here is that commitment transactions can only be claimed after the fork, so any HTLCs on top of that will have expired before the commitment transactions can be claimed and the HTLCs enforced.  Unfortunately, I know of no other construction that would allow creation of a payment channel network on top of a payment channel primitive.  So much for OP_CHECKHARDFORKVERIFY.

Regards,
ZmnSCPxj
-------------------------------------
My apologies for a delay in responding to emails on this list; I have
been fighting a cold.

(Also my apologies to Johnson Lau, as I forgot to forward this to the list.)

On Sep 8, 2017, at 2:21 AM, Johnson Lau <jl2012@xbt.hk> wrote:


I believe you meant "unclean stack," and you are correct. This was
also pointed out last tuesday by a participant at the in-person
CoreDev meetup where the idea was presented.

This doesn't kill the idea, it just complicates the implementation
slightly. A simple fix would be to allow tail-recursion to occur if
the stack is not clean (as can happen with legacy P2SH as you point
out, or yet to be defined version 1+ forms of segwit script), OR if
there is a single item on the stack and the alt-stack is not empty.
For segwit v0 scripts you then have to move any arguments to the alt
stack before ending the redeem script, leaving just the policy script
on the main stack.


I disagree with this design requirement.

The SigOp counting method used by bitcoin is flawed. It incorrectly
limits not the number of signature operations necessary to validate a
block, but rather the number of CHECKSIGs potentially encountered in
script execution, even if in an unexecuted branch. (Admitedly MAST
makes this less of an issue, but there are still useful compact
scripts that use if/else constructs to elide a CHECKSIG.) Nor will it
account for aggregation when that feature is added, or properly
differentiate between signature operations that can be batched and
those that can not.

Additionally there are other resources used by script that should be
globally limited, such as hash operations, which are not accounted for
at this time and cannot be statically assessed, even by the flawed
mechanism by which SigOps are counted. I have maintained for some time
that bitcoin should move from having multiple separate global limits
(weight and sigops, hashed bytes in XT/Classic/BCH) to a single linear
metric that combines all of these factors with appropriate
coefficients.

A better way of handling this problem, which works for both SigOps and
HashOps, is to have the witness commit to the maximum resources
consumed by validation of the spend of the coin, to relay this data
with the transaction and include it in the SigHash, and to use the
committed maximum for block validation. This could be added in a
future script version upgrade. This change would also resolve the
issue that led to the clean stack rule in segwit, allowing future
versions of script to use tail-call recursion without involving the
alt-stack.

Nevertheless it is constructive feedback that the current draft of the
BIP and its implementation do not count SigOps, at all. There are a
couple of ways this can be fixed by evaluating the top-level script
and then doing static analysis of the resulting policy script, or by
running the script and counting operations actually performed.

Additionally, it is possible that we take this time to re-evaluate
whether we should be counting SigOps other than for legacy consensus
rule compliance. The speed of verification in secp256k1 has made
signature operations no longer the chief concern in block validation
times.


This is correct. Your feedback will be incorporated.


Is there a repo that contains the latest implementation of BIP 114,
for comparison purposes?

Kind regards,
Mark Friedenbach


-------------------------------------
- The BIP91 portion of the fork seems OK to me.  There are some issues with
timing, but since this is for miner coordination of segwit activation, and
has little to do with other network users, it could be included as an
option.   (I'm a fan of adding options;plugins, etc. to Bitcoin... some
others aren't.)

- This hard fork portion of the proposal is being deployed with "emergency"
speed... even though there is not an emergency on the network today that I
am aware of.   If enacted, it will certainly result in two chains - and
with no replay protection..  The results of this will be confusing - two
ledgers with many transactions appearing on both and others appearing only
on one.

- The BIP should be modified to provide evidence and justification for the
timeline that is consistent with the level of risk the network would bear
if it were enacted.

- The coercion used to drive production of this BIP is mired in a
misinterpretation of BIP9 and sets a precedent for Bitcoin that may
undermine the value prospect of all cryptocurrency in general.   For this
reason alone - even if all of the engineering concerns and timelines are
improved - even assigning this BIP a number could be considered
irresponsible.

- If you still want to code up a fork for the Bitcoin network, consider
starting with Luke's hard fork code and changing the rates of growth as
needed for your desired effect.   Also you might want to read this first
(code references are in there):
https://petertodd.org/2016/hardforks-after-the-segwit-blocksize-increase .
Plans are already underway for a hard fork, for reasons that have nothing
to do with block size, but could include a timeline for a block size growth
consistent with global average residential bandwidth growth.
-------------------------------------
Since I have been working on crypto currencies/bitcoin, I kept
repeating: btc should make a priority to significantly increase the
ridiculous number of full nodes we have today, design an incentive for
people to run full nodes and design a system for people to setup full
nodes in an acceptable timeframe

Unfortunately, this was not a priority at all, maybe because of some
historical consensus between miners and devs, so here we are today, some
miners became crazy, the situation would be much more different if more
full nodes were there

Because, how comes everybody perfectly knows the plans of the conspiring
miners? They were stupid enough to explain very precisely how they will
perform the attack?

If I were them I would in addition setup quite a lot of nodes (which
probably they are planning to do, because anyway they need them for the
new sw), not difficult, not so expensive

Defending against abnormal blocks looks to be a non issue, I suppose
that the btc devs perfectly know how to create a pattern based on
history to detect abnormal blocks (including some strange transactions)
and reject them, but this further depends on the ability of current full
nodes to upgrade, which apparently is not what they do the best

I don't know what "Time is running short I fear" stands for and when 50%
is supposed to be reached

Given that it looks difficult to quickly increase the number of full
nodes, that increasing the mining power by standard means looks too
expensive, useless and not profitable, that a counter attack based on a
new proof of something does not look to be ready, then maybe the btc
folks should ask Bram Cohen (who by some luck is participating to this
list) to resurrect the bitcoin miner Epic Scale which Bittorrent Inc (in
an umpteenth dubious attempt to make money) tried some time ago to
include quietly in utorrent forgetting to ask the authorization of the
selected users, then utorrent users might upgrade (potentially 150 M),
and the resulting mining power might be sufficient, depending on the
incentive for this, which is TBD

Or activate by anticipation proof of space... unlike bitcoin-qt,
utorrent sw is quite good to be intrusive, run in background when you
think you have closed it, run things you don't know, etc, so quite
efficient in this situation

Then if btc folks wants to promote full nodes too, it would not be a bad
idea to put the bitcoin-qt blockchain + chain state in a torrent making
sure it is seeded correctly (there are plenty of academics here, they
can do it and run full nodes too) so people can download it and setup
full nodes (incentive TBD too) assuming they know about upnp/port forwarding


Le 24/03/2017  17:03, CANNON via bitcoin-dev a crit :
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
No, there could only have not more than 201 opcodes in a script. So you may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB

For OP_CAT, just check if the returned item is within the 520 bytes limit.


-------------------------------------
On Wed, Jan 04, 2017 at 11:45:18PM -0800, Eric Voskuil via bitcoin-dev wrote:

And it's a great way to tell every miner who you are and what
transactions you are sending/receiving. An absolute privacy
nightmare...

-- cdecker

-------------------------------------
Miner signalling is not enough to avoid two forks - as has been proven in
the past (e.g. when miners signaled they were fully validating blocks when
there we in fact only validating headers). To really protect against two
forks happening, the code needs to detect this happening (i.e. monitor the
other fork) and if it's clear that the signalling was dishonest, then it
needs to abort, IMHO.

On Thu, Apr 6, 2017 at 10:42 PM, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Addresses are entirely a user-interface issue. They don’t factor into the bitcoin protocol at all.

The bitcoin protocol doesn’t have addresses. It has a generic programmable signature framework called script. Addresses are merely a UI convention for representing common script templates. 1.. addresses and 3… addresses have script templates that are not as optimal as could be constructed with post-segwit assumptions. The newer bech32 address just uses a different underlying template that achieves better security guarantees (for pay-to-script) or lower fees (for pay-to-pubkey-hash). But this is really a UI/UX issue.

A “fork” in bitcoin-like consensus systems has a very specific meaning. Changing address formats is not a fork, soft or hard.

There are many benefits to segregated witness. You may find this page helpful:

https://bitcoincore.org/en/2016/01/26/segwit-benefits/ <https://bitcoincore.org/en/2016/01/26/segwit-benefits/>

-------------------------------------
Given the overwhelming support for SegWit across the ecosystem of businesses and users, this seems reasonable to me.

On May 22, 2017 6:40:13 PM EDT, James Hilliard via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
accept a particular chain is an important part of bitcoins security
model.

What you're describing is effectively the same as BU.

Nodes follow chains, they do not decide the victor.  The average user
follows the default of the software, which is to follow the longest valid
chain.  Forcing the average user to decide which software to run is far
more valuable than allowing "the software" to decide things, when in fact
all it will do is decide the previous default.


This is true and a good point.  A false signal from miners could trick the
honest miners into forking off prematurely with a minority.


This is the job of the stratum server and the pool operator.  These are
distinct responsibilities; Miners should choose a pool operator in line
with their desires.  Solo mining is basically dead, as it will never again
be practical(and has not been for at least 2 years) for the same hardware
that does the mining to also do full node operation.

If the pool operator/stratum server also does not do validation, then any
number of problems could occur.




On Mon, Jun 12, 2017 at 10:44 PM, James Hilliard via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Sure, was just upper bounding it anyways. Even less of a problem!


RE: OP_CAT, not as OP_CAT was specified, which is why it was disabled. As
far as I know, the elements alpha proposal to reenable a limited op_cat to
520 bytes is somewhat controversial...



--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>

On Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau <jl2012@xbt.hk> wrote:

-------------------------------------
Lets call it blocktrain instead of blockchain. Because it is fixed amount
of blocks moving forward on the time axis. Oldest blocks are detached from
the tail of that blockTrain and goes to depot.

2017-09-26 4:33 GMT+03:00 Patrick Sharp via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
Based on how fast we saw segwit adoption, why is the BIP149 timeout so
far in the future?

It seems to me that it could be six months after release and hit the
kind of density required to make a stable transition.

(If it were a different proposal and not segwit where we already have
seen what network penetration looks like-- that would be another
matter.)

-------------------------------------


Why so? It seems you are describing it as a softfork. With hardfork or extension block, a new rule could simply grant extra space when the tagged UTXO is spent. So if the usual block size limit is 1MB, when the special UTXO is made, the block size limit decreases to 1MB-700 byte, and the user has to pay for that 700 byte. When it is spent, the block size will become 1MB+700 byte.

But miners or even users may abuse this system: they may try to claim all the unused space when the blocks are not congested, or when they are mining empty block, and sell those tagged UTXO later. So I think we need to limit the reservable space in each block, and deduct more space than it is reserved. For example, if 700 bytes are reserved, the deduction has to be 1400 byte.

With BIP68, there are 8 unused bits in nSequence. We may use a few bits to let users to fine tune the space they want to reserve. Maybe 1 = 256 bytes

I think this is an interesting idea to explorer and I’d like to include this in my hardfork proposal.

-------------------------------------
Hi,
Just a thought,
Bitcoin developers shouldn't care about miners business model, they can
always sell their hw and close the bz as soon as bitcoin hardforks to
better ways of doing.
Just focus on making a better cryptocurrency, the more decentralized the
best.

M




-------------------------------------
If this was in place I would contribute more and I wouldn't have to create
throw-away accounts.

This is not a space where you want to be a recognisable target.

Today, BitFury's CEO threatened to sue developers if they didn't kowtow to
his demands to leave the PoW alone. This is unacceptable. Decisions have to
be made on merit and the interest of the project, and nothing else.

This is very important and needs to be given priority. Most Core developers
and all the main ones except Satoshi have built a public persona, either
for ego or for practical monetary reasons. Obviously there's academia where
everything is about plastering your name as much as possible and getting
cited. So it's understood. Although I understand the difficulty of getting
funded and getting trusted without a face, there needs to be an outlet so
people can interact and contribute in a proper cypherpunk way.

Also, GitHub is quite anti-privacy. So I recommend not reusing personal
accounts from work.

-muyuu
-------------------------------------
Tom,

It's clear that you have some rather large gaps in your knowledge of Bitcoin, its rules, implementation and game theory. I highly encourage you spend some time learning more about these things before continuing posting here. 

https://www.reddit.com/r/BitcoinBeginners/ is a good place to start. It's a safe place where you can ask any question you want without fear of being laughed at.

Kind regards,


Jean-Paul

-------------------------------------
I strongly disagree here - we don't only soft-fork out transactions that
are "fundamentally insecure", that would be significantly too
restrictive. We have generally been willing to soft-fork out things
which clearly fall outside of best-practices, especially rather
"useless" fields in the protocol eg soft-forking behavior into OP_NOPs,
soft-forking behavior into nSequence, etc.

As a part of setting clear best-practices, making things non-standard is
the obvious step, though there has been active discussion of
soft-forking out FindAndDelete and OP_CODESEPARATOR for years now. I
obviously do not claim that we should be proposing a soft-fork to
blacklist FindAndDelete and OP_CODESEPARATOR usage any time soon, and
assume that it would take at least a year or three from when it was made
non-standard to when a soft-fork to finally remove them was proposed.
This should be more than sufficient time for folks using such weird (and
largely useless) parts of the protocol to object, which should be
sufficient to reconsider such a soft-fork.

Independently, making them non-standard is a good change on its own, and
if nothing else should better inform discussion about the possibility of
anyone using these things.

Matt

On 11/15/17 14:54, Mark Friedenbach via bitcoin-dev wrote:

-------------------------------------
I can speak from personal experience regarding another very prominent
altcoin that attempted to utilize an asic-resistant proof of work
algorithm, it is only a matter of time before the "asic resistant"
algorithm gets its own Asics.  The more complicated the algorithm, the more
secretive the asic technology is developed.  Even without it,
multi-megawatt gpu farms have already formed in the areas of the world with
low energy costs.  I'd support the goal if I thought it possible, but I
really don't think centralization of mining can be prevented.

On Apr 9, 2017 1:16 PM, "Erik Aronesty via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Fri, Apr 14, 2017 at 3:58 PM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
Greg is correct. There is effectively no risk to a non-upgrade
accidentally mining a new invalid block itself, the only risk is that
a non-upgraded miner could itself mine on top of an invalid block. You
would have to intentionally modify the code to mine an invalid block
which is not something that would be likely to happen accidentally.

-------------------------------------
On Mar 5, 2017 6:48 PM, "Eric Voskuil" <eric@voskuil.org> wrote:


Independent of one's opinion on the merits of one fork or another, the
state of centralization in Bitcoin is an area of great concern. If "we" can
sit down with 75% of the economy and/or 90% of the hash power (which of
course has been done) and negotiate a change to any rule, Bitcoin is a
purely political money.

If "we" can do this, so can "they".

e


There is no doubt that politics play a big role in all of this. Also no
doubt that broader decentralization would be superior. But miner activated
soft forks and user activated soft forks do not need discussions with
centralized parties to move forward. It is merely two different methods for
pushing a soft fork through the network.

The key is that it's a soft fork. Old nodes continue to work as always,
whether the soft fork deploys or not.

User activated soft forks, or perhaps more accurately called 'economically
forced soft forks' are a tool to use if the miners are in clear opposition
to the broader economy. They only work if the broader economy actually
fully supports the soft fork, which is much more difficult to measure than
miner support. And miners with deeper pockets may be able to resist for
some time, effectively performing a rewardless 51% attack and maintaining a
split network for some time. The miners would lose lots of money, but old
nodes would feel all the burn of a hard fork, followed by a sudden deep
reorg when the network finally 'heals'.

I guess in some sense you'd be playing chicken with the miners. If the
split is not instantly successful there would be a lot of damage to old
nodes, even if the majority of new nodes had upgraded. (but there would
also be a lot of damage to the miners).

On Mar 5, 2017 9:31 PM, "Nick ODell" <nickodell@gmail.com> wrote:

the UASF has the higher coin price, the other chain will be annihilated. If
the UASF has a lower coin price, the user activated chain can still exist
(though their coins can be trivially stolen on the majority chain).

I don't think that's true. Say there are two forks of Blahcoin. Alice
thinks there's a 55% chance that Fork A will succeed. Bob thinks there's a
55% chance that Fork B will succeed. Alice trades all of her Fork B coins
for all of Bob's Fork A coins. Now, Bob and Alice both have a stake in one
fork or the other succeeding. Alice starts spending more time around Fork A
users; Bob starts spending his time with Fork B users.


This is not relevant to a UASF. The existing nodes on the network have a
single formal definition for longest chain. If the UASF is successful, the
old nodes will follow the new soft fork and there will be only one chain.
Spirit of Bitcoin or not, the UASF is successful and there is no coin split
or network fork.
-------------------------------------
Hello,

I am new, so apologies if this has been asked before.

Here are a few questions to start with -

I was wondering in terms of mass adoption, instead of long wallet
addresses, maybe there should be a DNS-like decentralized mapping service
to provide a user@crypto address?

This address translation can happen with confirmations from the network. So
instead of providing a long string, or a QR code that needs an app, you
simply type in a human readable address, and the wallet software converts
it to a wallet address.

Please let me know where I can research this more - if there already is
literature about this somewhere.

thanks!
-------------------------------------
Only if your keys are online and the transaction is self-signed. It wouldnt let you pre-sign a transaction for a third party to broadcast and have it clear at just the market rate in the future. Like a payment channel refund, for example.

-------------------------------------
Is that what passes for a technical argument these days? Sheesh.

Whereas in Drivechain users are forced to give up their coins to a single group for whatever sidechains they interact with, the generic sharding algo lets them (1) keep their coins, (2) trust whatever group they want to trust (the miners of the various sidechains).

Drivechain offers objectively worse security.

--
Sent from my mobile device.
Please do not email me anything that you are not comfortable also sharing with the NSA.

-------------------------------------
On Thursday, 20 April 2017 22:32:12 CEST Andrew Poelstra wrote:

Nice to join bitcoin-dev, Andrew. Haven’t seen you post here before.

I’m not sure how you reached that strange number, but I have to point out 
your number is quite useless.

The actual amount of nodes you need to be 100% sure you find all the blocks 
when you know each node will have a completely random 25% of the blocks is 
not a maths problem that leads to a single answer because of the randomness 
involved.
The actual answer is a series of probabilities.

Same as the answer is to the age old question; how many coin flips does it 
take to be 100% certain I have at least one “Heads”.

In our blocks retrieval scenario; with num-nodes < 4, probability is zero.
There is a really really small chance you will get 100% of the blocks with 4 
nodes (actual number depends on the amount of total blocks you are looking 
for).
And this goes up as you add more nodes, but never reaches 100%

At the other end of this question you can ask what the chance is of at least 
one block being lost when there are N nodes, a block nobody has. That chance 
is small with current > 6000 nodes, but not zero (a second reason why the 
previous parag never reaches 100%).

Bottom line, it is silly to assume 100% of the nodes would be partial-
pruning, and if you continue on that path you will only have probabilities 
to predict how many nodes it takes to have 100% coverage, exact numbers are 
worse than useless, they are misleading.

As I said in my initial email, statistics is hard. Crypto is much easier in 
that it is absolute. Either correct or false. Never in between.

To repeat, the goal of this pruning method is not to replace a full 
“archival” node, the goal of this pruning node is to provide an improvement 
over the current pruning node which stops any and all serving of historical 
blocks.
Anyone that feels the need to talk about pruning modes like 100% of the full 
nodes will run it are in actual fact not talking about the real world. 
Distributed systems will never (and should never) end up being a mono-
culture. Diversity is the essential thing you aim for.

I would suggest we focus on the real world and not on irreleavant math 
experiments that only lead to confusion.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 04/07/2017 11:39 AM, Bram Cohen via bitcoin-dev wrote:

While this may seem to be the case it is not generally optimal. The
question is overly broad as one may or may not be optimizing for any
combination of:

startup time (first usability)
warm-up time (priming)
shutdown time (flush)
fault tolerance (hard shutdown survivability)
top block validation (read speed)
full chain validation (read/write speed)
RAM consumption
Disk consumption
Query response
Servers (big RAM)
Desktops (small RAM)
Mining (fast validation)
Wallets (background performance)
SSD vs. HDD

But even limiting the question to input validation, all of these
considerations (at least) are present.

Ideally one wants the simplest implementation that is optimal under
all considerations. While this may be a unicorn, it is possible to
achieve a simple implementation (relative to alternatives) that allows
for the trade-offs necessary to be managed through configuration (by
the user and/or implementation).

Shoving the entire data set into RAM has the obvious problem of
limited RAM. Eventually the OS will be paging more of the data back to
disk (as virtual RAM). In other words this does not scale, as a change
in hardware disproportionately impacts performance. Ideally one wants
the trade between "disk" and "memory" to be made by the underlying
platform, as that is its purpose. Creating one data structure for disk
and another for memory not only increases complexity, but denies the
platform visibility into this trade-off. As such the platform
eventually ends up working directly against the optimization.

An on-disk structure that is not mapped into memory by the application
allows the operating system to maintain as much or as little state in
memory as it considers optimal, given the other tasks that the user
has given it. In the case of memory mapped files (which are optimized
by all operating systems as central to their virtual memory systems)
it is possible for everything from zero to the full store to be memory
resident.

Optimization for lower memory platforms then becomes a process of
reducing the need for paging. This is the purpose of a cache. The seam
between disk and memory can be filled quite nicely by a small amount
of cache. On high RAM systems any cache is actually a de-optimization
but on low RAM systems it can prevent excessive paging. This is
directly analogous to a CPU cache. There are clear optimal points in
terms of cache size, and the implementation and management of such a
cache can and should be internal to a store. Of course a cache cannot
provide perfect scale all the way to zero RAM, but it scales quite
well for actual systems.

While a particular drive may not support parallel operations one
should not assume that a disk-based store does not benefit from
parallelism. Simply refer to the model described above and you will
see that with enough memory the entire blockchain can be
memory-resident, and for high performance operations a fraction of
that is sufficient for a high degree of parallelism.

In practice a cache of about 10k transactions worth of outputs is
optimal for 8GB RAM. This requires just a few blocks for warm-up,
which can be primed in inconsequential time at startup. Fault
tolerance can be managed by flushing after all writes, which also
reduces shutdown time to zero. For higher performance systems,
flushing can be disabled entirely, increasing shutdown time but also
dramatically increasing write performance. Given that the blockchain
is a cache, this is a very reasonable trade-off in some scenarios. The
model works just as well with HDD as SSD, although certainly SSD
performs better overall.

e
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBCAAGBQJY5+7GAAoJEDzYwH8LXOFOsAsH/3QK55aWH6sAi6OsTwV1FLZV
Y/2SSjwn1vUh55MDkPpCxDwV99JqVwpk0vGM8mGg5s4ZS8sxOPqwGiBz/SZWbF9v
oStJS0DjUPnbYtI/mrC30GuAYVcKnc5DFDHvjX6f0xrLIzViFR7eiW0npUH6Xipt
RI9Mockaf1CqqGExtbIqWal0YDEQGH0ekXRp7uEjh8nPUoKqTVvxDCgqVooQfvfx
EeKX9ruSv/r91EM1JQuH8HBBF7+R24tmMtwbpGx0zrDg5ytpIyrRzVH/ze1Mj2a3
ZxThvofGzhKcDiTPWiJI11DBYUvhSH4Kx0uWLzFUA0gxPfWkZQKJWNDl2CEwljk=
=C7rD
-----END PGP SIGNATURE-----

-------------------------------------
Hello Gregory,

Thanks for you feedback.

The BIP has been updated to explicitly specify the multiparty key
derivation scheme which hopefully addresses your concerns.

Please have a look at the updated draft of the BIP at the link below:

https://github.com/commerceblock/pay-to-contract-protocol-specification/blob/master/bip-draft.mediawiki

Any feedback is highly appreciated.

Regards,
Omar

On Tue, Aug 15, 2017 at 7:40 PM, omar shibli <omarshib@gmail.com> wrote:

-------------------------------------
Hi Juan





  Protocol development, especially one in control of people's money cannot be based on beliefs. Do you have actual data to show significant increases in desktop CPU, memory and bandwidth?


All empirical evidence points to the opposite.

Intel has been struggling to eek out 5-10% gains for each generation of its CPUs. The growth of the total blockchain size at 1MB alone is much faster than this.

CPU Core counts have also been stagnant for a decade.

Disk Space growth has also been slowing and with the trend towards SSDs, available disk space in a typical PC has turned negative sharply.


Regards

Luv


________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of Juan Garavaglia via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Wednesday, March 29, 2017 6:36 AM
To: Alphonse Pace; Wang Chun
Cc: Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] Hard fork proposal from last week's meeting


Alphonse,



Even when several of the experts involved in the document you refer has my respect and admiration, I do not agree with some of their conclusions some of their estimations are not accurate other changed like Bootstrap Time, Cost per Confirmed Transaction they consider a network of 450,000,00 GH and today is 3.594.236.966 GH, the energy consumption per GH is old, the cost of electricity is wrong even when the document was made and is hard to find any parameter used that is valid for an analysis today.



Again with all respect to the experts involved in that analysis is not valid today.



I tend to believe more in Moores law, Butters' Law of Photonics and Kryders Law all has been verified for many years and support that 32 MB in 2020 are possible and equals or less than 1 MB in 2010.



Again may be is not possible Johnson Lau and LukeJr invested a significant amount of time investigating ways to do a safe HF, and may be not possible to do a safe HF today but from processing power, bandwidth and storage is totally valid and Wang Chung proposal has solid grounds.



Regards



Juan





From: Alphonse Pace [mailto:alp.bitcoin@gmail.com]
Sent: Tuesday, March 28, 2017 2:53 PM
To: Juan Garavaglia <jg@112bit.com>; Wang Chun <1240902@gmail.com>
Cc: Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] Hard fork proposal from last week's meeting



Juan,



I suggest you take a look at this paper: http://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf  It may help you form opinions based in science rather than what appears to be nothing more than a hunch.  It shows that even 4MB is unsafe.  SegWit provides up to this limit.

On Scaling Decentralized Blockchains<http://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf>
fc16.ifca.ai
On Scaling Decentralized Blockchains (A Position Paper) Kyle Croman 0 ;1, Christian Decker 4, Ittay Eyal , Adem Efe Gencer , Ari Juels 0 ;2, Ahmed Kosba 0 ;3, Andrew ...





8MB is most definitely not safe today.



Whether it is unsafe or impossible is the topic, since Wang Chun proposed making the block size limit 32MiB.





Wang Chun,

Can you specify what meeting you are talking about?  You seem to have not replied on that point.  Who were the participants and what was the purpose of this meeting?



-Alphonse



On Tue, Mar 28, 2017 at 12:33 PM, Juan Garavaglia <jg@112bit.com<mailto:jg@112bit.com>> wrote:

Alphonse,



In my opinion if 1MB limit was ok in 2010, 8MB limit is ok on 2016 and 32MB limit valid in next halving, from network, storage and CPU perspective or 1MB was too high in 2010 what is possible or 1MB is to low today.



If is unsafe or impossible to raise the blocksize is a different topic.



Regards



Juan





From: bitcoin-dev-bounces@lists.linuxfoundation.org<mailto:bitcoin-dev-bounces@lists.linuxfoundation.org> [mailto:bitcoin-dev-bounces@lists.linuxfoundation.org<mailto:bitcoin-dev-bounces@lists.linuxfoundation.org>] On Behalf Of Alphonse Pace via bitcoin-dev
Sent: Tuesday, March 28, 2017 2:24 PM
To: Wang Chun <1240902@gmail.com<mailto:1240902@gmail.com>>; Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>>
Subject: Re: [bitcoin-dev] Hard fork proposal from last week's meeting



What meeting are you referring to?  Who were the participants?



Removing the limit but relying on the p2p protocol is not really a true 32MiB limit, but a limit of whatever transport methods provide.  This can lead to differing consensus if alternative layers for relaying are used.  What you seem to be asking for is an unbound block size (or at least determined by whatever miners produce).  This has the possibility (and even likelihood) of removing many participants from the network, including many small miners.



32MB in less than 3 years also appears to be far beyond limits of safety which are known to exist far sooner, and we cannot expect hardware and networking layers to improve by those amounts in that time.



It also seems like it would be much better to wait until SegWit activates in order to truly measure the effects on the network from this increased capacity before committing to any additional increases.



-Alphonse







On Tue, Mar 28, 2017 at 11:59 AM, Wang Chun via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:

I've proposed this hard fork approach last year in Hong Kong Consensus
but immediately rejected by coredevs at that meeting, after more than
one year it seems that lots of people haven't heard of it. So I would
post this here again for comment.

The basic idea is, as many of us agree, hard fork is risky and should
be well prepared. We need a long time to deploy it.

Despite spam tx on the network, the block capacity is approaching its
limit, and we must think ahead. Shall we code a patch right now, to
remove the block size limit of 1MB, but not activate it until far in
the future. I would propose to remove the 1MB limit at the next block
halving in spring 2020, only limit the block size to 32MiB which is
the maximum size the current p2p protocol allows. This patch must be
in the immediate next release of Bitcoin Core.

With this patch in core's next release, Bitcoin works just as before,
no fork will ever occur, until spring 2020. But everyone knows there
will be a fork scheduled. Third party services, libraries, wallets and
exchanges will have enough time to prepare for it over the next three
years.

We don't yet have an agreement on how to increase the block size
limit. There have been many proposals over the past years, like
BIP100, 101, 102, 103, 104, 105, 106, 107, 109, 148, 248, BU, and so
on. These hard fork proposals, with this patch already in Core's
release, they all become soft fork. We'll have enough time to discuss
all these proposals and decide which one to go. Take an example, if we
choose to fork to only 2MB, since 32MiB already scheduled, reduce it
from 32MiB to 2MB will be a soft fork.

Anyway, we must code something right now, before it becomes too late.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev




-------------------------------------
Most people do not want to go out and buy new hardware to run a Bitcoin
node. The want to use the hardware that they already own, and usually that
hardware is going to have a non-generous amount of disk space. 500GB SSD
with no HDD is common in computers today.

But really, the best test is to go out and talk to people. Ask them if they
run a full node, and if they say no, ask them why not. In my experience,
the most common answer by a significant margin is that they don't want to
lose the disk space. That psychology is far more important than any example
of cheap hard drives. People don't want to go out and buy a hard drive so
that they can run Bitcoin. It's a non-starter.
-------------------------------------
On Mon, 2017-03-06 at 16:54 -0500, Tim Ruffing via bitcoin-dev wrote:
Forgot one thing:
For longpolling, maybe we would like to have the possibility to request
some periodic message from the server. Otherwise clients cannot
distinguish between the situations 1. "value is still in the requested
bounds (minrate, maxrate)" and 2. "connection has dropped". So the user
may take a wrong decision because  he assumed that the value is still
in bounds holds but actually the server has died.

Tim 

-------------------------------------
On Wednesday 05 July 2017 8:06:33 AM Gregory Maxwell via bitcoin-dev wrote:

Nothing is "orphaned" unless miners are acting negligently or maliciously. 
Incentivising honest behaviour from miners is inherently part of Bitcoin's 
design, and these changes are necessary for both that and keeping the network 
secure. This doesn't do harm; it reduces risk of harm.


I don't appreciate the uncalled-for character assassination, and it doesn't 
belong on this mailing list.


Since you apparently have a drastically different opinion on this subject, I 
think it may be best to wait until after BIP148 to continue the discussion 
(thereby having more real-world information to work from).

Therefore, I have opened a new pull request with just the parts you seem to be 
objecting to removed. Please let us know if this version is satisfactory.

    https://github.com/bitcoin/bips/pull/551

Luke

-------------------------------------
Hi guys,

I wonder why automake has become the build system for Bitcoin Core?
I mean - why not cmake which is considered better?
Can you please point to the relevant discussion or explanation?

Thanks,
--- Kosta Z.
-------------------------------------
I agree, addresses create vulnerability, an OP_RETURN signal seems the
safest way to go for UA signalling.   I can model a BIP after BIP9, with
some discussion of how to properly collect statistics, and the ability for
nodes to activate features based on an "economic majority" defined in this
way.

On Tue, Apr 18, 2017 at 6:29 PM, Tim Ruffing via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

It's a small amount by itself, but miners who are aware of Bounty Payout
Transaction will try to include both these transactions (and both are valid
both on SW and non-SW chains by definition of SW being a soft fork).

If you set timelock of BPT to (H+1) then you sort of discourage this
behavior because a miner of block H might be not the same as miner of block
(H+1), thus he cannot grab this bounty for sure.

Still, there is a chance that same miner will mine both blocks, so
game-theoretically it makes sense to insert SAT into your block since your
expected payoff is positive.

So I'm afraid miners will just grab these bounties regardless of segwit
activation.
-------------------------------------
Responses inline.

On 6/22/2017 9:45 AM, Erik Aronesty wrote:

Thus far you've claimed that these transactions would be "cheap", "[not]
controlled by miners", and "secure".

They would certainly not be cheap, because they are relatively more
expensive due to the extra depreciation cost.

I also doubt that they would be free of control by miners. 51% hashrate
can always filter out any message they want from anywhere.

For the same reason, I don't understand why they would be any more or
less secure.

So I think your way is just a more expensive way of accomplishing
basically the same result.


As I posted to bitcoin-discuss last week, I support UTXO commitments for
sidechains.


I don't think that blind merged mining messes with the main chain's
incentive structure. Miners are free to ignore the sidechain (and yet
still get paid the same as other miners), as are all mainchain users.

Paul

-------------------------------------

Not necessarily. When the BIP50 hard fork happened, it didn't create
two incompatible ledgers. It *could* have, but it didn't. If every
single transaction mined during that time has been "double spent" on
the other chain, then it would have created a very bad situation. When
one side of the fork gets abandoned, actual users would have lost
money. Since only one person was able to perform this double spend,
only the miners and that one double spender lost money when the one
side was abandoned. If there had been a significant number of users
who had value only on the chain that was eventually abandoned, that
chain would have incentive to not be abandoned and that *would* have
resulted in a permanent incompatible split. It was essentially the
replay *effect* (not "attack") that allowed bitcoin to survive that
hard fork. BIP50 was written before the term "replay attack" or
"replay effect" has been coined, so it doesn't say much about how
transactions replayed...

On 1/25/17, Johnson Lau <jl2012@xbt.hk> wrote:

-------------------------------------


On 5/22/2017 8:13 PM, ZmnSCPxj wrote:

Softforked, of course.

  From my understanding, the code as

Your understanding may exceed my own. I don't understand the principle
of your distinction, as it seems to me that one could add a new protocol
rule which says that the block is invalid unless the OP Code does
results in arbitrary-item-x. The intent is to mimic CLTV or CSV
behavior, by causing something that would otherwise succeed, to fail, if
arbitrary new conditions are met.


That would indeed be a bug, if it happened as you described. I will
check when I get the chance, thanks.


Yes. Sorry if that was confusing.


The sidechain software can indeed, but the mainchain software cannot
(without making validation of both chains part of the mainchain, which
defeats the original purpose of sidechains).

The purpose of OP_BRIBE is to allow "Sam" (on the sidechain) and "Mary"
(a mainchain miner) to work together. Sam would pay X BTC to Mary, if
Mary could provide Sam with some guarantee that Sam's sidechain block
[defined by h*] would make it into the largest chain.

So, as I see it, this needs to be a mainchain consensus rule, but one
which enforces the bare minimum criteria.



If A, B, and C are transacting, and each has an account on both chains.
Then your example would be something like:

1. main:A sends 100 to side:A, then transfers 100 to side:B in exchange
for B's good or service (provided on the sidechain)
2. side:B attempts to move side-to-main with the 100 BTC, using the
lightning network. He swaps 100 side:BTC for 100 of C's main:BTC.
3. C attempts to move side-to-main, using the slow, settlement method.
4. C's side-to-main sidechain tx (wt) is bundled with others and becomes
a withdrawal attempt (WT^)
5. The WT^ attempt is initiated on the mainchain.
6. After a waiting period, the WT^ begins to accumulate ACKs or NACKs
(upvotes / downvotes), on the mainchain.
7. The transaction either succeeds or fails.

I'm not sure, but your question seems to concern B, who exploits a reorg
that happens just after step 2. After the reorg, the sidechain chain
history will have a different side-to-main withdrawal in part 3. The
time between each of these step is very long, on the order of weeks
(summing to a length of time totaling months), for exactly this reason
(as well as to encourage people to avoid using this 'formal' method, in
favor of the cooperative LN and Atomic Swaps).

I think that this principle of scale (ie, very VERY slow withdrawals) is
important and actually makes the security categorically different.

For extraordinary DAO-like situations, disinterested mainchain miners
merely need a single bit of information (per sidechain), which is
"distress=true", and indicates to them to temporarily stop ACKing
withdrawals from the sidechain. This alone is enough to give the reorg
an unlimited amount of time to work itself out.



Correct


Mainchain miners do need to maintain some data about the sidechains, but
this is very minimal, and certainly does not include the transaction
data (or arbitrary messages) of the sidechain.

No problem.


In one sense, I mean "you have already endorsed this 'fees only will
work' premise, by endorsing Bitcoin".

In another sense I mean "isn't it great that you will get a tiny
preview, today, of future-Bitcoin's behavior?".


That mechanism is enforced by drivechain itself, not OP_BRIBE. (OP Bribe
is itself only ~half of BMM. I admit it is getting a little confusing.)

Drivechain requires a soft fork to add each new sidechain. It requires
this literally for a few good reasons...but the best is: there is an
implicit requirement that the miners not steal from the sidechain
anyway. In this way drivechain knows how to keep track of what it should
expect.


Precisely.


I hope that my replies above already help with these. If not, let me know.

Thanks for your attention,
Paul

-------------------------------------
Hi ZmnSCPxj,

However, a lockbox on one chain is a WT on the other


I'm not sure if I follow what you are saying here. What do you mean by
'free lockbox'? I was assuming that I created an arbitrary blockchain, say
ChrisChain, that is NOT pegged to the bitcoin blockchain? I.e. the tokens
on ChrisChain are worthless. Then I create a lockbox on ChrisChain with my
worthless tokens and attempt to transfer them into TeeCoin's chain? However
this doesn't make sense with


However, this parameter is used to determine if it is a WT.  Sidechain

because I could arbitrarily set this parameter to 0. It seems that a
sidechain upon inception should pay all of it's tokens to a single UTXO and
prevent minting of coins after that. I'm fairly certain this is what
elements does in it's genesis block.

The is unrelated to the problem above, but it will be a problem in
sidchain-headers-on-mainchain if we have a limited amount of mining slots
in the coinbase_tx output vector.

Let us assume we have a fixed set of sidechain slots in the coinbase output
vector, in this case 10. However there are 15 competing sidechains for
these 10 slots. It may be possible for sidechains (say 15 sidechains) to
compete indefinitely for these 10 slots -- causing indefinite forks. Let us
say sidechain 10 and sidechain 11 alternate block hashes in
coinbase_tx.vout[10] output. This means that a WT^ will never be considered
valid because it will appear to mainchain miners that there are competing
forks of the SAME sidechain, when in reality it is two unique sidechains
competing to mine the the limited coinbase output vector space.

-Chris

On Fri, Sep 8, 2017 at 9:56 AM, ZmnSCPxj <ZmnSCPxj@protonmail.com> wrote:

-------------------------------------
I like the idea of having some way for developers to show that they've
given an idea legitimate consideration, as I feel some proposals are often
considered much more in depth before rejection than the proposer realizes,
however I don't think any sort of on-chain system really makes sense. It
complicates things a lot, adds code, incentives, etc. when really all you
care about is some sort of indication of consideration, support, or
rejection.

I also prefer to think of Bitcoin as a system of vetos rather than a system
of approvals. A lot of times changes will be small, highly technical, and
have no visible impact to your every day user. These types of changes don't
really need support outside the devs. Furthermore, I frankly don't give a
crap if we proposal has support from 85% of the participants if there is a
legitimate technical, social, or political reason that it is a bad idea.

And finally, I don't think it should cost money or political power to raise
an objection. A 13yo who has never been seen before should be able to raise
an objection if they indeed have a legitimate objection. Involving money is
almost certainly going to shut down important valid opinions.

And again, I mostly agree with the motivation. It would be good if it were
easier to figure out who had considered a proposal and what their
objections or praises were. But I would like to see that without any
systemization around what is required to pass or fail a proposal, and with
no barrier to entry (such as voting or sending coins or having a recognized
name like 'Bitfury') to provide an opinion.
-------------------------------------
On Mon, Oct 30, 2017 at 9:42 PM, Matt Corallo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

For some framing-- I think we're still a long way off from proposing
something like this in Bitcoin, and _how_ it's ultimately proposed is
an open question.

There are many ways to use simplicity, for an extreme example:  one
could define a collection of high level operations and combinators at
the level of things in Bitcoin Script (op_sha256, op_equal, op_cat,
etc.)  and make an interpreter that implements these operations as
discounted jets and ONLY these operations at all.

At that point you have a system which is functionally like Bitcoin
Script-- with the same performance characteristics-- but with a pretty
much perfectly rigorous formal specification and which is highly
amenable to the formal analysis of smart contracts written in it.

At the other extreme, you expose a full on Bitmachine and allow
arbitrary simplicity--  But this is probably slow enough to not be
very useful.  Simplicity itself is so simple that it doesn't natively
have a concept of a _bit_, library code programs the concept of a bit,
then the concept of a half adder ... and so on.   As a result a
completely unjetted implementation is slow (actually remarkably fast
considering that it's effectively interpreting a circuit constructed
from pure logic).

The most useful way of using it would probably be in-between: a good
collection of high level functions, and mid-level functions (e.g.
arithmetic and string operations) making a wide space of useful but
general software both possible and high performance.  But to get there
we need enough experience with it to know what the requisite
collection of operations would be.

One challenge is that I don't think we have a clear mental model for
how nominal validation costs are allowed to be before there is a
negative impact.  It's probably safe to assume 'pretty darn nominal'
is a requirement, but there is still a lot that can be done within
that envelope.

As far as consensus discounted jets goes:


Is a particular script-root jetted or not in an implementation?
 -- In and of itself this is not of consensus consequence; esp.
because a major design feature of simplicity is that it should be
possible using to prove that an optimized C implementation of a
simplicity program is complete and correct (using VST+COQ).

Is a particular script-root 'standard and known' in the P2P network:
 -- This means that you can skip communicating it when sending
witnesses to peers; but this is something that could be negotiated on
a peer by peer basis-- like compressing transactions, and isn't at all
consensus normative.

Is a particular jet discounted and what are the discounts:
 -- This is inherently a consensus question; as the bitmachine costing
for a program is consensus normative (assuming that you allow
arbitrary simplicity code at all).

A script-versioning like mechanism can provide for a straight-forward
way to upgrade discounted cost tables in a compatible way--  if you're
running old software that doesn't have the required jets to justify a
particular discount collection -- well that's okay, you won't validate
those scripts at all. (so they'll be super fast for you!)

Another potential tool is the idea of sunsetting cost limits that
sunset; e.g. after N years, the limits go away with an assumption that
updated limits have been softforked in that ativate at that time and
themselves expire in N years.  Old software would become slower
validating due to newly discounted code they lack jets for... but
would continue validating (at least until they run out of performance
headroom).

This is theoretically attractive in a number of regards, but
unfortunately I think our industry hasn't shown sufficient maturity
about engineering tradeoffs to make this a politically viable choice
in the mid-term-- I known I'm personally uncomfortable with the
outspokenness of parties that hold positions which I think can fairly
be summarized "We should remove all limits and if the system crashes
and burns as a result, we'll just make a new one! YOLO.". But it's
interesting to think about in the long term.

There are also hybrid approaches where you can imagine this decision
being made by node operators, e.g. continuing to validate code that
exceeds your effort limits on probabilistic and best effort basis;
even more attractive if there were a protocol for efficiently showing
others that an operation had an invalid witness. Though there is a lot
to explore about the brittleness to partitioning that comes from any
expectation that you'd learn about invalid updates by exception.

In any case, these are all options that exist completely independently
of simplicity.  I think we should think of simplicity as a rigorous
base which we could _potentially_ use to build whatever future
direction of script we like out of... by itself it doesn't mandate a
particular depth or level of adoption.

And for the moment it's still also mostly just a base-- I don't
anticipate typical smart contracting end users programming directly w/
simplicity even if Bitcoin did support arbitrary simplicity--  I
expect they'd program in user friendly domain specific languages which
are formally tied to their implementations in simplicity that allow-
but do not force- closed loop formal reasoning about their contracts
all the way from their high level business rules straight through to
the machine code implementing the interpreter(s) that run in the
network.

But to get there we'll have to prove in practice that this is actually
workable. We have some evidence that it is,  e.g. Roconnor's SHA2
implementation in simplicity is proven to implement the same function
that a C implementation implements (via the compcert formalization of
C).  but there will need to be more.

-------------------------------------
On Tuesday, 7 March 2017 00:23:47 CET Gareth Williams via bitcoin-dev wrote:

It is incorrect to say that censoring of transactions is what Edmund 
suggested. It's purely about the form they take, you can re-send the 
transaction in a different form with the same content and they go through. 
Hence, not transaction censoring.

I do believe the point that Edmund brought up is a very good one, the idea 
that a set of users can force the miners to do something is rather silly and 
the setup that a minority miner fraction can force the majority to do 
something is equally silly. This is because the majority mining hashpower 
can fight back against this attack upon them.

Don’t be mistaken; a hash-minority attacking the hash-majority is in actual 
fact an attack upon Bitcoin as a whole.
If this were possible then next year we’d see governments try to push 
through changes in the same UASF way. I’m very happy that UASFs can’t work 
because that would be the end of Bitcoin's freedom and decentralized nature.


I definitely welcome that approach.

The result would be that you have two chains, but also you ensure that the 
chain that the miners didn’t like will no longer be something they can mine. 
Not even the minority set of miners that like the softfork can mine on it. 
This is a win-win and then the market will decide which one will "win".
 

This goes both ways, miners both generate value (in the form of security) 
and they take value (in the form of inflation).
If the majority of the users are hostile and reject blocks that the miners 
create, or change the POW, then what the miners bring to the table is also 
removed.
Bitcoin would lose the security and in the short term even the ability to 
mine blocks every 10 minutes.

So, lets correct your statement a little;
«Bitcoin only works when the majority of the hashpower and the (economic)
  majority of the users are balanced in power and have their goals aligned.»

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
"A. For users on both existing and new fork, anti-replay is an option,
not mandatory"

To maximize fork divergence, it might make sense to require this. Any
sensible proposal for a hard fork would include a change to the sighash
anyway, so might as well make it required, no?

Matt

On 01/24/17 14:33, Johnson Lau via bitcoin-dev wrote:

-------------------------------------
Good morning,


As of this moment, BT1 / BT2 price ratio in BitFinex is slightly higher than 7 : 1.  Twice the transaction rate cannot overcome this price ratio difference.  Even if you were to claim that the BitFinex data is off by a factor of 3, twice the transaction rate still cannot overcome the price ratio difference.  Do you have stronger data than what is available on BitFinex?  If not, your assumptions are incorrect and all conclusions suspect.


Mining infrastructure follows price.  If bitcoins were still trading at 1 USD per coin, nobody will build mining infrastructure to the same level as today, with 5000 USD per coin.

Price will follow user needs, i.e. demand.


For the very specific case of 2X, it is very easy to make this identification.  Even without understanding the work being done, one can reasonably say that it is far more likely that a loose group of 100 or more developers will contain a few good or excellent developers, than a group of a few developers containing a similar number of good or excellent developers.

User needs will get met only on the chain that good developers work on.  Bitcoin today has too many limitations: viruses on Windows can steal all your money, fee estimates consistently overestimate, fees rise during spamming attacks, easy to lose psuedonymity, tiny UTXOs are infeasible to spend, cannot support dozens of thousands of transactions per second.  Rationally, long-term hodlers will select a chain with better developers who are more likely to discover or innovate methods to reduce, eliminate, or sidestep those limitations.  Perhaps the balance will change in the future, but it is certainly not the balance now, and thus any difficulty algorithm change in response to the current situation will be premature, and far more likely to cause disaster than avert one.


This requires that all chains follow the same difficulty adjustment: after all, it is also entirely the possibility that 2X will be the lower-hashrate coin in a few months, with the Core chain bullying them out of existence.  Perhaps you should cross-post your analysis to bitcoin-segwit2x also.  After all, the 2X developers should also want to have faster price discovery of the true price of 2X, away from the unfavorable (incorrect?) pricing on BitFinex.


Are developers any wiser, either?

Then consider this wisdom: The fewer back-incompatible changes to a coin, the better.  Hardforks of any kind are an invitation to disaster and, at this point, require massive coordination effort which cannot be feasibly done within a month.  Fast market determination can be done using off-chain methods (such as on-exchange trades), and are generally robust against temporary problems on-chain, although admittedly there is a counterparty risk involved.  The coin works, and in general there is usually very little need to fix it, especially using dangerous hardforks.


Is that your goal?  This is a massive departure from the conception of Bitcoin as having a fixed limit and effectively becoming deflationary.  It will also lead to massive economic distortions in favor of those who receive newly-minted coins.  I doubt any developer would want to have this property.

Regards,
ZmnSCPxj
-------------------------------------
Great catch, and a good proposal for a fix.  Pushing the activation height out to allow existing hardware to enter obsolescence prior to activation may help reduce miner resistance.  It may also avoid legal threats from those currently abusing.  If miners still resist, the threat of an earlier activation height is a good negotiating tool.

Raystonn

On 5 Apr 2017 2:39 p.m., Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
A month ago I was explaining the attack on Bitcoin's SHA2 hashcash which
is exploited by ASICBOOST and the various steps which could be used to
block it in the network if it became a problem.

While most discussion of ASICBOOST has focused on the overt method
of implementing it, there also exists a covert method for using it.

As I explained one of the approaches to inhibit covert ASICBOOST I
realized that my words were pretty much also describing the SegWit
commitment structure.

The authors of the SegWit proposal made a specific effort to not be
incompatible with any mining system and, in particular, changed the
design at one point to accommodate mining chips with forced payout
addresses.

Had there been awareness of exploitation of this attack an effort
would have been made to avoid incompatibility-- simply to separate
concerns.  But the best methods of implementing the covert attack
are significantly incompatible with virtually any method of
extending Bitcoin's transaction capabilities; with the notable
exception of extension blocks (which have their own problems).

An incompatibility would go a long way to explain some of the
more inexplicable behavior from some parties in the mining
ecosystem so I began looking for supporting evidence.

Reverse engineering of a particular mining chip has demonstrated
conclusively that ASICBOOST has been implemented
in hardware.

On that basis, I offer the following BIP draft for discussion.
This proposal does not prevent the attack in general, but only
inhibits covert forms of it which are incompatible with
improvements to the Bitcoin protocol.

I hope that even those of us who would strongly prefer that
ASICBOOST be blocked completely can come together to support
a protective measure that separates concerns by inhibiting
the covert use of it that potentially blocks protocol improvements.

The specific activation height is something I currently don't have
a strong opinion, so I've left it unspecified for the moment.

<pre>
  BIP: TBD
  Layer: Consensus
  Title: Inhibiting a covert attack on the Bitcoin POW function
  Author: Greg Maxwell <greg@xiph.org>
  Status: Draft
  Type: Standards Track
  Created: 2016-04-05
  License: PD
</pre>

==Abstract==

This proposal inhibits the covert exploitation of a known
vulnerability in Bitcoin Proof of Work function.

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.

==Motivation==

Due to a design oversight the Bitcoin proof of work function has a potential
attack which can allow an attacking miner to save up-to 30% of their energy
costs (though closer to 20% is more likely due to implementation overheads).

Timo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,
which they have so far not licensed for free and open use by the public.
They have been marketing their patent licenses under the trade-name
ASICBOOST.  The document takes no position on the validity or enforceability
of the patent.

There are two major ways of exploiting the underlying vulnerability: One
obvious way which is highly detectable and is not in use on the network
today and a covert way which has significant interaction and potential
interference with the Bitcoin protocol.  The covert mechanism is not
easily detected except through its interference with the protocol.

In particular, the protocol interactions of the covert method can block the
implementation of virtuous improvements such as segregated witness.

Exploitation of this vulnerability could result in payoff of as much as
$100 million USD per year at the time this was written (Assuming at
50% hash-power miner was gaining a 30% power advantage and that mining
was otherwise at profit equilibrium).  This could have a phenomenal
centralizing effect by pushing mining out of profitability for all
other participants, and the income from secretly using this
optimization could be abused to significantly distort the Bitcoin
ecosystem in order to preserve the advantage.

Reverse engineering of a mining ASIC from a major manufacture has
revealed that it contains an undocumented, undisclosed ability
to make use of this attack. (The parties claiming to hold a
patent on this technique were completely unaware of this use.)

On the above basis the potential for covert exploitation of this
vulnerability and the resulting inequality in the mining process
and interference with useful improvements presents a clear and
present danger to the Bitcoin system which requires a response.

==Background==

The general idea of this attack is that SHA2-256 is a merkle damgard hash
function which consumes 64 bytes of data at a time.

The Bitcoin mining process repeatedly hashes an 80-byte 'block header' while
incriminating a 32-bit nonce which is at the end of this header data. This
means that the processing of the header involves two runs of the compression
function run-- one that consumes the first 64 bytes of the header and a
second which processes the remaining 16 bytes and padding.

The initial 'message expansion' operations in each step of the SHA2-256
function operate exclusively on that step's 64-bytes of input with no
influence from prior data that entered the hash.

Because of this if a miner is able to prepare a block header with
multiple distinct first 64-byte chunks but identical 16-byte
second chunks they can reuse the computation of the initial
expansion for multiple trials. This reduces power consumption.

There are two broad ways of making use of this attack. The obvious
way is to try candidates with different version numbers.  Beyond
upsetting the soft-fork detection logic in Bitcoin nodes this has
little negative effect but it is highly conspicuous and easily
blocked.

The other method is based on the fact that the merkle root
committing to the transactions is contained in the first 64-bytes
except for the last 4 bytes of it.  If the miner finds multiple
candidate root values which have the same final 32-bit then they
can use the attack.

To find multiple roots with the same trailing 32-bits the miner can
use efficient collision finding mechanism which will find a match
with as little as 2^16 candidate roots expected, 2^24 operations to
find a 4-way hit, though low memory approaches require more
computation.

An obvious way to generate different candidates is to grind the
coinbase extra-nonce but for non-empty blocks each attempt will
require 13 or so additional sha2 runs which is very inefficient.

This inefficiency can be avoided by computing a sqrt number of
candidates of the left side of the hash tree (e.g. using extra
nonce grinding) then an additional sqrt number of candidates of
the right  side of the tree using transaction permutation or
substitution of a small number of transactions.  All combinations
of the left and right side are then combined with only a single
hashing operation virtually eliminating all tree related
overhead.

With this final optimization finding a 4-way collision with a
moderate amount of memory requires ~2^24 hashing operations
instead of the >2^28 operations that would be require for
extra-nonce  grinding which would substantially erode the
benefit of the attack.

It is this final optimization which this proposal blocks.

==New consensus rule==

Beginning block X and until block Y the coinbase transaction of
each block MUST either contain a BIP-141 segwit commitment or a
correct WTXID commitment with ID 0xaa21a9ef.

(See BIP-141 "Commitment structure" for details)

Existing segwit using miners are automatically compatible with
this proposal. Non-segwit miners can become compatible by simply
including an additional output matching a default commitment
value returned as part of getblocktemplate.

Miners SHOULD NOT automatically discontinue the commitment
at the expiration height.

==Discussion==

The commitment in the left side of the tree to all transactions
in the right side completely prevents the final sqrt speedup.

A stronger inhibition of the covert attack in the form of
requiring the least significant bits of the block timestamp
to be equal to a hash of the first 64-bytes of the header. This
would increase the collision space from 32 to 40 or more bits.
The root value could be required to meet a specific hash prefix
requirement in order to increase the computational work required
to try candidate roots. These change would be more disruptive and
there is no reason to believe that it is currently necessary.

The proposed rule automatically sunsets. If it is no longer needed
due to the introduction of stronger rules or the acceptance of the
version-grinding form then there would be no reason to continue
with this requirement.  If it is still useful at the expiration
time the rule can simply be extended with a new softfork that
sets longer date ranges.

This sun-setting avoids the accumulation of technical debt due
to retaining enforcement of this rule when it is no longer needed
without requiring a hard fork to remove it.

== Overt attack ==

The non-covert form can be trivially blocked by requiring that
the header version match the coinbase transaction version.

This proposal does not include this block because this method
may become generally available without restriction in the future,
does not generally interfere with improvements in the protocol,
and because it is so easily detected that it could be blocked if
it becomes an issue in the future.

==Backward compatibility==


==Implementation==


==Acknowledgments==


==Copyright==

This document is placed in the public domain.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
Hi Jameson:


How does the users show their opinion? They can fork away and leave. But what remains will be united. Are you afraid of the united users or the fork?

I agree with you that the vote is not accurate. Could you kindly suggest an other word for that?

I think users should have choice to follow the miners or not. Do you agree with this or not?


Arrrgh. I think in the BIP, the miners just invalids tx version 1 temporarily. Thats a soft fork right? If they dislike the idea, they can leave as always.


Regards

LIN Zheming

-------------------------------------
On Sun, Apr 2, 2017 at 4:13 PM, Johnson Lau via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


 Someone told me a while back that it would be more natural if we move the
nHeight from the coinbase script to the coinbase locktime.  Have you
considered doing this?
-------------------------------------
Hi everyone,

As some of you know, I am working on a complete open source replacement of
Bitpay for allowing merchant to accept cryptocurrency payments while having
a way to sell automatically.

A crucial, missing part, is fiat conversion. And I figured out a simple
protocol that exchanges (or adapters) can implement to allow any merchant
to cash out BTC in fiat while giving them the freedom to choose their own
payment processor solution.

This also have positive impact on scalability: Before, a merchant would
receive the bitcoin from the customer then would send to the exchange,
resulting in two transactions.
With this specification, it would be one transaction.

Special thanks to anditto and kallewoof for reviewing. I am waiting for
your feedback:

Github link:
https://github.com/NicolasDorier/bips/blob/master/bip-xxx.mediawiki

<pre>
  BIP: XXX
  Layer: Applications
  Title: Crypto Open Exchange Protocol (COX)
  Author: Nicolas Dorier <nicolas.dorier@gmail.com>
  Comments-Summary: No comments yet.
  Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-XXX
  Status: Draft
  Type: Standards Track
  Created: 2017-12-20
  License: BSD-3-Clause
           CC0-1.0
</pre>

==Abstract==

A simple protocol for decoupling payment processor solutions from exchanges.

==Motivation==

Cryptocurrency merchant adoption is mainly driven by availability, ease of
use and means of acceptance.
We call such solutions `Payment Processors`.

Until now, payment processing solutions fall into one of the two following
categories:

# Self-hosted with the customer paying in cryptocurrency and the merchant
receiving it directly.
# Centralized, coupled with an exchange feature, with the customer paying
in cryptocurrency to the merchant, and receiving fiat or cryptocurrency on
his exchange account.

The self-hosted solution has two issues:

# The merchant becomes vulnerable to the wild volatility of
cryptocurrencies.
# It is wasteful of blockchain space, if the merchant does not pay
suppliers in crypto, as they need a second transaction to change to his
exchange,

The centralized solution has two issues:

# It locks-in the merchant to a particular payment processor whose
intentions might not be aligned (e.g. Bitpay who tried to redefine Bitcoin
as being a different chain, without merchant approval)
# It has to deal with local regulations (e.g. Bitpay does not provide fiat
CAD to canadian merchants)

The goal of this BIP is to specify a simple protocol which makes possible
decoupling of payment processors from exchanges.

We believe this BIP will gather a lot of interest among local exchanges
which do not have the resources to develop their own payment solutions.

Their customers can decide which payment processor solution they prefer,
while the exchanges give them a way to protect against cryptocurrency
volatility.

==Summary==

The merchant log in to its exchange website, go into "Address sources"
section of it, an click on "Create a new address source".

The address source creation wizard asks him questions about what to do when
crypto currency is sent to this the address source. (Cryptocurrency, Market
sell order, limit order of past day average etc...)

The merchant receives an "address source URI" which they can input inside
the payment processor.

An exchange compatible with the Crypto Open Exchange Protocol would reply
to any HTTP POST request to this  "address source URI" returning the
following information (more details in the Specification part)

# A deposit address for accepting a payment
# The current rate
# Optional: If the exchange is willing to take the risk of rate
fluctuation, until when this rate is guaranteed and under which conditions.

<img src="bip-xxx/overview.png"></img>

===Interaction===

* Manny (the "merchant") wants to accept Bitcoin payments on his e-commerce
website.
* Manny chooses the payment processor "PROCCO" which has a powerful plugin
for his e-commerce website.
* Manny is based in Canada and already has an account on the exchange
"MYCOIN" which supports the Crypto Open Exchange Protocol.
* Manny connects to the exchange website, and creates a new address source.
* In the configuration screen of the address source, for each payment sent
to this address source, Manny decides to keep 30% in Bitcoin and place a
market sell order for the remaining 70% of the amount.
* "MYCOIN" creates the address source, and gives the "address source URI"
to the merchant. (e.g. https://example.com/addresssources/abd29ddn92)
* Manny copies the address source URI and goes inside "PROCCO" settings,
and configures his store to use this address source URI.

Now a customer, Carol, wants to order a brand new phone for 0.01 BTC on
Manny's store and decides to pay in Bitcoin.

* The E-Commerce website plugin requests the creation of an invoice from
PROCCO.
* PROCCO queries the "address source URI" and retrieves the rate, the
expiration of this rate and conditions.
* PROCCO can now show the Bitcoin Payment Checkout page.
* Carla pays.
* PROCCO marks the payment as paid and redirects to the e-commerce website.
* MYCOIN, under its own policy (typically after 6 confirmations), credits
Manny's account of 0.01 BTC and simultaneously creates a market sell order
of 0.007 BTC on behalf of Manny.

==Specification==

The payment processor sends a POST request to the "address source URI", the
response from a Crypto Open Exchange Protocol exchange would be:

If the exchange does not guarantee the rate:

    {
        "depositAddress" : "13....abd",
        "currencyCode" : "CAD",
        "cryptoCurrencyCode" : "BTC",
        "rate" : "15600",
        # When the merchant account get credited on the exchange
        "requiredConfirmations" : blockcount
    }


If the exchange guarantee the rate:

    {
        "depositAddress" : "13....abd",
        "currencyCode" : "CAD",
        "cryptoCurrencyCode" : "BTC",
        "rate" : "15600",
        "requiredConfirmations" : blockcount
        "conditions" :
        {
            # When the transaction should be seen on the blockchain to
guarantee the rate
            "receivedBefore" : timestamp,
            # When the transaction should be confirmed on the blockchain to
guarantee the rate
            "confirmedBefore" : timestamp
        }
    }


The payment processor is responsible for giving feedback to the customer if
the fees of the received transaction are not enough to guarantee the rate.

==Note on adoption==

While local exchanges have incentives to implement this simple protocol, it
is not strictly needed.

An alternative is to develop an adapter server which expose Crypto Open
Exchange Protocol endpoint and connect to underlying exchange's API.

The only downside is that the rate can't be guaranteed.

==Copyright==

This document is dual licensed as BSD 3-clause, and Creative Commons CC0
1.0 Universal.


Nicolas,
-------------------------------------
You're asking for newly minted bitcoin to go to you but you burned the bitcoin used in the peg. You're effectively losing your money and then stealing from the miners to gain it back. The miners had to issue your amount of bitcoin 2 times (once for your original bitcoin, again to make you whole). Why would they agree to this?
--
hudon



-------------------------------------
On Wed, Feb 22, 2017 at 08:11:47PM -0500, Peter Todd via bitcoin-dev wrote:

Thinking about this a bit more, by not being forced to calculate a TXO
commitment for every block, we may be able to do significantly better than
delayed TXO commitments by lazily hashing.

Suppose we have the following perfect merkle tree, which we're using as a
key-value map. We'll represent inner nodes for which we've calculated digests
with "0"'s to represent what version of the tree they correspond too:

               0
              / \
             /   \
            /     \
           /       \
          /         \
         0           0
        / \         / \
       /   \       /   \
      0     0     0     0
     / \   / \   / \   / \
    a   b c   d e   f g   h

If a value is updated, digests above it become out of date and need to be
recalculated:


               1
              / \
             /   \
            /     \
           /       \
          /         \
         0           1
        / \         / \
       /   \       /   \
      0     0     0     1
     / \   / \   / \   / \
    a   b c   d e   f g   H

               2
              / \
             /   \
            /     \
           /       \
          /         \
         0           2
        / \         / \
       /   \       /   \
      0     0     2     1
     / \   / \   / \   / \
    A   b c   d e   F g   H

               3
              / \
             /   \
            /     \
           /       \
          /         \
         0           3
        / \         / \
       /   \       /   \
      0     0     2     3
     / \   / \   / \   / \
    a   b c   d e   F G   H

Suppose however that your implementation does lazy hashing; after the 3rd
update your state will be:

               .
              / \
             /   \
            /     \
           /       \
          /         \
         0           .
        / \         / \
       /   \       /   \
      0     0     .     .
     / \   / \   / \   / \
    a   b c   d e   F G   H

Basically all the digests on the right side is out of date and need to be
recalculated. Now, first of all it's obviously possible for your implementation
to keep updating values in the tree given their keys - you've essentially
regressed to a bog standard binary tree.

But what happens if you discard part of your dataset? Let's suppose you've
discarded the left half:

               .
              / \
             /   \
            /     \
           /       \
          /         \
         0           .
                    / \
                   /   \
                  .     .
                 / \   / \
                e   F G   H

Note how you still have sufficient information to calculate the current merkle
tip commitment: the left side hasn't changed yet. But what happens when someone
gives you an update proof? Specifically, suppose they want to change b -> B.
That requires them to provide you with the part of the merkle tree proving that
position #1 is b. Now you might think that's this data:

               3
              / \
             /   \
            /     \
           /       \
          /         \
         0           3
        / \
       /   \
      0     0
     / \
    a   b

But the inner node digests marked "3" are useless to you: you haven't
calculated those digests yet so you can't compare them to anything. What you
can compare is the following:

         0
        / \
       /   \
      0     0
     / \
    a   b

With that extra data your local knowledge is now:

               .
              / \
             /   \
            /     \
           /       \
          /         \
         0           .
        / \         / \
       /   \       /   \
      0     0     .     .
     / \         / \   / \
    a   b       e   F G   H

Allowing you to apply the update:

               .
              / \
             /   \
            /     \
           /       \
          /         \
         .           .
        / \         / \
       /   \       /   \
      .     0     .     .
     / \         / \   / \
    a   B       e   F G   H

If you want to again prune that data, simply recalculate the digests so you
can verify a copy given to you by a peer in the future:

               .
              / \
             /   \
            /     \
           /       \
          /         \
         4           .
        / \         / \
       /   \       /   \
      4     0     .     .
     / \         / \   / \
    a   B       e   F G   H

And prune, leaving you with:

               .
              / \
             /   \
            /     \
           /       \
          /         \
         4           .
                    / \
                   /   \
                  .     .
                 / \   / \
                e   F G   H


So tl;dr: the reason this works is that we can substitute commitments for
pointers: our merkle tree can also be viewed as a binary tree. So a reasonable
real-world implementation would be to delay computation of digests for anything
we have in RAM, and only compute digests as in-RAM data is flushed to disk.
Equally, on disk we can use standard time-space tradeoffs to only store a
subset of the digests, recalculating the rest on the fly. Given that'd we could
effectively combine both a cryptographic data structure and a standard
pointer-based data structure in one, I suspect we can get good performance out
of this.

The main subtlety of this approach will be how exactly to handle the proofs:
the level of verification possible depends on what digests a given node has
calculated, and we want to avoid making network splitting attacks possible by
attackers deliberately giving nodes proofs with upper digests that are
incorrect, something only some nodes can detect. Not sure yet exactly what's
the right approach there.

Finally, notice how this entire approach depends on schemes like MMR's where
the overall structure of the tree does not change as nodes are added and
updated; it would be much harder to implement this idea for something like a
merklized red-black tree where the structure changes as the tree is rebalanced.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
unless the mainchain reorganizes, since the consensus loop only cares about
matching the current block; it ignores splits and does not consider them
valid.

Maybe I am misunderstanding you, but isn't this a flaw not a feature? What
if a attacker pays a large fee to have his *invalid* block hash included in
the bitcoin mainchain? Would this block *have* to be included in the
sidechain's blockchain forever since *it was* included in bitcoin
blockchain?

the coinbase and once in the briber's separate transaction?

Yes, my BIP proposal does this.

hash is the indicator of which sidechain it is extending.  From your other
emails on this list, it seems the ratchet is for withdrawals from sidechain
to mainchain?  If so, should it not only appear in only some of the
sidechains (the ones which are currently doing some withdrawal?)?

Maybe I am missing something here, but why we do *explicitly* commit to the
previous block hash? Isn't it implicitly committed to via SHA256(SHA256())?
If a drivechain node tries to sync the drivechain from bitcoin's commitment
headers, it will invalidate that block since
the block hash does not correctly reference the previous block hash. AFAICT
there is no need to explicitly specify the previous block hash in the OP_BV
output. In general, I don't think we should assume these commitment headers
dictate the strict ordering of blocks on the sidechain -- only potential
blocks that
*might* be valid. To guarantee full validity drivechain nodes will have to
download the full block and figure out if they follow all of the consensus
rules.

This is sort of like headers first sync in bitcoin core:

https://bitcoin.org/en/developer-guide#headers-first

-Chris

On Thu, Jun 29, 2017 at 11:00 PM, ZmnSCPxj <ZmnSCPxj@protonmail.com> wrote:

-------------------------------------
On 07/09/17 05:52, Kabuto Samourai wrote:

Birthday is something SPV wallet developers have been wanting for years.
It helps them with the initial scan, so SPV wallet does not have to
download every block in the blockchain, but only the ones after birthday.


I will add some test vectors, when we agree this is the way to go.


+1

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
CTO, SatoshiLabs

-------------------------------------
Hi Russell,

the documentation about how drivechains are supposed to work scattered and
difficult to follow. So, without advocating for or against this proposal,
I'd also suggest that adding an opcode is not the best way to implement
this bribe.

Despite the flaws in this draft BIP, the goal is to start consolidating
this information into a more compact format. This BIP is *only*
meant to address the Blind Merging Mining Process of drivechains. It does
*not* address the withdrawal process from drivechain -> bitcoin.

transaction to a script that uses the OP_BRIBE code that fixes the critical
hash (and the sidechain id), and then a second transaction is needed to pay
the bribe to the miner.

That is intentional, this allows for a competitive process (like bitcoin
mining) for a block to be 'found' on the sidechain. The OP_BV output that
rewards
the bitcoin miner the most amount of money should be the one that is
included in the bitcoin blockchain. If I understand your scheme correctly,
you are
assuming the the bitcoin miner is *also* following the sidechain --
Sztorc's scheme does not make this assumption. The *number one goal* of BMM
is to *minimize* the resource burden on bitcoin miners for mining on a
drivechain.

To gmaxwell/luke-jr,

I agree my commitment scheme is flawed. Thanks for pointing it out. Is
there any way we could manipulate a coinbase transaction
into spending these OP_BV outputs? According to instagibbs, and AFIACT he
is right, we cannot have coinbase transactions
spend any outputs in previous blocks without a hard fork. This is
unfortunate because it might make more sense for the coinbase transaction
to spend these OP_BV outputs. We could design the coinbase transaction's
scriptSig to push the critical hash onto the stack and
place an OP_EQUAL on the OP_BV output to verify they were equal.
If I understand gmaxwell's concern about 'monotone' (or stateless) blocks
correctly, I *think*
this solution might fix that as well.

Another way we could fix this is by *fixing* the drivechain indices.
Therefore the mining rewards and witness commitments must
*not* occupy one of those indices -- but can occupy any other indice in the
coinbase output.
This would give us future flexibility for committing to new soft forks. For
instance, we would say
the mining reward must *not* be index 0 of the coinbase transaction, but
can occupy index 1 - 256. The same would apply for witness commitments.

-Chris

On Wed, Jun 28, 2017 at 5:49 PM, Russell O'Connor <roconnor@blockstream.io>
wrote:

-------------------------------------
+10k

Indeed, as any project Github issues should be enabled for BIPs,
wondering too since some time why this is not the case, and then if an
issue is worth discussing here it can be redirected to the list


Le 03/11/2017  10:50, Sjors Provoost via bitcoin-dev a crit:

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
Is it too late to propose to freeze Bitcoin protocol as it is today (no
SegWit, no bigger block)?

Hear me out: assume Bitcoin is frozen as it is now: no more forks, no more
change in consensus rules. Call it Bitcoin 1 (BC1). So, how to evolve and
scale Bitcoin?

Use a very conservative sidechain, call it Bitcoin 2 (BC2), tightly
coupled, block per block, with Bitcoin 1 chain. No fancy experimental
changes there: fix the bare minimum to support more general two-way
sidechains, linear scalability transaction verification time and
transaction malleability. Can be one way only if that makes it simple
enough: coins sent to BC2 can never come back to BC1.

Miners choose to mine a BC2 block alongside a BC1 block, and it can be done
for free once a BC1 block is found. There may be BC1 blocks in the chain
whose corresponding BC2 blocks are missing (and that is not a big deal),
but there will be an economic incentive to mine a BC2 block, since
transaction fees on that chain will be collected.

The economic adoption is completely voluntary and independent of anyone
else, doesn't require the strong consensus of SegWit. If it is feasible to
create a simple enough two way sidechain over BC1, good. If not, economic
value of BC1 will always be at least that of BC2, but never lower. BC2 may
have a lower price than BC1 at first, but since it will have every feature
of BC1, plus the possibility of massive scalability and instantaneous
transactions with a lightning network built over BC2 (a practical
economical advantage), it will most likely value over what BC1 could ever
be alone, raising BC1 price with it.

If these predictions are correct, fewer and fewer people will transact BC1,
eventually with blocks mined empty except for the coinbase, which the miner
may choose to send directly to BC2. If I turn out to be wrong, and people
prefer to stick to the old BC1 for some reason, BC2 becomes just another
speculative altcoin. Thus, everyone migrating should be fully conscious
about the risk of immediate devaluation.

The 2 major risks I see that could hamper the adoption are:
 - Just plain fear: I find the whole economical incentive scenario of BC2
sound and plausible, just as I found Bitcoin sound and plausible when I
heard about it for the first time. I also fear to put my money in BC2, just
as I did when I heard about Bitcoin. BC2 case is even worse, because I can
safely hold my coins in BC1 until I am sure BC2 is solid, so at first we
may experience only enthusiast adoption;
- Technological inertia: everyone wishing to accept BC2 will have to
implement this payment system as for any other altcoin. Every exchange
supporting both will have to create separated markets for BC1 and BC2.
Remember: both chains must be obviously distinguishable. It is even
desirable that the address format in BC2 to be incompatible with BC1 (maybe
staring with "2" ?).

Finally, due to the present state of affairs, with impending activation of
segwit with a promise to hardfork in 3 months, I believe this proposal is
too late, not to mention the stress, emotions and hard feelings of the
situation will make it go largely ill received or ignored. But since I
genuinely believe this to be the best and most conservative way to go with
Bitcoin, I wrote this anyway.

Please forgive me if something like this was discussed a thousand times
before and you wasted your time reading this.

-- 
Lucas Clemente Vella
lvella@gmail.com
-------------------------------------
Hi,
The only solution other than Dead Man's Switch to avoid gradual loss
Bitcoins in transaction is increasing the divisibiliy of Bitcoins. Then
Bitcoin values will need integer of more than 64 bits. Could that be done
with soft fork?

On Dec 11, 2017 9:42 PM, <bitcoin-dev-request@lists.linuxfoundation.org>
wrote:

-------------------------------------
I haven't investigated, but you may be seeing segwit-invalid blocks...0.13.0+ nodes will enforce segwit as it activated some time ago on testnet, 0.12.X nodes will not.

On March 23, 2017 3:37:34 PM PDT, Juan Garavaglia via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
-------------------------------------
Some related thoughts and suggestion for an extension that kanzure
suggested I post here:

Hardware Wallet attacks by input ownership omission and fix
----------------------------------------------------------------------------------
So a while back I realized that to have HW wallets do safe automated
coinjoins(without any user interaction be sure there are no fee dumps or
handing money to others) you have to protect yourself from the case of
signing one set of inputs while the other owned set is hidden from the
device, then repeating the same action with the two sets reversed.

Note that there is no support for such a mode in HW wallets today, but
could possibly greatly increase liquidity of JoinMarket like systems.

First signing pass:
1 BTC (yours, host tells ledger about it) --------
1 BTC (yours, host fails to tell ledger about it)-

Second signing pass:

1 BTC (yours, host fails to tell ledger) ---------
1 BTC (yours, host tells ledger about it)---------

In this scenario, you sign the first input, thinking "great I'm getting 0.5
BTC for running coinjoin" when in reality this will simply be re-played
again later with the inputs switched, *costing* you 0.5 BTC. (Ledger
doesn't support "negative fees", but imagine more more inputs are included
that aren't yours.)

More recently I noticed a more common issue along the same lines:

With Segwit inputs, the entire transaction referred to in the prevout is
generally no longer included for HW wallet signing API. This greatly speeds
up signing since potentially multiple MBs of transactions are no longer
passed into the device, but comes with a cost: An attacker can claim
certain inputs' value is much lower than it actually is. In the first pass,
the host reports the first input's value properly, and the second as lower.
The signature on the first input will go through fine(value included in the
sighash is only for that input), then attacker prompts a restart of
signing, reporting the 2nd value properly, and first value improperly low,
which allows the attacker to report the same fee twice on the device. Both
signatures over each input are correct, but the user was prompted with an
invalid fee amount(too low).

To fix this I consulted with andytoshi and got something we think works for
both cases:

1) When a signing device receives a partially signed transaction, all
inputs must come with a ownership proof:
- For the input at address A, a signature over H(A || x) using the key for
A. 'x' is some private fixed key that only the signing device knows(most
likely some privkey along some unique bip32 path).
- For each input ownership proof, the HW wallet validates each signature
over the hashed message, then attempts to "decode" the hash by applying its
own 'x'. If the hash doesn't match, it cannot be its own input.
- Sign for every input that is yours

This at a minimum makes sure that the wallet's total "balance" will not go
down more than the reported fee.

Benefits:
- Still small memory footprint compared to legacy signing
- Allows user-interactionless coinjoins without putting funds at risk
- These proofs can be created at any time, collected at the front of any
CoinJoin like protocol.
- These proofs can be passed around as additional fields for Partially
Signed Bitcoin Transactions:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-August/014838.html

On Sun, Aug 20, 2017 at 5:00 PM, Bryan Bishop via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
It kills Bitcoin as a store of value.  Disk space is not the problem; bandwidth is.  The blockchain won't go to infinity as you suggest, as it is bounded by certain constraints.  It's growth is a function of the transactions in a block, and the number of blocks is linear in growth.  

Sent from my iPhone

-------------------------------------
On Mon, May 22, 2017 at 06:32:38PM -0400, Russell O'Connor wrote:

Cartesian product can mean a lot of things.

What specifically do you mean by "cartesian product" here?

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On 08/30/2017 12:24 AM, shiva sitamraju via bitcoin-dev wrote:


small nit with 3.

It seems to me that the wallet would perform initial discovery on m/44
and m/49, and then would find transactions at one or the other, so it
can then record the type somewhere and from then on need only monitor
one branch.

Still, I agree it is ugly, makes initial discovery up to 2x slower, etc.


speaking as author of tools hd-wallet-addrs and hd-wallet-derive, I
agree this is problematic.

would be great if xpub/xprv could somehow encode their absolute path in
wallet for tools to read.  Users cannot be expected to know.



-------------------------------------
Hi James:

Thank you very much for detailed feedback. Sorry for my understanding of English being poor. Ill try to answer that.



ǵΪЩڵǳҪ˲Ը⿴ЩڵΪ޷ԤϿܷĸıʧЩڵȻӵѡȨͨ BIP148 ķ

I admitted that these nodes a very important. so we dont want these nodes suffer financial loss by undetectable network change. These nodes always have choice like BIP148.


޷Դ˽б֤ܹṩһЩڵ˽Ⲣ벿ıļ
We can not have any guarantee. but we can have incentives that they participate and be aware about the change happening.
ûǿԽѡ
Users always have choice.


ǿڵĽ׶ЩסǷɹǶҪΪ׼ǲܱƭΪʲôûз
They can be re-enable in the successful or unsuccessful activation of the fourth stage. Whether or not, what they need is to be prepared for the future coming. But they cant be left behind or be cheated like nothing happened.


źŵҿ޷ͬı䣬ΪʲôǻҪƭбϹʶǳʵŵģǾͲκ⡣ֳܷͨʤ
False signal cant be solved in my opinion. If the majority part just dont agree with the change, why they cheat? If the majority part is honest as described in nakamoto consensus, I think that wont be a problem. CPU power always decides.



ҲǴҲ϶ SPY ڿ
Maybe Im wrong. Please give some advice that how to make it compatible with SPY mining.


ⲢûǿǵĽڵκθı乲ʶıʾЩڵΪܵĸı׼
It would not force our nodes to do anything that changes the consensus. But they should be prepared for the **maybe** upcoming changes.
Эĸı佫ͨͶƱӦñнڵ֪ϡ
Protocol upgrades could be done using miners vote. but the progress of voting should be acknowledged by all nodes.



ǳʵģźŲ⡣
False signal wont be a problem if majority miners are honest.


ҪڿȷûӰ²ܹͶƱ
Miners should vote unbiasedly under the condition that most users are not affected by protocol upgrading.


-------------------------------------
One way to reduce fees is to encourage usage of Replace-By-Fee, BIP 125 [0]. It allows wallets to recommend lower fees, because if a transaction gets stuck due to underestimation, the fee can easily be bumped.

Bitcoin Core has had support for RBF for a while, and as of v0.15.0 recommends lower fees [1] when the user chooses to use RBF.

I recently submitted a pull request that would turn on RBF by default, which triggered some discussion [2]. To ease the transition for merchants who are reluctant to see their customers use RBF, Matt Corallo suggested that wallets honor a no125=1 flag.

So a BIP-21 URI would look like this: bitcoin:175t...45W?amount=20.3&no125=1

When this flag is set, wallets should not use RBF, regardless of their default, unless the user explicitly overrides the merchant's preference.

Afaik adding this flag won't break existing BIP-21 support. It doesn't use the req- prefix, because it's optional. I'm also not aware of any ad hoc standards that use no125 in BIP-21-ish URIs.

- Sjors

P.S. I'd similarly suggest adding a bech32 param, but that's for another discussion

[0] https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
[1] https://bitcoincore.org/en/2017/09/01/release-0.15.0/#better-fee-estimates
[2] https://github.com/bitcoin/bitcoin/pull/11605
[3] https://github.com/bitcoin/bitcoin/issues/11828
-------------------------------------
I've been puzzling over your email since receiving it. I'm not sure it
is possible to perform the attack you describe with the tree structure
specified in the BIP. If I may rephrase your attack, I believe you are
seeking a solution to the following:

Want: An innocuous script and a malign script for which

   double-SHA256(innocuous)

is equal to either

   fast-SHA256(double-SHA256(malign) || r) or
   fast-SHA256(r || double-SHA256(malign))

where r is a freely chosen 32-byte nonce. This would allow the
attacker to reveal the innocuous script before funds are sent to the
MAST, then use the malign script to spend.

Because of the double-SHA256 construction I do not see how this can be
accomplished without a full break of SHA256. The trick of setting r
equal to the padding only works when a single SHA256 is used for leaf
values. This is why double-SHA256 is specified in the BIP, and I will
edit the text to make that more clear.

Which brings us to the point that I think your original request of
separating the hash function of leaves from internal nodes is already
in the specification. I misunderstood your request at first to be that
MERKLEBRANCHVERIFY should itself perform this hash, which I objected
to as it closes of certain use cases such as chained verification of
proofs. But it is explicitly the case that leaf values and internal
updates are calculated with different hash functions.

I'm not intrinsicly opposed to using a different IV for fast-SHA256 so
as to remove the incompatability with single-SHA256 as the leaf hash
function, if that is the consensus of the community. It just adds
complication to implementations and so I want to make sure that
complication is well justified.

Sincerely,
Mark Friedenbach


-------------------------------------
Jimmy Song,

Why would the actual end users of Bitcoin (the long term and short term owners of bitcoins) who run fully verifying nodes want to change Bitcoin policy in order to make their money more vulnerable to 51% attack?

If anything, we would be making policy changes to prevent the use of patented PoW algorithms instead of making changes to enable them.

Thanks,
Praxeology Guy
-------------------------------------
As some of you may know, the MAST proposal I sent to the mailing list
on September 6th was discussed that the in-person CoreDev meetup in
San Francisco. In this email I hope to summarize the outcome of that
discussion. As chatham house rules were in effect, I will refrain from
attributing names to this summary..

* An introductory overview of the BIPs was presented, for the purpose
  of familiarizing the audience with what they are attempting to
  accomplish and how they do so.

* There was a discussion of a single vs multi-element MBV opcode. It
  was put forward that there should perhaps be different opcodes for
  the sake of script analysis, since a multi-element MBV will
  necessarily consume a variable number of inputs. However it was
  countered that if the script encodes the number of elements as an
  integer push to the top of the stack immediately before the opcode,
  then static analyzability is maintained in such instances. I took
  the action item to investigate what an ideal serialization format
  would be for a multi-element proof, which is the only thing holding
  back a multi-element MBV proposal.

* It was pointed out that the non-clean-stack tail-call semantics is
  not compatible with segwit's consensus-enforcement of the clean
  stack rule. Some alternatives were suggested, such as changing
  deployment mechanisms. After the main discussion session it was
  observed that tail-call semantics could still be maintained if the
  alt stack is used for transferring arguments to the policy script. I
  will be updating the BIP and example implementation accordingly.

* The observation was made that single-layer tail-call semantics can
  be thought of as really being P2SH with user-specified hashing. If
  the P2SH script template had been constructed slightly differently
  such as to not consume the script, it would even have been fully
  compatible with tail-call semantics.

* It was mentioned that using script versioning to deploy a MAST
  template allows for saving 32 bytes of witness per input, as the
  root hash is contained directly in the output being spent. The
  downside however is losing the permissionless innovation that comes
  with a programmable hashing mechanism.

* The discussion generally drifted into a wider discussion about
  script version upgrades and related issues, such as whether script
  versions should exist in the witness as well, and the difference in
  meaning between the two. This is an important subject, but only of
  relevance in far as using a script version upgrade to deploy MAST
  would add significant delay from having to sort through these issues
  first.

This feedback led to some minor tweaks to the proposal, which I will
be making, as well as the major feature request of a multi-element
MERKLE-BLOCK-VERIFY opcode which requires a little bit more effort to
accomplish. I will report back to this list again when that work is
done.

Sincerely,
Mark Friedenbach

-------------------------------------
On Fri, Mar 31, 2017 at 1:29 PM, praxeology_guy via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


That would have the unfortunate effect of incentivizing miners to not
completely fill blocks, because low fee marginal transactions could cost
them money.

An alternate approach would be to incentivize miners to follow transaction
fees more by reducing mining rewards, which could be done by soft forking
in a requirement that a chunk of all mining rewards be sent to an
unspendable address.
-------------------------------------
On Fri, Jul 7, 2017 at 10:44 PM, Matt Corallo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Indeed, their code previously did not increase the blocksize but it
was adjusted at the last minute to do so-- so it may actually do that
now. Because they don't appear to have implemented any tests for it, I
wouldn't be too surprised if it still didn't work at all but also
wouldn't be surprised if it did.

You are correct that the specification text appears to refer to the
prior change that did not. (In my response I just assumed that it
meant what they actually did-- good catch).

-------------------------------------
On Tuesday, 4 April 2017 20:01:51 CEST Luke Dashjr via bitcoin-dev wrote:

Can you explain how miners are irrelevant if the upgrade is not a soft fork?

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
<html><head></head><body><div style="font-family: Verdana;font-size: 12.0px;"><div style="font-family: Verdana;font-size: 12.0px;">
<div style="font-family: Verdana;font-size: 12.0px;">
<div>Hi Guys,</div>

<div>&nbsp;</div>

<div>I have a question about the use of txmempool. find attached the code in txmempool.h</div>

<div>&nbsp;</div>

<div>&nbsp;</div>

<div>======================================================</div>

<div>
<div>/* Adding transactions from a disconnected block can be very time consuming,<br/>
&nbsp;* because we don&#39;t have a way to limit the number of in-mempool descendants.<br/>
&nbsp;* To bound CPU processing, we limit the amount of work we&#39;re willing to do<br/>
&nbsp;* to properly update the descendant information for a tx being added from<br/>
&nbsp;* a disconnected block. &nbsp;If we would exceed the limit, then we instead mark<br/>
&nbsp;* the entry as &quot;dirty&quot;, and set the feerate for sorting purposes to be equal<br/>
&nbsp;* the feerate of the transaction without any descendants. */</div>

<div><br/>
class CTxMemPoolEntry</div>

<div>{</div>

<div>&nbsp; &nbsp;private:</div>

<div>&nbsp; &nbsp;// ...&nbsp;&nbsp; &nbsp;</div>

<div>&nbsp; &nbsp;// Information about descendants of this transaction that are in the<br/>
&nbsp; &nbsp;// mempool; if we remove this transaction we must remove all of these</div>

<div>&nbsp; &nbsp;// descendants as well. if nCountWithDescendants is 0, treat this entry as<br/>
&nbsp; &nbsp;// dirty, and nSizeWithDescendants and nModFeesWithDescendants will not be</div>

<div>&nbsp; &nbsp;// correct.<br/>
&nbsp; &nbsp;</div>

<div>&nbsp; &nbsp;int64_t nCountWithDescendants; //!&lt; number of descendant transactions<br/>
&nbsp; &nbsp;// ...</div>

<div>
<div>======================================================</div>

<div>&nbsp;</div>

<div>&nbsp;</div>

<div>Now, the only place where&nbsp;nCountWithDescendants is modified is the following (txmempool.cpp):</div>

<div>&nbsp;</div>

<div>
<div>&nbsp;</div>

<div>======================================================</div>
void CTxMemPoolEntry::UpdateDescendantState(int64_t modifySize, CAmount modifyFee, int64_t modifyCount)<br/>
{<br/>
&nbsp; &nbsp; nSizeWithDescendants += modifySize;<br/>
&nbsp; &nbsp; assert(int64_t(nSizeWithDescendants) &gt; 0);<br/>
&nbsp; &nbsp; nModFeesWithDescendants += modifyFee;<br/>
&nbsp; &nbsp; nCountWithDescendants += modifyCount;<br/>
&nbsp; &nbsp; assert(int64_t(nCountWithDescendants) &gt; 0);<br/>
}</div>

<div>
<div>======================================================</div>

<div>&nbsp;</div>

<div>&nbsp;</div>

<div>Therefore,&nbsp;nCountWithDescendants is never zero.</div>

<div>Am i missing something? Where is this concept of &quot;dirty&quot; defined?</div>

<div>&nbsp;</div>

<div>Thanks a lot,</div>

<div>DJ</div>
</div>
</div>

<div>&nbsp;</div>
</div>

<div>&nbsp;</div>

<div>&nbsp;</div>

<div>&nbsp;</div>

<div>&nbsp;</div>
</div>
</div></div></body></html>

-------------------------------------
As good of an idea as it may or may not be to remove this feature from the code base, actually doing so would be crossing a boundary that we have not previously been willing to do except under extraordinary duress. The nature of bitcoin is such that we do not know and cannot know what transactions exist out there pre-signed and making use of these features.

It may be a good idea to make these features non standard to further discourage their use, but I object to doing so with the justification of eventually disabling them for all transactions. Taking that step has the potential of destroying value and is something that we have only done in the past either because we didn’t understand forks and best practices very well, or because the features (now disabled) were fundamentally insecure and resulted in other people’s coins being vulnerable. This latter concern does not apply here as far as I’m aware.

-------------------------------------
I would like to propose an efficient UTXO commitment scheme.

A UTXO commitment can be useful for:

1. Fast syncing a full node, by downloading the UTXO-set
2. Proofing (non) existence of a UTXO..

Various schemes have been proposed:

* Merkle/radix trees and variants; all of which have the problem that
they significantly increase the burden of maintaining the UTXO set.
Furthermore, such schemes tend to practically prescribe the UTXO storage
format severely limiting continuous per-implementation optimizations.
* A "flat" rolling hash, eg the ECMH proposed by Pieter Wiulle which is
cheap to calculate but only solves (1) and not (2).

I propose a hybrid approach, with very limited extra burden to maintain
and reasonably small proofs: 

We divide the UTXO set in buckets by prefix of their TXID, then maintain
a rolling hash for each bucket. The commitment is then the root of the
tree constructed from the resulting bucket hashes. To construct the
tree: For each depth, we group the hashes of the next depth per 64
hashes and calculate the rolling hash of each. (Effectively, this is a
prefix tree with a fixed branch-size of 64).

Bucketcount
-------------------
txcount = number of TXIDs in the UTXO set
bucketcount = (smallest power of 2 larger than sqrt(txcount))  << 6

Rationale for bucketcount:

* This currently gives a bucketcount of 2^19, which is very cheap to
maintain with a 16mb array of rolling hashes.
* This currently gives an average bucket size of 4kb. With a rolling
hash, full nodes don't need to maintain the buckets themselves, but they
are used for proofs.
* The burden of future UTXO growth is divided among maintaining the
rolling hashes and size of the proof: 10,000x as large UTXO set (20TB),
gives ~400kb buckets and ~1.6gb in maintaining rolling hashes.
* This gives a tree depth of 5, which means the cost of every UTXO
update is increased  by ~3 rolling hashes (and a double SHA), as the
lowest depths don't benefit from caching.
* A proof for (non) existence of a UTXO is ~ 4*64*32 =8kb (branch-nodes)
+ 4kb (bucket) = ~12kb

Specification [WIP]
---------------------------
We define the "UTXO commitment" as the serialized byte array: "U" "T"
"X" "O" VARINT(version) VARINT(txcount) UINT256(UTXO-root)    [todo
clarify]

A block that contains an output in the coinbase whose scriptPubKey
consists solely of OP_RETURN [UTXO commitment] must be rejected if in
the UTXO commitment the version equals 1 and either 
* After updating the UTXO state, the number of distinct TXIDs in the
UTXO set is not equal to the txcount value of the UTXO commitment
* After updating the UTXO state, the UTXO-root in the UTXO commitment is
not equal to the UTXO-root defined below.

The UTXO-root can be calculated as follows:

* Define _bucketcount_ as (smallest power of 2 larger than
sqrt(txcount))  << 6
* Given a TXID in the UTXO set, define UTXO(TXID) as the double SHA256
of (TXID + coins). (coins is the serialization of unspent outputs to be
spec'ed). 
* Let bucket N be the set of values UTXO(TXID) for each TXID in the
UTXO-set where (TXID mod _bucketcount_) equals N.
* Let rhash N be the rolling hash (TBD) of all values in bucket N
* Let the hash sequence be the ordered sequence  rhash
[0,_bucketcount_).

1. If the hash sequence contains at most 64 entries, then the UTXO-root
is the rolling hash of all entries in the hash sequence, otherwise:
2. Group the hash sequence in ordered subsequences of 64 entries each.
3. Find the rolling hash of each subsequence
4. Continue with 1., with the hash sequence being the ordered sequence
of these rolling hashes.

Note: an implementation may want to maintain and update the set of
rolling hashes at higher depths on each UTXO set operation.

Note: the secure ECMH is a good candidate for the bucket hash. This
could also be used for the branch rolling hashes, but it might be worth
considering XOR for those as there seem to be simply not enough
candidates to find a colliding set?

Note: two magic numbers are used: "<< 6" for the bucket count, and "64"
for the branch size. They work nicely but are pulled out of a dark place
and merit some experimentation.

Use cases for light clients
-------------------------------------
These UTXO proofs could be used as compact fraud proofs, although the
benefit of this is not generally agreed upon.

They can also be used to increase low-conf security to light clients, by
validating the signatures and order-validity of incoming transactions
against the right bucket of the current UTXO set.

An interesting use case may be another type of light client. It could be
interesting for a light client to abandon the bloom filters, and instead
use the UTXO proofs to verify whether an incoming or outgoing
transaction is confirmed. This could be beneficial for "rarely active"
light clients such as smartphone apps, as it prevents the need to
synchronize previous blocks with bloom filters, and allows syncing to
the latest block with 12kb/output.

Summary 
--------------
* Allows fast full node syncing.
* Costs full nodes ~20mb extra in RAM
* Costs full nodes ~3 rolling hash operations per UTXO operation.
* Allows UTXO (non) existence proofs for currently avg ~12kb.
* Size of proof grows O(sqrt(N)) with UTXO set
* Size of extra full node memory grows O(sqrt(N)) with UTXO set


Tomas van der Wansem
bitcrust

-------------------------------------
(Subject was: [bitcoin-dev] Version 1 witness programs (first draft)), but
I'm moving part of that conversation to this thread.

On Sun, Oct 1, 2017 at 5:32 PM, Johnson Lau <jl2012@xbt.hk> wrote:



I would very much like to retain the ability to do static analysis.  More
generally, the idea of interpreting arbitrary data as code, as done in
OP_EVAL and in TAILCALL, makes me quite anxious.  This at the root of many
security problems throughout the software industry, and I don't relish
giving more fuel to the underhanded Bitcoin Script contestants.

On Sun, Oct 1, 2017 at 8:45 PM, Luke Dashjr <luke@dashjr.org> wrote:


Actually, I have a half-baked idea I've been thinking about along these
lines.

The idea is to add a flag to each stack item in the Script interpreter to
mark whether the item in the stack is "executable" or "non-executable", not
so different from how computers mark pages to implement executable space
protection.  By default, all stack items are marked "non-executable".  We
then redefine OP_PUSHDATA4 as OP_PUSHCODE within ScriptSigs.  The
operational semantics of OP_PUSHCODE would remain the same as OP_PUSHDATA4
except it would set the pushed item's associated flag to "executable".  All
data pushed by OP_PUSHCODE would be subject to the sigops limits and any
other similar static analysis limits.

Segwit v0 doesn't use OP_PUSHDATA codes to create the input stack, so we
would have to mark executable input stack items using a new witness v1
format. But, IIUC, TAILCALL isn't going to be compatible with Segwit v0
anyway.

During a TAILCALL, it is required that the top item on the stack have the
"executable" flag, otherwise TAILCALL is not used (and the script succeeds
or fails based on the top item's data value as usual).

All other operations can treat "executable" items as data, including the
merkle branch verification.  None of the Script operations can create
"executable" items; in particular, OP_PUSHDATA4 within the ScriptPubKey
also would not create "executable" items.  (We can talk about the behaviour
of OP_CAT when that time comes).

One last trick is that when "executable" values are duplicated, by OP_DUP,
OP_IFDUP, OP_PICK. then the newly created copy of the value on top of the
stack is marked "non-executable".

Because we make the "executable" flag non-copyable, we are now free to
allow unbounded uses of TAILCALL (i.e. TAILCALL can be used multiplie times
in a single input).  Why is this safe?  Because the number of "executable"
items decreases by at least one every time TAILCALL is invoked. the number
of OP_PUSHCODE occurrences in the witness puts an upper bound on the number
of invocations of TAILCALL allowed.  Using static analysis of the script
pubkey and the data within the OP_PUSHCODE data, we compute an upper bound
on the number of operations (of any type) that can occur during execution.

Unbounded TAILCALL should let us (in the presence of OP_CHECKSIGFROMSTACK)
have unbounded delegation.

Overall, I believe that OP_PUSHCODE

1. is fully backwards compatible.
2. maintains our ability to perform static analysis with TAILCALL.
3. never lets us interpret computed values as executable code.
4. extends TAILCALL to safely allow multiple TAILCALLs per script.
-------------------------------------
Chris/Greg,

For pending withdrawals (side-to-main transfers), all of the data is
stored in a teeny tiny extension block which contains all the drivechain
data (which we called "MinerDB"). And miners were supposed to commit to
this and put it in the coinbase in some locate-able place (for example,
index 1).

I had assumed that this would go the same for BMM, since it is all
drivechain-themed. Thus, all drivechains, and all drivechain-stuff (BMM
included), would claim one output index.

Moreover, while DC claims an output "slot", the claim doesn't need to be
permanent...if the BTC-value of the relevant output is not equal to
zero, the BMM code could just ignore it. The sidechains would each
assume that no sidechain block was merged-mined in this period. And DC
would assume that no forward progress was made on any side-to-main
transfers (ie, that everyone "abstained").

Perhaps that addresses Greg's third, first, and second concerns.

I am having some trouble understanding concern #4. I think you mean to
say that the output coins of a transaction which is encumbered by OP
BribeVerify are different from other coins. Indeed they are, and coins
locked by OP BribeVerify cannot be moved until their associated
sidechain header ("h*") is ~100 blocks deep in the sidechain (hence the
earlier conversation about the "ratchet", which attempts to measure this).

The timeline that CryptAxe and I discussed, as I last remember it, is that:
0. (setup) The sidechain node is run by a briber, and this briber
constructs a sidechain block paying himself all the fees. These fees
total q=4 BTC.
1. Negotiations happen out of bound, between Briber and all miners.
(still setting up) Each new transaction the Briber makes, he chooses a
completely new h* (which is trivial for him to do by incrementing a
nonce/anything), and he may as well also fund each of these txns with
the same unspent output (owned by him). This prevents a miner from
annoying the Briber by including many ultimately-invalid transactions.
2. Miner1 includes h* in the coinbase of today's block thus BMMing a
sidechain block today, he also includes the transaction he just
negotiated with the Briber (call it, "tx1"). This tx1 is one where
Briber pays z=(q-0.001) to an op_bribe script that will eventually pay z
BTC to Bitcoin address M owned by "miner1".
3. After ~123 blocks, the ratchet (on mainchain) indicates that the
sidechain headers have advanced sequentially by x=100 places. This
allows miner1 to spend from tx1 to address M.
3. OR, after ~250 blocks, the second timing threshold is reached*, and
the Briber can spend from his script back to an address he controls
(also predefined in steps 1 and 2).


*This is the dual-threshold time-locking technique that the LN uses to
prove a negative.

The second timelock setup is required because it is possible that miner
will earnestly BMM a sidechain block, but then reorg such that this
block is orphaned out of the longest (side)chain. In this case, the
Briber doesn't get paid his tx-fees, so he is entitled to a refund.


So, maybe this BIP will need to be edited a little. : ) Nonetheless, I'm
glad Chris is taking the initiative and doing this work. And I'm sorry
if the documentation has shifted too much. At the bottom of the spec
blog post there are some notes, but they probably aren't very helpful.



On 6/28/2017 12:07 AM, Gregory Maxwell via bitcoin-dev wrote:

-------------------------------------


I think the current idea of sigagg is something like this: the new OP_CHECKSIG still has 2 arguments: top stack must be a 33-byte public key, and the 2nd top stack item is signature. Depends on the sig size, it returns different value:

If sig size is 0, it returns a 0 to the top stack
If sig size is 1, it is treated as a SIGHASH flag, and the SignatureHash() “message” is calculated. It sends the (pubkey, message) pair to the aggregator, and always returns a 1 to the top stack
If sig size is >1, it is treated as the aggregated signature. The last byte is SIGHASH flag. It sends the (pubkey, message) pair and the aggregated signature to the aggregator, and always returns a 1 to the top stack.

If all scripts pass, the aggregator will combine all pairs to obtain the aggkey and aggmsg, and verify against aggsig. A tx may have at most 1 aggsig.

(The version I presented above is somewhat simplified but should be enough to illustrate my point)

So if we have this script:

OP_1 OP_RETURNTRUE <pubkey> OP_CHECKSIG

Old clients would stop at the OP_RETURNTRUE, and will not send the pubkey to the aggregator

If we softfork OP_RETURNTRUE to something else, even as OP_NOP11, new clients will send the (key, msg) pair to the aggregator. Therefore, the aggregator of old and new clients will see different data, leading to a hardfork.

OTOH, OP_NOP based softfork would not have this problem because it won’t terminate script and return true.



I don’t think it’s worth the code complexity, just to save a few bytes of data sent over wire; and to be a soft fork, it still takes the block space.

Maybe we could create many OP_DROPs and OP_2DROPs, so new VERIFY operations could pop the stack. This saves 1 byte and also looks cleaner.

Another approach is to use a new script version for every new non-verify type operation. Problem is we will end up with many versions. Also, signatures from different versions can’t be aggregated. (We may have multiple aggregators in a transaction)




-------------------------------------
ASICBOOST causes Bitcoin's PoW to become more memory/latency throttled instead of raw computation throttled.

There is the equation:
Power Cost + Captial Rent + Labor ~= block reward + fees

Capital Rent is a barrier to entry, and hence in desiring a more distributed system, we would like to minimize the Capital Rent portion of the equation.

Resolving memory/latency throttle requires a greater Captial Rent than raw computation throttle.

Hence (agreeing with Luke), ASICBOOST is not desirable, even if it wasn't a government enforced monopoly on mining.

Please let me know if I made a mistake.

Thanks,
Praxeology Guy
-------------------------------------
Bitcoin nodes could also keep a spentness status list, where each bit in the spentness status list corresponds to whether a txo in the MMR is spent. This could make it so that disconnected wallets didn't have to guess the pruned relative spentness status when it reconnects to the network... and help prevent DoS attacks.

Keeping such a bit list would consume considerably less space if stxos were never added to the MMR. Putting portions of such a list in the node at height DLH_REQUIRED would made R/W operations on the bit list more local to other data that is going to be R/W.

Cheers,
Praxeology Guy
-------------------------------------
Regarding storage space, have you heard about pruning? Probably you should.

On 27 Aug 2017 12:27 am, "Adam Tamir Shem-Tov via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
It is relevant to note that BIP 117 makes an insecure form of CODESEPARATOR delegation possible, which could be made secure if some sort of CHECKSIGFROMSTACK opcode is added at a later point in time. It is not IMHO a very elegant way to achieve delegation, however, so I hope that one way or another this could be resolved quickly so it doesn’t hold up either one of those valuable additions.

I have no objections to making them nonstandard, or even to make them invalid if someone with a better grasp of history can attest that CODESEPARATOR was known to be entirely useless before the introduction of P2SH—not the same as saying it was useless, but that it was widely known to not accomplish what a early-days script author might think it was doing—and the UTXO set contains no scriptPubKeys making use of the opcode, even from the early days. Although a small handful could be special cased, if they exist.

-------------------------------------
Very true, if Moore's law is still functional in 200 years, computers will
be 2^100 times faster (possibly more if quantum computing becomes
commonplace), and so old wallets may be easily cracked.

We will need a way to force people to use newer, higher security wallets,
and turning coins to mining rewards is better solution than them just being
hacked.

On Tue, 22 Aug 2017, 7:24 pm Thomas Guyot-Sionnest <dermoth@aei.ca> wrote:

-------------------------------------
On 7/12/2017 4:50 AM, ZmnSCPxj wrote:

I do agree with this description. And I am not exactly comfortable with
it, but theoretically the random chase would serve no direct benefit to
any attacker (and thus be a ~purposeless use of attacker's money), and
empirically I do not recall anyone complaining about this happening in
Namecoin. And I think it is generally agreed that low-conf transactions
are of categorically lower reliability -- and therefore that we are
relatively less interested in taking care of users who want the
blockchain to provide them with...immediate gratification (for lack of a
better term).



Yes, it is a valid concern. I'm glad you brought this up. My view is
that there will be no undercutting attempts in the future, if Bitcoin is
popular enough and transactions are constantly arriving.

In short, the reason I feel that way is because miners will be both [1]
willing and [2] able, to maximize their fee income by imposing a
blocksize limit on themselves. They would do this by orphaning
non-compliants -- this would be something softfork-esque, but not
necessarily enforced by non-mining nodes as it is limited to miner
tx-acceptance policy.

Here is a link to a presentation of my thoughts on the issue:
https://www.youtube.com/watch?v=YErLEuOi3xU&list=PLw8-6ARlyVciNjgS_NFhAu-qt7HPf_dtg&index=4

As I say there, I believe that miners currently have no significant
reason to fee-maximize today, which is why we haven't seen this
behavior. (Also, someone would need to write the fee-maximization code
for this, and that not only would take time, but it would require a
person with a very complex intersection of skills.)

The paper you mention was written 1.5 months after my presentation was
recorded. My conclusion contradicts the first sentence of the last
paragraph of "3.1 Model of the system" which reads: "We also assume that
miners always have space to include all available transactions." In my
model miners do NOT always (or, really, ever) have space to include all
available transactions. And miners are happy that they do not, because
all of them make more total money as a result (both per block and overall).

I think the arguments of the presentation were original, so I would be
grateful to you if you offered me your thoughts on it.



I don't understand the word "anyway" in the second sentence. Is that a
summary of my proposal, or an assertion of yours. Because as it stands,
the bribe part is quite optional -- miners could just mine both chains
themselves, and then they would know which h* to include, paying
themselves the sidechain's tx fees. (Of course, under what you propose
here, miners could also mine it themselves, by placing it in an
OP_BRIBEVERIFY). However, if it is limited to a coinbase, then there is
at most only 1 hash to process every 10 minutes, which I think is desirable.

I like your idea as it is simpler. But a second concern I have is that
if a sidechain user wants to use SPV mode, the software will want to
know exactly where to find the sidechain headers. If they are always in
a known part of the coinbase, then the spv sidechain wallet knows where
to look.

Re: impose ordering on coinbase outputs, what do you think of a scheme
which searches index 1 for an OP RETURN, and if it finds something it
interprets that as the root hash of merkle tree of merged mined
sidechain h*'s ? If it doesn't find a hash commitment in index 1 it just
assumes that no sidechains were mined in this ~10 minute period.


I am actually not in favor of permiessionless creation of sidechains,
because the sidechains can interfere with each other to a degree that
impacts their usefulness. We do not allow "permissionless creation of
transactions", because we do not allow invalid or double-spent
transactions. I feel the same logic should apply to the chains themselves.

Another youtube presenation about this:
https://www.youtube.com/watch?v=xGu0o8HH10U&list=PLw8-6ARlyVciMH79ZyLOpImsMug3LgNc4&index=1
(Much shorter) written post: http://www.truthcoin.info/blog/wise-contracts/

That said, I am fully in favor of forcing the sidechain's permanent
deposit address to be equal to some deterministic function of the sha256
hash of its version 0.1 release.

Paul


-------------------------------------
In any case when Hal Finney do not wake up from his 200years
cryo-preservation (because unfortunately for him 200 years earlier they
did not know how to preserve a body well enough to resurrect it) he
would find that advance in computer technology made it trivial for
anyone to steal his coins using the long-obsolete secp256k1 ec curve
(which was done long before, as soon as it became profitable to crack
down the huge stash of coins stale in the early blocks)

I just don't get that argument that you can't be "your own bank". The
only requirement coming from this would be to move your coins about once
every 10 years or so, which you should be able to do if you have your
private keys (you should!). You say it may be something to consider when
computer breakthroughs makes old outputs vulnerable, but I say it's not
"if" but "when" it happens, and by telling firsthand people that their
coins requires moving every once in a long while you ensure they won't
do stupid things or come back 50 years from now and complain their
addresses have been scavenged.

--
Thomas

On 22/08/17 10:29 AM, Erik Aronesty via bitcoin-dev wrote:

-------------------------------------
On Tuesday 06 June 2017 10:39:28 PM Tao Effect via bitcoin-dev wrote:

Replay is a solved problem. It can be improved on and made simpler, but at 
this point, replay only occurs when the sender is either negligent or 
intending it.


This is nothing but unfounded FUD. It is very simple to implement and 
guaranteed to work eventually. It may be time consuming, but that is the only 
truth here. The only risk is that of a long reorg, the same as double spend 
attacks.


What kind of "fungibility" does this FUD claim it destroys? Destroying cross-
chain fungibility is the very *intent* of replay protection. And it does not 
destroy same-chain fungibility any more than any other miner spending.


Lack of replay protection does not mean there is no coin. Replay protection is 
equally a concern for the main (BIP148) chain and any legacy chains malicious 
miners might choose to split off. And none of this changes the fact that such 
miners will be unable to sell their legacycoins at Bitcoin market prices, 
because whether other transactions are replayed or not, *their* coins won't be 
valid on the main chain.

Luke

-------------------------------------
Dear list,

In previous arguments over Drivechain (and Drivechain-like proposals) I promised that better scaling proposals — that do not sacrifice Bitcoin's security — would come along.

I planned to do a detailed writeup, but have decided to just send off this email with what I have, because I'm unlikely to have time to write up a detailed proposal.

The idea is very simple (and by no means novel*), and I'm sure others have mentioned either exactly it, or similar ideas (e.g. burning coins) before.

This is a generic sharding protocol for all blockchains, including Bitcoin.

Users simply say: "My coins on Chain A are going to be sent to Chain B".

Then they burn the coins on Chain A, and create a minting transaction on Chain B. The details of how to ensure that coins do not get lost needs to be worked out, but I'm fairly certain the folks on this list can figure out those details.

- Thin clients, nodes, and miners, can all very easily verify that said action took place, and therefore accept the "newly minted" coins on B as valid.
- Users client software now also knows where to look for the other coins (if for some reason it needs to).

This doesn't even need much modification to the Bitcoin protocol as most of the verification is done client-side.

It is fully decentralized, and there's no need to give our ownership of our coins to miners to get scale.

My sincere apologies if this has been brought up before (in which case, I would be very grateful for a link to the proposal).

Cheers,
Greg Slepak

* This idea is similar in spirit to Interledger.

--
Please do not email me anything that you are not comfortable also sharing with the NSA.

-------------------------------------
On Thu, Sep 28, 2017 at 03:43:05PM +0300, Sjors Provoost via bitcoin-dev wrote:

The BIP-70 payment protocol used via BIP-72 URI's is insecure, as payment qr
codes don't cryptographically commit to the identity of the merchant, which
means a MITM attacker can redirect the payment if they can obtain a SSL cert
that the wallet accepts.

For example, if I have a wallet on my phone and go to pay a
merchant, a BIP-72 URI will look like the following(1):

    bitcoin:mq7se9wy2egettFxPbmn99cK8v5AFq55Lx?amount=0.11&r=https://merchant.com/pay.php?h%3D2a8628fc2fbe

A wallet following the BIP-72 standard will "ignore the bitcoin
address/amount/label/message in the URI and instead fetch a PaymentRequest
message and then follow the payment protocol, as described in BIP 70."

So my phone will make a second connection - likely on a second network with a
totally different set of MITM attackers - to https://merchant.com

In short, while my browser may have gotten the correct URL with the correct
Bitcoin address, by using the payment protocol my wallet is discarding that
information and giving MITM attackers a second chance at redirecting my payment
to them. That wallet is also likely using an off-the-shelf SSL library, with
nothing other than an infrequently updated set of root certificates to use to
verify the certificate; your browser has access to a whole host of better
technologies, such as HSTS pinning, certificate transparency, and frequently
updated root certificate lists with proper revocation (see Symantec).

As an ad-hoc, unstandardized, extension Android Wallet for Bitcoin at least
supports a h= parameter with a hash commitment to what the payment request
should be, and will reject the MITM attacker if that hash doesn't match. But
that's not actually in the standard itself, and as far as I can tell has never
been made into a BIP.

As-is BIP-72 is very dangerous and should be depreciated, with a new BIP made
to replace it.

1) As an aside, it's absolutely hilarious that this URL taken straight from
   BIP-72 has the merchant using PHP, given its truly terrible track record for
   security.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
I prefer not to do anything that requires pools software upgrade or wallet upgrade. So I prefer to keep the dummy marker, and not change the commitment structure as suggested by another post.

For your second suggestion, I think we should keep scriptSig empty as that should be obsoleted. If you want to put something in scriptSig, you should put it in witness instead.

Maybe we could restrict witness to IsPushOnly() scriptPubKey, so miners can’t put garbage to legacy txs. But I think relaxing the witness program size to 73 bytes is enough for any purpose.




-------------------------------------
Maybe there's some hole in Jorge's logic and scrapping blockmaxsize has
quadratic hashing risks, and maybe James' 10KB is too ambitious; but even
if so, a simple 1MB tx size limit would clearly do the trick.  The broader
point is that quadratic hashing is not a compelling reason to keep
blockmaxsize post-HF: does someone have a better one?


On May 30, 2017 9:46 PM, "Jean-Paul Kogelman via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Pruning is already implemented in the nodes... Once enabled only unspent
inputs and most recent blocks are kept. IIRC there was also a proposal
to include UTXO in some blocks for SPV clients to use, but that would be
additional to the blockchain data.

Implementing your solution is impossible because there is no way to
determine authenticity of the blockchain mid way. The proof that a block
hash leads to the genesis block is also a proof of all the work that's
been spent on it (the years of hashing). At the very least we'd have to
keep all blocks until a hard-coded checkpoint in the code, which also
means that as nodes upgrades and prune more blocks older nodes will have
difficulty syncing the blockchain.

Finally it's not just the addresses and balance you need to save, but
also each unspent output block number, tx position and script that are
required for validation on input. That's a lot of data that you're
suggesting to save every 1000 blocks (and why 1000?), and as said
earlier it doesn't even guarantee you can drop older blocks. I'm not
even going into the details of making it work (hard fork, large block
sync/verification issues, possible attack vectors opened by this...)

What is wrong with the current implementation of node pruning that you
are trying to solve?

--
Thomas

On 26/08/17 03:21 PM, Adam Tamir Shem-Tov via bitcoin-dev wrote:

-------------------------------------
On Tue, Jun 13, 2017 at 3:13 AM, Zheming Lin <heater@gmail.com> wrote:

They generally don't have to switch to an altcoin when they get to
choose which blocks they accept ultimately. This is a key component of
Bitcoin's incentive model since it makes it so miners are unlikely to
do a hard fork if their blocks would not be accepted by nodes/users.
For example if 55% of miners wanted to hard fork and change the block
reward back to 50 BTC the minority side would not need to switch to an
altcoin, they would just need to ignore those 50 BTC block reward
blocks.


By backwards compatible I mean being able to continue functioning
without updating the node, soft forks are generally backwards
compatible because you can still transact even if you don't upgrade
your node to support the soft fork, you just won't be able to use the
new soft fork features.

-------------------------------------
On Tue, Jun 20, 2017 at 02:01:45AM +0800, Wang Chun via bitcoin-dev wrote:


You have to specify what you mean by "PoS" - there's dozens of variations.
Equally, existing pure PoS schemes probably don't make sense as a "bolt-on"
add-on, as once you introduce PoW to it you should design something that uses
the capabilities of both systems.

FWIW, I've heard that the Ethereum guys are leaning towards abandoning pure PoS
and are now trying to design a PoW + staking system instead.


To be clear, you mean such a scheme would protect the multi-billion dollar
investments non-malicious miners have made in SHA256^2 hardware by ensuring it
remains useful, right?


Note that if those PoS blocks are *pure* PoS, you'll create a significant risk
of double-spend attacks, as there's zero inherent cost to creating a pure-PoS
block. Such blocks can't be relied on for confirmations; even "slasher" schemes
have significant problems with sybil attacks.


The scaling problem is one of scalability; PoS does nothing to improve
scalability (though many in the ETH community have been making dishonest
statements to the contrary).


As a sidechain yes, but in what you propose above the extra blocks wouldn't
contain transactions that non-PoS-aware nodes could understand in a
backwards-compatible way.


All the above aside, I don't think it's inherently wrong to look at adding PoS
block *approval* mechanisms, where a block isn't considered valid without some
kind of coin owner approval. While pure-PoS is fundamentally broken in a
decentralized setting, it may be possible to mitigate the reasons it's broken
with PoW and get a system that has a stronger security model than PoW alone.

FWIW there's some early discussions by myself and others about this type of
approach on the #bitcoin-wizards IRC channels, IIRC from around 2014 or so.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
Hi all,

        This is an alternative to jl2012's BIPZZZ (OP_PUSHTXDATA[1]).
It riffs on the (ab)use of OP_CHECKSIGFROMSTACK that Russell[2]
used to implement covenants[3].  I've been talking about it to random
people for a while, but haven't taken time to write it up.

The idea is to provide a OP_TXMERKLEVERIFY that compares the top stack
element against a merkle tree of the current tx constructed like so[4]:

        TXMERKLE = merkle(nVersion | nLockTime | fee, inputs & outputs)
        inputs & outputs = merkle(inputmerkle, outputmerkle)
        input = merkle(prevoutpoint, nSequence | inputAmount)
        output = merkle(nValue, scriptPubkey)

Many variants are possible, but if we have OP_CAT, this makes it fairly
easy to test a particular tx property.  A dedicated OP_MERKLE would make
it even easier, of course.

If we one day HF and add merklized TXIDs[5], we could also use this method
to inspect the tx *being spent* (which was what I was originally trying to
do).

Thanks for reading!
Rusty.

[1] https://github.com/jl2012/bips/blob/vault/bip-0ZZZ.mediawiki
[2] Aka Dr. "Not Rusty" O'Connor.  Of course both of us in the same thread will
    probably break the internet.
[3] https://blockstream.com/2016/11/02/covenants-in-elements-alpha.html
[4] You could put every element in a leaf, but that's less compact to
    use: cheaper to supply the missing parts with OP_CAT than add another level.
[5] Eg. use the high nVersion bit to say "make my txid a merkle".

-------------------------------------
On Tue, Jun 13, 2017 at 2:35 PM, Jared Lee Richardson
<jaredr26@gmail.com> wrote:

BU by default uses an "Accept Depth" parameter which effectively lets
miners decide block size rules and allows for resource requirements
that are too high for many users to validate. The block size settings
there are effectively placebo controls.


That's largely true that they typically don't decide the victor in
soft forks unless they are the ones to activate the rules
changes(satoshi did this a few times in the early days), however they
make it very difficult for a hard fork to be activated without
consent. Yes, I'm not advocating for having runtime consensus settings
for nodes either, I'm advocating that resource requirements be low
enough that full validation remains possible for a large percentage of
the economy.


More likely the false signal would be used during the orphaning period
to prevent blocks from being orphaned for miners that don't want to
follow the fork.


Yes, there is a good amount of risk with validationless mining right
now here since it's well known that over half of mining pools use
validationless mining to some degree. This may not be too bad though
due to fallbacks but the risk is probably fairly implementation
specific.

-------------------------------------
Coincidentally, the kind of Merkle tree that Mark describes in his
proposal is exactly the one that we use at Stampery.

The Stampery BTA whitepaper[1] includes pseudocode for many of the
algorithms outlined by this proposal, including fast-SHA256, the tree
building process and the inclusion proving routine.

The wording is slightly different but the logic is just the same, so I
hope it helps future implementations in case of eventual adoption.


[1]
https://s3.amazonaws.com/stampery-cdn/docs/Stampery-BTA-v6-whitepaper.pdf


Best,
-- 
Adán Sánchez de Pedro Crespo
CTO, Stampery Inc.
San Francisco - Madrid
T: +34 663 163 375

-------------------------------------
On Thu, Sep 28, 2017 at 06:06:29PM -0700, Mark Friedenbach via bitcoin-dev wrote:

This proposed fix is itself broken, because the miner can easily include *only*
transactions paying out-of-band, at which point the fee can be anything.
Equally, miners can provide fee *rebates*, forcing up prices for everyone else
while still allowing them to make deals.

Also, remember that you can pay fees via anyone-can-spend outputs, as miners
have full ability to control what transactions end up spending those outputs.

The fact these countermeasures are all likely to be implemented - all of which
harm the overall ecosystem by reducing visibility of fees and making it harder
to compete with centralized miners - makes me very dubious about that proposal.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------


The fact that this is possible should be enough for us to implement 
meassures against it.

On Fri, 7 Apr 2017, Daniele Pinna via bitcoin-dev wrote:

-------------------------------------
- It would be cool if any rate-limiting POW was specified as bytecode ...
so nodes can plug in as many "machine-captcha" things as they please, and
solvers can choose to solve... or just say "nope too hard".

- Alternately, it would be a lot nicer if you just required people to pay a
nanobit .... that could prevent DDOS even better, and generate a revenue
stream for nodes.


On Sun, May 7, 2017 at 10:48 PM, Karl Johan Alm via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
It seems to me that the same statement can be made for *any* key storage
mechanism depending on one's security/threat model, including
bitcoin-core's internal wallet storage.  There certainly are cases where
a paper (or metal) offline wallet makes a lot of sense, particularly for
long-term offline storage... something that electronic media pretty much
sucks at.

Though if you care to elaborate I'd be interested to learn of your
specific critiques, if you have any beyond the generic statements here:
https://en.bitcoin.it/wiki/Paper_wallet

Regardless, the APIs I've proposed have uses beyond paper wallets.  It
can also be used by third party wallets, or any number of reasons that
individuals or devs might have to generate keys.



On 09/29/2017 02:03 PM, Luke Dashjr wrote:


-- 
Dan Libby

Open Source Consulting S.A.
Santa Ana, Costa Rica
http://osc.co.cr
phone: 011 506 2204 7018
Fax: 011 506 2223 7359

-------------------------------------
Please read my email more carefully; the replay threat would be moot because there would be no alternative chain to replay the TX on, as the non-148 chain would have been reorganized into oblivion.

Sent with [ProtonMail](https://protonmail.com) Secure Email.

-------- Original Message --------
Subject: Re: [bitcoin-dev] Replay attacks make BIP148 and BIP149 untennable
Local Time: June 7, 2017 3:26 AM
UTC Time: June 7, 2017 12:26 AM
From: contact@taoeffect.com
To: Kekcoin <kekcoin@protonmail.com>
Anthony Towns <aj@erisian.com.au>, bitcoin-dev@lists.linuxfoundation.org <bitcoin-dev@lists.linuxfoundation.org>

I don't know what you mean by "render the replay threat moot."

If you don't have replay protection, replay is always a threat. A very serious one.

--
Please do not email me anything that you are not comfortable also sharing with the NSA.

On Jun 6, 2017, at 5:19 PM, Kekcoin <kekcoin@protonmail.com> wrote:

Hmm, that's not the difference I was talking about. I was referring to the fact that using "post-chainsplit coinbases from the non-148 chain" to unilaterally (ie. can be done without action on the 148-chain) taint coins is more secure in extreme-adverserial cases such as secret-mining reorg attacks (as unfeasibly expensive they may be); the only large-scale (>100 block) reorganization the non-148 chain faces should be a resolution of the chainsplit and therefore render the replay threat moot.
-------------------------------------
I would like to propose two new script features to be added to the
bitcoin protocol by means of soft-fork activation. These features are
a new opcode, MERKLE-BRANCH-VERIFY (MBV) and tail-call execution
semantics.

In brief summary, MERKLE-BRANCH-VERIFY allows script authors to force
redemption to use values selected from a pre-determined set committed
to in the scriptPubKey, but without requiring revelation of unused
elements in the set for both enhanced privacy and smaller script
sizes. Tail-call execution semantics allows a single level of
recursion into a subscript, providing properties similar to P2SH while
at the same time more flexible.

These two features together are enough to enable a range of
applications such as tree signatures (minus Schnorr aggregation) as
described by Pieter Wuille [1], and a generalized MAST useful for
constructing private smart contracts. It also brings privacy and
fungibility improvements to users of counter-signing wallet/vault
services as unique redemption policies need only be revealed if/when
exceptional circumstances demand it, leaving most transactions looking
the same as any other MAST-enabled multi-sig script.

I believe that the implementation of these features is simple enough,
and the use cases compelling enough that we could BIP 8/9 rollout of
these features in relatively short order, perhaps before the end of
the year.

I have written three BIPs to describe these features, and their
associated implementation, for which I now invite public review and
discussion:

Fast Merkle Trees
BIP: https://gist.github.com/maaku/41b0054de0731321d23e9da90ba4ee0a
Code: https://github.com/maaku/bitcoin/tree/fast-merkle-tree

MERKLEBRANCHVERIFY
BIP: https://gist.github.com/maaku/bcf63a208880bbf8135e453994c0e431
Code: https://github.com/maaku/bitcoin/tree/merkle-branch-verify

Tail-call execution semantics
BIP: https://gist.github.com/maaku/f7b2e710c53f601279549aa74eeb5368
Code: https://github.com/maaku/bitcoin/tree/tail-call-semantics

Note: I have circulated this idea privately among a few people, and I
will note that there is one piece of feedback which I agree with but
is not incorporated yet: there should be a multi-element MBV opcode
that allows verifying multiple items are extracted from a single
tree. It is not obvious how MBV could be modified to support this
without sacrificing important properties, or whether should be a
separate multi-MBV opcode instead.

Kind regards,
Mark Friedenbach

-------------------------------------
Two concerns:

1) This makes block validity depend on things that aren't in the
blockchain. If you and I have different minrelaytxfee's, we will have
different mempool sizes. Your node will discard a block, but my node will
think it is valid, and our nodes will not come to consensus.

2) This is trivially bypassed. A miner can take some coins they own
already, and create a zero-value transaction that has a scriptPubKey of
OP_1. (i.e. anyone-can-spend.) Then, they create another transaction
spending the first transaction, with an empty scriptSig, and the same
scriptPubKey. They do this over and over until they fill up the block.

The last OP_1 output can be left for the next miner. Since the above
algorithm is deterministic, a merkle tree containing every transaction
except the coinbase can be precomputed. The 'malicious' miners do not need
to store this fake block.

On Fri, Mar 24, 2017 at 10:03 AM, CANNON via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Wed, Apr 26, 2017 at 4:01 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I'm not sure what you are referring to here.  The data in the scriptSigs
are never signed.  The scriptSigs are always stripped from the transaction
before the sigHash is made.
-------------------------------------
On 09/29/2017 11:07 AM, Andrew Johnson wrote:

true that.  Though there's nothing stopping a diligent person from
installing bitcoin-core on a dedicated offline machine.  The blockchain
wouldn't need to be synced at all for key generation purposes.


yeah, so I noticed this issue about Paper Wallet generation not being
possible with bitcoin-core exactly because I was recommending to a
non-technical user to use paper wallets, but then I also had to point
out that really bitaddress code should be downloaded, audited, etc,
before use.  Things that are actually impossible for a non-technical user.

So I figured that instead I would make a simple script for them that
would use bitcoin-core to generate the addresses... and that's when it
dawned on me that it won't actually work with present day RPCs that are
all tied to internal wallet.

hence, this proposal.


yes, agreed.

-------------------------------------
On 13 July 2017 at 03:04, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I believe that a good reason not to wish your node to be segwit compliant
is to avoid having to deal with the extra bandwidth that segwit could
require.   Running a 0.14.2 node means being ok with >1MB blocks, in case
segwit is activated and widely used. Users not interested in segwit
transactions may prefer to keep the cost of their node lower.
-------------------------------------
Replies inline.

On 01/28/17 07:28, Johnson Lau wrote:

Agreed.


Agreed. Would like 1, dont care about 2, not a fan of 3. 2 could even be
implemented easily as a softfork if we allow the
spend-other-coinbase-outputs from 1.


Hmm? I'm saying that "the header" should be viewed as both the
"top-level" PoW-proving header, and the sub-header. There is no need to
have nBits in both?


We can safely disable SPV clients post-fork by just keeping the header
format sufficiently compatible with PR#9443 without caring about the
coinbase transaction, which I think should be the goal.

Regarding firmware upgrade, you make a valid point. I suppose we need
something that looks sufficiently like a coinbase transaction that
miners can do nonce-rolling using existing algorithms. Personally, I'd
kinda prefer something like a two-leaf merkle tree root as the merkle
root in the "primary 80-byte header" (can we agree on terminology for
this before we go any further?) - the left one is a
coinbase-transaction-looking thing, the right one the header of the new
block header.


Yes, I got the max() being at the transaction level (at the block level
would just be stupid) :).

This does not, however, make UTXO selection trivial, indeed, the second
you start having not-completely-homogeneous UTXOs in your wallet you
have to consider "what if the selection of this UTXO would switch my
criteria from one to another", which I believe makes this nonlinear.


So lets apply this only to non-Segwit-hashed transactions (incl
transactions which opted into the new sighash rules using the
anti-replay stuff)?


Yes, I tend to agree that there isnt much way around a sigops limit (as
we have now).


Totally agree there, but we can easily discount inputs more than outputs
to accomplish this for most potential outputs, I believe.

Did I miss a justification for there being a separate b (witness
serialized size) and c (txweight-with-discounts?).


Or by allowing coinbase txn to spend previous coinbase outputs. This
seems to not be an issue at present, though is something to consider in
future soft forks, so, agreed, lets table this and make sure we're set
up to do it in a soft fork if we need to.


Hmm, cant we accomplish this with a future sighash mode in which you
simply include the block's hash in the sighash and then spend to an
anyone-can-spend?

-snip-


Hmm, I think you missed my point - if we're soft-forking in a new limit,
we can trivially add a new merkle tree over only that limit, which is
sufficient to make fraud proofs for the new limit.


Sure, but now miners can disable fraud proofs using a simple majority,
which really, super sucks.


Yes, lets skip it for now, I dont see much value in debating it for a HF
now.

-------------------------------------
You could technically call myself and Chris 'core developers'. You don't
get to have a fixed rate of Bitcoin and a second way to mint coins at the
same time.

On Oct 10, 2017 1:46 PM, "Tao Effect via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
AFAICT, re-enabling these old OP-codes would require a hardfork.

If we had SegWit enabled, we could via a soft fork allocate new OP-codes
for the same functionality (by introducing a new version of Script).
I believe the Elements alpha project has been experimenting with
re-enabling old OP-codes: https://elementsproject.org/elements/opcodes/

2017-05-19 8:07 GMT+02:00 Mark Boldyrev via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
On Wed, Jun 7, 2017 at 5:53 PM, Jared Lee Richardson <jaredr26@gmail.com> wrote:
Higher transaction fees on a minority chain can compensate miners for
a lower price which would likely be enough to get the BIP148 chain to
a difficulty reduction. BIP148 however is a consensus change that can
be rectified if it gets more work, this would act as an additional
incentive for mine the BIP148 side since there would be no wipeout
risk there. ETC replay protection was done after the fork on an as
needed basis(there are multiple reliable techniques that can be used
to split UTXO's), the same can happen with BIP148 and it is easier to
do with Bitcoin than with the ETH/ETC split IMO.
A big reason BIP148 still has support is because up until SegWit
actually activates there's no guarantee segwit2mb will actually have
the necessary support to activate SegWit.
This is largely an issue due to segwit2x's bundling, if the SW and HF
part of segwit2x were unbundled then there would be no reason to delay
BIP91 activation, this is especially a problem since it takes a good
deal of time to properly code and test a HF. Unfortunately segwit2x
has been quite inflexible in regards to the bundling aspect even
though there are clearly no technical reasons for it to be there.

-------------------------------------
On Wed, Jun 7, 2017 at 12:02 AM Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



Please don't spread misinformation. Whatever you think of the DAO hard
fork, it's a simple fact that the Ethereum ledger was not edited.

-Nick Johnson
-------------------------------------
On Friday, January 27, 2017 11:53:02 PM Andrew Johnson via bitcoin-dev wrote:

Assume as a premise (despite your apparent disagreement below) that for 
Bitcoin to function, a supermajority of economic activity needs to be verified 
using full nodes operated by the recipient. Evidence suggests that at this 
current time, at best 10% of economic activity is in fact using a full node to 
verify the transaction. On this basis, it seems pretty clear that serious 
action must be taken to change the status quo, and so for efforts to do so 
without dropping the block size have proven ineffective.


Satoshi envisioned a system where full nodes could publish proofs of invalid 
blocks that would be automatically verified by SPV nodes and used to ensure 
even they maintained the equivalent of full node security so long as they were 
not isolated. But as a matter of fact, this vision has proven impossible, and 
there is to date no viable theory on how it might be fixed. As a result, the 
only way for nodes to have full-node-security is to actually be a true full 
node, and therefore the plan of only having full nodes in datacenters is 
simply not realistic without transforming Bitcoin into a centralised system.


I think it's likely safe to say that if this were a possibility, everyone 
would want to continue to move in that direction. But as the facts stand, it 
simply isn't possible.

Luke

-------------------------------------
On Monday, 10 July 2017 20:38:08 CEST Jorge Timn via bitcoin-dev wrote:

Good news!

Code to support 2x (the hard fork part of the proposal) has been out and 
tested for much longer than that.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------

Yes, absolutely. It would still be helpful if it is somewhat
standardized on the Client level.


I agree and I really appreciate the explanation! Transaction Processor
is not optimal, I brought up BSA: Block Space Authority before to
slice the pie in terms of it legitimate power structure instead of its
functionality. I maintain that this is better for higher level
discourse. Maybe something like "Consensus Space Sovereign" would be
more suitable.


Functionally, a node can moderate the network in the following way:
1. Relay transactions (mempool) and the global memory (blockchain)
2. Construct block headers (PoW: deterministic diffusion puzzle
solutions) and write to the global memory (create "blocks", which are
"ledger change candidates")
3. Read, verify and accept changes to the global memory


"Block Generator" (ala Satoshi) may be better but let's look at the
power structure analog to the functionality:
1. Authority to propose a change
2. Authority to approve a change
3. Authority to reject a change

This can easily be understood in current political terms. In Spain a:
1. Citizen can propose to engage in a business (voice)
2. (Special) Citizen (the King) can disapprove of the business (sovereign)
3. Citizen can leave Spain (exit)

Likewise, citizens can engage with the sovereign in order to change
some regulation, say average transaction tax. The question is simply
what legitimate authority a node has. We can map that quite neatly
onto the terminology of 'Politics'.

That does also away with naive 51% attack scenarios and the like. They
are akin to aliens invading earth. Surely it is not impossible that
aliens invade and in Bitcoin land, it is currently conceivable, but
most conflicts are not brought about by a single devilish aggressor
but a special interest group that wants to concentrate benefits and
diffuse costs.
- Transaction Tiering could be a great basis for decentralized negotiations.

Regards
Martin


On Sat, Mar 25, 2017 at 10:30 PM, praxeology_guy
<praxeology_guy@protonmail.com> wrote:

-------------------------------------
Just going to throw in my support for a POW change, not any particular implementation, but the idea.

Bitcoin is technically owned by China now. That's not acceptable.

- Greg

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
On Sat, Jun 10, 2017 at 8:04 PM, Jacob Eliosoff via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This still doesn't prevent the split if 45% or more of the hashrate
keeps blocking segwit, so I don't see how this help.


Miners could start signaling bit 1 today, before they use bip91 too
and signal bit 4 in addition.
But they aren't doing it, it seems they prefer to block segwit. I
don't see why changing using bit 4 or reducing the threshold would
change their mind.


Or you can replace this whole plan with the step 3, convincing miners
to stop blocking segwit, upgrade to segwit capable code if they
haven't already and signal bit 1 to activate it.
If you don't get that, there's going to be a split. Unless bip148 is
aborted in favor of bip149, which seems unlikely.

If we had 51%+ of the hashrate currently signaling segwit, I believe
there would be no problem convincing people to move from bip148 to
bip91, but we don't have that.

To me the lesson is not rushed deployments but bip8 and never commit
the mistake of giving miners the ability to block changes again, like
we did with csv and segwit, but using bip8 instead of bip9 from now
on.


-------------------------------------
How do you trust your <1000 block blockchain if you don't
download/validate the whole thing? (I know it should be easy to spot
that by looking at the blocks/tx or comparing to other nodes, but from a
programmatic point of view this is much harder). You can of course
include a checkpoint in the code to tell which recent block is valid
(which is already done afaik), but you still need all blocks from that
checkpoint to validate the chain (not 10!). If you rely on such
checkpoint, why not just include the UTXO's as well so you can start
mid-way based on code trust?

Indeed pruning doesn't allow you to start mid-way yet but there are much
easier solutions to that than what you propose.

--
Thomas

On 26/08/17 06:32 PM, Adam Tamir Shem-Tov wrote:



-------------------------------------
There was an error on page 5 of the paper, which made the block-chaining odds formula confusing. The error was in the text, not in the formula, and consisted of assuming the affected route as always being the rewarded one, which is false. The corrected version is already available at the same URL ([https://proof-of-loss.money](https://proof-of-loss.money/)). The new file's date is 02/07/2017, and downloading it will probably require clearing the browser's cache.

I would greatly appreciate any feedback, preferably directly to my email, to avoid overloading this list with something not directly related to Bitcoin.

Mirelo
-------------------------------------
As i understand it, the transactions to be included in a block are 
entirely up to the miner of that block.


What prevents a miner from implementing the proposal on their own?


If this is adopted as some kind of "policy", what forces a miner to 
follow it?

Jim Renkel

On 12/2/2017 10:07 PM, Damian Williamson via bitcoin-dev wrote:

-------------------------------------
<div>Hello Kalle,</div><div> </div><div>If want to talk theoretically:</div><div> </div><div>"a hacker has have to hash power than hash value of the entire bitcoin network, it could mislead all operations and invalidate the node."</div><div> </div><div>This bitcoin.pdf is clearly written.</div><div><div> </div><div>Ask your questions, Yes will now hacked the blockchain that has more of total power and No the stub record is up-to-date and secure.</div><div>So, the bans node solution is not good, if good bitcoin couldn't be a reliable proof-of-work currency.</div><div> </div><div>Read: <a href="http://satoshi.nakamotoinstitute.org/posts/">http://satoshi.nakamotoinstitute.org/posts/</a></div><div> </div><div>Regards</div><div> </div><div>Ozgur</div></div><div> </div><div>18.12.2017, 14:55, "Kalle Rosenbaum via bitcoin-dev" &lt;bitcoin-dev@lists.linuxfoundation.org&gt;:</div><blockquote type="cite"><div><div>Dear list,</div><div> </div><div>I find it hard to understand why a full node that does initial block</div><div>download also must download witnesses if they are going to skip</div><div>verification anyway. If my full node skips signature verification for</div><div>blocks earlier than X, it seems the reasons for downloading the</div><div>witnesses for those blocks are:</div><div> </div><div>* to be able to send witnesses to other nodes.</div><div> </div><div>* to verify the witness root hash of the blocks</div><div> </div><div>I suppose that it's important to verify the witness root hash because</div><div>a bad peer may send me invalid witnesses during initial block</div><div>download, and if I don't verify that the witness root hash actually</div><div>commits to them, I will get banned by peers requesting the blocks from</div><div>me because I send them garbage.</div><div> </div><div>So both the reasons above (there may be more that I don't know about)</div><div>are actually the same reason: To be able to send witnesses to others</div><div>without getting banned.</div><div> </div><div>What if a node could chose not to download witnesses and thus chose to</div><div>send only witnessless blocks to peers. Let's call these nodes</div><div>witnessless nodes. Note that witnessless nodes are only witnessless</div><div>for blocks up to X. Everything after X is fully verified.</div><div> </div><div>Witnessless nodes would be able to sync faster because it needs to</div><div>download less data to calculate their UTXO set. They would therefore</div><div>more quickly be able to provide full service to SPV wallets and its</div><div>local wallets as well as serving blocks to other witnessless nodes</div><div>with same or higher assumevalid block. For witnessless nodes with</div><div>lower assumevalid they can serve at least some blocks. It could also</div><div>serve blocks to non-segwit nodes.</div><div> </div><div>Do witnessless nodes risk dividing the network in two parts, one</div><div>witnessless and one with full nodes, with few connections between the</div><div>parts?</div><div> </div><div>So basically, what are the reasons not to implement witnessless</div><div>nodes?</div><div> </div><div>Thank you,</div><div>/Kall</div></div></blockquote>
-------------------------------------
On Tue, Jul 11, 2017 at 8:18 PM, Paul Sztorc via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

A fine intention, but I've checked with many of the top contributors
and it sounds like the only regular developer you spoke with was
Luke-Jr.  Next time you seek to represent someone you might want to
try talking to them!


I think the project is not philosophically against hardforks, at least
not in an absolute sense.

Part of the reason they were discussed in the capacity document was to
make it clear that I wasn't, and to invite other project members to
expose disagreement (though I'd mostly checked in advance...)

But these recently proposed ultra-hasty highly contentious changes,
that seem to be being suggested on shorter and shorter timeframes; I
do think the project is actually opposed to those in a very strong
sense.

But if you were instead to talk about things like fixing timewarp,
recovering header bits, etc. It would clearly be the other way.

At least at the moment computers and bandwidth are improving; I don't
think any regular developers are opposed to hardforks that change
capacity well tech improvements and protocol improvements have made it
obviously not much of a trade-off.

Personally, I wish the project had previously adopted a license that
requires derived works to not accept any block the derived-from work
wouldn't accept for at least two years, or otherwise the derivative
has to be clearly labeled not-bitcoin. :P

In any case, I think it's safe to say that people's opinions on
hardforks are complicated. And all the smoke right now with unusually
poorly executed proposals probably clouds clear thinking.

-------------------------------------
sufficient hashrate can leverage large blocks to exacerbate selfish mining.

Can you give me a link to this?  Having done a lot of mining, I really
really doubt this.  I'm assuming the theory relies upon propagation times
and focuses on small miners versus large ones, but that's wrong.
Propagation times don't affect small miners disproportionately, though they
might affect small POOLS disproportionately, that isn't the same thing at
all.  No miner since at least 2014 has operated a full node directly with
each miner - it is incredibly impractical to do so.  They retrieve only the
merkle root hash and other parameters from the stratum server, which is a
very small packet and does not increase with the size of the blocks.  If
they really want to select which transactions to include, some pools offer
options of that sort(or can, I believe) but almost no one does.  If they
don't like how their pool picks transactions, they'll use a different pool,
that simple.

If there's some other theory about a miner exploiting higher blocksizes
selfishly then I'd love to read up on it to understand it.  If what
you/others actually meant by that was smaller "pools," that's a much much
smaller problem.  Pools don't earn major profits and generally are at the
mercy of their miners if they make bad choices or can't fix low
performance.  For pools, block propagation time was a major major issue
even before blocks were full, and latency + packet loss between mining
units and the pool is also a big concern.  I was seeing occasional block
propagation delays(over a minute) on a fiber connection in 2013/4 due to
minute differences in peering.  If a pool can't afford enough bandwidth to
keep propagation times down, they can't be a pool.  Bigger blocksizes will
make it so they even more totally-can't-be-a-pool, but they already can't
be a pool, so who cares.  Plus, compact blocks should have already solve
nearly all of this problem as I understand it.

So definitely want to know more if I'm misunderstanding the attack vector.

transactions) can be leveraged in ways that both damages the network and
increases miner profits.

Maybe you're meaning an attack where other pools get stuck on validation
due to processing issues?  This is also a nonissue.  The smallest viable
pool has enough difficulties with other, non-hardware related issues that
buying the largest, beefiest standard processor available with ample RAM
won't even come up on the radar.  No one cares about $600 in hardware
versus $1000 in hardware when it takes you 6 weeks to get your peering and
block propagation configuration just right and another 6 months to convince
miners to substantially use your pool.

If you meant miners and not pools, that's also wrong.  Mining hardware
doesn't validate blocks anymore, it hasn't been practical for years.  They
only get the merkle root hash of the valid transaction set.  The pool
handles the rest.

Bitcoin has by far the strongest development team, and also is by far the
most decentralized.

Markets only care a little bit what your development team is like.
Ethereum has Vitalik, who is an incredibly smart and respectable dude,
while BU absolutely hates the core developers right now.  Markets are more
likely to put more faith in a single leader than core right now if that
comparison was really made.

"Most decentralized" is nearly impossible to quantify, and has almost no
value to speculators.  Since all of these markets are highly speculative,
they only care about future demand.  Future demand relies upon future use.
Unsubstantiated?  Ethereum is already 28% of Bitcoin by cap and 24% by
trading.  Four months ago that was 4%.  Their transaction volume also
doubled.  What world are you living in?

that's okay. Ethereum has very different properties and it's not something
I would trust as a tool to provide me with political sovereignty.

Well great, I guess so long as you're ok with it we'll just roll with it.
Wait, no.  If Bitcoin loses its first-mover network effect, a small cadre
of die-hard libertarians are not going to be able to keep it from becoming
a page in the history books.  Die hard libertarians can barely keep a voice
in the U.S. congress - neither markets nor day-to-day users particularly
care about the philosophy, they care about what it can do for them.

superior to Bitcoin.

The markets have literally told us why Ethereum is shooting up.  Its
because the Bitcoin community has fractured around a debate with nearly no
progress on a solution for the last 3 years, and especially because BU
appears to be strong enough to think they can fork and the markets know
full well what a contentious fork will do to Bitcoin's near-term future.

blockchains.

Then it would have happened not when the BU situation imploded but when
Microsoft announced they were working with Ethereum on things like that.
No one cared about Microsoft's announcement.  You don't seriously believe
what you're saying, do you?


I agree with you, but Bitcoin becoming a page in the history books because
a few die-hard libertarians didn't think price or adoption was important is
a big, big concern, especially when they almost have veto power.  Markets
don't care about philosophy, they care about future value.  Bitcoin has
value because we think it may be the most useful new innovation in the
future.  If we screw that future usefulness up, philosophy gives us no more
value than Friendster has today.

On Thu, Mar 30, 2017 at 4:19 AM, David Vorick via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Mon, 2017-03-06 at 17:30 -0500, Tim Ruffing via bitcoin-dev wrote:

Wait, forget this reply, I mixed up the two issues of keepalive and
definition of low, high etc... -.-

1. Keepalive for longpolling:
As I said, this can be useful for an out-of-date warning. I don't know
if this is better solved with TCP keepalive or on the higher layer.

2. Definition of low, high:
My feeling is that there is nothing wrong with providing exact
definitions in the BIP, i.e.., giving up the flexibility does not too
hurt much. However all of this is a minor issue after all.

Tim


-------------------------------------
On Sun, Jun 11, 2017 at 3:44 PM, Martijn Meijering via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Right, that would be part of it, as well as not removing the BIP141
deployment with bip9.
See https://github.com/jtimon/bitcoin/commit/62efd741740f5c75c43d78358d6318941e6d3c04


No, if segwit activates pre nov15, bip149 nodse can detect and
interpret that just fine.
The problem if it activates post nov15, then you need a separate
service bit in the p2p network, for pre-BIP149 will think sw hasn't
activated while post-BIP149 would know it has activated.

If you release it only after nov15, you don't need to test
compatibility between the two for neither of this two cases.
Or do you? Actually you only save testing the easier case of pre-nov15
activation.



-------------------------------------

My proposal makes it configurable (as well as window size, grace period etc.)


I still like the coinbase idea though - more than using up the BIP9 versionbits range for verbose signaling.

BIP9 (and other proposals which use those 29 versionbits) currently assume that the participants on the network will coordinate in some form or other, to agree on what the bits mean (in terms of change deployments).

It would be very easy to also agree on a set of "standard" threshold levels and map those onto e.g. 1 byte.

Then, in the coinbase, one could have pairs of bit numbers and bytes, e.g. "/1A/2B/3C/" where the bytes values corresponding to 'A', 'B', 'C', ... are standardized deployment schedules that people find useful.
So a BIP9 conformant schedule could be A = 95% / 2016 window,
while B = 75%/2016, etc.

This would be quite a compact yet still readable signaling. The space of values is large enough that I doubt we'd see much contention.

Regards
Sancho
-------------------------------------
While BIP91 is probably not terribly harmful, because the vast majority of
nodes and users are prepared for it - the hard fork portion of this BIP is
being deployed like an emergency patch or quick bug fix to the system.

Please consider updating the BIP to include some justification for the
urgency of the consensus change, and the reasons for not delaying until a
better engineered solution (spoonet, BIP103, etc.) can be deployed.


On Thu, Jul 13, 2017 at 3:19 PM, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On 02/25/2017 08:10 AM, Ethan Heilman via bitcoin-dev wrote:

You have to not only produce a ripemd160 collision, you have to produce 
a collision that is also a valid sha-256 hash - and that's much much 
much more difficult.


-------------------------------------
Hi ZmnSCPxj,


In my scheme, if you read carefully through the pseudocode, a block list

It seems this is a contradiction against the "blind" part of blind merge
mining. How can a bitcoin blockchain node enforce this without tracking the
sidechain?

Basically, in my scheme, the OP_RETURN data *is* the sidechain block


It seems both of our schemes need to include 2 32 bit hashes in the
blockchain. Your scheme needs a previous block header hash and the current
block header hash, while mine includes the current block header hash
twice.  We can just commit to all that information via the block header
hash and if a sidechain node lies to us will we are doing IBD the hashes
won't match with what was included in the bitcoin blockchain.

I'll follow your discussion with Paul about sidechain reorgs, but I think
his proposal is more desirable -- it follows what actually happens in the
bitcoin mining process where we *can* have chain splits when miners
simultaneously find a block. Other miners will pick one of the two blocks
to mine on top of and eventually one chain will become longer than the
other. Therefore that chain will have it's block's orphaned and the
miners/nodes following the dead chain will reorg on top of the longest
chain.

In Paul's scheme, we replace PoW with a bribe. At the conceptual level
these are somewhat similar. In PoW a miner is willing to pay a certain
amount of money (on electricity) to try to find a bitcoin block. With
OP_BRIBEVERIFY a sidechain miner is willing pay a certain amount of money
to find a block.

In PoW, there is nothing at the software level that says a miner cannot
just decide to build on a old block. I could decide to build on the genesis
block if I wanted to. Obviously this is a stupid idea as I'll never
overtake the bitcoin blockchain with 8 years of PoW behind it -- but it
doesn't mean I couldn't try if I wanted too. Your scheme from what I
understand prevents this from happening -- and I don't think that is
desirable. You might be able to make an argument that a rich attacker can
*stall* mining progress on the drivechain, but I think the same argument
can be made with a rich miner on the bitcoin blockchain as well. I think
miners have threatened to do that if BIP148 caused a chain split.

Can you link to the aforementioned pseudocode? I must have missed it on the
mailing list.

-Chris

On Tue, Jul 4, 2017 at 2:21 AM, ZmnSCPxj <ZmnSCPxj@protonmail.com> wrote:

-------------------------------------
On 17.09.2017 04:29, Pieter Wuille wrote:

Thanks for the info. I guess this means that a bech32 format for private
keys is not going to happen soon. Even if such a format was available,
the issue would remain for segwit-in-p2sh addresses, which use base58.

The ambiguity of the WIF format is currently holding me from releasing a
segwit-capable version of Electrum. I believe it is not acceptable to
use the current WIF format with segwit scripts; that would just create
technological debt, forcing wallets to try all possible scripts. There
is a good reason why WIF adds a 0x01 byte for compressed pubkeys; it
makes it unambiguous.

I see only two options:
 1. Disable private keys export in Electrum Segwit wallets, until a
common WIF extension has been agreed on.
 2. Define my own WIF extension for Electrum, and go ahead with it.

Defining my own format does make sense for the xpub/xprv format, because
Electrum users need to share master public keys across Electrum wallets.
It makes much less sense for WIF, though, because WIF is mostly used to
import/sweep keys from other wallets.

I would love to know what other wallet developers are going to do,
especially Core. Are you going to export private keys used in segwit
scripts in the current WIF format?


-------------------------------------
I've changed the proposal so only 8 bits are given to grinding so something
like 20 bits are available for signaling.

I have to say I'm at a loss here as to what's next? Should I make a new BIP
or try to convince the authors of BIP141 to modify their BIP? Could someone
inform me on the next part of the process?

On Tue, Apr 11, 2017 at 8:25 AM, Sancho Panza via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Wed, Jun 14, 2017 at 11:29 AM, Zheming Lin <heater@gmail.com> wrote:

of communication platforms. Though if you're looking for a way for users to
signal their intentions at the protocol level, every proposal for doing
that to date has been arguably flawed. Measuring meatspace consensus is
pretty tricky if not completely impossible, especially given the fact that
the vast majority of Bitcoin users do not voice any opinions on the matter
of consensus rules.

Most attempts at measuring user consensus would probably be best described
as signaling rather than voting given that the act of doing so has no
actual power to affect consensus. Every user who runs a fully validating
node is free to enforce the rules with which the agree regardless of what
rules other entities are enforcing.

confirming transactions that have a version less than X then it should be a
soft fork, yes.

-------------------------------------
Hi,
1. If there are 16.4 million mined and 4 million are lost, that results in
12.4 million in circulation vs 14.4 million.
2. Satoshi addressed this as have numerous other people (
https://bitcointalk.org/index.php?topic=198.msg1647#msg1647 ) - lost coins
decrease supply, increasing value of the remaining coins.
3. This assumes this is a problem.  Bitcoin is divisible, 100 million,
potentially more if necessary. (
https://en.bitcoin.it/wiki/Help:FAQ#How_divisible_are_bitcoins.3F)
4. Why is it okay to steal bitcoins from people who's bitcoins have been
"dormant" for a fixed period, 10 years in your example?
5. What happens to bitcoins that, say, Hal Finney still had (if any) and he
put in cold storage while he is in ultimate cold storage (
https://en.wikipedia.org/wiki/Hal_Finney_(computer_scientist)#Death) ?
Ditto for someone, say, in a coma for 11 years, in jail for 11 years or any
other similar event?  Or a 20 year old sets aside coins for retirement.
The following year, the system is changed, and when he looks again after
not paying attention for a decade or two, they are gone.
6. This encourages censorship by miners for people attempting to move coins.
7. This has been discussed many times before and everyone is welcome to
fork bitcoin code and the block chain and convince people to follow this
chain and code.  Then you can see if you can get many people to agree that
this is a good idea.








On Mon, Dec 11, 2017 at 12:30 PM, Teweldemedhin Aberra via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I am not sure that this discussion is really off topic for this list,
this is a real issue, would everybody even here say that they feel very
comfortable with their keys? That if something happen to them there is
no pb for the family or trusted parties to retrieve the keys? That this
process is secured in case the trusted parties are finally untrusted? etc

I don't think so, if experts are not comfortable then how can we expect
non experts people to manage this? (except going to a wallet sw asking
them all the info, even online, crazy but they just don't know)

Comments below


Le 30/09/2017  06:49, Jonas Schnelli via bitcoin-dev a crit:


Personnaly I don't see also the advantage of proposals such as BIP39 versus backing up a seed 


This is the intent of https://github.com/Ayms/bitcoin-wallets and
https://github.com/Ayms/zcash-wallets

But even myself can get confused, where did I put the backup seed? But
remember you did not backup the seed but the first derivation step and
you mixed it secretely, so nobody can reconstitute it except you,
well... what did I do exactly? What version is my real wallet? What is
the encryption key? How did I do last time to add the key in qt? etc


Is there really nothing existing yet to address all of this?


-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms


-------------------------------------
On Thu, Feb 23, 2017 at 3:51 PM, Peter Todd <pete@petertodd.org> wrote:


You can readily prove something is in the TXO or STXO set using the actual
blockchain, and the proofs will be nice and compact because even light
nodes are expected to already have all the historical headers.

What you can't do with MMRs or the blockchain is make a compact proof that
something is still in the utxo set, which is the whole point of utxo
commitments.

It's totally reasonable for full nodes to independently update and
recalculate the utxo set as part of their validation process. The same
can't be done for a balanced version of the txo set because it's too big.
Relying on proofs as a crutch for using the full txo set would badly
exacerbate the already extant problem of miners doing spv mining, and
increase the bandwidth a full validating node had to use by a multiple.

This whole conversation is badly sidetracked. If people have comments on my
merkle set I'd like to engage further with them, but mmrs need to be argued
independently on their own merits before being used as a counterpoint to
utxo commitments.
-------------------------------------
One consideration of exposing this in QT is that it may encourage users to
generate paper wallets(which are generally used and recommended for cold
storage) from online machines, rendering them moreso lukewarm rather than
cold, since the keys weren't generated in an air-gapped environment.  When
using bitaddress.org locally(we *are *all only using it locally and not
directly from the online webpage, right? ;) ) you've at least made the
effort to seek out the repo, clone it locally, and use it on an offline
machine and not retain any data from that session.

If we include this as a function in the reference implementation, how many
people are going to be making paper wallets with the intention of cold
storage on a machine that's potentially compromised?  As
adoption(hopefully) continues to increase the number of less than tech
savvy people using bitcoin will increase.

I'd suggest that any UI in QT include some sort of a modal dialog that
informs the user that this is not a secure cold storage address unless it
was created on an offline machine and printed on a non-networked printer,
and the prompt must be accepted and dismissed before the wallet will
provide the requested keys.


On Fri, Sep 29, 2017 at 12:29 PM, Dan Libby via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
Andrew Johnson
-------------------------------------
Hi Peter, thank you for the review.  See below

On Mon, Nov 6, 2017 at 11:50 AM Peter Todd <pete@petertodd.org> wrote:


The interesting thing is that the cost of attack varies smoothly as you
vary the POW weights.
To attack non-upgraded nodes, you still have to "51%" the original POW.
The reward going to that POW will vary smoothly between 1.0 * block_reward
and whatever
target value (e.g. 0.5 * block_reward) and the difficulty of attack will
tend to be proportional to that.

In a real hard-fork, your software just breaks at the fork point.  In this
case, it's just the non-upgraded
node security level declining from 100% to 50% over a long period of time.

I envision the transition of POW weights will be over 1-3 years, which
leaves plenty of time to
upgrade after the fork activates.



Note that the total transaction rate and block size don't materially
change, so I don't
see why the orphan rate will change.  Normal blocks are constrained to have
all of the txs of the aux blocks, so propagation time should stay the
same.  Am I missing
something?



"smooth-fork" perhaps? :)


-------------------------------------
Back in 2010, there was a bug found in Core which allowed denial-of-service
attacks due to the software crashing on some machines while executing a
script - see CVE-2010-537.
I believe the removed ("disabled") opcodes should be re-introduced along
with a standardized behavior definition.
For example, when execution of an opcode results in an arithmetic error,
such as OP_DIV with a zero divisor, the script should exit and fail.
The string splice opcodes should also check their arguments for
correctness, etc.

These opcodes would enhance the flexibility of scripts and allow
sophisticated native smart contracts to be created.
-------------------------------------
Dear Chris,


I have heard this nonsense repeated countless times in order to justify adopting Drivechain.

This is not how security works.

A child can "opt-in" to using a loaded gun, but is it a good idea to make it easy for them to do that?

No.

This is effectively the same thing Drivechains is doing.

It is a request to modify the Bitcoin protocol to make it easy for Bitcoin users to give their Bitcoins to miners.

Does that sound like a good idea to anyone?

If so, please leave, you are compromising Bitcoin's security.

Security is about making it difficult to shoot yourself in the face.

If I design a car that has a button that randomly causes the brakes to give out if pressed, is that a good idea? Can I justify pushing for such a "feature" just because it's "opt-in"?

No. That is fallacy.

It is not how secure systems are designed.

It is how *insecure* systems are designed.



Sure, happy to, as soon as I have it written up in detail.

Kind regards,
Greg Slepak

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
What I mean is that spoonet and other HF improvements, and a slower
timeline needs to be folded in ...before the HF activation date - to make
it far more likely that the community adopts the whole proposal and the
chain doesn't fragment.

If you try to push a 2mb with no safety checks and nothing else improved -
nothing will happen.

Take a quick look at the COOP proposal...it gets us to 4mb blocks in 4
years....gradually, no massive fee swings.


On Fri, Jun 2, 2017 at 5:51 PM, Sergio Demian Lerner <
sergio.d.lerner@gmail.com> wrote:

-------------------------------------
On Sunday, 2 April 2017 22:39:13 CEST Russell O'Connor via bitcoin-dev 
wrote:

That change would not be a consensus change and thus free to make any day.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
Below is a proposal for a wallet data structure that can enable a wallet to be user friendly.

This is a proposed partial solution to "Pruned wallet support #9409": https://github.com/bitcoin/bitcoin/issues/9409
It is envisioned to work with "Complete hybrid full block SPV mode #9483": https://github.com/bitcoin/bitcoin/pull/9483

The data structure implies that the wallet would/could run in a different process than a Bitcoin node. Furthermore, the wallet would have a set of whitelisted/trusted nodes, which may only be the user's nodes.

Cheers,
Praxeology Guy

The primary purpose of this data structure is to enable the creation of a responsive and informative open-and-go wallet. Some of my design goals include:
- Instant on. Can be put in cold storage, and years later be immediately operational.
- Scans optional/happen in background. Supports partial scans of the block height range.
- Functional even with only utxo set data
- Fast loading of external private keys and other watched identifiers.
- Supports wallet merging
- Supports connection with different kinds of nodes/providers of transaction evidence
- Communicates the reliability of the evidence with the user
- Supports transaction deprecation and double spending
- Can work in a different process than a Bitcoin relay node.
- Works immediately/quickly even with a node that has only prefetched headers and only begun validating blocks.
- Potentially allows a wallet to display unconfirmed transactions to the user even if it only has an SPV evidence source.

=== Wallet ===
- List: Spend Term
- List: Wallet Coin
- List: Spend Attempt
- List: Transaction Evidence
- List: Coin Scan
- List: Evidence Source

// The wallet would also need other data that existing wallets have such as information about deterministic keys and pre-allocated keys etc. For the wallet to work with SPV security, it would also need block headers.

=== Spend Term ===
- GUID
- Name (Human readable)
- Type: [private key/public key/address/out script]
- Term Value
- Creation date
- List: Wallet Coin

// A spend term is anything that can be used to identify a coin that should be tracked by this wallet. GUID should probably be the address or a hash of the out script.

=== Evidence Source ===
- GUID
- Operator Name
- Device Name
- Software Name
- Software Version
- Public Key
- Operator Trust: Self, Reliable Friend, Stranger
- Device Security: Air Gap, Single Purpose Networked, Multipurpose Networked, Dedicated Remote Hosted, VPS Remote Hosted

// An Evidence Source is a {operator, device, software} that fully validates blocks. It provides evidence of coin confirmations and spends. Maybe GUID should be a hash of the public key.

=== Wallet Coin ===
Required:
- Spend Term GUID
- TXID
- vout.ix
- amount
- output script
Optional:
- Wallet first discovery date
- full TX data
- List: Transaction Evidence
- List: Spend Attempt
- TXID Spent

// A wallet coin is a bitcoin txo with some extra data relevant to the wallet. A wallet coin's ID is {TXID, vout.ix}.

=== Spend Attempt ===
- Wallet Coins List: {TXID, vout.ix}
- TXID
- Creation date
- Relay date(s)
- Is Deprecated?
- Deprecated date
- Replace By Fee (RBF) enabled?
- List: Transaction Evidence

// deprecating a spend attempt will clear the "TXID Spent" in Wallet Coin, and set "Is Deprecated?" and "Deprecated date".

=== Evidence ===
Required:
- tip hash
- tip height
- date
- Evidence Source GUID
- Evidence Source Signature

=== Transaction Evidence : Evidence ===
Required:
- TXID
Optional:
- discover date

// Evidence of the creation or spend of a coin. There are different kinds of evidence from different sources. The kind and source impact the user's confidence in the validity and finality of a transaction.

=== Confirmed UTXO Set Presence : Transaction Evidence ===
Required:
- Is in UTXO set?
Optional:
- block height
- block hash
- block timestamp
- Future: utxo set snaphot merkle proof
- confirm date

// Evidence (from a trustworthy source needed) that a coin is or is not in the confirmed utxo set. With the future possibility of utxo set snapshot merkle proof, the trustworthiness of the source is not as important. This evidence type is only used for Wallet Coins. It is not used for Spend Attempts.

=== Confirmation : Transaction Evidence ===
Required:
- block height
- block hash
- block timestamp
- tx merkle proof
- wtx merkle proof
- is in greatest PoW chain?
Optional:
- confirm date
- 51% attack cost to double spend
- Future: Are/which reliable miners still creating blocks in the greatest PoW chain (network split risk)

// Evidence that a transaction was confirmed.

=== Discovery (Unconfirmed TX) : Transaction Evidence ===
Optional:
- Confirmable Evidence: tree of source input transactions that lead back to utxo outputs. Unconfirmed parents: discover date, size, fee.

// Evidence merely that a transaction was discovered. There is no guarantee that the transaction will be confirmed. Preferably the Source that is providing the wallet with the discovery evidence will also provide evidence that the transaction could be confirmed... especially if the Source is not trusted. The wallet could ask other Sources whether the confirmed inputs are present in the Confirmed UTXO Set.

=== Spend Term Scan : Evidence ===
- Term List: Spend Term GUID
- Target: [blocks, confirmed utxo set]
- Range Start
- Range End

// The wallet would request a scan from the Source. It would provide the spend terms, target, and range. The Source would respond with the range actually scanned, in addition to any Transaction Evidence discovered.
-------------------------------------
On Fri, Jul 21, 2017 at 12:54 PM, Jameson Lopp <jameson.lopp@gmail.com>
wrote:


I don’t think it’s like demurrage in Freicoin at all. The purpose of the
proposal is to help Bitcoin scale, which is not the purpose of Freicoin’s
demurrage fee. Demurrage fee is not optional in Freicoin, and with this
proposal most users will likely never have to burn any coins at all given
how long it would take for bitcoins to lose their luster.



practice would not affect 99.9% of users because it is unlikely that coins
will ever get to the point where they start losing their luster.



less controversial. Because by putting a cap on the block chain size and
UTXO set, we know exactly how much disk space and RAM a node needs to run a
full node.



the block chain. I mean to say that we should prune the old UTXOs along
with the old blocks. This would mean that we would have to create a
checkpoint every ~150 fifty years (base on my example) and node would drop
blocks older then those checkpoints.  This would mean new nodes would start
syncing from the checkpoint not the genesis block.



-------------------------------------


If Alice still has full control, she is already protected by my proposal, which does not require any protecting child transaction.

But in many cases she may not have full control. Make it clearer, consider that’s actually a 2-of-2 multisig of Alice and Bob, and the time locked tx is sending to Bob. If the time locked tx is unprotected in the first place, Bob will get all the money from both forks anyway, as there is no reason for him to renegotiate with Alice.
-------------------------------------
The discussion of UASF got me thinking about whether such a method might lead to sybil attacks, with new nodes created purely to inflate the node count for a particular implementation in an attempt at social engineering.


I had an idea for an anonymous, opt-in, unique node identification mechanism to help counter this.


This would give every node the opportunity to create a node address/unique identifier. This could even come in the form of a Bitcoin address.


The node on first installation generates and backs up a private key. The corresponding public key becomes that nodes unique identifier. If the node switches to a new software version or a new IP, the identifier can remain constant if the node operator chooses.


Asking a node for its identifier can be done by sending a message the command identify and a challenge. The node can then respond with its unique identifier and a signature for the challenge to prove it. The node can also include what software it is running and sign this information so it can be verified as legitimate by third parties.


Why would we do this?


Well, it adds a small but very useful piece of data when compiling lists of active nodes.


Any register of active nodes can have a record of when a node identifier was first seen, and how many IPs the same identifier has broadcast from. Also, crucially, we could see what software the node operator has been seen running historically.


This information would make it easy to identify patterns. For example if a huge new group of nodes appeared on the network with no history for their identifier they could likely be dismissed as sybil attacks. If a huge number of nodes that had been reporting as Bitcoin Core for an extended period of time started switching to a rival implementation, this would add credibility but not certainty (keys could be traded), that the shift was more organic.


This would be trivial to implement, is (to me?) non-controversial, and would give a way for a node to link itself to a pseudo-anonymous identity, but with the freedom to opt-out at any time.


Keen to hear any thoughts?


Thanks,


John Hardy

john@seebitcoin.com
-------------------------------------
I have completed updating the three BIPs with all the feedback that I have received so far. In short summary, here is an incomplete list of the changes that were made:

* Modified the hashing function fast-SHA256 so that an internal node cannot be interpreted simultaneously as a leaf.
* Changed MERKLEBRANCHVERIFY to verify a configurable number of elements from the tree, instead of just one.
* Changed MERKLEBRANCHVERIFY to have two modes: one where the inputs are assumed to be hashes, and one where they are run through double-SHA256 first.
* Made tail-call eval compatible with BIP141’s CLEANSTACK consensus rule by allowing parameters to be passed on the alt-stack.
* Restricted tail-call eval to segwit scripts only, so that checking sigop and opcode limits of the policy script would not be necessary.

There were a bunch of other small modifications, typo fixes, and optimizations that were made as well.

I am now ready to submit these BIPs as a PR against the bitcoin/bips repo, and I request that the BIP editor assign numbers.

Thank you,
Mark Friedenbach



-------------------------------------
To have a BIP, you need to explain not only *why* you want to do something, 
but also *what specifically* to do, and *how* to do it. This concept 
(historically known as "flip the chain" and/or "UTXO commitments") is not new, 
merely complicated to design and implement.

Luke


On Wednesday 16 August 2017 4:20:45 PM Алексей Мутовкин via bitcoin-dev wrote:

-------------------------------------
Hi Erik,

As you know:

1. If a sidechain is merged mined it basically grows out of the existing
Bitcoin mining network. If it has a different PoW algorithm it is a new
mining network.
2. The security (ie, hashrate) of any mining network would be determined
by the total economic value of the block. In Bitcoin this is
(subsidy+tx_fees)*price, but since a sidechain cannot issue new tokens
it would only be (tx_fees)*price.

Unfortunately the two have a nasty correlation which can lead to a
disastrous self-fulfilling prophecy: users will avoid a network that is
too insecure; and if users avoid using a network, they will stop paying
txn fees and so the quantity (tx_fees)*price falls toward zero, erasing
the network's security. So it is quite problematic and I recommend just
biting the bullet and going with merged mining instead.

And, the point may be moot. Bitcoin miners may decide that, given their
expertise in seeking out cheap sources of power/cooling, they might as
well mine both/all chains. So your suggestion may not achieve your
desired result (and would, meanwhile, consume more of the economy's
resources -- some of these would not contribute even to a higher hashrate).

Paul



On 6/19/2017 1:11 PM, Erik Aronesty wrote:

-------------------------------------
This proposal is written under the assumption that the signatories to the Consensus 2017 Scaling Agreement[1] are genuinely committed to the terms of the agreement, and intend to enact the updates described therein. As such, criticisms pertaining to the chosen deployment timeline or hard fork upgrade path should be treated as out-of-scope during the initial discussion of this proposal.

Because it includes the activation of a hard fork for which community consensus does not yet exist, this proposal is not likely to be merged into Bitcoin Core in the immediate future, and must instead be maintained and reviewed in a separate downstream repository. However, it is written with the intent to remain cleanly compatible with future network updates and changes, to allow for the option of a straightforward upstream merge if community consensus for the proposal is successfully achieved in the following months.

<pre>
BIP: ?
Layer: Consensus
Title: Compatibility-oriented omnibus proposal
Author: Calvin Rechner <calvinrechner@protonmail.com>
Comments-Summary: No comments yet.
Comments-URI: ?
Status: Draft
Type: Standards Track
Created: 2017-05-28
License: PD
</pre>

===Abstract===

This document describes a virtuous combination of James Hilliard’s “Reduced signalling threshold activation of existing segwit deployment”[2], Shaolin Fry’s “Mandatory activation of segwit deployment”[3], Sergio Demian Lerner’s “Segwit2Mb”[4] proposal, Luke Dashjr’s “Post-segwit 2 MB block size hardfork”[5], and hard fork safety mechanisms from Johnson Lau’s “Spoonnet”[6][7] into a single omnibus proposal and patchset.

===Motivation===

The Consensus 2017 Scaling Agreement[1] stipulated the following commitments:

• Activate Segregated Witness at an 80% threshold, signaling at bit 4
• Activate a 2 MB hard fork within six months

This proposal seeks to fulfill these criteria while retaining maximum compatibility with existing deployment approaches, thereby minimizing the risks of a destructive chain split. Additionally, subsequent indications of implied criteria and expectations of the Agreement[8][9] are satisfied.

The proposed hard fork incorporates a legacy witness discount and 2MB blocksize limit along with the enactment of Spoonnet-derived protectionary measures, to ensure the safest possible fork activation within the constraints of the requirements outlined in the Scaling Agreement.

===Rationale===

To the extent possible, this represents an effort at a best-of-all-worlds proposal, intended to provide a common foundation from which all mutually-inclusive goals can be achieved while risks are minimized.

The individual constituent proposals include the following respective rationales:

James Hilliard’s “Reduced signalling threshold activation of existing segwit deployment”[2] explains:


Shaolin Fry’s “Mandatory activation of segwit deployment”[3] is included to:


Both of the aforementioned activation options (“fast-activation” and “flag-day activation”) serve to prevent unnecessary delays in the network upgrade process, addressing a common criticism of the Scaling Agreement and providing an opportunity for cooperation and unity instead.

Sergio Demian Lerner’s “Segwit2Mb”[4] proposal explains the reasoning behind linking SegWit’s activation with that of a later hard fork block size increase:


Luke Dashjr’s “Post-segwit 2 MB block size hardfork”[5] suggestions are included to reduce the marginal risks that such an increase in the block size might introduce:


Johnson Lau’s anti-replay and network version updates[6][7] are included as general hard fork safety measures:


===Copyright===

This document is placed in the public domain.

===Specification===

###Proposal Signaling###

The string “COOP” is included anywhere in the txn-input (scriptSig) of the coinbase-txn to signal compatibility and support.

###Soft Fork###

Fast-activation (segsignal): deployed by a "version bits" with an 80% activation threshold BIP9 with the name "segsignal" and using bit 4... [with a] start time of midnight June 1st, 2017 (epoch time 1496275200) and timeout on midnight November 15th 2017 (epoch time 1510704000). This BIP will cease to be active when segwit is locked-in.[2]

Flag-day activation (BIP148): While this BIP is active, all blocks must set the nVersion header top 3 bits to 001 together with bit field (1<<1) (according to the existing segwit deployment). Blocks that do not signal as required will be rejected... This BIP will be active between midnight August 1st 2017 (epoch time 1501545600) and midnight November 15th 2017 (epoch time 1510704000) if the existing segwit deployment is not locked-in or activated before epoch time 1501545600. This BIP will cease to be active when segwit is locked-in. While this BIP is active, all blocks must set the nVersion header top 3 bits to 001 together with bit field (1<<1) (according to the existing segwit deployment). Blocks that do not signal as required will be rejected.[3]

###Hard Fork###

The hard fork deployment is scheduled to occur 6 months after SegWit activates:

(HardForkHeight = SEGWIT_ACTIVE_BLOCK_HEIGHT + 26280)

For blocks equal to or higher than HardForkHeight, Luke-Jr’s legacy witness discount and 2MB limit are enacted, along with the following Spoonnet-based improvements[6][7]:

* A "hardfork signalling block" is a block with the sign bit of header nVersion is set [Clearly invalid for old nodes; easy opt-out for light wallets]

* If the median-time-past of the past 11 blocks is smaller than the HardForkHeight... a hardfork signalling block is invalid.

* Child of a hardfork signalling block MUST also be a hardfork signalling block

* Hardfork network version bit is 0x02000000. A tx is invalid if the highest nVersion byte is not zero, and the network version bit is not set.

===Deployment===

Deployment of the “fast-activation” soft fork is exactly identical to Hilliard’s segsignal proposal[2]. Deployment of the “flag-day” soft fork is exactly identical to Fry’s BIP148 proposal[3]. HardForkHeight is defined as 26280 blocks after SegWit is set to ACTIVE. All blocks with height greater than or equal to this value must adhere to the consensus rules of the 2MB hard fork.

===Backwards compatibility===

This deployment is compatible with the existing "segwit" bit 1 deployment scheduled between midnight November 15th, 2016 and midnight November 15th, 2017.

To prevent the risk of building on top of invalid blocks, miners should upgrade their nodes to support segsignal as well as BIP148.

The intent of this proposal is to maintain full legacy consensus compatibility for users up until the HardForkHeight block height, after which backwards compatibility is waived as enforcement of the hard fork consensus ruleset begins.

===References===

[1] https://medium.com/@DCGco/bitcoin-scaling-agreement-at-consensus-2017-133521fe9a77
[2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014380.html
[3] https://github.com/bitcoin/bips/blob/master/bip-0148.mediawiki
[4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013921.html
[5] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014399.html
[6] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-February/013542.html
[7] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013473.html
[8] https://twitter.com/sysmannet/status/867124645279006720
[9] https://twitter.com/JihanWu/status/867139046786465792
-------------------------------------
Good morning Paul and Chris,


I actually devised a way to work around this collective action problem, and discussed it obliquely in a private e-mail with Chris, while I was preparing my article on sidechain weaknesses.  I removed it before publication of the sidechain weaknesses article, but perhaps I should not have.

Collective action can be ensured by contract.  In a world where some system can enforce certain actions programmatically, it is possible to ensure collective action via a program, i.e. a "smart contract".

The thief pays out to the destination address that is a P2SH of the below script:

OP_IF
  OP_HASH160 <hash> OP_EQUALVERIFY
  OP_DUP OP_HASH160 <thiefPubKeyHash> OP_EQUALVERIFY OP_CHECKSIG
OP_ELSE
  <withdrawTime+1week> OP_CHECKLOCKTIMEVERIFY OP_DROP
  OP_TRUE
OP_ENDIF

If the thief does not publish the preimage of the hash within 1 week of the withdrawal time, then it becomes possible for anyone to spend the above script; basically, some lucky miner who wins the first block past the specified time will get the entire winnings.  Let us call the above script, the Theft Contract.

The thief then recruits accomplices to the theft.  Note that the attack can be prepared and initiated before the accomplices are even recruited.

The thief locks some coins (the "cut" the accomplice gets), to the below script, for each accomplice it tries to entice:

OP_IF
  OP_HASH160 <hash> OP_EQUALVERIFY
  OP_DUP OP_HASH160 <accomplicePubKeyHash> OP_EQUALVERIFY OP_CHECKSIG
OP_ELSE
  <withdrawTime+2week> OP_CHECKLOCKTIMEVERIFY OP_DROP
  OP_DUP OP_HASH160 <thiefPubKeyHash> OP_EQUALVERIFY OP_CHECKSIG
OP_ENDIF

Let us call the above script, the Accomplice Contract.  If the accomplice accepts, he or she then starts to vote for the invalid withdrawal.

If the invalid withdrawal succeeds, the thief acquires the entire theft price from the Theft Contract by publishing the preimage to the <hash>.  (If he or she does not, then, some randomly-selected miner will acquire the money after the timeout, so the thief needs to publish the hash, before the timeout in the Theft Contract).

This publishes the preimage on the blockchain.  Each accomplice can then acquire their "cut" of the theft by copying the preimage and claiming from the Accomplice Contract.

If the theft never succeeds, then there is no reason for the thief to ever publish the preimage, and after the timeout on the Accomplice Contract, the thief can recover his or her offered funds at no loss (minus transaction fees),  This incentivizes accomplices to actually cooperate with the thief, as they will not get paid if the theft does not push through.

All that is necessary is for a single "mastermind" thief to begin this process.  Accomplices can be recruited later, with the "cut" they get negotiated according to how much hashpower they can bring to bear on theft.

Newly-created miners and mining pools can be enticed at the time they arise by offering an Accomplice Contract to them.  Thus, newly-created miners and mining pools can be brought into cooperation with the thief as soon as they make a presence on the blockchain.

Even if some mining pool makes a public statement that they will not assist in the theft, the thief may still commit an Accomplice Contract for them on-chain anyway, and publicize it, in order to put the integrity of that mining pool in question and drive out support from that mining pool.  True accomplices may pretend to initially be honest and then signal dishonestly later, in order to make it more plausible that a pool that "committed" to not support the theft is not trustable since they have an Accomplice Contract that will compensate them if they support the theft, creating further confusion and discord among honest miners.  The thief may also use the existence of such an Accomplice Contract while negotiating with more minor miners and mining pools, in order to entice those also to join, and thus gain additional buffer against the stochastic process of miner voting.

With the Theft Contract and the Accomplice Contract, negotiation can be done in parallel with the theft attempt, reducing the cost of organizing collective action, as we have all hoped "smart contracts" would do.

----

While it is true, that this requires that the thief have significant funds in reserve prior to theft (in order to fund the Accomplice Contracts he or she will have to offer to potential accomplices), we have always been assured that theft can be initiated by miners only, and that miners already have a significant amount of money they control.  So it will be no problem for a potential thief to reserve some funds for paying to Accomplice Contracts.

This vulnerability can be fixed if withdrawals are restricted to simple P2PKH or P2WPKH only, but in the presence of Scriptless Script and Bellare-Neven signatures, that may be sufficient to create the Theft Contract and the Accomplice Contract (but I know too little of Scriptless Script to be sure).

Regards,
ZmnSCPxj
-------------------------------------
With all due respect, this isn't a BIP. It's idle speculation regarding
what one person considers to be a problem and others may not. Please
read https://github.com/bitcoin/bips/blob/master/bip-0001.mediawiki and
try again. Among other things:

- Convince us this is a real issue, and that your data is accurate.
Who's to say which coins are truly lost? Maybe the people controlling
the keys are just laying low, wish to use their coins in 2112, etc.
- Propose formulas for how the miners will take everything into account,
and explain why they're acceptable.
- Write code that implements your ideas.

Good luck!

-- 
---
Douglas Roark
Cryptocurrency, network security, travel, and art.
https://onename.com/droark
joroark@vt.edu
PGP key ID: 26623924

-------------------------------------


Le 07/01/2017  21:26, Chris Priest via bitcoin-dev a crit :

Certainly not


This is what we could call a decentralized system, when everybody is
affected


That's an obvious weakness of the system


Well, probably you did not mean this, this is not fair. "Just a node"...

Still wondering why you guys don't care about the ridiculous number of
full nodes, no incentive to run one and what would happen if someone
were to control a majority of full nodes

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms


-------------------------------------
Ok, I see your point. I was just thinking about the number of bitcoins tied
up in wallets in which people lost the keys, but I suppose this isn't so
much of a problem if it's well known that the bitcoins are all tied up. It
would be impossible to distinguish between bitcoins people have lost access
to, and bitcoins that people have just left in the same wallet for a long
time.

On Tue, 22 Aug 2017, 3:45 pm Chris Riley <criley@gmail.com> wrote:

-------------------------------------
Hi all,

Recently I think a lot about combining Stealth addresses with SPV but
I did not come to a satisfying conclusion, so I post this as a
challenge to the wider community. Maybe you have an idea.

## Explanation of SPV
In SPV a thin client puts his public keys in a bloom filter
and asks a full node to give him Merkle proofs of all transactions
whose pubkey are in the bloom filter. Since a bloom filter has a lot
of false positives depending on the parameters, this gives privacy to
the thin client, since the full node cannot detect if a specific
transaction belongs to the thin client. This is cool if you want to
use Bitcoin on your smartphone.

## Explanation of Stealth Addresses
Stealth addresses on the other hand enable receiver privacy. The
sender of a transaction derives a one-time pubkey to which he sends the
money. The receiver can check if the money was sent to him and recover
the one-time private key. This is cool, since an observer cannot
decide if two payments belong to the same recipient. Further the
recipient needs only to have one pubkey.
For a more formal explanation see https://github.com/genjix/bips/blob/master/bip-stealth.mediawiki#Reuse_ScanPubkey
I will use their notation in the following.

## The Problem
My line of thought was to combine stealth addresses with spv, so that
I can use stealth addresses on my smart phone without losing privacy.

Basically to check if a payment belongs to a pubkey (Q,R), the full
node needs to check if R' = R + H(dP)*G for each transaction. For this
it needs the private scanning key d.
This sucks, since when I give my d to a full node, he can link all my
transactions. For an online-wallet this may be okay, but not for thin
client synchronisation.

## Ideas
In the following I detail some ideas of me which did not work.

It does not suffice to have a Bloom filter and check if d is
contained since there is no way to recompute d from the equation. If
there were a way to recompute d, the scheme would offer no privacy,
since anyone could compute the private scanning key d and scan for
payments.
So, if we modify the scheme we need to be sure that d is kept private.

Multiparty computation may be possible in theory. The full node and
the thin client could collaboratively check R' = R + H(dP)*G, where d
is the private input of the thin client and R, R',P is provided by the
full node. But this is costly and they need to do it for each
transaction. It may be more costly than simply setting up a full node.

I do not think that some kind of search functionality without leaking
the search pattern (PIR?) would work, since the full node needs to compute on the
data it has found. And further it needs to retrieve the whole Merkle
proofs.

Any better ideas?

Best,
Henning

-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp

-------------------------------------
On Wed, Apr 5, 2017 at 9:37 PM, Gregory Maxwell <greg@xiph.org> wrote:

It was just pointed out to me that the proposed ID (which I just
selected to be above the segwit one) collides with one chosen in
another non-BIP proposal.  This wasn't intentional, and I'll happily
change the value when I update the document.

-------------------------------------
Hola,

I've submitted the generalized versionbits specification for BIP number assignment:

https://github.com/bitcoin/bips/pull/532

Your feedback and comments welcome.

The spec has been updated to include a link to the reference implementation.
I hope to find time soon to produce a similar reference implementation on Bitcoin Core.

Sancho
-------------------------------------
I can't think of any resistance to this, but the code, on a tight timeline,
isn't going to be easy.   Is anyone volunteering for this?

On May 29, 2017 6:19 AM, "James Hilliard via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Good morning,


The problem, is that, the rate of conversion of Bitcoin-> hashrate is different for different people.

For some, it's very cheap to convert Bitcoin -> hashrate. For others, it's very expensive. The reason, is the large difference in electricity rates depending on country.

It's all very well for those who can get electricity cheaply. But, for some, it is not.

Thus, paying someone with better Bitcoin->hashrate conversion via fees such as these is more economically logical, than to suffer a lower Bitcoin->hashrate conversion.


It is also, very obviously, clear that you are operating under very strange assumptions, that all people are already equal somehow, or that someone who is paid x10 more is strictly superior, even though skill-wise and ability-wise, they are the same, and the one paid less is simply suffering due to the country where he or she is born in, through no fault of their own.

Regards,
ZmnSCPxj
-------------------------------------
- no quadratic hashing solution
- no way to prevent spamming the network to blow up block sizes
- no mention of release schedule/consensus levels, etc.   should be
mentioned
- this is similar to other BIP already in place... see BIP107


On Mon, Mar 13, 2017 at 9:08 AM, ashish khandekar via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Jan 27, 2017 03:03, "Andrew Johnson via bitcoin-dev" <bitcoin-dev@lists.
linuxfoundation.org> wrote:

Other researchers have come to the conservative conclusion that we could
handle 4MB blocks today.


I believe this is a mischaracterization of the research conclusions.  The
actual conclusion was that the maximum value for the blocksize that the
network can safely handle (at that time) is some value that is
(conservatively) no more than 4MB.  This is because the research only
studies one aspect of the effect of blocksize on the network at a time and
the true safe value is the minimum of all aspects.  For example, the 4MB
doesn't cover the aspect of quadratic hashing for large transactions in
large blocks.
-------------------------------------


I don’t fully understand your storage engine. So the following deduction is just based on common sense.

a) It is possible to make unlimited number of 1-in-100-out txs

b) The maximum number of 100-in-1-out txs is limited by the number of previous 1-in-100-out txs

c) Since bitcrust performs not good with 100-in-1-out txs, for anti-DoS purpose you should limit the number of previous 1-in-100-out txs. 

d) Limit 1-in-100-out txs == Limit UTXO growth

I’m not surprised that you find an model more efficient than Core. But I don’t believe one could find a model that doesn’t become more efficient with UTXO growth limitation.

Maybe you could try an experiment with regtest? Make a lot 1-in-100-out txs with many blocks, then spend all the UTXOs with 100-in-1-out txs. Compare the performance of bitcrust with core. Then repeat with 1-in-1-out chained txs (so the UTXO set is always almost empty)

One more question: what is the absolute minimum disk and memory usage in bitcrust, compared with the pruning mode in Core?

-------------------------------------
My full-RBF patched branch of Bitcoin Core v0.14.0rc1 is now available:
https://github.com/petertodd/bitcoin/tree/replace-by-fee-v0.14.0rc1

As with replace-by-fee-v0.13.2, this version uses the nRelevantServices
machinery to do preferential peering, so it's just a few lines of code changed
between it and Bitcoin Core v0.14.0rc1.

The relevant services machinery is less agressive at connecting to full-RBF
peers than the earlier custom code previous versions used. But it seems to work
well enough to keep RBF peers connected to each other, so I'm inclined to keep
using it as doing so makes maintaining this patched branch pretty trivial every
time a new upstream version is released.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
What?

That is not correct.

There is a fixed amount of Bitcoin, as I said.

The only difference is what chain it is on.

It is precisely because there is a fixed amount that when you burn-to-withdraw you mint on another chain.

I will not respond to any more emails unless theyfre from core developers. Gotta run.

--
Sent from my mobile device.
Please do not email me anything that you are not comfortable also sharing with the NSA.



-------------------------------------
Segwit allows old -> old, old -> new, new -> old and of course new -> new
txs.

On 17 Mar 2017 1:47 a.m., "Erik Aronesty via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Yeah, it does make things harder, and it's easy enough to soft fork to
handle arbitrary opt-in protocol improvements, new much larger block sizes,
whatever you want.   Even OK to migrate to a new system by not allowing
old->old or new->old transactions.



_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------

I see this use case.
But I did receive bank wire transfers for the last decades without _immediately_ knowing that someone sent funds to me.
I personally would ALWAYS trade the higher bandwidth consumption (300MB mempool filtering) or slower notification time (maybe ~1h) for preserving privacy.
I agree, there are use cases where you want immediate notification, those use cases could probably be solved by not trowing away privacy („parsing“ all transactions and running in the background).

/jonas
-------------------------------------
On Sat, Feb 25, 2017 at 03:53:12PM -0500, Russell O'Connor wrote:

I'm very aware of that, in fact I think I may have even been the first person
to post on this list the commit-reveal mitigation.

Note how I said earlier in the message you're replying to that "P2SH's 160-bits
is insufficient in certain use-cases such as multisig"

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On Tue, Jun 13, 2017 at 2:23 AM, Zheming Lin via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Your English is much better than my Chinese.  Thank you for taking the
time to write this.

I am still reading and trying to completely understand your proposal
but I wanted to make one observation:


This is not true. Non-mining wallet nodes were considered, and their
upgrade practices are not usually slower than miners.

Even in the very first version of the software it did not mine unless
the user went into the settings and explicitly turned it on or used a
command-line option.  By default, every installation of Bitcoin was a
non-mining wallet node.

The enforcement of the system's rules by users broadly, and not just
miners, is specifically described in the white paper (section 8,
paragraph 2, it especially clear in the last sentence).  This is
critical for the security of Bitcoin especially with the current
degree of centralization in pools.  Without it, Bitcoin's security
would look a lot more like the Ripple system.

Frequently it is the miners that are "passive and lazy" in upgrading.
In some cases when new versions have had major improvements specific
to mining (such as for 0.8) miners upgraded much faster than other
nodes. But often, it is the other way around and miners adopt new
versions much slower than other nodes. If you look at block
construction today you will see that many miners are running highly
outdated node software which is more than one or even two years old.
(and as a result, they lose out on a considerable amount of
transaction fees.)

In fact, many miners have the most severe form of passive behavior:
they do not run a node at all but simply sell their hash power to
pools (which themselves are often slow to upgrade).  By comparison,
http://luke.dashjr.org/programs/bitcoin/files/charts/branches.html 95%
of reachable nodes are running software now from the last year and a
half.

I do not, however, believe that it is a problem that anyone is slow to upgrade.

Reliability cannot be maintained in infrastructure if it is rapidly
changing.  A normal deployment process for major systems
infrastructure outside of Bitcoin usually takes months because time
must be given to test and find bugs.

Miners depend on their income from mining and interruptions can be
very costly.  Many pools are also involved with altcoins which are
constantly breaking and they have their attention directed elsewhere
and cannot quickly spare the time required to upgrade their software.
These delays are the natural consequence of a decentralized system
where no one has the power to force other people to adopt their
priorities.

If you look at the deployment processes of major internet protocols,
HTTP2, new versions of SSH, BGP,  or IP itself you will find that
upgrades often happen slower than the entire life of Bitcoin so far--
and none of these protocols have the difficult consistency challenges
of Bitcoin or as much risk of irreparable financial loss if things go
wrong.

Because many people in the Bitcoin community appears to expect
upgrades much faster than even centralized ISP backbones upgrade their
router software I think they have unrealistic expectations with how
fast upgrading can occur while preserving stability, security, and
decentralization and unrealistic expectations of how fast upgrading
will occur so long as no one has the ability to force other people to
run their upgrades.

I look forward to competing my understanding of your proposal.

Cheers,

-------------------------------------
I had these following ideas as I was thinking about how to allow larger
blocks without incentivizing the creation of excessively large blocks. I
don't know how much of this is novel, I'd appreciate if anybody could link
to any relevant prior art. I'm making no claims on this, anything novel in
here is directly released into public domain.

In short, I'm trying to rely on simple but direct incentives to improve the
behavior of Bitcoin.

Feedback requested. Some simulations requested, see below if you're willing
to help. Any questions are welcome.

---

Expedience fees. Softfork compatible.

You want to really make sure your transaction gets processed quickly?
Transactions could have a second fee type, a specially labeled
anyone-can-spend output with an op_return value defining a "best-before"
block number and some function describing the decline of the fee value for
every future block, such that before block N the miners can claim the full
expedience fee + the standard fee (if any), between block N+1 and N+X the
miner can claim a reduced expedience fee + standard fee, afterwards only
the standard fee.

When a transaction is processed late such that not the full expedience fee
can be claimed, the remainder of the expedience fee output is returned to
the specified address among the inputs/outputs (could be something like
in#3 for the address used by the 3rd UTXO input). This would have to be
done for all remaining expedience fees within the last transaction in the
block, inserted there by the miner.

These additional UTXO:s does increase overhead somewhat, but hopefully not
by too much. If we're going to modify the transaction syntax eventually,
then we could take the chance to design for this to reduce overhead.

My current best idea for how to handle returned expedience fees in
multiuser transactions (coinjoin, etc) is to donate it to an agreed upon
address. For recurring donation addresses (the fee pool included!), this
reduces the number of return UTXO:s in the fee processing transaction.

The default client policy may be to split the entire fee across an
expedience fee and a fee pool donation, where the donation part becomes
larger the later the transaction gets processed. This is expected to slow
down the average inclusion speed of already delayed transactions, but they
remain profitable to include.

The dynamics here is simple, a miner is incentivized to process a
transaction with an expedience fee before a standard fee of the same
value-per-bit in order to not reduce the total value of the available fees
of all standing transactions they can process. The longer they wait, the
less total fees available.

Sidenote: a steady stream of expedience fees reduces the profitability of
block withholding attacks (!), at some threshold it should make it entirely
unprofitable vs standard mining. This is due to the increased risk of
losing valuable expedience fees added after you finished your first block
(as the available value will be reduced in your block #2, vs what other
miners can claim while still mining on that previous block).
(Can somebody verify this with simulations?)

---

Fee pool. Softfork compatible.

We want to smooth out fee payments too for the future when the subsidy
drops, to prevent deliberate forking to steal fees. We can introduce a
designated P2SH anyone-can-spend fee pool address. The miner can never
claim the full fees from his block or claim the full amount in the pool,
only some percentage of both. The remainder goes back into the pool (this
might be done at the end of the same expedience fee processing transaction
described above). Anybody can deliberately pay to the pool.

The fee pool is intended to act as a "buffer" such that it remains
profitable to not try to steal fees but to just mine normally, even during
relatively extreme fee value variance (consider the end of a big
international shopping weekend).

The fee value claimed by the miners between blocks is allowed to vary, but
we want to avoid order-of-magnitude size variation (10x). We do however
want the effect of expedience fees to have an impact. Perhaps some
logarithmic function can smooth it out? Forcing larger fees to be
distributed over longer time periods?

---

Block size dependent difficulty scaling. Hardfork required.

Larger blocks means greater difficulty - but it doesn't scale linearly,
rather a little less than linearly. That means miners can take a penalty in
difficulty to claim a greater number of high fee transactions in the same
amount of time (effectively increasing "block size bitrate"), increasing
their profits. When such profitable fees aren't available, they have to
reduce block size.

In other words, the users literally pay miners to increase block size (or
don't pay, which reduces it).

(Sidenote: I am in favor of combining this with the idea of a 32 MB max
blocksize (or larger), with softforked scheduled lower size caps (perhaps
starting at 4 MB max) that grows according to a schedule. This reduces the
risk of rapidly increasing load before we have functional second layer
scaling in place.)

In order for a miner to profit from adding additional transactions, their
fees must exceed the calculated cost of the resulting difficulty penalty to
make it worth it to create a larger block. Such loads are expected during
international shopping weekends.
With only a few available high value transactions the incentive becomes the
reverse, to create a smaller block with lower difficulty to faster claim
those fees.

To keep the average 10 minute block rate and to let this mechanism shift
the "block size bitrate" as according to the fee justified block size
changes, we set an Expected blocksize value that changes over time, and we
change the difficulty target into the Standard difficulty target, where
each block must reach a Scaled difficulty target .

In terms of math we do something like this:
Scaled difficulty = Standard difficulty * f(blocksize), where f would
likely be some logarithmic function, and blocksize is defined in terms of
units of Expected blocksize (a block 1.5x larger than Expected blocksize
gets a value of 1.5).

When we retarget the Standard difficulty and Expected blocksize we do this:
Standard difficulty = Network hashrate per 10 minutes (approximately same
as before, but now we take the Scaled difficulty of the last period's
previous blocks into consideration)
Standard blocksize = Recent average effective block bitrate = (sum of
recent (weighted!) block sizes / length of timeperiod) / number of blocks
in a retargeting period.

Thus, generating larger blocks drives up the long term standard block
bitrate, smaller blocks reduces it, in both cases we strive to average 1
block per 10 minutes.

Combining this with expedience fees makes it even more effective;

There's always a cutoff for where a miner stops including unprocessed
transactions and let the rest remain for the next block. For standard fees,
this would result in a fairly static block size and transactions backlog.
With expedience fees your transaction can bypass standard fees with same
value-per-bit, as explained above, because otherwise the miners reduces the
value of their future expected fees. The more people that do this, the
greater incentive to not delay transactions and instead increase the
blocksize. (Can somebody help with the math here? I want simulations of
this.)

(Sidenote: I'm in favor of RBF, replace-by-fee. This makes the above work
much more smoothly. Anybody relying on the security of unconfirmed
transactions for any significant value *have to* rely on some kind of
incentive protected multisignature transaction, including LN type second
layer schemes. The other option is just not secure.)

If load is low then you can add a high expedience fee to incentivize the
creation of a smaller block with your transaction, since difficulty will be
reduced for the smaller block. This means the miner has a higher chance of
beating the competition. Adding additional lower fee transactions may
reduce his average value-per-bit to become less profitable.

Miners simply aim to maximize their fees-per-bit, while also paying as
little as possible in mining costs.

To make this work as intended for those willing to explicitly pay to reduce
block size, one could tag such an expedience fee with a maximum allowed
blocksize (where the fee will be claimed in such a smaller block if it is
the more profitable option), such that it won't be countered by others
making more high expedience fees to increase blocksize. Note: I'm not
particularly in favor of this idea, just mentioning the possibility.
-------------------------------------
Humans are very visually oriented, recognizing differences in images more
easily than differences in text.

What about generating an image based on the bytes of an address, using
something like identicon, used by gravatar? Any small change to the text
input produces a significantly different image.

-Danny

On Oct 30, 2017 7:43 AM, "Moral Agent via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Erik,

I completely agree that it will be in the long term interest of bitcoin to migrate, gradually, toward a commoditized POW away from the current mass centralization. There is a big problem here though: Hundreds of millions of dollars have been spent on the current algorithm, and will be a huge loss if this is not done slowly enough, and the miners who control the chain currently would likely never allow this change to happen.

Do you have any ideas regarding how to mitigate the damage of such a change for the invested parties? Or even how we can make the change agreeable for them?

Warm regards,
Garrett

--
Garrett MacDonald
+1 720 515 2248
g@cognitive.ch
GPG Key

On Apr 9, 2017, 2:16 PM -0600, Erik Aronesty via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>, wrote:
-------------------------------------
Hi,

I've moved the bitcoin-dev list to bcc:, as this question is better suited
to forums dedicated to Bitcoin Core implementation specifics, rather than
the general bitcoin development list.

Please feel free in the future to ask questions like this on the
bitcoin-core-dev mailing list (https://lists.linuxfoundation
.org/mailman/listinfo/bitcoin-core-dev) or on the #bitcoin-core-dev
freenode IRC channel.

The work limit (that was put in place in https://github.com/bitcoin/
bitcoin/pull/6654, when the concept of "dirty" entries was introduced) was
removed in https://github.com/bitcoin/bitcoin/pull/7594, in preparation for
ancestor-feerate-mining.  So those comments should have been cleaned up to
match the new code.

Please feel free to file an issue or open a PR to update those comments at
https://github.com/bitcoin/bitcoin.

Thanks,
Suhas


On Mon, May 8, 2017 at 5:38 AM, DJ Bitcoin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thu, Feb 23, 2017 at 03:13:43PM -0800, Bram Cohen wrote:

In what way do you see MMRs as redundant?

Remember that with UTXO commitments because access patterns are uniform, you'll
over time have a lot more "redundancy" in the form of lost-coins evenly spread
out across the whole keyspace.


That statement is incorrect with pruning: you can maintain a commitment to the
TXO set, without actually storing the entire TXO set, because you don't need to
store anything for nodes that have already been spent.

Concretely, this can be done with nothing more than adding a FullySpent node
type to the MMR definition I published earlier, with the rule being that only a
left or right child of an inner node be a FullySpent node, not both; if both
sides are spent, the inner node itself becomes FullySpent. Equally, I think you
can re-use the Empty node for this, but I need to think a little about the
implications re: partial inner nodes.

Regardless, with a generalized commitment scheme, the serialization/commitment
to an Empty node is simply '0', the encoding of an unspent txout surrounded by
spent txouts will be similar in size to a position integer followed by the
txout...


A subtlety of this construction is that you can only prove that a specific
txout # is unspent, but that's actually sufficient, as you can also prove what
# a txout txid corresponds too with a previous version of the MMR.


Well, I think at this point there's still discussion over whether or not a UTXO
set commitment is the right approach to begin with; if it's not your
implementation isn't relevant.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------


I do agree that solutions like `SIGHASH_BLOCKCOMMIT` are superior in the sense that they are very difficult to circumvent. However, a fork could also follow the original chain in SPV mode and allow transactions protected with these mechanism. Since it's fundamentally impossible to disallow transactions in future projects, the goal shouldn't be to make this overly complicated.

Furthermore, this schema is not just adding replay protection. It makes transacting safer overall (due to a dedicated address format per fork) and allows light clients to differentiate between multiple forks. In the past three months, at least $600k has been lost by users sending BCH to a BTC address [1].


Whether the transaction is replay protected or not is specified by setting a bit in the `SigHashId`. If this bit is set, then the signature *preimage* MUST have `nForkId` appended. `nForkId` is not part of the final transaction, someone who wants to verify the transaction must know which `nForkId` it was created with.

If the bit isn't set, it means `nForkId=0`, which allows other forks to validate the signature.


Sorry, I was careless with the use of `>=` there. You are correct, forks form a tree. For this proposal, every leaf must be assigned a unique `nForkId`. The relationship between `nForkId` is irrelevant (e.g. which number is bigger), as long as they are unique. Transactions are only valid IFF `nForkId` matches exactly the `nForkId` of the software validating it. As described above, the transaction doesn't even contain `nForkId`, and the node surely is not starting to guess which one it could be.

[1]
https://twitter.com/khannib/status/930223617744437253 <https://twitter.com/khannib/status/930223617744437253>
-------------------------------------
On 27.01.2017 20:03, Luke Dashjr via bitcoin-dev wrote:
Lets think like people in sales and marketing for a moment.

There's an implicit assumption here that ANY protocol or consensus-rule 
based solution exists that would reverse the trend of diminishing full 
node verified economic activity. Since there's no economic advantage to 
running a full node, there's no inherent motivation for implementation 
(or outright purchase) of full nodes by the very large percentage of 
people who fall in the non-technical "I just want it to work, and I 
don't want my money stolen" category. Yes, anyone on this list 
understands that "don't want my money stolen" is inherently connected to 
running your own node and using it for transactions, but the average 
user does not, and even if they did, they don't have the resources (time 
and/or money) to do anything about it. Running your own full node 
increases the protection agains double spend attacks and other protocol 
bases shenanigans, but now you've taken on another set of security 
exposures related to the physical box that is running the node. 
Anti-virus, off and on-site backups, multiple boxes/devices for 
multi-sig, backup of key seeds.

Reducing (or even maintaining) the block size doesn't somehow increase 
the number of people who are capable of running full nodes, and it 
doesn't add any incentive for people already in that "capable" set to 
suddenly take up the task of running and transacting via a full node. 
I'd argue that the size of the block-chain and the time to download it 
are the least concerning aspects to anyone faced with running their own 
node and actually storing some of their wealth on it and using it for 
transactions.

You're looking for a (maybe dangerous/maybe impossible) balance between 
choking off casual (not full node) usage of bitcoin and yet trying to 
make it more popular among the people (and organizations) who have the 
capability and resources to run and transact on full nodes.

We should sit with this for a moment.

On one hand, Bitcoin may ultimately end up as digital currency "only for 
geeks and B2B transactions." I'd speculate we'd loose a big subset of 
the geeks that way too, unless they happen to do a lot of transactions 
with medium to large size businesses. (Small businesses won't be able to 
afford the expense of or the time to maintain the node.) There's some 
level of risk that this pushes bitcoin into oblivion. And is it really a 
decentralized P2P currency if it's only used by medium and large 
businesses and a small set of technically capable individuals that 
transact with those entities directly in BTC? And is it really a 
decentralized currency in this scenario if its used mainly by medium and 
large businesses, banks, and exchanges? (I've purposely excluded small 
businesses because while they like the benefits of flexible payment 
systems, more don't have the time or skill (or resources to hire the 
skill) needed to do a full node implementation.)

I feel inherent cognitive dissonance between "keep it decentralized" and 
"not useful to small business and individuals." One can make the 
argument that L2 solutions will be available for the small businesses 
and individuals but that doesn't solve the stated intent of reversing 
the trend of transactions not originating from or being received by full 
nodes. I guess you're saying bitcoin will be stronger, more resistant to 
outside power agency and censorship if its only used by exchanges, 
banks, large businesses, and die-hard technically inclined people.


On the other hand, maybe there's a scenario where an average person 
walks into a big box electronics store in any developed country and buys 
a "personal digital bank" appliance. I frame it this way because the 
majority of the worlds population is never going to run a full node on 
their desktop or laptop. There's no viable scenario where that happens. 
Laptops and desktops are already diminishing in market share due to the 
introduction of tablets and smartphones. General purpose OS's are also 
inherently un-secure, so  going down this route means we are immediately 
in the realm of lots of theft. Preventing theft (or loss due to errors) 
requires additional digital key devices, or additional devices for 
multi-sig transactions just for basic financial safety, not to mention a 
functioning backup plan, including off-site backups. 
Ransomeware/phishing protection? Checking email and surfing the web on 
the computer that holds your standard (non-multi-sig) wallet? 
Forgetaboutit. It'll never reach critical mass. It's not a viable 
proposal. Not to mention, you can't physically carry your laptop with 
you when you go to the shopping mall. In order for this appliance model 
to function, smartphone based implementations will need to interact with 
your personal or family server/appliance, and you'll need to be able to 
do multi-sig with a smartphone and another physical token you carry with 
you. Imagine a 2 of 5 multi-sig wallet where your phone and an NFC or LE 
bluetooth device are sufficient to create a transaction on your home 
node while shopping. Or your phone has a single sig wallet and you top 
it up from your appliance and it never has a high balance. In any case, 
I've made the argument before that the definition of "bitcoin protocol" 
should, in addition to the consensus protocol, probably include a secure 
API protocol between wallet client and full node, and it still seems to 
be an important missing piece. I want to be able to travel and spend BTC 
and I DON'T want to do general purpose computing like email and web 
surfing on the same computer where I have a big chunk of life savings 
stored! I think defining this API will actually really support the use 
of user controlled full nodes for transactions! Imagine Trezor owners 
using their own node for transactions! Bitpay is the only player I know 
of that provides enough of a software stack to set this up for yourself.

I think reversing the non-full node transaction trend will have to be 
based the appliance usage model. You buy a new 200-500Gb nvme SSD every 
year and put it in one of the free slots. You upgrade when all slots are 
full. This is one scenario that could put us on a trend of increasing 
transactions originating and being received by personal full nodes, i.e. 
reversing centralization trends.


If there is any solution to this problem, it will need to recognize the 
fact that the supermajority of people on the planet are not technically 
savvy nor are they inclined to take the time to learn how to protect 
themselves with basic computer security much less how to use a full node 
for bitcoin transactions. The solution, if it exists, will need to be 
handed to them, and they'll need a reason to buy it. Any solution will 
also need to recognize the fact that it will cost resources (time and 
money) to run a full node. Lots of people spend a huge portion of their 
income just to get a smartphone because it's a useful communication 
device that does lots of other useful things. There's not nearly the 
same level of need to spend on a full node for bitcoin security.

Any solution to this problem should also recognize the fact that there's 
a significant amount of work to do to have a functioning personal 
implementation of a node and to use it for transactions. Even in my 
imagined future of polished and easy to use appliances, if you have 
enough capital in BTC that you need it and you can afford to buy it, 
you're now only starting to deal with implementation issues. You've now 
become your own bank. Now you have to secure that appliance physically, 
secure and back up the key seed material, secure the devices used to 
access it, connect an app on your smartphone to the appliance so you can 
create transactions while out of your home, connect your home 
computer(s) to the appliance, do key exchange with the app/PC and the 
appliance or implement some sort of PKI on all devices. You've just 
taken on the responsibility of a bank and a sysadmin! The higher the 
balance, the more of a target you are, and the more time/money you have 
to spend mitigating risk. This is a huge centralizing force that no one 
really seems to talk about. If you're the average person, you want to 
find a trustworthy company or trusted friend/family to take care of that 
stuff for you. If you're a technically inclined person AND maybe there's 
a way to reap some of the mining reward on a small scale, you're 
slightly more interested.

As a sysadmin for many years, I've seen first hand that most people want 
tools that just work, whether its software to make spreadsheets, 
operating systems, phones, or thermostats. My point here is that the 
number of people in the world who have the technical chops to run a node 
is ALWAYS going to be vastly lower than the number of people who will be 
using bitcoin (or cryptocurrency).

Of course we can make the argument that the definition of "bitcoin" is 
by design something to be used exclusively by institutions and geeks, 
and that this definition falls out of the necessity to ensure that it 
remains decentralized and censorship resistant. However, I'm not sure 
that logic holds or that it doesn't introduce risk that that sort of 
definition drives bitcoin toward diminished relevance.

At the end of all this though experiment, I'm still convinced that if 
the tools are built to enable flexible usage of full nodes (i.e. my 
phone, tablet or desktop app interfaces with the full node) then there's 
a large potential for increased usage of full nodes.

Thanks,
G

-------------------------------------

I'd just like to point out, invalidating asicboost has only a very
limited number of potential detractors.  Only a mining farm that
self-mined and used custom software would be able to exploit this.
Every other mining farm on the planet, plus any users wishing for more
transactions to be included in blocks would be in favor of this,
assuming the theory that it favors fewer transactions is correct.
That makes it less contentious than many other alternatives.  It might
even force the mining operation(s) in question to flip and support SW
in order to avoid losing face and/or appearing guilty.

As an additional plus, nearly all of the BU crowd and most BU
supporting miners would have little reason to object to Asicboost -
Based on philosophy alone, but not based on any practical
considerations.

Jared

On Wed, Apr 5, 2017 at 8:23 PM, David Vorick via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Praxelogy_guy,
Yes I understand that segwit2mb represents a "potential" 4Mb block size
increase.
But Segwit does not immediately lead to 2 Mb blocks, but can only achieve
close to a 2Mb increase if all Bitcoin wallets switch to segwit, which will
take a couple of years.
Therefore I don't expect transactions per block to quadruple from one day
to another.


On Fri, Mar 31, 2017 at 6:22 PM, praxeology_guy <
praxeology_guy@protonmail.com> wrote:

-------------------------------------
The main issue that I see with this proposal is that miners can still spam
the network for free even with high sat/byte fee levels. They can first
choose the sat/byte rate that maximize their profit, and then include a lot
of spam transactions at that rate that will only pay fees to themselves,
effectively spamming the chain for free and increasing the cost of running
a node.

On 30 Nov 2017 03:40, "Ben Kloester via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Something similar to this has been proposed  in this article by Ron Lavi,
Or Sattath, and Aviv Zohar, and discussed in this bitcoin-dev thread
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-September/
015093.html

They only discussed changing the fee structure, not removing the block size
limit, as far as I know.

    "Redesigning Bitcoin's fee market"
    https://arxiv.org/abs/1709.08881



*Ben Kloester*

On 30 November 2017 at 11:47, William Morriss via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Hmm, that's not the difference I was talking about. I was referring to the fact that using "post-chainsplit coinbases from the non-148 chain" to unilaterally (ie. can be done without action on the 148-chain) taint coins is more secure in extreme-adverserial cases such as secret-mining reorg attacks (as unfeasibly expensive they may be); the only large-scale (>100 block) reorganization the non-148 chain faces should be a resolution of the chainsplit and therefore render the replay threat moot.

Sent with [ProtonMail](https://protonmail.com) Secure Email.

-------- Original Message --------
Subject: Re: [bitcoin-dev] Replay attacks make BIP148 and BIP149 untennable
Local Time: June 7, 2017 3:04 AM
UTC Time: June 7, 2017 12:04 AM
From: contact@taoeffect.com
To: Kekcoin <kekcoin@protonmail.com>
Anthony Towns <aj@erisian.com.au>, bitcoin-dev@lists.linuxfoundation.org <bitcoin-dev@lists.linuxfoundation.org>

You keep referring to 148 coinbase coins, what is the rationale behind this? Why would you prefer using 148 coinbases over legacy coinbases for this purpose?

OK, maybe "post-UASF coinbase coins" is a better term? I just wanted to make it clear that this refers to coins that come from blocks generated after the UASF is activated.

--
Please do not email me anything that you are not comfortable also sharing with the NSA.

On Jun 6, 2017, at 4:59 PM, Kekcoin <kekcoin@protonmail.com> wrote:

You keep referring to 148 coinbase coins, what is the rationale behind this? Why would you prefer using 148 coinbases over legacy coinbases for this purpose?

Sent with [ProtonMail](https://protonmail.com/) Secure Email.
-------------------------------------
Good morning,


Indeed, this is the reason why CLTV and CSV do not pop off their parameters when executed, and require a subsequent OP_DROP. I suggest, that OP_BRIBE should not manipulate stack (pop, then push 0/1); my understanding is that this requirement is necessary for compatibility with old nodes, which will not manipulate stack on OP_NOP4. Instead, OP_BRIBE should imitate CLTV and CSV code, and raise an error in script execution if the check fails.


Regarding "largest chain", do you mean mainchain or sidechain?

An OP_RETURN is still some guarantee that it will make it into the longest mainchain. If OP_RETURN tx is in a shorter mainchain but not on the longer mainchain, then on the longer mainchain, the utxo's funding the OP_RETURN tx is still unspent and the OP_RETURN tx will still be mineable by any miner following the longer mainchain. The X BTC would be the OP_RETURN transaction's fee, which Mary would still want to mine into the longest mainchain, as it is still money on the table if it is not mined on the longest mainchain.

Or, does OP_BRIBE somehow assure that Sam's block goes onto the longer sidechain? But then, do not side blocks refer to their previous side block to define the sidechain?


I see.

Is there some predictable schedule for side->main withdrawals? If a withdrawal is imminent, or if some actor can get "insider information" about whether a withdrawal is imminent, cannot some actor induce the above, with potentially shorter time to reach step 3?

From my reading, Blockstream's sidechains proposal supports a reorg proof after a side->main withdrawal on the mainchain side, with a reorg proof burn window after the main:side->main withdrawal, preventing its utxo from being used. If the reorg proof is published and shows that a sidechain reorg invalidates a particular side->main withdrawal, then the main:side->main withdrawal's utxo is burned.


Do you have some document containing these details? I cannot find this in the blog posts I've read so far.


As above, do you have document containing what data mainchain needs to track?


I endorse this on the basis of Greg Maxwell's analysis that a block size limit is necessary to have a fee market.


Can you provide the details of this mechanism? For example, does h* actually include some information identifying the sidechain and OP_BRIBE is supposed to do some additional checking not shown in your current code, or ....?


Oh.

My understanding is that with Blockstream's zk-SNARKs, a new sidechain would not require a soft fork at all (or even miner voting on the validity of WT^: the validity of side:side->main transactions is assured by proof that the zk-SNARK checking that transaction was executed correctly, and the lack of a reorg proof during the burn window after the main:side->main).

Is your model then, that each sidechain maintainer has to maintain a patchset or some plugin system to Core? And miners who want to support particular sidechains to modify their software, applying the patch for each sidechain they want to support?

It seems this is somewhat brittle and may cause sidechain coding problems to leak into mainchain.

I think, it is much less interesting to have to softfork in every sidechain, rather than to support a general mechanism (zk-SNARK) to allow sidechains to be launched without any modification to Core code.


It seems to be, more of "completely sighted merged mining" than "blind merge mining".


I find this point now moot, as drivechains require a softfork for each sidechain, and the size of the datacenter is pointless if there is some need to softfork in every sidechain.

Regards,
ZmnXCPxj
-------------------------------------
The fast hash for internal nodes needs to use an IV that is not the
standard SHA-256 IV. Instead needs to use some other fixed value, which
should itself be the SHA-256 hash of some fixed string (e.g. the string
"BIP ???" or "Fash SHA-256").

As it stands, I believe someone can claim a leaf node as an internal node
by creating a proof that provides a phony right-hand branch claiming to
have hash 0x80000..0000100 (which is really the padding value for the
second half of a double SHA-256 hash).

(I was schooled by Peter Todd by a similar issue in the past.)

On Wed, Sep 6, 2017 at 8:38 PM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

Ilansky wrote:

I generally agree with ZmnSCPxj that
good ideas => good devs => hodlers => price => mining

Except that each step is not an absolute, and can be biased by things
like miners who seek profit via fees and other means that are not good
for everyone else. Llansky's belief itself influences price away from
the ideal. Marketing "easy profits for hodlers!" and first-to-market
monopoly are other elements that influence price and thereby guide
mining away from good ideas (like a constant value currency). Then
price pulls in good devs that pulls in more mining. So it can snowball
into a monster.

We need not debate cause and effect since it's distant from the list's
goals. The relevance to me is that the biases away from ZmnSCPxj's
ideal are a reason a more responsive difficulty is needed.

Mining is for determining truth of the blockchain, not to make sure
there is only 1 blockchain. ZmnSCPxj indicates we should not do
anything that has more precision or speed in determining the correct
difficulty if it reduces Bitcoin's ability to be a monopoly. Not
coincidentally, the monopoly helps ensure hodlers become the new 1%. A
fork clone that uses the faster difficulty would attack BTC's slow
difficulty if it achieves a comparable price. All other things being
equal, it would lower BTC's value until it forks to fix the
difficulty.

On Fri, Oct 13, 2017 at 8:27 AM, Ilan Oh via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------


The lack of signed maximum segwit stack size was one of the objections to
segwit I presented last year. This together with the unlimited segwit stack
size.

However, committing to the maximum stack size (in bytes) for an input is
tricky. The only place where this could be packed is in sequence_no, with a
soft-fork. E.g. when transaction version is 2 and and only when lock_time
is zero.

For transactions with locktime >0, we could soft-fork so transactions add a
last zero-satoshi output whose scriptPub contains OP_RETURN and followed by
N VarInts, containing the maximum stack size of each input.
Normally, for a 400 byte, 2-input transaction, this will add 11 bytes, or a
2.5% overhead.








-------------------------------------
On 06/19/2017 05:49 PM, Jonas Schnelli via bitcoin-dev wrote:


Another number: I'm answering dozens of support inquiries about
delayed/missing transactions per day. Over the 7 years of Bitcoin
Wallet's existence, I estimate about 50000 inquiries.

On the other hand, I remember only 1 (one) inquiry about the privacy
problems of BIP37 (or privacy at all).

everyone would take it for free, but certainly not if it a) delays
incoming payments or b) quickly eats up your traffic quota.


-------------------------------------
Hi everyone,

I would like to propose a standard format for unsigned and partially signed
transactions.

===Abstract===

This document proposes a binary transaction format which contains the
information
necessary for a signer to produce signatures for the transaction and holds
the
signatures for an input while the input does not have a complete set of
signatures.
The signer can be offline as all necessary information will be provided in
the
transaction.

===Motivation===

Creating unsigned or partially signed transactions to be passed around to
multiple
signers is currently implementation dependent, making it hard for people
who use
different wallet software from being able to easily do so. One of the goals
of this
document is to create a standard and extensible format that can be used
between clients to allow
people to pass around the same transaction to sign and combine their
signatures. The
format is also designed to be easily extended for future use which is
harder to do
with existing transaction formats.

Signing transactions also requires users to have access to the UTXOs being
spent. This transaction
format will allow offline signers such as air-gapped wallets and hardware
wallets
to be able to sign transactions without needing direct access to the UTXO
set and without
risk of being defrauded.

The full text can be found here:
https://github.com/achow101/bips/blob/bip-psbt/bip-psbt.mediawiki

Andrew Chow
-------------------------------------
(sorry, I forgot to reply-all earlier)

The very short answer to this question is that I plan on using Luke's
fail-success-on-unknown-operation in Simplicity.  This is something that
isn't detailed at all in the paper.

The plan is that discounted jets will be explicitly labeled as jets in the
commitment.  If you can provide a Merkle path from the root to a node that
is an explicit jet, but that jet isn't among the finite number of known
discounted jets, then the script is automatically successful (making it
anyone-can-spend).  When new jets are wanted they can be soft-forked into
the protocol (for example if we get a suitable quantum-resistant digital
signature scheme) and the list of known discounted jets grows.  Old nodes
get a merkle path to the new jet, which they view as an unknown jet, and
allow the transaction as a anyone-can-spend transaction.  New nodes see a
regular Simplicity redemption.  (I haven't worked out the details of how
the P2P protocol will negotiate with old nodes, but I don't forsee any
problems.)

Note that this implies that you should never participate in any Simplicity
contract where you don't get access to the entire source code of all
branches to check that it doesn't have an unknown jet.

On Mon, Oct 30, 2017 at 5:42 PM, Matt Corallo <lf-lists@mattcorallo.com>
wrote:

-------------------------------------
I came across the proposed Bitcoin Core implementation of BIP159 [0] in this PR [1]. The goal is to allow pruned nodes to "serve a limited number of historical blocks" (as opposed to none at all).

It contains a counter-measure for peer fingerprinting. I'm trying to understand how that impacts extendibility.


This means pruned nodes can only serve the last 288 blocks:


As the blockchain keeps growing there will be ever more pruned nodes (perhaps offset by new nodes with more storage).  Although a strict improvement over todays situation, it seems a bit wasteful to have a node with 10-100 GB of storage only be able to share the most recent 288 blocks.

It would be nice if a future extension of this BIP allows more flexibility. To limit the ability to fingerprint nodes, we could limit the number of choices to e.g. 288 + 1000 * 2^n. That yields only 8 possibilities at the current chain size. A slightly better formula could take into account typical hard drive size increments, leaving enough space for the OS and other data. Node operators could opt-in to this if they think the increased fingerprint risk outweighs their desire to share archived blocks.

I can also imagine - but not implement :-) - a future scenario where nodes prune a random subset of their chain, meaning that even nodes with little storage can be of help during Initial Blockchain Download (IBD) of other nodes.


How would such extension be signaled for? Would we need a whole new version bit?

Would upgraded nodes need a new message type to communicate the chosen prune depth? Or can that information tag along some existing message?

Jonas Schnelli pointed out on the Github discussion that waiting for BIP150 would be appropriate. Can you explain how this is related? Although I can see why whitelisted peers can be exempted from the anti-fingerprinting measure, I would not want to restrict it to just those.


Some minor suggestions for improving the BIP itself:
* add link to mailinglist discussion(s) in reference section
* explain that 288 is not just the minimum limit for Bitcoin Core, but also the bulk of traffic (as I understand from earlier discussion [2])

Cheers,

Sjors

[0] https://github.com/bitcoin/bips/blob/master/bip-0159.mediawiki
[1] https://github.com/bitcoin/bitcoin/pull/10387
[2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/thread.html#14315
-------------------------------------
Hi Colin


Would you then assume that userWalletPubKey is a hot key (stored on the users computer eventually in a browser based local storage container)?
In case of an attack on the server responsible for serverWalletPubKey (where also the personal information of the user are stored [including the xpub == amount of funds hold by the user)), wound’t this increase the users risk of being an possible target (False sense of multisig security, comparing to cold storage / HWW keys)?


I guess this will result in protecting the funds stored in this transaction entirely on the users identity information and eventually the optional recovery password, though I guess you are adding additional security by protecting via the server nonce from brute-forcing.

Why 1025bit for the nonce?
Why SHA512 instead of SHA256 (I guess you need 256bit symmetric key material for the key encryption)?
Considered using a (H)KDF for deriving the symmetric key (even if the server based nonce reduces the possibility of brute-forcing)?

Your modal has probably the TORS (trust on recovery setup) weakness (compared to a HWW where you [should] be protected on compromised systems during private key creation).

</jonas>
-------------------------------------
You need a majority of miners enforcing BIP148 upon BIP148 activation
to prevent a split, not just a majority signalling segwit. This
provides a miner coordination mechanism for BIP148 mandatory
signalling enforcement.

On Tue, Jun 6, 2017 at 8:11 PM, Karl Johan Alm
<karljohan-alm@garage.co.jp> wrote:

-------------------------------------
varies depending on the context (95%, 75%, 51%). Nowhere does it say
"everyone needs to agree".

There's a pretty huge gap between 90% and nearly 100%.  90% excluding 10%
only 7 times results in only 48% of the original base.

is no longer interesting.

Your definition of forward may be different than other users.


Yes, I chose Bitcoin because it relies on a strictly held consensus
mechanism and not one that changes on the whims of the majority.  We have
tens of dozens of political currencies for that.

increase it when we needed it.  The white paper even talks about scaling to
huge capacity.  Not sure where you got the idea that we all agreed to stay
at 1MB forever, I certainly didn't.  It was never stated or implied that we
could change the coin cap later(please cite if I'm mistaken).

The community has not agreed that it is needed at this time.  Perhaps they
will change their mind at some point in the future.  We have also learned a
great deal since the publication of the initial whitepaper, such as the
unstable state without a backlog or subsidy.  Fortunately, participation in
this system is voluntary, and you are free to leave at any time.

This seems to be venturing quite off topic, and perhaps would be better
suited for the bitcoin-discuss list.

On Wed, Feb 8, 2017 at 1:56 PM, Andrew Johnson <andrew.johnson83@gmail.com>
wrote:

-------------------------------------
Hi Greg,

On Wed, Apr 05, 2017 at 09:37:45PM +0000, Gregory Maxwell via bitcoin-dev wrote:

Decentralized systems without patent encumbrance is an important topic
for me. We'd be very interested in adding this into extension blocks.

Claims like these merit serious attention. If you can provide any kind
of proof or documentation of this (doesn't need to be conclusive, just
something), I will provide my word and promise publicly here and now
that I will personally see to it that a commitment which solves this
(albeit possibly using a slightly different format to make it
compatible) is added into the Extension Blocks spec. If there is
evidence, my support and authorship of the Extension Block specification
is contingent upon resolving this issue.

We have added an issue here:
https://github.com/tothemoon-org/extension-blocks/issues/6

I'm interested in a more detailed explanation on how the Merle tree
structure works so we can add it to the spec, I didn't follow exactly
the new consensus rule and its mechanism in those several lines.

We will begin making a pull request adding it into our specification,
but more clarity on how to do it on its own would be helpful. We will
also consider the code exposure change to adding in SegWit on the
Canonical/1MB chain if it is more elegant to implement.

Packaging this into our proposal would not only be important, but
helpful to the end goals of this proposal as it becomes a standard
soft-fork consensus rule which has greater guarantees around
enforcibility than user-actication.

Further, can you provide clarity and confirmation into why this
commitment wasn't required as part of SegWit? 

-- 
Joseph Poon

-------------------------------------
On Tue, Mar 28, 2017 at 9:59 AM, Wang Chun via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Much as it may be appealing to repeal the block size limit now with a grace
period until a replacement is needed in a repeal and replace strategy, it's
dubious to assume that an idea can be agreed upon later when it can't be
agreed upon now. Trying to put a time limit on it runs into the possibility
that you'll find that whatever reasons there were for not having general
agreement on a new setup before still apply, and running into the
embarrassing situation of winding up sticking with the status quo after
much sturm and drang.
-------------------------------------
I shamefully was not aware. However familiarized myself with them.

Non official chains suffer from the fact that few if any miners are going
to mine them so they lack security on par with the main chain. And more
over most users aren't going to use them because its not magic.

That being said think they are and will always be a great place to develop
and prove out concepts.

If my ultimate goal is official side chains that include part of the reward
such security is at parity between all chains and that the official
software automatically enable users to distribute their burden, would my
course of action be to build an external proof-of-concept side chain of
side chains?
or do you doubt that official reward splitting chains will ever find their
way into bitcoin core?

On Mon, Sep 25, 2017 at 4:58 PM, CryptAxe <cryptaxe@gmail.com> wrote:

-------------------------------------
On Sat, Apr 15, 2017 at 1:42 PM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I do not follow the argument that a critical design feature of a particular
"user activated soft fork" could be that it is users don't need to be
involved.  If the goal is user activation I would think that the
expectation would be that the overwhelming majority of users would be
upgrading to do it, if that isn't the case, then it isn't really a user
activated softfork-- it's something else.



So it has to be supported by the public but I can't say why I don't support
it? This seems extremely suspect to me.
-------------------------------------
On 8 Apr 2017 8:31 pm, "praxeology_guy via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

There is the equation:
Power Cost + Captial Rent + Labor ~= block reward + fees


I don't know why many people insist on calling the subsidy the blick
reward. Thw block reward is both the block subsidy plus the block fees.
-------------------------------------
Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
writes:

Agreed, I would suggest 16th December, 2017 (otherwise, it should be
16th January 2018; during EOY holidays seems a bad idea).

This means this whole debacle has delayed segwit exactly 1 (2) month(s)
beyond what we'd have if it used BIP8 in the first place.

Cheers,
Rusty.

-------------------------------------
On Wed, Sep 27, 2017 at 02:01:40PM -0500, Bryan Bishop via bitcoin-dev wrote:

As part of this, we may want to say that the BIP editor should
cryptographically sign (and ideally timestamp) all their changes as a secondary
measure to make it clear who actually made the change.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
Good morning Paul,

I read only http://www.truthcoin.info/blog/blind-merged-mining/

From just this document, I can't see a good justification for believing that a main->side locking transaction can be safely spent into a side->main unlocking transaction. Do you have a better explanation?

OP_is_h_in_coinbase, as described, does not seem to protect against a sidechain reorg in your next section of the document. If I attempt to spend a main->side locking transaction on the basis of a "mistaken" side block #49, what prevents me from this sequence:

1. Put a side:side->main transaction into a block together with TheDAO's hacked money.
2. Wait for a reorg to revert TheDAO.
3. Spend my now-free-in-the-reorg funds on Lightning Network to get mainchain funds.
4. Create a main:side->main transaction with the side:side->main transaction in the TheDAO-hacked block as witness.
5. Get another set of mainchain funds from the same sidechain funds.

So far, the only good side->main transfer I know of is in Blockstream's original sidechains paper, with the main:side->main transaction spending into a timelocked transaction that may be burned if a reorg proof is submitted (i.e. you try to create a main:side->main transaction with the side:side->main transaction in the mistaken #49 and #50 as your proof, but someone else can come along and show a corrected #49, #50, #51 without your side:side->main transaction and burn your funds). Is your proposal at the technical level actually similar, or does it truly seem to be riskier? It seems to me that your OP_is_h_in_coinbase should scan a series of sidechain block headers backed by mainchain (meaning at the minimum that sidechains should have some common header format prefix), rather than just mainchain depth as your article seems to imply.

Also, blinded merge mining seems strictly inferior to proof-of-burn: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-December/007012.html

Proof-of-burn integrates a lottery to reduce the ability of a mainchain-rich attacker to reorg the sidechain by burning its greater funds. However it still seems to me that a rich attacker can simply make more bets in that scheme by some trivial modification of the side block. Blind merged mining seems strictly inferior as a rich attacker can simply reorg the sidechain outright without playing such games.

Or is your proposal strictly for centralized sidechains, where only one entity creates side blocks? How does your proposal handle multiple side block creators on the same sidechain, with the possibility that chain splits occur?

Regarding your dig about people who dislike data centers, the main issue with miners blindly accepting sidechain commitments is that it violates "Don't trust, verify", not that allows datacenters to be slightly smaller by not including side:nodes.

Regards,
ZmnSCPxj

Sent with ProtonMail Secure Email.

-------- Original Message --------
Subject: [bitcoin-dev] Drivechain -- Request for Discussion
Local Time: May 22, 2017 6:17 AM
UTC Time: May 22, 2017 6:17 AM
From: bitcoin-dev@lists.linuxfoundation.org
To: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>

Dear list,

I've been working on "drivechain", a sidechain enabling technology, for
some time.

* The technical info site is here: www.drivechain.info
* The changes to Bitcoin are here:
https://github.com/drivechain-project/bitcoin/tree/mainchainBMM
* A Blank sidechain template is here:
https://github.com/drivechain-project/bitcoin/tree/sidechainBMM

As many of you know, I've been seeking feedback in person, at various
conferences and meetups over the past year, most prominently Scaling
Milan. And I intend to continue to seek feedback at Consensus2017 this
week, so if you are in NYC please just walk up and start talking to me!

But I also wanted to ask the list for feedback. Initially, I was
hesitant because I try not to consume reviewers' scarce time until the
author has put in a serious effort. However, I may have waiting too
long, as today it is actually quite close to a working release.

Scaling Implications
---------------------

This upgrade would have significant scaling implications. Since it is
the case that sidechains can be added by soft fork, and since each of
these chains will have its own blockspace, this theoretically removes
the blocksize limit from "the Bitcoin system" (if one includes
sidechains as part of such a system). People who want a LargeBlock
bitcoin can just move their BTC over to such a network [1], and their
txns will have no longer have an impact on "Bitcoin Core". Thus, even
though this upgrade does not actually increase "scalability" per se, it
may in fact put an end to the scalability debate...forever.

This work includes the relatively new concept of "Blind Merged Mining"
[2] which I developed in January to allow SHA256^2 miners to merge-mine
these "drivechains", even if these miners aren't running the actual
sidechain software. The goal is to prevent sidechains from affecting the
levelness of the mining "playing field". BMM is conceptually similar to
ZooKeeV [3] which Peter Todd sketched out in mid-2013. BMM is not
required for drivechain, but it would address some of the last remaining
concerns.

Total Transaction Fees in the Far Future
-----------------------------------------

Some people feel that a maximum blocksize limit is needed to ensure that
future total equilibrium transaction fees are non-negligible. I
presented [4] on why I don't agree, 8 months ago. The reviewers I spoke
to over the last year have stopped bringing this complaint up, but I am
not sure everyone feels that way.

Juxtaposition with a recent "Scaling Compromise"
-------------------------------------------------

Recently, a scalability proposal began to circulate on social media. As
far as I could tell, it goes something like "immediately activate
SegWit, and then HF to double the nonwitness blockspace to 2MB within 12
months". But such a proposal is quite meager, compared to a "LargeBlock
Drivechain". The drivechain is better on both fronts, as it would not
require a hardfork, and could *almost immediately* add _any_ amount of
extra blockspace (specifically, I might expect a BIP101-like LargeBlock
chain that has an 8 MB maxblocksize, which doubles every two years).

In other words, I don't know why anyone would support that proposal over
mine. The only reasons would be either ignorance (ie, unfamiliarity with
drivechain) or because there are still nagging unspoken complaints about
drivechain which I apparently need to hear and address.

Other Thoughts
---------------

Unfortunately, anyone who worked on the "first generation" of sidechain
technology (the skiplist) or the "second generation" (federated /
Liquid), will find that this is very different.

I will admit that I am very pessimistic about any conversation that
involves scalability. It is often said that "talking politics lowers
your IQ by 25 points". Bitcoin scalability conversations seem to drain
50 points. (Instead of conversing, I think people should quietly work on
whatever they are passionate about until their problem either is solved,
or it goes away for some other reason, or until we all agree to just
stop talking about it.)

Cheers,
Paul

[1] http://www.drivechain.info/faq/#can-sidechains-really-help-with-scaling
[2] http://www.truthcoin.info/blog/blind-merged-mining/
[3] https://s3.amazonaws.com/peter.todd/bitcoin-wizards-13-10-17.log
[4]
https://www.youtube.com/watch?v=YErLEuOi3xU&list=PLw8-6ARlyVciNjgS_NFhAu-qt7HPf_dtg&index=4

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Ryan Grant,

TLDR Unless I'm missing something, your claim that a misconfiguration would result in a stop chain is wrong because BIP9 only works on soft forks.

Does BIP9 work with hard forks? Pretty sure it is only for soft forks. If you want to make a hard fork, there is not much point in waiting for any particular miner hash power adoption rate.

With a softfork, here is the only condition for a "stopped chain":
1. User adopts more stringent rules.
2. Someone maliciously creates an invalid block as evaluated by the more stringent rules in #1, but that is valid to older nodes
3. No one ever mines a different block at the height of the block in #2, instead all of the miners only build on top of the block built at #2.

The user would have to adopt a soft fork at a time where no miner has also done the same, and where someone creates a contradictory block (which normally wouldn't happen unless someone was being malicious).

Never the less, I kind of like the idea of the user being notified when a newly activated more stringent soft fork rule caused a block to be rejected. The first time it happens, a message could come up, and then for some time after maybe it would be logged somewhere easily accessible. Such an event could be an excellent trigger to enable replay attack prevention, although maybe not automatically... unless everyone was pretty sure that a long-standing competing fork was likely to occur.

Cheers,
Praxeology Guy

-------- Original Message --------
Subject: Re: [bitcoin-dev] Draft BIP: Version bits extension with guaranteed lock-in
Local Time: April 7, 2017 8:55 AM
UTC Time: April 7, 2017 1:55 PM
From: bitcoin-dev@lists.linuxfoundation.org
To: Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>

The primary failure mode of a user's misconfiguration of nTimeout will
be a stopped chain.

If less-sophisticated users are offered these configuration settings
then chaintip progress failures that result from them should be
prominently displayed.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
ASIC boost is definitely a protocol vulnerability.

It makes Bitcoin resistant to current and future modifications which are
necessary to preserve decentralization.

That alone should be enough to prioritize a swift preventative measure.

On May 18, 2017 3:29 PM, "Ryan Grant via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

On Thu, May 18, 2017 at 9:44 AM, Cameron Garnham via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
‘ASICBOOST’.

On Thu, May 18, 2017 at 10:59 AM, Tier Nolan via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
block
optimisation.

One principled way to proceed would be to fault not the exploit, but
the protocol design.

Bits in the block header have been discovered which could be used for
dual meanings, and at least one meaning does not preserve the
incentive balances intended and assumed by others.  This unexpectedly
creates an incentive to block protocol improvements.  The protocol
must be repaired.

In this view, which focuses on covert-ASICBOOST, how work is done is
up to the implementation.  But if the hashing work specified possibly
could gain from blocking development work, then we have a
vulnerability.

I believe this is clear grounds for taking action without any delay.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 04/08/2017 04:58 PM, Tomas wrote:

Maybe it's an issue of terminology. I have never used the terms
base/peak load. However I've been trying to get across, poorly I
suppose, that this is actually implemented in libbitcoin. I generally
refer to it as tx pre-validation. I've also tried to relate that you
are unnecessarily relating pre-validation to compactness. These are
unrelated ideas and better considered independently. One can get
nearly all of the benefit of pre-validation while still receiving
blocks (vs. compact blocks). The advantage of compactness is reduced
latency of the block announcement. The reason for pre-validation is
amortization of the validation and/or storage cost of a block.


As I understand it you would split tx inputs and outputs and send them
independently, and that you intend this to be a P2P network
optimization - not a consensus rule change. So my comments are based
on those inferences. If we are talking about consensus changes this
conversation will end up in an entirely different place.

I don't agree with the input/output relevance statements above. When a
tx is announced the entire tx is relevant. It cannot be validated as
outputs only. If it cannot be validated it cannot be stored by the
node. Validating the outputs only would require the node store invalid
transactions.

I do accept that a double-spend detection is not an optimal criteria
by which to discard a tx. One also needs fee information. But without
double-spend knowledge the node has no rational way to defend itself
against an infinity of transactions that spend the minimal fee but
also have conflicting inputs (i.e. risking the fee only once). So tx
(pool) validation requires double-spend knowledge and at least a
summary from outputs.


Inputs that are already valid against prevouts remain valid assuming
consensus rules have not changed. But any input that spends a coinbase
must be validated for prevout height once there is a block context for
validation. Additionally the set of txs must be validated for total
size, sigops, and fee claim. So it's not true that conflict detection
alone is sufficient. Yet one can cache a tx's size, sigops, fee and
minimum height in a graph so that when a block appears that contains
that tx the input validation can be skipped.

Ignoring the (actual) requirement for the full tx on the pool
validation, the required "order" validation at (compact or other)
block arrival basically consists of traversing each tx, ensuring none
are confirmed in a block below the fork point; traversing each each of
its confirmed inputs, ensuring that none are spent in a block below
the fork point; and ensuring the block's set of transactions do not
contain missing inputs and do not double spend internal to the block.

This and the above-mentioned other required per-transaction block
validation data can be cached to an in-memory structure as a potential
optimization over navigating the store, and as you say, does not
therefore require the actual outputs (script/value). But the original
issue of needing full transactions for independent transaction
validation remains.


A reorg is conceptual and cannot be engineered out. What you are
referring to is a restructuring of stored information as a consequence
of a reorg. I don't see this as related to the above. The ability to
perform reorganization via a branch pointer swap is based not on the
order or factoring of validation but instead on the amount of
information stored. It requires more information to maintain multiple
branches.

Transactions have confirmation states, validation contexts and spender
heights for potentially each branch of an unbounded number of
branches. It is this requirement to maintain that state for each
branch that makes this design goal a very costly trade-off of space
and complexity for reorg speed. As I mentioned earlier, it's the
optimization for this scenario that I find questionable.


Full separation of concerns allows all validation to be performed in
isolation from the store. As such validation state can be faked and
provided to a tx, block or chain, for the purpose of test. Validation
that interacts with a complex store during validation is harder to
fake and tests can be hard to verify.

It's not really the "one-step" approach that make this possible. In
fact that's not an accurate description. Validation and storage of txs
and blocks consists of four steps:

(1) context free
(2) contextual (chain-based)
(3) expensive (script eval)
(4) storage and notification

So we have:

tx.check()
tx.accept(state)
tx.connect(state)
chain.organize(tx)

block.check()
block.accept(state)
block.connect(state)
chain.organize(block)

...where "chain" is the store, from which "state" is derived. The
state for an unconfirmed tx is based on the presumption that the tx
would be mined in the next block. If that is not the case then its
pre-validation can become invalidated. So from my perspective, this
discussion is all about populating state. Anything that cannot be
placed into that pattern would complicate both the conceptual model
and testing. We've also seen that this isolation also has performance
advantages, as it facilitates optimizations that are otherwise
challenging.


Because choosing the lesser amount of work is non-consensus behavior.
Under the same circumstances (i.e. having seen the same set of blocks)
two nodes will disagree on whether there is one confirmation or no
confirmations for a given tx. This disagreement will persist (i.e. why
take the weaker block only to turn around and replace it with the
stronger block that arrives a few seconds or minutes later). It stands
to reason that if one rejects a stronger block under a race condition,
one would reorg out a stronger block when a weaker block arrives a
little after the stronger block. Does this "optimization" then apply
to chains of blocks too?


Implementations are free to choose no blocks. That's not really the issu
e.


Accepting a block that all previous implementations would have
rejected under the same circumstance could be considered a hard fork,
but you may be right.

Yet the classification is not essential to my point. Nor is any
material change required to validate blocks in parallel. We can do it
using current design, but it doesn't make sense to do so.


This is not an optimization, since it should always be optimal to
validate blocks independently. Performing multiple together inherently
slows both of them. And the advantage to not validating *either* would
remain.


Hope is a bug.


You cannot have a useful performance measure without full compliance.


If you intend this to be useful it has to help build the chain, not
just rely on hardwiring checkpoints once rule changes are presumed to
be buried deeply enough to do so (as the result of other implementations
).

I understand this approach, it was ours at one time. There is a
significant difference, and your design is to some degree based on a
failure to fully consider this. I encourage you to not assume any
consensus-related detail is too small.


It's worth noting that many of your stated objectives, including
modularity, developer platform, store isolation, consensus rule
isolation (including optional use of libbitcoinconsensus) are implemente
d.

It seems like you are doing some good work and it's not my intent to
discourage that. Libbitcoin is open source, I don't get paid and I'm
not selling anything. But if you are going down this path you should
be aware of it and may benefit from our successes as well as some of
the other stuff :). And hopefully we can get the benefit of your
insights as well.

e
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBCAAGBQJY7DUUAAoJEDzYwH8LXOFOTB0H/jDtfnC6B9CtGrCTPtET+dDx
r0uQ0SXo40AUTplyKQ228rVkjmZyczTOtIP5uNvKpvlr9wW8TyYzFzNW4RNCNtdP
xZ9OjrfC24J2n+m1b9z9+CA85qAQxzLztBybDYzXCJG/dQ+y++7BR+rILGiRWUhs
lROeaEMqlDl0fy5J3dlpe0RGZJPSRqlxW7EBNHYc3IEDNL+j5m80/tWb6H5a3Mv8
7GTr6ulZef/04u/hRTXQ0ONy0MAIoi63HNHQuR0wF70ewGVmtFY4RHXEnNi+ucIG
w3QZuNTPtjqIS+ZbpFuqBop+L3CtId9+jxaBAao2tEieoIUl/faLjdTPP+r0n6A=
=5mz8
-----END PGP SIGNATURE-----

-------------------------------------
Just a quick follow-up on BIP91's prospects of avoiding a BIP148 chain
split, because I may have left an overly pessimistic impression -

In short: the timing isn't as dire as I suggested, BUT unless concrete
progress on a plan starts taking shape, esp miner support, *the split is
indeed coming.*

THE GOOD NEWS: several refinements have been noted which could get BIP91
(or splitprotection, Segwit2x, etc) deployed faster:
- The lock-in window could be shortened, eg to splitprotection's 504 blocks
(3.5 days)
- Of course the 80% threshold could just be reduced, eg to
splitprotection's 65%
- BIP91 nodes could start signaling on bit 1 the moment bit 4 reaches
lock-in, rather than waiting another period until it  "activates".
 (Orphaning of non-bit-1-signaling blocks would probably also have to start
at or shortly after the same time [1].)

Combining these approaches, *July 26* is an approximate hard deadline for
significantly less tight than my previous June 30 (or my original,
erroneous "a few days ago").

THE BAD NEWS: no one should underestimate the steps that would need to be
completed by that deadline:
1. Coordinate on a solution (BIP91, splitprotection, Segwit2x, BIP148
itself, ...)
2. Implement and test it
3. Convince >50% of miners to run it [2]
4. Miners upgrade to the new software and begin signaling

In particular, #3: afaict a lot of convincing is still needed before miner
support for any of these reaches anything like 50%.  (With the exception of
Segwit2x, but it has the additional handicap that it probably needs to
include deployable hard fork code, obviously ambitious in 1.5 months.)


[1] See Saicere's comment:
https://github.com/btc1/bitcoin/pull/11#discussion_r121086886, and related
discussion at https://github.com/btc1/bitcoin/pull/11#issuecomment-307330011
.

[2] Note that >50% need to run the *solution*, eg BIP91; old BIP141 nodes
signaling segwit support do *not* count, since they won't orphan non-bit-1
blocks.  The impending split isn't between nodes that support segwit vs
don't, but between those that reject non-segwit-supporting blocks vs don't.


On Fri, Jun 9, 2017 at 1:23 AM, Jacob Eliosoff <jacob.eliosoff@gmail.com>
wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 05/12/2017 10:45 PM, Luke Dashjr wrote:

You seem to be suggesting that in order to decentralize mining nobody
should mine. I'm having a hard time making sense out of that.


So maybe you are just saying that nobody should mine because it's a
zero sum game that one miner will always win and therefore we should
not push up the hash rate by trying to compete because the same miner
just makes more money on the hardware. Apparently it is economically
impossible for anyone else to compete in hardware as well.

I agree that there is a serious problem of mining centralization (and
economic/validation centralization). If these problems are not solved
Bitcoin will fail. It will rise again, with people a little wiser, but
the disruption will be unfortunate for many.

I don't want to see that, so I tend to not advocate for solutions that
run counter to the security model. Many people must mine, there is no
way around it. And if people want a say with respect to mining, they
should mine. As a developer I would rather work toward fixing that
problem than putting a band-aid over it that basically tells people
that the way they get their say is by donating to the big mining
personality of their choice.


No, really?

If it wasn't clear, I was relating two sets of proposals. One aims to
find ways to fund node operation and the other aims to fund miner
signaling. The former fails to understand the economics and security
model of full node operation and the latter fails to understand that
distributed mining is as essential to Bitcoin survival as distributed
validation.


I assumed that people understand how markets work. Miners compete for
fees. By eliminating a subset of potential sellers (currently by ~70%)
the buyer raises his own price. Presumably the price is raised even
further by increasing the size of the transaction. This is either a
donation to the cause or a purchase of the signal, depending on how
you want to describe it (all donations are purchases of a sort).

So there is a cost increase that could alternatively be incurred by
mining (i.e. assuming a lossy operation). If one is going to spend
money on influencing mining one might as well not do it in a way that
contributes to centralization while training people to rely on it.


Miners absolutely "control Bitcoin in some way" - that is their
purpose. They control the ordering of transactions, and with
sufficient hash power can double-spend and therefore make the network
unusable. Why would you bother to make me type this?


Absolute nonsense, a miner incurs no obligation to the "greater
economy". He is offering a service in voluntary trade. He is likely to
do what it takes to spend his coinbase, assuming he wants to. This
gives the economy strong economic control over his behavior. But
nothing whatsoever obligates him to signal soft forks (or not optimize
his operations).

Double spending is an attack, on the person who has been robbed. The
state enforcing a patent is an attack, on the person against whom it
is enforced. These are called attacks **because they are actually
theft**. You are conflating normal operation (despite disagreement
with some unmeasurable "wishes") with robbery.

e
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBCAAGBQJZFqstAAoJEDzYwH8LXOFOsvsH/2aWlsfi5hB1IrnX1UBsMJl8
+R6BZE+d5C5uNkk6/yENHqwwgTv8yhOKav2Y7xYx/DedhVftX90h9CtdeKGgCS2H
cYNtoNauAvF2nlEMGGGcinLkYbS0dyQm07zwOI8gwuzbkslFGxLFClngFlFgMF4S
4/YCWvtRJ0O5dkrAZuKwG/7JQ1JNopbDTxssirA/OzwTGjq7BUv7INyR8nBbOp6I
xcrjq2bXja6Kxo08pr3+UrWc+0LO8fvX9z3rkm6USyin7TueS85gEUsk30h1Xng3
Al1QccJ9KKJ+iQKdGozeHD2OlTFC1zW2kZaWbhgxOewDlmf7cNwZXEUwfr4C4Hs=
=j5eo
-----END PGP SIGNATURE-----

-------------------------------------
Greetings bitcoin-dev,

  We’re returning to this thread to give an update on the Dandelion project
after several months of additional work. (Dandelion is a new
privacy-preserving transaction propagation method, which we are proposing
as a BIP. See the original post in this thread
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-June/014571.html
for more background) The feedback on our initial BIP from Greg Maxwell in
this thread touched on several important issues affecting the protocol
design, which it has taken us until now to adequately address.

The focus of this update is a new variant of the Dandelion++ mechanism
presented earlier. The new variant is called “Per-Incoming-Edge” routing.
In a nutshell, while the earlier Dandelion++ variant calls for routing
*each* stem transaction through a randomly chosen path, Per-Incoming Edge
routing causes each transaction from the same source to traverse the same
pseudorandom path. The most important benefit of Per-Incoming-Edge is that
it prevents “intersection attacks” that result if a client broadcasts
multiple transactions over a short period of time. We validate this new
variant with new analysis and simulation as explained below.

Today’s update also includes an outline of our next development plans. We
have not yet completed a reference implementation, so this update does not
include a new BIP. Instead we’re just outlining the steps we plan to take
before an updated BIP. The new approach also  impacts our implementation
approach. Since Per-Incoming Edge routing simplifies the handling of orphan
transactions, we’re now planning on adopting Greg Maxwell’s suggestion to
bypass the txMempool for dandelion stem transactions.

======

The feedback on Dandelion from Greg Maxwell touched on a few important
issues: (1) robustness to observations over time, aka “intersection
attacks”, (2) protocol- or implementation-level data leaks, and (3) graph
learning.

(1) With time, the adversary may be able to observe many message
trajectories, thereby eventually learning the underlying graph structure
and/or improving its deanonymization estimate for a given estimate of the
graph structure. In our original Dandelion BIP, we addressed this by
changing the anonymity graph topology from a directed line to a directed
4-regular graph. (In short, instead of a single outgoing edge for Dandelion
transactions, each node selects from  *two* such edges). This topology
provides robustness to adversaries who are able to learn the graph, but
those results still assume that each node generates only one transaction in
each “epoch” (time between reshuffling the anonymity graph). Hence a big
remaining question is to understand the effect of intersection attacks--an
adversary observing multiple dependent transactions--on deanonymization
precision and recall.

(2) The second issue is protocol- or implementation-level behavior that
would allow an adversary to actively probe Dandelion to learn more
information than before. As you correctly note, we want to avoid the
adversary using conflicting transactions to infer which nodes are in the
stem. This issue is related to issue (1), in that our mechanism for
addressing intersection attacks will determine what data structures we need
in the implementation.

(3) The third issue is that an adversary may be able to infer the structure
of the graph by observing network traffic. We want to prevent this.

----------

Intersection Attacks

----------


An adversary’s ability to launch intersection attacks depends on the
internal Dandelion routing policy. Two natural ways to approach routing are
the following:

 1. Per-Hop: For each incoming stem transaction, make an independent random
decision of (a) whether to transition to “fluff” phase, and (b) if “stem”,
then which node should we relay to. This means that two transactions, even
starting from the same source, take independent random walks through the
anonymity graph. This is what our current implementation does.

 2. Per-Inbound-Edge: For each inbound edge e, randomly select one outbound
edge g, and relay all transactions arriving on edge e to edge g (assuming
the transaction remains in stem phase). Each node uses this relay mapping
for an entire epoch, which lasts about 10 min. Each source also randomly
chooses one outbound edge g’ for its own transactions; so if a node
generates 5 transactions, they will all get propagated over edge g’. This
approach has the property that during an epoch, all transactions from a
single source will take the same path through the stem graph.

We have simulated and analyzed these two routing protocols, and find that
per-inbound-edge routing seems to be more robust to intersection attacks.
For our simulations we consider the “first-spy” estimator --- this means
the rule where the attacker simply guesses that the first peer to relay a
transaction to a spy node is the real source. Figure 1 (link below)
illustrates the first-spy precision for per-incoming-edge routing and
per-transaction routing when each node has *one* transaction. Higher
precision means worse anonymity. For comparison, this figure includes
diffusion, which is the spreading mechanism currently used. Here ‘p’
denotes the fraction of nodes in the network that are spies. (Recall that
in our model, we treat the attacker has having control over some fraction
of random nodes). The turquoise curve (labelled ‘p’) is shown for
reference---it does not represent any routing protocol.

https://github.com/gfanti/bips/blob/master/per-edge-vs-per-tx.jpg

First, note that the first-spy estimator is thought to be significantly
suboptimal for diffusion (red line). Prior literature has shown that on
certain classes of graphs, there exist estimators that can detect diffusion
sources with much higher probability than the first-spy estimator. While
it’s unclear how to apply those algorithms to Bitcoin’s graph, it is likely
that strong algorithms exist. Hence the first-spy estimator serves as a
lower bound on precision for diffusion. On the other hand, we can show
theoretically that the first-spy precision for per-tx and per-incoming-edge
routing is within a small constant factor of the optimal precision for
per-incoming-edge routing. Thus, we expect that the green (per-edge) and
blue (per-tx) lines reflect the near-optimal attack, whereas the red line
(diffusion) could be much higher in practice.

The second issue to note is that the blue line (per-tx forwarding) has the
lowest precision of the three protocols for one tx per node. The green line
(per-edge forwarding) has higher precision than per-tx forwarding when
there are very few spies, but approaches per-tx forwarding as p increases.
Moreover, it has lower precision than diffusion for p>=0.05.

However, the real benefits of per-edge forwarding emerge as nodes start to
transmit multiple transactions. Under per-edge forwarding, even if nodes
transmit multiple transactions each, those transactions will traverse the
same path in the anonymity graph, thereby preventing the adversary from
learning any new information from later transactions. Meanwhile, under
per-tx routing, we find empirically that as nodes generate an increasing
number of transactions, each source generates a unique signature of
spy-node-observations (we are currently working on a more detailed
exploration of this question). We expect that such signatures can be used
to exactly deanonymize users in cases where the adversary learns the
graph. Hence
per-tx forwarding is actually quite fragile to adversaries learning the
graph, whereas per-incoming-edge is robust to intersection attacks. This is
one key reason for adopting per-incoming-edge forwarding.

Adopting per-incoming-edge forwarding has another important implication: it
becomes easy to enforce the condition that child transactions never enter
fluff mode before parent transactions. This significantly simplifies orphan
handling, and means that adversaries cannot infer that a preceding
transaction is still in stem mode just by passively listening to network
traffic. We revisit this issue in the next section.

----------

Implementation-Level Metadata Leaks

----------

tl;dr: concept ACK for gmaxwell’s suggestion on a new per-peer data
structure instead of mempool

Regardless of which routing policy we choose, it is important that
implementations do not leak more information about transactions than they
do in our model. It’s especially important that spies do not get an
“off-path” view of the nodes involved in the stem of a transaction. This
practically means that implementations must be careful not to expose
whether or not a stem transaction was received, to any node except the two
randomly chosen ones. (i.e., not to supernodes that may make inbound
connections to thousands of nodes).

We are currently developing a reference implementation for Dandelion++, as
a patch against Bitcoin Core. It requires thoughtful integration to make
this patch, and the choice of routing policy informs our approach. We have
so far considered two main integration approaches, whose main difference is
whether or not they reuse the existing *txMempool* data structure to store
stem mode transactions.

A. Mempool embargo:

This how is our current implementation works. Stem transactions are only
relayed if they are accepted to mempool. Stem transactions are “embargoed”
by suppressing them from MEMPOOL and INV messages sent from the node. This
was the easiest to implement while preserving all of Bitcoin’s existing DoS
prevention. In particular, it simplifies the handling of orphan
transactions, because the AcceptToMempool routine already handles orphan
transactions. However, this approach comes with a risk of indirect leakage,
especially if some edge case is missed in implementation.

B. Avoid modifying mempool (or any global structure) for stem transactions:

This is the approach preferred by Greg Maxwell. The main benefit is that it
is much more clear that there is no indirect leakage, although it makes it
harder to argue there is no additional DoS concern. We have already taken a
couple of steps towards implementing this here:
https://github.com/gfanti/bitcoin/commits/dandelion-nomempool The main idea
is to avoid duplicating the rules for whether a transaction would be
accepted into mempool or not, by adding a “dry run” option to the
AcceptToMempool function. Our implementation of this approach is not yet
finished; it still remains to develop the per-peer data structure.

Orphan transactions are important for per-tx routing, because with per-tx
routing, the child and the parent might travel along different stems. A
burst of transactions from a single sender would have to be queued so they
enter fluff mode sequentially. A lot of our testing (with the included test
framework) involved ensuring such transactions were handled effectively.
This was also the deciding factor for our choice of using Option B “Mempool
Embargo” above. With Per-incoming Edge routing, however, orphan transaction
handling can be simplified, since out-of-order transactions would not be
sent along stems.

We therefore plan to re-engineer a much of our reference implementation to:

1) use per-incoming edge routing,

2) simplify handling of orphan transactions,

3) adopt the proposed approach of avoiding the mempool data structure for
stem transactions.

We’ll give an update soon on our development progress before updating the
BIP.

----------

Graph Learning

----------


Greg Maxwell also asked:

```

Has any work been given to the fact that dandelion propagation
potentially making to measure properties of the inter-node connection
graph?  e.g.  Say I wish to partition node X by disconnecting all of
its outbound connections, to do that it would be useful to learn whom
is connected to X.  I forward a transaction to X, observe the first
node to fluff it,  then DOS attack that node to take it offline.  Will
I need to DOS attack fewer or more nodes  to get all of X's outbounds
if X supports rapid stem forwarding?

```

In terms of graph learning, there are two graphs to consider: the anonymity
graph (i.e., the stem set of each node), and the main P2P graph. Dandelion
has at least as good graph-hiding properties as diffusion for a natural
class of attacks (which include the attack described in the comment above).


Consider the task of learning the main P2P graph in today’s network (under
diffusion spreading). Suppose a supernode is connected to all nodes, and
wants to learn the 1-hop neighbors of a given target node. The eavesdropper
passes a transaction to the target, and waits to hear which nodes relay the
transaction first. If the target has 8 outbound neighbors, then in each
experiment, the supernode will receive 8 independent relay timestamps from
the target’s 1-hop neighbors. By repeating this many times, the adversary
can infer the 1-hop neighbors as the nodes who relay the transaction with
the appropriate mean delay (taking into account the appropriate exponential
parameters). Eventually, this set will be learned with high certainty.

Now consider the same task if the target is a Dandelion node. Note that the
supernode’s probe tx must be relayed as a Dandelion message to observe any
difference with the prior experiment. First of all, the target will only
pass the tx to one node in its stem set. Hence, in each experiment, the
supernode can learn at most one timestamp from a relevant node, whereas
previously it learned eight per experiment. This inherently reduces the
adversary’s learning rate. Second, if the target’s relay is a Dandelion
node and chooses to extend the stem, then the supernode will not receive any
relevant timestamp (i.e. a timestamp from a 1-hop neighbor) unless the
supernode lies in the relay’s stem set. This happens with a probability
that depends on the level of deployment and the number of (seemingly)
distinct nodes being run by the supernode, but is strictly smaller than 1.

   _ Hence, the rate at which an attacker can learn the main P2P graph is
strictly higher under diffusion (as in Bitcoin Core today) compared to
using Dandelion. _

A similar argument can be made for the anonymity graph, which we currently
implement as an overlay to the main P2P graph.

=====

Responses to Other Miscellaneous Comments

====

```

An alternative construction would be that when a stem transaction goes
out there is a random chance that the stem flag is not set (with
suitable adjustment to keep the same expected path length)

For some reason I believe this would be a superior construction, but I
am only able to articulate one clear benefit:  It allows non-dandelion
capable nodes to take on the role of the last stem hop, which I
believe would improve the anonymity set during the transition phase.

```

Agreed, this is actually what we have implemented.


---------

Thanks!

Giulia Fanti <gfanti@andrew.cmu.edu>
Andrew Miller <soc1024@illinois.edu>
Surya Bakshi <sbakshi3@illinois.edu>
Shaileshh Bojja Venkatakrishnan <bjjvnkt2@illinois.edu>
Pramod Viswanath <pramodv@illinois.edu>



Date: Tue, 13 Jun 2017 01:00:50 +0000
-------------------------------------
The ones that *could* pay non-mining full nodes are miners/pools, by
outsourcing transaction selection using a different PoW.  By doing so
they could buy proof-of-uncensored-selection and proof-of-goodwill for a
small fee.
We would allow full nodes to generate and broadcast a template
block which:
* Does not contain a valid header yet
* Contains the transaction selection
* Contains a  coinbase output with a predetermined part of the block
  reward (say 0.5%) to themselves* Contains a nonce for PoW of a predetermined currently ASIC resistant
  hash function behind a OP_RETURN.
The template with the highest PoW since the last block would be leading.
A miner/pool can then choose to use this instead of their own, adding
the rest of the reward and the SHA nonce themselves. That way they would
set up a competition among full nodes.
This would of course be voluntary but provable, so maybe in a pool's
interest to do this via naming and shaming.
Tomas
bitcrust

On Wed, May 3, 2017, at 23:43, Ben Thompson via bitcoin-dev wrote:

-------------------------------------


On Fri, Apr 7, 2017, at 02:32, Gregory Maxwell wrote:


Bitcrust separates script validation (base load, when transaction come
in) from order validation (peak load, when blocks come in).

For script validation it would obviously need the ~2GB (or I think
~1.5GB) of outputs needed to validate these.  For order validation it
needs ~200mb or the spent-index (for bit-lookups) and I would guess
roughly ~500mb of the spent-tree (for scanning), though I don't think
the 5.7GB full spend tree isn't worth pruning anytime soon.

Then it is currently using a  ~1.5GB   index for transaction hash to
fileptr lookups, though this could be made more space efficient.

-------------------------------------
Im very worried about the state of miner centralisation in Bitcoin.

I always felt the centralising effects of ASIC manufacturing would resolve themselves once the first mover advantage had been exhausted and the industry had the opportunity to mature.

I had always assumed initial centralisation would be harmless since miners have no incentive to harm the network. This does not consider the risk of a single entity with sufficient power and either poor, malicious or coerced decision making. I now believe that such centralisation poses a huge risk to the security of Bitcoin and preemptive action needs to be taken to protect the network from malicious actions by any party able to exert influence over a substantial portion of SHA256 hardware.

Inspired by UASF, I believe we should implement a Malicious miner Reactive Proof of Work Additions (MR POWA).

This would be a hard fork activated in response to a malicious attempt by a hashpower majority to introduce a contentious hard fork.

The activation would occur once a fork was detected violating protocol (likely oversize blocks) with a majority of hashpower. The threshold and duration for activation would need to be carefully considered.

I dont think we should eliminate SHA256 as a hashing method and change POW entirely. That would be throwing the baby out with the bathwater and hurt the non-malicious miners who have invested in hardware, making it harder to gain their support.

Instead I believe we should introduce multiple new proofs of work that are already established and proven within existing altcoin implementations. As an example we could add Scrypt, Ethash and Equihash. Much of the code and mining infrastructure already exists. Diversification of hardware (a mix of CPU and memory intensive methods) would also be positive for decentralisation. Initial difficulty could simply be an estimated portion of existing infrastructure.

This example would mean 4 proofs of work with 40 minute block target difficulty for each. There could also be a rule that two different proofs of work must find a block before a method can start hashing again. This means there would only be 50% of hardware hashing at a time, and a sudden gain or drop in hashpower from a particular method does not dramatically impact the functioning of the network between difficulty adjustments. This also adds protection from attacks by the malicious SHA256 hashpower which could even be required to wait until all other methods have found a block before being allowed to hash again.

50% hashing time would mean that the cost of electricity in relation to hardware would fall by 50%, reducing some of the centralising impact of subsidised or inexpensive electricity in some regions over others.

Such a hard fork could also, counter-intuitively, introduce a block size increase since while were hard forking it makes sense to minimise the number of future hard forks where possible. It could also activate SegWit if it hasnt already.

The beauty of this method is that it creates a huge risk to any malicious actor trying to abuse their position. Ideally, MR POWA would just serve as a deterrent and never activate.

If consensus were to form around a hard fork in the future nodes would be able to upgrade and MR POWA, while automatically activating on non-upgraded nodes, would be of no economic significance: a vestigial chain immediately abandoned with no miner incentive.

I think this would be a great way to help prevent malicious use of hashpower to harm the network. This is the beauty of Bitcoin: for any road block that emerges the economic majority can always find a way around.
-------------------------------------

On Fri, Sep 29, 2017, at 04:55, Peter Todd via bitcoin-dev wrote:

By that reasoning, we also shouldn't go to https://coinbase.com or
https://kraken.com to buy any bitcoins? As a MITM can redirect the site
_if_ they obtain the coinbase or kraken certificate.

Obviously, HTTPS is secured under the assumption that certificates are
secure.  

Using the payment protocol simply means paying to a secure endpoint (eg
https://tomasvdw.nl/pay) instead of an address.


So we should not use HTTPS for secure transfer because the
implementation may not be good enough? This incorrectly conflates
implementation with specification. There is nothing stopping a developer
from using a proper implementation.


Currently it is widely used by merchants, but not yet for light clients
_receiving_ money. If it becomes more wide spread,   it offers a range
of advantages as  the bitcoin-address of the URI can and should be
deprecated (made impossible with "h="). A payment address just becomes a
secure endpoint.

This means no more address reuse is possible. Also, it drops the need
for mempool synchronization among non-miners, solely as a "notification"
mechanism. In addition it means light clients know exactly when a
transaction is coming in, so they can efficiently rely on client-side
filtering a small set of blocks, improving their privacy.

In my opinion, the payment protocol is key to scaling.


Sorry, but maybe you  could explain better how secure communication over
HTTPS is "very dangerous"? I think some websites would like to know :)

Tomas van der Wansem
bitcrust

-------------------------------------
Except if people have some incentive to do it, simple example: I have
some servers, they are doing some work but are not so busy finally, I
can decide to run some nodes, this does not cost me more (and less for
the planet than setting up new servers) and I get some rewards (as an
illustration of this my servers are mining zcash and running zcash
nodes, this is of course absolutely not profitable but since this does
not disturb what the servers are primarly intended for and I get some
small zecs with no additionnal costs, why not doing it?) Of course we
can then consider that people doing this are finally using the network...


Le 30/03/2017 à 12:34, Tom Zander via bitcoin-dev a écrit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms


-------------------------------------
I approve of this idea. Counterparty has the same problem. Their API
returns a unsigned transaction that is formed differently from how
other unsigned transactions, which causes friction. Someone needs to
write up a specification that is standardized so that all unsigned
transactions are of the same form. Basically the signature section of
the should contains all the information required to make the
signature, and it needs to be encoded in a way that the signing
application (a blockchain library like BitcoinJ or BitcoinJS) can tell
that it is unsigned.

On 1/9/17, 木ノ下じょな via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Sun, Apr 9, 2017 at 11:44 AM, Erik Aronesty via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


That would force hard forks, cause huge governance problems on selecting
the new PoW algorithm, and probably cause even worse mining chip
manufacturer centralization because it would force miners to buy new chips
instead of sticking with the ones they've already got. They'll likely have
to keep buying new ones anyway as technology improves but it doesn't help
to force that process to go even faster.
-------------------------------------
Maybe this has already been discussed, but I have not found anything online.

To the best of my knowledge, the only BIP which specifies a HD structure
for multisig wallets is BIP45. Unfortunately, when used in a
multi-account fashion, BIP45 gets very tricky very fast. In fact, one
has to either use a new master for every multisig account (hence having
to backup many master private keys) or use the same master for many
multisig accounts, resulting in deterministic but complex and
undesirable key reuse.
I would like to propose a new structure for multi-account multisig
wallets. This structure follows the derivation scheme of other proposals
(in particular BIP44 and BIP49) but adds a level to take into account
multisig accounts separation. In particular, the structure should be as
follows:

m/purpose'/coin_type'/account'/cosigner_index/change/address_index

In this case, a user can create many multisig accounts (each one will be
a different account number) and give his/her account's public derivation
to the cosigners. From this point on, the creation of a multisig P2SH
address will follow the same procedure as described in BIP45, with each
cosigner selecting his branch from the other cosigners' trees.

Would this proposal be acceptable as a BIP?

Simone Bronzini

-------------------------------------
I'm highly unconvinced of this point. Sure, you can change fewer lines
of code, but if the result is, lets be honest, shit, how do you believe
its going to have a higher chance of getting acceptance from the broader
community? I think you're over-optimizing in the wrong direction.

Matt

On 05/09/17 20:58, Sergio Demian Lerner wrote:

-------------------------------------
Juan,

I suggest you take a look at this paper:
http://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf  It may help you form
opinions based in science rather than what appears to be nothing more than
a hunch.  It shows that even 4MB is unsafe.  SegWit provides up to this
limit.

8MB is most definitely not safe today.

Whether it is unsafe or impossible is the topic, since Wang Chun proposed
making the block size limit 32MiB.


Wang Chun,

Can you specify what meeting you are talking about?  You seem to have not
replied on that point.  Who were the participants and what was the purpose
of this meeting?

-Alphonse

On Tue, Mar 28, 2017 at 12:33 PM, Juan Garavaglia <jg@112bit.com> wrote:


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

When the original white paper was written the idea was that nodes
would be miners at same time. That the distribution of mining power
being mostly on par with the distribution of nodes if I understand
correctly. The problem we face now I fear, is the mining power
becoming centralized. Even if every bitcoin node invested a $1000
into mining power and mined at a loss, it still would not even
make a dent in hash distribution. Currently there are around 6000
known nodes. If each node invested $1000 for say 10 ths of hashing
power. At current hashrate of around 3,674,473,142 GH/s this would
only make up %16 of hash power. This is out of balance as while
nodes are distributed mining power is becoming very centralized
due to the creation of monopolization of ASICs. The problem we
are facing is a small group of a couple people whom control a
large amount and growing of hash power. At time of this writing
it has quickly risen to 39% and at current rate will soon become
50% of hashing power that is controlled by a small group of a few
people. Their intentions are too hijack the bitcoin network to a
cryptocurrency that suits their dangerous agenda. Dangerous because
their plan would centralize power of consensus as I understand it,
to themselves the miners. Dangerous also because the code base of
the attempting subverters is buggy, insecure, and reckless from a
technological standpoint. Even though they only have very minute
amount of nodes compared to legitimate bitcion nodes, the danger
is that they are very quickly taking over in mining power. While
it is true that nodes will not accept invalid blocks that would be
attempted to be pushed by the conspirators, they are threatening to
attack the valid (or in their words, "minority chain") by dedicating
some mining power soley to attacking the valid chain by mining empty
blocks and orphaning the valid chain. So even though the majority
of nodes would be enforcing what blocks are valid and as a result
block the non-compliant longer chain, the conspiring miner can simply
(as they are currently threatening to) make the valid chain unuseable
by mining empty blocks.

If a malicious miner with half or majority control passes invalid
blocks, the worst case scenario is a hardfork coin split in which
the non-compliant chain becomes an alt. However the problem is that
this malicious miner is very recently threatening to not just simply
fork, but to kill the valid chain to force economic activity to the
adversary controlled chain. If we can simply defend against attacks
to the valid chain, we can prevent the valid chain from dying.

While empty or near empty blocks would generally be protected by
the incentive of miners to make money. The threat is there if the
malicious miner with majority control is willing to lose out on
these transaction fees and block reward if their intention is to
suppress it to force the majority onto their chain.

Proposal for potential solution Update nodes to ignore empty blocks,
so this way mined empty blocks cannot be used to DOS attack the
blockchain. But what about defense from say, blocks that are
not empty but intentionally only have a couple transactions
in it? Possible to have nodes not only ignore empty blocks,
but also blocks that are abnormally small compared to number of
valid transactions in mempool? 

For example would be something like this:
If block = (empty OR  <%75 of mempool) THEN discard
This threshold just an example.

What would be any potentials risks
and attacks resulting from both having such new rulesets and not
doing anything?

Lets assume that the first problem of blocking empty or near empty
blocks has been mitigated with the above proposed solution. How
likely and possible would it be for a malicious miner with lots of
mining power to orphan the chain after so many blocks even with
non empty blocks? Is there a need to mitigate this? 
If so is it possible?

Time is running short I fear. There needs to be discussion on various
attacks and how they can be guarded against along with various
other contingency plans.

- -- 
Cannon
PGP Fingerprint: 2BB5 15CD 66E7 4E28 45DC 6494 A5A2 2879 3F06 E832 
Email: cannon@cannon-ciota.info

NOTICE: ALL EMAIL CORRESPONDENCE NOT SIGNED/ENCRYPTED WITH PGP SHOULD 
BE CONSIDERED POTENTIALLY FORGED, AND NOT PRIVATE. 
If this matters to you, use PGP.
-----BEGIN PGP SIGNATURE-----

iQIcBAEBCgAGBQJY1UH/AAoJEAYDai9lH2mw2QYQALDLBxjdO5WTG7VXfuAE476p
D3o1MMGw23tb+DFUO5WV6aFqfy3VSxbVXz6UuWbj6kHgp3ys6qxg5TX0Dy8tKSZM
V28kovuS/pfen4gTxw1FCNff7YVW1R8QX+cSYxSD5EoEaTbpIPgi8zMusDxUVZk2
WG3ItoyvkLvoNIYGDcU3gR3UkjDS5lOPiHu5BKSj1dEiibOXhr8JEBCznfUSyxCG
TjVRJaUPlwCU06nad8jAZiDrsW3l866iNkBKaMzMavYuMLvCGIdRkbf54B2ZlIT/
S/owusxqeIdQpydi/3ydnrqyeWo3znMnn+oOvdvfYEHKLts6gar3Zv8cZ40yYSIE
z7C7GQFIo5TYDUNOk+2VE7NNtdX39Wj3gJql/305miaIt0qMsf1D30ODjePwyxUQ
LQ96ZeF1K/0RSTN5TFvLjV9ZmaaN/tFm3kx0PunptJaZT8d9EgMeHREjCF4di04A
6Dp3Qeug41X/zdIc2AM387QnPwmGB1TpfrY9qgvcrIe26T6An2V5LHwVmslCX3ui
DYAl0o5ODQqnnakF1FIrr1blMVqm7FqDPQc1I5TfzQuxX2+x+5zdrciPC2HUMCMQ
jMujge5IdGL3kjEwjt+M6kqLu0/T0fhdUetb2DWrRJUcEVoIaiUL7qLJC+4KMR3d
Gu3oWoE1ld+BC6At28AD
=SSuj
-----END PGP SIGNATURE-----

-------------------------------------
On Mar 25, 2017 10:38 PM, "Alex Morcos via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:


As a Bitcoin user I find it abhorrent the way you are proposing to
intentionally cripple the chain and rules I want to use instead of just
peacefully splitting.


I just want to point out what appears to be doublespeak going on here.

First, I think it would seem obvious to an observer that a sizable portion
of the community (certainly greater than 5%) view segwit as preventing
"rules I want to use instead of just peacefully splitting" but no
consideration was given to these people when designing segwit as a
softfork. I believe it was Luke who went as far as saying consensus does
not matter when it comes softforks.

Furthermore, when segwit was first introduced it kicked off a round of
softfork/hardfork debate which I participated in. The primary concern that
I and other raised was precisely what is going on now.. that miners could
unilaterally impose an unpopular change to the protocol rules.

At the time I told, rather forcefully, by multiple people that miners have
an "absolute right" to softfork in whatever rules they want. Which, of
course, is absurd on it's face.

But I don't see how people can make such claims on the one hand, and then
complain when this process is used against them.

It amounts to nothing more than "When it's rules I like we get to impose
them on non-consenting users. When it's rules I don't like it's an attack
on the network".

It was completely obvious this entire time that softforks were a very
slippery slope, now we are indeed sliding down that slope.
-------------------------------------
On Mon, 2017-03-06 at 07:09 +0000, Luke Dashjr wrote:
Sure, HTTPS is the way to go. But I think that should be required or at
least noted in the BIP, because people could miss easily, e.g., "I
don't need TLS, all the data is public anyway."

Having the rate at the time of payment is indeed very useful, yes.
However that requires just a single value per payment, and there is no
query that tells the server "give me the value closest to timestamp t"
or similar.
Of course the client can download and keep a large part of history and
extract the information on its own but I can imagine that not every
clients wants to do that, and also the client does not know in advance
the bounds (from, to) that it must query.

In the current draft the client or the server cannot specify
granularity. If the clients only wants one value per day but for an
entire year, then it has to perform many requests or download and
process a very large response.
Also, I think it's okay that the type field allows for arbitrary user-
defined values, but it should also have some precisely defined values
(e.g. the mentioned low/high/open/close/typical).
For example, it's not clear currently what "low" means for a timestamp
(as opposed to a time span). Is it the low of the entire day or the low
since the previous record or something different?  

One has to be careful not to add too much complexity though. As soon as
one moves away from timestamps to something like hours or days, all
kind of issues with timezone, daylight saving time etc. appear. Maybe a
simple way to let the client ask "give me one value for every interval
of 3600 seconds" or similar. 


That makes a lot of sense, yes.


Tim

-------------------------------------
I think at least the three following things have to be done before the block size can be increased by any significant amount:
1. A network protocol defined UTXO snapshot format be defined, UTXO snapshots being created automatically in a deterministic periodic and low-cost fashion. Ability to synchronize starting from such a UTXO snapshot as requested by a user.
2. SPV support from a pruned node that has the latest UTXO snapshot. Probably requires committing the UTXO snapshot hash to the block.
3. Given the above fixes the problem of needing full block chain history storage, and people are comfortable with such a security model, a good portion of the network can switch to this security model, and still satisfy our desire for the system to be sufficiently distributed. This requires lots of testing.
4. More current studies on the effect of increasing the block size on synchronizing node drop out due to other reasons such as network bandwidth, memory, and CPU usage.

Without doing the above, scheduling to increasing the block size would be wreckless.

Cheers,
Praxeology Guy

-------- Original Message --------
Subject: Re: [bitcoin-dev] Hard fork proposal from last week's meeting
Local Time: March 29, 2017 2:10 PM
UTC Time: March 29, 2017 7:10 PM
From: bitcoin-dev@lists.linuxfoundation.org
To: Martin Lízner <martin.lizner@gmail.com>, Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>

In order for any blocksize increase to be agreed upon, more consensus is needed. The proportion of users believing no blocksize increases are needed is larger than the hardfork target core wants(95% consensus). The proportion of users believing in microtransactions for all is also larger than 5%, and both of those groups may be larger than 10% respectively. I don't think either the Big-blocks faction nor the low-node-costs faction have even a simple majority of support. Getting consensus is going to be a big mess, but it is critical that it is done.

On Wed, Mar 29, 2017 at 12:49 AM, Martin Lízner via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

If there should be a hard-fork, Core team should author the code. Other dev teams have marginal support among all BTC users.

Im tending to believe, that HF is necessary evil now. But lets do it in conservative approach:
- Fix historical BTC issues, improve code
- Plan HF activation date well ahead - 12 months+
- Allow increasing block size on year-year basis as Luke suggested
- Compromise with miners on initial block size bump (e.g. 2MB)
- SegWit

Martin Lizner

On Tue, Mar 28, 2017 at 6:59 PM, Wang Chun via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
I've proposed this hard fork approach last year in Hong Kong Consensus
but immediately rejected by coredevs at that meeting, after more than
one year it seems that lots of people haven't heard of it. So I would
post this here again for comment.

The basic idea is, as many of us agree, hard fork is risky and should
be well prepared. We need a long time to deploy it.

Despite spam tx on the network, the block capacity is approaching its
limit, and we must think ahead. Shall we code a patch right now, to
remove the block size limit of 1MB, but not activate it until far in
the future. I would propose to remove the 1MB limit at the next block
halving in spring 2020, only limit the block size to 32MiB which is
the maximum size the current p2p protocol allows. This patch must be
in the immediate next release of Bitcoin Core.

With this patch in core's next release, Bitcoin works just as before,
no fork will ever occur, until spring 2020. But everyone knows there
will be a fork scheduled. Third party services, libraries, wallets and
exchanges will have enough time to prepare for it over the next three
years.

We don't yet have an agreement on how to increase the block size
limit. There have been many proposals over the past years, like
BIP100, 101, 102, 103, 104, 105, 106, 107, 109, 148, 248, BU, and so
on. These hard fork proposals, with this patch already in Core's
release, they all become soft fork. We'll have enough time to discuss
all these proposals and decide which one to go. Take an example, if we
choose to fork to only 2MB, since 32MiB already scheduled, reduce it
from 32MiB to 2MB will be a soft fork.

Anyway, we must code something right now, before it becomes too late.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
On Saturday, 27 May 2017 01:09:10 CEST James Hilliard wrote:

Hmm, the flags are identical in 0.13 and 0.14 clients.

Either way, this is rather trivial to solve. If bugs in older clients mean 
they can’t operate properly when SW is activated (via bit 4) but they don’t 
know its activated (since they only look at bit1), then just ban them when 
they misbehave.
And tell people to upgrade to a version where SegWit is less buggy.


Heh, well, this is rather simple to solve by not having all those activation 
codepaths and just picking **one**.

You can safely replace the bit1 activation code with a bit4 activation 
logic, which is based on 80% and has no end-date.
We both know that the bip9 (bit1) based activation will not trigger before 
the expiration date anyway.

These worries are rather trivial to solve if you do a little bit of proper 
architecture of the solution.  This honestly can’t be a reason for saying NO 
to the majority of the mining hash power giving you a break and offering a 
better SegWit activation.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
This is a link to the most updated version of the problem and my proposed
solution, granted it still needs work, but this problem needs to be
resolved quickly. So I hope it will receive the attention it deserves, even
if the solution comes from somebody else.
https://bitcointalk.org/index.php?topic=2126152.new#new

The latest version of the day:

*Solving the Scalability issue for Bitcoin  *

*What am I trying to solve?* Currently Bitcoin’s blockchain is around 140GB.
In 2011 it took 1GB, and it was predicted back then that in 2020 that size
would be 4GB.
As you can see it is not yet 2020, and we are way over that predicted size.
At our current time, prune nodes which make the block smaller, but they can
not be validated without the full node. And this full node is getting
exponentially bigger, we need to stop that. Because if we don’t no private
citizen will have the capability of storing the full node in his computer,
and all full nodes will be at private multi-million dollar companies. That
would literally be the end of decentralization (or non-centralization).
What I am proposing also makes sure the blockchain has a maximum finite
size, because today the blockchain can grow to any size without limit while
it approaches an infinite size!
Today our blockchain is growing at speed which is much faster than Moore’s
law! This proposal will help set storage growth at a reasonable number.


*A short list of what I am about to explain:   Steps that need to be taken:*
---------------------------------------------------------------------------------------------------------------------
(The details are not described in this order)
1) Create a pair of keys, called the Genesis Pair, or Genesis Account, a
private and public key which will be publicly known to all and yet it’s use
will be restricted and monitored by all. The key will be the source of all
funds (Point A).
2) Preserve the Genesis Block, its hash code is needed. And personally I
think its of historical value.
3) Combine all Blocks up to the most recent (not including the Genesis
Block), and cut out all intermediary transactions, by removing All
transactions, and replacing them with new transactions sent from A to every
public key which has funds in the most recent block, in the amount they
have. And sign these transactions with A’s private-key. And create a new
block with this information.
4) This Combined/Pruned Block should point to the Genesis Block hash, and
the next block created should point to the Pruned Blocks hash. The random
number used for this pruned block will be predefined, this random number
normally used to meet the hash difficulty requirement in this case is not
needed, since no difficulty setting is necessary for this block, and by
predefining it, this block can be easily identified.
5) Download the pruned block from another node or create it yourself, the
hash code will be identical for everyone, since the block will be created
exactly the same everywhere.
6) Preserve a certain amount of the most recent blocks, just in case a
longer blockchain is discovered, and then the Pruned Block should be
recalculated.

---------------------------------------------------------------------------------------------------------------------
*Now for a more detailed description: *
I have this idea to solve the scalability problem I wish to make public.
If I am wrong I hope to be corrected, and if I am right we will all gain by
it.
Currently each block is being hashed, and in its contents are the hash of
the block preceding it, this goes back to the genesis block.

What if we decide, for example, we decide to combine and prune the
blockchain in its entirety every 999 blocks to one block (Genesis block not
included in count).

How would this work?: Once block 1000 has been created, the network would
be waiting for a special "pruned block", and until this block was created
and verified, block 1001 would not be accepted by any nodes.
This pruned block would prune everything from block 2 to block 1000,
leaving only the genesis block. Blocks 2 through 1000, would be calculated,
to create a summed up transaction of all transactions which occurred in
these 999 blocks.

And its hash pointer would be the Genesis block.
This block would now be verified by the full nodes (or created by them),
which if accepted would then be willing to accept a new block (block 1001,
not including the pruned block in the count).

The new block 1001, would use as its hash pointer the pruned block as its
reference. And the count would begin again to the next 1000. The next
pruned block would be created, its hash pointer will be referenced to the
Genesis Block. And so on..
In this way the ledger will always be a maximum of 1000 blocks.

 A bit more detail:

All the relevant outputs needed to verify early transactions will all be
preserved in the pruning block. The only information you lose are of the
intermediate transactions, not the final ones the community has already
accepted. Although the origin of the funds could not be known, there
destination is preserved, as well a validation that the transactions are
legitimate.
For example:

A = 2.3 BTC, B=0 BTC, C=1.4 BTC. (Block 1)
If A sends 2.3 BTC to B.  (Block 2)
And then B sends 1.5 BTC to C. (Block 3)
The pruning block will report:
A->B = 0.8 BTC and A->C=2.9 BTC.
The rest of the information you lose, is irrelevant. No one needs to know,
what exactly happened, who sent who what, or when. All that is needed is
the funds currently owned by each key.

Note:  The Transaction Chain would also need to be rewritten, to delete all
intermediate transactions, it will show as though transactions occurred
from the Genesis block directly to the pruned block, as though nothing ever
existed in between. This will be described below in more detail.

You can keep the old blocks on your drive for 10 more blocks or so, just in
case a longer block chain is found, but other than that the information it
holds is useless, since it has all been agreed upon. And the pruning block
holds all up to date account balances, so cheating is impossible.

Granted this pruning block can get extremely large in the future, it will
not be the regular size of the other blocks. For example if every account
has only 1 satoshi in it, which is the minimum, then the amount of accounts
will be at its maximum. Considering a transaction is about 256bytes. That
would mean the pruning block would be approximately 500PB, which is 500,000
Terra-bytes. That is a theoretical scenario, which is not likely to occur.
(256bytes*21M BTC*100M (satoshis in 1 BTC))

A scenario which could be solved by creating a minimum transaction fee of
for example: 100 satoshis, which would insure that even in the most
unlikely scenario, at worst the pruning block would be 5PB in size.
Which is still extremely large for today. But without implementing this
idea the blockchain literally does not have a finite maximum size, and over
time approaches infinity!

*Also, this pruning block does not even need to be downloaded, it could be
created by already existing information, each full node by itself, by: *
1) combining and pruning all previous blocks
2) using the genesis block as its hash pointer
3) using a predefined random number "2", which will be used by all. A
random number which is normally added to a block to ensure the block's
hash-rate difficulty, is not needed in this case, since all information can
be verified by each node by itself through pruning.
This number can also be used to identify this block as the Pruned/Combined
Block since it is static.
4) Any other information which is needed for the SHA256 hash, for example a
time-stamp could be copied off the last block in the block chain.
These steps will ensure each full node, will get the exact hash code as the
others have gotten for this pruning block.

And as I previously stated the next block will use this hash code as its
hash reference.
By creating a system like this, the pruning block does not have to be
created last minute, but gradually over time, every time a new block comes
in, and only when the last block arrives (block 1000), will it be
finalized, and hashed.
And since this block will always be second, it should go by the name
"Exodus Block".

Above, I showed a way to minimize the blocks on the block chain, to lower
the amount of space it takes on the hard drive, without losing any relevant
information.
I added a note, saying that the transaction chain needs to be rewritten,
but I did not give much detail to it.

Here is how that would work:

*The Genesis Account (Key Pair):*
---------------------------------------------------
The problem with changing the transaction and block chain, is that it
cannot be done without knowing the private key of the sender of the of the
funds for each account.
To illustrate the problem: If we have a series of block chains with a
string of transactions that are A→B→C→D, and to simplify the problem, all
money was sent during each transaction, so that no money is left in A or B
or C. And I was to prune these transactions, by replacing them with A→D.
Only this transaction never occurred, nor can anyone create it without A’s
private key.
There is however a way to circumvent that problem. That is to create a
special account called the “Genesis Account”, this account’s Private Key
and Public Key will be available to everyone.
(Of course, accounts do not really exist in Bitcoin, when I say account
what I really mean is a Private/Public Key pair)
This account will be the source of all funds
But this account will not be able to send or receive any funds in a normal
block, it will be blocked--blacklisted. So no one can intentionally use it.
The only time this account will be used is in the pruning block, a.k.a
Exodus Block.
When creating the new pruned block chain and transaction chain, all the
funds that are now in accounts must be legitimate, and it would be
difficult to legitimize them unless they were sent from a legitimate
private key, which can be verified. That is where the Genesis account comes
in. All funds in the Exodus Block will show as though they originated and
were sent with the Genesis private-key to generate each transaction.
The funds which are sent, must match exactly the funds existing in the most
updated ledger in block 1000.
In this way the Exodus Block can be verified, and the Genesis Account
cannot give free money to anyway, because if someone tried to, it would
fail verification.

Now the next problem is that the number of Bitcoins keeps expanding and so
the funds in the Genesis Account need to expand as well. That can be done
by showing as though this account is the account which is mining the coins,
and it will be the only account in the Exodus Block which “mines” the
coins, and receives the mining bonus. In the Exodus Block all coins mined
by the real miners will show as though they were mined by Genesis and sent
to the miners through a regular transaction.

I hope this proposal will be implemented as soon as possible so that we can
avoid a problem which is growing by the minute. It was brought up about 6
years ago when the blockchain was only 1GB in size, nobody imagined back
then that it would grow so quickly, and the problem was ignored.
Today all solutions implemented have been implemented by software, and not
on the blockchain itself, these solutions are not helpful in the long run.

The full node needs to be publicly available to everyone, and at this rate,
nobody will have the hard-drive capacity to store. This will make us more
dependent on private corporation’s to store the blockchain, which will lead
us quickly to a centralized currency platform. By then it will be too late,
and the corporations will have complete control of what happens next.
Please take this problem seriously and work with me, to prevent it while we
still have some time.
The exact details can be worked out at a later time, but for now we need at
least an acknowledgment that this problem is dire, and needs to be solved
in a year’s time. I have presented a solution, if someone has a better one,
then let him/her step forward, but in any case a solution needs to be
implemented as soon as possible.

*I have given a basic proposal, I am sure there are those among us with
more technical understanding to the nuances of how this idea should be
implemented. I am counting on their help to see this through.*

Adam Shem-Tov
-------------------------------------
I think you could check out and coordinate with the OpenAssets proposal.

Your current draft also claims to solve a lot of problems that it doesn't 
actually solve technically...

Luke


On Wednesday 06 September 2017 11:44:47 Luca Venturini via bitcoin-dev wrote:

-------------------------------------
I feel particularly disappointed that while this BIP is 80% similar to my proposal made 2 months ago ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013490.html ), Matt Corallo was only the person replied me. Also, this BIP seems ignored the txid malleability of the resolution tx, as my major technical critique of xblock design.

But anyway, here I’m only making comments on the design. As I said in my earlier post, I consider this more as an academic topic than something really ready for production use.


Softforks by definition tighten consensus rules


so the authors don’t consider segwit as a consensus-layer solution to increase transaction throughput, or not think segwit is safe? But logically speaking if segwit is not safe, this BIP could only be worse. OTOH, segwit also obviously increases tx throughput, although it may not be as much as some people wish to have.


The 2013 one is outdated. As the authors are not quoting it, not sure if they read my January proposal


I think extension block in the proposed form actually breaks BIP141. It may say it activates segregated witness as a general idea, but not a specific proposal like BIP141


It needs to be more specific here. How are they exactly arranged? I suggest it uses a root of all txids, and a root of all wtxids, and combine them as the commitment. The reason is to allow people to prune the witness data, yet still able to serve the pruned tx to light wallets. If it makes txid and wtxid as pairs, after witness pruning it still needs to store all the wtxids or it can’t reconstruct the tree


This hits the biggest question I asked in my January post: do you want to allow direct exit payment to legacy addresses? As a block reorg will almost guarantee changing txid of the resolution tx, that will permanently invalidate all the child txs based on the resolution tx. This is a significant change to the current tx model. To fix this, you need to make exit outputs unspendable for up to 100 blocks. Doing this, however, will make legacy wallet users very confused as they do not anticipate funding being locked up for a long period of time. So you can’t let the money sent back to a legacy address directly, but sent to a new format address that only recognized by new wallet, which understands the lock up requirement. This way, however, introduces friction and some fungibility issues, and I’d expect people using cross chain atomic swap to exchange bitcoin and xbitcoin

To summarise, my questions are:
1. Is it acceptable to have massive txid malleability and transaction chain invalidation for every natural happening reorg?  Yes: the current spec is ok; No: next question (I’d say no)
2. Is locking up exit outputs the best way to deal with the problem? (I tried really hard to find a better solution but failed)
3. How long the lock-up period should be? Answer could be anywhere from 1 to 100
4. With a lock-up period, should it allow direct exit to legacy address? (I think it’s ok if the lock-up is short, like 1-2 block. But is that safe enough?)
5. Due to the fungibility issues, it may need a new name for the tokens in the ext-block


I suggest to only allow push-only and OP_RETURN scriptPubKey in xblock. Especially, you don’t want to replicate the sighash bug to xblock. Also, requires scriptSig to be always empty


Why 7? There are 16 unused witness program versions


There is a flaw here: witness script with no sigop will be counted as 0 and have a lot free space


so 72 bytes is 1 point or 0 point? Maybe it should just scale everything up by 64 or 128, and make 1 witness byte = 1 point . So it won’t provide any “free space” in the block.


I’d suggest to have at least 16 points for each witness v0 output, so it will make it always more expensive to create than spend UTXO. It may even provide extra “discount” if a tx has more input than output. The overall objective is to limit the UTXO growth. The ext block should be mainly for making transactions, not store of value (I’ll explain later)


In general I think it’s ok, but I’d suggest a higher threshold like 5000 satoshi. It may also combine the threshold with the output witness version, so unknown version may have a lower or no threshold. Alternatively, it may start with a high threshold and leave a backdoor softfork to reduce it.


It is a double-edged sword. While it is good for us to be able to discard an unused chain, it may create really bad user experience and people may even lose money. For example, people may have opened Lightning channels and they will find it not possible to close the channel. So you need to make sure people are not making time-locked tx for years, and require people to refresh their channel regularly. And have big red warning when the deactivation SF is locked in. Generally, xblock with deactivation should never be used as long-term storage of value.

————
some general comments:

1. This BIP in current form is not compatible with BIP141. Since most nodes are already upgraded to BIP141, this BIP must not be activated unless BIP141 failed to activate. However, if the community really endorse the idea of ext block, I see no reason why we couldn’t activate BIP141 first (which could be done in 2 weeks), then work together to make ext block possible. Ext block is more complicated than segwit. If it took dozens of developers a whole year to release segwit, I don’t see how ext block could become ready for production with less time and efforts.

2. Another reason to make this BIP compatible with BIP141 is we also need malleability fix in the main chain. As the xblock has a deactivation mechanism, it can’t be used for longterm value storage.

3. I think the size and cost limit of the xblock should be lower at the beginning, and increases as we find it works smoothly. It could be a predefined growth curve like BIP103, or a backdoor softfork. With the current design, it leaves a massive space for miners to fill up with non-tx garbage. Also, I’d also like to see a complete SPV fraud-proof solution before the size grows bigger.





-------------------------------------
On 12/05/2017 08:49 PM, ZmnSCPxj via bitcoin-dev wrote:


Limiting the withdrawal outputs to P2PKH and perhaps P2WPKH (would there
be any network benefit to supporting witness pubkeys for withdrawals?)
wouldn't be too much work for me. The downside is that people might want
to withdraw to multisig scripts, or any other legitimate P2SH. If it
prevents a huge issue, then it is probably worth it.



I'm curious if anyone on this list could help answer this.

Thanks!



-------------------------------------
 - N \log_2 \epsilon * 1.44

N = 41000 blocks
epsilon = 1/41000 (fp rate)

= 904689.8bits

~ 1 MB


On Thu, Jul 28, 2016 at 5:07 PM, Leo Wandersleb via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thursday, 30 March 2017 22:51:45 CEST Jared Lee Richardson wrote:

No, there is a lot you and I can do about it. They call it a fee market for 
a reason because you can take your money elsewhere. You can choose to not 
make the transfer at all, use another crypto or just use fiat.

Bitcoin has value because we use it as money, supporess that usecase and the 
value of it goes down.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------

Thanks for the clarification.  How would a tx specify a constraint like
"nForkId>=1"?  I was thinking of it just as a number set on the tx.

Also note that since forks form a partial order, but IDs (numbers) form a
total order, ">=" will miss some cases.  Eg, suppose BCH had forked with
nForkId 2, and then you set up a LN funding tx on BCH with nForkId>=2, and
then Segwit2x forked (from BTC!) with nForkId 3.  The BCH funding tx would
be valid on Segwit2x.  This is more of a fundamental problem than a bug -
to avoid it you'd have to get into stuff like making each fork reference
its parent-fork's first block or something, someone has written about
this...


On Mon, Nov 13, 2017 at 5:03 AM, Mats Jerratsch <mats@blockchain.com> wrote:

-------------------------------------
Ethically, this situation has some similarities to the DAO fork. We have an entity who closely examined the code, found an unintended characteristic of that code, and made use of that characteristic in order to gain tens of millions of dollars. Now that developers are aware of it, they want to modify the code in order to negate as much of the gains as possible.

There are differences, too, of course: the DAO attacker was explicitly malicious and stole Ether from others, whereas Bitmain is just optimizing their hardware better than anyone else and better than some of us think they should be allowed to.

In both cases, developers are proposing that the developers and a majority of users collude to reduce the wealth of a single entity by altering the blockchain rules.

In the case of the DAO fork, users were stealing back stolen funds, but that justification doesn't apply in this case. On the other hand, in this case we're talking about causing someone a loss by reducing the value of hardware investments rather than forcibly taking back their coins, which is less direct and maybe more justifiable.

While I don't like patented mining algorithms, I also don't like the idea of playing Calvin Ball on the blockchain. Rule changes should not be employed as a means of disempowering and empoverishing particular entities without very good reason. Whether patenting a mining optimization qualifies as good reason is questionable.
-------------------------------------
Since I have been working on crypto currencies/bitcoin, I kept
repeating: btc should make a priority to significantly increase the
ridiculous number of full nodes we have today, design an incentive for
people to run full nodes and design a system for people to setup full
nodes in an acceptable timeframe

Unfortunately, this was not a priority at all, maybe because of some
historical consensus between miners and devs, so here we are today, some
miners became crazy, the situation would be much more different if more
full nodes were there

Because, how comes everybody perfectly knows the plans of the conspiring
miners? They were stupid enough to explain very precisely how they will
perform the attack?

If I were them I would in addition setup quite a lot of nodes (which
probably they are planning to do, because anyway they need them for the
new sw), not difficult, not so expensive

Defending against abnormal blocks looks to be a non issue, I suppose
that the btc devs perfectly know how to create a pattern based on
history to detect abnormal blocks (including some strange transactions)
and reject them, but this further depends on the ability of current full
nodes to upgrade, which apparently is not what they do the best

I don't know what "Time is running short I fear" stands for and when 50%
is supposed to be reached

Given that it looks difficult to quickly increase the number of full
nodes, that increasing the mining power by standard means looks too
expensive, useless and not profitable, that a counter attack based on a
new proof of something does not look to be ready, then maybe the btc
folks should ask Bram Cohen (who by some luck is participating to this
list) to resurrect the bitcoin miner Epic Scale which Bittorrent Inc (in
an umpteenth dubious attempt to make money) tried some time ago to
include quietly in utorrent forgetting to ask the authorization of the
selected users, then utorrent users might upgrade (potentially 150 M),
and the resulting mining power might be sufficient, depending on the
incentive for this, which is TBD

Or activate by anticipation proof of space... unlike bitcoin-qt,
utorrent sw is quite good to be intrusive, run in background when you
think you have closed it, run things you don't know, etc, so quite
efficient in this situation

Then if btc folks wants to promote full nodes too, it would not be a bad
idea to put the bitcoin-qt blockchain + chain state in a torrent making
sure it is seeded correctly (there are plenty of academics here, they
can do it and run full nodes too) so people can download it and setup
full nodes (incentive TBD too) assuming they know about upnp/port forwarding


Le 24/03/2017  17:03, CANNON via bitcoin-dev a crit :
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
Reposting /u/BashCo's post on reddit here, for visibility:

---8<---------------------------------------------------------------

science term, here's a list of homonyms [https://en.wikipedia.org/
wiki/List_of_true_homonyms] that you use every day. Homonyms are fine
because our brains are able to interpret language based on context, so it's
a non-argument.


This ignores the fact that there exists multiple meanings of bits *within
the same context*, and that beginners likely can't tell them apart.

Feel free to try it yourself - talk about Bitcoin "bits" of a particular
value with somebody who  doesn't understand Bitcoin. Then explain that the
cryptography uses 256 bit keys. I would be surprised if you could find
somebody who would not be confused by that.

Let's say a website says a song is 24 bits. Was that 24 bit audio
resolution or 24 bit price? Somebody writes about 256 bit keys, are that
their size or value?

You guys here can probably tell the difference. Can everybody...? Bits will
cause confusion, because plenty of people will not be able to tell these
apart. They will not know WHEN to apply one definition or the other.

https://www.reddit.com/r/bitcoin/comments/24m3nb/_/ch8gua7
-------------------------------------
@Russell: Appreciate the historical note, but as that op code was
simultaneously disabled in that patch I don't think we can look back to how
it was non-functionally changed (that number means nothing... maybe Satoshi
was trying it out with 520 bytes but then just decided to all-out disable
it and accidentally included that code change? Hard to say what the intent
was.).

@Jorge:
That's one part of it that is worth hesitation and consideration. I'm not a
fan of the 520 byte limit as well. My gut feeling is that the "right"
answer is to compute the memory weight of the entire stack before/after
each operation and reasonably bound it.

Below text is from the chain core documentation:

"""
Most instructions use only the data stack by removing some items and then
placing some items back on the stack. For these operations, we define the
standard memory cost applied as follows:

Instruction’s memory cost value is set to zero.
For each item removed from the data stack, instruction’s memory cost is
decreased by 8+L where L is the length of the item in bytes.
For each item added to the data stack the cost is increased by 8+L where L
is the length of the item in bytes.
​----​
Every instruction has a cost that affects VM run limit. Total instruction
cost consists of execution costand memory cost. Execution cost always
reduces remaining run limit, while memory usage cost can be refunded
(increasing the run limit) when previously used memory is released during
VM execution.
"""

​Is there a reason to favor one approach over the other? I think one reason
to favor a direct limit on op_cat is it favors what​
​
​ I'll dub "context free" analysis, where the performance doesn't depend on
what else is on the stack (perhaps by passing very large arguments to a
script you can cause bad behavior with a general memory limit?).​ On the
other hand, the reason I prefer the general memory limit is it solves the
problem for all future memory-risky opcodes (or present day memory risks!).
Further, OP_CAT is also a bit leaky, in that you could be catting onto a
passed in large string.  The chief argument I'm aware of against a general
memory limit argument is that it is tricky to make a non-implementation
dependent memory limit (e.g., can't just call DynamicMemoryUsage on the
stack), but I don't think this is a strong argument for several
(semi-obvious? I can go into them if need be) reasons.


--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>

On Wed, Jan 4, 2017 at 9:45 AM, Jorge Timón <jtimon@jtimon.cc> wrote:

-------------------------------------
By magic I meant that that it happens all by itself without any extra
configuring.

Thank you for your responses. I have been enlightened. As ZmnSCPxj has
pointed out lightning network and pruning accomplishes everything I set out
to accomplish. And sharding is exactly what I had in mind. I will keep this
in the back of my mind and perhaps even attempt will implement it if it
still seems worth doing later.

You guys are totally awesome!!!

I here by withdraw my proposal for the time being.

-patrick

On Mon, Sep 25, 2017 at 6:35 PM, ZmnSCPxj <ZmnSCPxj@protonmail.com> wrote:

-------------------------------------
Excuse me, yes, for previously-signed transactions this is required. We might consider some limits on UTXO-chain-from-before-the-fork-length and likely something like move towards only allowing one transaction per block from the old mode over time.

I highly disagree that compatibility with existing transaction signing software should be considered (but for hardware which cannot be upgraded easily we do need to consider it). Wallets which can upgrade should, as much as possible, upgrade to a new form to maximize chain divergence and are going to end up having to upgrade to know a new header format anyway, so am extra few lines of code to change a transaction version should be trivial.

On January 26, 2017 12:21:37 PM EST, Gavin Andresen <gavinandresen@gmail.com> wrote:
-------------------------------------
I do not know why people make the leap that the proposal requires a consensus on the transaction pool. It does not.


It may be helpful to have the discussion from the previous thread linked here.

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015370.html


Where I speak of validating that a block conforms to the broadcast next block size, I do not propose validating the number broadcast for the next block size itself, only that the next generated block is that size.


Regards,

Damian Williamson


________________________________
From: Damian Williamson <willtech@live.com.au>
Sent: Saturday, 16 December 2017 7:59 AM
To: Rhavar
Cc: Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks


There are really two separate problems to solve.


  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?


Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.



I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.


Not a necessary function, just an effect of using a probability-based distribution.


I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.

It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.

Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.


I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.


If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.


Regards,

Damian Williamson


________________________________
From: Rhavar <rhavar@protonmail.com>
Sent: Saturday, 16 December 2017 3:38 AM
To: Damian Williamson
Cc: Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks


Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)

But just some quick notes:

* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.

* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.

* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.

If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.




-Ryan


-------- Original Message --------
Subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
Local Time: December 15, 2017 3:42 AM
UTC Time: December 15, 2017 9:42 AM
From: bitcoin-dev@lists.linuxfoundation.org
To: Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>




I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?

I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.

It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.



If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?

Regards,
Damian Williamson


________________________________

From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of Damian Williamson via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Friday, 8 December 2017 8:01 AM
To: bitcoin-dev@lists.linuxfoundation.org
Subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks



Good afternoon,

The need for this proposal:

We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.

I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.

I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.

Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.

Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.

Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.

I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.

I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.

This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.

I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.

# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

## The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.

The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.

Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.

Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.

The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.

## Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.

Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.

The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?

**Explanation of the operation of priority:**


I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.

## Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.

## Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.

## Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.

1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.

## Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.

I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.

Regards,
Damian Williamson


-------------------------------------
mine blocks every 10 minutes.

Presumably a POW hard fork would be accompanied by a difficulty reset, so that the deployment didn't have *both* of these problems from the outset.  There's really little difference between 10 minutes at little/no security and zero conf. See testnet. But people might feel better about still seeing blocks.

But in any case it's not clear to me why you assume a loss of security in the "short term" is something that can be overcome. The short term can presumably prevent the long term from ever becoming.

e


-------------------------------------
I think there may be merit to this idea, allowing for political compromise without sacrificing the technological integrity of Bitcoin. There are a few mechanical problems I see with it, though.

1. It should change its activation logic from BIP9-style to BIP8-style with a flagday of August 1. This to maintain backwards compatibility with the current deployment of BIP148 nodes. This proposal seems to be a measure to prevent a chainsplit, so it must make sure to avoid triggering one.

2. This should be for miners only; non-miners should not enforce this. It severely weakens the block-signalling activation mechanism in several ways (lowered threshold, short deployment timeframe, no "locked in" delay before activation) and by doing so opens up attack vectors for consensus-partitioning attacks using malicious false signalling. For non-miners that seek to take their fate into their own hands, enforcing BIP148 is enough.

3. Even for miners this is more risky than usual; only 31% of hashrate is required to false-signal the activation to fork-off honest miners. This attack vector is magnified by the lack of "locked in" delay that would allow laggards to upgrade before activation. I suggest adding in at least a 1-week lock-in period (given the shorter timeframes 2 weeks may eat up too much of the available voting time before the brick wall of BIP148 activation on August 1).

Under the assumption that this is indeed compatible with the terms of the Silbert agreement, we can presume the involved miners are willing to trust eachother more than usual so such a short lock-in period should be acceptable.

-------- Original Message --------
Subject: [bitcoin-dev] Reduced signalling threshold activation of existing segwit deployment
Local Time: May 23, 2017 1:40 AM
UTC Time: May 22, 2017 10:40 PM
From: bitcoin-dev@lists.linuxfoundation.org
To: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>

I would like to propose an implementation that accomplishes the first
part of the Barry Silbert proposal independently from the second:

"Activate Segregated Witness at an 80% threshold, signaling at bit 4"
in a way that

The goal here is to minimize chain split risk and network disruption
while maximizing backwards compatibility and still providing for rapid
activation of segwit at the 80% threshold using bit 4.

By activating segwit immediately and separately from any HF we can
scale quickly without risking a rushed combined segwit+HF that would
almost certainly cause widespread issues.

Draft proposal:
https://github.com/jameshilliard/bips/blob/bip-segsignal/bip-segsignal.mediawiki

Proposal text:
<pre>
BIP: segsignal
Layer: Consensus (soft fork)
Title: Reduced signalling threshold activation of existing segwit deployment
Author: James Hilliard <james.hilliard1@gmail.com>
Status: Draft
Type: Standards Track
Created: 2017-05-22
License: BSD-3-Clause
CC0-1.0
</pre>

==Abstract==

This document specifies a method to activate the existing BIP9 segwit
deployment with a majority hashpower less than 95%.

==Definitions==

"existing segwit deployment" refer to the BIP9 "segwit" deployment
using bit 1, between November 15th 2016 and November 15th 2017 to
activate BIP141, BIP143 and BIP147.

==Motivation==

Segwit increases the blocksize, fixes transaction malleability, and
makes scripting easier to upgrade as well as bringing many other
[https://bitcoincore.org/en/2016/01/26/segwit-benefits/ benefits].

This BIP provides a way for a simple majority of miners to coordinate
activation of the existing segwit deployment with less than 95%
hashpower. For a number of reasons a complete redeployment of segwit
is difficulty to do until the existing deployment expires. This is due
to 0.13.1+ having many segwit related features active already,
including all the P2P components, the new network service flag, the
witness-tx and block messages, compact blocks v2 and preferential
peering. A redeployment of segwit will need to redefine all these
things and doing so before expiry would greatly complicate testing.

==Specification==

While this BIP is active, all blocks must set the nVersion header top
3 bits to 001 together with bit field (1<<1) (according to the
existing segwit deployment). Blocks that do not signal as required
will be rejected.

==Deployment==

This BIP will be deployed by a "version bits" with an 80%(this can be
adjusted if desired) activation threshold BIP9 with the name
"segsignal" and using bit 4.

This BIP will have a start time of midnight June 1st, 2017 (epoch time
1496275200) and timeout on midnight November 15th 2017 (epoch time
1510704000). This BIP will cease to be active when segwit is
locked-in.

=== Reference implementation ===

<pre>
// Check if Segregated Witness is Locked In
bool IsWitnessLockedIn(const CBlockIndex* pindexPrev, const
Consensus::Params& params)
{
LOCK(cs_main);
return (VersionBitsState(pindexPrev, params,
Consensus::DEPLOYMENT_SEGWIT, versionbitscache) ==
THRESHOLD_LOCKED_IN);
}

// SEGSIGNAL mandatory segwit signalling.
if ( VersionBitsState(pindex->pprev, chainparams.GetConsensus(),
Consensus::DEPLOYMENT_SEGSIGNAL, versionbitscache) == THRESHOLD_ACTIVE
&&
!IsWitnessLockedIn(pindex->pprev, chainparams.GetConsensus()) &&
// Segwit is not locked in
!IsWitnessEnabled(pindex->pprev, chainparams.GetConsensus()) ) //
and is not active.
{
bool fVersionBits = (pindex->nVersion & VERSIONBITS_TOP_MASK) ==
VERSIONBITS_TOP_BITS;
bool fSegbit = (pindex->nVersion &
VersionBitsMask(chainparams.GetConsensus(),
Consensus::DEPLOYMENT_SEGWIT)) != 0;
if (!(fVersionBits && fSegbit)) {
return state.DoS(0, error("ConnectBlock(): relayed block must
signal for segwit, please upgrade"), REJECT_INVALID, "bad-no-segwit");
}
}
</pre>

https://github.com/bitcoin/bitcoin/compare/0.14...jameshilliard:segsignal-v0.14.1

==Backwards Compatibility==

This deployment is compatible with the existing "segwit" bit 1
deployment scheduled between midnight November 15th, 2016 and midnight
November 15th, 2017. Miners will need to upgrade their nodes to
support segsignal otherwise they may build on top of an invalid block.
While this bip is active users should either upgrade to segsignal or
wait for additional confirmations when accepting payments.

==Rationale==

Historically we have used IsSuperMajority() to activate soft forks
such as BIP66 which has a mandatory signalling requirement for miners
once activated, this ensures that miners are aware of new rules being
enforced. This technique can be leveraged to lower the signalling
threshold of a soft fork while it is in the process of being deployed
in a backwards compatible way.

By orphaning non-signalling blocks during the BIP9 bit 1 "segwit"
deployment, this BIP can cause the existing "segwit" deployment to
activate without needing to release a new deployment.

==References==

*[https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013714.html
Mailing list discussion]
*[https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283
P2SH flag day activation]
*[[bip-0009.mediawiki|BIP9 Version bits with timeout and delay]]
*[[bip-0016.mediawiki|BIP16 Pay to Script Hash]]
*[[bip-0141.mediawiki|BIP141 Segregated Witness (Consensus layer)]]
*[[bip-0143.mediawiki|BIP143 Transaction Signature Verification for
Version 0 Witness Program]]
*[[bip-0147.mediawiki|BIP147 Dealing with dummy stack element malleability]]
*[[bip-0148.mediawiki|BIP148 Mandatory activation of segwit deployment]]
*[[bip-0149.mediawiki|BIP149 Segregated Witness (second deployment)]]
*[https://bitcoincore.org/en/2016/01/26/segwit-benefits/ Segwit benefits]

==Copyright==

This document is dual licensed as BSD 3-clause, and Creative Commons
CC0 1.0 Universal.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Sounds like demurrage to me, which has been implemented in other
cryptocurrencies such as Freicoin - http://freico.in/

While it's an interesting to apply this line of thinking from a scaling
perspective, I suspect most would find it untenable from a monetary policy
perspective.

You have touched on a scaling issue, the cost of node operation, that I
think is really the root cause of a lot of the debate. Thus even if your
proposal was implemented, we'd still have to solve the problem of finding a
consensus for CONOP.

I think you may have misapplied your logic to the total size of the
blockchain, however. Are you suggesting that pruning of the old UTXOs would
also enable pruning of old blocks from all nodes? Those things aren't
really related, plus someone would still have to keep old blocks around in
order to facilitate new nodes syncing from genesis.

- Jameson

On Fri, Jul 21, 2017 at 3:28 PM, Major Kusanagi via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
You launched the political football by coming here with a verbose
'recommendation'. Without a code submission in form of pull request to
the core repo on github this was never a technical discussion.

On Thu, 2017-11-02 at 19:53 -0400, Scott Roberts via bitcoin-dev wrote:



-------------------------------------
Den 1 apr. 2017 16:35 skrev "Eric Voskuil via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org>:

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 03/31/2017 11:18 PM, Jared Lee Richardson wrote:

"Governments are good at cutting off the heads of a centrally
controlled networks..."


That's what's so great about Bitcoin. The blockchain is the same
everywhere.

So if you can connect to private peers in several jurisdictions, chances
are they won't all be lying to you in the exact same way. Which is what
they would need to do to fool you.

If you run your own and can't protect it, they'll just hack your node and
make it lie to you.
-------------------------------------
On 7/17/2017 5:47 PM, David A. Harding wrote:
I meant only to convey that the document would appear on bitcoincore.org
iff the PR were ultimately accepted. In other words, while it is up to
"the community of Core Contributors" in a philosophical sense, it is up
to the maintainers of BitcoinCore.org in a practical sense, because they
are the ones who ultimately decide if the standard has been met.

I think it is perfectly reasonable to keep site-updates narrowly
organized in the GitHub PR sphere (and to ignore everything else).

I appreciate you saying that. Thank you.

-Paul






-------------------------------------
Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:

I do prefer the (2) approach, BTW, as it reuses existing primitives, but
I know "simpler" means a different thing to mathier brains :)

Since it wasn't explicit in the proposal, I think the txout information
placed in the hash here is worth discussing.

I prefer a simple txid||outnumber[1], because it allows simple validation
without knowing the UTXO set itself; even a lightweight node can assert
that UTXOhash for block N+1 is valid if the UTXOhash for block N is
valid (and vice versa!) given block N+1.  And miners can't really use
that even if they were to try not validating against UTXO (!) because
they need to know input amounts for fees (which are becoming
significant).

If I want to hand you the complete validatable UTXO set, I need to hand
you all the txs with any unspent output, and some bitfield to indicate
which ones are unspent.

OTOH, if you serialize more (eg. ...||amount||scriptPubKey ?), then the UTXO
set size needed to validate the utxohash is a little smaller: you need
to send the txid, but not the tx nVersion, nLocktime or inputs.  But in a
SegWit world, that's actually *bigger* AFAICT.

Thanks,
Rusty.

[1] I think you could actually use txid^outnumber, and if that's not a
    curve point SHA256() again, etc.  But I don't think that saves any
    real time, and may cause other issues.

-------------------------------------
With a credit card you have an institution worth billions of dollars
asserting that a payment has been made, with the option that it may be
retracted under special circumstances by the card issuer.

Unconfirmed Bitcoin transactions with a SPV client has you trusting
that the un-authenticated DNS seed lookup has not been tampered with,
the connection to the random node that you connect to has not been
tampered with, and that the seed nor the node are attempting to
manipulate you.

The two scenarios aren’t even remotely comparable.


On 2017-01-04 00:56, Aaron Voisine wrote:

-------------------------------------
Thanks Mats, this proposal makes sense to me (especially the idea of
fork-specific addresses).  It prevents replay across forks, and makes it
easy for client software, and thus potentially users, to specify which fork
a tx is for.  But, like other (rougher) past proposals I've seen, it does
little to prevent users from accidentally sending on the wrong fork.

Take the specific and common case of non-upgraded wallet software.  Suppose
a HF happens, and becomes the network used by 90% of users.  Will old
wallets still default to the old nForkId (10% legacy chain)?  If so, I'd
expect a lot of accidental mis-sends on that chain.

This is just a gap in your proposal, not a flaw, but it's worth thinking
about less hazard-prone ways wallets could default nForkId.  Perhaps they
could listen to all forks, and default to the one whose last (recent) block
had the highest difficulty?  Or just check those blocks to see if multiple
forks are (nontrivially) active, and if so warn the user and force them to
confirm?  Something like that.


On Nov 6, 2017 7:05 AM, "Mats Jerratsch via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Presented is a generalised way of providing replay protection for future
hard forks. On top of replay protection, this schema also allows for
fork-distinct addresses and potentially a way to opt-out of replay
protection of any fork, where deemed necessary (can be beneficial for some
L2 applications).

## Rationale

Currently when a hard fork happens, there is ad-hoc replay protection built
within days with little review at best, or no replay protection at all.
Often this is either resource problem, where not enough time and developers
are available to sufficiently address replay protection, or the idea that
not breaking compatibility is favourable. Furthermore, this is potentially
a recurring problem with no generally accepted solution yet. Services that
want to deal in multiple forks are expected to closely follow all projects.
Since there is no standard, the solutions differ for each project,
requiring custom code for every fork. By integrating replay protection into
the protocol, we advocate the notion of non-hostile forks.

Users are protected against accidentally sending coins on the wrong chain
through the introduction of a fork-specific incompatible address space. The
coin/token type is encoded in the address itself, removing some of the
importance around the question _What is Bitcoin?_. By giving someone an
address, it is explicitly stated _I will only honour a payment of token X_,
enforcing the idea of validating the payment under the rules chosen by the
payee.

## Iterative Forks

In this schema, any hard fork is given an incremented id, `nForkId`.
`nForkId` starts at `1`, with `0` being reserved as a wildcard. When
project X decides to make an incompatible change to the protocol, it will
get assigned a new unique `nForkId` for this fork. A similar approach like
for BIP43 can be taken here. Potentially `nForkId` can be reused if a
project has not gained any amount of traction.

When preparing the transaction for signing or validation, `nForkId` is
appended to the final template as a 4B integer (similar to [1]). Amending
BIP143, this would result in

```
Double SHA256 of the serialization of:
    1. nVersion of the transaction (4-byte little endian)
    2. hashPrevouts (32-byte hash)
    3. hashSequence (32-byte hash)
    4. outpoint (32-byte hash + 4-byte little endian)
    5. scriptCode of the input (serialized as scripts inside CTxOuts)
    6. value of the output spent by this input (8-byte little endian)
    7. nSequence of the input (4-byte little endian)
    8. hashOutputs (32-byte hash)
    9. nLocktime of the transaction (4-byte little endian)
   10. sighash type of the signature (4-byte little endian)
   11. nForkId (4-byte little endian)
```


For `nForkId=0` this step is ommitted. This will immediately invalidate
signatures for any other branch of the blockchain than this specific fork.
To distinguish between `nForkId=0` and `nForkId` hardcoded into the
software, another bit has to be set in the 1B SigHashId present at the end
of signatures.

To make this approach more generic, payment addresses will contain the fork
id, depending on which tokens a payee expects payments in. This would
require a change on bech32 addresses, maybe to use a similar format used in
lightning-rfc [2]. A wallet will parse the address, it will extract
`nForkId`, and it displays which token the user is about to spend. When
signing the transaction, it will use `nForkId`, such that the transaction
is only valid for this specific token. This can be generalised in software
to the point where replay protection *and* a new address space can be
introduced for forks without breaking existing clients.

For light clients, this can be extended by enforcing the coinbase/block
header to contain the `nForkId` of the block. Then the client can
distinguish between different chains and tokens it received on each.
Alternatively, a new P2P message type for sending transactions could be
introduced, where prevOut and `nForkId` is transmitted, such that the lite
client can check for himself, which token he received.

Allowing signatures with `nForkId=1` can be achieved with a soft fork by
incrementing the script version of SegWit, making this a fully backwards
compatible change.

[1]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/
2017-February/013542.html

[2]
https://github.com/lightningnetwork/lightning-rfc/blob/master/11-payment-
encoding.md

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
I have written a height based reference implementation as well as updated the BIP text in the following proposals

"lockinontimeout" was just an implementation detail to allow BIP8 the BIP9 implementation code. With the change to height based, we can dispense with it entirely.
So the two changes BIP8 brings is BIP9 modified to use height not time, and remove the veto failed state.
Code: https://github.com/bitcoin/bitcoin/compare/master...shaolinfry:bip8-height
BIP: https://github.com/bitcoin/bips/compare/master...shaolinfry:bip8-height

-------------------------------------
Over the past few weeks I've been explaining the MERKLEBRANCHVERIFY
opcode and tail-call execution semantics to a variety of developers,
and it's come to my attention that the BIPs presentation of the
concept is not as clear as it could be. Part of this is the fault of
standards documents being standards documents whose first and foremost
responsibility is precision, not pedagogy.

I think there's a better way to explain this approach to achieving
MAST, and it's worked better in the face to face and whiteboard
conversations I've had. I'm forwarding it to this list in case there
are others who desire a more clear explanation of what the
MERKLEBRANCHVERIFY and tail-call BIPs are trying to achieve, and what
any of it has to do with MAST / Merklized script.

I've written for all audiences so I apologize if it starts of at a
newbie level, but I encourage you to skim, not skip as I quickly start
varying this beginner material in atypical ways.


Review of P2SH

It's easiest to explain the behavior and purpose of these BIPs by
starting with P2SH, which we are generalizing from. BIP 16 (Pay to
Script Hash) specifies a form of implicit script recursion where a
redeem script is provided in the scriptSig, and the scriptPubKey is a
program that verifies the redeem script hashes to the committed value,
with the following template:

  HASH160 <hash> EQUAL

This script specifies that the redeem script is pulled from the stack,
its hash is compared against the expected value, and by fiat it is
declared that the redeem script is then executed with the remaining
stack items as arguments.

Sortof. What actually happens of course is that the above scriptPubKey
template is never executed, but rather the interpreter sees that it
matches this exact template format, and thereby proceeds to carry out
the same logic as a hard-coded behavior.


Generalizing P2SH with macro-op fusion

This template-matching is unfortunate because otherwise we could
imagine generalizing this approach to cover other use cases beyond
committing to and executing a single redeem script. For example, if we
instead said that anytime the script interpreter encountered the
3-opcode sequence "HASH160 <20-byte-push> EQUAL" it switched to
interpreting the top element as if it were a script, that would enable
not just BIP 16 but also constructs like this:

  IF
    HASH160 <hash-1> EQUAL
  ELSE
    HASH160 <hash-2> EQUAL
  ENDIF

This script conditionally executes one of two redeem scripts committed
to in the scriptPubKey, and at execution only reveals the script that
is actually used. All an observer learns of the other branch is the
script hash. This is a primitive form of MAST!

The "if 3-opcode P2SH template is encountered, switch to subscript"
rule is a bit difficult to work with however. It's not a true EVAL
opcode because control never returns back to the top-level script,
which makes some important aspects of the implementation easier, but
only at the cost of complexity somewhere else. What if there are
remaining opcodes in the script, such as the ELSE clause and ENDIF in
the script above?  They would never be executed, but does e.g. the
closing ENDIF still need to be present? Or what about the standard
pay-to-pubkey-hash "1Address" output:

  DUP HASH160 <20-byte-key-hash> EQUALVERIFY CHECKSIG

That almost looks like the magic P2SH template, except there is an
EQUALVERIFY instead of an EQUAL. The script interpreter should
obviously not treat the pubkey of a pay-to-pubkey-hash output as a
script and recurse into it, whereas it should for a P2SH style
script. But isn't the distinction kinda arbitrary?

And of course the elephant in the room is that by choosing not to
return to the original execution context we are no longer talking
about a soft-fork. Work out, for example, what will happen with the
following script:

  [TRUE] HASH160 <hash-of-[TRUE]> EQUAL FALSE

(It returns false on a node that doesn't understand generalized
3-opcode P2SH recursion, true on a node that does.)


Implicit tail-call execution semantics and P2SH

Well there's a better approach than trying to create a macro-op fusion
franken-EVAL. We have to run scripts to the end to for any proposal to
be a soft-fork, and we want to avoid saving state due to prior
experience of that leading to bugs in BIP 12. That narrows our design
space to one option: allow recursion only as the final act of a
script, as BIP 16 does, but for any script not just a certain
template. That way we can safely jump into the subscript without
bothering to save local state because termination of the subscript is
termination of the script as a whole. In computer science terms, this
is known as tail-call execution semantics.

To illustrate, consider the following scriptPubKey:

  DUP HASH160 <20-byte-hash> EQUALVERIFY

This script is almost exactly the same as the P2SH template, except
that it leaves the redeem script on the stack rather than consuming
it, thanks to the DUP, while it _does_ consume the boolean value at
the end because of the VERIFY. If executed, it leaves a stack exactly
as it was, which we assume will look like the following::

  <argN> ... <arg1> <redeemScript>

Now a normal script is supposed to finish with just true or false on
the stack. Any script that finishes execution with more than a single
element on the stack is in violation of the so-called clean-stack rule
and is considered non-standard -- not relayable and potentially broken
by future soft-fork upgrades. But so long as at least one bit of
<redeemScript> is set, it is interpreted as true and the script
interpreter would normally interpret a successful validation at this
point, albeit with a clean-stack violation.

Let's take advantage of that by changing what the script interpreter
does when a script finishes with multiple items remaining on the stack
and top-most one evaluates as true -- a state of affairs that would
pass validation under the old rules. Now instead the interpreter
treats the top-most item on the stack as a script, and tail-call
recurse into it, P2SH-style. In the above example, <redeemScript> is
popped off the stack and is executed with <arg1> ... <argN> remaining
on the stack as its arguments.

The above script can be interpreted in English as "Perform tail-call
recursion if and only if the HASH160 of the script on the top of the
stack exactly matches this 20-byte push." Which is, of course, what
BIP 16 accomplishes with template matching. However the implicit tail
call approach allows us to do much more than just P2SH!

For starters, it turns out that using HASH160 for P2SH was probably a
bad idea as it reduces the security of a multi-party constructed hash
to an unacceptable 80 bits. That's why segwit uses 256-bit hashes for
its pay to script hash format, for 128-bit security. Had we tail call
semantics instead of BIP 16, we could have just switched to a new
address type that decodes to the following script template instead:

  DUP HASH256 <32-byte-hash> EQUALVERIFY

Ta-da, we're back to full 128-bit security with no changes to the
consensus code, just a new address version to target this script
template.


MAST with tail-call alone?
Or: an aside on general recursion

Our IF-ELSE Merklized Abstract Syntax Tree example above, rewritten to
use tail-call evaluation, might look like this (there are more compact
formulations possible, but our purpose here is not code golf):

  IF
    DUP HASH160 <hash-1> EQUALVERIFY
  ELSE
    DUP HASH160 <hash-2> EQUALVERIFY
  ENDIF

Either execution pathway leaves us with one of the two allowed redeem
scripts on the top of the stack, and presumably its arguments beneath
it. We then execute that script via implicit tail-call.

We could write scripts using IF-ELSE branches or other tricks to
commit to more than two possible branches, although this unfortunately
scales linearly with the number of possible branches. If we allow the
subscript itself to do its own tail-call recursion, and its subscript
and so on, then we could nest these binary branches for a true MAST in
the original sense of the term.

However in doing so we would have enabled general recursion and
inherit all the difficulties that come with that. For example, some
doofus could use a script that consists of or has the same effect as a
single DUP to cause an infinite loop in the script interpreter. And
that's just the tip of the iceberg of problems general recursion can
bring, which stem generally from resource usage no longer being
correlated with the size of the witness stack, which is the primary
resource for which there are global limits.

This is fixable with a gas-like resource accounting scheme, which
would affect not just script but also mempool, p2p, and other
layers. And there is perhaps an argument for doing so, particularly as
part of a hard-fork block size increase as more accurate resource
accounting helps prevent many bad-block attacks and let us set
adversarial limits closer to measured capacity in the expected/average
use case. But that would immensely complicate things beyond what could
achieve consensus in a reasonably short amount of time, which is a
goal of this proposal.

Instead I suggest blocking off general recursion by only allowing the
script interpreter to do one tail-call per input. To get log-scaling
benefits without deep recursion we introduce instead one new script
feature, which we'll cover in the next section. But we do leave the
door open to possible future general recursion, as we will note that
going from one layer of recursion to many would itself be a soft-fork
for the same reason that the first tail-call recursion is.


Merkle branch verify to the rescue!

In #bitcoin-wizards and elsewhere there has been a desire for some
time to have an opcode that proves that an item was drawn from the set
used to construct a given Merkle tree. This is not a new idea although
I'm not aware of any actual proposal made for it until now. The most
simple version of the opcode, the version initially proposed, takes
three arguments:

  <proof> <leaf-hash> <root-hash> MERKLEBRANCHVERIFY 2DROP DROP

<root-hash> is the 32-byte hash label of the root of the Merkle tree,
calculated using a scheme defined in the fast Merkle hash tree BIP.

<leaf-hash> is 32 bytes of data which we are proving is part of the
Merkle hash tree -- usually the double-SHA256 hash of an item off the
stack.

<proof> is the path through the Merkle tree including the hashes of
branches not taken, which is the information necessary to recalculate
the root hash thereby proving that <leaf-hash> is in the Merkle tree.

The 2DROP and DROP are necessary to remove the 3 arguments from the
stack, as the opcode cannot consume them since it is soft-forked in.
There are two primary motivating applications of Merkle branch verify
(MBV), which will be covered next.

The MBV BIP will be extended to support extraction of more than one
item from the same Merkle tree, but for the rest of this explanation
we assume the current implementation of a single item proof, just for
simplicity.


MBV and MAST

This new opcode combines with single tail-call execution semantics to
allow for a very short and succinct MAST implementation:

  OVER HASH256 <root-hash> MERKLEBRANCHVERIFY 2DROP DROP

That's it. This script expects an input stack in the following format:

  <argN> ... <arg1> <policyScript> <proof>

At the end of execution the script has verified that <policyScript> is
part of the Merkle tree previously committed to, and <proof> is
dropped from the stack. This leaves the stack ready for a tail-call
recursion into <policyScript>.


MBV and Key Aggregation

If the signature scheme supports key aggregation, which it happens
that the the new signature aggregation scheme being worked on will
support as a side effect, then there is a very cool and useful
application that would be supported as well: tree signatures as
described by Pieter Wuille[1].  This looks almost exactly the same as
the MAST script, but with a CHECKSIG tacked on the end:

  OVER HASH256 <root-hash> MERKLEBRANCHVERIFY 2DROP DROP CHECKSIG

This script expects an input stack of the following form:

  <sig> <pubkey> <proof>

And proves that the pubkey is drawn from the set used to construct the
Merkle hash tree, and then its signature is checked. While it should
be clear this has 1-of-N semantics, what might be less obvious is that
key aggregation allows any signature policy expressible as a monotone
Boolean function (anything constructible with combinations of AND, OR,
and k-of-N thresholds) to be transformed to a 1-of-N over a set of key
aggregates. So the above script is a generic template able to verify
any monotone Boolean function over combinations of pubkeys, which
encompasses quite a number of use cases!

[1] https://blockstream.com/2015/08/24/treesignatures.html


An argument for permission-less innovation

The driving motivation for the tail-call and MBV proposals, and the
reason they are presented and considered together is to enable
Merklized Abstract Syntax Trees. However neither BIP actually defines
a MAST template, except as an example use case of the primitive
features. This is very much on purpose: it is the opinion of this
author that new bitcoin consensus features should follow the UNIX
philosophy as expressed by Peter Salus and Mike Gancarz and
paraphrased by yours truly:

  * Write features that do one thing and do it well.
  * Write features to work together.
  * Simple is beautiful.

By using modularity and composition of powerful but simple tools like
MERKLEBRANCHVERIFY and single tail-call recursion to construct MAST we
enable a complex and desirable feature while minimizing changes to the
consensus code, review burden, and acquired technical debt.

The reusability of the underlying primitives also means that they can
be combined with other modular features to support use cases other
than vanilla MAST, or reused in series to work around various limits
that might otherwise be imposed on a templated form of MAST. At the
moment the chief utility of these proposals is the straightforward
MAST script written above, but as primitives they do allow a few other
use cases and also combine well with features in the pipeline or on
the drawing board. For example, in addition to MAST you can:

1. Use MERKLEBRANCHVERIFY alone to support honeypot bounties, as
   discussed in the BIP.

2. Use a series of MERKLEBRANCHVERIFY opcodes to verify a branch with
   split proofs to stay under script and witness push limitations.

3. Combine MERKLEBRANCHVERIFY with key aggregation to get
   Wuille-Maxwell tree signatures which support arbitrary signing
   policies using a single, aggregatable signature.

4. Combine tail-call execution semantics with CHECKSIGFROMSTACK to get
   delegation and signature-time commitment to subscript policy.

5. Combine MERKLEBRANCHVERIFY with a Merkle proof prefix check opcode
   and Lamport signature support to get reusable Lamport keys.

I believe these benefits and possible future expansions to be strong
arguments in favor of extending bitcoin in the form of small, modular,
incremental, and reusable changes that can be combined and used even
in ways unforeseen even by their creators, creating a platform for
unrestricted innovation.

The alternative approach of rigid templates achieves the stated goals,
perhaps even with slightly better encoding efficiency, but at the cost
of requiring workaround for each future innovation. P2SH is just such
an example -- we couldn't even upgrade to 128-bit security without
designing an entirely different implementation because of the
limitations of template pattern matching.


Efficiency gains from templating

Finally, I need to point out that there is one efficiency gain that a
proper template-matching implementation has over user-specified
schemes: reduction of witness data. This is both a practical side
effect of more efficient serialization that doesn't need to encode
logic as opcodes, as well as the fact that since the hashing scheme is
fixed, one layer of hashes can be removed from the serialization. In
the case of MAST, rather than encode the Merkle root hash in the
redeem script, the hash is propagated upwards and compared against the
witness commitment. The amount space saved from adopting a template is
about equal to the size of the redeem script, which is approximately
40 bytes of witness data per MAST input.

That is arguably significant enough to matter, and in the long term I
think we will adopt a MAST template for those efficiency gains. But I
feel strongly that since MAST is not a feature in wide use at this
time, it is strongly in our interests to deploy MBV, tail-call, as
well overhaul the CHECKSIG operator before tackling what we feel an
ideal MAST-supporting witness type would look like, so that with some
experience under our belt we can adopt a system that is as simple and
as succinct as possible while supporting the necessary use cases
identified by actual use of the features.

Kind regards,
Mark Friedenbach

-------------------------------------
Without at least a majority hashrate validating blocks, it is possible just a 
single invalid block could split the chain such that the majority continue 
building a most-work on that invalid block.

This failure to validate a softfork is similar in some respects to a hardfork, 
but with one critical difference: the default behaviour of old nodes will be 
to follow the chain with the most-work that was valid under the pre-softfork 
rules. This actually *inverts* the benefit of the softfork over a hardfork, 
and makes a softfork deployed in such a manner de facto behave as if it had 
been a hardfork, IF someone ever mines a single malicious block.

For this reason, I think a minority-hashrate softfork requires a much higher 
degree of social support than merely the widespread agreement typical of 
softforks. It might perhaps require less than the full ~100% consensus 
hardforks require, but it likely comes somewhat close.

Once it gets over 50% hashrate enforcement, however, the situation improves a 
lot more: a malicious block may split obsolete miners off the valid chain, but 
it will eventually resolve on its own given enough time. Due to natural 
fluctuations in block finding, however, automatic measurement may need to look 
for >75%.

So I would suggest that instead of a simple flag day activation, this proposal 
would be improved by changing the flag day to merely reduce the hashrate 
requirement from 95% to 75%.

(In addition to the above concerns, if >50% of miners are hostile to the 
network, we likely have other problems.)

Luke

-------------------------------------
any size (within the current limit), thus triggering the conditions by
which your proposal would raise the limit further.

There might be a long term incentive to keep increasing the blocksize, to
further centralize the network (and kick smaller miners out), but it comes
with the cost of losing out on transaction fees.
Miners have always needed to plan for the short term, I see no rational
scenario where miners would spam their blocks with their own transactions
(or low fee transactions) to keep increasing the blocksize limit.

Hampus
-------------------------------------
I rarely post here, out of respect to the mailing list. But since my name
was mentioned...

I much prefer Gregory Maxwell's proposal to defuse covert ASICBOOST (only)
with a segwit-like commitment to the coinbase which does not obligate
miners to signal Segwit or implement Segwit, thus disarming any suspicion
that the issue is being exploited only to activate Segwit.

This proposal is unnecessarily conflating two contentious issues and will
attract criticism of self serving motivation.

Politicising CVE  is damaging to the long term bitcoin development and to
its security. Not claiming that is the intent here, but the damage is done
by the mere appearance of motive.



On May 26, 2017 16:30, "Cameron Garnham via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
This is the paper detailing the research behind my talk "Optimizing
fee estimation via the mempool state" (the presentation only covers
part of the paper) at Scaling Stanford (this coming Sunday). Feedback
welcome.

https://bc-2.jp/mempool.pdf

-------------------------------------
On Tuesday, 28 March 2017 19:34:23 CEST Johnson Lau via bitcoin-dev wrote:

That was not suggested.

Maybe you can comment on the very specific suggestion instead?

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
If a miner try to hurt the network mining just empty blocks at some time the rest will start rejecting their blocks and will be orphans so will loss the reward incentive and that miner will join the behavior of the rest of the miners, if that miner has 51% of hashrate there the smallest problem are the empty blocks.

-----Original Message-----
From: bitcoin-dev-bounces@lists.linuxfoundation.org [mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Stian Ellingsen via bitcoin-dev
Sent: Monday, March 27, 2017 2:50 PM
To: Btc Ideas <btcideas@protonmail.com>; Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] Encouraging good miners

On 27/03/17 18:12, Btc Ideas via bitcoin-dev wrote:


This would encourage miners to make their own tiny junk transactions to fill up their blocks, perhaps leaving larger, more space-efficient transactions in the mempool.



"Good" miners should probably build upon the block with a set of transactions more similar to what they themselves would include based on their mempool at the time.  However, miners don't have an incentive to do so today.  Instead, they may be better off building upon the block that leaves the most valuable transactions in the mempool, e.g. a small or empty block, and maybe leave some valuable transactions in the mempool for the next miner.[1]  This issue could possibly be addressed by a soft-fork that requires miners to pay a portion of their fees to future miners.

[1]
https://freedom-to-tinker.com/2016/10/21/bitcoin-is-unstable-without-the-block-reward/

--
Stian


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
Try to find 1TB dedicated server hosting ...

If you want to set up an ecommerce site somewhere besides your living room,
storage costs are still a concern.

On Mon, Apr 17, 2017 at 3:11 AM, Danny Thorpe via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On May 28, 2017 06:09, "Russell O'Connor" <roconnor@blockstream.io> wrote:



On May 28, 2017 03:16, "Peter Todd" <pete@petertodd.org> wrote:

On Mon, May 22, 2017 at 06:32:38PM -0400, Russell O'Connor wrote:

Cartesian product can mean a lot of things.

What specifically do you mean by "cartesian product" here?


Oops, I forgot to reply all.  Below is my reply.

Given two types A and B, then A × B is the type of pairs of A and B in the
sense of type theory as used in Standard ML or Haskell or other typed
languages.


To follow up, by "sha256Compress : Word256 × Word512 -> Word256" I mean
that sha256Compress is a function that takes two arguments, the first being
a 256 bit word and the second being a 512 bit word, and returns a 256 bit
word (or equivalently sha256Compress is a function that takes a pair as
input, the first component being a 256 bit word and the second component
being a 512 bit word, and returns a 256 bit word).

sha256Compress is meant to be the compression function defined by the
SHA256 standard, though nothing here depends on anything more that its type
signature.
-------------------------------------

Johnson,

I feel that's not as much of an issue with v0 witness programs. Segwit
isn't activated yet, and segwit-capable wallets aren't as widely
deployed for production. Not to mention, they're all going to require
further development anyway: the address serialization for witness
programs only became a BIP this week. No segwit wallets should ever be
planning to receive money to naked witness programs right now, since
addresses are for it aren't even available.

I think we have the benefit of timing here. The state of segwit wallet
development incidentally creates a window of time where this maturity
rule can be implemented.

On Wed, May 10, 2017 at 01:56:28AM +0800, Johnson Lau wrote:

--
Christopher Jeffrey (JJ) <chjjeffrey@gmail.com>
CTO & Bitcoin Menace, purse.io
https://github.com/chjj
-------------------------------------
relationships with to start including the root hash of the (lagging) UTXO
set in their coinbase transactions, in order to begin transforming this
idea into reality.

By itself, this wouldn't work without a way for a new node to differentiate
between a false history and a true one.

controlled by known people that include the same root hash in an OP_RETURN
output, which would allow cross-checking against the miners’ UTXO
commitments, as part of this initial “prototype”

This might work, but I fail to understand how a new node could verify an
address / transaction without a blockchain to back it.  Even if it could,
it becomes dependent upon those addresses not being compromised, and the
owners of those addresses would become targets for potential government
operations.

Having the software silently attempt to resolve the problem is risky unless
it is foolproof.  Otherwise, users will assume their software is showing
them the correct history/numbers implicitly, and if the change the utxo
attacker made was small, the users might be able to follow the main chain
totally until it was too late and the attacker struck with an address that
otherwise never transacted.  Sudden, bizarre, hard to debug fork and
potentially double spend against people who picked up the fraudulent utxo.

Users already treat wallet software with some level of suspicion, asking if
they can trust x or y or z, or like the portion of the BU community
convinced that core has been compromised by blockstream bigwigs.  Signed
releases could provide the same thing but would encourage both open-source
security checks of the signed utxo's and potentially of users to check
download signatures.

Either approach is better than what we have now though, so I'd support
anything.

On Wed, Mar 29, 2017 at 1:28 PM, Peter R via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
This was very helpful at least to me, thank you

I've been struggling to follow the discussion of tail-call execution and
understand the options for implementing MAST. This clarified everything. I
can now follow the previous discussions.




On Sep 20, 2017 18:56, "Mark Friedenbach via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Over the past few weeks I've been explaining the MERKLEBRANCHVERIFY
opcode and tail-call execution semantics to a variety of developers,
and it's come to my attention that the BIPs presentation of the
concept is not as clear as it could be. Part of this is the fault of
standards documents being standards documents whose first and foremost
responsibility is precision, not pedagogy.

I think there's a better way to explain this approach to achieving
MAST, and it's worked better in the face to face and whiteboard
conversations I've had. I'm forwarding it to this list in case there
are others who desire a more clear explanation of what the
MERKLEBRANCHVERIFY and tail-call BIPs are trying to achieve, and what
any of it has to do with MAST / Merklized script.

I've written for all audiences so I apologize if it starts of at a
newbie level, but I encourage you to skim, not skip as I quickly start
varying this beginner material in atypical ways.


Review of P2SH

It's easiest to explain the behavior and purpose of these BIPs by
starting with P2SH, which we are generalizing from. BIP 16 (Pay to
Script Hash) specifies a form of implicit script recursion where a
redeem script is provided in the scriptSig, and the scriptPubKey is a
program that verifies the redeem script hashes to the committed value,
with the following template:

  HASH160 <hash> EQUAL

This script specifies that the redeem script is pulled from the stack,
its hash is compared against the expected value, and by fiat it is
declared that the redeem script is then executed with the remaining
stack items as arguments.

Sortof. What actually happens of course is that the above scriptPubKey
template is never executed, but rather the interpreter sees that it
matches this exact template format, and thereby proceeds to carry out
the same logic as a hard-coded behavior.


Generalizing P2SH with macro-op fusion

This template-matching is unfortunate because otherwise we could
imagine generalizing this approach to cover other use cases beyond
committing to and executing a single redeem script. For example, if we
instead said that anytime the script interpreter encountered the
3-opcode sequence "HASH160 <20-byte-push> EQUAL" it switched to
interpreting the top element as if it were a script, that would enable
not just BIP 16 but also constructs like this:

  IF
    HASH160 <hash-1> EQUAL
  ELSE
    HASH160 <hash-2> EQUAL
  ENDIF

This script conditionally executes one of two redeem scripts committed
to in the scriptPubKey, and at execution only reveals the script that
is actually used. All an observer learns of the other branch is the
script hash. This is a primitive form of MAST!

The "if 3-opcode P2SH template is encountered, switch to subscript"
rule is a bit difficult to work with however. It's not a true EVAL
opcode because control never returns back to the top-level script,
which makes some important aspects of the implementation easier, but
only at the cost of complexity somewhere else. What if there are
remaining opcodes in the script, such as the ELSE clause and ENDIF in
the script above?  They would never be executed, but does e.g. the
closing ENDIF still need to be present? Or what about the standard
pay-to-pubkey-hash "1Address" output:

  DUP HASH160 <20-byte-key-hash> EQUALVERIFY CHECKSIG

That almost looks like the magic P2SH template, except there is an
EQUALVERIFY instead of an EQUAL. The script interpreter should
obviously not treat the pubkey of a pay-to-pubkey-hash output as a
script and recurse into it, whereas it should for a P2SH style
script. But isn't the distinction kinda arbitrary?

And of course the elephant in the room is that by choosing not to
return to the original execution context we are no longer talking
about a soft-fork. Work out, for example, what will happen with the
following script:

  [TRUE] HASH160 <hash-of-[TRUE]> EQUAL FALSE

(It returns false on a node that doesn't understand generalized
3-opcode P2SH recursion, true on a node that does.)


Implicit tail-call execution semantics and P2SH

Well there's a better approach than trying to create a macro-op fusion
franken-EVAL. We have to run scripts to the end to for any proposal to
be a soft-fork, and we want to avoid saving state due to prior
experience of that leading to bugs in BIP 12. That narrows our design
space to one option: allow recursion only as the final act of a
script, as BIP 16 does, but for any script not just a certain
template. That way we can safely jump into the subscript without
bothering to save local state because termination of the subscript is
termination of the script as a whole. In computer science terms, this
is known as tail-call execution semantics.

To illustrate, consider the following scriptPubKey:

  DUP HASH160 <20-byte-hash> EQUALVERIFY

This script is almost exactly the same as the P2SH template, except
that it leaves the redeem script on the stack rather than consuming
it, thanks to the DUP, while it _does_ consume the boolean value at
the end because of the VERIFY. If executed, it leaves a stack exactly
as it was, which we assume will look like the following::

  <argN> ... <arg1> <redeemScript>

Now a normal script is supposed to finish with just true or false on
the stack. Any script that finishes execution with more than a single
element on the stack is in violation of the so-called clean-stack rule
and is considered non-standard -- not relayable and potentially broken
by future soft-fork upgrades. But so long as at least one bit of
<redeemScript> is set, it is interpreted as true and the script
interpreter would normally interpret a successful validation at this
point, albeit with a clean-stack violation.

Let's take advantage of that by changing what the script interpreter
does when a script finishes with multiple items remaining on the stack
and top-most one evaluates as true -- a state of affairs that would
pass validation under the old rules. Now instead the interpreter
treats the top-most item on the stack as a script, and tail-call
recurse into it, P2SH-style. In the above example, <redeemScript> is
popped off the stack and is executed with <arg1> ... <argN> remaining
on the stack as its arguments.

The above script can be interpreted in English as "Perform tail-call
recursion if and only if the HASH160 of the script on the top of the
stack exactly matches this 20-byte push." Which is, of course, what
BIP 16 accomplishes with template matching. However the implicit tail
call approach allows us to do much more than just P2SH!

For starters, it turns out that using HASH160 for P2SH was probably a
bad idea as it reduces the security of a multi-party constructed hash
to an unacceptable 80 bits. That's why segwit uses 256-bit hashes for
its pay to script hash format, for 128-bit security. Had we tail call
semantics instead of BIP 16, we could have just switched to a new
address type that decodes to the following script template instead:

  DUP HASH256 <32-byte-hash> EQUALVERIFY

Ta-da, we're back to full 128-bit security with no changes to the
consensus code, just a new address version to target this script
template.


MAST with tail-call alone?
Or: an aside on general recursion

Our IF-ELSE Merklized Abstract Syntax Tree example above, rewritten to
use tail-call evaluation, might look like this (there are more compact
formulations possible, but our purpose here is not code golf):

  IF
    DUP HASH160 <hash-1> EQUALVERIFY
  ELSE
    DUP HASH160 <hash-2> EQUALVERIFY
  ENDIF

Either execution pathway leaves us with one of the two allowed redeem
scripts on the top of the stack, and presumably its arguments beneath
it. We then execute that script via implicit tail-call.

We could write scripts using IF-ELSE branches or other tricks to
commit to more than two possible branches, although this unfortunately
scales linearly with the number of possible branches. If we allow the
subscript itself to do its own tail-call recursion, and its subscript
and so on, then we could nest these binary branches for a true MAST in
the original sense of the term.

However in doing so we would have enabled general recursion and
inherit all the difficulties that come with that. For example, some
doofus could use a script that consists of or has the same effect as a
single DUP to cause an infinite loop in the script interpreter. And
that's just the tip of the iceberg of problems general recursion can
bring, which stem generally from resource usage no longer being
correlated with the size of the witness stack, which is the primary
resource for which there are global limits.

This is fixable with a gas-like resource accounting scheme, which
would affect not just script but also mempool, p2p, and other
layers. And there is perhaps an argument for doing so, particularly as
part of a hard-fork block size increase as more accurate resource
accounting helps prevent many bad-block attacks and let us set
adversarial limits closer to measured capacity in the expected/average
use case. But that would immensely complicate things beyond what could
achieve consensus in a reasonably short amount of time, which is a
goal of this proposal.

Instead I suggest blocking off general recursion by only allowing the
script interpreter to do one tail-call per input. To get log-scaling
benefits without deep recursion we introduce instead one new script
feature, which we'll cover in the next section. But we do leave the
door open to possible future general recursion, as we will note that
going from one layer of recursion to many would itself be a soft-fork
for the same reason that the first tail-call recursion is.


Merkle branch verify to the rescue!

In #bitcoin-wizards and elsewhere there has been a desire for some
time to have an opcode that proves that an item was drawn from the set
used to construct a given Merkle tree. This is not a new idea although
I'm not aware of any actual proposal made for it until now. The most
simple version of the opcode, the version initially proposed, takes
three arguments:

  <proof> <leaf-hash> <root-hash> MERKLEBRANCHVERIFY 2DROP DROP

<root-hash> is the 32-byte hash label of the root of the Merkle tree,
calculated using a scheme defined in the fast Merkle hash tree BIP.

<leaf-hash> is 32 bytes of data which we are proving is part of the
Merkle hash tree -- usually the double-SHA256 hash of an item off the
stack.

<proof> is the path through the Merkle tree including the hashes of
branches not taken, which is the information necessary to recalculate
the root hash thereby proving that <leaf-hash> is in the Merkle tree.

The 2DROP and DROP are necessary to remove the 3 arguments from the
stack, as the opcode cannot consume them since it is soft-forked in.
There are two primary motivating applications of Merkle branch verify
(MBV), which will be covered next.

The MBV BIP will be extended to support extraction of more than one
item from the same Merkle tree, but for the rest of this explanation
we assume the current implementation of a single item proof, just for
simplicity.


MBV and MAST

This new opcode combines with single tail-call execution semantics to
allow for a very short and succinct MAST implementation:

  OVER HASH256 <root-hash> MERKLEBRANCHVERIFY 2DROP DROP

That's it. This script expects an input stack in the following format:

  <argN> ... <arg1> <policyScript> <proof>

At the end of execution the script has verified that <policyScript> is
part of the Merkle tree previously committed to, and <proof> is
dropped from the stack. This leaves the stack ready for a tail-call
recursion into <policyScript>.


MBV and Key Aggregation

If the signature scheme supports key aggregation, which it happens
that the the new signature aggregation scheme being worked on will
support as a side effect, then there is a very cool and useful
application that would be supported as well: tree signatures as
described by Pieter Wuille[1].  This looks almost exactly the same as
the MAST script, but with a CHECKSIG tacked on the end:

  OVER HASH256 <root-hash> MERKLEBRANCHVERIFY 2DROP DROP CHECKSIG

This script expects an input stack of the following form:

  <sig> <pubkey> <proof>

And proves that the pubkey is drawn from the set used to construct the
Merkle hash tree, and then its signature is checked. While it should
be clear this has 1-of-N semantics, what might be less obvious is that
key aggregation allows any signature policy expressible as a monotone
Boolean function (anything constructible with combinations of AND, OR,
and k-of-N thresholds) to be transformed to a 1-of-N over a set of key
aggregates. So the above script is a generic template able to verify
any monotone Boolean function over combinations of pubkeys, which
encompasses quite a number of use cases!

[1] https://blockstream.com/2015/08/24/treesignatures.html


An argument for permission-less innovation

The driving motivation for the tail-call and MBV proposals, and the
reason they are presented and considered together is to enable
Merklized Abstract Syntax Trees. However neither BIP actually defines
a MAST template, except as an example use case of the primitive
features. This is very much on purpose: it is the opinion of this
author that new bitcoin consensus features should follow the UNIX
philosophy as expressed by Peter Salus and Mike Gancarz and
paraphrased by yours truly:

  * Write features that do one thing and do it well.
  * Write features to work together.
  * Simple is beautiful.

By using modularity and composition of powerful but simple tools like
MERKLEBRANCHVERIFY and single tail-call recursion to construct MAST we
enable a complex and desirable feature while minimizing changes to the
consensus code, review burden, and acquired technical debt.

The reusability of the underlying primitives also means that they can
be combined with other modular features to support use cases other
than vanilla MAST, or reused in series to work around various limits
that might otherwise be imposed on a templated form of MAST. At the
moment the chief utility of these proposals is the straightforward
MAST script written above, but as primitives they do allow a few other
use cases and also combine well with features in the pipeline or on
the drawing board. For example, in addition to MAST you can:

1. Use MERKLEBRANCHVERIFY alone to support honeypot bounties, as
   discussed in the BIP.

2. Use a series of MERKLEBRANCHVERIFY opcodes to verify a branch with
   split proofs to stay under script and witness push limitations.

3. Combine MERKLEBRANCHVERIFY with key aggregation to get
   Wuille-Maxwell tree signatures which support arbitrary signing
   policies using a single, aggregatable signature.

4. Combine tail-call execution semantics with CHECKSIGFROMSTACK to get
   delegation and signature-time commitment to subscript policy.

5. Combine MERKLEBRANCHVERIFY with a Merkle proof prefix check opcode
   and Lamport signature support to get reusable Lamport keys.

I believe these benefits and possible future expansions to be strong
arguments in favor of extending bitcoin in the form of small, modular,
incremental, and reusable changes that can be combined and used even
in ways unforeseen even by their creators, creating a platform for
unrestricted innovation.

The alternative approach of rigid templates achieves the stated goals,
perhaps even with slightly better encoding efficiency, but at the cost
of requiring workaround for each future innovation. P2SH is just such
an example -- we couldn't even upgrade to 128-bit security without
designing an entirely different implementation because of the
limitations of template pattern matching.


Efficiency gains from templating

Finally, I need to point out that there is one efficiency gain that a
proper template-matching implementation has over user-specified
schemes: reduction of witness data. This is both a practical side
effect of more efficient serialization that doesn't need to encode
logic as opcodes, as well as the fact that since the hashing scheme is
fixed, one layer of hashes can be removed from the serialization. In
the case of MAST, rather than encode the Merkle root hash in the
redeem script, the hash is propagated upwards and compared against the
witness commitment. The amount space saved from adopting a template is
about equal to the size of the redeem script, which is approximately
40 bytes of witness data per MAST input.

That is arguably significant enough to matter, and in the long term I
think we will adopt a MAST template for those efficiency gains. But I
feel strongly that since MAST is not a feature in wide use at this
time, it is strongly in our interests to deploy MBV, tail-call, as
well overhaul the CHECKSIG operator before tackling what we feel an
ideal MAST-supporting witness type would look like, so that with some
experience under our belt we can adopt a system that is as simple and
as succinct as possible while supporting the necessary use cases
identified by actual use of the features.

Kind regards,
Mark Friedenbach
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
On Wed, Apr 05, 2017 at 07:39:08PM -0700, Bram Cohen wrote:

Agreed! There's no benefit to Bitcoin for having it - one way or the other
miners are going to destroy ~12BTC/block worth of energy. Meanwhile it appears
to have lead to something like a year of stupid political bullshit based on a
secret advantage - there's no reason to invite a repeat of this episode.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
Aside from that such change would require a hard fork it also violates one
of basic rules of bitcoin, which has long term consequences for miners and
for whole Bitcoin economy. In short, after altering the supply limit it
would not be "bitcoin" anymore.

On Mon, Dec 11, 2017 at 6:30 PM, Teweldemedhin Aberra via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------


I though two service bits allow three states and we should define all three combinations.
But I guess an adequate „definition“ would be to reserve it for future „definitions“.
Or use Gregory's proposal of min 2016*2 blocks & keep it „undefined“ for now.

49 days was chosen to allow SPV peers to be „offline“ for a month and still be capable to catch-up with a peer pruned to a datadir of ~10GB.


AFAIK Core does also guaranteed the 288 blocks post segwit activation:
https://github.com/bitcoin/bitcoin/blob/08a7316c144f9f2516db8fa62400893f4358c5ae/src/validation.h#L204
But maybe I’m confused.


Indeed. I’ll try to make that more clear.


AFAIK, Core does currently only relay NODE_NETWORK addresses.
But yes, It may be a problem already.

</jonas>

-------------------------------------
On Tue, May 16, 2017 at 4:01 AM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

I'm aware, I agree, and I even referenced that mail in my original post.

However, all of those approaches still require a network wide choice
to be useful. A validating node that does not maintain a UTXO X must
get a proof of its unspentness from somewhere for at least the block
which contains a spend of X. In a world where such a model is deployed
network-wide, that proof information is generated by the wallet and
relayed wherever needed. In a partial deployment however, you need
nodes that can produce the proof for other nodes, and the ability to
produce a proof is significantly more expensive than running either an
old or a new full node.

This ability to produce proofs becomes even harder when there are
different models deployed at once. Even just having a different
criterion for which UTXOs need a proof (eg. "only outputs created more
than 1000 blocks ago") may already cause compatibility issues. Combine
that with the multitude of ideas about this (insertion-ordered TXO
trees, txid-ordered UTXO Patricia tries, AVL+ trees, append-only
bitfield, ...) with different trade-offs (in CPU, RAM for validators,
complexity for wallets/index services, ...), I don't think we're quite
ready to make that choice.

To be clear: I'm very much in favor of moving to a model where the
responsibilities of full nodes are reduced in the long term. But
before that can happen there will need to be implementations,
experiments, analysis, ...

Because of that, I think it is worthwhile to investigate solutions to
the "how can we efficiently compare UTXO sets" problem separately from
the "how do we reduce full node costs by sending proofs instead of it
maintaining the data". And rolling UTXO set hashes are a solution for
just the first - and one that has very low costs and no normative
datastructures at all.

-- 
Pieter

-------------------------------------
Thank you Gregory,
so SegWit P2PHK have strictly less weight than P2WPKH, 3 fewer bytes (-1%) no matter the n. of outputs.
Instead, Segwit P2WPKH/P2SH cost 11% more than P2PHK, while compared to P2SH, SegWit transaction P2WSH and P2WSH/P2SH cost respectively 6% and 19% more space. And it can be much more if outputs are, as you say, an absurd number, which is the case of tx made by an exchange.

I understand there is a rationale for the overhead size. It's transparent from here: https://bitcoincore.org/en/2016/01/26/segwit-benefits
I don't understand your two points, which are new to me, maybe I lack of some technical details?
1) outputs were previously undercosted
2) a great many TXOUTs are putting a serious long term cost on the network

But independently from that, my point is: does an exchange have economic incentives in adopting SegWit? I think the answer is no.
Does SegWit help making bitcoin more scalable? Not until Lightning Network, since transactions just take up more space. 

Instead, if bech32 really makes possible to save up to 22% of space (I still need confirmation), it would help a lot in scaling bitcoin. We just need to coordinate the network to bring it on most of the wallets. Since using bech32 everybody will benefit and pay smaller fees there are economic incentives in implementing it. For this reason I think an agreement about a transition to bech32 as default address for every wallet is a highly realistic scenario, while widespread SegWit adoption without bech32 is not, in my opinion, because poor or negative economic incentives (including also the costs companies sustain for development).

I know an agreement is a political matter, should it be discussed elsewhere?
I think a network upgrade to bech32 addresses requires the same coordination of a hard fork, but.. just a hint: bech32 helps saving space, in the meanwhile we reach widespread segwit adoption. If transition is achieved, we would have blocks of 2mb fulfilled with transactions using bech32, which have a 20% discount. Compared to the current situation of 1.05mb average blocks and 400k tx daily, we could have about 2mb blocks and 960k tx daily...

Thank you,
Alberto De Luigi
(.com)



-----Original Message-----
From: Gregory Maxwell [mailto:gmaxwell@gmail.com]
Sent: martedì 19 dicembre 2017 09:06
To: mail@albertodeluigi.com; Bitcoin Protocol Discussion
<bitcoin-dev@lists.linuxfoundation.org>
Cc: Mark Friedenbach <mark@friedenbach.org>
Subject: Re: [bitcoin-dev] Clarification about SegWit transaction size and
bech32

Alberto,

You are confused about the impact.  Ordinary P2WPKH have strictly less
weight no matter how many outputs you have.  P2WSH in very output heavy
transactions can be more, but this is inherent in the upgrade from
inadequate 80-bit security to 128-bit security, an intentional
change: because outputs were previously _radically_ undercosted in the
system and any party making a great many new TXOUTs are putting a serious
long term cost on the network,  and in any case it's except for transactions
that make an absurd number of P2WSH outputs at once the difference is pretty
small.

On Mon, Dec 18, 2017 at 10:19 PM, Alberto De Luigi via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:





-------------------------------------
Apparently we will not get an understanding and we will probably be told
soon that this is going off topic, so short answer

Eh --> No, maybe you would like to quote Mozilla or the W3C too, all of
those organizations are financed by the big companies and are promoting
their interests (specs, DRM, etc), then would you really trust them?

A full node does not have to validate all tx and blocks, I am not aware
of any P2P system organized with peers and intermediate nodes (with no
incentive) that did survive (diaspora for example), and the most famous
one (who btw is handling much more traffic than what you describe) is
doing well because there is an intrinsic incentive for the users, see my
comment here
https://ec.europa.eu/futurium/en/content/final-report-next-generation-internet-consultation,
surprising to see that nobody raised those issues during the consultation

Paradoxally crypto currencies allow now to reward/sustain other systems,
then probably they should concentrate first on how to reward/sustain
themselves, different ideas have surfaced to reward the full nodes but
still seem very far to materialize

Coming back again to the subject, does anyone have any idea of who are
behind the existing full nodes and how to rank them according to their
participation to the network? Up to now there has been quasi no
discussion about what are the plans for the full nodes which tends to
suggest that this is obvious


Le 30/03/2017 à 03:14, Jared Lee Richardson a écrit :
-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms
-------------------------------------
community in disarray.

I really disagree with this sentiment, you don't need to provide
alternatives to criticize a technical proposal. I don't like this "active
segwit at all costs" theme that has been going around the community. I am a
fan of segwit, but we shouldn't push things through in an unsafe manner.

the long term.  The non-SegWit miners will probably just quickly give up
their orphans once they realize that money users like being able to have
non-mutable TX IDs.  If they do create a long lasting branch... well that
is good too, I'd be happy to no longer have them in our community.  Good
luck to them in creating a competitive money, so that we can all enjoy
lower transaction fees.

This seems like a lot of reckless hand waving to me.

Food for thought, why are we rejecting *all* blocks that do not signal
segwit? Can't we just reject blocks that *do not* signal segwit, but *do*
contain segwit transactions? It seems silly to me that if a miner mines a
block with all pre segwit txs to reject that block. Am I missing something
here?

-Chris

On Fri, Apr 14, 2017 at 11:50 AM, praxeology_guy via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version 0.14.2 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.14.2/>

Or by torrent:

  magnet:?xt=urn:btih:b4fc7820df95b8b39603ad246c241272ec403619&dn=bitcoin-core-0.14.2&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce

This is a new minor version release, including various bugfixes and
performance improvements, as well as updated translations.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

To receive security and update notifications, please subscribe to:

  <https://bitcoincore.org/en/list/announcements/join/>

Compatibility
==============

Bitcoin Core is extensively tested on multiple operating systems using
the Linux kernel, macOS 10.8+, and Windows Vista and later.

Microsoft ended support for Windows XP on [April 8th, 2014](https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support),
No attempt is made to prevent installing or running the software on Windows XP, you
can still do so at your own risk but be aware that there are known instabilities and issues.
Please do not report issues about Windows XP to the issue tracker.

Bitcoin Core should also work on most other Unix-like systems but is not
frequently tested on them.

Notable changes
===============

miniupnp CVE-2017-8798
- ----------------------------

Bundled miniupnpc was updated to 2.0.20170509. This fixes an integer signedness error
(present in MiniUPnPc v1.4.20101221 through v2.0) that allows remote attackers
(within the LAN) to cause a denial of service or possibly have unspecified
other impact.

This only affects users that have explicitly enabled UPnP through the GUI
setting or through the `-upnp` option, as since the last UPnP vulnerability
(in Bitcoin Core 0.10.3) it has been disabled by default.

If you use this option, it is recommended to upgrade to this version as soon as
possible.

Known Bugs
==========

Since 0.14.0 the approximate transaction fee shown in Bitcoin-Qt when using coin
control and smart fee estimation does not reflect any change in target from the
smart fee slider. It will only present an approximate fee calculated using the
default target. The fee calculated using the correct target is still applied to
the transaction and shown in the final send confirmation dialog.

0.14.2 Change log
=================

Detailed release notes follow. This overview includes changes that affect
behavior, not code moves, refactors and string updates. For convenience in locating
the code changes and accompanying discussion, both the pull request and
git merge commit are mentioned.

### RPC and other APIs
- - #10410 `321419b` Fix importwallet edge case rescan bug (ryanofsky)

### P2P protocol and network code
- - #10424 `37a8fc5` Populate services in GetLocalAddress (morcos)
- - #10441 `9e3ad50` Only enforce expected services for half of outgoing connections (theuni)

### Build system
- - #10414 `ffb0c4b` miniupnpc 2.0.20170509 (fanquake)
- - #10228 `ae479bc` Regenerate bitcoin-config.h as necessary (theuni)

### Miscellaneous
- - #10245 `44a17f2` Minor fix in build documentation for FreeBSD 11 (shigeya)
- - #10215 `0aee4a1` Check interruptNet during dnsseed lookups (TheBlueMatt)

### GUI
- - #10231 `1e936d7` Reduce a significant cs_main lock freeze (jonasschnelli)

### Wallet
- - #10294 `1847642` Unset change position when there is no change (instagibbs)

Credits
=======

Thanks to everyone who directly contributed to this release:

- - Alex Morcos
- - Cory Fields
- - fanquake
- - Gregory Sanders
- - Jonas Schnelli
- - Matt Corallo
- - Russell Yanofsky
- - Shigeya Suzuki
- - Wladimir J. van der Laan

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCgAGBQJZRRTMAAoJEB5K7WKYbNJdqk0IANF5Q49ID3B77b0CwSKzjTxk
Ktp0qgvtig0ZMnzVlgjULUsRW8EbecWCQwmgRo8uUoCGmNS2u7u+s28kIIkicELE
BpWcW4eC6NdCCjB1CSnmX/tno4gFwOZutVj/XUXJCBEuBbo6fIK0cVDas5vw8UVa
gXL5ytwXeCws3z9f3iiD1Nl0k+J+dRb0sJ2u0A1+XqoMFfInMUFiP/fa9XWaimKo
62jD07IJDKtH4PEKG8v+FLZounRP7t1lhU0AiQ0Uj67mBmllwWD0KeZi0f4SokMX
aezEH+2UIW3Ph/QbG+ktZYUzbDALnRIHEBP4GQUuWiUPZKo3vAS3yhvh1nvYUW4=
=VBdE
-----END PGP SIGNATURE-----

-------------------------------------
On Saturday, 7 January 2017 00:55:19 CET Eric Lombrozo wrote:

To explain why I didn't write that;

Bitcoin Classic is not incompatible with the current Bitcoin network and its 
consensus rules.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
Clean stack should be eliminated for other possible future uses, the most obvious of which is recursive tail-call for general computation capability. I’m not arguing for that at this time, just arguing that we shouldn’t prematurely cut off an easy implementation of such should we want to. Clean stack must still exist as policy for future soft-fork safety, but being a consensus requirement was only to avoid witness malleability, which committing to the size of the witness also accomplishes.

Committing to the number of witness elements is fully sufficient, and using the number of elements avoids problems of not knowing the actual size in bytes at the time of signing, e.g. because the witness contains a merkle proof generated by another party from an unbalanced tree, and unbalanced trees are expected to be common (so that elements can be placed higher in the tree in accordance with their higher expected probability of usage). Other future extensions might also have variable-length proofs.



-------------------------------------
Since there is no surviving argument in this thread contrary to my original
post, I'll begin work on a BIP.
-------------------------------------
size

Luke, how do you know the community opposes that? Specifically, how did you
come to this conclusion?


Why do you think blocks are "too large"? Please cite some evidence. I've
asked this before and you ignored it, but an answer would be helpful to the
discussion.

- t.k.

On Sun, Feb 5, 2017 at 6:02 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I guess I wasn't clear on the wildcard, `nForkId=0`

This proposal puts Bitcoin at `nForkId=1`, with the purpose of having `nForkId=0` valid on *all* future forks. This means you can create a `nLockTime` transaction, delete the private key and still be assured to not lose potential future tokens.

In theory `nForkId=0` could be used for an address too, the sending wallet should display a warning message about unknown side effects though. This address would be future-safe, and you can put it into a safe-deposit box (even though I see little reason to back up an _address_. You would always back up a _private key_, which translates into funds on any fork.)

Furthermore, `nForkId=0` can be used for L2 applications. Let's say Alice and Bob open a payment channel. One week later, project X decides to fork the network into a new token, implementing a custom way of providing strong two-way replay protection. The protocol Alice and Bob use for the payment channel has not implemented this new form of replay protection. Alice and Bob now have to make a choice:

(1) Ignore this new token. This comes with an evaluation of how much this new token could be worth in the future. They will continue normal channel operation, knowing that their funds on the other branch will be locked up until eternity. When they close their payment channel, the closing transaction will get rejected from the other network, because it's not following the format for replay protected transactions.

(2) Close the payment channel before the fork. The transaction, which closes the payment channel has to be mined before the fork, potentially paying a higher-than-normal fee.

With this proposal implemented, there are two additional choices

(3) Create the commitment transactions with `nForkId=0`. This ensures that when the channel gets closed, funds on other chains are released accordingly. This also means that after the fork, payments on the channel move both, the original token and the new token. Potentially, Alice and Bob want to wait before further transacting on the channel, to see if the token has substantial value. If it has, they can *then* close the channel and open a new channel again. (Note: The funding transaction can use a specific `nForkId`, preventing you from locking up multiple coins when funding the channel, but you can choose to settle with `nForkId=0` to not lock up future coins)

(4) Make the protocol aware of different `nForkId`. After the fork, the participants can chose to *only* close the payment channel on the new token, making the payment channel Bitcoin-only again. This is the preferred option, as it means no disruption to the original network.


I was considering this too. On the other hand, it's only _human readable_ because thy bytes used currently encode 'bc'. For future forks, this would just be two random letters than, but potentially acceptable.

-------------------------------------
On Thursday 07 September 2017 12:38:55 AM Mark Friedenbach via bitcoin-dev 
wrote:

Just noticed this doesn't count sigops toward the block sigop limit.
Is that really safe? How long would it take, to verify a malicious block with 
only inputs such that there is nearly 4 MB of sigops?

(I do already understand the difficulty in supporting the sigop limit.)

Luke

-------------------------------------
This design purposefully does not distinguish leaf nodes from internal nodes. That way it chained invocations can be used to validate paths longer than 32 branches. Do you see a vulnerability due to this lack of distinction?

-------------------------------------
Gregory wrote:

Ahh, sipa brought this up other day, but I thought he was referring to the
coding loop (which uses a power of 2 divisor/modulus), not the
siphash-then-reduce loop.

http://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/

Very cool, I wasn't aware of the existence of such a mapping.

Correct me if I'm wrong, but from my interpretation we can't use that
method as described as we need to output 64-bit integers rather than
32-bit integers. A range of 32-bits would be constrain the number of items
we could encode to be ~4096 to ensure that we don't overflow with fp
values such as 20 (which we currently use in our code).

If filter commitment are to be considered for a soft-fork in the future,
then we should definitely optimize the construction of the filters as much
as possible! I'll look into that paper you referenced to get a feel for
just how complex the optimization would be.


Yep! Nice catch. Our code is correct, but mistake in the spec was an
oversight on my part. I've pushed a commit[1] to the bip repo referenced
in the OP to fix this error.

I've also pushed another commit to explicitly take advantage of the fact
that P is a power-of-two within the coding loop [2].

-- Laolu

[1]:
https://github.com/Roasbeef/bips/commit/bc5c6d6797f3df1c4a44213963ba12e72122163d
[2]:
https://github.com/Roasbeef/bips/commit/578a4e3aa8ec04524c83bfc5d14be1b2660e7f7a


On Wed, Jun 7, 2017 at 2:41 PM Gregory Maxwell <greg@xiph.org> wrote:

-------------------------------------
Just to add on to the ethical issue of blocking this.


If blocking the covert form of ASICBOOST is seen as unethical, then the same can be said about libsecp256k1, various client optimisations, Compactblocks.

All of which seek to reduce the efficacy of large miners and selfish mining.


I also find it very ironic that the author of the Selfish Mining paper who rang alarm bells about miner centralisation in 2013 is now opposing attempts to reduce miner centralisation.


________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of Luv Khemani via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Thursday, April 6, 2017 8:02 PM
To: Gregory Maxwell; Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the Bitcoin POW function


Hi Greg


Great work in discovering this!


is exploited by ASICBOOST and the various steps which could be used to
block it in the network if it became a problem.


Could you elaborate on why you consider ASICBOOST to be an attack? Attack here implies ill-intent by the practitioner towards the network as a primary motivating factor.

Personally, i see this as a miner acting in his self-interest and had i been a miner and knew about the covert method, i would use it too.

So while i'm no fan of Bitmain/Jihan, i do not condone the vilification he has received over the use of ASICBOOST to gain an edge.

I know i'm griping over semantics, but in the current political climate, they can be amplified by some to cause more drama than is healthy.


Other thoughts:


Several people have commented that blocking the use of this covert technique is unethical or "wrong".
To quote Emin:


This is a poor analogy and extremely misleading as the the basis for blocking has nothing to do with efficiency and more to do with the following:

1) Blocking upgrades to the protocol that are deemed by the vast majority of the technical community/Bitcoin Businesses as being the best way forward

2) An advantage by a miner/group, especially one with majority hashrate is a threat to decentralisation and security of the network and it is entirely justifiable for devs to nullify such an advantage.
You can see it as an arms race where miners are always finding ways to gain an edge and devs trying to discover such edges and nullify them to level the playing field.
This is how the game works and it should not be viewed in a political angle or taken personally by either party. Miners are acting in their self-interest and Devs are trying to secure the network and increase decentralisation.
Both are doing their job.

Just by revealing the info, you have effectively ensured the nullification of any edge enjoyed by miners using the covert technique in the medium to long term.
Either miners not using the technique will all start signalling for SegWit to nullify their competitors edge or they will procure hardware which has the edge.

Given the threat to decentralisation, i also believe UASF will gain more momentum as users seek to protect the network from further miner centralisation.


________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Thursday, April 6, 2017 5:37 AM
To: Bitcoin Dev
Subject: [bitcoin-dev] BIP proposal: Inhibiting a covert attack on the Bitcoin POW function

A month ago I was explaining the attack on Bitcoin's SHA2 hashcash which
is exploited by ASICBOOST and the various steps which could be used to
block it in the network if it became a problem.

While most discussion of ASICBOOST has focused on the overt method
of implementing it, there also exists a covert method for using it.

As I explained one of the approaches to inhibit covert ASICBOOST I
realized that my words were pretty much also describing the SegWit
commitment structure.

The authors of the SegWit proposal made a specific effort to not be
incompatible with any mining system and, in particular, changed the
design at one point to accommodate mining chips with forced payout
addresses.

Had there been awareness of exploitation of this attack an effort
would have been made to avoid incompatibility-- simply to separate
concerns.  But the best methods of implementing the covert attack
are significantly incompatible with virtually any method of
extending Bitcoin's transaction capabilities; with the notable
exception of extension blocks (which have their own problems).

An incompatibility would go a long way to explain some of the
more inexplicable behavior from some parties in the mining
ecosystem so I began looking for supporting evidence.

Reverse engineering of a particular mining chip has demonstrated
conclusively that ASICBOOST has been implemented
in hardware.

On that basis, I offer the following BIP draft for discussion.
This proposal does not prevent the attack in general, but only
inhibits covert forms of it which are incompatible with
improvements to the Bitcoin protocol.

I hope that even those of us who would strongly prefer that
ASICBOOST be blocked completely can come together to support
a protective measure that separates concerns by inhibiting
the covert use of it that potentially blocks protocol improvements.

The specific activation height is something I currently don't have
a strong opinion, so I've left it unspecified for the moment.

<pre>
  BIP: TBD
  Layer: Consensus
  Title: Inhibiting a covert attack on the Bitcoin POW function
  Author: Greg Maxwell <greg@xiph.org>
  Status: Draft
  Type: Standards Track
  Created: 2016-04-05
  License: PD
</pre>

==Abstract==

This proposal inhibits the covert exploitation of a known
vulnerability in Bitcoin Proof of Work function.

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.

==Motivation==

Due to a design oversight the Bitcoin proof of work function has a potential
attack which can allow an attacking miner to save up-to 30% of their energy
costs (though closer to 20% is more likely due to implementation overheads).

Timo Hanke and Sergio Demian Lerner claim to hold a patent on this attack,
which they have so far not licensed for free and open use by the public.
They have been marketing their patent licenses under the trade-name
ASICBOOST.  The document takes no position on the validity or enforceability
of the patent.

There are two major ways of exploiting the underlying vulnerability: One
obvious way which is highly detectable and is not in use on the network
today and a covert way which has significant interaction and potential
interference with the Bitcoin protocol.  The covert mechanism is not
easily detected except through its interference with the protocol.

In particular, the protocol interactions of the covert method can block the
implementation of virtuous improvements such as segregated witness.

Exploitation of this vulnerability could result in payoff of as much as
$100 million USD per year at the time this was written (Assuming at
50% hash-power miner was gaining a 30% power advantage and that mining
was otherwise at profit equilibrium).  This could have a phenomenal
centralizing effect by pushing mining out of profitability for all
other participants, and the income from secretly using this
optimization could be abused to significantly distort the Bitcoin
ecosystem in order to preserve the advantage.

Reverse engineering of a mining ASIC from a major manufacture has
revealed that it contains an undocumented, undisclosed ability
to make use of this attack. (The parties claiming to hold a
patent on this technique were completely unaware of this use.)

On the above basis the potential for covert exploitation of this
vulnerability and the resulting inequality in the mining process
and interference with useful improvements presents a clear and
present danger to the Bitcoin system which requires a response.

==Background==

The general idea of this attack is that SHA2-256 is a merkle damgard hash
function which consumes 64 bytes of data at a time.

The Bitcoin mining process repeatedly hashes an 80-byte 'block header' while
incriminating a 32-bit nonce which is at the end of this header data. This
means that the processing of the header involves two runs of the compression
function run-- one that consumes the first 64 bytes of the header and a
second which processes the remaining 16 bytes and padding.

The initial 'message expansion' operations in each step of the SHA2-256
function operate exclusively on that step's 64-bytes of input with no
influence from prior data that entered the hash.

Because of this if a miner is able to prepare a block header with
multiple distinct first 64-byte chunks but identical 16-byte
second chunks they can reuse the computation of the initial
expansion for multiple trials. This reduces power consumption.

There are two broad ways of making use of this attack. The obvious
way is to try candidates with different version numbers.  Beyond
upsetting the soft-fork detection logic in Bitcoin nodes this has
little negative effect but it is highly conspicuous and easily
blocked.

The other method is based on the fact that the merkle root
committing to the transactions is contained in the first 64-bytes
except for the last 4 bytes of it.  If the miner finds multiple
candidate root values which have the same final 32-bit then they
can use the attack.

To find multiple roots with the same trailing 32-bits the miner can
use efficient collision finding mechanism which will find a match
with as little as 2^16 candidate roots expected, 2^24 operations to
find a 4-way hit, though low memory approaches require more
computation.

An obvious way to generate different candidates is to grind the
coinbase extra-nonce but for non-empty blocks each attempt will
require 13 or so additional sha2 runs which is very inefficient.

This inefficiency can be avoided by computing a sqrt number of
candidates of the left side of the hash tree (e.g. using extra
nonce grinding) then an additional sqrt number of candidates of
the right  side of the tree using transaction permutation or
substitution of a small number of transactions.  All combinations
of the left and right side are then combined with only a single
hashing operation virtually eliminating all tree related
overhead.

With this final optimization finding a 4-way collision with a
moderate amount of memory requires ~2^24 hashing operations
instead of the >2^28 operations that would be require for
extra-nonce  grinding which would substantially erode the
benefit of the attack.

It is this final optimization which this proposal blocks.

==New consensus rule==

Beginning block X and until block Y the coinbase transaction of
each block MUST either contain a BIP-141 segwit commitment or a
correct WTXID commitment with ID 0xaa21a9ef.

(See BIP-141 "Commitment structure" for details)

Existing segwit using miners are automatically compatible with
this proposal. Non-segwit miners can become compatible by simply
including an additional output matching a default commitment
value returned as part of getblocktemplate.

Miners SHOULD NOT automatically discontinue the commitment
at the expiration height.

==Discussion==

The commitment in the left side of the tree to all transactions
in the right side completely prevents the final sqrt speedup.

A stronger inhibition of the covert attack in the form of
requiring the least significant bits of the block timestamp
to be equal to a hash of the first 64-bytes of the header. This
would increase the collision space from 32 to 40 or more bits.
The root value could be required to meet a specific hash prefix
requirement in order to increase the computational work required
to try candidate roots. These change would be more disruptive and
there is no reason to believe that it is currently necessary.

The proposed rule automatically sunsets. If it is no longer needed
due to the introduction of stronger rules or the acceptance of the
version-grinding form then there would be no reason to continue
with this requirement.  If it is still useful at the expiration
time the rule can simply be extended with a new softfork that
sets longer date ranges.

This sun-setting avoids the accumulation of technical debt due
to retaining enforcement of this rule when it is no longer needed
without requiring a hard fork to remove it.

== Overt attack ==

The non-covert form can be trivially blocked by requiring that
the header version match the coinbase transaction version.

This proposal does not include this block because this method
may become generally available without restriction in the future,
does not generally interfere with improvements in the protocol,
and because it is so easily detected that it could be blocked if
it becomes an issue in the future.

==Backward compatibility==


==Implementation==


==Acknowledgments==


==Copyright==

This document is placed in the public domain.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
bitcoin-dev -- Bitcoin Protocol Discussion - Linux Foundation<https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
lists.linuxfoundation.org
Bitcoin development and protocol discussion. This list is lightly moderated. - No offensive posts, no personal attacks. - Posts must concern development of bitcoin ...



-------------------------------------
Yeah, it does make things harder, and it's easy enough to soft fork to
handle arbitrary opt-in protocol improvements, new much larger block sizes,
whatever you want.   Even OK to migrate to a new system by not allowing
old->old or new->old transactions.
-------------------------------------
I am surprised nothing like this exists already, so am all for the idea.

Maybe I am misunderstanding, but I'm not sure you want to have
thousand separators and other locale stuff in there. All currencies
including USD are often shown in Swedish using space or dot as
thousand separator and comma as decimal separator, e.g. "1.234,56 USD"
or "1 234,56 USD". I.e. this is something that the locale of the
user's environment defines, not something the server should have
opinions about. It is also not ideal to propose a given format based
on the user's locale, as some users have preferences for this (I
personally use US locale for numbers, and a US person who is visiting
Sweden wouldn't want this to suddenly change).

-Kalle.

On Sat, Mar 4, 2017 at 12:27 AM, Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hi I'm AJ West, I made a service http://preferredminer.com which is a
proof-of-concept project designed to spur discussion on exactly this issue
of "miners as service providers."

The current status is that Bitcoin end users are looking to support
specific miners, whether that's because they agree with a miner/pool's
political positions, their consensus ideology, physical location (yes some
people would like only miners in particular countries to mine their
transactions) and the list of reasons goes on. The main attitude right now
is that people would like to 'support' miners who signal for the features
they care about.

I strongly believe, whether the Bitcoin developer community facilitates it
or not, certain miners will become preferred by users. In summary, there
are realistically two proposed ways of providing this service in the
present-day situation: 1) By creating 'segregated mempools' where an
authenticated third-party like my web service Preferred Miner manages the
access to pending transactions destined for a specific set of miners, and
2) by creating transactions where the mining fee is in one way or another,
an output to an address owned by the preferred miner(s).

There are some terrible pitfalls with both of these methods. The first
being that you have to trust a lot of people, including the 3rd party (me)
and the pools to work in your users' interest ("don't give my transactions
to other miners or broadcast to mempool please"). Then there are the extra
fees users will have to pay to offset the risk of a miner losing out for
having to send the network a not-yet-broadcasted transaction. And finally,
the other method requires that they be larger transactions, and a directory
of mining pools' receiving addresses for outputs must be maintained. Then
you have to hope the miner will be setup to scoop in your transaction
knowing it's got a fee for them. Plus, how many nodes going forward are
going to hold what seem to be 0-fee transactions in mempool (because the
fee is in the outputs)?

I am not necessarily looking for answers or solutions to these issues, but
simply to show a case and to express that this idea of having specific
miners/pools process their transactions, is important to some people.
-------------------------------------
On 07/12/2017 06:48 PM, Anthony Towns via bitcoin-dev wrote:


I wish to NOT signal for segwit if mining.


I wish to NOT enforce segwit consensus rules.


good point, thanks for clarifying.


I've set the nTimeout to 0 already.  I will look into the NODE_WITNESS
p2p bit.

I think that logically, if coded correctly, my node would have no more
risks than any other legacy (pre-segwit) node on the network...


fair enough.  But these are the same risks as running any pre-segwit
node, correct?    For example bitcoin-core 0.13.0, or any version of
btcd to date...


-- 
Dan Libby

Open Source Consulting S.A.
Santa Ana, Costa Rica
http://osc.co.cr
phone: 011 506 2204 7018
Fax: 011 506 2223 7359

-------------------------------------
After reading
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012194.html
I see that Adam is correct. Unfortunately this SF would make Felix's
confidential transactions
more complicated. The blinding and unblinding transactions would have to
be created with
minimal output values, and this will need to be considered when checking
that the fee is equal
to the total amount of input. (it would now be SUM(inputs) -
SUM(minimalOutputs))

Blinding transaction:
  Ins:
    All non-confidential inputs are valid
  Outs:
  - 0..N: (new confidential outputs)
    amount: 0
    scriptPubkey: OP_2 <0x{32-byte-hash-value}>
    witnessOut: <0x{petersen-commitment}> <0x{range-proof}>
  - last:
    amount: 0
    scriptPubkey: OP_RETURN OP_2 {blinding-fee-amount}
  Fee: Sum of the all inputs value


However, looking at the format of the blinding transaction, and how the
GCTXO is added to the UTXO set
by miners, it seems that a change to the blinding scriptPubKey could
allow for the use of 0 value
outputs. Even with the SF proposed by this email thread.

OP_RETURN could be added to the scriptPubKey during blinding. The amount
and scriptPubKey destination of
unblinded funds is part of the witness and the outputs of an unblinded
transaction are unspendable, so
why not also make them unspendable in the blind transaction? As far as I
can tell those outputs don't need to
be spendable, they are really just encoding data. It doesn't seem like
anything besides the confidential base
transaction and the fee output from the blind transaction need to be in
the UTXO set.

Is it still possible to add this data to the witness if the scriptPubKey
is unspendable? :

witnessOut: <0x{petersen-commitment}> <0x{range-proof}>

I think I'm missing something obvious, someone point out why this is
stupid please :)

On 09/06/2017 06:29 PM, Adam Back wrote:


-------------------------------------
Jorge,

Why won't the attacker use asicboost too? (Please don't say because of
We're assuming the ASIC optimization in my example is incompatible with
ASICBoost. But if the new optimization were compatible with ASICBoost,
you're right, the network would be in an equivalent situation whether
ASICBoost was banned or not.

I want to point out again that overt ASICBoost can be used on the network
today. My proposal is to bring ASICBoost usage out into the open vs hiding
it. Banning ASICBoost via protocol changes is another issue completely.

Jimmy


-------------------------------------
On Tue, May 09, 2017 at 09:59:06PM -0400, Russell O'Connor via bitcoin-dev wrote:

If you seed the randomization with every R value (which would come for free
if you used, say, the witness root) then Wagner's attack no longer applies.

The idea is that no aggregation occurs until a miner produces a block. You
have a bunch of independent Schnorr sigs (s_i, R_i). Then the _miner_ multiples
each s_i by H(witness root || index) or whatever, sums up the s_i's, and commits
the sum somewhere where it doesn't affect the root.

Verifiers then multiply each R_i by the same multiplying factors and are able
to do a batch verification of them.


Verifiers who have seen a signature before and cached it as valid can save
themselves a bit of time by subtracting H(witness root || index)*s_i from
the summed s-value and then skipping R_i in the above step. These are scalar
operations and are extremely cheap.

They can recognize the signature given only the transaction it signs and R_i,
which uniquely determine a valid signature.


I believe this is what Tadge was referring to when he mentioned a talk of mine.
It's roughly what I've had in mind whenever I talk about non-interactive Schnorr
aggregation.



Cheers
Andrew


-- 
Andrew Poelstra
Mathematics Department, Blockstream
Email: apoelstra at wpsoftware.net
Web:   https://www.wpsoftware.net/andrew

"A goose alone, I suppose, can know the loneliness of geese
 who can never find their peace,
 whether north or south or west or east"
       --Joanna Newsom

-------------------------------------
On Sun, May 07, 2017 at 02:39:14PM -0700, Pieter Wuille via bitcoin-dev wrote:

Exactly - knowledge of the English language isn't a binary. Equally, I don't
remember ever learning names of special characters in French class back in
elementary school, but I do recall us drilling the alphabet and especially
numbers repeatedly.

If I were trying to tell a French speaker a BTC address, I'd probably be able
to succesfully do it with bech32, but not with any encoding using special
characters.


FWIW, I also did a partial rust implementation of just the Bech32 encoding for
a prototype non-BTC use-case. Other than the version number being it's own
"chunk" I found it very straight-forward to implement and I think it'll make
for a nice replacement for what otherwise would have been hex digests.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
My mistake, apologies all.


 - I honestly thought everyone just took the next available number and published up their BIP's.


And, I see you have something of a master list.


As a suggestion, would it be worth considering linking to some of that information in the list welcome email? Web search is not always your friend for locating everything relevant.


Regards,

Damian Williamson

________________________________
From: Luke Dashjr <luke@dashjr.org>
Sent: Sunday, 24 December 2017 6:21:24 PM
To: Damian Williamson
Cc: bitcoin-dev@lists.linuxfoundation.org
Subject: Re: [bitcoin-dev] BIP 177: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

BIP 177 is NOT assigned. Do not self-assign BIP numbers!

Please read BIP 2:

    https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki
[https://avatars0.githubusercontent.com/u/528860?s=400&v=4]<https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki>

bips/bip-0002.mediawiki at master  bitcoin/bips  GitHub<https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki>
github.com
Abstract. A Bitcoin Improvement Proposal (BIP) is a design document providing information to the Bitcoin community, or describing a new feature for Bitcoin or its ...



Luke


On Sunday 24 December 2017 2:57:38 AM Damian Williamson via bitcoin-dev wrote:
-------------------------------------
Karl wrote:


Originally we hadn't considered such an idea. Grasping the concept a bit
better, I can see how that may result in considerable bandwidth savings
(for purely negative queries) for clients doing a historical sync, or
catching up to the chain after being inactive for months/weeks.

If we were to purse tacking this approach onto the current BIP proposal,
we could do it in the following way:

   * The `getcfilter` message gains an additional "Level" field. Using
     this field, the range of blocks to be included in the returned filter
     would be Level^2. So a level of 0 is just the single filter, 3 is 8
     blocks past the block hash etc.

   * Similarly, the `getcfheaders` message would also gain a similar field
     with identical semantics. In this case each "level" would have a
     distinct header chain for clients to verify.


For larger blocks (like the one referenced at the end of this mail) full
construction of the regular filter takes ~10-20ms (most of this spent
extracting the data pushes). With smaller blocks, it quickly dips down to
the nano to micro second range.

Whether to keep _all_ the filters on disk, or to dynamically re-generate a
particular range (possibly most of the historical data) is an
implementation detail. Nodes that already do block pruning could discard
very old filters once the header chain is constructed allowing them to
save additional space, as it's unlikely most clients would care about the
first 300k or so blocks.


Yep, this is only a hold-over until when/if a commitment to the filter is
soft-forked in. In that case, there could be some extension message to
fetch the filter hash for a particular block, along with a merkle proof of
the coinbase transaction to the merkle root in the header.


Interesting, are you creating the equivalent of both our "regular" and
"extended" filters? Each of the filter types consume about ~3.5GB in
isolation, with the extended filter type on average consuming more bytes
due to the fact that it includes sigScript/witness data as well.

It's worth noting that those numbers includes the fixed 4-byte value for
"N" that's prepended to each filter once it's serialized (though that
doesn't add a considerable amount of overhead).  Alex and I were
considering instead using Bitcoin's var-int encoding for that number
instead. This would result in using a single byte for empty filters, 1
byte for most filters (< 2^16 items), and 3 bytes for the remainder of the
cases.


Does that include the time required to read the blocks from disk? Or just
the CPU computation of constructing the filters? I haven't yet kicked off
a full re-index of the filters, but for reference this block[1] on testnet
takes ~18ms for the _full_ indexing routine with our current code+spec.

[1]: 000000000000052184fbe86eff349e31703e4f109b52c7e6fa105cd1588ab6aa

-- Laolu


On Sun, Jun 4, 2017 at 7:18 PM Karl Johan Alm via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Bryan Bishop via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
writes:


Thanks!

Has anyone categoried list discussions by topic like this? It seems a
lot of this stuff is scattered between mailing lists, irc conversations,
etc and can be hard to know whats floating out there.

-- 
https://jb55.com

-------------------------------------
Dear All,

The Call for Proposals (CFP) for 'Scaling Bitcoin 2017: Stanford' is now
open.

Please see https://scalingbitcoin.org for details

*Important Dates*

Sept 25th - Deadline for submissions to the CFP

Oct 16th - Applicant acceptance notification

Hope to see you in California (Nov 4-5 2017)

Full CFP can be found at https://scalingbitcoin.org/event/stanford2017#cfp
-------------------------------------
On 07/09/17 18:30, Kabuto Samourai wrote:

Blockheight depends on the chain. XPUB is not tied to particular
chain/coin.

Also there are already cryptocurrencies that do not use blockchain, but
directed acyclic graph (DAG) for storing transactions. So it would not
be obvious what number to use as a blockheight.

OTOH all blockchains contain timestamps in their blocks, so we can use that.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
CTO, SatoshiLabs

-------------------------------------
merge it with some small pushback - allow segwit to activate in Aug, then
"upgrade" the hard fork to be "spoonet in 18 months" instead.

On Sat, Apr 1, 2017 at 8:33 AM, Jorge Timón via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
The confusion below stems from his conflation of several different ideas.

I will try to explicitly clarify a distinction between several types of
user (or, "modes" of use if you prefer):

[DC#0] -- Someone who does not upgrade their Bitcoin software (and is
running, say, 0.13). However, they experience the effects of the new
rules which miners add (as per the soft fork[s] to add drivechain
functionality and individual drivechains).
[DC#1] -- Someone who always upgrades to the latest version of the
Bitcoin software, but otherwise has no interest in running/using sidechains.
[DC#2] -- Someone who upgrades to the latest Bitcoin version, and
decides to also become a full node of one or more sidechains, but who
ever actually uses the sidechains.
[DC#3] -- Someone who upgrades their software, runs sidechain full
nodes, and actively moves money to and from these.


On 7/12/2017 6:43 PM, Tao Effect wrote:
FYI that document is nearly two years old, and although it is still
overwhelmingly accurate, new optimizations allow us (I think) to push
the waiting period to several weeks and the total ACK counting period up
to several months.

[DC#0] Yes
[DC#1] Yes
[DC#2] Yes
[DC#3] Yes

Because if a node doesn't have the sidechain's information, it will just
assume every withdrawal is valid. This is comparable to someone who
still hasn't upgraded to support P2SH, in cases [DC#0] and [#1].

(And this is the main advantage of DC over extension blocks).


[DC#0] They do.
[DC#1] They do.
[DC#2] They do.
[DC#3] They do.

Again, from the perspective of a mainchain user, every withdrawal is valid.


There is no *need* to this in Drivechain, either, for [DC#0] or [DC#1].

[DC#2] and [DC#3] would certainly have an interest in understanding what
is going on, but that has absolutely nothing whatsoever to do with
Bitcoin Core and so is off-topic for this mailing list.


Somehow I doubt it.


Paul
-------------------------------------
On Friday, 1 September 2017 20:15:53 CEST Cserveny Tamas wrote:

The real limit is set by the technology. Just like in 1990 we could not 
fathom having something like YouTube and high-res video streaming (Netflix), 
the limits of what is possible continually shifts.

This is basically how any successful product has ever grown, I think that it 
is not just desirable, it is inevitable.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
While I fully agree with the intent (increasing full nodes so a big
miner waking up in a bad mood can't threaten the world any longer every
day as it is now) I am not sure to get the interest of this proposal,
because:

- it's probably not a good idea to encourage the home users to run full
nodes, there are many people running servers far from their capacity
that could easily run efficient full nodes

- if someone can't allocate 100 GB today to run a full node, then we
can't expect him to allocate more in the future

- the download time is a real concern

- this proposal is a kind of reinventing torrents, while limiting the
number of connections to something not efficient at all, I don't see why
something that is proven to be super efficient (torrents) would be
needed to be reinvented, I am not saying that it should be used as the
bittorrent network is doing but the concepts can be reused

- I don't get at all the concept of "archival" nodes since it's another
useless step toward centralization

I think the only way to increase full nodes it to design an incentive
for people to run them


Le 17/04/2017  08:54, David Vorick via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
Thomas et.al.

So, in your minds, anyone who locked up coins using CLTV for their child to
receive on their 21st birthday, for the sake of argument, has effectively
forfeit those coins after the fact?  You are going to force anyone who took
coins offline (cryptosteel, paper, doesn't matter) to bring those coins
back online, with the inherent security risks?

In my mind, the only sane way to even begin discussing an approach
implementing such a thing - where coins "expire" after X years - would be
to give the entire ecosystem X*N years warning, where N > 1.5.  I'd also
suggest X would need to be closer to the life span of a human than zero.
Mind you, I'd suggest this "feature" would need to be coded and deployed as
a future-hard-fork X*N years ahead of time.  A-la Satoshi's blog post
regarding increasing block size limit, a good enough approximation would be
to add a block height check to the code that approximates X*N years, based
on 10 minute blocks.  The transparency around such a change would need to
be radical and absolute.

I'd also suggest that, similar to CLTV, it only makes sense to discuss
creating a "never expire" transaction output, if such a feature were being
seriously considered.

If you think discussions around a block size increase were difficult, then
we'll need a new word to describe the challenges and vitriol that would
arise in arguments that will follow this discussion should it be seriously
proposed, IMHO.

I also don't think it's reasonable to conflate the discussion herein with
discussion about what to do when ECC or SHA256 is broken.  The
weakening/breaking of ECC poses a real risk to the stability of Bitcoin -
the possible release of Satoshi's stash being the most obvious example -
and what to do about that will require serious consideration when the time
comes.  Even if the end result is the same - that coins older than "X" will
be invalidated - everything else important about the scenarios are
different as far as I can see.

Rodney



-------------------------------------
Greg,

If I understand correctly, the crux of your argument against BIP148 is that
it requires the segwit BIP9 activation flag to be set in every block after
Aug 1st, until segwit activates. This will cause miners which have not
upgrade and indicated support for BIP141 (the segwit BIP) to find their
blocks ignored by UASF nodes, at least for the month or two it takes to
activate segwit.

Isn't this however the entire point of BIP148? I understand if you object
to this, but let's be clear that this is a design requirement of the
proposal, not a technical oversight. The alternative you present (new BIP
bit) has the clear downside of not triggering BIP141 activation, and
therefore not enabling the new consensus rules on already deployed full
nodes. BIP148 is making an explicit choice to favor dragging along those
users which have upgraded to BIP141 support over those miners who have
failed to upgrade.

On an aside, I'm somewhat disappointed that you have decided to make a
public statement against the UASF proposal. Not because we disagree -- that
is fine -- but because any UASF must be a grassroots effort and
endorsements (or denouncements) detract from that.

Mark Friedenbach
-------------------------------------
This formatting of JSON isn't unheard of though, it's typically called JSON
Streaming[1]. As long as exchanges implementing the API actually follow the
BIP and keep one JSON object per line, it shouldn't be a problem to decode.

1. https://en.wikipedia.org/wiki/JSON_Streaming

On Tue, Mar 7, 2017 at 8:07 AM Marcel Jamin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Wed, Sep 13, 2017 at 4:57 AM, Mark Friedenbach via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Sidenote-ish, but I also believe it would be fairly trivial to keep a
per UTXO tally and demand additional fees when trying to respend a
UTXO which was previously "spent" with an invalid op count. I.e. if
you sign off on an input for a tx that you know is bad, the UTXO in
question will be penalized proportionately to the wasted ops when
included in another transaction later. That would probably kill that
DoS attack as the attacker would effectively lose bitcoin every time,
even if it was postponed until they spent the UTXO. The only thing
clients would need to do is to add a fee rate penalty ivar and a
mapping of outpoint to penalty value, probably stored as a separate
.dat file. I think.

-------------------------------------
Thomas,


This situation applies to soft forks as well.

- if you wish your software to validate correctly, it is not opt-in
- it requires coordination to activate without much orphan risk to miners (hence BIP9). Witness the long preparation time ahead of SegWit deployment for wallet providers, miners etc. to coordinate to support it on their systems
- after activation, it depends on people running it (most notably miners, otherwise the soft-fork is no longer enforced leading to a hard fork)
- awareness alone does not ensure full validation capability is retained during a soft fork

Therefore, these differences seem insignificant enough to merit treating soft and hard forks equal in terms of the coordination features afforded through the versionbits.

Sancho
-------------------------------------
I don't think encouraging mining more transactions is a good idea since 
it would promote inefficient transaction patterns. It's more efficient 
to send transactions with a high number of outputs/inputs instead of 
creating long transaction chains as some services do.

You might consider incentivizing miners to mine blocks that reduce the 
UTXO set size the most, or some other metric that promotes efficient 
uses of the blockchain.

On 27/03/17 17:12, Btc Ideas via bitcoin-dev wrote:

-------------------------------------
"Activation of segwit is defined by BIP9. After 15 Nov 2016 and before 15 Nov 2017 UTC, if in a full retarget cycle at least 1916 out of 2016 blocks is signaling readiness, segwit will be activated in the retarget cycle after the next one"

Just change BIP9 and the code to say "if in a full retarget cycle at least 1 out of 2016 blocks" and call it done. Or something very similar to this that effectively does the exact same thing. :) Wasting too much time on this. 15 Nov 2017 is plenty of time to be ready for SegWit, and if a participant is not ready by then they are either unreasonably lazy, a manipulator, or manipluted, and we don't need them.

If non-upgrading miners refuse to build on segwit blocks, or build on malicious invalid segwit blocks, then so be it. We fork. We have spent enough time trying to convince people who don't think for themselves... if they are still manipulated now then its time for us to give up on helping them see the light and instead let them learn the hard way.

Cheers,
Praxeology Guy

-------- Original Message --------
Subject: Re: [bitcoin-dev] Flag day activation of segwit
Local Time: March 13, 2017 5:18 PM
UTC Time: March 13, 2017 10:18 PM
From: bitcoin-dev@lists.linuxfoundation.org
To: shaolinfry <shaolinfry@protonmail.ch>, Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>

This has a different start time from the first post.

Thanks,
--Nick

On Mon, Mar 13, 2017 at 4:36 AM, shaolinfry via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
From: luke@dashjr.org
On Sunday, March 12, 2017 3:50:27 PM shaolinfry via bitcoin-dev wrote:

I don't think this is actually BIP 9 compatible. Once activated, the bit loses
its meaning and should not be set. So you need to check that it hasn't locked-
in already...

I believe that is handled.

time >= 1506816000 && time <= 1510704000 && !IsWitnessEnabled()

Signalling is only required from October 1st until the BIP9 timeout, or, until segwit is activated. The bit becomes free after activation/timeout as per BIP9. Also, the default behaviour of BIP9 in Bitcoin Core is to signal through the LOCKED_IN period - it would be trivial to add a condition to not require mandatory signalling during LOCKED_IN but since miners signal by default during this period, I figured I would leave it.

I thought about 5% tolerance. but I don't think it makes sense since miners will already have plenty of warning this is coming up and the intent of the mandatory signalling period is quite clear. It also seems a bit weird to say "it's mandatory but not for 5%". If miners are required to signal, they need to signal. It also adds unnecessary complexity to an otherwise simple patch.

That said, I have no strong feelings either way on both counts, but I chose to present the simplest option first.

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
On Fri, Mar 31, 2017 at 10:09 PM, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Miners signalling they have upgraded by flipping a bit in the nVersion
field has little relevance in a hard fork. If 100% of the hash power
indicates they are running this proposal, but the nodes don't upgrade, what
will happen?

For the record, I actually talk a lot about hard forks with various
developers and am very interested in the research that Johnson in
particular is pioneering. However, I have failed to understand your point
about 95% miner signalling in relation to a hard fork, so I am eagerly
awaiting your explanation.
-------------------------------------
This has been brought up several times in the past, and I agree with
Jonas' comments about users being unaware of the privacy losses due to
BIP37.  One thing also mentioned before but not int he current thread
is that the entire concept of SPV is not applicable to unconfirmed
transactions.  SPV uses the fact that miners have committed to a
transaction with work to give the user an assurance that the
transaction is valid; if the transaction were invalid, it would be
costly for the miner to include it in a block with valid work.

Transactions in the mempool have no such assurance, and are costlessly
forgeable by anyone, including your ISP.  I wasn't involved in any
debate over BIP37 when it was being written up, so I don't know how
mempool filtering got in, but it never made any sense to me.  The fact
that lots of lite clients are using this is a problem as it gives
false assurance to users that there is a valid but yet-to-be-confirmed
transaction sending them money.

-Tadge

-------------------------------------
On Wednesday 31 May 2017 1:22:44 AM Jorge Timn via bitcoin-dev wrote:

Because the bottleneck is hashing the transaction, which costs (in CPU time) 
based on its size. Maybe it would make sense to factor sigops into the limit, 
though?

On Wednesday 31 May 2017 1:09:26 AM Jean-Paul Kogelman via bitcoin-dev wrote:

Make it 100kB and I think we'd be okay. Those have always been policy-
forbidden so there should be no expectation they'd be acceptable in the 
future.

While we're at it, I suggest also specifying a minimum transaction size as 
well. The raw minimum possible is 60 bytes, but any sane output would need at 
least a hash, so I'd say make the minimum be 8 (60 + 160-bit hash) bytes?

Luke

-------------------------------------
I posted the following on bitcointalk.org and slack bitcoinunlimited. 
This isn't a technical paper, just fleshing out my thoughts and hoping for
some help & feedback.  I understand bitcoin as well as any non-programer
realisticly could, but I am not a programmer, so if this isn't feasible,
someone please let me know why.

-MoonShadow

Con-Peg Sidechain Model

I know that this is going to sound similar to the Fed-Peg model, so don't
whine about that. It's not the Fed-Peg model, not quite, and the
differences are critical, I believe.

Every proposal that I've seen so far require some kind of soft or hard
fork to the current bitcoin model, but I think that I've come up with a
way to make a sidechain work without new modifications to the running
bitcoin protocol.

I think I will call it the Confederation-Peg model.

Imagine a confederation of large corporations, all of which would benefit
from the ability to process a large number of bitcoin transactions for net
zero or near zero transaction fees.
These corporations would, most likely, have to have the following
characteristics...

1) Multi-national in scope, with employee bases in several different
nations using several different fiat currencies.
2) Have a rather large employee base.
3) Not in direct competition with each other
4) and not dependent upon any particular government.
With just a bit of google-fu, I will use the following corporations in
this example...

Wal-Mart, with more than 2 million employees worldwide.
Volkswagon, with more than half a million employees worldwide.
General Electric, with about 300K employees worldwide.
and Johnson & Johnson, with More than 100K.

Let's call these corporations the confederation sponsors. These sponsors
would decide most of the sidechain's rules by consensus amongst
themselves, but let me lay out, in general, how I think that such a
sidechain can be set up so that the sidechain is secure while also
contributing to the overall security of the main blockchain.

First, these sponsors agree upon a deposit/escrow amount that they will
each commit to a multi-sig address on the main blockchain; for a round
number, let's say they all contribute 10 BTC to the cause. Next, they all
agree that they must each either build or contract out bitcoin mining
capability of a minimum standard; high enough that the collection of
sponsors can mine a block on a regular interval. Let's say once each day.
But when they mine a main blockchain block, they place into the 100 byte
large "2nd nonce" space of the coinbase transaction the following data.

1) a code that identifies the sponsor who mined this block to the other
sponsors,
2) the merkle tree root hash of the sidechain block that the sponsor is
about to release on the sidechain network.
3) a cryptographic signing of the two prior pieces of data. (this might be
unnecessary, I'm not sure)

Once a sponsor's mining agent releases this block to the network, and it's
accepted as valid by the main blockchain, The sponsor then releases the
sidechain block to the other sponsors. This block can be of an arbitrarily
large size; enough to accommodate all of the transactions that all of the
sponsors (and their clients) have produced in the past day. Since it's
likely that every sponsor has seen every valid transaction, this block
might only include the merkle tree created by the most recent mining
sponsor.

This looks a lot like merged mining, but it's not, because the side chain
doesn't use proof-of-work, and doesn't require it. It uses
proof-of-authority. Specifically, releasing a valid block onto the main
blockchain is the proof of the authority to release the next sidechain
block. This achieves several things for the sponsors.

1) It contributes mining power to the main blockchain, thus supporting
main chain security regardless of the profitability for those sponsor
miners, since their incentive is to reduce the costs of their own
transactions, not win mining rewards or fees.
2) It creates a definate timeline of the blockchain of the sidechain,
without need for cryptographic proof-of-work, by tying each sidechain
block to a known main chain block. Thus leveraging the main chain's
security model without needing to attract miners willing to drop the main
chain work to mine the sidechain.
3) It establishes a definitive authority amonst the sponsors about who has
the right to publish the next block, as well as claim any sidechain
transaction fees.
4) It allows all sponsors to keep each other honest, because if any
sponsor were to cut back on their main chain mining responsibilities, they
would all be able to tell.
5) It allows the sponsors to chose to accept as many free transactions as
they like, which may or may not benefit themselves,
6) as well as keep any transaction fees that might have been issued on the
sidechain, for which odds are high that they would have had to pay. Thus
transaction fees most likely travel in a circle (for the sponsors, not the
clients)

In order to add btc to this sidechain, a main chain bitcoiner would have
to send funds to one of the sponsors, after acquiring their agreement to
issue sidechain coins using a special sidechain coinbase transaction
that...

1) creates or destroys sidechain bitcoins
2) references the main chain transaction that would permit it
3) and identifies the sponsor creating the sidechain funds

In this way, bitcoins can flow into the sidechain, and each of the
sponsors can watch the other sponsors to make certain that they aren't
creating more sidechain funds than their main chain holding would permit.
I would imagine that the rule would be that a sponsor can't issue more
side chain bitcoins than it has in it's public main chain addresses, and
if that were to be violated, the other sponsors would automatically ignore
their (otherwise valid) sidechain blocks.

This security model requires more trust than the trustless model of the
main blockchain, but permits the sidechain to structure itself in any way
necessary to permit safe referencing of unconfirmed transactions, thereby
permitting nearly instant follow-up transactions. Sponsors could also
detect, and potentially punish, double spend attemps. Any other rapid
transaction model, such as the Lightening Network, could be permitted to
work on the sidechain; but I doubt they would be necessary.
Sponsors could attract "clients" by a number of incentives. For example,
Wal-mart could offer free sidechain transactions to any paying customer,
as well as a limited number of main chain transactions to their own
employees; whereas Johnson & Johnson might only offer free transactions to
their employees and associated businesses. I can even imagine a deposit &
(fully BTC reserve backed) sidechain credit system, complete with interest
rates.

Paid for transaction fees could be based upon whatever the sponsors agree
to, including a transaction fee based upon a percentage of the transfer
value instead of the byte-size of a transaction. This would make the fee
model much closer to how current day credit card transfer fees work, but
would almost certainly be less.

Getting BTC back out of the sidechain (via the main chain) would work like
a sponsor's coinbase transaction with a negative value, also referencing a
valid transaction (which may or may not be confirmed yet) that can be seen
on the main bitcoin network. Alternatively, in a world where several such
sidechains exist, sponsors of one sidechain could be clients on another,
potentially permitting value to transfer from one sidechain directly to
another without creating a main blockchain transaction at all. The details
of the rules of both sidechains would matter in this possibility.

Since declaring weeknesses of one's own ideas is a convention in the
cryptocurrency world, let me begin...

Since this is a some-trust model; i.e. individuals have to trust an
institution, at least a little bit, in order to get onto the sidechain.
It's possible that a sponsor might take your main chain BTC and claim you
never sent them, but you'd still have the transaction you produced, so
you'd still have recourse through traditional courts.

Moving funds in the other direction, it's possible for your leaving
transaction to be blocked, but only if all of the sponsors refuse to deal
with you. Likewise, as a client, your ability to transact on the sidechain
could be hindered or blocked by the sponsors, but only if all of them
blacklist you. But that only risks the possibility that you can't spend
your bitcoins on the sidechain, not that the sponsors could take them from
you without your participation.

This is a move towards some centralization, yes; but not for bitcoin as a
whole. For the most part, "clients" choose whether the lower transaction
costs & convenience at these institutions is worth the re-addition of
trust to some portion of their bitcoin activities. Perhaps employees don't
get a choice about being a client on this sidechain, but they still get to
choose if they work for a sponsor.

This low-trust model depends upon the idea that the sponsors don't
entirely trust one another, and will keep an eye on each other for bad
behavior; much in the same way that the banks of the free banking era
would occasionally challenge one another to produce the gold for the
currencies they issued, either driving them out of business or harming
their businesses should they misbehave. It also depends upon the idea
that, for the "clients", no one on the sidechain has more to lose from
getting caught defrauding a client than the sponsors themselves, because
the integrity of the sidechain and of their own reputations are of great
value to the sponsors. It's possible that all sponsors turn to the dark
side at once, crash the sidechain & steal all of the main chain bitcoins
in their reserve addresses. Since this isn't one trusted authority, but
many in a trust-distrust relationship (and in different industries) this
possibility seems remote to me.

I could also imagine sidechains that were explicitly not worldwide in
scope, such as those limited to a particular nation or economic block.

I.E., there might be a Eurozone specific sidechain, a United States
specific sidechain (but would that be redundant?) and a Francophone
sidechain. There might be a sidechain for Portuguese speaking nations
around the world, or a sidechain just for nations in South America that
don't speak Portuguese.

There could be a sidechain that exists entirely on Tor, using high
anonymity rules; or a sidechain sponsored by governments for the expressed
purpose of paying taxes (but who would join this voluntarily?)

Many people have complained that Bitcoin isn't anonymous, because the
entire transaction history is visible. Sidechains would fix that
immediately, even without improved anonymity rules.

For that matter, since the extra-nonce space available in the coinbase
transaction is 100 bytes, that's enough to record an entire sidechain
block header anyway, so there might not be any reason to record the
headers anyplace else.







-------------------------------------
On Wed, Sep 06, 2017 at 09:59:54PM -0400, Russell O'Connor via bitcoin-dev wrote:

Note that in general, designs should *not* create new hash functions by using
custom IVs, but rather use bog-standard SHA256, and make a fixed first block.
That allows unoptimised implementations to just hash a block with the second
initialization value, and optimized implementations to start with the fixed
midstate.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
I concur with Mark's reply. Just to underscore this: Miners arent going to bother to signaling or changing a setting unless they have to. Anything that requires time--especially if requiring a restart/any time not mining or risks a crash---reduces income. So why would they change any settings unless they have to?

-Ryan J. Martin


On Jun 20, 2017 1:26 PM, Mark Friedenbach via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
I think it is very naïve to assume that any shift would be temporary.
We have a hard enough time getting miners to proactively upgrade to
recent versions of the reference bitcoin daemon. If miners interpret
the situation as being forced to run non-reference software in order
to prevent a chain split because a lack of support from Bitcoin Core,
that could be a one-way street.

On Tue, Jun 20, 2017 at 9:49 AM, Hampus Sjöberg via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
On 11/30/2017 10:20 PM, mandar mulherkar via bitcoin-dev wrote:

Check out namecoin, there is also blockstack as someone mentioned, but I personally feel namecoin is technologically better.

If do not want a static bitcoin address mapped to a username, could use a stealth address (we need more
support for stealth addresses in bitcoin wallets).

-------------------------------------
On Feb 12, 2017 23:58, "Eric Voskuil via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

The BIP151 proposal states:

the encinit messages.

This statement is incorrect. Sending content that existing nodes do not
expect is clearly an incompatibility. An implementation that ignores
invalid content leaves itself wide open to DOS attacks. The version
handshake must be complete before the protocol level can be determined.
While it may be desirable for this change to precede the version
handshake it cannot be described as backward compatible.


The worst possible effect of ignoring unknown messages is a waste of
downstream bandwidth. The same is already possible by being sent addr
messages.

Using the protocol level requires a strict linear progression of (allowed)
network protocol features, which I expect to become harder and harder to
maintain.

Using otherwise ignored messages for determining optional features is
elegant, simple and opens no new attack vectors. I think it's very much
preferable over continued increments of the protocol version.

-- 
Pieter
-------------------------------------
Not sure if you are BFD or BF Trolling D, BFTD. But I will bite this time.

Sorry I mistakenly forgot to change the subject back to "A Better MMR Definition" when I decided to send the email to the dev list instead of directly to Peter. So then you made such a reply without knowing context.

With using the MMR data structure for txo commitments, its preferable that wallets only keep information pertinent to their own spendable coins. In previous communication we talked about how wallets could maintain the changing MMR proof for their old coins. Yes wallets know which of their own coins are spent. But with MMR proofs wallets also need to know the spentness status of close relatives in the MMR tree... in order to construct a valid MMR proof that their own coin is not spent.

Hope that... clears it up for you.

Cheers,
P. Guy

-------- Original Message --------
Subject: Re: [bitcoin-dev] Guessing the spentness status of the pruned relatives
Local Time: April 1, 2017 6:38 PM
UTC Time: April 1, 2017 11:38 PM
From: bfd@cock.lu
To: praxeology_guy <praxeology_guy@protonmail.com>, Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>

If a wallet is unaware of spends of its own coins (ie, transactions
were made it can't have known about), there's probably bigger problems
going on. You might enjoy the topic on this mailing list on committed
bloom filters however, as this solves a similar issue without needing
an ever-growing list of hundreds of millions of spent outputs.

On 2017-04-02 06:04, praxeology_guy via bitcoin-dev wrote:
-------------------------------------
Finally got some time over the Chinese New Year holiday to code and write this up. This is not the same as my previous forcenet ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html ). It is much simpler. Trying to activate it on testnet will get you banned. Trying to activate it on mainnet before consensus is reached will make you lose money.

This proposal includes the following features:

1. A fixed starting time. Not dependent on miner signalling. However, it requires at least 51% of miners to actually build the new block format in order to get activated.

2. It has no mechanism to prevent a split. If 49% of miners insist on the original chain, they could keep going. Split prevention is a social problem, not a technical one.

3. It is compatible with existing Stratum mining protocol. Only pool software upgrade is needed

4. A new extended and flexible header is located at the witness field of the coinbase transaction

5. It is backward compatible with existing light wallets

6. Dedicated space for miners to put anything they want, which bitcoin users could completely ignore. Merge-mining friendly.

7. Small header space for miners to include non-consensus enforced bitcoin related data, useful for fee estimation etc.

8. A new transaction weight formula to encourage responsible use of UTXO

9. A linear growth of actual block size until certain limit

10. Sighash O(n^2) protection for legacy (non-segwit) outputs

11. Optional anti-transaction replay

12. A new optional coinbase tx format that allows additional inputs, including spending of immature previous coinbase outputs



Specification [Rationales]:


Activation:

* A "hardfork signalling block" is a block with the sign bit of header nVersion is set [Clearly invalid for old nodes; easy opt-out for light wallets]

* If the median-time-past of the past 11 blocks is smaller than the HardForkTime (exact time to be determined), a hardfork signalling block is invalid.

* Child of a hardfork signalling block MUST also be a hardfork signalling block

* Initial hardfork signalling is optional, even if the HardForkTime has past [requires at least 51% of miners to actually build the new block format]

* HardForkTime is determined by a broad consensus of the Bitcoin community. This is the only way to prevent a split.


Extended header:

* Main header refers to the original 80 bytes bitcoin block header

* A hardfork signalling block MUST have a additional extended header

* The extended header is placed at the witness field of the coinbase transaction [There are 2 major advantages: 1. coinbase witness is otherwise useless; 2. Significantly simply the implementation with its stack structure]

* There must be exactly 3 witness items (Header1; Header2 ; Header3)
**Header1 must be exactly 32 bytes of the original transaction hash Merkle root.
**Header2 is the secondary header. It must be 36-80 bytes. The first 4 bytes must be little-endian encoded number of transactions (minimum 1). The next 32 bytes must be the witness Merkle root (to be defined later). The rest, if any, has no consensus meaning. However, miners MUST NOT use this space of non-bitcoin purpose [the additional space allows non-censensus enforced data to be included, easily accessible to light wallets]
**Header3 is the miner dedicated space. It must not be larger than 252 bytes. Anything put here has no consensus meaning [space for merge mining; non-full nodes could completely ignore data in this space; 252 is the maximum size allowed for signal byte CompactSize]

* The main header commitment is H(Header1|H(H(Header2)|H(Header3)))  H() = dSHA256() [The hardfork is transparent to light wallets, except one more 32-byte hash is needed to connect a transaction to the root]

* To place the ext header, segwit becomes mandatory after hardfork


A “backdoor” softfork the relax the size limit of Header 2 and Header 3:

* A special BIP9 softfork is defined with bit-15. If this softfork is activated, full nodes will not enforce the size limit for Header 2 and Header 3. [To allow header expansion without a hardfork. Avoid miner abuse while providing flexibility. Expansion might be needed for new commitments like fraud proof commitments]


Anti-tx-replay:

* Hardfork network version bit is 0x02000000. A tx is invalid if the highest nVersion byte is not zero, and the network version bit is not set.

* Masked tx version is nVersion with the highest byte masked. If masked version is 3 or above, sighash for OP_CHECKSIG alike is calculated using BIP143, except 0x02000000 is added to the nHashType (the nHashType in signature is still a 1-byte value) [ensure a clean split of signatures; optionally fix the O(n^2) problem]

* Pre-hardfork policy change: nVersion is determined by the masked tx version for policy purpose. Setting of Pre-hardfork network version bit 0x01000000 is allowed.

* Details: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013473.html


Sighash limitation:

* Sighash impact is estimated by “Loose estimation” in https://github.com/jl2012/bips/blob/065ea7429035d43ff90965f42b086fb7e1517291/bip-sighash.mediawiki

* Only txs with masked version below 3 are counted. [because they are fixed by the BIP-143 like signature]

* Each SigHashSize is defined as 1 tx weight (defined later).

* SIGHASH_SCALE_FACTOR is 90 (see the BIP above)


New tx weight definition:

* Weight of a transaction is the maximum of the 4 following metrics:

** The total serialised size * 2 * SIGHASH_SCALE_FACTOR  (size defined by the witness tx format in BIP144)

** The adjusted size = (Transaction weight by BIP141 - (number of inputs - number of non-OP_RETURN outputs) * 41) * SIGHASH_SCALE_FACTOR

** nSigOps * 50 * SIGHASH_SCALE_FACTOR. All SigOps are equal (no witness scaling). For non-segwit txs, the sigops in output scriptPubKey are not counted, while the sigops in input scriptPubKey are counted.

** SigHashSize defined in the last section

Translating to new metric, the current BIP141 limit is 360,000,000. This is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of adjusted size, or 80000 nSigOp.

See rationales in this post: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html


Block weight growing by time:

* Numbers for example only. Exact number to be determined.

* Block weight at HardForkTime is (5,000,000 * SIGHASH_SCALE_FACTOR)

* By every 16 seconds growth of the median-time-past, the weight is increased by (1 * SIGHASH_SCALE_FACTOR)

* The growth stops at (16,000,000 * SIGHASH_SCALE_FACTOR)

* The growth does not dependent on the actual hardfork time. It’s only based on median-time-past [using median-time-past so miners have no incentive to use a fake timestamp]

* The limit for serialized size is 2.5 to 8MB in about 8 years. [again, numbers for example only]


New coinbase transaction format:

* Existing coinbase format is allowed, except the new extended header in the coinbase witness. No OP_RETURN witness commitment is needed.

* A new coinbase format is defined. The tx may have 1 or more inputs. The outpoint of the first input MUST have an n value of 0xffffffff, and use the previous block hash as the outpoint hash [This allows paying to the child of a particular block by signing the block hash]

* ScriptSig of the first (coinbase) input is not executed. The size limit increased from 100 to 252 (same for old coinbase format)

* Additional inputs MUST provide a valid scriptSig and/or witness for spending

* Additional inputs may come from premature previous coinbase outputs [this allows previous blocks paying subsequent blocks to encourage confirmations]


Witness merkle root:

* If the coinbase is in old format, the witness merkle root is same as BIP141 by setting the witness hash of the coinbase tx as 0 (without the 32 byte witness reserved value)

* If the coinbase is in new format, the witness hash of the coinbase tx is calculated by first removing the extended header

* The witness merkle root is put in the extended header 2, not as an OP_RETURN output in coinbase tx.

* The witness merkle root becomes mandatory. (It was optional in BIP141)


Other consensus changes:

* BIP9 will ignore the sign bit. [Setting the sign bit now is invalid so this has no real consensus impact]

========

An experimental implementation of the above spec could be found at https://github.com/jl2012/bitcoin/tree/spoonnet1

Not the same as my previous effort on the “forcenet”, the “spoonnet” is a full hardfork that will get you banned on the existing network.

Haven’t got the time to test the codes yet, not independently reviewed. But it passes all existing tests in Bitcoin Core. No one should use this in production, but I *think* it works fine on testnet like a normal bitcoind (as long as it is not activated)

Things not implemented yet:

1. Automated testing

2. Post-hardfork support for old light wallets

3. Wallet support, especially anti-tx-replay

4. New p2p message to transmit secondary header (lower priority)

5. Full mining and mempool support (not my priority)

========

Potential second stage change:

Relative to the actual activation time, there could be a second stage with more drastic changes to fix one or both of the following problems:

1. SHA256 shortcut like ASICBoost. All fixes to ASICBoost are not very elegant. But the question is, is it acceptable to have bitcoin-specific patent in the consensus protocol? Still, I believe the best way to solve this problem is the patent holder(s) to kindly somehow release the right to the community. 

2. Providing more nonce space in the 80-byte main header. However, this depends on ASICBoost being a free technology.

3. Block withholding attack. There are pros and cons, but I generally agree with the analysis by Peter Todd at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012046.html . One point he didn’t mention is that only small really needs pool mining, for the purpose of variance reduction. Big miners using pools are just lazy, and they work well without pool. That means only big solo miners are able to attack pools (i.e. small miners), while pools cannot do any counterattack. This obviously shows why fixing this is pro-small-miners. Also, with same hash rate, block withholding attack is more effective against a smaller pool than a big pool.

All of these changes involve a header change and require light wallets to upgrade. They also require firmware upgrade for all existing miners (change 2 doesn’t). I think these shouldn’t happen at least 2 years after the actual activation of the hardfork so people will have enough time to upgrade.

-------------------------------------
Hello,

Really wish I'd known you were working on this a few weeks ago, but
such is life. Hopefully I can provide some useful feedback.

On Fri, Jun 2, 2017 at 4:01 AM, Olaoluwa Osuntokun via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Is it necessary to maintain the index all the way to the beginning of
the chain? When would clients request "really old digests" and why?


I haven't tried the tool yet, and maybe it will answer some of my questions.

On what data were the simulated wallets on actual data based? How did
false positive rates for wallets with lots of items (pubkeys etc) play
out? Is there a maximum number of items for a wallet before it becomes
too bandwidth costly to use digests?


I will definitely try to reproduce my experiments with Golomb-Coded
sets and see what I come up with. It seems like you've got a little
less than half the size of my digests for 1-block digests but I
haven't tried making digests for all blocks (and lots of early blocks
are empty).

On the BIP proposal itself:

In Compact Filter Header Chain, you mention that clients should
download filters from nodes if filter_headers is not identical, and
ban offending nodes. What about temporary forks in the chain? What
about longer forks? In general, I am curious how you will deal with
reorgs and temporary non-consensus related chain splits.

I am also curious if you have considered digests containing multiple
blocks. Retaining a permanent binsearchable record of the entire chain
is obviously too space costly, but keeping the last X blocks as
binsearchable could speed up syncing for clients tremendously, I feel.

It may also be space efficient to ONLY store older digests in chunks
of e.g. 8 blocks. A client syncing up finding a match in an 8-block
chunk would have to grab those 8 blocks, but if it's not recent, that
may be acceptable. It may even be possible to make 4-, 2-, 1-block
digests on demand.

How fast are these to create? Would it make sense to provide digests
on demand in some cases, rather than keeping them around indefinitely?

-------------------------------------
Just to expand a tiny bit here, while the testnet setup of only a few nodes acting as "bridges", mainnet already has many systems which act as effective bridges today - there are several relay networks in use which effectively bypass the P2P network, including my legacy relay network (which many miners historically have used, and I'd expect those who aren't paying attention and don't upgrade will not turn off, fixing the issue for them), ViaBTC's super aggressive bandwidth-wasting block announcement network which pushes blocks from several pools to many nodes globally, and Bitcoin.com's private relay network. (Of course many other miners and pools have private relay networks, but the several other such networks I'm aware of are already segwit-compatible, even for pools not signaling segwit).

Matt

On March 27, 2017 12:22:43 PM PDT, Suhas Daftuar via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
-------------------------------------
These are non-answers.  Someone must decide.  Someone must decide what kind
of company counts (e.g. does a dark market seller count as a business?
Does some guy who sells $10/year worth of goods using Bitcoin count the
same as large companies like Coinbase/BitPay/Blockstream).  Someone must
decide which websites are checked for votes or addresses.  Someone must
decide if a rogue employee made a transaction on behalf of the company or
not.

Registering domain names is trivial and can be automated if the incentives
were needed for it.

You mention developers who have commit access.  This excludes the vast
majority of developers.  You also don't mention which repositories count.
Do the developers of bcoin count or not?

These questions all would need to be answered before any kind of proposal
like this can be taken seriously.  Without these kinds of answers, this
proposal is far from complete.


On Fri, Feb 3, 2017 at 12:20 PM, t. khan <teekhan42@gmail.com> wrote:

-------------------------------------
Jonas, I think his proposal is to enable extending the P2P layer, e.g.
adding new message types. Are you suggesting having externalized
message processing? That could be done via RPC/ZMQ while opening up a
much more narrow attack surface than dlopen, although I imagine such
an interface would require a very complex API specification.

On Sun, Aug 13, 2017 at 1:00 PM, Jonas Schnelli via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Thank you for your time Gregory, I really appreciate that.

What we are describing here is a method to embed cryptographic signatures
into a public key based on HD Wallets - BIP32.
In a practical application, we should have two cryptographic signatures
from both sides, I don't think in that case your scenario would be an issue.

More specifically in our application, we do the following construction:

contract base: m/200'/0'/<contract_number>'
payment base (merchant commitment):
contract_base/<merchant_contract_signature>
payment address (customer commitment):
contract_base/<merchant_contract_signature>/<customer_contract_signature>

payment address funds could be reclaimed only if the
customer_contract_signature is provided by the customer.

In terms of durability, our app is pretty simple at this point, we don't
store anything, we let customer download and manage the files.

I will update the BIP to address your concerns.

On Tue, Aug 15, 2017 at 8:12 AM, Gregory Maxwell <greg@xiph.org> wrote:

-------------------------------------
On Sunday 01 October 2017 9:32:56 PM Johnson Lau wrote:

I like (A) and (B). Use B when practical, and (A) when more fundamental 
changes are needed. SigAgg is a concern, but there are ways to adapt it.

(C) is harmless, but I think unnecessary with (A) and/or (B).


Note that my BIP draft supports both (A) and (C).


It seems inevitable at this point. Maybe we could add a separate "executable-
witness" array (in the same manner as the current witness was softforked in), 
and require tail-call and condition scripts to merely reference these by hash, 
but I'm not sure it's worth the effort?

Thinking further, we could avoid adding a separate executable-witness 
commitment by either:
A) Define that all the witness elements in v1 are type-tagged (put the minor
   witness version on them all, and redefine minor 0 as a stack item?); or
B) Use an empty element as a delimiter between stack and executable items.

To avoid witness malleability, the executable items can be required to be 
sorted in some manner.

The downside of these approaches is that we now need an addition 20 or 32 
bytes per script reference... which IMO may possibly be worse than losing 
static analysis. I wonder if there's a way to avoid that overhead?

Luke

-------------------------------------
I also am for the idea of removing blocksize limits if it is workable, however, would propose an alternative method for selecting transactions to be included in a block.


Some of the issues discussed in other replies to this thread are valid.


Alternative proposal:

Provide each transaction with a transaction weight, being a function of the fee paid (on a curve), and the time waiting in the pool (also on a curve) out to n days (n=30 ?), the transaction weight serving as the likelihood of a transaction being included in the current block, and then use an uncapped block size. The curve allows that the higher a fee allows a transaction to be much more likely to be included, the highest fee gets 100%, and, transactions at the n days limit get near 100%. Would need protocol enforcement since, as I understand, no miner would mine more transactions than are necessary to meet min blocksize. Other than that it should function fine. Non-urgent transactions pay a lower fee, people choose fees from fee recommendation based on how many days before a tx begins confirmation, all transactions are eventually included in the blockchain.


Regards,

Damian Williamson

________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of William Morriss via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Thursday, 30 November 2017 11:47:43 AM
To: bitcoin-dev@lists.linuxfoundation.org
Subject: [bitcoin-dev] BIP Idea: Marginal Pricing

Comrades,

Long term, tx fees must support hash power by themselves. The following is an economic approach to maximize total fee collection, and therefore hashpower.

Goals
Maximize total transaction fees
Reduce pending transaction time
Reduce individual transaction fees

Challenges
Validators must agree on the maximum block size, else miners can cheat and include extra transactions.
Allowing too many transactions per block will increase the cost of the mining without collecting much income for the network.

Problem
In the transaction market, users are the demand curve, because they will transact less when fees are higher, and prefer altcoins. The block size is the supply curve, because it represents miners' willingness to accept transactions.
Currently, the supply curve is inelastic:
[cid:ii_jalpxsnl1_1600a3d9def1eaff]
​Increasing the block size will not affect the inelasticity for any fixed block size. The downsides of a fixed block size limit are well-known:
- Unpredictable transaction settlement time
- Variable transaction fees depending on network congestion
- Frequent overpay

Proposal
1. Miners implicitly choose the market sat/byte rate with the cheapest-fee transaction included in their block. Excess transaction fees are refunded to the inputs.
2. Remove the block size limit, which is no longer necessary.

Benefits
- Dynamic block size limit regulated by profit motive
- Transaction fees maximized for every block
- No overpay; all fees are fair
[cid:ii_jalqir4g2_1600a4c89811347a]
​Miners individually will make decisions to maximize their block-reward profit.
Miners are incentivized to ignore low-fee transactions because they would shave the profits of their other transactions and increase their hash time.
Users and services are free to bid higher transaction fees in order to reach the next block, since their excess bid will be refunded.

The block size limit was added as a spam-prevention measure, but in order for an attacker to spam the network with low-fee transactions, they would have to offset the marginal cost of reducing the price with their own transaction fees. Anti-spam is thus built into the marginal system without the need for an explicit limit.

Rarely, sections of the backlog would become large enough to be profitable. This means every so many blocks, lower-fee transactions would be included en masse after having been ignored long enough. Low-fee transactions thus gain a liveness property not previously enjoyed: low-fee transactions will eventually confirm. Miners targeting these transactions would be at a noteworthy disadvantage because they would be hashing a larger block. I predict that this scheme would result in two markets: a backlog market and a real-time market. Users targeting the backlog market would match the price of the largest backlog section in order to be included in the next backlog block.

Examples

Scenario 1
Sat/byte        Bytes   Reward
400     500000  200000000
300     700000  210000000
200     1000000 200000000
100     1500000 150000000
50      5000000 250000000
20      10000000        200000000
A miner would create a 5MB block and receive 0.25 BTC

Scenario 2
Sat/byte        Bytes   Reward
400     600000  240000000
300     700000  210000000
200     1000000 200000000
100     1800000 180000000
50      4000000 200000000
20      10000000        200000000
A miner would create a 600KB block and receive 0.24 BTC

Thanks,
William Morriss
-------------------------------------
On Wed, Sep 27, 2017 at 12:06:54PM -0400, Peter Todd via bitcoin-dev wrote:

Just remembered: it's notable how Coinbase has a 10 minute timeout on their
payment window, which is in effect a 10 minute expiry time for the address.
Presumably they'd make use of this feature if it existed.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
vote, no repercussions, and no collective action barrier that needs to be
overcome.

There is another interesting analysis on BMM and drivechains from /u/almkglor
on reddit
<https://www.reddit.com/r/Bitcoin/comments/6ztp3b/lets_discuss_something_techrelated_for_a_change/dn0rsdo/>.
I'm going to share here for visibility.

The problem with drivechains and blind merged mining is the disconnect
TLDR: a miner is most profitable if he always accepts BMM bribes, but
downvotes withdrawal transactions (WT). This obviously isn't ideal because
a withdrawal will never occur from the drivechain if enough miners employ
this strategy -- which seems to be the most profitable strategy.

-Chris


On Mon, Dec 4, 2017 at 1:36 PM, Chris Pacia via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hi Y'all,

Thanks to luke-jr and jl2012 for publishing your analysis of the
xblocks proposal. I'd like to also present some analysis but instead focus
on the professed LN safety enhancing scheme in the proposal. It's a bit
underspecified, so I've taken the liberty of extrapolating a bit to fill
in the gaps to the point that I can analyze it.

TLDR; The xblock proposal includes a sub-proposal for LN which is
essentially a block-size decrease for each open channel within the network.
This decrease reserves space in blocks to allow honest parties guaranteed
space in the blocks to punish dishonest channel counter parties. As a result
the block size is permanently decreased for each channel open. Some may
consider this cost prohibitively high.



As the proposal stands now, it seems that users _are_ able to unilaterally
use this for all their Bitcoin transactions, as there's no additional cost
to using the smart-contract safety feature outlined in the proposal.

The new safety measures proposed near the end of this xblock proposal
could itself consume a dedicated document outlining the prior background,
context, and implications of this new safety feature. Throughout the rest
of this post, I'll be referring to the scheme as a Pre-Allocated
Smart-contract Dispute arena (PASDA, chosen because it sounds kinda like
"pasta", which brings me many keks). It's rather insufficiently described
and
under specified as it stands in the proposal. As a result, if one doesn't
have the necessary prior context, it might've been skipped over entirely
as it's difficult to extract the sub-proposal from the greater proposal. I
think I possess the necessary prior context required to required to
properly analyze the sub-proposal. As a result, I would like to illuminate
the readers of the ML so y'all may also be able to evaluate this
sub-proposal independently.


## Background

First, some necessary background. Within LN as it exists today there is
one particularly nasty systematic risk related to blockchain availability
in the case of a channel dispute. This risk is clearly outlined in the
original white paper, and in my opinion a satisfactory solution to the
risks which safe guard the use of very high-value channels has yet to be
presented.


### Chain Spam/Censorship Attack Vector

The attack vector mentioned in the original paper is a reoccurring attack
in systems of this nature: DoS attacks. As it stands today, if a channel
counterparty is able to (solely, or in collaboration with other attackers)
prevent one from committing a transaction to the chain, they're able to
steal money from the honest participant in the channel. The attack
proceeds something like this:

   * Mallory opens a very large channel with me.
   * We transfer money back and forth in the channel as normal. The nature
     of these transfers isn't very important. The commitment balances may
     be modified due to Mallory making multi-hop payments through my
     channel, or possibly from Mallory directly purchasing some goods I
     offer, paying via the channel.
   * Let's call the current commitment number state S_i. In the lifetime
     of the channel there may exist some state S_j (i < j) s.t Mallory's
     balance in S_i, is larger than S_j.
   * At this point, depending on the value of the channel's time-based
     security parameter (T) it may be possible for Mallory to broadcast
     state S_i (which has been revoked), and prevent me being able to
     include by my punishment transaction (PTX) within the blockchain.
   * If Mallory is able to incapacitate me for a period of time T, or
     censor my transactions from the chain (either selectively or via a
     spam attack), then at time K (K > T + B, where B is the time the
     commitment transaction was stamped in the chain), then she'll be free
     to walk away with her settled balance at state S_i. For the sake of
     simplicity, we're ignoring HTLC's.
   * Mallory's gain is the difference between the balance at state S_i and
     S_j. Deepening on the gap between the states, my settled balance at
     state S_i and the her balance delta, she may be able to fully recoup
     the funds she initially place in the channel.


### The Role of Channel Reserves as Partial Mitigation

A minor mitigation to this attack that's purely commitment transaction
policy is to mandate that Mallory's balance in the channel never dips
below some reserve value R. Otherwise, if at state S_j, Mallory has a
settled balance of 0 within he channel (all the money if on my side), then
the attack outline above can under certain conditions be _costless_ from
her PoV. Replicate this simultaneously across the network in a synchronized
manner (possibly getting some help from your miner friends) and this
becomes a bit of a problem (to say the least).

Taking this a step further another mitigation that's been proposed is to
also use the channel reserve to implement a _ceiling_ on the maximum size
of _any_ in flight HTLC. Similar to the scheme above, this is meant to
eliminate the possibility of a "costless" attack, as if channel throughput
is artificially constrained, then the value of pending HTLC's isn't
enticing enough to launch a channel breach attack.


### Analysis of Attack Feasibility/Difficulty

The difficulty of the attack is dependant on the time-denominated security
parameter T, and the adversaries ability to collude with miners. Purely
spamming the chain given a very larger T value may be prohibitively
expensive for the attacker and their profit from launching the attack
would need to outweigh the cost in transaction fees and idle bitcoin
required to launch the attack. Considering the case of colluding with
miners, if mining is highly centralized (as it is now), then that may be a
more attractive attack avenue. In a world of highly decentralized mining
(let's say a lofty goal of no pool commanding > 5% of the hash power),
then the attack is much more difficult.

(as an aside schemes that involve transactions committing to the inputs
they're spending and revealing them at a later date/block (committed
transactions) may address the miner censorship attack vector)

Depending one's target use of channels, the individuals they open channels
with, the applications that run on top of the channels, the amount of
coins within the channel, and the choice of the time parameter T, the
attack outline above may or may not be an issue from your PoV.  However,
in order to realize LN's maximum potential of being able to enter a
smart-contract with a complete stranger on the internet trustlessly,
without fearing conditions that may lead to monetary losses, the attack
vector should be mitigated if possible.

In the words of The Architect of the Matrix (and referenced by Tadge at
his "Level of LN" talk at Scaling Bitcoin Hong Kong: "There are levels of
survival we are prepared to accept". There exist levels of LN and usage of
channels, that may not consider this a dire issue.

OK, with the necessary background and context laid out, I'll now analyze
the solution proposed within the greater xblock proposal, making a brief
detour to briefly described another proposed solution.

### Timestop

A prior proposed solution to the failure scenario described above is
what's known as "time stop". This was proposed by gmaxwell and was briefly
touched upon in the original LN white paper. The mechanism of the
time-denominated security parameter T in today's channel construction is
enforced using OpCheckSequenceVerify. After broadcasting a commitment
transaction, all outputs paying to the broadcaster of the commitment are
encumbered with a relative time delay of T blocks, meaning they are unable
to claim the funds until time T has elapsed. This time margin gives the
honest party an opportunity to broadcast their punishment transaction
iff, the broadcaster has broadcast a prior revoked state.

The idea of time stomp is to introduce a special sequence-locks block
height to the system. This block height would increase with each block
along with the regular block height _unless_ the block reaches a certain
sustained "high water mark". As an example, let's assume that when 3
blocks in row are above 75% capacity, then the sequence-lock clock stops
ticking.

The effect of this change is to morph the security risk into simply a
postponement of the judgment within the contract. With this, DoS attacks
simply delay the (seemingly) inevitable punishment of the dishonest party
within the contract.

Aside from some informal discussions and the brief section within the
original white paper, many details of this proposal are left
underspecified. For example: how do miners signal to full nodes that the
sequence-lock clock has stopped? What's the high water mark threshold? Can
it go on indefinitely? Should this feature be opt-in?

I think this proposal should be considered in tandem with the proposal
within the xblock proposal as both have a few unanswered questions that
need to be further explored.

## Pre-Allocated Smart-Contract Dispute Area (PASDA)

Aight, now to the LN enhancing proposal that's buried within the
greater xblock proposal. Introducing some new terminology, I've been
calling this a: Pre-Allocated Smart-contract Dispute Arena or (PASDA) for
short. In a nut shell, the key idea of the proposal is this: transactions
that mark the commencement of a smart contract who's security depends on
availability of block space for disputes are able to _pre allocate_ a
section of the block that will _always_ be _reserved_ for dispute
transactions. With this, contracts is  _guaranteed_ space in blocks to
handle disputes in the case that the contract breaks down. As an analogy:
when you enter in a contract with a contractor to build your dream
kitchen, you _also_ go to a court and reserve a 1-hour block in their
scheduled to handle a dispute _just in case_ one arises. In the event of a
peaceful resolution to the contract, the space is freed up.

The description in the paper is a bit light on the details, so I'll say up
front that I'm extrapolating w.r.t to some mechanisms of the construction.
However, I've been involved in some private conversations where the idea
was thrown around, so I think I have enough context to _maybe_ fill in
some of the gaps in the proposal.

I'll now restate the proposal. Smart contract transactions set a certain
bit in their version number. This bit indicates that they wish to
pre-allocate N bytes in _all_ further blocks _until_ the contract has been
reserved. In the specific context of payment channels, this means that
once a channel is open, until it has been closed, it _decreases_ the
available block size for all other transactions. As this is a very
aggressive proposal I think the authors took advantage of the new design
space within xblocks to include something that may not be readily accepted
as a modification to the rules of the main chain.

The concrete parameters chosen in the proposal are: each channel opening
transaction reserves 700-bytes within _each_ block in the chain until the
transaction has been closed. This pre-allocation has the following
constraint: a transaction can _only_ take advantage of this allocation iff
it's spending the _first_ output of a smart-contract transaction (has a
particular bit in the version set). This means that only dispute
resolution transactions can utilize this space.

The proposal references two allocations, which I've squinted very hard at
for half a day in an attempt to parse the rules governing them, but so far
I've been unable to glean any further details. From my squinting, I
interpret that half of the allocation is reserved for spending the
self-output of a transaction in the last 2016 blocks (two weeks) and the
other half is dedicated to spending the first output of a commitment
transaction in the _same_ block.

I'm unsure as to why these allocations are separate, and why they aren't
just combined into a single allocation.

### Modification to LN Today

This change would require a slight modification to LN as it's currently
defined today. ATM, we use BIP 69 in order the inputs and outputs of a
transaction. This is helpful as it lets us just send of signatures for new
states as both sides already know the order of the inputs and outputs.
With PASDA, we'd now need to omit the to-self-output (the output in my
commitment transaction paying to myself my settled balance) from this
ordering and _always_ make it the first output (txid:0).

The second change is that this proposal puts a ceiling on on the CSV value
allowed by any channel. All CSV delays _must-weeks otherwise, they're
unable to take advantage of the arena.

### Modifications to Bitcoin

In order to implement this within Bitcoin, a third utxo set (regular
block, xblock) must be maintained by all full nodes. Alternatively, this
can just be a bit in the xblock utxo set. The implementation doesn't
really matter. Before attempting to pack transactions into a block, the
total allocation within the PASDA utxo-set must be summed up, and
subtracted from the block size cap. Only transactions which validly spend
from one of these UTXO's are able to take advantage of the new space in
the block.

## Analysis of PASDA

OK, now for some analysis. First, let's assume that transactions which
create PASDA UTXO's aren't subject to any additional constraints. If so,
then this means that _any_ transaction can freely create PASDA UTXO's and
_decrease_ the block size for _all_ transactions until the UTXO has been
spent. If my interpretation is correct, then this introduces a new attack
vector which allows _anyone_ to nearly permanently decrease the block size
for all-time with next to zero additional cost. If this is correct, then
it seems that miners have _zero_ incentive to _ever_ include a transaction
that creates a PASDA output in their blocks as it robs them of future
revenue and decreases the available capacity in the system, possibly
permanently proportionally to _each_ unspent PASDA output in the chain.

Alternatively, let's say the transactions which create PASDA outputs
_must_ pay a disproportionately high fee in order to pay up front for
their consumption of the size within all future blocks. If so, then a
question that arises is: How large a fee? If the fee is very large, then
the utilization of the smart-contract battling arena is only reserved to
very high valued channels who can afford very high fees. This may be
acceptable as if you have a $5 channel, then are you really at risk at
such a large scale attack on Bitcoin just to steal $5 from you? It's
important to note that many attacks on LN's contract resolution
capabilities are also a direct attack on Bitcoin. However, in a world of
dynamic fees, then it may be the case that the fee paid 6 months ago is
now a measly fee an no longer covers the costs to miners (and even the
entire system...).

Finally, here's something I thought of earlier today that possibly
mitigates the downside from the PoV of the miners (everyone else must
still accept the costs of a permanent block size decrease). Let's say that
in order to create a PASDA output fees are paid as normal. However, for
_each_ subsequent block, the participants of the contract _must_ pay a
tribute to miners to account for their loss in revenue due to the
reduction in block size. Essentially, all PASDA outputs must pay _rent_
for their pre-allocated space. If rent isn't paid sufficiently and on-time,
then the pre-allocate arena space is revoked by miners. There're a few
ways to construct this payment, but I'll leave that to follow up work as I
just want to shed some light on the PASDA and its implications.

## Conclusion

I've attempted to fill in some gaps for y'all w.r.t exactly what the
sub-proposal within the greater xblock proposal consists of and some
possible implications. I'd like to note that I've taken the liberty of
filling on some gaps within the sub-proposal as only a single section
within the greater proposal has been allocated to it. PASDA itself could
likely fill up an entirely distinct propsal by itself spanning several
pages. To the authors of the proposal: if my interpretation is inaccurate
please correct me as I'd also like to better understand the proposal. It's
possible that everything I've said in this (now rather long) email is
incorrect.

If you've made it this far, thank you for taking the time out of your day
to consider my thoughts. It's my hope that we can further analyze this
sub-proposal in detail and discuss its construction as well as its
implications on smart-contracts like payment channels on top of Bitcoin.

PASDA purports to address one half of the systematic risks in LN by
possibly eliminating the DoS vector attack against LN. However, the costs
of PASDA are very high, and possibly prohibitively so. In my opinion, the
second attack vector lies in the ability of miners to arbitrarily censor
transactions spending a particular output. Fungibility enhancing
techniques such as Committed Transactions may be a viable path forward to
patch this attack vector.

-- roasbeef


On Tue, Apr 4, 2017 at 8:35 PM Johnson Lau via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
The BIP151 proposal states:

the encinit messages.

This statement is incorrect. Sending content that existing nodes do not
expect is clearly an incompatibility. An implementation that ignores
invalid content leaves itself wide open to DOS attacks. The version
handshake must be complete before the protocol level can be determined.
While it may be desirable for this change to precede the version
handshake it cannot be described as backward compatible.

e

-------------------------------------
Using daily average block size over the past year (source:
https://blockchain.info/charts/avg-block-size?daysAverageString=14&timespan=1year
), here's how Block75 would have altered max block sizes:

[image: Inline image 1]

As of today, the max block size would be 1,135KB.

Looking forward and using the last year's growth rate as a model:

[image: Inline image 2]

This shows the max block size one year from now would be 2,064KB, if
Block75 activated today.

Of course, this is just an estimate, but even accounting for a substantial
increase in transactions in the last quarter of 2017 and changes brought
about by SegWit (hopefully) activating, Block75 alters the max size in such
a way that allows for growth, keeps blocks as small as possible, *and*
maintains transaction fees at a level similar to May/June 2016.

If anyone has an alternate way to model future behavior, please run it
through the Block75 algorithm.

Every 2016 blocks, do this:

new max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY))

TARGET_CAPACITY = 0.75    //Block75's target of keeping blocks 75% full
AVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a
decimal
x = current max block size


Thanks,

- t.k.
-------------------------------------
Hi,

First, thanks for resurrecting this, I agree this is worth pursuing.

On Mon, Dec 11, 2017 at 4:04 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I think it would be nice, though, to not require the consensus-correct
calculation of nBits in order to process p2p messages.  For instance, I
think there's a use for nBits at the p2p layer for calculating the work on
a chain, which can be used as an anti-DoS measure, even without verifying
that the difficulty adjustments are following the consensus rules.

Moreover I think it's a bit messy if the p2p layer depends on intricate
consensus rules in order to reconstruct a message -- either we'd need to
interact with existing consensus logic in a potentially new way, or we'd
reimplement the same logic in the p2p layer, neither of which is very
desirable imo.

But I think we should be able to get nearly all the benefit just by
including nBits in any messages where the value is ambiguous; ie we include
it with the first header in a message, and whenever it changes from the
previous header's nBits.

I would rather not change the serialization of existing messages,

I agree with this.  Specifically the way I envisioned this working is that
we could introduce a new 'cmpctheaders'/'getcmpcthdrs' message pair for
syncing using this new message type, while leaving the existing
'headers'/'getheaders' messages unchanged.  So when communicating with
upgraded peers, we'd never use 'getheaders' messages, and we'd only use
'headers' messages for potentially announcing new blocks.

Of course, we'll have to support the existing protocol for a long time.
But one downside I've discovered from deploying BIP 130, which introduced
block announcements via headers messages, is that overloading a 'headers'
message to be either a block announcement or a response to a 'getheaders'
message results in p2p-handling logic which is more complicated than it
needs to be.  So splitting off the headers chain-sync functionality to a
new message pair seems like a nice side-effect benefit, in the long run.
-------------------------------------
Hi Sergio,


Thanks for your response, interesting work, very excited for RSK.


I like the ephemeral payload, I suppose that aspect of my proposal could be described as ephemeral blockspace.


I'm curious about the challenge phase, what incentive do nodes to have to check other nodes' responses? Is any validation of responses mandatory, or does policing the system rely on altruism?


I also wondered how time-based responses are enforced? What prevents a miner censoring challenge responses so they do not get included in a block 'in time' - if  inclusion within a block is the mechanism used?


I saw your tweet on Lumino - sounds very promising. Would be keen to take a look at the paper if you're looking for any additional review at this stage.


Regards,


John Hardy


________________________________
From: Sergio Demian Lerner <sergio.d.lerner@gmail.com>
Sent: Sunday, February 12, 2017 8:22 PM
To: John Hardy; Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] Proof of Nodework (PoNW) - a method to trustlessly reward nodes for storing and verifying the blockchain

Hi John,
 RSK platform (a Bitcoin sidechain) is already prepared to do something similar to this, although very efficiently. We set apart 1% of the block reward to automatically reward full nodes.

We have two systems being evaluated: the first is based on PoUBS (Proof of Unique Blockchain Storage) which uses asymmetric-time operations to encode the blockchain based on each user public key such that decoding is fast, but encoding is slow. The second is more traditional proof of retrievability, but it requires some ASIC-resistance assumptions.

In both cases, a special smart contract is being called at every block that creates periodic challenges. Every full node that wants to participate can submits a commitment to the Merkle hash root of a pseudo-random sequence of encoded blocks. Then the smart contract chooses random elements from the committed dataset, and each full node has a period to submit Merkle-proofs that such random elements belong to the commitment.

To prevent blockchain bloat we designed a very cool new type of transaction payload: Ephemeral Payload. Ephemeral payload is a payload in a transaction that gets discarded after N blocks if no smart contract does reference it. If is does, it's solidified forever in the blockchain.
Then there is a challenge phase where other full nodes can inform the smart contract if they find an error in the submitted responses. Then the smart contract ONLY evaluates the responses which have been questioned by users.

This way the smart contract does very little computation (only when a user misbehaves) and the blockchain normally does not store any proof forever (only the ones created by misbehaving users).

Because RSK/Rootstock has a very short block interval (10 seconds), all this happens very quickly and does not require much computation.

Best regards,
 Sergio Lerner
 Chief Scientist RSK (aka Roostock)


On Tue, Feb 7, 2017 at 8:27 AM, John Hardy via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:

Proof of Nodework (PoNW) is a way to reward individual nodes for keeping a full copy of and verifying the blockchain.


Hopefully they also do useful traditional node activities too like relay transactions and blocks, but there isnt really any way I can think of to trustlessly verify this also.


PoNW would require a new separate area of block space, a nodeblock, purely concerned with administering the system. A nodeblock is committed to a block as with SegWit. A recent history of nodeblocks needs to be stored by nodes, however the data eventually becomes obsolete and so does not need to be retained forever.


In order to prevent Sybil, a node must register an Bitcoin address by submitting an addNode transaction - along with a security deposit to prevent cheating.


This transaction will be stored in the nodeblock. Once a node can see that its addNode transaction has been added it can begin the PoNW process. The nodes registered address will be hashed with the block header of the block it wants to work on. This will determine exactly where within the blockchain to begin the PoNW.


The PoNW method could be as simple as creating a Merkle tree from the randomly generated point on the blockchain, though a method that is CPU/Memory heavy and less likely to be replaced by dedicated hardware like ASICs would be better. This process could not begin until the most recent block has been fully verified, and while being carried out should still enable normal relay activities to proceed as normal, since it shouldnt tie up network at all. The data processed should also be mixed with data from the latest block so that it cannot be computed in advance.


A node can do as much PoNW for a block as it likes. Once finished it will then create a nodeWorkComplete transaction for that block with its final proof value, add how much work it did - and create a couple of assertions about what it processed (such as there were x number of pieces of data matching a particular value during calculating). These assertions can be accurate or inaccurate.


The system will run in epochs. During each epoch of say 2016 blocks, there will be an extended window for PoNW transactions to be added to nodeblocks to limit minor censorship.


The random hash generated from a nodes address and blockhash will also be used to determine nodeWorkComplete transactions from a previous block that the node must also verify, and correctly calculate whether the assertions it made were true or false. The average PoNW that a node performed in its previous x nodeblocks will be used to determine the target PoNW for the node to verify - and this will randomly be a large number of smaller PoNW transactions, or a smaller number of large PoNW. This process will be deterministic based on that block and address hash. All the data will be put together in a transaction and then signed by the node addresses private key.


If a nodeWorkComplete transaction contains any incorrect information in an attempt to cheat the validation process a challenge transaction can be created. This begins a refereeing process where other nodes check the challenge and vote whether it is to be upheld or not. The losing node is punished by losing their accrued PoNW for that epoch and a percentage of their security deposit.


Nodes will also be punished if they broadcast more than one signed transaction per block.


In order to prevent nodes from having multiple keys registered - which would enable them choose to perform PoNW on a subset of the data that they hold - the share of reward that the node gets will be multiplied based on the number of blocks within an epoch that the node performs PoNW on. The share of reward is limited based on how much security deposit has been staked. The higher the PoNW the higher the deposit needed in order to claim their full allocation of any reward.


At the end of an epoch, with a wait period for any delayed or censored transactions or challenges to be included and settled up, the process of calculating the reward each node is due can begin. This will then be then paid in a regular block, and means for all the data involved in PoNW, the only permanent mark it makes on the main blockchain is for a transaction that pays all addresses their share of the reward at the end of epoch. Any miner who creates a block without correctly calculating and paying the due reward will have mined an invalid block and be orphaned.


The question of where and how much the reward comes from is a different one. It could come from the existing miner reward, or a special new tx donation fee for nodes. If there was some way for users to donate to the reward pool for nodes this would increase the incentive for additional nodes to participate on the network in the event of centralisation.


This is a relatively effective way to create a reward for all nodes participating on a network. Id be keen to field any questions or critiques.

Thanks,


John Hardy

john@seebitcoin.com<mailto:john@seebitcoin.com>

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
I don't really see how this would increase the likelihood of an
extended chain split assuming BIP148 is going to have
non-insignificant economic backing. This BIP is designed to provide a
risk mitigation measure that miners can safely deploy. Since this BIP
only activates with a clear miner majority it should not increase the
risk of an extended chain split. At this point it is not completely
clear how much economic support there is for BIP148 but support
certainly seems to be growing and we have nearly 2 months until BIP148
activation. I intentionally used a shorter activation period here so
that decisions by miners can be made close to the BIP148 activation
date.

On Wed, Jun 7, 2017 at 4:29 PM, Jared Lee Richardson <jaredr26@gmail.com> wrote:

-------------------------------------
Add a preference for mined blocks to be the one with more transactions. This comes into play when 2 blocks of the same height are found. The first good block mined would be orphaned if it had less transactions than another. Optionally, have this rule apply to the current block and the previous one.

This increases incentive for full blocks because a miner thinking the faster propagation of a smaller block will win him the reward, but that would no longer be a good assumption.

I read some miners could attack a chain by mining small or empty blocks. This makes that a little more difficult, but they can still attack the chain many ways.

Sent with [ProtonMail](https://protonmail.com) Secure Email.
-------------------------------------
On 7/11/2017 5:31 PM, Gregory Maxwell wrote:
That is false. I could provide a list of names but I'm really not sure
what would be gained as result. You yourself admit that it is an
excellent list of research, almost all which you support directly.

So I think your only real objection is that I didn't talk to you
specifically.

That is why I included them despite being personally against them.
It links to bitcoinhardforkresearch.github.io , which I assumed would
contain the hard fork wishlist somewhere, but perhaps it does not.


Yes, of course. But is your position that if something is complicated we
should not try to clarify it?



-------------------------------------
(This is a sketch, not a fully-formed proposal, just to kick off the discussion.)

Confidential Transactions (by GMaxwell & Poelstra) require a new accounting model, 
new representation of numbers (EC points as Pedersen commitments) and range proofs 
per number. Setting aside performance and bandwidth concerns (3-4Kb per output, 
50x more signature checks), how would we deploy that feature on Bitcoin network 
in the most compatible manner?

I'll try to present a sketch of the proposal. I apologize if this discussion already
happened somewhere, although I couldn't find anything on this subject, apart from Elements 
sidechain proposal, of course.

At first glance we could create a new extblock and transaction format, add a protocol to
"convert" money into and from such extblock, and commit to that extblock from the 
outer block's coinbase transaction. Unfortunately, this opens gates to a flood of
debates such as what should be the block size limit in such block, should we 
take opportunity to fix over 9000 of pet-peeve issues with existing transactions
and blocks, should we adjust inflation schedule, insert additional PoW, what would
Satoshi say etc. Federated sidechain suffers from the same issues, plus adds 
concerns regarding governance, although it would be more decoupled, which is useful.

I tried to look at a possibility to make the change as compatible as possible,
sticking confidential values right into the existing transaction structure and
see how that would look like. As a nice bonus, confidential transactions would have 
to fit into the hard-coded 1 Mb limit, preserving the drama around it :-P

We start with a segwit-enabled script versioning and introduce 2 new script versions:
version A has an actual program concatenated with the commitment, while version B 
has only the commitment and allows mimblewimble usage (no signatures, non-interactive 
cut-through etc). Legacy cleartext amount can nicely act as "min value" to minimize
the range proof size, and range proofs themselves are provided separately in the
segregated witness payload.

Then, we soft fork additional rules:

1. In non-coinbase tx, sum of commitments on inputs must balance with sum of commitments
   on the outputs plus the cleartext mining fee in the witness.
2. Range proof can be confidential, based on borromean ring signature.
3. Range proof can be non-confidential, consisting of an amount and raw blinding factor.
4. Tx witness can have an excess value (cf. MW) and cleartext amount for a miner's fee.
5. In coinbase tx, total plaintext reward + commitments must balance with subsidy, 
   legacy fees and new fees in the witness.
6. Extra fees in the witness must be signed with the excess value's key.

The confidential transactions use the same UTXO set, can be co-authored with plaintext inputs/outputs
using legacy software and maybe even improve scalability by compressing on-chain transactions
using mimblewimble cut-through.

The rules above could have been made more complicated with export/import logic to allow users
converting their coins to and from confidential ones, but that would require
more complex support from miners to respect and merge outputs representing "plaintext value bank",
mutate export transactions, which in turn requires introduction of a non-malleable TxID
that excludes miner-adjustable export/import outputs.

The rules above have a nice side effect that miners, being the minters of confidential coins, 
can sell them at a premium, which creates an incentive for them to actually support
that feature and work on improving performance of rangeproof validation (e.g. in GPUs).

Would love to hear comments and criticism of that approach.

Thanks!
Oleg.




-------------------------------------
Hi Luke,

thanks for your feedback.

I can try to coordinate with the OpenAssets group, but the base 
requirements are completely different. For example, they are very far 
from any form of plausible deniability.

Please tell me which problems are not solved technically, so that I can 
try to fix the differences between the claims and the technical part, if 
any.

Luca

On 09/06/2017 08:58 PM, Luke Dashjr wrote:


-------------------------------------
On Thu, Apr 6, 2017 at 12:39 AM, Joseph Poon <joseph@lightning.network> wrote:

I apologize for the glib talk on chat and I hope you understand that
the tone in such venues is significantly informal; and that my remark
was a causal one among friends which was not intended in a spirit as
seriously as you've taken it.

That said, two days ago you participated in a highly unusual
announcement of a protocol change that-- rather than being sent for
community review in any plausible venue for that purpose-- was
announced as a done deal in embargoed media announcements.  This
proposed protocol change seemed custom tailored to preserve covert
boosting, and incorporated direct support for lightning -- and the
leading competing theory was that a large miner opposed segwit
specifically because they wanted to block lightning. Moreover, I have
heard reports I consider reliable that this work was funded by the
miner in question.

In the time since, when people asked for revisions to the proposal to
not block segwit they received responses from the Bcoin account on
twitter that "there would be no amendments", and I was sent leaked
chatlogs of you making considerably hostile statements, claiming that
if your extension block proposal is "a litmus test for corruption",
and claimed (before AFAIK anyone had had a chance to comment on it)
that the Bitcoin project contributors opposed it for "nonsense
reasons".

It is with this in mind that when you tried to pull me into an off the
record conversation that I responded stating:

"[...] I am disinclined to communicate with you except in email where I can
get third party transferable proof of our communication.  I'm
concerned that you may now be involved in a conspiracy which I do not
want to be implicated in myself.

It is my estimation that, for that above reason, it would be in my
best interest to not communicate with you at all.  But in all your
prior interactions you appeared to have integrity and sense, so out of
respect for that history I'm willing to communicate with you, but only
in public or in email where my end is on gmail."

This was two days ago and you did not respond further.

With that in mind I hope you do not find some casual crap-talking on
chat to be especially surprising.

I understand that you didn't intend for the initial message to be
posted in public, so I'm sorry for continuing the thread here-- but I
thought it was useful for people to understand the context behind that
glib remark: Including the point that I do not know for a fact that
you are complicit in anything, but I consider your recent actions to
be highly concerning.

-------------------------------------


On 05.09.2017 19:03, Luke Dashjr wrote:


That does not seem desirable to everybody.

If you want to guarantee that users will be able to recover all their
funds from their mnemonic seed (and that is what they expect), then
wallets must implement all script formats, even the ones that are
deprecated. In addition, the list of script formats that must be
supported is not defined in advance, but it keeps growing. This makes
wallet implementation increasingly difficult. In the long run, seed
portability is guaranteed to fail in such a system.


That's not a reason. The fact that xpub/xprv can be used for both P2PKH
and P2SH has already resulted in users receiving coins on addresses they
do not control.

-------------------------------------
I believe the filter can be more compact than this, but even if not an
order of magnitude saving of disk space is still significant.


On 2016-05-11 13:29, Bob McElrath wrote:

-------------------------------------
Andrew,

Block 475776 and block 477792 (July 26) are the last 2 difficulty
adjustment periods before Aug 1st.

Charlie

On Thu, Jul 13, 2017 at 3:48 PM, Andrew Chow via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I'm not sure if this has been brought up elsewhere in this thread.

This proposal doesn't seem to be a complete replacement of BIP37: It
doesn't provide a filter for unconfirmed transactions like BIP37 does.

That means that most light clients will continue to use BIP37 even if
they may use this BIP as a supplement. Otherwise users would not get
timely notification of incoming payments any more.


On 06/01/2017 09:01 PM, Olaoluwa Osuntokun via bitcoin-dev wrote:


-------------------------------------
On Sun, Oct 1, 2017 at 4:39 PM, Mark Friedenbach <mark@friedenbach.org>
wrote:


Script validation isn't the correct place to do this.  The reason is that
script operations are not aware of whether the stack items they are
processing are witness malleable items or Script computed values.  Let me
take OP_IF as one example.  When OP_IF operates directly on witness data,
it is subject to witness malleability, and therefore one needs to add extra
code around that to prevent witness malleability.  On the other hand, when
OP_IF operates on computed data, it isn't subject to malleability and can
safely process non-zero-or-one values. If OP_IF were restricted to
requiring canonical inputs, then for the cases that OP_IF operates on
computed data, they will need to add extra code to canonicalize their
inputs.  I don't think there is a correct answer here.  That is because I
believe this isn't the correct place to aim to restrict witness
malleability.

OTOH, signatures are a fine place to aim to restrict witness malleability.
In fact, if signatures could securely cover all witness data, I think
everyone here would jump at the opportunity to implement that.  However,
since that isn't known to be possible, we are left with doing the best we
can, which is to have signatures cover weight (or bytes).  This prevents
the worst effects of witness malleability and does so without burdening
Script development.  (This also requires signatures have a fixed size, so
it is understandable that signature-covers-weight wasn't included in Segwit
v0 scripts).




I would be fine your suggestion above, though I think Luke's suggestion of
having both SIGHASH_WITNESS_SIZE and SIGHASH_WITNESS_DEPTH flag is better
because it is simpler.

Those people worried about restarting interactive signing session in the
unlikely event of parties not knowing what keys they are planning to use
can use just the SIGHASH_WITNESS_DEPTH flag.  Those people worried about
counterparties fiddling with fee rates can use both flags.  The choice
doesn't even need to be made at script commitment time.
-------------------------------------
I feel like this would be pointless as the vast majority of users would
likely download the blockchain from a node that was not enforcing a tip
requirement as it would seem like unnecessary cost as in protocol​s such as
BitTorrent there is no such tips in sharing files and the blockchain
distribution is in eccense the same thing. However perhaps I am
underestimating the generosity of node operators but I feel that adding a
cost to the blockchain (assuming that all users add a tip requirement)
would lead to centralisation.

On Wed, 3 May 2017, 22:21 Erik Aronesty via bitcoin-dev, <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
---------- Forwarded message ----------
From: Andrew Poelstra <apoelstra@wpsoftware.net>
Date: Mon, Mar 20, 2017 at 5:11 PM
Subject: [Mimblewimble] Lightning in Scriptless Scripts
To: mimblewimble@lists.launchpad.net





In my last post about scriptless scripts [2] I described a way to do
deniable atomic swaps by pre-sharing a difference of signatures. This
had the limitation that it required at least one party to be shared
between the signatures, allowed only pairwise linking, and required
both signatures to cover data that is known at the time of setup.
Linking a multi-hop Lightning channel with these constraints has proved
difficult.

* * *

Recently I've found a different construction that behaves much more like
a hash preimage challenge, and this can actually be used for Lightning.
Further, it supports reblinding, so you can learn a preimage but hide
which one you're looking for. (Ethan, one might actually overlap with
TumbleBit, sorry :)).

It works like this. We'll treat x -> xG as a hash function, so x is the
preimage of xG. There are two separate but related things I can do: (a)
construct a signature which reveals the preimage; or (b) create a
"pre-signature" which can be turned into a signature with the help of
the preimage.

Here's how it works: suppose I send xG to Rusty and he wants to send
me coins conditional on my sending him x. Lets say I have key P1 and
nonce R1; he has key P2 and nonce R2. Together we're going to make a
multisignature with key P1 + P2 and Rusty is going to set things up
so that I can't complete the signature without telling him x.

Here we go.

  0. We agree somehow on R1, R2, P1, P2.

  1. We can both compute a challenge e = H(P1 + P2 || R1 + R2 || tx).

  2. I send s' = k1 - x - x1e, where R1 = k1G and P1 = x1G. Note he
     can verify I did so with the equation s'G = R1 - xG - eP1.

  3. He now sends me s2 = k2 - x2e, which is his half of the multisig.

  4. I complete the sig by adding s1 = k1 - x1e. The final sig is
     (s1 + s2, R1 + R2).

Now as soon as this signature gets out, I can compute x = s1 - s'.

* * *

Ok, pretty nifty. But now suppose Rusty wants to receive coins conditioned
on him revealing x, say, because he's a middle hop in a Lightning channel.
You might think he could act the same as I did in step (2), computing
s' = k1 - x - x1e, but actually he can't, because he doesn't know x himself!
All good. Instead he does the following.

To put names on things, let's say he's taking coins from Tadge. The
protocol is almost the same as above.

  0. They agree somehow on R1, R2, P1, P2. Tadge's key and nonce are
     R1 and P1, but there's a catch: P1 = x1G as before, but now
     R1 - xG = k1G. That is, his nonce is offset by k1G.

  1. They can both compute a challenge e = H(P1 + P2 || R1 + R2 || tx).

  2. Tadge sends the "presignature" s' = k1 - x1e. Rusty can verify this
     with the equation s'G = R1 - xG - eP1.

  3. Now whenever Rusty obtains x, he can compute s1 = s' - x, which is
     Tadge's half of the final signature.

  4. Rusty computes s2 himself and completes the signature.

* * *

Ok, even cooler. But the real Rusty complained about these stories, saying
that it's a privacy leak for him to use the same xG with me as he used with
Tadge. In a onion-routed Lightning channel, this xG-reuse would let all
any two participants in a path figure out that they were in one path, if
they were colluding, even if they weren't directly connected.

No worries, we can fix this very simply. Rusty chooses a reblinding factor
rG. I give him x, as before, but what Tadge demands from him is (x + r).
(I give xG to Rusty as a challenge; he forwards this as xG + rG to Tadge.)
Since Rusty knows r he's able to do the translation. The two challenges
appear uniformly independently random to any observers.

* * *

Let's put this together into my understanding of how Lightning is supposed
to work. Suppose Andrew is trying to send coins to Drew, through Bob and
Carol. He constructs a path

  A --> B --> C --> D

where each arrow is a Lightning channel. Only Andrew knows the complete
path, and is onion-encrypting his connections to each participant (who
know the next and previous participants, but that's it).

He obtains a challenge T = xG from D, and reblinding factors U and V
from B and C. Using the above tricks,

  1. A sends coins to B contingent on him learning the discrete logarithm
     of T + U + V.

  2. B sends coins to C contingent on him learning the discrete logarithm
     of T + V. (He knows the discrete log of U, so this is sufficient for
     him to meet Andrew's challenge.)

  3. C sends to D contingent on him learning the discrete log of T, which
     is D's original challenge. Again, because C knows the discrete log
     of V, this is sufficient for her to meet B's challenge.

The resulting path consists of transactions which are signed with single
uniformly random independent Schnorr signatures. Even though they're all
part of an atomic Lightning path.

* * *

Note that the s' values need to be re-communicated every time the
transaction
changes (as does the nonce). Because it depends on the other party's nonce,
this might require an additional round of interaction per channel update.

Note also that nothing I've said depends at all on what's being signed. This
means this works just as well for MimbleWimble as it would for
Bitcoin+Schnorr
as it would for Monero (with a multisig ring-CT construction) as it would
for Ethereum+Schnorr. Further, it can link transactions across chains.


I'm very excited about this.

Cheers
Andrew


[1] https://lists.launchpad.net/mimblewimble/msg00036.html




--
Andrew Poelstra
Mathematics Department, Blockstream
Email: apoelstra at wpsoftware.net
Web:   https://www.wpsoftware.net/andrew

"A goose alone, I suppose, can know the loneliness of geese
 who can never find their peace,
 whether north or south or west or east"
       --Joanna Newsom


--
Mailing list: https://launchpad.net/~mimblewimble
Post to     : mimblewimble@lists.launchpad.net
Unsubscribe : https://launchpad.net/~mimblewimble
More help   : https://help.launchpad.net/ListHelp




-- 
- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
Hi Chris,

On 7/11/2017 12:03 PM, Chris Stewart wrote:
It depends on interest, I think. What remains to be done is more easily
parallelized, and in some cases (eg smartphone wallets) there are
incentives for private individuals and businesses to hustle their part
out (merely for reasons of competitiveness).


I wrote the roadmap to try to be representative of a Core / developer
position. I am philosophically against hard forks, but HFs were in the
end of the previous roadmap so I felt it should stay. And, I felt that
if I decided to unilaterally remove it, it would be perceived as
excessive campaigning for Drivechain. And I also felt that to remove it,
when so many industry members recently put their weight behind a fast
hard fork, would be perceived as a reaction to that attempt, and would
then overwhelm the document with political polarization, for really no
benefit.

Paul


-------------------------------------


I think the following features are necessary for a hardfork. The rest are optional:

1. A secondary header
2. Anti-replay
3. SigHash limit for old scripts
4. New tx weight accounting

Optional:
1. New coinbase format is nice but not strictly needed. But this can’t be reintroduced later with softfork due to the 100 block maturity requirement
2. Smooth halving: could be a less elegant softfork
3. Mekle sum tree: definitely could be a softfork


Without nBits in the header, the checking of PoW become contextual and I think that may involve too much change. The saving of these 4 bytes, if it is really desired, might be done on a p2p level 



Regarding the header format, a big question we never came into consensus is the format of the hardfork. Although I designed forcenet to be a soft-hardfork, I am now more inclined to suggest a simple hardfork, given that the warning system is properly fixed (at the minimum: https://github.com/bitcoin/bitcoin/pull/9443 <https://github.com/bitcoin/bitcoin/pull/9443>)

Assuming a simple hardfork is made, the next question is whether we want to keep existing light wallets functioning without upgrade, cheating them by hiding the hash of the new header somewhere in the transaction merkle tree.

We also need to think about the Stratum protocol. Ideally we should not require firmware upgrade.

For the primary 80 bytes header, I think it will always be a fixed size. But for the secondary header, I’m not quite sure. Actually, one may argue that we already have a secondary header (i.e. coinbase tx), and it is not fixed size.


The max() is at transaction level, not block level. Unless your wallet is full of different types of UTXOs, coin selection would not be more difficult than current.

Among the 4 limits, the SigHash limit is mostly a safety limit that will never be hit by a tx smaller than 100kB. As part of the replay attack fix, a linear SigHash may be optionally used. So wallets may just ignore this limit in coin selection

Similarly, the SigOp limit is also unlikely to be hit, unless you are using a very big multi-sig. Again, this is very uncommon and wallets primarily dealing with signal sig may safely ignore this

Finally, an important principle here is to encourage spending of UTXO, and limiting creation of UTXO. This might be a bit difficult to fully optimise for this, but I think this is necessary evil.

More discussion at: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013504.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013504.html>


Yes, but maybe we just don’t need this at all. This could also be done with a softfork using OP_CSV, just a bit ugly.


This allows people to sign an input, to be part of a coinbase tx, but limited to a particular previous block hash. This is currently not possible, but through a later softfork we could introduce a new SigHash function that allows something between SIGHASH_ALL and SIGHASH_ANYONECANPAY, so people may sign its own input and another input, while ignoring the rests of input. (in other words: change the name SIGHASH_ANYONECANPAY to SIGHASH_SINGLE_INPUT, and we introduce SIGHASH_DUAL_INPUT. But we don’t need to do this in this hardfork)



This is just a demo, and I agree this could be added through a softfork later. But even if we add this as a softfork, we have to have the ability to disable it through a special softfork. I think I have explained the reason but let me try again.

Here, when I talking about “tx weight”, it’s the “tx weight” defined in point 4, which covers not only size, but also other limits like SigOp. For a fraud proof to be really useful, it has to cover every type of block level limits. One feature of segwit is the script versioning, which allows introduction of new scripts. In the process, we will change the definition of SigOp: previous 0 SigOp scripts now carries some amount of SigOp. This is by itself a softfork (we did this type of softfork twice already: P2SH and segwit). However, if we have a merkle sum root covering the SigOp, old nodes won’t count these new SigOps, and they will fail to validate the sum root.

Without a backdoor to disable the sum tree validation in old nodes, the only way would be keeping the original sum tree untouched, while create another sum tree, every time we have a new script version. This is not acceptable at all.

But even such backdoor would not be harmful to the security of full nodes because they will still fully verify the tx and witness merkle root.

I’d argue that any fraud proof related commitment: sum tree, delayed UTXO commitment etc will require such a backdoor to disable. Maybe we should just remove this from here and make this a new topic. We could even do this as a softfork today.


-------------------------------------
I came across O(N) behavior of two scripting opcodes, OP_IF and OP_ROLL. By
exploiting edge cases for each of these two sub-optimal algorithms, I
manage to simulate a Segwit block that takes up to 5.6 seconds to verify on
a Ubuntu VM running on a single Core i5 processor. The simulation is based
on a single thread executing EvalScript(), the Bitcoin script execution
function. The tests were not performed processing actual blocks. These
results should not make anyone worry, because there are worse problems in
Bitcoin block verification, and because Bitcoin employs several worker
threads for verifying scripts in parallel. For example, a Segwit block can
request 80000 signature verifications when all transactions are P2WSH. It
is said that Bitcoin Core (in a modern multi-core machine, using its
multi-threading verification capabilities) can verify 8000 ECDSA signatures
per second. Therefore a malicious miner can create a Segwit block that
requires approximately 10 seconds to be verified. Since the examples
presented in this post consume less than 10 seconds, I don’t consider my
findings as vulnerabilities. However, if the block size is to be increased
in the future, these problems should be considered prior increasing the
block size. The scripts presented here as examples do not leave the value
stack empty, but the Bitcoin protocol does not require it. Bitcoin only
requires the top value to be true to accept the script.

OP_IF abuse

Every time a Bitcoin script executes the OP_IF opcode, a boolean value
indicating if the condition was true, false or the conditional was skipped
(also represented as false) is pushed into the vfExec stack.  Every time an
opcode is executed, the number of  false values in the vfExec stack is
counted using the following line:

bool fExec = !count(vfExec.begin(), vfExec.end(), false);

If the count is non-zero, all subsequent instructions except OP_ELSE and
OP_ENDIF are skipped. It is clear that the longer the conditional stack is,
the more it takes to count the false elements.

The following scriptPub or ScriptSig exploits this problem:

0
OP_IF { 100 times }

0 { 9798 times }

OP_ENDIF { 100 times }
1

The vfExec vector is filled with 100 elements, and then each element is
scanned 9799 times, totaling more than 979K items scanned. This took 2.5
seconds in my test VM (for a block filled with these scriptSigs).

To re-write this logic with a O(1) algorithm, one simply has to count the
number of true conditions in one variable (trueCount), and the number of
false or skipped conditions following all true conditions in another
(ignoreCount). Detecting if code needs to be executed or not requires just
testing if ignoreCount is zero.

The handling of OP_IF / OP_NOTIF / OP_ELSE should be like the following
pseudo-code:

fExec = (ignoreCount==0);
...
case OP_IF:
case OP_NOTIF:
 {
   if (fExec)
    {
     ....compute fValue...
     if (fValue) trueCount++; else ignoreCount++;
    } else
    ignoreCount++;
 } break;
 case OP_ELSE:
 {
 if ((trueCount==0) && (ignoreCount==0))
     return set_error(serror, SCRIPT_ERR_UNBALANCED_CONDITIONAL);
 if (ignoreCount==0) { trueCount--; ignoreCount++; } else
 if (ignoreCount==1) { trueCount++; ignoreCount--; }
 } break;
case OP_ENDIF:
 {
    if ((trueCount==0) && (ignoreCount==0))
        return set_error(serror, SCRIPT_ERR_UNBALANCED_CONDITIONAL);
    if (ignoreCount>0) ignoreCount--; else trueCount--;
 }
 break;

You may have noticed the strange behavior of Bitcoin’s ELSE statement.
Bitcoin allows one to switch between true and false conditions several
times. For example, the following script is valid and leaves the value 2 on
the stack:

1 OP_IF OP_ELSE OP_ELSE 2 OP_ENDIF

OP_ROLL

The second problem lies in the OP_ROLL opcode. This opcode removes a value
at a given index from the value stack, and pushes it on top. As the Bitcoin
Core stack stores a list of char std::vector by value (not by reference),
and because the stack is itself a std::vector (not a linked list), then
removing the first elements requires moving all elements one position in
memory. The value stack can store a maximum of 1000 elements. The following
script fills the stack and then moves each stack element 200 times, so the
number of moved elements is 200K. This took almost 5.6 seconds in my test
VM (for a block filled with these scriptSigs).

1  {999 times}
998 OP_ROLL { 200 times }

I tried other scripts, such as filling the stack with values of size 520
using DUP3, and then performing rolls, but all of them led to a block that
took less time, if the block is to be filled with the scripts.

One solution to this problem is use a linked list data structure instead of
a std::vector, to allow O(1) removal of items, but it still requires O(N)
for element lookup. A balanced tree where each internal node is augmented
with the number of children underneath can be used to provide efficient
indexed access and efficient element removal. However, the overhead of such
data structure may kill its benefits.

So it may be the case that optimizing OP_ROLL will never really be
required.

But these minor issues have to be taken into account if the scripting
system is modified in any way. There are many subtle interactions. For
instance, it may seem that Segwit allows a transaction having a stack
containing 2 million items to verify correctly, by having the witness stack
filled with 2M zero values, and by executing an empty witness script.
However this is prevented by the cleanstack check.
-------------------------------------
I recently posted about so called "user activated soft forks" and received a lot of feedback. Much of this was how such methodologies could be applied to segwit which appears to have fallen under the miner veto category I explained in my original proposal, where there is apparently a lot of support for the proposal from the economy, but a few mining pools are vetoing the activation.

It turns out Bitcoin already used flag day activation for P2SH[[1](https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283)], a soft fork which is remarkably similar to segwit. The disadvantage of a UASF for segwit is there is an existing deployment. A UASF would require another wide upgrade cycle (from what I can see, around 80% of existing nodes have upgraded from pre-witness, to NODE_WITNESS capability[[2](http://luke.dashjr.org/programs/bitcoin/files/charts/services.html)][[3](https://www.reddit.com/r/Bitcoin/comments/5yyqt1/evidence_of_widespread_segwit_support_near50_of/)]. While absolute node count is meaningless, the uprgrade trend from version to version seems significant.

Also it is quite clear a substantial portion of the ecosystem industry has put in time and resources into segwit adoption, in the form of upgrading wallet code, updating libraries and various other integration work that requires significant time and money. Further more, others have built systems that rely on segwit, having put significant engineering resources into developing systems that require segwit - such as several lightning network system. This is much more significant social proof than running a node.

The delayed activation of segwit is also holding back a raft protocol of innovations such as MAST, Covenants, Schnorr signature schemes and signature aggregation and other script innovations of which, much of the development work is already done.

A better option would be to release code that causes the existing segwit deployment to activate without requiring a completely new deployment nor subject to hash power veto. This could be achieved if the economic majority agree to run code that rejects non-signalling segwit blocks. Then from the perspective of all existing witness nodes, miners trigger the BIP9 activation. Such a rule could come into effect 4-6 weeks before the BIP9 timeout. If a large part of the economic majority publicly say that they will adopt this new client, miners will have to signal bip9 segwit activation in order for their blocks to be valid.

I have drafted a BIP proposal so the community may discuss https://gist.github.com/shaolinfry/743157b0b1ee14e1ddc95031f1057e4c (full text below).

References:
[1]: https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283
[2]: http://luke.dashjr.org/programs/bitcoin/files/charts/services.html
[3]: https://www.reddit.com/r/Bitcoin/comments/5yyqt1/evidence_of_widespread_segwit_support_near50_of/

Proposal text:

<pre> BIP: bip-segwit-flagday Title: Flag day activation for segwit deployment Author: Shaolin Fry <shaolinfry@protonmail.ch> Comments-Summary: No comments yet. Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-???? Status: Draft Type: Informational Created: 2017-03-12 License: BSD-3-Clause CC0-1.0 </pre> ==Abstract== This document specifies a BIP16 like soft fork flag day activation of the segregated witness BIP9 deployment known as "segwit". ==Definitions== "existing segwit deployment" refer to the BIP9 "segwit" deployment using bit 1, between November 15th 2016 and November 15th 2017 to activate BIP141, BIP143 and BIP147. ==Motivation== Cause the mandatory activation of the existing segwit deployment before the end of midnight November 15th 2017. ==Specification== All times are specified according to median past time. This BIP will be activate between midnight October 1st 2017 (epoch time 1538352000) and midnight November 15th 2017 (epoch time 1510704000) if the existing segwit deployment is not activated before epoch time 1538352000. This BIP will cease to be active when the existing segwit deployment activates. While this BIP is active, all blocks must set the nVersion header top 3 bits to 001 together with bit field (1<<1) (according to the existing segwit deployment). Blocks that do not signal as required will be rejected. === Reference implementation === <pre> // mandatory segwit activation between Oct 1st 2017 and Nov 15th 2017 inclusive if (pindex->GetMedianTimePast() >= 1538352000 && pindex->GetMedianTimePast() <= 1510704000 && !IsWitnessEnabled(pindex->pprev, chainparams.GetConsensus())) { if (!((pindex->nVersion & VERSIONBITS_TOP_MASK) == VERSIONBITS_TOP_BITS) && (pindex->nVersion & VersionBitsMask(params, Consensus::DEPLOYMENT_SEGWIT)) != 0) { return state.DoS(2, error("ConnectBlock(): relayed block must signal for segwit, please upgrade"), REJECT_INVALID, "bad-no-segwit"); } } </pre> ==Backwards Compatibility== This deployment is compatible with the existing "segwit" bit 1 deployment scheduled between midnight November 15th, 2016 and midnight November 15th, 2017. ==Rationale== Historically, the P2SH soft fork (BIP16) was activated using a predetermined flag day where nodes began enforcing the new rules. P2SH was successfully activated with relatively few issues By orphaning non-signalling blocks during the last month of the BIP9 bit 1 "segwit" deployment, this BIP can cause the existing "segwit" deployment to activate without needing to release a new deployment. ==References== [https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283 P2SH flag day activation]. ==Copyright== This document is placed in the public domain.
-------------------------------------
Let me describe the possible improvement of the bitcoin blockchain database
(BBD)  size in general terms.

We can implement new routine : annual split of the BBD. Reason is that
140gb full wallet unconvinience.

BBD splits in two parts :
1) old blocks before the date of split and
2) new blocks, starting from first technical block with all rolled totals
on the date of split.
    (also possible transfer of tiny totals due to their unprofitability to
the miners, so we cut long tail of tiny holders)
3) old blocks packs into annual megablocks and stores in the side archive
chain for some needs for FBI investigations or other goals.


Thanks for your attention,

Alexey Mutovkin
-------------------------------------
Hi Jeff

There where previous discussions about similar approaches [1] [2].

I’m not sure if compression should be built into the protocol.
My humble understanding of it, is, that it should be built into different layers.

If bandwidth is a concern, then on the fly gzip compression like apaches mod_deflate could be something. But I expect fast propagation is often more important then a ~30% bandwidth reduction.
Bandwidth may be a concern for historical blocks transmission. If you continue the proposal, I think you should focus on historical blocks.

If disk space is a concern, then the database layer should handle the compression.

Thanks
—
</jonas>


[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011692.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011692.html>
[2] https://github.com/bitcoin/bitcoin/pull/6973 <https://github.com/bitcoin/bitcoin/pull/6973>




-------------------------------------
On Tue, Jul 11, 2017 at 10:17 PM, Paul Sztorc <truthcoin@gmail.com> wrote:

That might be your impression, then you've misunderstood what I
intended-- What I wrote was carefully constructed as a personal view
of how things might work out. It never claimed to be a a project
roadmap. Though as usual, I work hard to propose things that I believe
will be successful... and people are free to adopt what they want.

And to the extent that it got taken that way I think it was an error
and that it has harmed progress in our community; and created more
confusion about control vs collaboration.

With perfect hindsight I wouldn't have posted it; especially since
we've learned that the demand for increased capacity from many people
was potentially less than completely earnest. (The whole, can't double
capacity until we quadruple it thing...)

and the concept of mining centralization on this list in the recent
past.

I don't agree that you have; but for the purpose of this thread
doesn't really matter if I (specifically) do or don't agree.  It's an
objective truth that many people do not yet believe these concerns
have been addressed.


That Adam asked me to write publish a new "roadmap" for Bitcoin as
you've done here, with particular features and descriptions, which I
declined; and explained why I didn't believe that was the right
approach.  And Adam worked with you on the document, and provided
content for it (which I then recognized in the post).

Set aside what you know to be true for a moment and consider how this
might look to an outsider who found out about it.  It could look a
like Blockstream was trying to influence the direction of Bitcoin by
laundering proposals through an apparently unaffiliated third party.
Doubly so considering who participated in your drafting and who didn't
(more below).

I don't think in actuality you did anything remotely improper
(ill-advised, perhaps, since your goal to speak for developers but you
didn't speak to them, IMO--) but I think transparency is essential to
avoid any appearance of misconduct.


I did--

As Bryan pointed out... with the exception of the intro and end and a
couple sentences in the middle my entire response to your post, with
the position on "roadmaps" was written a long time ago, specifically
to complain about and argue against that particular approach.


I think you may be mistaking a roadmap with "communications"-- people
taking about research they are exploring is great! and we should have
more of it.  But like with RedHat and kernel features, we can't really
roadmap things that are unresourced and currently just unimplemented
exploration or test code-- without seeing things which are more or
less done the community can't rightfully decide if they'd want to
support something or not.  Perhaps they'll be good things perhaps
awful-- the devil is in the details, and until an effort is fairly
mature, you cannot see the details.

There have, for example, been signature aggregation proposals in the
past that required address reuse (could only aggregate if they're
reused).  I would strongly oppose such a proposal, and I hope everyone
else here would too.  So if I were a third party looking at your
message, rather than the person who initially proposed the agg sig
thing you're talking about, I wouldn't know if I could love it or hate
it yet; and probably couldn't be expected to have much of an opinion
on it until there is a BIP and/or example implementation.

To the extent that a roadmap differs from communications in general,
it is in that it also implies things that none of us can promise
except for our own efforts; Completion of implementations, success of
experiments, adoption-- basically assuming a level of top down control
that doesn't exist in a wide public collaboration.

One of the great challenges in our industry is that we don't have
rings of communication: We don't have much in the way of semi-experts
to communicate to semi-semi-experts to communicate to media pundits to
communicate to the unwashed masses-- at each level closing the
inferential gap and explaining things in terms the audience
understands. We don't have things like LWN.   We'll get there, but it
it took the Linux world decades to build to what it has now. I think
various forces in the industry work against us-- e.g. we lose a mot of
mid tier technical people to the allure of striking it rich printing
money in their own altcoins.

It might be attractive to try to end-run the slow development
appropriate web of reliable communications by deploying high authority
edicts, but it ultimately can't work because it doesn't map to how
things are accomplished, not in true collaborative open source, and
certainly not in an effort whos value comes substantially from
decentralization. Doing so, I fear, creates a false understanding of
authority.

(It's a common misunderstanding, for example, that people do what I
want (for example); but in reality, I just try to avoid wasting my
time advocating things that I don't think other people are already
going to do; :) otherwise, if _I_ want something done I've got to do
it myself or horse trade for it, just like anyone else.)

One of the great things about general communications is anyone can do
it.  Of course, unless they're talking about their own work, they
can't promise any of it will be completely-- but that is always true.
 If someone wants some guarantee about work, they have to be willing
to get it done themselves (and, of course, if it's a consensus
feature-- that much is necessary, but still not sufficient to get a
guarantee).

[from your other reply]

Come now, this is needlessly insulting. I would have made the same
comment if you had talked to me because you didn't talk to most/all of
Matt Corallo, Wladimir, Pieter Wuille, Alex Morcos, etc.... e.g. the
people doing most of the work of actually building the system.  Before
making that comment I went and checked with people to find out if only
I was left out.  Talking to Adam (who isn't involved in the project)
and Luke-jr (who is but is well known for frustratingly extreme
minority positions and also contracts for Blockstream sometimes) isn't
a great approach for the stated goal of speaking for developers!

-------------------------------------
On Fri, Sep 29, 2017 at 10:43 AM, Daniele Pinna via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

If I'm not mistaken that is is nothing new or interesting: You can
delay some transaction by paying more than it offered by every block
you delay it from. E.g. if the next full block would pay 0.8 BTC in
fees, you just need to make transactions paying more than that. But
you'll pay it for each delay and the people you push out only pay once
(when they are successful), so it gets awfully expensive fast.

(Arguably the monopoly price model is better because outbidding party
doesn't need to bloat the chain to do their thing; arguable its
somewhat worse because its harder to do by accident.)

My thought on this was the same as PT's initial: miners and users can
arrange OOB payments (and/or coinjoin rebates) and bypass this. I
don't see why it wouldn't be in their individual best interest to do
so, and if they do that would likely be a centralizing effect.

-------------------------------------

Hi Luke,

On 6/28/2017 1:20 AM, Luke Dashjr via bitcoin-dev wrote:

This is one of the assumptions which BMM exploits, see #4 of:
http://www.truthcoin.info/blog/blind-merged-mining/#focus

The idea is that this is a safe assumption, because it is already the
case today. If we assume that miners revenue-maximize, and further that
the "bidder" frames his payments in tx-fees, then a willing buyer can
control the next block by simply filling it with high tx-fee spam.
Anyone who is willing to pay the most can already 'get' the next
mainchain block (only, indirectly).


Yes, it is unclear, but Chris' email is specific to blind merged mining
(BMM), which is kind of a "sidechains +". So this does not directly
enable any sidechains. Instead, it enables the "+" part.

--Paul

-------------------------------------
I believe this proposal still suffers from one problem that bip37 did,
albiet by a much lesser extent. Combining the partial information from
the block downloads with the transaction subgraph information from the
blockchain can in some cases still reveal which addresses belong to the
wallet. Nonetheless this proposal still has many benefits and is well
worth working on.

==BIP37==

As a recap, probably the biggest and most problematic way that bip37 was
broken was by combining the partial wallet information from the bloom
filter with the transaction subgraph information from the blockchain

Suppose a wallet synchronizes it's history, if it spent a coin from its
address A, it must also also add the change address B to the bloom
filter, which is connected to A directly on transaction graph.

As an example, consider five typical transactions that consume one input
each and produce two outputs.
A, B, C, D, E refer to transactions. A1, A2, etc refer to addresses
within those transactions

          -> C1
A1 -> B2  -> C2
   -> B2  -> D1
          -> D2 -> E1
                -> E2

If a bip37 bloom filter matches addresses A1, B2, D2, E1 then it can be
seen that they form a "peel chain" [this terminology comes from
https://cseweb.ucsd.edu/~smeiklejohn/files/imc13.pdf]

          -> X
A1 -> X   -> X
   -> B2  -> X
          -> D2 -> E1
                -> X

The same five transactions with non-matching addresses replaced by X.
The peel chain is visible, it's clear that B2, D2, E1 are change
addresses which belong to the same wallet as A1.

For a given false-positive rate fp and given length of peel chain C, the
odds of a false positive peel chain happening by chance is fp^C which
rapidly gets very small as the wallet makes more transactions (increases C).

If only one address was matched from the above group (for example B2)
then it likely to be a false positive by the fact that it doesn't make
any transactions to another address that also matches the bloom filter.
Another possibility is that the address is a payment output that the
wallet received but hasn't spent yet, but the wallet cant spend it
without adding the change address to the bloom filter and thus revealing
itself to the spy.

I believe the committed bloom filter proposal is vulnerability to this
same kind of attack because it still leaks information about which
addresses the wallet is interested in.

==Committed Bloom Filter Maths==

I'll try to analyze this now. I'll find the expectation value of the
number of transaction subgraphs in those blocks that appear just by
chance. If this expectation goes to zero, then the only transaction
subgraph left will be the real one that the wallet is actually
interested in. In that case it will be possible to spy on the wallet.

Assuming outputs have the same probability of being spent in each time
interval (i.e. they are spent in a Poisson process) This is
approximately true, see the graphs from
[https://letstalkbitcoin.com/blog/post/rise-of-the-zombie-bitcoins].
This means we can assign
a single probability P that an output is spent in each block.

Assume every transaction has one change address only and spending of
unconfirmed change doesn't happen (its more efficient to use RBF to add
additional outputs anyway)

Number of transactions per block = Q (about 1800 today)
Number of outputs per block = Z = 2*Q (approximately)
Length of peel chain = Number of transactions in wallet = C
Average time an output is unspent for = T (about 1 month, very roughly
estimating from the above blog post)
Probability an output being spent in any particular later block = P =
10minutes/T

Assume no false positive blocks
Say wallet downloaded two blocks and they are ordered by block height
The expected number of tx subgraphs between them, E(#G)
E(#G) = number of outputs created in block 1 that get spent in block 2
      = Z*P

Say the wallet downloaded three blocks
Expected number of subgraphs going through them all
E(#G) = number of outputs created in block 1 get spent in block 2, that
create a change address which gets spent in block 3
      = Z*P*P

Say the wallet downloaded C blocks
Expected number of tx subgraphs going through all the blocks by chance
E(#G) = Z*P^C
which gets small quickly as C goes up, because P < 1

Now drop the assumption about no false positive blocks.

Let the number of candidate blocks be D.
This is how many blocks the wallet scans, it's related to how far in the
past the wallet's keys was created. At one extreme wallet was created at
genesis block and so D = ~450000, at other extreme created now so D = 0.
Note that D = 0 must also imply C = 0

Expected number of false positive blocks downloaded = F = fp*D

In all these situations the blocks are sorted by block height

Suppose have C=2, F=1, and false one is in the middle.
I want to find E(#G|CF), the expected number of transaction subgraphs
that appear just by chance, given C and F.
E(#G|CF) = how many outputs which are created in block 1 get spent in
block 3
         = Z*P

Same situation, but false one at the start instead of middle.
E(#G|CF) = how many outputs which are created in block 2 get spent in
block 3
         = Z*P

Same situation but false one could be anywhere, result in the sum of the
probability for any false block position
E(#G|CF) = C(3, 1)*Z*P = 3*Z*P

where C() is the number of order-independent ways of choosing 1 element
out of a set of 3 elements, also known as the binomial coefficient

Now suppose C=3 and F=1
The same argument leads to
E(#G|CF) = C(4, 1)*Z*P^2 = 4*Z*P^2


Now suppose C=3 and F=2, with fp blocks at the end
E(#G|CF)
= how many outputs are created in block 1, are spent in block 2 and
change address spent in block 3
= Z*P^2

Same situation but fp blocks can be anywhere, add up all the possible
combinations of them within the rest
E(#G|CF) = C(5, 2)*Z*P^2 = 5*Z*P^2

With these same rules, its clear the general expression for any F and C
E(#G|CF) = C(F + C, F)*Z*P^(C - 1)

A more interesting value might be the time evolution of E(#G)
Let B be the blocks in the blockchain since the wallet creation date, as you
know it increases at an average rate of one every ten minutes

w = wallet transaction creation rate, expressed per-block
C = w * B
F = fp * B

J = average blocks between wallet transactions = 1440 (10 days)
w = 1/J

E(#G|B) = C((fp + w)*B, fp*B)*Z*P^(w*B - 1)

This goes to zero as B becomes big, although choosing very high values
of fp makes it go to zero slower.

This is only approximate maths, in actuality you cannot take the number
of false positive blocks to be fp*B, you have to sum over all blocks
weighted by probability. And outputs might not be spent in an exact
Poisson process so you cant just multiply by P each time. Plus if your
false positive rate is very high then some of your false positive blocks
will actually contain your real transactions, this analysis
double-counts them.

Using some reasonable values and plotting E(#G|B) against B can show how
quickly it drops and therefore leaves only the true transaction subgraph.

(note: in LibreOffice Math and Microsoft Excel the binomial coefficient
function is COMBIN)

==Notes==

*) The expected number of transaction subgraphs that happen by chance
goes to zero eventually as the blockchain steps ahead. Unless the fp
rate is very high (close to 1) and time between wallet transaction very
long, in which case the binomial coefficient term gets larger more
quicker than the exponential decay P^B term gets smaller.
*) fp rate doesn't help in most cases that much compared to the
exponential drop-off from time ticking ahead requiring more downloading
of blocks
*) its good for privacy if bitcoin outputs are spent more frequently so
P is higher, because that creates more transaction subgraphs in the
anonymity set.
*) its good for privacy if more outputs are made per block, although
still only linearly which is no match for the exponential reduction from
the P^B term.
*) its good for privacy to make less of your own transactions (increase
J and reduce w), for low-activity users the privacy of committed bloom
filters can be actually pretty good, for high-activity users who use
bitcoin's blockchain all the time it's not very good
*) For the reasonable values I tried for a once-a-month user with fp=1%,
their chance-transaction-subgraph-count drops below 1 in about eight months.
*) Because of the exponential nature, E(#G) goes from "billions of
billions" to "about 10" fairly quickly.

==Discussion of ways to mitigate this==

One way is to not use change outputs. This is unrealistic, doesn't match
people's behavior and money must be divisible.

A better way to mitigate this is to not leak the information that all
those blocks are interesting to the same wallet. Don't download all
blocks from the same archival node. If you download blocks from many
different nodes, it gives an incentive for surveillance startups to
create lots of sybil nodes they control and can then correlate together
block downloads with the wallet IP address. Many such startups are
already doing this today to try to detect the origin IP address of
broadcasted transactions.

Another solution could be to download a few blocks from different nodes
with new tor circuits used. This would delink the wallet IP address from
the downloads and would help a lot. This has the issue that tor is
slower (but still not as slow as downloading the entire blockchain)

Another way a wallet could be correlated with its block downloads is
timing correlations. At any one time only a certain number of peers
would be downloading blocks which narrows down which wallets are
downloading what. However even today Bitcoin Core downloads blocks in
parallel from many nodes so there's probably quite a large anonymity set
for lightweight wallets using committed bloom filters. Plus timing
correlation can be reduced simply by waiting longer. Wallets are not
sync'd from backup very often so it might be okay to wait.

Another way to improve privacy could be for the wallet to choose random
transaction subgraphs and download all the blocks related to them as well.

Wallet developers might choose to allow the user to configure their own
fp rate. This is probably not a good idea since the relationship between
fp rate and anonymity set is non-obvious. It might be better to ask the
user how often they expect to make transactions.

==Conclusion==

I think this committed bloom filter idea is very good and much better
than bip37, but for good privacy for when bitcoin is used often still
requires certain behavior namely downloading blocks
from many different peers with new tor circuits.

Note that I've been dealing with counting transaction subgraphs but
actually finding them from blocks might also be computationally
infeasible. Although a Bayesian approach worked very
well for similar transaction subgraph linking
[https://arxiv.org/pdf/1612.06747v3.pdf]

It would also be interesting to analyze what information a spy can get
if they are missing some blocks that the wallet downloaded.

For the long term, private and high-volume bitcoin use will be best
served by off-chain transactions. They will probably be a huge win just
because the large and public blockchain is such a non-private
way of doing things.

On 09/05/16 09:26, bfd--- via bitcoin-dev wrote:

-------------------------------------
Thinking about this a bit, I support this proposal for a BIP.
This is not Bitcoin, but address types are bound to meet in meat-space and 
it would be good to have a central place where this is defined.

I would very much appreciate someone that worked on BIP32/BIP43 itself to 
comment on the details.

Quoting bip 43;


"We encourage different schemes to apply for assigning a separate BIP
number and use the same number for purpose field, so addresses won't be
generated from overlapping BIP32 spaces."



On Wednesday, 12 April 2017 12:02:37 CEST Nick Johnson via bitcoin-dev 
wrote:


-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
I agree with Greg and Laolu; BIP-37 filtering for transactions is no better
than for blocks and completely destroys privacy.

A constant stream of transactions is OK, but even cheaper for light clients
would be Laolu's proposal of streaming more tx data than existing inv
messages but less than existing tx messages.

We could make a bit field of things to include in every inv-with-metadata
message, such as:
- witness data
- scriptSig data pushes
- scriptPubKey
- hash of scriptPubKey (unnecessary if full scriptPubKey is sent)
- scriptPubKey data pushes
- etc.

This way a full node might be able to tell what application (or type of
application) a light client is running, but not the client's addresses or
outputs, except maybe when the client originates transactions.

On Thu, Jun 1, 2017 at 10:28 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I know there are posts, and an issue opened against it, but is there anyone
writing a BIP for Sign / Verify message against a SegWit address?

I realize it is not a feature in wide use, but I think it still serves an
important purpose, such as when proof of assets are requested.

ref: https://github.com/bitcoin/bitcoin/issues/10542
-------------------------------------
On 01/07/2017 12:26 PM, Chris Priest via bitcoin-dev wrote:


It's a bug, not a feature.

e

-------------------------------------
It would not change the number of Bitcoins in existence.

--
Sent from my mobile device.
Please do not email me anything that you are not comfortable also sharing with the NSA.

-------------------------------------
I own some miners, but realistically their end of life is what, 6 months
from now if I'm lucky?    If we used difficulty ramps on two selected
POW's, then the migration could be made smooth.   I don't think changing
the POW would be very challenging.  Personally, I would absolutely love to
be back in the business of buying GPU's instead of ASICs which are
uniformly sketchy.   Does anyone *not* mine their own equipment before
"shipping late" these days?

Maybe sample a video game's GPU operations and try to develop a secure hash
whose optimal implementation uses them in a similar ratio?   Ultimately, I
think it would very challenging to find a POW that doesn't make a bad
problem worse.  I understand that's why you suggested SHA3.

Hopefully, the "nanometer race" we have will work more smoothly once the
asicboost issue is resolved and competition can return to normal.   But
"waiting things out" rarely seems to work in Bitcoin land.






On Mon, Apr 10, 2017 at 11:25 AM, g <g@cognitive.ch> wrote:

-------------------------------------
On Wed, Mar 8, 2017 at 5:16 PM, Eric Voskuil <eric@voskuil.org> wrote:

So you're saying that a -onlyacceptconnectionsfrom=IP option wouldn't
be a concern to you because it can't exclude people? Of course it can
exclude people - just not your ISP or a state-level attacker.

Please, Eric. I think I understand your concern, but this argument
isn't constructive either.

The proposal here is to introduce visible node identities on the
network. I think that's misguided as node count is irrelevant and
trivial to fake anyway. But you bringing up BIP150 here isn't useful
either. I know that you equate the concept of having verifiable
identity keys in the P2P with a step towards making every node
identifiable, but they are not the same. It's just a cryptographic
tool to keep a certain class of attackers from bypassing restrictions
that people can already make.

-- 
Pieter

-------------------------------------
On Apr 7, 2017 12:42, "Gregory Maxwell" <greg@xiph.org> wrote:

On Fri, Apr 7, 2017 at 6:52 PM, Tom Harding via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Only with the additional commitment structure such as those proposed
by Peter Todd in his stxo/txo commitment designs, e.g.
https://petertodd.org/2016/delayed-txo-commitments


Light nodes are improved by detecting invalid transactions, even before
they are mined.
-------------------------------------
Doing a second soft-fork from 50% to 75% sounds more difficult since
that's going from a more restrictive ruleset to less restrictive, you
might be able to hack around it but it wouldn't be a fully backwards
compatible change like going from 75% to 50% would be. 50% vs 75% does
affect max transactions/second in practice, the exact amount depends
on the real world usage of course though.

On Tue, May 9, 2017 at 11:19 AM, Sergio Demian Lerner via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

Point to this solved problem?

Your "solution" here is not a solution:

https://www.reddit.com/r/Bitcoin/comments/6f1urd/i_think_its_time_we_have_an_educated_discussion/diey21t/?context=3 <https://www.reddit.com/r/Bitcoin/comments/6f1urd/i_think_its_time_we_have_an_educated_discussion/diey21t/?context=3>


Let's assume you invented a simple way to double-spend txns to self (which you haven't, fyi), then that is an issue in of itself as the point of bitcoin is to *prevent* double-spending to self.

There would need to be much more time for the community to discuss the implications of wallets have a "double-spend to self" button in them.


Yes it does destroy same-chain fungibility, as discussed on twitter [1], you're making miner coins special on both chains.


It effectively does. If people want to proceed blindly, ignoring replay, they're welcome to read about the consequences [2].

[1] https://twitter.com/taoeffect/status/872226556571131905 <https://twitter.com/taoeffect/status/872226556571131905>
[2] http://gist.github.com/taoeffect/c910ebb16d9f6d248e9f1f3c6e10b1b8 <http://gist.github.com/taoeffect/c910ebb16d9f6d248e9f1f3c6e10b1b8>

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
Russell O'Connor via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
writes:

I have a similar proposal to Russell; use tx nVersion.  However, my
subset is simpler, and uses fewer precious nVersion bits:

1. Top version 26 bits must be 1 (say)
2. Next bit indicates positive (must have bit set) or negative (must NOT
   have bit set).
3. Bottom 5 bits refer to which BIP8/9 bit we're talking about.

This only allows specifying a single bit, and only support BIP8/9-style
signalling.

I believe we can skip the timeout: miners don't signal 100% either way
anyway.  If a BIP is in LOCKIN, wallets shouldn't set positive on that
bit (this gives them two weeks).  Similarly, if a BIP is close to
FAILED, don't set positive on your tx.  Wallets shouldn't signal until
any bit until see some minimal chance it's accepted (eg. 1 in 20 blocks).


This is gentler on miners than a UASF flag day, and does offer some
harder-to-game signalling from bitcoin users.

False signalling miners still have the 2 week LOCKIN period to upgrade,
otherwise they can already lose money.  You could argue they're *more*
likely to upgrade with a signal that significant parts of the economy
have done so.

Cheers,
Rusty.

-------------------------------------
On Sat, Apr 1, 2017 at 3:15 PM, Natanael <natanael.l@gmail.com> wrote:

No, because of the way the weight is calculated, it is impossible to
create a block that old nodes would perceive as bigger than 1 mb
without also violating the weight limit.
After segwit activation, nodes supporting segwit don't need to
validate the 1 mb size limit anymore as long as they validate the
weight limit. The weight is also the only notion of cost miners need
to consider when comparing txs by feerate (fee per cost, before segwit
tx_fee/tx_size, post-segwit tx_fee/tx_weight).
This is important to remember, because having 2 separated limits or
costs would make block creation and relay policies much harder to
implement.

Therefore a hardfork after segwit can just increase the weight limit
and completely forget about the pre-segwit 1 mb size limit.

-------------------------------------


Exactly!


This is actually incorrect. There are two transactions involved in LN. The funding transaction, which opens a payment channel, and a commitment transaction, which closes the channel when broadcasted to the network (the cooperative closing transaction can be considered a commitment transaction in a loose sense).

Now you want to protect the funding transaction, as otherwise you would be subject to a replay-attack on all *past* forks. So you will set `nForkId>=1` for the funding transaction, making this payment channel non-existent on any *past* forks. However, if during the lifetime of the payment channel another fork happens, the payment channel exists for both tokens. So for the commitment transaction, you will have `nForkId=0`, making it valid on both of these chains. While this `nForkId` is valid on all chains, the parent transaction it tries to spend (the funding transaction) does only exist on two chains, the original one you created the channel for and the one that forked away.
-------------------------------------
BIP 177: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks


This BIP proposes to address the issue of transactional reliability in Bitcoin, where valid transactions may be stuck in the mempool for extended periods.


There are two key issues to be resolved:


  1.  The current transaction bandwidth limit.
  2.  The current ad-hoc methods of including transactions in blocks resulting in variable and confusing confirmation times for valid transactions, including transactions with a valid fee that may never confirm.


It is important with any change to protect the value of fees as these will eventually be the only payment that miners receive. Rather than an auction model for limited bandwidth, the proposal results in a stable fee for priority service auction model.


I will post the full proposal up on to my blog in the coming days and, re-review incorporating feedback that I have received on and off thread. It would not be true to suggest that all feedback received has been entirely positive although, most of it has been constructive.


The previous threads for this BIP are available here:

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/subject.html


Regards,

Damian Williamson
-------------------------------------
Martin:

Re: Miners not relaying unconfirmed txs... I was saying that this was a good thing from your perspective in wanting to give users the choice on which miners would get to confirm the tx. So then like we don't need to implement any kind of special bloated transaction that is only mine-able by some explicit set of miners... No fork or compatibility problems are necessary, can be completely implemented as an added feature.

Re: "Miners": I don't really like calling them "transaction processors" because in bitcoin, every synchronizing node that verifies signatures is a transaction processor. What sets them apart from full relay nodes is they create "blocks", which are "ledger change candidates" that included transactions and proof-of-work (PoW: deterministic diffusion puzzle solutions). They help create confidence that transactions in blocks will never by double spent by requiring that double spending would need lots of economic resources for someone else to re-perform the PoW.

Given the above definition of a "block", I would be happy calling them "Block Producers"... which does not imply that they do all of the necessary "transaction processing": that all users should be fine with running Electrum wallets or even SPV clients. They produce blocks, but its still up to other users in the network to do "transaction processing": decide for themselves if they want to accept particular blocks.

Cheers,
Praxeology Guy

-------- Original Message --------
Subject: Re: [bitcoin-dev] Inquiry: Transaction Tiering
Local Time: March 25, 2017 12:15 PM
UTC Time: March 25, 2017 5:15 PM
From: martin@stolze.cc
To: praxeology_guy <praxeology_guy@protonmail.com>
bitcoin-dev@lists.linuxfoundation.org

Thanks, those are valid concerns.

That is the idea. Transaction Processors could source transactions
from the public mempool as well their proprietary mempool(s).

Not so, a user may want to incentivise a specific Transaction
Processor or many. A user can detect this behavior and withdraw his
future business if he notices that his transaction is not included in
a block despite there being transactions with lower fees included.
Remember, the transaction can be advertised to different mempools and
a Transaction Processor could lose this business to a competitor who
processes the next block if he holds it back.

Best Regards
Martin

PS: It seems not too late to get rid of misleading terms like "miner".
Block rewards (infrastructure subsidies) will be neglectable for
future generations and the analogy is exceedingly poor.

On Sat, Mar 25, 2017 at 4:42 AM, praxeology_guy
<praxeology_guy@protonmail.com> wrote:
-------------------------------------
Dear all,

An initial reference implementation of bip-genvbvoting (spec: [1]) is now available at

https://github.com/sanch0panza/bitcoin/commits/genvbvoting-bu-dev-clean1

starting at commit fdd83383436ee43b072c258d4a6feb713902c77e .

This development is based against the Bitcoin Unlimited 'dev' branch, and has been submitted as PR458 to BU [2].
The naming of the new 'bipgenvb_forks' output section in the 'getblockchain' RPC interface is to be considered temporary while the BIP has no formal number.

I would be happy to get any feedback while I implement a corresponding pull request for a reference implementation on Bitcoin Core. Due to other commitments this may come at a later stage - if someone else is eager to port it over, please feel free.

Regards,
Sancho

[1] https://github.com/sanch0panza/bips/blob/bip-genvbvoting/bip-genvbvoting.mediawiki
[2] https://github.com/BitcoinUnlimited/BitcoinUnlimited/pull/458

P.S. The revised "unknown version check" code is considered an implementation specific and not part of core functionality, and is consequently not fully covered by regression tests.
-------------------------------------
Maybe I'm getting this wrong but wouldn't this scheme imply that a miner is
incentivized to limit the amount of transactions in a block to capture the
maximum fee of the ones included?

As an example, mined blocks currently carry ~0.8 btc in fees right now. If
I were to submit a transaction paying 1 btc in maximal money fees, then the
miner would be incentivized to include my transaction alone to avoid that
lower fee paying transactions reduce the amount of fees he can earn from my
transaction alone. This would mean that I could literally clog the network
by paying 1btc every ten minutes.

Am I missing something?

Daniele
-------------------------------------
On 11/21/2017 01:16 PM, Adán Sánchez de Pedro Crespo via bitcoin-dev wrote:

Where can I find more resources on this described DoS attack?
And how does SegWit prevent this if using SegWit transactions are not enforced?

Thanks

-------------------------------------
On Monday, 19 June 2017 18:30:18 CEST Jonas Schnelli wrote:

I mentioned that it was a usability point for a reason, and your personal 
behavior makes me want to quote one of the main UX guidelines; 
  “You are not your user”

http://uxmyths.com/post/715988395/myth-you-are-like-your-users
older;
http://52weeksofux.com/post/385981879/you-are-not-your-user

I think we should defer to actual real numbers and user reseach, as has been 
quoted by Andreas. You disagreeing based on your own experience and behavior 
is worse than useless. As the above links show.

Don’t fall in that trap :)
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
Hi all,

I did not follow the whole discussion, but wanted to throw in some
literature on the failure of crypto primitives in Bitcoin.

There is a paper which discusses the problems, but does not give any
remedies: https://eprint.iacr.org/2016/167.pdf

And there are also contingency plans on the wiki:
https://en.bitcoin.it/wiki/Contingency_plans These are not very
detailed and my impression is that this information should be viewed
very critically (E.g., when ECDSA is broken, the suggested vague
response is "Switch to the stronger algorithm." Yeah. And "Code for
all of this should be prepared." Surely. As far as I know, there is no
such code and no-one is working on it).

Best,
Henning


On Sat, Feb 25, 2017 at 09:45:36AM -0800, Shin'ichiro Matsuo via bitcoin-dev wrote:

-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp

-------------------------------------
Here is the text of a post to reddit from Feb 2017, discussing a similar
idea, and wondering if it could reduce the incentive to delay broadcast of
solved blocks:

# How an anchored Proof of Stake Sidechain can help the Bitcoin main chain

# Steps:

1. Soft fork Bitcoin to enable side chains

2. Create a side chain that is secured with Proof of Stake. Call blocks on
this chain "POS-blocks." The original chain is made of "POW-blocks."

3. Side chain mints one POS-block after each POW-block on the main chain,
and contains the hash of the POW-block, and the hash of the previous
POS-block. See diagram [here.](https://s32.postimg.org/916n9zxlx/Pos_Sf1.png
)

4. Side chain Minters must make a deposit in order to mint. If they mint an
invalid POS-block, they lose the deposit.

5. Side chain blocks are small enough to broadcast and validate quickly
(e.g. 100 - 300 KB).

6. Soft fork a new rule into Bitcoin that encourages POW-blocks to include
the hash of the prior POS-block. Specifically, penalize POW-blocks which do
not point to the prior POS-block by doubling the difficulty necessary for
them to be valid. Call POW-blocks which do not point to the prior POS-block
but are valid because of their very high POW "hard blocks."

In the following diagram POW2 and POW4 are valid because they point to the
prior POS-block and also satisfy the difficulty requirement. POW3 does not
point to the prior POS-bock, but is valid anyway because it contains proof
of work at twice the normal difficulty.

https://s27.postimg.org/6hc0b8ejn/Pos_Sf2.png

# Advantages:

1. Allow people who do not control ASICs to influence which transactions
happen.

2. Allow people who do not control ASICs to influence which chain gets
extended.

3. Reduce the incentive to selfish mine. Once a miner discovers a block,
they should broadcast it immediately in the hopes that a Minter will build
on it, because that is the most likely way their block will not go stale.
Miners will also immediately start trying to build a hard block. (Maybe
statistics could tell us what is the proper range for the Hardness
Multiplier. I guessed 2 would be good.)

4. Reduces the effective block interval while reducing the incidence of
stale blocks, empty blocks and SPV mining. After a POW-block is mined, it
is immediately broadcast to the network, seeking a qualified Minter to
extend it. Minters have a deposit, which they will lose if they mint an
invalid block. Pointing to the hash of an invalid POW-block would produce
an invalid minted block, so Minters have a strong incentive to completely
validate the POW-block before they mint on top of it. After validating,
they mint a block and broadcast it. While the Minter is validating the
previous POW-block, competing miners also have time to fully validate the
previous POW-block. By the time the Miners receive the POS-block, they know
what transactions they can and cannot include in their own block, because
the Minter only processes transactions on the side chain. There is less
reason to mine empty blocks, because there is adequate time to update the
mempool before mining the next soft block begins, and because hard blocks
take a long time to produce. The risks involved with mining on an
un-validated POS-block can be made small by the fact that there is a
deposit that will be destroyed if the POS-block is invalid. POS-blocks can
be validated quickly because they are small.

Here is a gantt chart of the schedule described above:

https://s22.postimg.org/hvnkyac5d/Pos_Sf3.png

5. Unlike a pure POS system, a cabal of early Stake holders cannot cheaply
hatch an alternate history. Much less imperative for nodes to go to a
trusted peer and ask whether the chain they are being fed is legitimate.

6. After step 6 above, the side chain would have essentially the same
security characteristics as the main chain, and could be used
interchangeably.

7. No hard fork required, and this soft fork could be deployed even if the
miners do not consent to it. Some hybrid POS system would be my
recommendation as preferable to simply changing POW algorithms in the face
of miners abusing their power.

8. Users can opt into the POS sidechain, and it can fully prove itself in
production before there is any attempt to anchor the main chain back to it.
Even if consensus cannot be obtained to execute step 6, the mere existence
of such a chain could deter tomfoolery on the part of POW miners, lest they
galvanize the community into throwing the switch.

Original reddit post here:

https://www.reddit.com/r/Bitcoin/comments/5vy4qc/how_an_anchored_proof_of_stake_sidechain_can_help/
-------------------------------------
I would like to propose an implementation that accomplishes the first
part of the Barry Silbert proposal independently from the second:

"Activate Segregated Witness at an 80% threshold, signaling at bit 4"
in a way that

The goal here is to minimize chain split risk and network disruption
while maximizing backwards compatibility and still providing for rapid
activation of segwit at the 80% threshold using bit 4.

By activating segwit immediately and separately from any HF we can
scale quickly without risking a rushed combined segwit+HF that would
almost certainly cause widespread issues.

Draft proposal:
https://github.com/jameshilliard/bips/blob/bip-segsignal/bip-segsignal.mediawiki

Proposal text:
<pre>
  BIP: segsignal
  Layer: Consensus (soft fork)
  Title: Reduced signalling threshold activation of existing segwit deployment
  Author: James Hilliard <james.hilliard1@gmail.com>
  Status: Draft
  Type: Standards Track
  Created: 2017-05-22
  License: BSD-3-Clause
           CC0-1.0
</pre>

==Abstract==

This document specifies a method to activate the existing BIP9 segwit
deployment with a majority hashpower less than 95%.

==Definitions==

"existing segwit deployment" refer to the BIP9 "segwit" deployment
using bit 1, between November 15th 2016 and November 15th 2017 to
activate BIP141, BIP143 and BIP147.

==Motivation==

Segwit increases the blocksize, fixes transaction malleability, and
makes scripting easier to upgrade as well as bringing many other
[https://bitcoincore.org/en/2016/01/26/segwit-benefits/ benefits].

This BIP provides a way for a simple majority of miners to coordinate
activation of the existing segwit deployment with less than 95%
hashpower. For a number of reasons a complete redeployment of segwit
is difficulty to do until the existing deployment expires. This is due
to 0.13.1+ having many segwit related features active already,
including all the P2P components, the new network service flag, the
witness-tx and block messages, compact blocks v2 and preferential
peering. A redeployment of segwit will need to redefine all these
things and doing so before expiry would greatly complicate testing.

==Specification==

While this BIP is active, all blocks must set the nVersion header top
3 bits to 001 together with bit field (1<<1) (according to the
existing segwit deployment). Blocks that do not signal as required
will be rejected.

==Deployment==

This BIP will be deployed by a "version bits" with an 80%(this can be
adjusted if desired) activation threshold BIP9 with the name
"segsignal" and using bit 4.

This BIP will have a start time of midnight June 1st, 2017 (epoch time
1496275200) and timeout on midnight November 15th 2017 (epoch time
1510704000). This BIP will cease to be active when segwit is
locked-in.

=== Reference implementation ===

<pre>
// Check if Segregated Witness is Locked In
bool IsWitnessLockedIn(const CBlockIndex* pindexPrev, const
Consensus::Params& params)
{
    LOCK(cs_main);
    return (VersionBitsState(pindexPrev, params,
Consensus::DEPLOYMENT_SEGWIT, versionbitscache) ==
THRESHOLD_LOCKED_IN);
}

// SEGSIGNAL mandatory segwit signalling.
if ( VersionBitsState(pindex->pprev, chainparams.GetConsensus(),
Consensus::DEPLOYMENT_SEGSIGNAL, versionbitscache) == THRESHOLD_ACTIVE
&&
     !IsWitnessLockedIn(pindex->pprev, chainparams.GetConsensus()) &&
// Segwit is not locked in
     !IsWitnessEnabled(pindex->pprev, chainparams.GetConsensus()) ) //
and is not active.
{
    bool fVersionBits = (pindex->nVersion & VERSIONBITS_TOP_MASK) ==
VERSIONBITS_TOP_BITS;
    bool fSegbit = (pindex->nVersion &
VersionBitsMask(chainparams.GetConsensus(),
Consensus::DEPLOYMENT_SEGWIT)) != 0;
    if (!(fVersionBits && fSegbit)) {
        return state.DoS(0, error("ConnectBlock(): relayed block must
signal for segwit, please upgrade"), REJECT_INVALID, "bad-no-segwit");
    }
}
</pre>

https://github.com/bitcoin/bitcoin/compare/0.14...jameshilliard:segsignal-v0.14.1

==Backwards Compatibility==

This deployment is compatible with the existing "segwit" bit 1
deployment scheduled between midnight November 15th, 2016 and midnight
November 15th, 2017. Miners will need to upgrade their nodes to
support segsignal otherwise they may build on top of an invalid block.
While this bip is active users should either upgrade to segsignal or
wait for additional confirmations when accepting payments.

==Rationale==

Historically we have used IsSuperMajority() to activate soft forks
such as BIP66 which has a mandatory signalling requirement for miners
once activated, this ensures that miners are aware of new rules being
enforced. This technique can be leveraged to lower the signalling
threshold of a soft fork while it is in the process of being deployed
in a backwards compatible way.

By orphaning non-signalling blocks during the BIP9 bit 1 "segwit"
deployment, this BIP can cause the existing "segwit" deployment to
activate without needing to release a new deployment.

==References==

*[https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013714.html
Mailing list discussion]
*[https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283
P2SH flag day activation]
*[[bip-0009.mediawiki|BIP9 Version bits with timeout and delay]]
*[[bip-0016.mediawiki|BIP16 Pay to Script Hash]]
*[[bip-0141.mediawiki|BIP141 Segregated Witness (Consensus layer)]]
*[[bip-0143.mediawiki|BIP143 Transaction Signature Verification for
Version 0 Witness Program]]
*[[bip-0147.mediawiki|BIP147 Dealing with dummy stack element malleability]]
*[[bip-0148.mediawiki|BIP148 Mandatory activation of segwit deployment]]
*[[bip-0149.mediawiki|BIP149 Segregated Witness (second deployment)]]
*[https://bitcoincore.org/en/2016/01/26/segwit-benefits/ Segwit benefits]

==Copyright==

This document is dual licensed as BSD 3-clause, and Creative Commons
CC0 1.0 Universal.

-------------------------------------
This unnecessarily complicates transaction selection for miners by
introducing a second (and possibly third if I understand your proposal
correctly) dimension to try to optimize.

See:  https://en.wikipedia.org/wiki/Bin_packing_problem

Segwit already solves this exact issue by replacing block size with block
weight, so I fail to see how this proposal would make any improvements
without introducing significant complications.



​
-------------------------------------
How about using for the first stage, `<...> OP_CALCMERKLEROOT <root> OP_EQUAL` 
instead of `<root...> OP_CHECKMERKLEBRANCH`? There's maybe 1 or 2 bytes extra, 
but it seems more future-proof (since there could more easily be alternatives 
to `<root> OP_EQUAL` in future script versions).

OTOH, OP_ADDTOSCRIPTHASH may be fatally incompatible with script versioning... 
Old nodes won't know how to check the witness program, which means an 
undefined version could be used to bypass the correct script entirely.
Need to think more on this still.

Luke


On Wednesday 01 November 2017 3:08:46 PM Mark Friedenbach wrote:

-------------------------------------
A hard-fork is a situation where non-upgraded nodes reject a block mined
and relayed by upgraded nodes.  This creates a fork that cannot heal
regardless of what follows.

This proposal is not a hard-fork, because the non-upgraded node *will heal*
if the attack has less than 1/2 of the original-POW power in the long term.

The cost of such an attack is the cost of a normal "51%" attack, multiplied
by the fractional weight of the original POW (e.g. 0.75 or 0.5).

So rather than saying this is a hard-fork, I would say that this is a
soft-fork with reduced security for non-upgraded nodes. I would also say
that the reduction in security is proportional to the reduction in weight
of the original POW at the time of attack.

As mentioned before, the original-POW weight starts at 1.0 and is reduced
over a long period of time.  I would set up the transition curve so that
all nodes upgrade by the time the weight is, say, 0.75.  In reality, nodes
protecting high economic value would upgrade early.

On Mon, Nov 6, 2017 at 3:55 PM Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Knowing that a transaction is property formatted and that it has been
broadcast to the gossip network is useful in many situations. You're only
thinking about whether you can know a transaction is valid and/or settled.
This is not the only possible useful information in actual real world use.
Any situation where credit card transactions are accepted today for
instance, it is useful to know that a transaction has been initiated, even
though it can be reversed at any time up to 60 days later.

Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com>

On Tue, Jan 3, 2017 at 4:10 PM, <bfd@cock.lu> wrote:

-------------------------------------

Presented is a generalised way of providing replay protection for future hard forks. On top of replay protection, this schema also allows for fork-distinct addresses and potentially a way to opt-out of replay protection of any fork, where deemed necessary (can be beneficial for some L2 applications).

## Rationale

Currently when a hard fork happens, there is ad-hoc replay protection built within days with little review at best, or no replay protection at all. Often this is either resource problem, where not enough time and developers are available to sufficiently address replay protection, or the idea that not breaking compatibility is favourable. Furthermore, this is potentially a recurring problem with no generally accepted solution yet. Services that want to deal in multiple forks are expected to closely follow all projects. Since there is no standard, the solutions differ for each project, requiring custom code for every fork. By integrating replay protection into the protocol, we advocate the notion of non-hostile forks.

Users are protected against accidentally sending coins on the wrong chain through the introduction of a fork-specific incompatible address space. The coin/token type is encoded in the address itself, removing some of the importance around the question _What is Bitcoin?_. By giving someone an address, it is explicitly stated _I will only honour a payment of token X_, enforcing the idea of validating the payment under the rules chosen by the payee.

## Iterative Forks

In this schema, any hard fork is given an incremented id, `nForkId`. `nForkId` starts at `1`, with `0` being reserved as a wildcard. When project X decides to make an incompatible change to the protocol, it will get assigned a new unique `nForkId` for this fork. A similar approach like for BIP43 can be taken here. Potentially `nForkId` can be reused if a project has not gained any amount of traction.

When preparing the transaction for signing or validation, `nForkId` is appended to the final template as a 4B integer (similar to [1]). Amending BIP143, this would result in

```
 Double SHA256 of the serialization of:
     1. nVersion of the transaction (4-byte little endian)
     2. hashPrevouts (32-byte hash)
     3. hashSequence (32-byte hash)
     4. outpoint (32-byte hash + 4-byte little endian)
     5. scriptCode of the input (serialized as scripts inside CTxOuts)
     6. value of the output spent by this input (8-byte little endian)
     7. nSequence of the input (4-byte little endian)
     8. hashOutputs (32-byte hash)
     9. nLocktime of the transaction (4-byte little endian)
    10. sighash type of the signature (4-byte little endian)
    11. nForkId (4-byte little endian)
```


For `nForkId=0` this step is ommitted. This will immediately invalidate signatures for any other branch of the blockchain than this specific fork. To distinguish between `nForkId=0` and `nForkId` hardcoded into the software, another bit has to be set in the 1B SigHashId present at the end of signatures.

To make this approach more generic, payment addresses will contain the fork id, depending on which tokens a payee expects payments in. This would require a change on bech32 addresses, maybe to use a similar format used in lightning-rfc [2]. A wallet will parse the address, it will extract `nForkId`, and it displays which token the user is about to spend. When signing the transaction, it will use `nForkId`, such that the transaction is only valid for this specific token. This can be generalised in software to the point where replay protection *and* a new address space can be introduced for forks without breaking existing clients.

For light clients, this can be extended by enforcing the coinbase/block header to contain the `nForkId` of the block. Then the client can distinguish between different chains and tokens it received on each. Alternatively, a new P2P message type for sending transactions could be introduced, where prevOut and `nForkId` is transmitted, such that the lite client can check for himself, which token he received.

Allowing signatures with `nForkId=1` can be achieved with a soft fork by incrementing the script version of SegWit, making this a fully backwards compatible change.

[1]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-February/013542.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-February/013542.html>
[2]
https://github.com/lightningnetwork/lightning-rfc/blob/master/11-payment-encoding.md <https://github.com/lightningnetwork/lightning-rfc/blob/master/11-payment-encoding.md>
-------------------------------------
Generally I like the idea, but maybe we should come up with a
(Bech32-based?) new standard that also includes the key birthdate (aka
"wallet birthdate").

Also I heard Core will mix addresses of all types on the same HD chain.
What prefix would it pick? "*pub"?


On 09/05/2017 12:25 PM, Thomas Voegtlin via bitcoin-dev wrote:


-------------------------------------
On Saturday, 18 March 2017 16:23:16 CEST Chris Stewart via bitcoin-dev 
wrote:

I agree with your assessment, the sides are political and picking sides 
makes people a target.
For that reason I know that many companies are not picking sides, we’ve seen 
some bad stuff happen to companies that did.

I’m not convnced it makes sense to use anonymous, but provable, identities 
is the way to solve this. Though.

I also don’t believe people are rejecting proposals purely based on the 
name. What I see is that pratically all proposals are ignored for the time 
being becaues we can’t make any changes anyway until we have made a protocol 
upgrade and came out stronger.
I do agree that bips are seen politically, but not based on the person that 
suggests them, but more based on the content being useful for their 
political side.

I am not entirely against pseudonymous submissions, but in that case I think 
it should be carried by a well known member of the Bitcoin community.

This raises the bar somewhat to a point where you have to convince someone 
that is already publicly known to propose it with you.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
We support a change to the version bits of the HD serialization that will
inform the receiving utility of the exact derivation method used for the
pubkeys. Third-parties handling xpubs must not require additional
information from the user about the derivation path or serialization format
of the addresses under that xpub. When you have to ask, "Is this a SegWit
xpub?" then you've already lost.

Avoiding a total UX nightmare is in everyone's interests.

I think Luke and Thomas may be talking past one another. When exporting a
root master HD seed, encoding the {x,y,z}{pub,prv} distinctions makes no
sense, as the root seed should derive all paths for all coins. Wallets may
need additional code to discover which paths have been used when importing
a root seed. But when exporting / importing an account-level seed for
watch-only and receive address generation, changing the serialization
version bytes is appropriate and (in our view) essential to avoid loss of
funds.

The Electrum approach is nice but may not go far enough, as xpub and zpub
both list "P2PKH or P2SH." Why not expand the number of version prefixes to
eliminate the ambiguity?


On Tue, Sep 5, 2017 at 1:09 PM, Thomas Voegtlin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
-Kabuto

PGP Fingerprint: 1A83 4A96 EDE7 E286 2C5A  B065 320F B934 A79B 6A99
-------------------------------------
Thanks for the feedback.
I'll post a link to more refined proposal on github once that elaboration is more complete.
For now I think more discussion will be very helpful.
I think the flexibility around the tallying window size will take the most careful consideration, so that a user of this proposal can retain full compatibility with BIP9 for a certain versionbit if (s)he wishes.

The entire point of BIP9 is to allow nodes that do not know about an upgrade
to still have a functional state machine. But I don’t see how you can have a
state machine if the two basic variables that drive it are not specified.

What I mean by the state machine remaining essentially unchanged is that its basic design (states and transitions) would remain the same.
But the parameters that decide those transitions would be unique per bit.
I think you misunderstood me if you think there will be strictly one singular state machine.

Instead nodes would effectively be running a state machine instance for each signaling bit - with each state machine possibly (but not necessarily!) configured differently.

An initial implementation might provide this all in compiled code.
A slightly more sophisticated implementation would push the signaling configuration mostly into an external configuration file which could adhere to a fixed format and could easily be adapted and shared between implementations.

But in my opinion we would not be able to have a state machine without those
variables in the actual BIP because old nodes would miss the data to
transition to certain states.

As I see it, this is the same situation we are in now with old nodes - they see that there is some action on unknown bits, but they can do nothing more than warn their operators about this.
This proposal does not fundamentally change that situation.

Maybe an idea; we have 30 bits. 2 currently in use (although we could reuse
the CSV one). Maybe we can come up with 3 default sets of properties and
when a proposal starts to use bit 11 it behaves differently than if it uses
22.

One could place conventions on how certain bit ranges are used, but I don't much see the point of the BIP doing this, although it could suggest examples.

I would prefer if the BIP's reference implementation provides strict BIP9 compatibility in that how it configures the bits (i.e. all with 2016 block windows evaluated in strict synchronicity with BIP9, and default 95% thresholds).
Of course in reality most bits are unused today.
Someone wishing to use a bit for a feature deployment would announce so publicly (e.g. in a BIP) and release an implementation which is suitably configured.
Others wishing to provide compatibility with that feature would adjust their code and bip-genvbvoting configuration files accordingly.

Sancho
-------------------------------------
Whatever their failings from their previous code or their adversarial
nature, they got this code right and I'm only presenting it as a real and
excellent solution for the impending threat to bitcoin. As a big core fan,
I really wanted to delete the word Cash from my post because I was afraid
someone would turn this technical discussion into a political football.

On Nov 2, 2017 7:37 PM, "Gregory Maxwell" <greg@xiph.org> wrote:

On Thu, Nov 2, 2017 at 11:31 PM, Scott Roberts via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This is the bitcoin development mailing list, not the "give free
review to the obviously defective proposals of adversarial competing
systems" mailing list. Your posting is off-topic.
-------------------------------------
Jaja. But no shit. Not perfect maybe, but Bitcoin was never perfect. It has
always been good enough. And at the beginning it was quite simple. Simple
enough it allowed gradual improvements that anyone with some technical
background could understand. Now we need a full website to explain an
improvement.
But this is becoming more and more out of topic.


On Wed, May 10, 2017 at 11:05 AM, Matt Corallo <lf-lists@mattcorallo.com>
wrote:

-------------------------------------
If there's a better factor than 0.25 I would change it now before deploying
segwit instead of leaving it to be changed later with a hf.

On 9 May 2017 10:59 pm, "Sergio Demian Lerner via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Checking the first few bytes of a Bitcoin Address should not be considered
sufficient for ensuring that it is correct as it takes less than a second
to generate a 3 character vanity address that matches the first 3
characters of an address.

On Mon, 30 Oct 2017, 11:44 shiva sitamraju via bitcoin-dev, <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Thanks for your feedback, I fixed what you suggested. As for the purpose
how should we move on? We would be inclined to use 46, but of course we
are open to any other number.


On 29/08/17 22:07, Luke Dashjr via bitcoin-dev wrote:

-------------------------------------
On Sat, Apr 1, 2017 at 6:58 PM, praxeology_guy <
praxeology_guy@protonmail.com> wrote:


In that case you should read my txo bitfield proposal, instead of taking my
postings yesterday as a prompt to respond to something completely unrelated.



My bitfield proposal is different from the patricia trie stuff. Also your
objection about patricia tries being 'too much work' is nonsensical because
they're quite a bit simpler than MMRs.
-------------------------------------
I don't know if i should response to this mail list or make a comment in
commit file (
https://github.com/sipa/bech32/commit/52b5a0fa6d3174c4150393fb7d6b58d34b4f5e0b#diff-d23a42e5c904045098e8f8b1189f481e
)

* Motivation:

Here I think it could worth to mention that 58 requires mathematical
operations over big numbers. This is not very fast and most of the
programming languages don't provide support for big numbers OOB.

* Why not make an address format that is generic for all scriptPubKeys?:

I understand that if a new generic encoding format is introduced that could
lead to some confusions but what if in the future there is a new type of
address that can also be encoded with bech32? Don't we need a address type
anyway?

thx


2017-03-29 7:07 GMT-03:00 Andreas Schildbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
Johnson,

Yeah, I do still see the issue. I think there are still some reasonable
ways to mitigate it.

I've started revising the extension block specification/code to coexist
with mainchain segwit. I think the benefit of this is that we can
require exiting outputs to only be witness programs. Presumably segwit
wallets will be more likely to be aware of a new output maturity rule
(I've opened a PR[1] which describes this in greater detail). I think
this probably reduces the likelihood of the legacy wallet issue,
assuming most segwit-supporting wallets would implement this rule before
the activation of segwit.

What's your opinion on whether this would have a great enough effect to
prevent the legacy wallet issue? I think switching to witness programs
only may be a good balance between fungibility and backward-compat,
probably better all around than creating a whole new
addr-type/wit-program just for exits.

[1] https://github.com/tothemoon-org/extension-blocks/pull/16

On Mon, Apr 10, 2017 at 06:14:36PM +0800, Johnson Lau wrote:

--
Christopher Jeffrey (JJ) <chjjeffrey@gmail.com>
CTO & Bitcoin Menace, purse.io
https://github.com/chjj
-------------------------------------
Unfortunately a non validating SPV wallet has absolutely no idea if
the information about an unconfirmed transaction they are seeing is
anything but properly formatted. They are connecting to an easily
manipulated, sybil attacked, and untrusted network and then asking
them for financial information. Seeing an unconfirmed transaction in a
wallet that's not also fully validating is at best meaningless.


On 2017-01-03 15:46, Aaron Voisine wrote:

-------------------------------------
Good morning Paul,

It seems many blocks have a coinbase that pays out to a P2PKH.

The public key hash of a potential Accomplice is then readily visible on-chain on the P2PKH of the coinbase.

What is more, the potential Accomplice's hashpower can be judged on-chain also: the more blocks pay out to their P2PKH, the greater their hashpower.

From this, the motivating Thief can blindly and automatically create HTLCs paying out to the public key hash of potential Accomplices, weighed according to how many blocks were mined by those.

Then the motivating Thief can broadcast (perhaps on some website they control, via social media, and so on) the fact of the HTLCs existing, without negotiating with the Accomplices.  It is a simple "take it or leave it": if the theft succeeds (whether the Accomplice assisted in the theft or not) the Accompilce can get paid.  Thus, communication overhead is reduced to a single broadcast message (the Thief might batch a number of different possible Accomplices, and in addition, might want to play on the psychological effect of the broadcast), and the Accomplice is simply faced with the choice: either participate in the theft (and increase the chance they earn money from it) or protect against the theft (and reduce the chance they earn money from it).

Regards,
ZmnSCPxj

Sent with [ProtonMail](https://protonmail.com) Secure Email.

-------------------------------------
On Wed, Sep 13, 2017 at 08:27:36AM +0900, Karl Johan Alm via bitcoin-dev wrote:

Ethereum does something quite like this; it's a very bad idea for a few
reasons:

1) If you bailed out of verifying a script due to wasted ops, how did you know the
transaction trying to spend that txout did in fact come from the owner of it?

2) How do you verify that transactions were penalized correctly without *all*
nodes re-running the DoS script?

3) If the DoS is significant enough to matter on a per-node level, you're going
to have serious problems anyway, quite possibly so serious that the attacker
manages to cause consensus to fail. They can then spend the txouts in a block
that does *not* penalize their outputs, negating the deterrent.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
Also Jonas Nick gave a fairly comprehensive presentation on privacy
leaks in bitcoin protocol including SPV and bloom query problem
specifics:

https://www.youtube.com/watch?v=HScK4pkDNds

Adam


On 20 June 2017 at 14:08, bfd--- via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Friday, 14 July 2017 20:43:37 CEST Clark Moody via bitcoin-dev wrote:

I’m not so clear on this, to be honest.

What is the point of having a user-readable tx-reference?

In the actual blockchain you will still be using txid, and if you want to 
change that then a less readable but more compact format is useful because 
we want to optimize for space, not for human comprehention.

Another usecase I can come up with is you wanting to spend a specific output, 
or you reporting a specific tx as proof to a merchant (or tax office).

For any such usecases you sill need to actually provide a proof of holding 
the private keys and using a human-readable format just doesn’t seem to make 
much sense because a cryptographic proof of ownership is not going to be 
readable however hard you try.

Apologies for missing the point,
can you list one or two usecases that you can see this being used for?
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
Hi,

I was thinking about how to scale the block-chain.

The fundamental problem is that if the miners add capacity it will only
increase (protect) their share of block reward, but does not increase the
speed of transactions. This will only raise the fee in the long run with
the block reward decreasing.
The throughput is limited by the block size and the complexity. Changing
any of variables in the above equation was raised already many times and
there was no consensus on them.

The current chain is effectively single threaded. If we look around in the
sw industry how single threaded applications could be scaled, one viable
option emerge: horizontal scaling. This is an option if the problem can be
partitioned, and transactions in partitions could be processed alongside.
Number of partitions would be start with a fairly low number, something
between 2-10, but nothing is against setting it to a higher number later on
according to a schedule.

Partitioning key alternatives:
*Ordering on inputs:*

1) In this case transactions would need to be mined per input address
partition.
2) TX have inputs in partition 1 and 2, then needs a confirmation in both
partitions.
3) All partitioned chains have the same longest valid approach.
4) Only one chain needed to be considered for double spending, others are
invalid in case they contain that input.

This opens up questions like:
- how the fee will be shared? Fees per partition?
- Ensure a good hash function which spreads evenly, because the inputs
cannot be manipulated for load balancing
- What to do about half mined transactions (Maybe they should be two
transactions and then there is less effect about it, but payment won't be
atomic in both partitions)

*Ordering on transaction ids:*

1) Transactions would be partitioned by their TX-id. Maybe a field allowing
txid-s to match a given partition.
2) Creating blocks like this parallel would be safe for bonefide
transactions. A block will be created each 10 mins.
3) In order to get malicious/doublespent transactions out of the system
another layer must be introduced.
- This layer would be used to merge the parallel blocks. It would have to
refer all previous blocks considered for unspent inputs.
- Most of the blocks will merge just fine as normally block 1 and block 2
would not contain double spending. (of course malicious double spending
will revert speed to current levels, because the miner might have to drop a
block in the partition because it contains a spent input on another
stronger branch)
- The standard longest chain wins strategy would be used for validity on
the meta level
- Meta does not require mining, a branches can be added and they are valid
unless there are double spent inputs inside. Block inside this meta already
"paid for".

Generally both ways would have an effect on the block reward and
complexity, which is needs to be adjusted. (not to create more BTC at the
end, reduced hashing power on partitions.)
I think complexity is not an issue, the important thing is that we tune it
to 10mins / block rate per partition.

Activation could be done by creating the infrastructure first and using
only one partitions only, which is effectively the same as today. Then
activate partitions on a certain block according to a schedule. From that
block, partition enforcement will be active and the transactions will be
sorted to the right partition / chain.

It is easy to make new partitions, just need to activate them on branch
block number.
Closing partitions is a bit more complex in case of TX partitioned
transactions, but managed by the meta layer and activated at a certain
partition block. Maybe it is not even possible in case of input partitions.

I could imagine that it is too big change. Many cons and pros on partition
keys.

What is your opinion about it?

Cheers,

Tamas
-------------------------------------
I'm really happy to see people trying to cooperate to get SegWit activated.
But I'm really unsure about the technicalities about Silbert's proposal.

If we're going to do a hardfork, it makes most sense to assist Johnson in
his spoonnet/forcenet proposals.
Just doing a simple 2MB without fixing anything else is very uninteresting,
and a hardfork without addressing replay protection seems really
unprofessional to me.
And proposing a hardfork in 4 months in the future, is completely insane.
You cannot expect a 100% of all nodes in P2P network to upgrade in 4 months.

I think it's much better to activate BIP141 ASAP, and then hardfork to 2MB
September 2018, or 2019 (together with forcenet/spoonnet).
And if not, BIP148 is gaining momentum once again so that sounds much more
interesting.

Hampus

2017-05-22 8:12 GMT+02:00 shaolinfry via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
This is a draft BIP proposal to redeploy segwit using BIP-8, from the day after the current BIP9 segwit times out.

This BIP could be deployed long before Nov 15th 2016, for example in July allowing wide deployment to begin soon. The timeout (and this useractivation) could be set to roughly a year from then. However, considering around 70% of nodes upgraded to witness capability within 5-6 months, I personally think we could reduce the time, especially considering how much people want segwit - but I understand the need for more caution in Bitcoin.

Preliminary dates are deploy within a couple months, startdate Nov 16th 2017, BIP8 timeout July 4th 2018.

<pre>
BIP: ?
Layer: Consensus (soft fork)
Title: Segwit deployment with versionbits and guaranteed lock-in
Author: Shaolin Fry <shaolinfry@protonmail.ch>
Comments-Summary: No comments yet.
Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????
Status: Draft
Type: Standards Track
Created: 2017-04-14
License: BSD-3-Clause
CC0-1.0
</pre>

==Abstract==

This document specifies a user activated soft fork for BIP141, BIP143 and BIP147 using versionbits with guaranteed lock-in.

==Motivation==

Miners have been reluctant to signal the BIP9 segwit deployment despite a large portion of the Bitcoin ecosystem who want the soft fork activated. This BIP specifies a user activated soft fork (UASF) that deploys segwit again using versionbits with guaranteed lock-in on timeout if the BIP is not already locked-in or activated by the timeout. This ensures users have sufficient time to prepare and no longer require a miner supermajority, while still allowing for an earlier miner activated soft fork (MASF).

==Reference implementation==

https://github.com/bitcoin/bitcoin/compare/master...shaolinfry:uasegwit-flagday

==Specification==

This deployment will set service bit (1<<5) as NODE_UAWITNESS.

==Deployment==

This BIP will be deployed by BIP8 with the name "uasegwit" and using bit 2.

For Bitcoin mainnet, the BIP8 starttime will be midnight 16 November 2017 UTC (Epoch timestamp 1510790400) and BIP8 timeout will be 4 July 2018 UTC (Epoch timestamp 1530662400).

For Bitcoin testnet, segwit is already activated so no deployment is specified.

==Rationale==

This BIP can be deployed well in advance of the BIP8 '''starttime''' so that the '''timeout''' will be sufficiently far in the future to allow Bitcoin users to uprgade in preparation.

The '''starttime''' of this BIP is after the BIP9 "segwit" timeout to remove compatibility issues with old nodes.

==References==

https://github.com/bitcoin/bips/blob/master/bip-0008.mediawiki

https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki

https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki

https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki

https://github.com/bitcoin/bips/blob/master/bip-0147.mediawiki

==Copyright==

This document is dual licensed as BSD 3-clause, and Creative Commons CC0 1.0 Universal.
-------------------------------------
It doesn’t matter what it does under the hood. The api could be the same.

-------------------------------------
You guys are both right that it is a different security model, with the
important distinction that it is opt-in. What I disagree with about what
you said is only that we are encouraging more risky behavior by adding
consensus rules via softfork. There are additional risks with
drivechains, but not because of how the new consensus rules would be
added (it would be the same risk as the P2SH softfork).

What's been explained to me a few times is that the
anyone-can-spend-ness of new transaction types that depend on softforked
consensus rules are exponentially less risky to the point that it is
infeasible to steal them as blocks are added to the chain that activated
the soft fork. I believe that Luke-Jr and Lopp are both very good at
explaining this and I know that Lopp has actually done some research as
to the cost of stealing these outputs. I can't remember the link but you
might find that with a google. One of them might even chime in and
explain that I'm totally wrong again!

Sorry for being a bit heated in my last response.


On 07/12/2017 02:55 PM, Tao Effect wrote:


-------------------------------------
On 13 December 2017 at 22:36, David A. Harding via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Reposting /u/BashCo's post on reddit here, for visibility:

---8<---------------------------------------------------------------








---8<---------------------------------------------------------------


I wouldn't expect people to type out µBTC. I think the best you can
hope for here is uBTC. As for saying "microbitcoins", I can virtually
guarantee that this will be abbreviated to "microbits" and/or
eventually "bits" anyway. Bits and sats.


-------------------------------------
Hi,

Thanks Thomas. The procedure described in
http://docs.electrum.org/en/latest/seedphrase.html is really what I was
looking for ! I really don't see any point of following BIP49, If possible
it would be great if you can propose an alternative to BIP49 that follows
similar structure to what is used in electrum.

I have proposed following changes to BIP32 serialization format
https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#serialization-format
to differentiate segwit xpub/xprv. Below the list of new version bytes,
resulting base58 prefix and network type:

0x042393df ,  sxpr ,   segwit mainnet private key
0x04239377 , sxpb , segwit mainnet public key
0x04222463 , stpb ,  segwit testnet public key
0x042224cc ,  stpr ,  segwit testnet private key

Let me know your thoughts

On Tue, Sep 5, 2017 at 12:12 AM, <
bitcoin-dev-request@lists.linuxfoundation.org> wrote:

-------------------------------------
Bitcoin Classic only changes the block format (by changing the rule
that they have to be 1MB or less). Miners are the only ones who make
blocks, so they are the only ones who mater when it comes to changing
block rules. Nodes, wallets and other software are not affected by
changing block rules. Unlike segwit, where *everybody* has to write
code to support the new transaction format.

Also, it doesn't matter that 75% of hashpower is made up of a dozen
people. That's how the system works, it's not a matter of opinion. If
you are just a node or just a wallet, and you want your voice to
matter, then you need to get a hold of some hashpower.


On 1/7/17, David Vorick <david.vorick@gmail.com> wrote:

-------------------------------------



And in doing so either reduce the claimable income from other transactions (miner wont do that), or require paying more non-rebateable fee than is needed to get in the block (why would the user do that?)

This is specifically addressed in the text you quoted. 


Discounted by the fact rebates would not be honored by other miners. The rebate would have to be higher than what they could get from straight fee collection, making it less profitable than doing nothing. 


Youd still have to pay the minimum fee rate of the other transactions or youd bring down the miners income. Otherwise this is nearly the same cost as the rebate fee, since they both involve explicit outputs claimed by the miner, but the rebate goes back to you. So why would you not want to do that instead?

A different way of looking at this proposal is that it creates a penalty for out of band payments. 
-------------------------------------
That's simply a 51% attack choosing to censor transactions. We could do
that today, ban all transactions that aren't approved by the PBoC.

You respond to that with a PoW hardfork, or by finding some way to prop up
/ subsidize non-censorship miners.

On Mar 13, 2017 5:59 AM, "Nick ODell via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hi,

Given today's presentation by Chris Jeffrey at the Breaking Bitcoin
conference, and the subsequent discussion around responsible disclosure
and industry practice, perhaps now would be a good time to discuss
"Bitcoin and CVEs" which has gone unanswered for 6 months.

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013751.html

To quote:

"Are there are any vulnerabilities in Bitcoin which have been fixed but
not yet publicly disclosed?  Is the following list of Bitcoin CVEs
up-to-date?

https://en.bitcoin.it/wiki/Common_Vulnerabilities_and_Exposures

There have been no new CVEs posted for almost three years, except for
CVE-2015-3641, but there appears to be no information publicly available
for that issue:

https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3641

It would be of great benefit to end users if the community of clients
and altcoins derived from Bitcoin Core could be patched for any known
vulnerabilities.

Does anyone keep track of security related bugs and patches, where the
defect severity is similar to those found on the CVE list above?  If
yes, can that list be shared with other developers?"

Best Regards,
Simon

-------------------------------------

On Fri, Jun 9, 2017, at 05:50, Olaoluwa Osuntokun wrote:

I will rephrase. The BIP reads:

lead to undesirable failure modes in applications whose safety
critically relies on responding to certain
on-chain events.

I understand that the compact  header chain is used to mitigate against
this, but I am unsure about the use 
cases and trade-offs.

For a normal wallet, the only thing I can imagine an attacker could do
is pretending a transaction did not confirm 
yet, causing nuisance.  

An application critically depending on knowing what happens on-chain 
surely is better off  downloading 
the TXIDs, providing PoW security? Gaining knowledge of incoming TXIDs
is nicely solved the payment protocol.

Are there enough use cases that critically depend on pub key hashes
being used on-chain, to make the compact header chain worth its costs? 

Regards,
Tomas

-------------------------------------
I was under the impression that RIPEMD160(SHA256(msg)) is used to turn a
PUBLIC key (msg) into a bitcoin address, so yeah, you could identify
ANOTHER (or the same, I guess - how would you know?) public key that has
the same bitcoin address if RIPEMD-160 collisions are easy, but I don't see
how that has any effect on anyone.  Maybe I'm restating what Peter wrote.
If so, confirmation would be nice.

On Sat, Feb 25, 2017 at 1:04 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------


AFAIK, client implementations such as your proposal are off-topic for this ML.
Better use bitcoin-core-dev (ML or IRC) or Github (bitcoin/bitcoin) for such proposals.



I have to agree with Luke.
And I would also extend those concerns to BIP39 plaintext paper backups.

IMO, private keys should be generated and used (signing) on a trusted, minimal and offline hardware/os. They should never leave the device over the channel used for the signing I/O. Users should have no way to view or export the private keys (expect for the seed backup). Backups should be encrypted (whoever finds the paper backup should need a second factor to decrypt) and the restore process should be footgun-safe (especially the lost-passphrase deadlock).


/jonas
-------------------------------------
On Tue, Dec 12, 2017 at 9:07 PM, Suhas Daftuar <sdaftuar@gmail.com> wrote:

Yes, that is what I was thinking last time we discussed it, just with
each header include a one byte flag that lets you express:

bit: meaning
(0) if nbits is the same as last,
(1) if timestamp is a full field or a small offset, (e.g. two bytes
defined as unsigned offset between the last time - 7200 and the new
time).
(2,3,4) if version is the same as the last distinct value .. 7th last,
or a new 32bit distinct value;
(5,6) if prev is entirely there, entirely gone, first 4 bytes
provided, or first 8 bytes provided. (if provided they override the
computed values).

That would be 7 bits in total; the 8th could be reserved or use to
signal "more headers follow" to make the encoding self-delimiting.

The downside with nbits the same as last as the optimization is that
if we ever change consensus rules to ones where difficulty management
works differently it may be the case that nbits changes every block.

Alternatively, nbits could get a differential encoding that could be
opted into for small differences-- though I haven't thought much about
it to see if a one byte difference would be that useful (e.g. can bch
differences usually be expressed with one byte?)

I'm kind of dubious of the consensus layer anti-dos separation:  nbits
minimum is so low compared to the speed of a mining device, virtually
any attack that you might do with invalid headers could still be done
with headers at the minimum difficulty. But I'm fully willing to
accept that simpler is better...




-------------------------------------
Hi

I agree that unconfirmed transactions are incredibly important, but not
over SPV against random peers.

If you offer users/merchants a feature (SPV 0-conf against random
peers), that is fundamentally insecure, it will – sooner or later – lead
to some large scale fiasco, hurting Bitcoins reputation and trust from
merchants.

Merchants using and trusting 0-conf SPV transactions (retrieved from
random peers) is something we should **really eliminate** through
education and by offering different solution.

There are plenty, more sane options. If you can't run your own full-node
as a merchant (trivial), maybe co-use a wallet-service with centralized
verification (maybe use two of them), I guess Copay would be one of
those wallets (as an example). Use them in watch-only mode.

For end-users SPV software, I think it would be recommended to...
... disable unconfirmed transactions during SPV against random peers
... enable unconfirmed transactions when using SPV against a trusted
peer with preshared keys after BIP150
... if unconfirmed transactions are disabled, show how it can be enabled
(how to run a full-node [in a box, etc.])
... educate, inform users that a transaction with no confirmation can be
"stopped" or "redirected" any time, also inform about the risks during
low-conf phase (1-5).

I though see the point that it's nice to make use of the "incoming
funds..." feature in SPV wallets. But – for the sake of stability and
(risk-)scaling – we may want to recommend to scarify this feature and –
in the same turn – to use privacy-preserving BFD's.

</jonas>


-------------------------------------
BIP 177 is NOT assigned. Do not self-assign BIP numbers!

Please read BIP 2:

    https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki

Luke


On Sunday 24 December 2017 2:57:38 AM Damian Williamson via bitcoin-dev wrote:

-------------------------------------
Thanks Gregory - to be clear should Native P2WPKH scripts only appear in redeem scripts?  From reading the various BIPs it had seemed like Native P2WPKH and Native P2WSH were also valid and identifiable if they were encoded in TxOuts.  The theoretical use case for this would be saving bytes in Txes with many outputs.

-----Original Message-----
From: Gregory Maxwell [mailto:gmaxwell@gmail.com] 
Sent: Monday, August 28, 2017 10:04 AM
To: Alex Nagy <optimiz3@hotmail.com>; Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] P2WPKH Scripts, P2PKH Addresses, and Uncompressed Public Keys

On Mon, Aug 28, 2017 at 3:29 PM, Alex Nagy via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

Absolutely not. You can only pay people to a script pubkey that they have specified.

Trying to construct some alternative one that they didn't specify but in theory could spend would be like "paying someone" by putting a cheque in a locked safe labeled "danger radioactive" that you quietly bury in their back yard.  Or taking the payment envelope they gave you stuffing it with cash after changing the destination name to pig latin and hiding it in the nook of a tree they once climbed as a child.

There have been technical reasons why some wallets would sometimes display some outputs they didn't generate but could spend, but these cases are flaws-- they're not generic for all cases they could in theory spend, and mostly exist because durability to backup recovery makes it impossible for it to tell what it did or didn't issue.

So regardless of your query about uncompressed keys, you cannot do what you described: Wallets will not see the payment and may have no mechanism to recover it even if you tell the recipient what you've done. And yes, the use of an uncompressed yet could later render it unspendable.
-------------------------------------
On Sat, Jan 28, 2017 at 07:43:48PM +0000, Luke Dashjr via bitcoin-dev wrote:

So, in that particular type of case, the ZK proof may show that the block
itself is valid and follows all the rules; there'd be no need to get the block
data to know that.

The issue here is other miners being able to mine. Exactly what happens here
depends on the exact construction of the ZK proofs, but at best the missing
data will mean that part of the UTXO state can no longer be updated by other
miners, and thus they can't mine all transactions; at worst they'd be
completely preventing from mining at all.

This is why part of the economic pressure that users exert on miners is
subverted by SPV/lite-clients: users that can transact without sufficient
blockchain data to allow others to mine aren't exerting pressure on miners to
allow other miners to mine - particularly new entrants to mining. In that
respect, ZK proofs are in fact quite harmful to the security of the system if
applied naively.

Equally, I'll point out that if ZK proofs can be made sufficiently powerful to
do all the above, genuinely scalable sharded systems like my own Treechains are
far easier to implement, changing the discussion entirely. Currently it is far
from proven that ZK proofs can in fact accomplish this; I hear that Zcash will
soon have to upgrade their ZK-SNARK scheme due to advances in cryptographic
analysis that may result in a full system break in the near future. We really
don't want to be depending on that technology for Bitcoin's security until
events like that become much less common.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
I think it might be important that the mandatory commitment expire as in 
Greg's proposal - when we do eventually hardfork, it will be simpler to do in 
a safe manner if such a commitment in the fake "old block" is not required.

I don't like your proposal because it allows ASICBoost. ASICBoost effectively 
makes SHA2 semi-ASIC-resistant. ASIC-resistance raises the barrier of entry to 
new mining chip manufacturers, and gives a larger advantage to the miners able 
to make use of it. Instead, IMO we should fix the vulnerability exploited by 
ASICBoost entirely to keep SHA2 as ASIC-friendly as possible - or change the 
PoW to an algorithm that is more ASIC-friendly.

That being said, I don't think I would oppose the proposal if it gained 
notably better support than Segwit currently has (as yet another compromise), 
and the above concerns were addressed (eg, Bitfury and Canaan state they can 
compete using ASICBoost and the patents are licensed freely to everyone).

Luke


On Saturday, April 08, 2017 12:05:16 AM Jimmy Song via bitcoin-dev wrote:

-------------------------------------
On Mon, Jul 17, 2017 at 02:49:22PM -0400, Alex Morcos via bitcoin-dev wrote:

Agreed!

A closely related example is my own Treechains work, which got a bunch of
excitement when I first published the idea. But would I have wanted it on a
roadmap? Hell no: sure enough, as it got more peer review others (and myself!)
found that it was going to be a harder than it initially looked to actually get
into production.

Drivechains is definitely in that situation right now.

Also don't forget that proper security peer review takes a *lot* of work. I
myself have a todo list item to respond to Paul's post on Drivechains, but I
need to spend a few days to do that and just haven't had the time (not to
mention that no-one is paying me to do general Bitcoin dev work right now).

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
I also think that the UASF is a good idea. Hashrate follows coin price. If
the UASF has the higher coin price, the other chain will be annihilated. If
the UASF has a lower coin price, the user activated chain can still exist
(though their coins can be trivially stolen on the majority chain).

The success of the UASF depends entirely on the price. And actually, the
price is easy to manipulate. If you, as an economically active full node,
refuse to acknowledge the old chain and demand that incoming coins arrive
over the UASF chain. In doing so, you drive down the utility of the old
chain and drive up the utility of the new chain. This ultimately impacts
the price.

I think it would be pretty easy to get high confidence of the success of a
UASF. Basically you need all the major economic hubs to agree to upgrade
and then exclusively accept UASF coins. I don't have a comprehensive list,
but if we could sign on 75% of the major exchanges and payment processors,
and get 75% of the wallets to upgrade, then the UASF would be very likely
to successfully obliterate the old rules, as miners would be unable to sell
their coins or pay their bills by stubbornly sticking to the old chain.
It's less risky than a hard fork by far, because there is zero risk of coin
split if the UASF has majority hashrate, which will follow majority
economic value.

A serious proposal I think would get all the code ready and merged, but
without setting a flag day. Then we would get signatures from the major
institutions promising to use the software and saying that they are ready
for a flag day. After that, you release a patch with a flag day 12 months
in the future. People can upgrade immediately, and have a full year to
transition.

That gives tons of time for people to upgrade, and tons of confidence that
the UASF will end up as the majority chain.

If we cannot get enough major exchanges, payment processors, and other
economic hubs to upgrade,  the flag day should remain upset, as the risk of
coin split will be non-zero.

I would suggest that a carefully executed UASF is much riskier than a soft
fork, but far, far less risky than a hard fork.
-------------------------------------
Miners blocking SegWit due to ASICBOOST requirements also means they 
would block future deployment of committed bloom filters.

On 2017-04-06 00:37, Gregory Maxwell via bitcoin-dev wrote:

-------------------------------------
Regarding #1, I agree with Johnson Lau and others who have responded since
then—this proposal is not appropriate and should not be adopted for the
following reasons:

1. Miners will view it as way too little, delivered way too late. And as
soon as you say 300kb blocks, you've lost them all.

2. "Spam" - You're very fixated on this concept of spam transactions, but
the transactions that you deem as spam are legitimate, fee-paying
transactions. They're not a problem for miners. It's only a problem to you
as you've arbitrarily decided some transactions are legit and some are not.
It's an imaginary problem and we should focus on designs that solve real
problems instead.

Also, even if you changed the max size to 300kb, transactions that you (and
as far as I can tell, only you) consider spam will still be in there!
They'll just be paying a ridiculous fee along with everyone else.

3. 17% per year growth rate - This is making the assumption that the
current 1MB limit is already at the upper limit supportable by the network.
This isn't even remotely true, and starting this rate at the current limit
would cause the system to lag far behind the actual capability of the
network for no reason.

4. Nodes - Individuals have no incentive to run full nodes and we've
already passed the time where it makes any sense for them to do so.
Therefore restricting the blockchain size in an attempt to keep individuals
running nodes is futile at best and likely very damaging. Miners and
businesses using Bitcoin do have an incentive to run nodes and over the
years we've seen a migration of nodes from weak hands (individuals) to
strong hands (businesses).

Overall, this proposal would hamstring Bitcoin Core and would drive miners
towards Unlimited.

- t.k.

On Thu, Jan 26, 2017 at 8:06 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Wed, Jun 28, 2017 at 12:37 AM, Chris Stewart via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This is an absurd restriction-- I hope it was not your intent to
directly ban P2Pool and probably any other form of decentralized or
less centralized mining pooling... but thats what doing that does.


This removes important flexibility that was intentionally preserved.
What happens when an additional commitment is needed for bitcoin?
must some sidechain be irreparably destroyed? looks like it in  your
proposal.


And what happens if index 1 isn't present? if index 35 is used must
there be 34 dummy outputs?


This is not monotone/reorg safe. It means that the output coins (if
any) are not equivalently fungible with other bitcoins (for, e.g. 100
blocks) because if there is a reorg this transaction cannot be
restored to the chain.  It's also impure and not compatible with
caching, which would be unfortunate and slow block propagation.

-------------------------------------
This seems to be a serious security problem.  Would it be possible to have
a flag-day softfork included in Bitcoin Core as soon as 0.14.1? I think that a trigger
3-6 months from release should be sufficient for enough of the economy to upgrade,
given the severity of the issue.

BIP 141 says that the the commitment is optional if there are no SegWit transactions in
the block,  so will today's SegWit-ready miners always produce it even when optional
according to BIP 141, as required by this softfork?

On Wed, Apr 5, 2017, at 04:37 PM, Gregory Maxwell via bitcoin-dev wrote:

-------------------------------------
I will repeat that Drivechain can sometimes be confusing because it is
different things to different people.

Here is my attempt to break it down into different modes:

[DC#0] -- Someone who does not upgrade their Bitcoin software (and is
running, say, 0.13). However, they experience the effects of the new
rules which miners add (as per the soft fork[s] to add drivechain
functionality and individual drivechains).
[DC#1] -- Someone who always upgrades to the latest version of the
Bitcoin software, but otherwise has no interest in running/using sidechains.
[DC#2] -- Someone who upgrades to the latest Bitcoin version, and
decides to also become a full node of one or more sidechains, but who
ever actually uses the sidechains.
[DC#3] -- Someone who upgrades their software, runs sidechain full
nodes, and actively moves money to and from these.

Greg is still conflating modes [DC#1] and [DC#3]. Specifically, he
equivocates on the team "steal", using it to mean two different things:
[a] spending an invalid transaction, and [b] a withdrawal that would not
match the report given by a sidechain node.

The two are quite different, see my comments below:


On 7/12/2017 9:15 PM, Tao Effect wrote:

Greg quoted a passage that contained an older parameter estimate of "a
few days", and I thought it would be helpful and informative if I
clarified that the parameter estimate had been updated to a new (more
secure) value.

In point of fact, he is wrong, because nodes do the counting. When
miners find a block, they can choose to move the counter up, down, or
not at all. But nodes do the counting.



As I stated above, the document is mostly accurate. There is no other
more up to date version.




Above, Greg omitted his original question. For reference, it was:


The answer is that both DC and P2SH use transactions which are 'always
valid' to some group of people (un-upgraded users) but which are
sometimes invalid to new users. So the only difference would be for
[DC#0] vs other versions, but this difference is trivial as miners will
censor invalid txns.

It is your classic soft fork situation.



Again, keep in mind that Greg continually conflates two different things:
1. Whether the soft fork rules have been followed, and
2. Whether the WT^ submitted by a majority hashrate matches the one
calculated by sidechain nodes.

The first case is exactly equal to P2SH. Just as old nodes accept every
P2SH transaction, so too will [DC#0] users accept every WT^ transaction.

In the second case, it so happens that [DC#1], [DC#2], and [DC#3] would
also accept any WT^ *that followed the Drivechain rules*, even if they
did not like the outcome (because the outcome in question was
arbitrarily designated as a "theft" of funds -- again, see the second
case in the list above). In this way, it is exactly similar to P2SH
because nodes will accept *any* p2sh txn **that follows the p2sh
rules**, even if they don't "like" the specific script contained within
(for example, because it is a theft of "their" BitFinex funds, or a
donation to a political candidate they dislike, etc).



Greg is, of course, not entitled to an answer to irrelevant questions --
just as he would not be entitled to an answer if he asked for my
favorite color or my home address. And such answers would needlessly
consume the mailing list's scarce time.



It is clear to me that, if we are not clear on the basics first, we
cannot hope to tackle anything intermediate. We will come back to this
after clearing up soft fork part.



In DC, all upgraded nodes will reject invalid DC transactions, period.



The [DC#2] and [DC#3] nodes would do exactly what the [DC#0] and [DC#1]
nodes do. This is what I mean by "every withdrawal is valid".



In DC, miners cannot steal funds, because all full nodes have a fully
automatic enforcement policy.

However, DC *allows* users to choose to place some of their BTC at the
relative mercy of the miners in creative ways, if they wish (as does
P2SH -- someone could write a script which donates funds to miners, and
then fund it... "paying" to that script). This is another example of
conflating [DC#1] and [DC#3].

Paul




-------------------------------------
On Sat, May 13, 2017 at 12:49:33AM +0000, Luke Dashjr wrote:

I'm not arguing that it changes that; I'm arguing that it further confuses the
situation.


I think you're assuming that the users paying for soft-fork signalling will
represent an economic majority; that's not necessarily the case.

For example, if miners decide there's no downside to false signalling, they may
take the extra fees provided by 1% of the users paying to signal a fork, while
the other 99% don't participate, resulting in a situation where we have blocks
violating the nVersion protocol, and an unknown % of that 99% rejecting those
blocks. At best that'd be no worse than a UASF, and at wost you're wrecked the
validity of the nVersion "gentlemans agreement"


Just read it: you have ten separate lines of dense English text describing
something that could have been specified instead by ten lines of much more
formally defined C++. In particular, note how many of those lines of English
text refer to C++ code anyway, like the sentence "minimal-length 40-bit
CScriptNum"

I don't want to have to learn another language - formally defined English that
still fails to be formally defined - just to read Bitcoin's specification.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
This [1] article says the current discount prevents witness spam. Witness
spam is free space in the witness part of the block that can be filled by
miners to create bigger blocks with almost no cost for the benefit a
cluster of miners with low latency, increasing centralization.

The 75% discount does not prevent it, but on the contrary leaves a lot of
extra witness space for spam.

If the maximum block weight is set to 2.7M, each byte of non-witness block
costs 1.7, and each byte of witness costs 1, then a normal filled block
would be 2.7M bytes (1.7+1), and there will be no need to create ever a 4
Mbyte block. The worst case would be the average case, and the transaction
rate would be the maximum possible.

The current 75% discount can only achieve more transactions per second if
the type of transactions change. Therefore the current 75% discount only
makes the block size worst case worse (4 Mbytes when it should be 2.7
Mbytes).

80% of all inputs/outputs are P2PKH. The only way to make use of the extra
witness
space If most P2PKH transactions are replaced by multisigs (typically for
LN).

So it seems the 75% discount has been chosen with the idea that in the
future the current transaction pattern will shift towards multisigs. This
is not a bad idea, as it's the only direction Bitcoin can scale without a
HF.
But it's a bad idea if we end up doing, for example, a 2X blocksize
increase HF in the future. In that case it's much better to use a 50%
witness discount, and do not make scaling risky by making the worse case
block size 8 Mbytes, when it could have been 2*2.7=5.4 Mbytes.

I've uploaded the code here:
https://github.com/SergioDemianLerner/SegwitStats

 [1] https://segwit.org/why-a-discount-factor-of-4-why-not-
2-or-8-bbcebe91721e.


On Mon, May 8, 2017 at 8:47 PM, Alphonse Pace via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I'd like to know this too.  Is it just that a 4MB blockmaxweight would
theoretically allow ~4MB blocks (if ~all witness data), which is too big?
Or is there a more subtle reason we still need blockmaxsize after a HF?


On May 30, 2017 9:28 AM, "Jorge Timón via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Why not simply remove the (redundant after sw activation) 1 mb size
limit check and increasing the weight limit without changing the
discount or having 2 limits?


On Wed, May 24, 2017 at 1:07 AM, Erik Aronesty via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
timing.
bandwidth/CPU/RAM
blocks
even
into
and
1
community.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
I want to resurrect this thread from August/September because it seems like
a significant improvement for light clients at very little cost. From the
mailing list, it seems like this got stalled in determining how many more
bytes could be save in addition to the prev_block.

The ideas I've gathered from Greg Maxwell's forwarded email are:

1. Omit nBits altogether and have the receiving node determine it from
chain context.
2. Include nBits only on headers with a height that is a multiple of 2016
since it does not change in between.
3. Compress nTime to two bytes by using the bounds on allowed values from
the consensus rules.

I propose just moving ahead with only the exclusion of the prev_block, as
IMO the other savings are not worth the added complexity.

Firstly, I don't like the idea of making the net header encoding dependent
on the specific header validation rules that Bitcoin uses (eg. the fact
that difficulty is only recalculated every 2016 blocks). This would be
coupling together the two layers, breaking net compatibility for some alts,
and possibly making consensus rule changes even more difficult for a
savings with insufficient benefit. So if you buy that argument, I'm not in
favor of #2 or #3.

Option 1 is still viable, though it has some downsides. The implementation
leaks into the validation code, whereas calculating prev_block can occur
just at the net layer (see implementation below). Also, nodes would now be
*required* to sync the header chain from the genesis block, whereas they
had the option of starting from some checkpoint before.

So switching gears, I'd like to ask what the best way to actually implement
this change is. Solutions I can think of are:

1. New headers command name like "cmpctheaders" or "headersv2".
2. Change serialization of existing headers message in a new protocol
version.
3. Change serialization of existing headers message with new service bit.

I wrote up some proof-of-concept implementations in Core a) just omitting
prev_block
<https://github.com/bitcoin/bitcoin/compare/master...jimpo:compact-headers>
and b) omitting nBits as well
<https://github.com/bitcoin/bitcoin/compare/master...jimpo:compact-headers-difficulty>.
If people think a) is reasonable, I'll write up a BIP.


-------------------------------------
Math should decide the max block size, not humans (miners in this
case). The goal of Block75 is to manage the max block size without any
human intervention.

Under Block75, miners don't have any direct control but could still choose
to mine smaller blocks (same as now), though doing so would cost them the
fees from transactions they didn't include in their blocks.

A maximum block size is necessary to prevent a single nefarious miner from
creating a ridiculously large block which would break the network.

- t.k.

On Mon, Jan 2, 2017 at 2:01 PM, Tom Zander <tomz@freedommail.ch> wrote:

-------------------------------------
Op 28 sep. 2017, om 17:13 heeft Andreas Schildbach via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> het volgende geschreven:

True and the more complicated fields, like a digital signature, are optional. Are you suggesting BIP-70 payment requests should be rendered with bech32? How long would those be if it's just the address and expiration date?


Do tools that generate BIP-70 payment requests generate addresses themselves or are those input manually by a user? In the former case, I assume it could avoid setting the optional expiration date?

Is it not allowed to scan the date even if it then sets the expires field to the same (redundant) value?

Sjors
-------------------------------------
We should distinguish collision resistance from 2nd pre-image resistance, in general.

As previously written, we should care both hash output length and algorithm itself. The weakness of SHA-0 (preliminary version of SHA-1) was reported in 2004, then many research on the structure of SHA-1 were conducted. In the case of SHA-2, it is harder than SHA-1 to find collisions.

Existing security consideration and evaluation criteria were extensively discussed in the NIST SHA-3 competition. Please see the following sites.

https://ehash.iaik.tugraz.at/wiki/The_SHA-3_Zoo
https://ehash.iaik.tugraz.at/wiki/Cryptanalysis_Categories

We need similar analysis on RIPEMD160 and impacts of attacks on (RIPEMD160(SHA2(msg)). 

We can also refer the security assumption of hash chain in Asiacrypt 2004 Paper. 
https://home.cyber.ee/~ahtbu/timestampsec.pdf

In the discussion of SHA3 competition, we choose another hash design structure, so called "sponge structure." This leads diversity of design principles of hash function and gives resilience even when one hash design structure becomes vulnerable. As Peter Todd wrote, discussion on design structure and algorithm is important. Discussions on all of algorithm, output length and security requirements are needed.

At some future moment, we should think about transition of underlying hash functions. I’m working on this subject and will present an idea at IEEE S&B.

Shin’ichiro Matsuo




-------------------------------------
Script versions makes this no longer a hard-fork to do. The script version would implicitly encode which jets are optimized, and what their optimized cost is.


-------------------------------------
Haha, no. Because you "burned" the coins.

On Oct 10, 2017 1:20 AM, "Tao Effect" <contact@taoeffect.com> wrote:

-------------------------------------
This is, by far, the safest way for miners to quickly defend against a
chain split, much better than a -bip148 option.   This allows miners to
defend themselves, with very little risk, since the defense is only
activated if the majority of miners do so. I would move for a very rapid
deployment.   Only miners would need to upgrade.   Regular users would not
have to concern themselves with this release.

On Wed, Jun 7, 2017 at 6:13 AM, James Hilliard via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Greg,

I would summarize your email as stating that: you regret writing the
first email, and regret the fact that it became a roadmap that everyone
signed. And that therefore it is obviously a concept NACK from you.

( That's pretty surprising to me, and I would expect others to find it
surprising as well. And I wonder whether you think we should take the
old one *down*, and why you would allow (?) so many other people to sign
it, etc. But I am not willing to press the issue. Some of your other
comments I also find confusing but there is little to be gained in
clarifying them. )

Generally, I still think that the roadmap was a helpful communication
device, which did more good than harm. And I am interested in hearing
what other people think.

Separately, and very important to me, is that you feel that there are
unresolved objections to drivechain's security model, which you decline
to share with me and/or the list. So I would hope that you instead
choose to share your thoughts (as is, presumably, the purpose of this list).

I will also respond to this:


Let me try to explain my point of view. I did speak to several people,
in addition to the two names that I privately volunteered to you when
you asked me in a personal email earlier today. From my point of view
you had done no research (you failed to uncover any additional names),
used the information I volunteered to you against me (in the form of
false characterizations of negligent email writing!), and you also
suggested that, other than yourself and a few others, no one is
qualified even to write a first draft of a summary of present day
activities. This response is typical of the hostile review environment
which has existed in Bitcoin for years (I am more than used to it). If
instead of writing the first draft, I had written nothing, I would be
accused of being the ideas guy and/or "not contributing". You also
(rather rudely), put me in an awkward position, as the people who I
*did* ask now almost certainly prefer that I not reveal their names
(yet, a low name count is held as a strike against my competence).

Such are the perils of posting to bitcoin-dev! Let all be warned! : )

Paul




On 7/11/2017 8:07 PM, Gregory Maxwell wrote:




-------------------------------------
There have been a number of similar (identical?) proposals over the years, some were discussed in these threads:
https://bitcointalk.org/index.php?topic=56226.0
https://bitcointalk.org/index.php?topic=505.0
https://bitcointalk.org/index.php?topic=473.0
https://bitcointalk.org/index.php?topic=52859.0
https://bitcointalk.org/index.php?topic=12376.0
https://bitcointalk.org/index.php?topic=74559.15


-------------------------------------
Credit card reversals involve an escrow agent with control over the entire network and with a strong interest in preserving the network. A better analogy would be blind acceptance of any slip of paper under the assumption that it is sufficient currency. It may or may not be so, but you are on your own in either case.

e

-------------------------------------
On Wed, Apr 05, 2017 at 09:37:45PM +0000, Gregory Maxwell via bitcoin-dev wrote:

While I'm in favour of blocking covert usage of ASICBOOST, there's every reason
to block non-covert usage of it as well. In a low margin business like mining,
the advatange it gives is enormous - quite possibly 10x your profit margin -
and given that barrier free access to being able to purchase ASICs is already
an archilles heal for Bitcoin there is every reason to eliminate this legal
vulnerability. Additionally, it's a technical vulnerability as well: we want
getting into the ASIC manufacturing and design business to have as low barriers
to entry as is feasible, and the ASICBOOST exploit significantly increases the
minimum capital requirements to do so.

Remember that the whole purpose of PoW is to destroy value on a level playing
field. Anything that inhibits a level playing field is an exploit. While this
isn't standard crypto - we can't fix every exploit completely - since we're
going to do a technical change to partially mitigate the ASCIBOOST exploit
there is every reason to fully mitigate it.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
I started writing this
https://gist.github.com/Ayms/aab6f8e08fef0792ab3448f542a826bf some time
ago, but stopped since I was under the impression that this was of very
little interest for the Bitcoin community

It's not final and finished at all, but since I wrote it and don't have
plans right now to pursue it, I placed it in a gist and publish the
link, probably not everything is correct and this does not cover
everything but it can maybe give some ideas (which are for some the
combination of concepts from former/other projects) that could be
reused, addressing:

- incentive to run full nodes

- make sure that they are indeed full nodes

- make sure that they participate to the network and are efficient enough

- make sure that they don't collude in pools to get the rewards and are
independent

- set up quickly a full node (incremental torrent-like download)

As this was written this was supposed to add some modifications to the
bitcoin protocol but I don't think that's necessarily a good idea, most
likely this can be handled via sidechains and/or external systems


Le 13/02/2017  15:48, Sergio Demian Lerner via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
I looked at the discussions about the block size and about Luke-Jr's
proposal on Reddit and Bitcointalk. From what I observed of all of the
discussions is that few users are in favor of the status quo, and even
fewer are in favor of decreasing the block size. The majority of users
favored Segwit because it was a block size increase (that was a commonly
used reason in support of it and in arguments about increasing the block
size).

Discussions about Luke-Jr's proposal indicated that many users disagreed
with the decrease in block size and the time that it took to increase
again to 1 MB. There was not only disagreement but explicit ridicule and
mocking of that aspect of the proposal.


On 2/6/2017 3:28 PM, Thomas Kerin wrote:
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On 03/24/2017 04:27 PM, Emin Gün Sirer wrote:

I know this is a delayed reply.

Without intending to revive an older thread, my intentions are to clarify
what I have meant in my original post just in case anyone misinterprets 
where I said

"For example would be something like this:
If block = (empty OR  <%75 of mempool) THEN discard
This threshold just an example."

I should have clarified that with this idea blocks would not be rejected if 
does not match what that nodes have in their mempool, because as you have said,
there is no consensus on the contents of mempool and the mempool will vary from
node to node.

Instead what I have meant is that with this idea, nodes would only reject blocks if
they are empty or less than a determined percentage when compared to what is in mempool.

While this specific defense proposal I posted may or may not be a good idea, was only 
throwing this idea out there to create discussion on possible defenses against an empty
or near empty block attack.



- --
Cannon
PGP Fingerprint: 2BB5 15CD 66E7 4E28 45DC 6494 A5A2 2879 3F06 E832 
Email: cannon@cannon-ciota.info

NOTICE: ALL EMAIL CORRESPONDENCE NOT SIGNED/ENCRYPTED WITH PGP SHOULD 
BE CONSIDERED POTENTIALLY FORGED, AND NOT PRIVATE. 
If this matters to you, use PGP.
-----BEGIN PGP SIGNATURE-----

iQIcBAEBCgAGBQJY8DIZAAoJEAYDai9lH2mwZKYP/jNJjyTeE09+IlsGolPV3Vp+
suJmUK26y8IbEzGxa8eVoX3w7407VNzqeT0jF8vK7oy97EPgszoiutbzYanKYH27
Rpck+FdW/Q5o6jqw59swX+KEvVao52ETPX3kV8ae5uA2txOBnn6C0qZbM5OxPVLN
IHr7E0+bn9BQVuTzhep1wNWi4cDzyeIjYfRGArBTkuSBKxFtbPmTMLa67qsBGKVu
JcGYm6rdDO0iVAR9od/Is9b+3gTW49x/3jBEdg7iCHc8KuGOilZaHfyU6xjt3fPo
L2lxXxUuobFD68/f4ervFVMpAPpmPaS/MEkHMIhJex3szdlSe/WZsQm+2/j799Rg
Ba62pMOYvSR43WwlwX8eySUlVsPtJNtObKnRvDBOmOICgsZ3T9tHKjI+9IPVi9Ib
s7yBBA1LFw4+c8wirzu1aaeDroJ3icqfU+tRe+nadQN1PMepk6sBUMu13bm8B3E3
R8oo+jFZRRvJmx7HDDlJX9GHri8hktCNm/gtt0ksWwEgAQHixukmKoDVssAmsiZ4
BbiWIA3ULciSKM782zDH7/GvDBbOurtV8TeubnV7DDARIA86COwuGjjk30Ltf3ia
5gnFIicLkmdRMh4AU0jvvEpxrHWFFJmreoR+jnAXHMBGoA6ExVaqR2VQzcpb5SIb
sqe/5499BqvJqS4ZFn7f
=q+nx
-----END PGP SIGNATURE-----

-------------------------------------
Sign-to-contract enables some interesting protocols, none of which are in wide use as far as I’m aware. But if they were (and arguably this is an area that should be more developed), then SPV nodes validating these protocols will need access to witness data. If a node is performing IBD with assumevalid set to true, and is also intending to prune history, then there’s no reason to fetch those witnesses as far as I’m aware. But it would be a great disservice to the network for nodes intending to serve SPV clients to prune this portion of the block history. 


-------------------------------------
On Fri, 2017-02-24 at 16:18 +0100, Aymeric Vitte via bitcoin-dev wrote:
What prevents the attacker to provide different past files when talking
to parties who are still in the initial state?

Then the question is: knowing the hash state, is it as easy to find a
With the original usage of the hash function, the hash state is always
the initial state. Now that the attacker has some control over the hash
state even. In other words, if the original use of the hash function
was vulnerable, then your scheme is vulnerable for the initial state.

Concrete attack: If you can find x != y with H(x) = H(y), then you can
also find m, x != y, with H(m||x) = H(m||y), just by setting m = "". 

Not sure if this is the right place to discuss that issue though...

Best,
Tim

-------------------------------------
BIP 140 looks like it solves Tx Malleability with least impact on current
practices. It is still a soft fork though.

Finally, if we were to create an alternative cyptocurrency similar to
Bitcoin, a Normalized Tx ID approach would be a better choice if I get it
right!
ᐧ

On Mon, Nov 20, 2017 at 11:15 PM, Johnson Lau <jl2012@xbt.hk> wrote:



-- 
Dr. Praveen Baratam

about.me <http://about.me/praveen.baratam>
-------------------------------------

This is not needed, if segwit is locked in by aug 1 (with or without
bip91), no split will happen even if segwit is not active yet.
So the hashrate majority could avoid the split that way (or adopting bip148).

But it doesn't seem like they are planning to do this (with or without
bip91), the last thing I've heard, it's they will wait until
"immediately" before they signal sw (but there must be some language
barrier here, perhaps "immediately" and "inmediatamente" are false
friends). The reason why they will wait until "immediately" instead of
just starting to signal sw today, it's still unclear to me.

The other way to prevent the split is if bip148 users abort bip148
deployment, but unfortunately that seems increasingly unlikely.

-------------------------------------
Could this risk mitigation measure be an optional flag?  And if so,
could it+BIP91 signal on a different bit than bit4?

The reason being, if for some reason the segwit2x activation cannot
take place in time, it would be preferable for miners to have a more
standard approach to activation that requires stronger consensus and
may be more forgiving than BIP148.  If the segwit2x activation is on
time to cooperate with BIP148, it could be signaled through the
non-bit4 approach and everything could go smoothly.  Thoughts on that
idea?  It may add more complexity and more developer time, but may
also address your concerns among others.


The concern I'm raising is more about the psychology of giving BIP148
a sense of safety that may not be valid.  Without several more steps,
BIP148 is definitely on track to be a risky chainsplit, and without
segwit2x it will almost certainly be a small minority chain. (Unless
the segwit2x compromise falls apart before then, and even in that case
it is likely to be a minority chain)

Jared


On Wed, Jun 7, 2017 at 2:42 PM, James Hilliard
<james.hilliard1@gmail.com> wrote:

-------------------------------------

Envisioning it in my head and trying to read the white paper, it
sounds like the process for a non-stratum mining farm would be this:

On primary server with sufficient memory, calculate ~4k-6k valid
left-side merkle tree roots and ~4k-6k right-side merkle tree roots.
Then try hashing every left-side option with every right-side option.
I'm not sure if modern asic chips are sufficiently generic that they
can also sha256-double-hash those combinations, but it seems logical
to assume that the permutations of those hashes could be computed on
an asic, perhaps via additional hardware installed on the server.
Hashing these is easier if there are fewer steps, i.e., fewer
transactions.

Out of this will come N(2-16 at most, higher not needed) colliding
merkle roots where the last 4 bytes are identical.  Those N different
merkle combinations are what can be used on the actual mining devices,
and those are all that needs to be sent for the optimization to work.

On the actual mining device, what is done is to take the identical
(collision) right 4 bytes of the merkle root and hash it with one
nonce value.  Since you have N(assume 8) inputs that all work with the
same value, calculating this single hash of once nonce is equivalent
to calculating 8 nonce hashes during the normal process, and this step
is 1/4th of the normal hashing process.  This hash(or mid-value?) is
then sent to 8 different cores which complete the remaining 3 hash
steps with each given collision value.  Then you increment the nonce
once and start over.

This works out to a savings of (assuming compressor and expander steps
of SHA2 require computationally the same amount of time) 25% * (7 / 8)
where N=8.

Greg, or someone else, can you confirm that this is the right
understanding of the approach?


As above, it doesn't require such a massive change.  They just need to
retrieve N different sets of work from the central server instead of 1
set of work.  The central server itself might need substantial
bandwidth if it farmed out the merkle-root hashing computational space
to miners.  Greg, is that what you're assuming they are doing?  Now
that I think about it, even that situation could be improved.  Suppose
you have N miners who can do either a merkle-tree combinatoric
double-sha or a block-nonce double-sha.  The central server calculates
the left and right merkle treeset to be combined and also assigns each
miner each a unique workspace within those combinatorics.  The miners
compute each hash in their workspace and shard the results within
themselves according to the last 16 bits.  Each miner then needs only
the memory for 1/Nth of the workspace, and can report back to the
central server only the highest number of collisions it has found
until the central server is satisfied and returns the miners to normal
(collided) mining.

Seems quite workable in a large mining farm to me, and would allow the
collisions to be found very, very quickly.

That said, it strikes me that there may be some statistical method by
which we can isolate which pools seem to have used this approach
against the background noise of other pools.  Hmm...

Jared



On Wed, Apr 5, 2017 at 7:10 PM, Jonathan Toomim via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On 21/07/17 03:59 PM, Lucas Clemente Vella via bitcoin-dev wrote:

I think if we wanted to burn lost/stale coins a better approach would be
returning them to miner's as a fee - there will always be lost coins and
miners will be able to get that additional revenue stream as the mining
reward halves. I also don't think we need to worry about doing a gradual
value loss neither, we should just put a limit on UTXO age in block
count (actually I would round it up to 210k blocks as explained below...).


So lets say for example we decide to keep 5 210k blocks "generations"
(that's over 15 years), then on the first block of the 6th generation
all UTXO's from the 1st generation are invalidated and returned into a
"pool".

Given these (values in satoshis):

Pool "P" (invalided UTXO minus total value reclaimed since last halving)
Leftover blocks "B" (210,000 minus blocks mined since last halving)

Then every mined block can reclaim FLOOR(P/B) satoshi in addition to
miner's reward and tx fees.

If the last block of a generation does not get the remainder of the pool
(FLOOR(P/1) == P) it should get carried over.


This would ensure we can clear old blocks after a few generations and
that burnt/lost coins eventually get back in circulation. Also it would
reduce the reliance of miners on actual TX fees.


To avoid excessive miner reward initially, for the first few iterations
the value of B could be increased (I haven't calculated the UTXO size of
the first 210k blocks but it could be excessively high...) or the value
each block can reclaim could be caped (so we would reclaim at an
artificial capacity until the pool depletes...).


Regards,

--
Thomas


-------------------------------------
The last few bytes can be generated to be the same also.

On Mon, 30 Oct 2017, 14:20 Ricardo Filipe via bitcoin-dev, <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I agree with you here, Erik. Greg's standard answer doesn’t apply to your 
suggestion.
I think he was a bit too trigger happy because we have seen a lot of similar 
suggestions that have the Sybill issue he mentioned.


On Thursday, 4 May 2017 15:15:02 CEST Erik Aronesty via bitcoin-dev wrote:

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
Hello,

Here is a BIP that matches the reference code that the Segwit2x group has
built and published a week ago.

This BIP and code satisfies the requests of a large part of the Bitcoin
community for a moderate increase in the Bitcoin non-witness block space
coupled with the activation of Segwit.

You can find the BIP draft in the following link:

https://github.com/SergioDemianLerner/BIPs/blob/master/BIP-draft-sergiolerner-segwit2x.mediawiki

Reference source was kindly provided by the Segwit2x group.

Best regards,
 Sergio.
-------------------------------------
See Segwit v2 thread. Maybe we can collaborate on combining these.

On Wednesday 26 April 2017 6:15:26 PM shaolinfry via bitcoin-dev wrote:

-------------------------------------
Further to recent posts to this list concerning mining with more than one
hash function, Adam Perlow and me have a (longish) proposal/analysis on
combining multi-hash with bitcoin stake voting on what the mix of hashes
should be. Two novelties are:

* Targeting a ratio of blocks mined under each hash function, in a similar
way to difficulty targeting.
* Analysis of voting on this ratio in terms of "voting on a simplex", using
extant research to choose a good method of doing so.

One point about mining under multiple hashes is that it offers existing
miners a way back in after a contentious hard fork. Shame to waste all that
hardware…

Read at
http://topynate.net/wp-content/uploads/2017/03/proportionateresponse.pdf
(archived at http://www.webcitation.org/6pEZLlZoW)
Nathan Cook
-------------------------------------

OK, maybe "post-UASF coinbase coins" is a better term? I just wanted to make it clear that this refers to coins that come from blocks generated after the UASF is activated.

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
Oops. That makes much more sense than what I said. Thanks a lot for the
clarification.

On 03.11.2017 02:10, Russell O'Connor via bitcoin-dev wrote:

-- 
Adán Sánchez de Pedro Crespo
CTO, Stampery Inc.
San Francisco - Madrid

-------------------------------------


GitHub doesn’t allow email addresses to have multiple accounts.

-------------------------------------
This is correct. Under assumptions of a continuous mempool model however this should be considered the outlier behavior, other than a little bit of empty space at the end, now and then. A maximum fee rate calculated as a filter over past block rates could constrain this outlier behavior from ever happening too.


-------------------------------------
If all transactions pay the proposed max then fee there are still going to be an awful lot of never confirming transactions once the transaction bandwidth limit is surpassed, as I suppose that it roughly is now:

https://bitinfocharts.com/comparison/bitcoin-transactions.html


This is what I have been working on as an alternative:

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015371.html


There is a previous thread, linked later on in the linked thread.


Regards,

Damian Williamson


________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of oscar via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Friday, 22 December 2017 7:26:12 PM
To: bitcoin-dev@lists.linuxfoundation.org
Subject: [bitcoin-dev] what do you think about having a maximum fee rate?

Hello,
I'm not a bitcoin developer, but I'd like to receive feedback on what
I think is a serious problem. Hope I'm not wasting your time.
I'm also sure this was already discussed, but google doesn't give me
any good result.

Let me explain: I think that the current incentive system doesn't
really align with the way miners are distributed (not very
decentralized, due to pools and huge asic producers).
I think big miners are incentivized to spam the network with low(ish)
fee transactions, thereby forcing regular users into paying extremely
high fees to be able to get their transactions confirmed.

Obviously this is the result of insufficient mining decentralization,
but as I will try to show, such an attack could be profitable even if
you are controlling just 5-10% of the hashing power, which could
always be easy for a big player and with some collusion.

Let's look at some numbers: https://i.imgur.com/sCn4eDG.png

[https://i.imgur.com/sCn4eDG.png]


These are 10 blocks mined yesterday, and they all have rewards hugely
exceeding the normal 12.5 mining output. Even taking the lowest value
of 20, it's a nice 60% extra profit for the miner. Let's say you
control 10% of the hashing power, and you spam enough transactions to
fill 144 blocks (1 day's worth) at 50 satoshi/byte, losing just 72 BTC
in fees.

(blocksize-in-bytes * fee-per-byte * Nblocks)/satoshis-in-btc => (1e6
* 50 * 144)/1e8 => 72

At the same time you will discover about 144*0.1=14.4 blocks per day.
Assuming the situation we see in the previous screenshot is what
happens when you have a mempool bigger than one day's worth of blocks,
you would get 20-12.5=7.5 extra BTC per block, which is 14.4*7.5=108
BTC, given your investment of 72 to spam the mempool. 32 btc extra
profit.

The big assumption here is that spamming 1 day of backlog in the
50satoshi/b range will get people to compete enough to push 7.5 btc of
fees in each block, but:

* https://jochen-hoenicke.de/queue/#30d this seems to confirm that
[https://jochen-hoenicke.de/queue/mempool-20170608.png]<https://jochen-hoenicke.de/queue/#30d>

Johoe's Mempool Size Statistics - jochen-hoenicke.de/queue<https://jochen-hoenicke.de/queue/#30d>
jochen-hoenicke.de
This page displays the number and size of the unconfirmed bitcoin transactions, also known as the transactions in the mempool. It gives a real-time view and shows how ...


about half the mempool is in the 50satoshi/b range or less.
* https://blockchain.info/pools there are miners that control more than 10%
Bitcoin Hashrate Distribution - Blockchain.info<https://blockchain.info/pools>
blockchain.info
A pie chart showing the hashrate distribution between the major bitcoin mining pools - Blockchain


* if you get enough new real transactions, it's not necessary to spam
a full 144 blocks worth each day, probably just ~50 would be enough,
cutting the spam cost substantially
* other miners could be playing the same game, helping you spam and
further reduce the costs of the attack
* you actually get 10% of the fees back by avoiding mining your spam
transactions in your own blocks
* most of the spam transactions won't actually end up in blocks if
there is enough pressure coming from real usage

This seems to indicate that you would actually get much higher profit
margins than my estimates. **PLEASE** correct me if my calculations or
my assumptions are wrong.

You might also say that doing this would force users out of the
system, decreasing the value of btc and disincentivizing miners from
continuing. On the other hand, a backlogged mempool could create the
impression of high(er) usage and increase scarcity by slowing down
movements, which could actually push the price upwards.

Of course, it's impossible to prove that this is happening. But the
fact that it is profitable makes me believe that it is happening.

I see some solutions to this, all with their own downsides:

- increasing block size every time there is sustained pressure
this attack wouldn't work, but the downsides have already been
discussed to death.

- change POW
Not clear it would fix this, aside from stimulating terrible
infighting. Controlling 5 to 10% of the hashing power seems too easy,
and I don't think it would be practical to change pow every time that
happens, as it would prevent the development of a solid POW support.

- protocol level MAX transaction fee
I personally think this would totally invalidate the attack by making
the spam more expensive than the fees you would recover.
There already is a minimum fee accepted by the nodes, at 1 satoshi per
byte. The maximum fee could be N times the minimum, maybe 100-200.
Meaning a maximum of 1-2btc in total fee rewards when the block size
is 1mb. Of course the actual values need more analysis, but 2btc -
together with the deflationary structure - seems enough to continue
motivating miners, without giving unfair advantage.

Yes, this would make it impossible to spend your way out of a
congested mempool. But if the mempool stays congested after this
change, you could have a bigger confidence that it's coming from real
usage or from someone willfully burning money, making a block size
increase much more justified.

Hope to hear your opinion,
have a nice day.

oscar
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
bitcoin-dev Info Page - Linux Foundation<https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
lists.linuxfoundation.org
Bitcoin development and protocol discussion. This list is lightly moderated. - No offensive posts, no personal attacks. - Posts must concern development of bitcoin ...


-------------------------------------
On Wed, Apr 05, 2017 at 09:37:45PM +0000, Gregory Maxwell via bitcoin-dev wrote:

That should probably be "incrementing"...

Cheers,
aj

-------------------------------------
Hey everyone, This is an idea that I had about Segwit and Gregory's
proposal from yesterday that I wanted to run by everyone on this list. I'm
not at all sure what this would mean for non-upgraded nodes on the network
and would like feedback on that. This is not a formal BIP as it's a
modification to a previously submitted one, but I'm happy to formalize it
if it would help.
----------------------------------------
MotivationOne of the interesting aspects of Gregory Maxwell’s proposal is
that it only precludes the covert version of ASICBoost. He specifically
left the overt version alone.

Overt ASICBoost requires grinding on the version bits of the Block header
instead of the Merkle Root. This is likely more efficient than the Merkle
Root grinding (aka covert ASICBoost) and requires way less resources (much
less RAM, SHA256 calculations, no tx shuffling, etc).

If we combine Gregory Maxwell’s proposal with BIP-141 (Segwit) and add a
slight modification, this should, in theory, make ASICBoost a lot more
useful to miners and appeal to their financial interests.
The Modification

Currently, the version bits (currently 4 bytes, or 32 bits) in the header
are used for BIP9 signaling. We change the version bits to a nonce-space so
the miners can use it for overt ASICBoost. The 32-bits are now moved over
to the Coinbase transaction as part of the witness commitment. The witness
commitment goes from 38 bytes to 42 bytes, with the last 4 bytes being used
as the version bits in the block header previously. The witness commitment
becomes required as per Gregory Maxwell’s proposal.
Reasoning

First, this brings ASICBoost out into the open. Covert ASICBoost becomes
much more costly and overt ASICBoost is now encouraged.

Second, we can make this change relatively quickly. Most of the Segwit
testing stays valid and this change can be deployed relatively quickly.

Note on SPV clients

Currently Segwit stores the witness commitment in the Coinbase tx, so
lightweight clients will need to get the Coinbase tx + Merkle proof to
validate segwit transactions anyway. Putting block version information in
the Coinbase tx will not impose an extra burden on upgraded light clients.
-------------------------------------
Some discussion today led me to believe that a post segwit hard fork could
include:

1MB old tx non-witness segment
XMB new segwit non-witness segment
XMB witness segment

By partitioning off old transactions, it allows users of older, more
expensive validation transactions to continue using them, albeit with
higher fees required for the restricted space.

New segwit blocks, which don't have the hashing problem could be included
in the new non-witness segment of the block.
-------------------------------------
Martin:

Re: Block Space Authority, or "authority": in general

An authority dictates policy.

Authority arises in 4 cases off the top of my head:
- Authority because entity threats violence/dominance
- Authority because entity's claim to property is respected to maintain friendship/benefits of specialization and trade. (one has authority over one's own property/business/contractually agreed claims)
- Authority because entity claims divine inspiration, and others accept such a claim
- Authority because entity gained respect and was voluntarily delegated

"Miners" do not fit in any of these categories. In fact "miners" do the exact opposite, their policy is dictated by market demand. They do us the service of creating block candidates. If a miner is a good businessman, he mines whatever currency gives him the most profit. The end users decide the policy and which currency is worth anything. Hence the users are the ones dictating to the miners how much work they should perform on each coin.

Miners compete against each other until there is only very slim profit. If they are devoting too much work to a coin they spend too much on energy/computers/network, and they have losses, so they reduce capacity on that coin. If mining a coin is extremely profitable, they expand their work until there is no profit.

So... miners don't really have any authority. Or if for some reason somebody does give them authority, its due to either the Divine (lol unlikely) or Respect reasons above... which is an unfounded/insecure reason.

Using miner signalling to determine when/whether SegWit is activated was a mistake in any extent that gave people the implication that miners have any authority. It was a poor way to schedule its activation. We assumed that the miners would activate it in a reasonable time because SegWit is undeniably good, so we just used this method to try to prevent a soft fork. Instead I recommend my proposed BitcoinUpdateBoard https://pastebin.com/ikBGPVfR. Or bitcoin core could include more entities such as specific miners and exchanges in their table located here: https://bitcoincore.org/en/segwit_adoption/.

We already have come to consensus that SegWit is good. So we should just schedule a date to activate it in the future where market participants have a reasonable time to prepare.

Cheers,
Praxeology Guy
-------------------------------------
Nick,


This sort of email is unhelpful to this conversation, and it certainly doesn't help with the perception that Ethereum is nothing but a bunch of hypocritical Bankers 2.0.

Everyone knows you didn't edit Ethereum Classic, but the the hard fork, which was re-branded as Ethereum, was edited.

- Greg

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
Based on feedback from this list and further simulations, here is a new
algorithm for Block75:

new max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY)

TARGET_CAPACITY = 0.75
AVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a
decimal
x = current max block size

Please note that this algorithm actually tries to keep blocks 75% full,
unlike the old one that unnecessarily capped growth at 250KB. While this
would theoretically allow a maximum increase of 25% over the previous max
block size, in practice it's not possible to get that high.

This would be calculated every 2016 blocks along with difficulty.

Block75 should maintain transaction fees at about the level they were in
May/June 2016 when blocks started hitting 75% full somewhat consistently.

Thoughts? For any predictions as to how this would behave, please provide
the numbers used to arrive at any conclusions.

Other questions:
1. How do we make Block75 play nice with SegWit?
2. Is there any need for a minimum max blocksize? Block75 allows for
decreasing the size as well as increasing it.

Activation:
To help negate some of the risk associated with a hard fork and to prevent
a single relatively small mining pool from blocking Block75's adoption,
activation would occur once 900 of the last 1,000 blocks mined signaled
support, with a grace period of 4,032 blocks.

Thank you again to all those who commented on the previous Block75 thread.
Together, we can make 2017 the year the block size debate ends (hopefully
forever).

Happy New Year!

- t.k.
-------------------------------------
Blockchains with fast confirmation times are currently believed to
suffer from reduced security due to a high stale rate.

As blocks take a certain time to propagate through the network, if miner
A mines a block and then miner B happens to mine another block before
miner A's block propagates to B, miner B's block will end up wasted and
will not "contribute to network security".

Furthermore, there is a centralization issue: if miner A is a mining
pool with 30% hashpower and B has 10% hashpower, A will have a risk of
producing a stale block 70% of the time (since the other 30% of the time
A produced the last block and so will get mining data immediately)
whereas B will have a risk of producing a stale block 90% of the time.

Thus, if the block interval is short enough for the stale rate
to be high, A will be substantially more efficient simply by virtue of
its size. With these two effects combined, blockchains which produce
blocks quickly are very likely to lead to one mining pool having a large
enough percentage of the network hashpower to have de facto control over
the mining process.

Another possible implication of reducing the average block time is that
block size should be reduced accordingly. In an hypothetical 5 minutes
block size Bitcoin blockchain, there would be twice the block space
available for miners to include transactions, which could lead to 2
immediate consequences: (1) the blockchain could grow up to twice the
rate, which is known to be bad for decentralization; and (2) transaction
fees might go down, making it cheaper for spammers to bloat our beloved
UTXO sets.

There have been numerous proposals that tried to overcome the downsides
of faster blocks, the most noteworthy probably being the "Greedy
Heaviest Observed Subtree" (GHOST) protocol:
http://www.cs.huji.ac.il/~yoni_sompo/pubs/15/btc_scalability_full.pdf

Personally, I can't see why Bitcoin would need or how could it even
benefit at all from faster blocks. Nevertheless, I would really love if
someone in the list who has already run the numbers could bring some
valid points on why 10 minutes is the optimal rate (other than "if it
ain't broke, don't fix it").

-- 
Adán Sánchez de Pedro Crespo
CTO, Stampery Inc.
San Francisco - Madrid

-------------------------------------
Hi Dave



Thanks for your proposal.

I agree that 100GB of data may be cumbersome for some systems, especially if you target end user systems (Laptops/Desktops). Though, in my opinion, for those systems, CPU consumption is the biggest UX blocker.
Bootstrapping a full node on a decent consumer system with default parameters takes days, and, during this period, you probably run at full CPU capacity and you will be disturbed by constant fan noise. Standard tasks may be impossible because your system will be slowed down to a point where even word processing may get difficult.
This is because Core (with its default settings) is made to sync as fast as possible.

Once you have verified the chain and you reach the chain tip, indeed, it will be much better (until you shutdown for a couple of days/hours and have to re-sync/catch-up).

1. I agree that we need to have a way for pruned nodes to partially serve historical blocks.
My personal measurements told me that around ~80% of historical block serving are between tip and -1’000 blocks.
Currently, Core nodes have only two modes of operations, „server all historical blocks“ or „none“.
This makes little sense especially if you prune to a target size of, lets say, 80GB (~80% of the chain).
Ideally, there would be a mode where your full node can signal a third mode „I keep the last 1000 blocks“ (or make this more dynamic).

2. Bootstrapping new peers
I’m not sure if full nodes must be the single point of historical data storage. Full nodes provide a valuable service (verification, relay, filtering, etc.). I’m not sure if serving historical blocks is one of them. Historical blocks could be made available on CDN’s or other file storage networks. You are going to verify them anyways,... the serving part is pure data storage.
I’m also pretty sure that some users have stopping running full nodes because their upstream bandwidth consumption (because of serving historical blocks) was getting intolerable.
Especially „consumer“ peers must have been hit by this (little experience in how to reduce traffic, upstream in general is bad for consumers-connections, little resources in general).

Having a second option built into full nodes (or as an external bootstrap service/app) how to download historical blocks during bootstrapping could probably be a relieve for "small nodes“.
It could be a little daemon that downloads historical blocks from CDN’s, etc. and feeds them into your full node over p2p/8333 and kickstarts your bootstrapping without bothering valuable peers.
Or, the alternative download, could be built into the full nodes main logic.
And, if it wasn’t obvious, this must not bypass the verification!

I’m also aware of the downsides of this. This can eventually reduce decentralisation of the storage of historical bitcoin blockchain data and – eventually – increase the upstream bandwidth of peers willing to serve historical blocks (especially in a transition phase to a second „download“-option).
Maybe it’s a tradeoff between reducing decentralisation by killing low resource nodes because serving historical blocks is getting too resource-intense _or_ reducing decentralisation by moving some percentage of the historical data storage away from the bitcoin p2p network.
The later seems more promising to me.


To your proposal:
- Isn’t there a tiny finger-printing element if peers have to pick an segmentation index?
- SPV bloom filter clients can’t use fragmented blocks to filter txns? Right? How could they avoid connecting to those peers?

</jonas>
-------------------------------------
I have rewritten and simplified BIP114, and renamed it to “Merklized Script”, as a more accurate description after consulting the original proposers of MAST. It could be considered as a special case of MAST, but has basically the same functions and scaling properties of MAST.

Compared with Friedenbach’s latest tail-call execution semantics proposal, I think the most notable difference is BIP114 focuses on maintaining the static analysability, which was a reason of OP_EVAL (BIP12) being rejected. Currently we could count the number of sigOp without executing the script, and this remains true with BIP114. Since sigOp is a block-level limit, any OP_EVAL-like operation means block validity will depend on the precise outcome of script execution (instead of just pass or fail), which is a layer violation.

Link to the revised BIP114: https://github.com/jl2012/bips/blob/vault/bip-0114.mediawiki

On top of BIP114, new script functions are defined with 5 BIPs:

VVV: Pay-to-witness-public-key: https://github.com/jl2012/bips/blob/vault/bip-0VVV.mediawiki
WWW: String and Bitwise Operations in Merklized Script Version 0: https://github.com/jl2012/bips/blob/vault/bip-0WWW.mediawiki
XXX: Numeric Operations in Merklized Script Version 0: https://github.com/jl2012/bips/blob/vault/bip-0XXX.mediawiki
YYY: ECDSA signature operations in Merklized Script Version 0: https://github.com/jl2012/bips/blob/vault/bip-0YYY.mediawiki
ZZZ: OP_PUSHTXDATA: https://github.com/jl2012/bips/blob/vault/bip-0ZZZ.mediawiki

As a summary, these BIPs have the following major features:

1. Merklized Script: a special case of MAST, allows users to hide unexecuted branches in their scripts (BIP114)
2. Delegation: key holder(s) may delegate the right of spending to other keys (scripts), with or without additional conditions such as locktime. (BIP114, VVV)
3. Enabling all OP codes disabled by Satoshi (based on Elements project with modification. BIPWWW and XXX)
4. New SIGHASH definition with very high flexibility (BIPYYY)
5. Covenant (BIPZZZ)
6. OP_CHECKSIGFROMSTACK, modified from Elements project (BIPYYY)
7. Replace ~72 byte DER sig with fixed size 64 byte compact sig. (BIPYYY)

All of these features are modular and no need to be deployed at once. The very basic BIP114 (merklized script only, no delegation) could be done quite easily. BIP114 has its own versioning system which makes introducing new functions very easy.

Things I’d like to have:

1. BIP114 now uses SHA256, but I’m open to other hash design
2. Using Schnorr or similar signature scheme, instead of ECDSA, in BIPYYY.

Reference implementation: https://github.com/jl2012/bitcoin/commits/vault

-------------------------------------
Hello List,

I would like to propose a BIP that specifies a way of referring to transactions that have been successfully inserted into the blockchain.

The format makes use of the excellent Bech32 encoding, and is designed to be short and useful for human use. A C reference implementation is included.

Special care has been taken so this BIP is naturally extendable to support future upgrades to Bitcoin, Bitcoin Sidechains, or even from other blockchain projects. However, only support for the Bitcoin Main Chain, and the Test Network is specified in this draft.

I hope that the participants of the bitcoin-development mailing list find this draft BIP both interesting and useful. You are welcomed to read the full text here: https://github.com/veleslavs/bips/blob/Bech32_Encoded_TxRef/bip-XXXX-Bech32_Encoded_Transaction_Postion_References.mediawiki

If assigned with a BIP number some small updates to the specification will be made in accommodation.

С наилучшими пожеланиями,

Велеслав
-------------------------------------
On Tue, Mar 28, 2017 at 12:56 PM, Paul Iverson via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Bitcoin Core's (nor any other software's) maintainers can already not
decide on a hard fork, and I keep being confused by the focus on Core
in this topic. Even if a hard forking change (or lack thereof) was
included into a new release, it is still up to the community to choose
to run the new software. Bitcoin Core has very intentionally no
auto-update feature, as the choice for what network rules to implement
must come from node operators, not developers. Ask yourself this: if a
new Bitcoin Core release would include a new rule that blacklists
<random famous person>'s coins. What do you think would happen? I hope
that people would refuse to update, and choose to run different full
node software.

Core is not special. It is one of many pieces of software that
implement today's Bitcoin consensus rules. If a hardfork is to take
place in a way that does not result in two currencies, it must be
clear that the entire ecosystem will adopt it. Bitcoin Core will not
merge any consensus changes that do not clearly satisfy that
criterion.

-- 
Pieter

-------------------------------------
Hi Greg,

Responses below:

On 6/18/2017 5:30 PM, Tao Effect wrote:

It would not be accurate to say that miners have "total" control. Miners
do control the destination of withdrawals, but they do not control the
withdrawal-duration nor the withdrawal-frequency.

So, if miners wish to 'steal' from a sidechain, they _can_ initiate a
theft, but they can not change the fact that their malfeasance will be
[a] obvious, and [b] on display for a long period of time.

We might draw a comparison between:

1. Classic Theft   -- A majority hashrate reorganizes the main Bitcoin
chain to double-spend funds (or coordinate with someone who is
double-spending). This is prevented/discouraged by waiting for many
confirmations.
2. Channel Theft -- A majority hashrate assists a Lightning-Network
thief, by censoring the punitive audit txn (possibly by exploiting some
excuse regarding fullness of blocks, or possibly induced to do so by the
thief provably splitting the proceeds with miners). This is
prevented/discouraged by using lengthy custodial periods, paying high
fees with your attacker's money, and using fungibility/non-communication
to interact with miners as little as possible (so as to frame LN-theft
as undermining the entire LN system, and not merely a single tragedy).
3. Drivechain Theft -- A majority hashrate initiates an unrepresentative
withdrawal from some sidechain. This is prevented/discouraged by only
using 'popular' sidechains (those that [a] increase the usefulness
("market price") of bitcoin, and [b] generate tx fees for miners). It is
also discouraged by the fact that egregious theft would probably end the
sidechain experiment, meaning that all present and future sidechains
would be forever unavailable (and unable to buoy the price or the tx
revenues).

I do not think that any of the three stands out as being categorically
worse than the others, especially when we consider the heterogeneity of
use-cases and preferences. As Luke-Jr has been pointing out on social
media recently, the very group which is more associated with miners (and
explicitly more willing to trust them, ie Bitcoin Unlimited et al),
happens to be the same group that would be expected to make use of a
LargeBlock drivechain. Some can argue that one type of security is more
"cryptographic" than others, but I think this is misguided (how many
'bits' of security does each have?) -- imho, all three security models
are 'game theoretic' (neither computer scientific, nor cryptographic).

More importantly, before a miner has any "control" over the sidechain
coins, users must voluntarily agree to subject themselves to these new
rules. This is similar to how an arbitrary piece of (open source)
software can have "total" control over your computer...if you choose to
install it.


I'm not sure it would "create a border", given that sidechains are
currently not accessible at all. If anything drivechain cuts a door into
an existing impassible border.


The qualifier "/any/ sidechain" would seem to imply that there is a way
to do sidechains that does not involve handing over some control to 51%
hashrate...I think this is false (even in the fabled case of ZK-SNARKS).
The first thing I do in the drivechain spec (
truthcoin.info/blog/drivechain ) is explain why.


Yes, but money could also be confiscated from _any_ Bitcoin users
(Chinese or otherwise) using any of the three methods I mentioned above.
And confiscation could strike Chinese Bitcoin users if they decided to
sell their Bitcoin for Chinese Yuan, which they then deposited in a
Chinese bank. Or if they sold their Bitcoin for an Altcoin controlled by
the Chinese govt in some other way.

It is not up to the members of this list to decide, USSR style, what
other people are allowed to do with their own money.

The exceptions to this rule would be (ie, "bitcoin-dev should care about
what users are doing when..."):

1. [Unreasonable use of Reviewer Time]  The user's use-case is either
nonexistent (ie "no one wants that"), or totally unachievable ("we can't
do that") thus rendering the conversation a complete waste of time /
reviewer attention.
2. [Harmful Interference] The user's use-case would impose harm on some
existing use-case(s).

No reasonable person will claim the first, given today's scaling debate
(not to mention today's 'bitcoin dominance index'). Therefore, critics
must claim the second (as, for example, Peter Todd has been doing on
this list).

Which is why I narrowly focus on inter-chain harms [1], leading
ultimately to a focus on the mining ecosystem [2,3] and the development
of Blind Merged Mining [4].

[1]
https://www.youtube.com/watch?v=0goYH2sDw0w&list=PLw8-6ARlyVciNjgS_NFhAu-qt7HPf_dtg&index=1
[2] http://www.truthcoin.info/blog/mirage-miner-centralization/
[3] http://www.truthcoin.info/blog/mining-threat-equilibrium/
[4] http://www.truthcoin.info/blog/blind-merged-mining/
[5] http://www.truthcoin.info/blog/measuring-decentralization/


I think that one has some duty to very clearly define something (like
"mining centralization" [2] or "centralization" [5]) before complaining
about it. I feel that people will occasionally use selfless complaints
to accomplish a selfish goal...especially when the artificial selfless
part is hard to discuss by virtue of its being poorly defined
(especially vague or abstract items like "the company", "our country",
etc). For example, those who take it upon themselves to "defend" "the
Bitcoin community" may have exactly that in mind as their primary
goal...but they may also end up with more visibility (and with it: more
influence, more job offers, more conference invites, more friends, etc)
and they may also end up with a megaphone for which to broadcast their
other views, or just a defend-able excuse for bragging loudly about how
great cypherpunks are and/or how devoted they-in-particular are to the
cypherpunk tribe, et cetera. To avoid this problem in my own technical
discourse, I try to avoid abstractions like "centralization" until I
have defined them [2,5].

You have defined centralization above, but the definition is itself
vague to the point where I do not think even you actually endorse it.
For example, you would need to say that Bitcoin centralizes whenever the
exchange rate increases (as this grants additional financial power to
miners) or when any new user joins Bitcoin, or when tx fee revenues
increase for any reason. You might also be forced to say that LN
centralizes Bitcoin (as LN grants new capability/control to miners), and
probably even that Bitcoin becomes more centralized when developers
release new software (as this grants new capability to miners,
specifically the ability to deny upgrades). This probably isn't what you
meant, but since you did not clearly explain what you meant we have no
way of knowing for sure.

It seems to me that you reject the premise that BMM [4] addresses these
issues. This is probably because BMM only addresses miner's interactions
with each other, and it does not address miner abilities as a group in
relation to other groups (for example, vs. users, developers,
investors). But, as I consistently emphasize, these groups of people are
free to ignore any sidechains that they do not like. In law there is a
saying 'volenti non fit injuria' which I would translate as "he who
volunteers cannot claim later to have been injured". This is a legal
theory, because otherwise everyone would be arbitrarily liable for
choices beyond their control (ie, responsible for decisions of other
unrelated people), which would be nonsense.


Currently no (P2P) sidechains exist, and therefore the set of choices
today would seem to be more "limited" than in a post-sidechain future.
(The set of options may decrease later, for ecological reasons, if and
only if 'exchanges' are a strictly inferior option to 'sidechains' for
some reason...I don't see why this would be the case. I also don't
understand the emphasis on "exchanges" [SCs are much more like Altcoins,
than exchanges] in the first place, nor the dubious qualifier
"trustworthy".)

--Paul


-------------------------------------
In case anyone wants to do this, I added the CSidechainAddress class to the
mainchainBMM branch of the Drivechain project a long time ago. The only
purpose it serves right now is to show that sidechain deposit addresses can
start with a '4'. We could simply add the sha256 hash described by Paul to
a script with OP_RETURN at the front and make that the standard.

On Jul 12, 2017 4:47 PM, "Paul Sztorc via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

On 7/12/2017 4:50 AM, ZmnSCPxj wrote:

...


That said, I am fully in favor of forcing the sidechain's permanent
deposit address to be equal to some deterministic function of the sha256
hash of its version 0.1 release.

Paul

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
On Thu, Feb 23, 2017 at 9:53 AM, Chris Priest via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I can't speak to MMRs (they look a bit redundant with the actual blockchain
history to my eye) but circling back to utxo commitments, the benefits are
that it enables actual proofs of non-fraud: You can prove the validity of a
block based on just the previous block (and maybe some previous headers
because of mining rewards) and can prove to a light node that a utxo hasn't
been spent yet.

A major factor in the way of getting utxo commitments in blocks is
performance. The txo set is of course vastly larger and more unwieldy. If
you make the utxo commitments trail by a small fixed number of blocks
(between 2 and 5) their latency problems shouldn't be a big deal as long as
the overall performance is good enough. My thesis is that with appropriate
format and implementation tricks it's possible to get performance good
enough to no longer be a gating factor to deployment.

Disappointingly there hasn't been any feedback about my implementation,
just discussion about merkle sets generally.
-------------------------------------
On Thu, Oct 5, 2017 at 4:33 PM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


For the record, it's Johnson Lau's proposal where I read this idea.
-------------------------------------
On Sat, Apr 8, 2017, at 02:44, Gregory Maxwell wrote:

Resource cost is not just a measure of storage requirement; data that
needs to be accessed during peak load induce more cost then data only
used during base load or only rarely used.


In Core, when a block comes the inputs are checked against the UTXO set
(which grows with outputs)  even if pre-synced, to verify order. Am I
wrong there? This is not in the case in bitcrust; it is instead checked
against the spend-tree (which grows with inputs).

How "significant" this is, I neither know nor claim,  but it is an
interesting difference. 


I think you are being a bit harsh here . I am also clearly explaining
the difference only applies to peak load, and just making a suggestion.
I simply want to stress the importance of protocol / implementation
separation as even though you are correct UTXO data is always a resource
cost for script validation (as I also state), the ratio of different
costs are  not necessarily *identical* across implementation. 

Note that the converse also holds: In bitcrust, if the last few blocks
contain many inputs, the peak load verification for this block is
slower. This is not the case in Core.

Tomas

-------------------------------------
On Thu, Feb 23, 2017 at 01:28:18PM -0500, G. Andrew Stone wrote:

Why do you want a non-existance proof?

It supports an efficient *spentness* proof, which is sufficient for what we
need in Bitcoin, and much more scalable.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On Tuesday, 28 March 2017 21:56:49 CEST Paul Iverson via bitcoin-dev wrote:

The suggestion was not to produce 32MB blocks, so your fear here is 
unfounded.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
On Mon, Sep 11, 2017 at 10:43:52AM -0700, Daniel Stadulis wrote:

That makes sense.

For comparison, Monero defines a response process that has three levels
and varies the response for each:

]     a. HIGH: impacts network as a whole, has potential to break entire
]        network, results in the loss of monero, or is on a scale of great
]        catastrophe
]     b. MEDIUM: impacts individual nodes, wallets, or must be carefully
]        exploited
]     c. LOW: is not easily exploitable

 -- https://github.com/monero-project/monero/blob/master/VULNERABILITY_RESPONSE_PROCESS.md

Among other things, HIGH gets treated as an emergency, MEDIUM get fixed
in a point release; LOW get deferred to the next regular release eg.

Additionally, independently of the severity, Monero's doc says they'll
either get their act together with a fix and report within 90 days,
or otherwise the researcher that found the vulnerability has the right
to publically disclose the issue themselves...

I wouldn't say that's a perfect fit for bitcoin core (at a minimum, given
the size of the ecosystem and how much care needs to go into releases,
I think 90 days is probably too short), but it seems better than current
practice...

For comparison, if you're an altcoin developer or just bitcoin core user,
and are trying to work out whether the software you're using is secure;
if you do a quick google and end up at:

  https://en.bitcoin.it/wiki/Common_Vulnerabilities_and_Exposures

you might conclude that as long as you're running version 0.11 or later,
you're fine. That doesn't seem like an accurate conclusion for people
to draw; but if you're not tracking every commit/PR, how do you do any
better than that?

Maybe transitioning from keeping things private indefinitely to having
a public disclosure policy is tricky. Maybe it might work to build up to it,
something like:

  * We'll start releasing info about security vulnerabilities fixed in
    0.12.0 and earlier releases as of 2018-01-01
  * Then we'll continue with 0.13.0 and earlier as of 2018-03-01
  * Likewise for 0.14.0 as of 2018-05-01
  * Thereafter we'll adopt a regular policy at http://...

That or something like it at least gives people relying on older,
potentially vulnerable versions a realistic chance to privately prepare
and deploy any upgrades or fixes they've missed out on until now.

Cheers,
aj


-------------------------------------
I think it's probably safer to have a fork-to-minumum (e.g. minimal
coinbase+header) after a certain date than to fork up at a certain date. At
least in that case, the default isn't breaking consensus, but you still get
the same pressure to fork to a permanent solution.

I don't endorse the above proposal, but remarked for the sake of guiding
the argument you are making.


--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>

On Tue, Mar 28, 2017 at 1:31 PM, Wang Chun via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I don't get it. At the moment, the number of Bitcoin is fixed (at 21
million) by the geometric decay of the block reward.

Adding any other means of creating coins besides the existing block reward,
or altering the block reward schedule, is extremely likely to be seen as
messing with fixed supply. And not adding another method to create coins
wouldn't work - because then redemptions would have to come out of miner's
block reward, which I don't imagine they're going to share just because you
ask.

The only way you might convince users that adding a second way to mint
coins is not messing with fixed supply, is if there is some kind of proof
that the number of coins being minted is accounted for by past burnt coins.
We could call this 'regeneration'. But then you also need a way to prevent
double-regeneration, in which the same burnt coins are used as proof twice.

And you would also need per-sidechain accounting, so that you can't just
regenerate burnt coins that were originally burnt for sidechain A when all
you have is coins on sidechain B. But where to put all this logic? Building
a system that enforces the accounting for sidechains into Bitcoin, as Lucas
pointed out, is not much different to just building the sidechain itself
directly into Bitcoin.

And if you did assemble all that, what you have anyway is a two way peg,
which I suspect will be isomorphic to the very sidechain proposals you seem
to be criticising/attempting to do better than.



*Ben Kloester*

On 11 October 2017 at 07:43, Tao Effect via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
2017-10-09 22:39 GMT-03:00 Paul Sztorc via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:


I understand the first-mover disadvantages, but I keep thinking that if the
new chain is Pareto optimal, i.e. is in all aspects at least good as the
original chain, but in some so much better to justify the change, the
initial resistance is an unstable equilibrium. Like a herd of buffaloes
attacking a lion: the first buffalo to attack is in awful disadvantage, but
if a critical mass of the herd follows, the movement succeeds beyond
turning back, and every buffalo benefited, both those who attacked the lion
and those that didn't (because the lion was chased away or killed).

-- 
Lucas Clemente Vella
lvella@gmail.com
-------------------------------------
Sorry again, is this discussion really serious?

In this thread, previous ones or others, the level of approximation is
obvious, often shadowed by useless maths/papers and strange calculations
that are not helping, at the end nobody knows what would happen "if",
how far the chain can roll back, etc

Then don't make pruning the default if it's the current concern, pruning
is of no use

Again, the priority should be to scale the full nodes


Le 11/05/2017  22:10, Jonas Schnelli via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
While this isn't an unreasonable proposal, it will orphan blocks from any
miner who isn't running it (or BIP148) by Aug 1, right?  That seems rather
rushed for a non-backwards-compatible SF, especially since in practice,
miners are unlikely to deploy it until it comes bundled with some version
of the Segwit2x HF code.

I realize this is a touchy topic but - how much hard evidence is there that
there *will* be significant disruption if miners simply ignore both this
and BIP148?  Correct me but afaict, BIP148 has ~0% hashrate support.

Unless the HF code is ready and agree on soon (say by Jul 1), my vote is to
keep the main chain backwards-compatible, especially if evidence of miner
support for BIP148 doesn't materialize soon.  It seems less disruptive for
recently-deployed BIP148 nodes to revert than to ask every miner in the
system to quickly upgrade or get orphaned.

Just my view, I respect that others will differ.


On Tue, Jun 6, 2017 at 9:54 PM, James Hilliard via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I don't think it's a huge deal if the miners need to run a non-Core node
once the BIP91 deployment of Segwit2x happens. The shift will most likely
be temporary.

I agree that the "-bip148"-option should be merged, though.

2017-06-20 17:44 GMT+02:00 Erik Aronesty via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
Hey Greg,

It wasn't my intention to insult anyone (a bit defensive?).

Maybe this is yet another example of a recurring criticism of Core: that core doesn't community these issues very well to journalists / reports / media / community outside of this list.

Because outside of this list it's been all about those 148 coins, and almost zero mention of replay attacks.


Are there other, more reasonable / feasible ways of addressing replay attacks in Bitcoin / BIP149 scenario?

Cheers,
Greg

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
On Tue, Sep 12, 2017 at 3:37 AM, Anthony Towns via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

No.


For embargoed fixes we test the specific fixes against experienced
developers inside the project, handing them the proposed commit and
informing them that it fixes a vulnerability and asking them to
identify it.

This does not guarantee that the fix won't leak the issue, but in
virtually all cases in the past the issues we've dealt with would not
be made worse off being leaked in that way vs just making it public
outright.

If we had an issue that would be-- e.g. an RCE that could lead to
private key theft, we would likely handle it differently (e.g. making
a public notice to take sensitive systems offline before attempting
any fix).


History does not support your assumptions.


Not really.  Any forced change still creates centralization,
dependence, and an opportunity for insecurity.


That is a concern too, but our bar for backport fixes is low enough
that they're often able to include more serious fixes without calling
attention to them.


This is true even outside of the consensus critical parts.  In the P2P
network other people upgrading can be protective.


Sure, a few have. Most do not because they are either not focused on
software quality or consider themselves as having an adversarial
relationship with Bitcoin.


If you'd like to provide the sort of valuable information to the
market which may get you sued or targeted for harassment of physical
attack-- feel free. Don't ask the rest of us to do so.

-------------------------------------
On Monday, 20 March 2017 21:12:36 CEST Martin Stolze via bitcoin-dev wrote:

Nag; they don’t have any authority.


This is not the case, it misunderstands Bitcoin and specifically is 
misunderstands that Bitcoin is distributed and decentralized.

What you call “block generators” or “transaction processors” are in reality 
called miners and they don’t have any authority to mine or not mine certain 
transactions. All they have is a business incentive to mine or not mine a 
certain transaction.
This is a crucial distinction as that makes it a economical decision, not a 
political.

The massive distribution of miners creating blocks means that one miner is 
free to add his political agenda. They can choose to not mine any satoshi-
dice transactions, should they want. But they can’t stop other miners from 
mining those transactions anyway, and as such this is not a political move 
that has any effect whatsoever, at the end of the day it is just an 
economcal decision.

The rest of your email is based on this misconception as well, and therefore 
the above answers your question.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
For some time now the relation between block size and propagation speed has 
been decoupled. Using xthin/compact blocks miners only send a tiny version 
of a block which then causes the receiving node to re-create it using the 
memory pool.  Immediately getting double benefits by including pre-verified 
transactions from the memory pool you avoid the old problem of having to 
validate them again when a block was mined.

As such there is no downside to a miner creating a bigger block, as long as 
all the transactions they include are actually in the mempool.

As such I'm personally convinced that the problem you are trying to solve 
has already been solved.

Cheers!


On Monday, 27 March 2017 18:12:19 CEST Btc Ideas via bitcoin-dev wrote:


-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
On Tuesday 25 April 2017 6:28:14 PM Gregory Maxwell via bitcoin-dev wrote:

FWIW, I disagree in this case. I think given the circumstances, if we are 
going to do a UASF for segwit at all, we need a clearly decisive outcome, 
which is given by BIP 148. Using the approach in BIP 8 makes sense in many 
cases, but in this case, it is liable to simply create a prolonged uncertainty 
where nobody knows the outcome when segwit's rules are challenged by a 
malicious miner.

If BIP 148 fails to achieve widespread support, we could do a BIP 8-based UASF 
with Segwit v2 (along with some other changes I suggested in the other 
thread), but I think the tradeoffs right now favour BIP 148 as the best UASF 
deployment.

Luke

-------------------------------------
On Fri, Jun 2, 2017 at 2:15 AM, Chris via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Really bad for privacy. Data for transactions at the tip is only
14kb/s-- potentially less if segwit is in use and you're not getting
witnesses. Is that really that burdensome?

FWIW, leaving a mobile browser just running while pointed at some
websites seems to use more traffic than that just loading advertising.
:)

-------------------------------------
Hi all,

I have a scaling solution idea that I would be interested in getting some
feedback on. I’m new to the mailing list and have not been in the Bitcoin
space as long as some have been, so I don’t know if anyone has thought of
this idea.

Arguably the biggest scaling problem for Bitcoin is the unbounded UTXO
growth. Current scaling solutions like Segregated Witness, Lighting
Network, and larger blocks does not address this issue. As more and more
blocks are added to the block chain the size of the UTXO set that miners
have to maintain continues to grow. This is the case even if the block size
were to remain at 1 megabyte. There is no way out of solving this
fundamental scaling problem other then to limit the maximum size of the
UTXO set.

The following soft fork solution is proposed. Any UTXO that is not spent
within a set number of blocks is considered invalid. What this means for
miners and nodes in the Bitcoin network is that they only have to ever
store that set number of blocks. In others words the block chain will never
be larger then the set number of blocks and the size of the block chain is
capped.

But what this means for users is that bitcoins that have not been spent for
a long time are “lost” forever. This proposed solution is likely a
difficult thing for Bitcoin users to accept. What Bitcoin users will
experience is that all of a sudden their bitcoins are spendable one moment
and the next moment they are not. The experience that they get is that all
of a sudden their old bitcoins are gone forever.

The solution can be improved by adding this new mechanism to Bitcoin, that
I will call luster. UTXO’s that are less then X blocks old has not lost any
luster and have a luster value of 1. As UTXO’s get older, the luster value
will continuously decrease until the UTXO’s become Z blocks old (where Z >
X), and has lost all it’s luster and have a luster value of 0. UTXO’s that
are in between X and Z blocks old have a luster value between 0 and 1. The
luster value is then used to compute the amount of bitcoins that must be
burned in order for a transaction with that UTXO to be included in a block.
So for example, a UTXO with a luster value of 0.5 must burn at least 50
percent of its bitcoin value, a UTXO with a luster value of 0.25 must burn
at least 75 percent of its bitcoin value, and a UTXO with a luster value of
0 must burn 100 percent of its bitcoin value. Thus the coins/UTXOs that
have a luster value of 0 means it has no monetary value, and it would be
safe for bitcoins nodes to drop those UTXOs from the set they maintain.

The idea is that coins that are continuously being used in Bitcoin economy
will never lose it’s luster. But coins that are old and not circulating
will start to lose its luster up until all luster is lost and they become
valueless. Or they reenter the economy and regains all its luster.

But at what point should coins start losing their luster? A goal would be
that we want to minimize the scenarios of when coins start losing their
luster. One reasonable answer is that coins should only starting losing its
luster after the lifespan of the average human. The idea being that a
person will eventually have to spend all his coins before he dies,
otherwise it will get lost anyways (assuming that only the dying person has
the ability to spend those coins). Otherwise there are few cases where a
person would never spend their bitcoins in there human life time. One
example is in the case of inheritance where a dying person does not want to
spend his remaining coins and have another person take them over. But with
this propose scaling solution, coins can be stilled inherited, but it would
have to be an on-chain inheritance. The longest lifespan of a human
currently is about 120 years old. So a blockchain that stores the last 150
years of history seems like one reasonable option.

Then the question of how large blocks should be is simply a matter of what
is the disk size requirement for a full node. For simplicity, assuming that
a block is created every 10 minute, the blockchain size in terabyte can be
express as the following.
blockSize MB * 6 * 24 * 365 * years /1000000 = blockchainSize TB

Example values:
blockSize = 1MB, years = 150 -> blockchainSize = 7.884 TB
blockSize = 2MB, years = 150 -> blockchainSize = 15.768 TB

So if we don’t want the block chain to be bigger then 8 TB, then we should
have a block size of 1 MB. If we don’t want the block chain to be bigger
then 16 TB, then we should have a block size of 2 MB and so on. The idea is
that base on how cheap disk space gets, we can adjust the target max block
chain size and the block size accordingly.

I believe that this proposal is a good solution to the UTXO growth problem.
The proposal being a soft fork is a big plus. It also keeps the block chain
size finite even if given a infinite amount of time. But there are other
things to considered, like how best should wallet software handle this? How
can this work with sidechains? More thought would need to be put into this.
But the fact is that if we want to make bitcoins last forever, we have the
accept unbounded UTXO growth, which is unscalable. So the only solution is
to limit UTXO growth, meaning bitcoins cannot last forever. This proposed
solution however does not prevent Bitcoin from lasting forever.
-------------------------------------
80% have set "NYA" in their coinbase string. We have no idea what that
means. People are equating it to BIP 91 -- but BIP 91 did not exist at
the time of the New York agreement, and differs from the actual text
of the NYA in substantive ways. The "Segwit2MB" that existed at the
time of the NYA, and which was explicitly referenced by the text is
the proposal by Sergio Demian Lerner that was made to this mailing
list on 31 March. The text of the NYA grants no authority for
upgrading this proposal while remaining compliant with the agreement.
This is without even considering the fact that in the days after the
NYA there was disagreement among those who signed it as to what it
meant.

I feel it is a very dangerous and unwarranted assumption people are
making that what we are seeing now is either 80% support for BIP-91 or
for the code in the btc1 repo.

On Tue, Jun 20, 2017 at 6:36 PM, Erik Aronesty <erik@q32.com> wrote:

-------------------------------------
Great stuff, although the ordering of the sections seems a little bit confusing.

I think it would be clearer to put the "Creation of proofs" section
before "Proof verification", maybe even before "Proof format" if a
high level defintion of "full tx size proof" is provided before.

Also, in "For the full-size proof, each transaction should be assumed
to be at a minimum the stripped-size rather than the fixed 60 bytes."
it seems you are referring to a "full-size block proof" as opposed to
a "full size tx proof", perhaps a better term could be "full-weight
block proof" if what you are referring to is the proof of the weight
instead of only the pre-segwit size.

Perhaps some short definitions for "stripped-size proof", "full tx
size proof", "full-size proof" and maybe also "size component" at the
beginning would be enough.

In "Network protocol", "It should not recheck blocks known to be
valid, " does "known to be valid" include the blocks that the peer
told us where valid (with their hash and 0 in the enumerated varint)?
Those could be invalid too if the peer was lying, no?
Do you mean "It should not recheck blocks known to be invalid,"?

Why do you need to have at least one full tx size?

In Rationale you have:
"
Why must a full tx size proof be included?

This is necessary to establish that the claimed block transaction
count is correct.
"

Why do you need to establish that? If you can establish that the
number of transactions is at least N and that N * 60 bytes is greater
than the size/weight limit, isn't it that enough?


On Wed, Mar 22, 2017 at 10:51 PM, Matt Corallo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
What if two sidechains are implemented at once? What if people get excited
about one sidechain today, but a second even-better one is published the
very next week? What if the original mainchain decides to integrate the
features of the sidechain that you just one-way pegged to?

In these cases, the user looses money, whereas in the two-way peg they
would not lose a thing.

While the one-way peg is interesting, it really doesn't compare.

Paul

On Oct 10, 2017 4:19 PM, "Lucas Clemente Vella" <lvella@gmail.com> wrote:

2017-10-09 22:39 GMT-03:00 Paul Sztorc via bitcoin-dev <bitcoin-dev@lists.
linuxfoundation.org>:


I understand the first-mover disadvantages, but I keep thinking that if the
new chain is Pareto optimal, i.e. is in all aspects at least good as the
original chain, but in some so much better to justify the change, the
initial resistance is an unstable equilibrium. Like a herd of buffaloes
attacking a lion: the first buffalo to attack is in awful disadvantage, but
if a critical mass of the herd follows, the movement succeeds beyond
turning back, and every buffalo benefited, both those who attacked the lion
and those that didn't (because the lion was chased away or killed).

-- 
Lucas Clemente Vella
lvella@gmail.com
-------------------------------------
1. If it only affects "old dust" UTXO's where the # of coins in the UTXO
aren't sufficient to pay some lower quantile of transaction fees, then
there can be little argument of theft or loss.

2. There's another use-case for demurrage as well.

Computation power may grow rapidly if quantum computing becomes more
common.  At some point, Bitcoin may have to change the public key format
for coins and the POW used.

In order to do this, old coins will have to transact on the network, moving
their value to a new format, with many more bits in the public key, for
example.   But since quantum computing isn't bounded by moore's law, so
this may need to be a regular upgrade every X years.   Rather than a
regular "bit widening hard fork", the number of bits needed in a public
address format could be scaled to the difficulty of the new quantum hashing
algorithm that *also must *now grow in the # of bits over time.   To ensure
that coins are secure, those with too few bits must drop off the network.
So the timing for old coin demurrage can effectively be based on the
quantum POW difficulty adjustments.   As long as the subsequent exponential
rate of computation increase can be reasonably predicted (quantum version
of moore's law), the new rate of decay can be pegged to a number of years.



On Mon, Aug 21, 2017 at 10:26 AM, Moral Agent via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Also.... how is this not a tax on coin holders? By forcing people to move
coins around you would be chipping away at their wealth in the form of
extorted TX fees.
-------------------------------------
Hi Danny,

On Mon, Apr 17, 2017 at 3:11 AM, Danny Thorpe wrote:

Yeah, but that's because most people (well, using myself as the
"target market" anyway) are upgrading to SSD's for the faster boot and
response times.  Modern consumer OS's run incredibly slow on
non-ssd drives!  And since the vast majority of consumer laptops sold
today fall into the $400 to $700 range, a 200 - 500gb SSD is about the
most storage upgrade people can afford.

And so I think David's premise, that having to devote only 30GB to
running a full node instead of 100, would remove a major obstacle that
prevents many more people running full bitcoin nodes.

My only suggestion is, does it scale?  I mean, if the bitcoin network
volume grows exponentially and in 2 years the blockchain is 500GB, can
the "small node" be adjusted down from one fifth of the blockchain to
just one-tenth, or one twentieth?  Can different smalInesses
interoperate? Can I choose to store a small node with 20 - 30% of the
blockchain, while others chose to share just 5% or 10% of it? Can I run
"less small" node today that's 50GB?

Can the default install be a "small node" that requires about 30GB of
storage (if that is indeed the sweet spot for enticing many more users to
bringing nodes online), but allow the user at install time, to choose *how*
small? To, say, drag a slider anywhere up and down the range from
10GB to 100GB?

If not, then it will have to be revisited constantly as the blockchain
grows, and disk storage prices drop.  I suspect the blockchain will
grow in size, at some point in the not too distant future, much faster
than storage prices drop, so making small, smaller and smallest nodes
that can be configured to store more or less of it will be necessary
to motivate most users to run nodes at all.  But when that happens,
there is likely to be exponentially *more* people using bitcoin, too!
So an exponentially growing number of users running (smaller and
smaller) nodes would take up the slack.

Then, the blockchain would begin to look a lot more like a bittorrent,
right? ;-) but -- happily -- one that you never need to download fully.

-dave

-------------------------------------
start double checking the last few bytes instead?

2017-10-30 8:56 GMT+00:00 shiva sitamraju via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
Eric Voskuil,

TL;DR: Electrical power is a general purpose consumer good vs PoW mining equipment is a single purpose consumer good. Hence the mining equipment rent is the barrier to entry, given if you invest in power generation capital you could use the power for a different purpose.

Each unit of electrical power (1 V* A = 1 Watt) is a finite unit of a highly non-durable consumable good.

It is true that electrical power is created by utilizing capital equipment, and the capital rent + labor of generating such power is the basis for the "Power Cost" component of the ideal miner competition profit equation.

But... electrical power is a general consumer good that can be used for many things, so investing in the capital to create it is not a very risky endeavor.

On the other hand, Bitcoin mining equipment capital is an EXTEREMELY specific kind of capital that only has exactly one use: efficiently/competitively mining a coin that has a particular PoW algorithm. Hence investing in bitcoin mining equipment is a more risky endeavor than power generation capital. Such a risk is a barrier to entry, and it is the barrier that is most considered when an entity considers mining Bitcoins.

Mature Arithmetic Logic Unit (ALU) bound PoW algorithms lacking new attacks (cryptographic definition) can only be out-dated by more efficient, more general purpose (less specific case proprietary) transistor fabrication technology.

Memory Latency bound PoW algorithms lacking new attacks (cryptographic definition) have the risk of being encumbered by all sorts of physical hardware patent inventions. This is because latency has significantly more room for such specific-to-PoW non-general purpose inventions... beyond additional patents relating to memory technology on top of ALU patents. Patents, I should point out, either cause the price of capital equipment to increase or enforce a monopoly on the capital... neither of which are desirable.

The capital maturity outlook of memory latency bound algorithms is also significantly worse than ALU bound... due to all of the expected future patent-able optimizations that could improve memory latency. Hence investing in memory latency bound mining equipment is even riskier because of the likeliness of a new patented optimization making your capital non-competitive, and given its specific nature, worthless.

This discussion brings me to a new insight. We have said that some places have "cheaper" power than others, due to the non-durable nature of electrical power. With the existence of Bitcoin, given other cost factors being less significant, Bitcoin causes all sources of power everywhere to be more equal in price at a particular time.

Now you might argue that memory latency bound PoW algorithms result in the mining capital component being the larger component than the electricity component being a good thing because: then mining would be less local to otherwise untapped (cheap) power sources. The problem with this is that as the mining capital matures (as all the optimizations are found, and the patents run out), we go strait back to the power cost being the largest component... and we had to suffer all the years of various entities unpredictably attaining a monopoly on mining in order to get there.

Please let me know if I made a mistake.

Thanks,
Praxeology Guy
-------------------------------------
It has taken almost 6 months for SegWit adoption to get to where it is
today. I don't think it will take that long to reach similar adoption for
UASF SegWit, but conservatively we want to give it at least that much time.

It's really important to stress here that a UASF will split and become the
minority chain if a majority of the transaction accepting nodes on the
network do not agree to strictly follow the UASF and outright reject blocks
that do not signal for SegWit at the designated date.

Before setting a flag day, I think we should get written cooperation
agreements from the largest economic players in Bitcoin. This would include:

Bitfinex
Bitflyer
BitGo
BitPay
Bitstamp
Blockchain.info
Blockcypher
Coinbase
Huobi
Kraken
Gemeni
OkCoin
Poloniex

(feel free to discuss this list)

100% cooperation is not necessary, but close to 100% cooperation is
strongly desired. It should be noted that their cooperation is only
required because they are sufficiently powerful to threaten the success of
a UASF, particularly because many of these entities hold users bitcoins.

Once a convincing majority is on-board, I suggest we release a UASF patch
that activates a full year after release. This is because a UASF is a big
gamble that requires a large majority of the economy has upgraded.

Though that is a very long time, SegWit can always be activated early with
miner cooperation.

------

As an extra note, if the UASF triggers with majority economy support and
the miners resist, a minority block reward chain may be the longest chain
for a while. However, when the majority block reward chain does catch up,
the minority reward chain will be entirely obliterated, eliminating all
block rewards, all transaction history, and making a ton of money vanish
all at once.

This makes it very dangerous for an exchange, payment processor, online
wallet, or miner to oppose the UASF if there is significant momentum behind
it. This gives the UASF a powerful snowball effect once a few major parties
(or the majority of tiny full nodes) have decided to commit to the UASF.

On the other hand, failure means a permanent coin split, so it is still
necessary to exercise caution that exceeds the caution of a normal soft
fork.
-------------------------------------
Hi everyone,

It has been 3 weeks -- responses so far have been really helpful. People
jumped right in, and identified unfinished or missing parts much faster
than I thought they would (ie, ~two days). Very impressive.

Currently, we are working on the sidechain side of blind merged mining.
As you know, most of the Bitcoin cryptosystem is about finding the
longest chain, and displaying information about this chain. CryptAxe is
editing the sidechain code to handle reorganizations in a new way (an
even bigger departure than Namecoin's, imho).

I believe that I have responded to all the on-list objections that were
raised. I will 1st summarize the on-list objections, and 2nd summarize
the off-list discussion (focusing on three key themes).


On-List Objection Summary
---------------------------

In general, they were:

* Peter complained about the resources required for the BMM 'crisis
audit', I pointed out that it is actually optional (and, therefore,
free), and that it doesn't affect miners relative to each other, and
that it can be done in an ultra-cheap semi-trusted way with high
reliability.
* Peter also asked about miner incentives, I replied that it is profit
maximizing to BMM sidechains, because the equation (Tx Fees - Zero Cost)
is always positive.
* ZmnSCPxj asked a long series of clarifying questions, and I responded.
* Tier Nolan complained about my equivocation of the "Bitcoin: no block
subsidy" case and the "sidechain" case. He cites the asymmetry I point
out below (in #2). I replied, and I give an answer below.
* ZmnSCPxj pointed out an error in our OP Code (that we will fix).
* ZmnSCPxj also asked a number of new questions, I responded. Then he
responded again, in general he seemed to raise many of the points
addressed in #1 (below).
* ZmnSCPxj wanted reorg proofs, to punish reorgers, but I pointed out
that if 51% can reorg, they can also filter out the reorg proof. We are
at their mercy in all cases (for better or worse).
* ZmnSCPxj also brought up the fact that a block size limit is necessary
for a fee market, I pointed out that this limit does not need to be
imposed on miners by nodes...miners would be willing-and-able to
self-impose such a limit, as it maximizes their revenues.
* ZmnSCPxj also protested the need to soft fork in each individual
sidechain, I pointed out my strong disagreement ("Unrestrained smart
contract execution will be the death of most of the interesting
applications...[could] destabilize Bitcoin itself") and introduced my
prion metaphor.
* ZmnSCPxj and Tier Nolan both identified the problem solved by our
'ratchet' concept. I explained it to ZmnSCPxj in my reply. We had not
coded it at the time, but there is code for it now [1]. Tier proposed a
rachet design, but I think ours is better (Tier did not find ours at
all, because it is buried in obscure notes, because I didn't think
anyone would make it this far so quickly).
* Tier also advised us on how to fix the problem that ZmnSCPxj had
identified with our NOP earlier.
* Tier also had a number of suggestions about the real-time negotiation
of the OP Bribe amount between nodes and miners. I'm afraid I mostly
ignored these for now, as we aren't there yet.
* Peter complained that ACKing the sidechain could be exploited for
political reasons, and I responded that in such a case, miners are free
to simply avoid ACKing, or to acquiesce to political pressure. Neither
affect the mainchain.
* Peter complained that sidechains might provide some miners with the
opportunity to create a pretext to kick other miners off the network. I
replied that it would not, and I also brought up the fact that my
Bitcoin security model was indifferent to which people happened to be
mining at any given time. I continue to believe that "mining
centralization" does not have a useful definition.
* Peter questioned whether or not sidechains would be useful. I stated
my belief that they would be useful, and linked to my site
(drivechain.info/projects) which contains a number of sidechain
use-cases, and cited my personal anecdotal experiences.
* Peter wanted to involve miners "as little as possible", I pointed out
that I felt that I had indeed done this minimization. My view is that
Peter felt erroneously that it was possible to involve miners less,
because he neglected [1] that a 51% miner group is already involved
maximally, as they can create most messages and filter any message, and
[2] that there are cases where we need miners to filter out harmful
interactions among multiple chains (just as they filter out harmful
interactions among multiple txns [ie, "double spends"]). Peter has not
yet responded to this rebuttal.
* Peter also suggested client-side validation as "safer", and I pointed
out that sidechains+BMM _is_ client-side validation. I asked Peter for
CS-V code, so that we can compare the safety and other features.
* Sergio reminded us of his version of drivechain. Sergio and I disagree
over the emphasis on frequency/speed of withdrawals. Also Sergio
emphasizes a hybrid model, which does not interest me.

If I missed any objections, I hope someone will point them out.


Off-List / Three Points of Ongoing Confusion
---------------------------------------------

Off list, I have repeated the a similar conversation perhaps 6-10 times
over the past week. There is a cluster of remaining objections which
centers around three topics -- speed, theft, and antifragility. I will
reply here, and add the answers to my FAQ (drivechain.info/faq).

1. Speed

This objection is voiced after I point out that side-to-main transfers
("withdrawals") will probably take a long time, for example 5 months
each ( these are customizable parameters, and open for debate -- but if
withdrawals are every x=3 months, and only x=1 withdrawal can make
forward progress [on the mainchain] at a time, and only x=1 prospective
withdrawal can be assembled [by the sidechain] at a time, then we can
expect total withdrawal time to average 4.5 months [(.5)*3+3] ). The
response is something like "won't such a system be too slow, and
therefore unusable?".

Imho, replies of this kind disregard the effect of atomic cross-chain
swaps, which are instant.

( In addition, while side-to-main transfers are slow, main-to-side
transfers are quite fast, x~=10 confirmations. I would go as far as to
say that, just as the Lightning Network is enabled by SegWit and CSV,
Drivechain is enabled by the atomic swaps and of Counterparty-like
embedded consensus. )

Thanks to atomic swaps, someone can act as an investment banker or
custodian, and purchase side:BTC at a (tiny, competitive discount) and
then transfer those side-to-main at a minimal inconvenience (comparable
to that of someone who buys a bank CD). Through market activities, the
_entire system_ becomes exactly as patient as its most-patient members.
As icing on the cake, people who aren't planning on using their BTC
anytime soon (ie "the patient") can even get a tiny investment yield, in
return for providing this service.


2. Security

This objection usually says something like "Aren't you worried that 51%
[hashrate] will steal the coins, given that mining is so centralized
these days?"

The logic of drivechain is to take a known fact -- that miners do not
steal from exchanges (by coordinating to doublespend deposits to those
exchanges) -- and generalize it to a new situation -- that [hopefully]
miners will not steal from sidechains (by coordinating to make 'invalid'
withdrawals from them).

My generalization is slightly problematic, because "a large mainchain
reorg today" would hit the entire Bitcoin system and de-confirm *all* of
the network's transactions, whereas a sidechain-theft would hit only a
small portion of the system. This asymmetry is a problem because of the
1:1 peg, which is explicitly symmetrical -- the thief makes off coins
that are worth _just as much_ as those coins that he did _not_ attack.
The side:BTC are therefore relatively more vulnerable to theft, which
harms the generalization.

As I've just explained, to correct this relative deficiency, we add
extra inconvenience for any sidechain thievery, which is in this case
the long long withdrawal time of several months. (Which is also the main
distinction between DC and extension blocks).

I cannot realistically imagine an erroneous withdrawal persisting for
several weeks, let alone several months. First, over a timeframe of this
duration, there can be no pretense of 'we made an innocent mistake', nor
one that 'it is too inconvenient for us to fix this problem'. This
requires the attacker to be part of an explicitly malicious conspiracy.
Even in the conspiring case, I do not understand how miners would
coordinate the distribution of the eventual "theft" payout, ~3 months
from now -- if new hashrate comes online between now and then, does it
get a portion? -- if today's hashrate closes down, does it get a reduced
portion? -- who decides? I don't think that an algorithm can decide
(without creating a new mechanism, which -I believe- would have to be
powered by ongoing sustainable theft [which is impossible]), because the
withdrawal (ie the "theft") has to be initiated, with a known
destination, *before* it accumulates 3 months worth of acknowledgement.

Even if hashrate were controlled exclusively by one person, such a theft
would obliterate the sidechain-tx-fee revenue from all sidechains, for a
start. It would also greatly reduce the market price of [mainchain] BTC,
I feel, as it ends the sidechain experiment more-or-less permanently.

And even _that_ dire situation can be defeated by UASF-style maneuvers,
such as checkpointing. Even the threat of such maneuvers can be
persuasive enough for them never to be needed (especially over long
timeframes, which make these maneuvers convenient).

A slightly more realistic worst-case scenario is that a monopolist
imposes large fees on side-to-main transfers (which he contrives so that
only he can provide). Unless the monopoly is permanent, market forces
(atomic swaps) will still lower the fee to ultra-competitive levels,
making this mostly pointless.


3. Antifragility

There is an absolutely crucial distinction of "layers", which is often
overlooked. Which is a shame, because its importance really cannot be
understated.

Taleb's concept of antifragility is explicitly fractal -- it has layers,
and an upper layer can only be antifragile if it is composed of
individual members of a lower layer who are each *fragile*. In one of my
videos I give the example of NYC food providers -- it is _because_ the
competition is so brutal, and because each individual NYC
restaurant/supermarket/food-truck is so likely to fail, (and because
there is no safety net to catch them if they do fail), that the
consumer's experience is so positive (in NYC, you can find almost any
kind of food, at any hour of the day, at any price, despite the fact
that a staggering ~15 million people will be eating there each day).

By this, I mean there is an absolutely crucial distinction between:

1. A problem with a sidechain that negatively impacts its parent chain.
2. A problem with a sidechain that only impacts the sidechain users.

The first type of problem is unacceptable, but the second type of
problem is actually _desirable_.

If we wanted to have the best BTC currency unit possible, we would want
everyone to try all kinds of things out, _especially_ the things that we
think are terrible. We'd want lots of things to be tried, and for the
losers to "fail fast".

In practice I highly doubt the sidechain ecosystem would be anywhere
near as dynamic as NYC or Silicon Valley. But certain questions, such as
"What if a sidechain breaks / has DAO-like problems?"; "What if the
sidechain has only a few nodes? Who will run them?"; "Who will maintain
these projects?"; -- really just miss the point, I think.

Ultimately, users can vote with their feet -- if the benefits of a
sidechain outweigh its risks, some users will send some BTC there. It is
a strict improvement over the current situation, where users would need
to use an Altcoin to achieve as much. Users who aren't comfortable with
the risks of new chains / new features, can simply decline to use them.


Another Objection
------------------

Finally, two people raised an objection which I will call the "too
popular" objection or "too big to fail (tbtf)". Something like "what if
a *majority* of BTC is moved to one sidechain, and then that sidechain
has some kind of problem?".

One simple step would be to cap the quantity of BTC that can be moved to
each sidechain, (at x=10% ? aka 210,000).

Other than that, I would point out that Bitcoin has always been the
"money of principle", and that we survived the MtGox announcement (in
which ~850,000/12,400,000 = 6.85% of the total BTC were assumed to be
stolen).

I look forward to the continued feedback! Thank you all very much!

Paul

[1]
https://github.com/drivechain-project/bitcoin/commit/c4beef5c2aa8e52d2c1e11de7c044528bbde6c60


On 5/22/2017 2:17 AM, Paul Sztorc wrote:

-------------------------------------
There's a pull req to core already for part of it:

https://github.com/bitcoin/bitcoin/pull/10444




On Tue, Jun 27, 2017 at 12:31 PM, Jorge Timón via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Wed, Nov 01, 2017 at 05:48:27AM +0000, Devrandom via bitcoin-dev wrote:

Some quick thoughts...


First of all, I don't think you can really call this a soft-fork; I'd call it a
"pseudo-soft-fork"

My reasoning being that after implementation, a chain with less total work than
the main chain - but more total SHA256^2 work than the main chain - might be
followed by non-supporting clients. It's got some properties of a soft-fork,
but it's security model is definitely different.


Note how you're basically proposing for the block interval to be decreased,
which has security implications due to increased orphan rates.


Exactly! Not really a soft-fork.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On Tue, May 23, 2017 at 1:03 PM, Steven Pine via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Essentially, if we make a potentially very harmful option easy to
enable for users, we are putting them at risk, so yes, this is about
protecting users of the base Bitcoin Core implementation. Users have,
hopefully, come to appreciate this implementation for the peer
review-based strict development process, and making a hasty decision
due to time constraints (segwit activation expiration) may have
undesirable consequences. Opinions among the regular contributors are
split on the matter, which to me is an indication we should be
cautious and consider all aspects before making a decision on the
matter.

-------------------------------------

IMO privacy its something developers should make sure users have it.
Also, I think, todays SPV wallets should make users more aware of the possible privacy implications.

Do users know, if they pay for a good in a shop while consuming the shops WIFI, that the shop-owner as well as the ISP can use that data to combine it with the user profile (and ~ALL FUTURE purchases you do with the same wallet IN ANY LOCATION online or in-person)?

Do users know, that ISPs (cellular; including Google) can completely link the used Bitcoin wallet (again: all purchase including future ones) with the to the ISP well known user profile including credit-card data and may sell the Bitcoin data to any other data mining company?

If you use BIP37, you basically give your transaction history (_ALL TRANSACTIONS_ including transactions in future) to everyone.



This may be true because they are not aware of the ramification and I don’t think client side filtering is a drop-in replacement for todays, smartphone SPV-model.


/jonas
-------------------------------------
Your BIP would take away the only way the *receiver* has to raise the
fee: CPFP. And the receiver is arguably the more important party in this
question. After all the sender has no real incentive for his payment to
be confirmed; it's receiver who has.


On 07/02/2017 10:35 PM, Rhavar via bitcoin-dev wrote:



-------------------------------------

Correct me if I'm wrong, but nothing  possible if the client software
was electrum-like and used two independent sources for verification.
No?


This and the next point are just reductio ad absurdem, since no one is
suggesting anything of the sort. Even in that situation, I can't think
of anything miners could do if clients used more than one independent
source for verification, ala electrum question above.


No one is suggesting anything like this.  The cost of running a node
that could handle 300% of the 2015 worldwide nonbitcoin transaction
volume today would be a rounding error for most exchanges even if
prices didn't rise.



On Fri, Mar 31, 2017 at 1:19 AM, Luv Khemani <luvb@hotmail.com> wrote:

-------------------------------------
Hi Jonathan,

The proposal raised here does not deny miners the ability to use ASICBOOST.
Miners can still use overt ASICBOOST by version bit fiddling and get the
same power savings.  In fact, overt ASICBOOST is much easier to implement
than covert ASICBOOST, so I don't really understand what the objection is.


On Apr 6, 2017 13:44, "Jonathan Toomim via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Ethically, this situation has some similarities to the DAO fork. We have an
entity who closely examined the code, found an unintended characteristic of
that code, and made use of that characteristic in order to gain tens of
millions of dollars. Now that developers are aware of it, they want to
modify the code in order to negate as much of the gains as possible.

There are differences, too, of course: the DAO attacker was explicitly
malicious and stole Ether from others, whereas Bitmain is just optimizing
their hardware better than anyone else and better than some of us think
they should be allowed to.

In both cases, developers are proposing that the developers and a majority
of users collude to reduce the wealth of a single entity by altering the
blockchain rules.

In the case of the DAO fork, users were stealing back stolen funds, but
that justification doesn't apply in this case. On the other hand, in this
case we're talking about causing someone a loss by reducing the value of
hardware investments rather than forcibly taking back their coins, which is
less direct and maybe more justifiable.

While I don't like patented mining algorithms, I also don't like the idea
of playing Calvin Ball on the blockchain. Rule changes should not be
employed as a means of disempowering and empoverishing particular entities
without very good reason. Whether patenting a mining optimization qualifies
as good reason is questionable.

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
On Monday, 2 January 2017 16:05:58 CET t. khan wrote:

In actual fact the block size *is* set by miners, not math. And always has 
been.
In your proposal the max blocksize continues to be set by miners as a 
secondary effect of them choosing the block size.

Saying the max is actually math is painting an illusion that is rather thin 
and easy to see through because every single usecase for your suggestion 
starts with the choice of blocksize that a human makes. There is not really 
any other input except some rather simple algorithm.


This is ignoring history where miners have successfully set policy on block 
size for years now.


Not sure about your "obviously". I don't agree. In fact, there is plenty of 
reason to think it does work.

Miners have always been the ones to decide on the block size, and they have 
always done this in a coordinated fashion. This is a natural consequence of 
the rather elegant (economic) design of Bitcoin.

*  Miners earn more fee-based income when they produce bigger blocks.
*  Miners take more risk of their blocks being orphaned with bigger blocks.
*  Miners want to avoid emptying the memory pool every block as that removes 
a total need for users to pay fees.
*  Miners want to make sure the mempool does not become backlogged because 
users that do not see their transactions confirmed will get disappointed and 
find other means to do payments. Which hurts the price and in effect hurts 
the miners income.

This behaviour in block size means blocks will not get huge and they will 
not stay small either, because there are reasons for both. And so the size 
will be something in the middle.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
On Wednesday, 8 March 2017 20:47:54 CET Jonas Schnelli via bitcoin-dev 
wrote:

Do you know the trick of having an open wifi basestation in a public street 
and how that can lead to tracking? Especially if you have a network of them.
The trick is this; you set up an open wifi base station with a hidden ssid 
and phones try to connect to it by saying “Are you ssid=xyz?”
This leads the basestation to know that the phone has known credentials with 
another wifi that has a specific ssid. (the trick is slightly more elaborate, 
but the basics are relevant here).

Your BIP is vulnarable to the same issue, as a node wants to connect using 
the AUTHCHALLENGE which has as an argument the hash of the person I’m trying 
to connect with.

Your BIP says "Fingerprinting the requesting peer is not possible”.
Unfortunately, this is wrong. Yes the peer is trivial to fingerprint. Your 
hash never changes and as you connect to a node anyone listening can see you 
sending the same hash on every connect to that peer, whereever you are or 
connect from.

Just like the wifi hack.

I think you want to use industry standards instead, and a good start may be 
https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
I couldn't agree more. It would require however for the Devs to throw their
weight behind this with a lot of momentum. Spoonnet has been under
development for quite some time now. Counter offering SegWit plus Spoonnet
12-24 months later would be a very progressive stance that I think would
catch the interest of large swaths of the community. I'd be curious to hear
Johnson's opinion on this. How much more testing would his proposal require?

Daniele


----------------------------------------------------------------------
-------------------------------------
On Thu, Feb 23, 2017 at 6:58 PM, Peter Todd <pete@petertodd.org> wrote:


After wading through your logic on how updates are done, I agree that that
can be done, but apples to apples compact proofs can also be done in a utxo
commitment, and proofs of the validity of updates can be done in a utxo
commitment, so there isn't any performance advantage to all that extra
complexity.
-------------------------------------
On 09/07/2017 06:23 PM, Pavol Rusnak via bitcoin-dev wrote:

I think that would work.


In case of Bitcoin Wallet, the depth is not null (m/0'/[0,1]) and still
we need this field. I think it should always be present if a chain is
limited to a certain script type.

There is however the case where even on one chain, script types are
mixed. In this case the field should be omitted and the wallet needs to
scan for all (known) types. Afaik Bitcoin Core is taking this path.


-------------------------------------
Hi,

I'm writing to suggest and discuss the addition of paper wallet
functionality in bitcoin-core software, starting with a single new RPC
call: genExternalAddress [type].

-- rationale --

bitcoin-core is the most trusted and most secure bitcoin implementation.

Yet today (unless I've missed something) paper wallet generation
requires use of third party software, or even a website such as
bitaddress.org.  This requires placing trust in an additional body of
code from a less-trusted and less peer-reviewed source.  Ideally, one
would personally audit this code for one's self, but in practice that
rarely happens.

In the case of a website generator, the code must be audited again each
time it is downloaded.  I cannot in good faith recommend to anyone to
use such third party tools for wallet generation.

I *would* recommend for others to trust a paper wallet that uses
address(es) generated by bitcoin-core itself.

At least for me, this requirement to audit (or implicitly trust) a
secondary body of bitcoin code places an additional hurdle or
disincentive on the use of paper wallets, or indeed private keys
generated outside of bitcoin-core for any purpose.

Unfortunately, one cannot simply use getnewaddress, getaccountaddress,
or getrawchangeaddress for this purpose, because the associated private
keys are added to the bitcoin-core wallet and cannot be removed... or in
the case of hd-wallets are deterministically derived.

As such, I'm throwing out the following half-baked proposal as a
starting point for discussion:


-----

    genexternaladdress ( "type" )

    Returns a new Bitcoin address and private key for receiving
    payments. This key/address is intended for external usage such as
    paper wallets and will not be used by internal wallet nor written to
    disk.

    Arguments:
    1. "type"        (string, optional) one of: p2pkh, p2sh-p2wpkh
                                        default: p2sh-p2wpkh

    Result:
    {
        "privKey"    (string) The private key in wif format.
        "address"    (string) The address in p2pkh or p2sh-p2wpkh
                              format.
    }


    Examples:


----

This API is simple to implement and use.  It provides enough
functionality for any moderately skilled developer to create their own
paper wallet creation script using any scripting language, or even for
advanced users to perform using bitcoin-cli or debug console.

If consensus here is in favor of including such an API, I will be happy
to take a crack at implementing it and submitting a pull request.

If anyone has reasons why it is a BAD IDEA to include such an RPC call
in bitcoind, I'm curious to hear it.

Also, I welcome suggestions for a better name, or maybe there could be
some improvements to the param(s), such as calling p2sh-p2wpkh "segwit"
instead.


---- further work ----


Further steps could be taken in this direction, but are not necessary
for a useful first-step.  In particular:

1. an RPC call to generate an external HD wallet seed.
2. an RPC call to generate N key/address pairs from a given seed.
3. GUI functionality in bitcoin-qt to facilitate easy paper wallet
generation (and printing?) for end-users, complete with nice graphics,
qr codes, etc.











-------------------------------------

Without this option, a miner has to guess whether a split will be
economically impacting.   With this option, his miner will automatically
switch to the chain least likely to get wiped out... as soon as a simple
majority of miners supports it.


On Wed, Jun 7, 2017 at 12:44 PM, Jacob Eliosoff <jacob.eliosoff@gmail.com>
wrote:

-------------------------------------
It's not pointless: it's a wake-up call for miners asleep "at the wheel", to 
ensure they upgrade in time. Not having a mandatory signal turned out to be a 
serious bug in BIP 9, and one which is fixed in BIP 148 (and remains a problem 
for BIP 149 as-is). Additionally, it makes the activation decisive and 
unambiguous: once the lock-in period is complete, there remains no question as 
to what the correct protocol rules are.

It also enables deploying softforks as a MASF, and only upgrading them to UASF 
on an as-needed basis.

Luke


On Wednesday 05 July 2017 4:00:38 AM shaolinfry wrote:

-------------------------------------
Even if the proposal involves a political compromise, any change to the
code must be technically evaluated.
The patch was made to require the least possible time for auditing. I'm
talking about reviewing 120 lines of code (not counting comments or
space) which 30 of them are changes to constants. A core programmer audited
it in less than one hour.

Also you're risking the unique opportunity to see segwit activated for
what?
Maybe we can reach a similar agreement for segwit activation in two years.
That's will be too late. The remaining cryptocurrency ecosystem do move
forward.



On Sat, Apr 1, 2017 at 12:03 AM, Samson Mow <samson.mow@gmail.com> wrote:

-------------------------------------
We can’t “just compute the Transaction ID the same way the hash for signing the transaction is computed” because with different SIGHASH flags, there are 6 (actually 256) ways to hash a transaction.

Also, changing the definition of TxID is a hardfork change, i.e. everyone are required to upgrade or a chain split will happen.

It is possible to use “normalised TxID” (BIP140) to fix malleability issue. As a softfork, BIP140 doesn’t change the definition of TxID. Instead, the normalised txid (i.e. txid with scriptSig removed) is used when making signature. Comparing with segwit (BIP141), BIP140 does not have the side-effect of block size increase, and doesn’t provide any incentive to control the size of UTXO set. Also, BIP140 makes the UTXO set permanently bigger, as the database needs to store both txid and normalised txid


-------------------------------------
The discount is designed to reduce UTXO bloat primarily, witness spam
data would not make it into the UTXO set. The discount brings the fee
of a transaction more in line with the actual costs to the network for
the transaction. A miner spamming the network with 4MB witness blocks
would have very little impact on the UTXO size compared with 1MB
non-witness blocks. UTXO size is a bigger issue than blockchain size
since full nodes can't prune the UTXO set.

The discount of 75% for the SegWit softfork doesn't really have any
effect on future hard forks as it can always be adjusted as needed
later on as part of a HF.

On Tue, May 9, 2017 at 8:49 AM, Sergio Demian Lerner via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Peter Todd,

This MMR structure looks good to me. I really like how wallets keep their MMR proof and txo index instead of requiring the entire network to maintain an index on txids w/ plain old utxo snapshots.

Re: "only left or right child of inner node be a fully spent node"... that sounds fine to me, but the software should virtually consider that the previous dissapearing leaf nodes still exist. It would instead say be a special case handled by the meta hashing function. Would save a good amount of time from unneccesary hashing. Might also do the rule: if a parent node has a single fully spent child node, its hash is equal to its other child's hash.

Below is questions about txo/utxo MMR commitments after reading: "https://petertodd.org/2016/delayed-txo-commitments".

I'm mainly concerned about the performance of recalculating all of the node hashes on old spends. But probably with a long enough delay policy, it shouldn't be an issue.

Then the issues with people keeping their MMR proofs up to date and discovering received txos before they get pruned. Sure would be nice if a wallet didn't have to keep on updating their MMR proof. Hopefully spends would refer to old txos by their MMR index.

How are you ordering MMR additions? Are you only adding old utxos to the MMR? Or every old txo? I think you are doing all old txos (mostly would be spent nodes), but why not just old utxos? Are you doing it to make MMR index = blockchain txo index, so such an index can be used in all TX inputs except for non-confirmed transactions? Potentially a tx could use a MMR.ix, allblock'stxo.ix (if we want to maintian that index), or tx.id & vout.ix depending on how old the tx is.

What is the process for removing old utxos from the utxo set, and placing them into the MMR? Are you going to keep height in the utxo db, and then iterate through the whole thing?

Are you still proposing a 1 year txo commitment delay? Do you have any data/preformance studies judging the cost of a larger utxo and longer delay vs smaller utxo and shorder delay? I would figure a longer delay would be better as long as the utxo doesn't get too big. Longer delay means less MMR maintainance.

Have you considered not making a new commitment on every block, instead maybe every 6*24 blocks or so? This could reduce the number of times the same nodes are re-hashed, while not having much impact on security.

What about re-orgs? You'd have to restore the previous leaf & inner nodes. You'd need both the txos and MMR proofs, right? It looks like you clear this info from the TXO journal when the delay duration threshold is met. I guess this info might also be stored with the block data, and could be recovered from there.

What are your current thoughts on block size weighting for MMR proofs? Just count the extra byte length that full nodes need to relay as simliar to SegWit witness data?

Cheers,
Praxeology Guy
-------------------------------------
Sorry, I cannot quite follow you. What do you mean with flag?

Best,
Henning


Am 04.05.2017 um 18:23 schrieb Chris Pacia:


-------------------------------------
According to my understanding, Bitcoin protocol is a combination of several
components (node, miner, wallet..), you can use different licenses for
different components, as long as the components are well structured and
inter APIs are well defined and encapsulated, therefore, incompatible
licenses could be not an issue.
Please note that I'm not legal advisor and this is just my personal opinion.

On Tue, Sep 26, 2017 at 12:53 AM, Radcliffe, Mark via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thu, Feb 23, 2017 at 06:50:10PM -0800, Bram Cohen wrote:

Sorry, but I was replying to your statement:


So to be clear, do you agree or disagree with me that you *can* extract a
compact proof from a MMR that a given output is unspent?

I just want to make sure we're on the same page here before we discuss
performance characteristics.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On Sun, Sep 10, 2017 at 07:02:36PM -0400, Matt Corallo via bitcoin-dev wrote:

That seems like it just says bitcoin core has two classes of users:
people who use it directly following mainnet or testnet, and people who
make derived works based on it to run altcoins.

Having a "responsible disclosure" timeline something like:

 * day -N: vulnerability reported privately
 * day -N+1: details shared amongst private trusted bitcoin core group
 * day 0: patch/workaround/mitigation determined, CVE reserved
 * day 1: basic information shared with small group of trusted users
      (eg, altcoin maintainers, exchanges, maybe wallet devs)
 * day ~7: patches can be included in git repo
      (without references to vulnerability)
 * day 90: release candidate with fix available
 * day 120: official release including fix
 * day 134: CVE published with details and acknowledgements

could make sense. 90 days / 3 months is hopefully a fair strict upper
bound for how long it should take to get a fix into a rc; but that's still
a lot longer than many responsible disclosure timeframes, like CERT's at
45 days, but also shorter than some bitcoin core minor update cycles...
Obviously, those timelines could be varied down if something is more
urgent (or just easy).

As it is, not publishing vulnerability info just seems like it gives
everyone a false sense of security, and encourages ignoring good security
practices, either not upgrading bitcoind nodes, or not ensuring altcoin
implementations keep up to date...

I suppose both "trusted bitcoin core group" and "small group of trusted
users" isn't 100% cypherpunk, but it sure seems better than not both not
disclosing vulnerability details, and not disclosing vulnerabilities
at all... (And maybe it could be made more cypherpunk by, say, having
the disclosures to trusted groups have the description/patches get
automatically fuzzed to perhaps allow identification of leakers?)

Cheers,
aj


-------------------------------------

When I first began to enter the blocksize debate slime-trap that we
have all found ourselves in, I had the same line of reasoning that you
have now.  It is clearly untenable that blockchains are an incredibly
inefficient and poorly designed system for massive scales of
transactions, as I'm sure you would agree.  Therefore, I felt it was
an important point for people to accept this reality now and stop
trying to use Blockchains for things they weren't good for, as much
for their own good as anyone elses.  I backed this by calculating some
miner fee requirements as well as the very issue you raised.  A few
people argued with me rationally, and gradually I was forced to look
at a different question: Granted that we cannot fit all desired
transactions on a blockchain, how many CAN we effectively fit?

It took another month before I actually changed my mind.  What changed
it was when I tried to make estimations, assuming all the reasonable
trends I could find held, about future transaction fees and future
node costs.  Did they need to go up exponentially?  How fast, what
would we be dealing with in the future?  After seeing the huge
divergence in node operational costs without size increases($3 vs
$3000 after some number of years stands out in my memory), I tried to
adjust various things, until I started comparing the costs in BTC
terms.  I eventually realized that comparing node operational costs in
BTC per unit time versus transaction costs in dollars revealed that
node operational costs per unit time could decrease without causing
transaction fees to rise.  The transaction fees still had to hit $1 or
$2, sometimes $4, to remain a viable protection, but otherwise they
could become stable around those points and node operational costs per
unit time still decreased.

None of that may mean anything to you, so you may ignore it all if you
like, but my point in all of that is that I once used similar logic,
but any disagreements we may have does not mean I magically think as
you implied above.  Some people think blockchains should fit any
transaction of any size, and I'm sure you and I would both agree
that's ridiculous.  Blocks will nearly always be full in the future.
There is no need to attempt to handle unusual volume increases - The
fee markets will balance it and the use-cases that can barely afford
to fit on-chain will simply have to wait for awhile.  The question is
not "can we handle all traffic," it is "how many use-cases can we
enable without sacrificing our most essential features?"  (And for
that matter, what is each essential feature, and what is it worth?)

There are many distinct cut-off points that we could consider.  On the
extreme end, Raspberry Pi's and toasters are out.  Data-bound mobile
phones are out for at least the next few years if ever.  Currently the
concern is around home user bandwidth limits.  The next limit after
that may either be the CPU, memory, or bandwidth of a single top-end
PC.  The limit after that may be the highest dataspeeds that large,
remote Bitcoin mining facilities are able to afford, but after fees
rise and a few years, they may remove that limit for us.  Then the
next limit might be on the maximum amount of memory available within a
single datacenter server.

At each limit we consider, we have a choice of killing off a number of
on-chain usecases versus the cost of losing the nodes who can't reach
the next limit effectively.  I have my inclinations about where the
limits would be best set, but the reality is I don't know the numbers
on the vulnerability and security risks associated with various node
distributions.  I'd really like to, because if I did I could begin
evaluating the costs on each side.


That's a good question, and one I don't have a good handle on.  How
does Bitcoin's current memory usage scale?  It can't be based on the
UTXO, which is 1.7 GB while my node is only using ~450mb of ram.  How
does ram consumption increase with a large block versus small ones?
Are there trade-offs that can be made to write to disk if ram usage
grew too large?

If that proved to be a prohibitively large growth number, that becomes
a worthwhile number to consider for scaling.  Of note, you can
currently buy EC2 instances with 256gb of ram easily, and in 14 years
that will be even higher.


I believe this is exactly the kind of discussion we should be having
14 years before it might be needed.  Also, this wouldn't be unique -
Some software I have used in the past (graphite metric collection)
came pre-packaged with the ability to scale out to multiple machines
split loads and replicate the data, and so could future node software.


Bandwidth costs are, as intra-datacenter bandwidth is generally free.
The other ones warrant evaluation for the distant future.  I would
expect that CPU resources is the first thing we would have to change -
13 thousand transactions per second is an awful lot to process.  I'm
not intimately familiar with the processing - Isn't it largely
signature verification of the transaction itself, plus a minority of
time spent checking and updating utxo values, and finally a small
number of hashes to check block validity?  If signature verification
was controlling, a specialized asic chip(on a plug-in card) might be
able to verify signatures hundreds of times faster, and it could even
be on a cheap 130nm chipset like the first asic miners rushed to
market.  Point being, there are options and it may warrant looking
into after the risk to node reductions.


I don't think this is as big a deal as it first might seem.  The
software would already come written to be spanned onto multiple
machines - it just needs to be configured.  For the specific question
at hand, the exchange would already have IT staff and datacenter
capacity/operations for their other operations.  In the more general
case, the numbers involved don't work out to extreme concerns at that
level.  The highest cpu usage I've observed on my nodes is less than
5%, less than 1% for the time I just checked, handling ~3 tx/s.  So
being conservative, if it hits 100% on one core at 60-120 tx/s, that
works out to ~25-50 8-core machines.  But again, that's a 2-year old
laptop CPU and we're talking about 14 years into the future.  Even if
it was 25 machines, that's the kind of operation a one or two man IT
team just runs on the side with their extra duties.  It isn't enough
to hire a fulltime tech for.


I mean, that's literally what Amazon does for you with S3, which was
even cheaper than the EBS datastore pricing I was looking at.  So....
Even disregarding that, raid operation was a solved thing more than 10
years ago, and hard drives 14 years out would be roughly ~110 TB for a
$240 hard drive at a 14%/year growth rate.  In 2034 the blockchain
would fit on 10 of those.  Not exactly a "failing every day" kind of
problem.  By 2040, you'd need *gasp* 22 $240 hard drives.  I mean, it
is a lot, but not a lot like you're implying.


That depends heavily upon the tradeoffs the businesses can make.  I
don't think node operation at an exchange is a five-nines uptime
operation.  They could probably tolerate 3 nines.  The worst that
happens is occasionally people's withdrawals and deposit are delayed
slightly.  It won't shut down trading.


Visa stores the only copy.  They can't afford to lose the data.
Bitcoin isn't like that, as others pointed out.  And for most
businesses, if their node must be rebooted periodically, it isn't a
huge deal.


Ok, when is that point, and what is the tradeoff in terms of nodes?
Just because something is hard doesn't mean it isn't worth doing.
That's just a defeatist attitude.  How big can we get, for what
tradeoffs, and what do we need to do to get there?


This is really not that hard.  Have a central database, update/check
the utxo values in block-store increments.  If a utxo has already been
used this increment, the block is invalid.  If the database somehow
got too big(not going to happen at these scales, but if it did), it
can be sharded trivially on the transaction information.  These are
solved problems, the free database software that's available is pretty
powerful.


NO, NOT CLEVER.  WE CAN'T DO THAT.

Sorry, I had to. :)


I know of and have experience working with systems that handled
several orders of magnitude more data than this.  None of the issues
brought up above are problems that someone hasn't solved.  Transaction
commitments to databases?  Data consistency across multiple workers?
Data storage measured in exabytes?  Data storage and updates
approaching hundreds of millions of datapoints per second?  These
things are done every single day at numerous companies.

On Fri, Mar 31, 2017 at 11:23 AM, David Vorick <david.vorick@gmail.com> wrote:

-------------------------------------
Agreed.

In contrast, BIP37 as used today is totally decentralized, and can me 
made much more secure, private, and scalable -- without giving up the 
utility of unconfirmed transactions.

Please don't read into this statement a belief that all the coffees 
should go on the chain, or that the security or privacy of BIP37 compare 
favorably to any other particular thing.

https://docs.google.com/presentation/d/13MzUo2iIH9JBW29TgtPMoaMXxeEdanWDfi6SlfO-LlA



On 1/5/2017 6:04 PM, bfd--- via bitcoin-dev wrote:


-------------------------------------
On Fri, Sep 15, 2017 at 10:14 AM, Adam Back via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


It depends on what software that the general user-base is using (especially
exchanges).  If a majority of miners have deployed a hidden soft fork, then
the soft fork will only last as long as they can maintain their majority.

If they drop below 50%, then the majority of miners will eventually make
and then build on a block that is invalid according to their hidden soft
fork rules.

If the userbase doesn't support a censorship soft fork, then it will only
last as long as a majority of miners support it.  Once the cartel loses its
majority, there is a strong incentive for members to disable their soft
fork rule.  Any that don't will end up mining a lower POW, but valid, chain.

Users updating their nodes to enforce the soft fork is what makes the soft
fork irreversible (without a hard fork).



It's only a hard fork to reverse if the community is enforcing the soft
fork.  Forking off a minority of miners doesn't make it a hard fork.


-------------------------------------

For what benefit? If your script actually uses all the items on the stack, and if your script is not written in such a way as to allow malleability (which cannot be prevented in general), then they’re equivalent. Using weight instead of depth only needlessly restricts other parties to select a witness size up-front.

And to be clear, signing witness weight doesn’t mean the witness is not malleable. The signer could sign again with a different ECDSA nonce. Or if the signer is signing from a 2-of-3 wallet, a common scenario I hope, there are 3 possible key combinations that could be used. If using MBV, a 3-element tree is inherently unbalanced and the common use case can have a smaller proof size.

Witnesses are not 3rd party malleable and we will maintain that property going forward with future opcodes.


Any time all parties are not online at the same time in an interactive signing protocol, or for which individual parties have to reconfigure their signing choices due to failures. We should not restrict our script signature system to such a degree that it becomes difficult to create realistic signing setups for people using best practices (multi-key, 2FA, etc.) to sign. If I am a participant in a signing protocol, it would be layer violating to treat me as anything other than a black box, such that internal errors and timeouts in my signing setup don’t propagate upwards to the multi-party protocol.

For example, I should be able to try to 2FA sign, and if that fails go fetch my backup key and sign with that. But because it’s my infrequently used backup key, it might be placed deeper in the key tree and therefore signatures using it are larger. All the other signers need care is that slot #3 in the witness is where my Merkle proof goes. They shouldn’t have to restart and resign because my proof was a little larger than anticipated — and maybe they can’t resign because double-spend protections!


-------------------------------------
If 'x' is public, that makes it identifiable and privacy-losing across
inputs.

To avoid "re-use" I suppose you'd want to sign some message like
`HMAC("ownership proof", H(A || x) )` instead. Otherwise any signature you
make using `A` ends up being used as a proof you don't know the input(this
seems like just details but to be more clear)...

To reiterate:

Sign `HMAC("ownership proof", H(A || x) )` using `A`. Public verifiers see
`HMAC("ownership proof", some_random_hash_connected_to_A )` and the HWW
that owns that input can recreate `some_random_hash_connected_to_A` by `H(A
|| x) )`

On Mon, Aug 21, 2017 at 2:36 PM, Jochen Hoenicke <hoenicke@gmail.com> wrote:

-------------------------------------
Bitcoin Classic only activates if 75% of the network adopts it. That
is not irresponsible or dangerous. It would only be dangerous if it
activates at 50%, because that would create a situation where its not
clear which side of the fork has the most proof of work.

On 1/7/17, Eric Lombrozo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Mon, Nov 20, 2017 at 5:24 PM, Praveen Baratam via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

That is effectively what segwit does, upto engineering minutia and
compatibility details.

Segwit does not serialize transactions in to a data structure where
signatures are separated from the rest of the transaction data; this
is a misunderstanding.  The "segregated" refers to them being excluded
from the TXID.   The serialization of segwit on the p2p network in
transactions and in blocks encodes the witness field inside the
transactions, immediately prior to the nlocktime field.

-------------------------------------
I am in support of having multiple PoW forks to choose from, but it is
indeed problematic to have one chain running a rotation of algorithms.

The reason I support multiple algos is because we don't want an attacker
secretly making asics ahead of time in the event of an emergency PoW fork.
We want them to be uncertain which of a large selection of algorithms are
going to be used, making pre-emptive asic manufacturing more difficult.

But once an algorithm is chosen that should be the end of the story, just
one algo without rotation.
-------------------------------------
Hi José

A similar/related proposal got posted last month : https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014396.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014396.html>
The Bech32 encoded transaction position reference allows to solve similar use cases (tx pointers) with a strong error-detection/correction using just 14 chars (encoded state, including error correction / unencoded 40bits).

/jonas
-------------------------------------
merge it with some small pushback - allow segwit to activate in Aug, then
"upgrade" the hard fork to be "spoonet in 18 months" instead.

On Fri, Mar 31, 2017 at 11:03 PM, Samson Mow via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Monday, 19 June 2017 14:26:46 CEST bfd--- via bitcoin-dev wrote:

Why would it not be needed? Any SPV client (when used as a payment-receiver) 
requires this from a simple usability point of view.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
On Fri, Sep 29, 2017 at 11:10 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Wouldn't this RBF loop behave pretty much the same in the Monopolistic
Price Mechanism? (I haven't grokked RSOP yet.)

In fact, so long as RBF works, isn't it possible to raise Pay-Your-Bid fees
and Monopolistic Price fees over time to express the time curve preference?



-------------------------------------
On Tue, Jul 04, 2017 at 09:30:26PM -0400, shaolinfry via bitcoin-dev wrote:

If there are miners that start doing 1 second timestamp advances, it would be
simpler (and probably safer) to require a minimum block time spacing of say
30 seconds or 1 minute, and orphan blocks that are too close in time and more
than say an hour behind real-time.

I cannot picture any realistic scenario in which an attempt to block activation
in this way is in anything other than a very expensive temper tantrum for any
miners foolish enough to attempt it.

It *might* be a delay tactic as a 'nuclear option' attack vector for a mining
cabal to run up the difficulty so high as to make it impractical to mine any
new blocks after the adjustment, but there are plenty of altcoins that have
hardforked and gotten along just fine after the same kind of thing due to
profit-switching pools.





-------------------------------------
Eric Voskuil via bitcoin-dev schreef op zo 29-01-2017 om 11:37 [-0800]:

What on first sight seems trivial may be totally different when looking
deeper. People here seem to not realise that a lot of data centers (the
IAAS ones) just are just grouping the computers and provide remote
access. The client have full control over the machines. The center thus
just provides the hardware, the power and the internet access. They
typically don't inspect your internet traffic only reduce the speed if
you are going above certain threshold. Additionally there are countries
like Iceland that specifically make laws to not let the government get
power over data and network traffic in data centers.
Domestic ISP services typically want to prioritize traffic and thus have
most of the time network traffic deep packet inspection (DPI)
capabilities. These are thus much easier forced by government to filter
certain traffic. Additionally these companies often fall under
telecommunication laws also given government more control over them than
in a typical data center.

I host my Bitcoin node in a German datacenter and am sure it is more
censorship resistant that a node going through any American ISP.

greets,
Staf.

-------------------------------------
On 02/13/2017 12:47 AM, Pieter Wuille wrote:

As I said, it *may* be desirable, but it is *not* backward compatible,
and you do not actually dispute that above.

There are other control messages that qualify as "optional messages" but
these are only sent if the peer is at a version to expect them -
explicit in their BIPs. All adopted BIPs to date have followed this
pattern. This is not the same and it is not helpful to imply that it is
just following that pattern.

As for DOS, waste of bandwidth is not something to be ignored. If a peer
is flooding a node with addr message the node can manage it because it
understands the semantics of addr messages. If a node is required to
allow any message that it cannot understand it has no recourse. It
cannot determine whether it is under attack or if the behavior is
correct and for proper continued operation must be ignored.

This approach breaks any implementation that validates traffic, which is
clearly correct behavior given the existence of the version handshake.
Your comments make it clear that this is a *change* in network behavior
- essentially abandoning the version handshake. Whether is is harder to
maintain is irrelevant to the question of whether it is a break with
existing protocol.

If you intend for the network to abandon the version handshake and/or
promote changes that break it I propose that you write up this new
behavior as a BIP and solicit community feedback. There are a lot of
devices connected to the network and it would be irresponsible to break
something as fundamental as the P2P protocol handshake because you have
a feeling it's going to be hard to maintain.

e

-------------------------------------
Good morning Ben,

I am not Mark, and I am nowhere near being a true Core developer yet, but I would like to point out that even under a 51% attack, there is a practical limit to the number of blocks that can be orphaned.  It would still take years to rewrite history from the Genesis block, for instance.

What little data we have (BT1 / BT2 price ratio on BitFinex) suggests that tokens solely on the 2X chain will not be valued as highly as tokens solely on the Core chain.  As miners generate tokens that are only for a specific chain, they will have higher incentive to gain tokens on the Core chain rather than the 2X chain.

As is commonly said, hodling is free, whereas mining is not.  Hodlers have much greater power in hardfork situations than miners have: simply by selling their tokens on the 2X chain and not in the Core chain, hodlers can impose economic disincentives for mining of the 2X chain.

Miners can switch to BCH, but that is valued even less than BT2 tokens are, and thus even less attractive to mine on.

We should also pay attention, that BCH changed its difficulty algorithm, and it is often considered to be to its detriment due to sudden hashpower oscillations on that chain.  We should be wary of difficulty algorithm changes, as it is the difficulty which determines the security of the chain.

--

If we attempt to deploy a difficulty change, that is a hardfork, and hodlers will be divided on this situation.  Some will sell the tokens on the difficulty-change hardfork, some will sell the tokens on the non-difficulty-change hardfork.  Thus the economic punishment for mining the 2X chain will be diluted due to the introduction of the difficulty-change hardfork, due to splitting of the hodler base that passes judgment over development.

Thus, strategy-wise, it is better to not hardfork (whether difficulty adjustment, PoW change, or so on) in response to a contentious hardfork, as hodlers can remain united against or for the contentious hardfork.  Instead, it is better to let the market decide, which automatically imposes economic sanctions on miners who choose against the market's decision.  Thus, it is better to simply let 2X die under the hands of our benevolent hodlers.

Later, when it is obvious which fate is sealed, we can reconsider such changes (difficulty adjustment, PoW change, block size) when things are calmer.  However, such changes cannot be safely done in response to a contentious hardfork.

--

If indeed the Core chain is eradicated, then Bitcoin indeed has failed and I would very much rather sell my hodlings and find some other means to amuse myself.

Regards,
ZmnSCPxj
-------------------------------------
Good morning Paul and Chris,


Assuming there are three large mining groups who will ruthlessly want to undercut their competition, and with roughly 33% of total hashpower each (with the remaining 1% being some negligible hoi polloi), then one strategy to undercut your competitors would be to upvote only your own withdrawals and downvote that of your competitors.  A miner using this strategy hopes that the other miners will give up on withdrawing their own coin and trade their sidecoins at a discount to the undercutting miner.  That is, it is a hostage attempt of the sidecoin funds of the other miners.

In the case of three large mining pools that mistrust one another, then, no withdrawal would ever push through and drivechains stabilize to one-way pegs.

Now suppose that two of the mining pools collude.  They join their withdrawals into a single withdrawal proposal and upvote that, while downvoting the withdrawal of the third miner.  I observe that this is an opposite disaster: the 66% colluding miners can instead decide to simply outright make an invalid withdrawal of all funds, split up in half between themselves.

--

But three exactly equal mining pools is unnatural, for that matter

Suppose that there are three mining pools: A with 34%, B with 33%, C with 32%, and the hoi polloi making up the remaining 1%.  Those three pools cannot safely let the others withdraw funds.

Suppose A colludes with C to join their withdrawal proposals and their hashpower to withdraw.  This means that B can be pressured to sell its sidecoins for a discount to the joint coalition of A and C, since B cannot withdraw its own coins.  This lowers the profitability of B, causing grinders to leave them in favor of either A or C.  Since A is slightly larger than C, it is slightly more preferable, so it grows in size slightly more than C does.  Eventually B dies due to the coalition of A and C.  A and C are the only remaining pools, with A slightly larger than C.  In this case, A can break from the coalition and squeeze C of its sidecoins, since only A can withdraw (as it has more hashpower than C).  Again, grinders will leave C for A.  A rational C that is capable of considering this possible future path will thus never ally with A in a coalition to crush B, as it will then be next in line for being destroyed.

Similar analyses for coalitions of (B, C) and (A, B).

Knowing this, and knowing that they will end up sidecoin bagholders if they cannot withdraw coins, all miners decide to collude together and put all their withdrawals into a single withdrawal proposal.  But this removes any check-and-balance that the single withdrawal proposal is truthful to the sidechain: that is, the single coalition of A,B, and C can decide to just steal all sidechain funds and reassign them in proportion to their hashpower.  This might be stable at end-of-life for the sidechain where all ordinary users of the sidechain have exited it, but is otherwise a theft risk if the sidechain is operating normally.

Regards,
ZmnSCPxj
-------------------------------------
On Thu, May 11, 2017 at 3:13 PM, Jonas Schnelli via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

The instructions for relay addresses should not instruct you to relay
these addresses but rather that you should relay addresses you would
connect to, under the generalized assumption that if it is useful to
you it will be useful to others.

This avoids instructing someone who might not consider
non-node-network peers useful from being directed by the BIP to relay
things that they don't find useful. (In particular, it means that the
obvious implementation of just throwing out the 'useless' information
works fine.)  I think would better reflect what people are likely to
actually do.

-------------------------------------
Hi!

Up to now, I have purposefully been running bitcoin releases prior to
0.13.1 as a way to avoid the (possible) segwit activation, at least
until such time as I personally am comfortable with it.

At this time, I would like to have some of the more recent features, but
without the possibility that my node will activate segwit, until I
choose to.

As I understand it, there is not any user setting that can disable
segwit from activating on my node.  If there was, I would use it.
Please correct me if wrong.

I am here to ask what is the simplest code change (fewest LOC changed) I
can make to 0.14.2+ code that would disable segwit from activating and
keep my node acting just like a legacy node with regards to consensus
rules, even if/when the rest of the network activates segwit.

I think, more generally, the same question applies to most any Bip9
versionbits feature.

I'm not looking for reasons NOT to do it, only HOW to do it without
unwanted side-effects.  My first untested idea is just to change the
segwit nTimeout in chainparams.cpp to a date in the past.  But I figured
I should ask the experts first.   :-)

thanks.


ps: full disclosure:  I may be the only one who wants this, but if
successful, I do plan to release my changes in case someone else wishes
to run with status-quo consensus rules.



-------------------------------------
One thing about BIP148 activation that may be affected by this is the
fact that segwit signalling non-BIP148 miners + BIP148 miners may hold
majority hash power and prevent a chain split. With this SF, that will
no longer be the case, right? Or am I completely confused on the
subject?

On Wed, Jun 7, 2017 at 9:56 AM, James Hilliard via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Bryan,

Interesting argument, but I think it is not an accurate comparison. People
usually mean that, for example, say 2^80 of the original operations are
needed rather than the intended 2^128 to find a collision. This could be
the case in a broken algorithms such as a toy SHA variant with too small
states and too few rounds. These kind of attacks usually refer to that
something is learned from prior evaluations that be should't be possible to
be learned. For example, if someone could somehow construct a pre-image in
256 evaluations, getting one additional bit right at a time. Similar to a
cheap combination lock where you can figure out the correct 4 digits in a
worst case of 4*10 attempts by "feeling" it, rather than having to do the
intended 10,000 attempts. That's the kind of thing that would be called an
"attack".

Here, however, we are talking about making the individual operations
cheaper by a constant of ~20%, not changing the number of operations. That
doesn't qualify as an attack in the sense that you mean.

Best,
Timo




On Thu, Apr 6, 2017 at 5:11 AM, Bryan Bishop via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------


Err, no, that's what happens when you double click the Ethereum icon
instead of the Bitcoin icon.  Just because you run "Bitcoin SPV"
instead of "Bitcoin Verify Everyone's Else's Crap" doesn't mean you're
somehow going to get Ethereum payments.  Your verification is just
different and the risks that come along with that are different.  It's
only confusing if you make it confusing.


If every block that is mined for them is deliberately empty because of
an attacker, that's nonfunctional.  You can use whatever semantics you
want to describe that situation, but that's clearly what I meant.


As above, if someone operates Bitcoin in SPV mode they are not
magically at risk of getting Dashcoins.  They send and receive
Bitcoins just like everyone else running Bitcoin software.  There's no
confusion about it and it doesn't have anything to do with hashrates
of anyone.  It is just a different method of verification with
corresponding different costs of use and different security
guarantees.


We're already fucked, China has 61% of the hashrate and the only thing
we can do about it is to wait for the Chinese electrical
supply/demand/transmission system to rebalance itself.  Aside from
that little problem, mining distributions and pool distributions don't
significantly factor into the blocksize debate.  The debate is a
choice between nodes paying more to allow greater growth and adoption,
or nodes constraining adoption in favor of debatable security
concerns.


Do you really consider it choosing when there is only a single option?
 And even if there was, the software would choose it for you?  If it
is a Bitcoin client, it follows the Bitcoin blockchain.  There is no
BU blockchain at the moment, and Bitcoin software can't possibly start
following Ethereum blockchains.


Yes you do, if the segment options are known (and if they aren't,
running a node likely won't help you choose either, it will choose by
accident and you'll have no idea).  You would get to choose whose
verifications to request/check from, and thus choose which segment to
follow, if any.


This is only true for the small minority that actually need that added
level of security & confidence, and the paranoid people who believe
they need it when they really, really don't.  Some guy on reddit
spouted off the same garbage logic, but was much quieter when I got
him to admit that he didn't actually read the code of Bitcoin that he
downloaded and ran, nor any of the code of the updates.  He trusted.
*gasp*

The average person doesn't need that level of security.  They do
however need to be able to use it, which they cannot right now if you
consider "average" to be at least 50% of the population.


Demand comes from usage and adoption.  Neither can happen us being
willing to give other people the option to trade security features for
lower costs.


Great.  Somehow I think Bitcoin's future involves very few more people
like you, and very many people who aren't paranoid and just want to be
able to send and receive Bitcoins.


No, it has its value for many, many reasons, trustless properties is
only one of them.  What I'm suggesting doesn't involve giving up
trustless properties except in your head (And not even then, since you
would almost certainly be able to afford to run a node for the rest of
your life if Bitcoin's value continues to rise as it has in the past).
And even if it did, there's a lot more reasons that a lot more people
than you would use it.


Are you really this dense?  If the cost of on-chain transactions
rises, numerous use cases get killed off.  At $0.10 per tx you
probably won't buy in-game digital microtransactions with it, but you
might buy coffee with it.  At $1 per tx, you probably won't buy coffee
with it but you might pay your ISP bill with it.  At $20 per tx, you
probably won't pay your ISP bill with it, but you might pay your rent.
At $300 per tx you probably won't use it for anything, but a company
purchasing goods from China might.  At $4000 per tx that company
probably won't use it, but international funds settlement for
million-dollar transactions might use it.

At each fee step along the way you kill of hundreds or thousands of
possible uses of Bitcoin.  Killing those off means fewer people will
use it, so they will use something else instead.


No they don't.  They only give people the option to pay more for
higher security or to accept lower security and use Bitcoin anyway.


So far as anyone has presented actual numbers, there's no reason to
believe larger blocksizes endanger anything of the sort, even if I
agreed that that was Bitcoin's primary proposition.  And I don't
believe we need an insignificant capacity increase, I used to think
that way though.  I strongly believe we can handle massive increases
by adjusting our expectations of what nodes do, how they operate, how
they justify the price of their services, and what levels of security
are available and appropriate for various levels of transaction risk.


Segwit is a miniscule blocksize increase and wholly inadequate
compared to the scope of the problem.  Good for other reasons, though.
Lightning is not Bitcoin, it is something different(but not bad IMO)
that has different features and different consequences.  I guess you
think it is ok that if your lightning node goes offline at the wrong
time, you could lose funds you never transacted with in the first
place?  No?  Oh, then you must be ok with lightning hub centralization
then as well as paying a monthly fee to lightning hubs for their
services.  Wait, that sounds an awful lot like visa....

I have no idea what you're referring to with the pre-loaded wallets point.


On Thu, Mar 30, 2017 at 9:21 PM, Luv Khemani <luvb@hotmail.com> wrote:

-------------------------------------
Hello everyone,

I've been thinking of an alternative to possibly get Segwit activated
sooner: bribing the miners. This proposal may not be necessary if everyone
is already set on doing a UASF, but  miners seem to optimize for short-term
profits and this may make it easier for BitMain to accept its fate in
losing the ASICBoost advantage.

Here is a possible trustless contract protocol where contributors could
pledge to a Segwit bounty which would be paid out to miners iff Segwit is
activated, else the funds are returned to the contributor:

# Setup

- The contributor picks a possible future height where Segwit may be
activated and when the funds should be released, H.
- The contributor chooses a bounty contribution amount X.
- The contributor generates 3 private keys (k1, k2, and k3) and
corresponding pubkeys (K1, K2, and K3).
- The contributor creates and broadcasts the Funding Transaction, which has
2 outputs:
  * Output 0, X BTC, P2SH redeem script:
    <H> CHECKLOCKTIMEVERIFY DROP
    <K1> CHECKSIGVERIFY
  * Output 1, 0.00001 BTC, P2SH redeem script:
    <H> CHECKLOCKTIMEVERIFY DROP
    <K2> CHECKSIGVERIFY
- The contributor builds the Segwit Assertion Transaction:
  * nTimeLock set to H
  * Input 0, spends from Funding Transaction output 1, signed with k2,
SIGHASH_ALL
  * Output 0, 0.00001 BTC, P2WPKH using K3
- The contributor builds the Bounty Payout Transaction:
  * nTimeLock set to H
  * Input 0, spends from Funding Transaction output 0, signed with k1,
SIGHASH_ALL
  * Input 1, spends from Segwit Assertion Transaction output 0, signed with
k3, SIGHASH_ALL
  * No outputs, all funds are paid to the miner
- The contributor publishes the Segwit Assertion Transaction and Bounty
Payout Transaction (with signatures) somewhere where miners can find them

# Process

1. After the setup, miners can find Funding Transactions confirmed on the
chain, and verify the other 2 transactions are correct and have valid
signatures. They can sum all the valid bounty contracts they find to factor
into their expected mining profit.
2A. Once the chain reaches height H-1, if Segwit has activated, miners can
claim the bounty payout by including the Segwit Assertion and Bounty Payout
transactions in their block H. Since Segwit has activated, the output from
the Segwit Assertion tx can be spent by the Bounty Payout, so both
transactions are valid and the miner receives the funds.
2B. If Segwit has not activated at height H, Input 1 of the Bounty Payout
is not valid since it spends a P2WPKH output, preventing the miner from
including the Bounty Payout transaction in the block. (However, the output
of the Segwit Assertion tx can be claimed since it is treated as
anyone-can-spend, although this is not an issue since it is a very small
amount). The contributor can reclaim the funds from Output 0 of the Funding
tx by creating a new transaction, signed with k1.

# Notes

- This is likely a win-win scenario for the contributors, since Segwit
activating will likely increase the price of Bitcoin, which gives a
positive return if the price increases enough. If it does not activate, the
funds will be returned so nothing is at risk.
- Contributors could choose H heights on or slightly after an upcoming
possible activation height. If contributors pay out to many heights, then
the bounty can be split among many miners, it doesn't have to be
winner-take-all.
- If Segwit does not activate at H, the contributor has until the next
possible activation height to claim their refund without risking it being
taken by another miner. This could be outsourced by signing a refund
transaction which pays a fee to some third-party who will be online at H
and can broadcast the transaction. If the contributor wants to pay a bounty
for a later height, they should create a new contract otherwise a miner
could invalidate the payout by spending the output of the Segwit Assertion.

Thanks, I'd like to hear everyone's thoughts. Let me know if you find any
glaring flaws or have any other comments.
Matt
-------------------------------------


This is ugly and actually broken, as different script path may require different number of stack items, so you don’t know how many OP_TOALTSTACK do you need. Easier to just use a new witness version


I like the idea to have an unified global limit and suggested a way to do it (https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html). But I think this is off-topic here.





In any case, I think maintaining static analysability for global limit(s) is very important. Ethereum had to give up their DAO softfork plan at the last minute, exactly due to the lack of this: http://hackingdistributed.com/2016/06/28/ethereum-soft-fork-dos-vector/

Otherwise, one could attack relay and mining nodes by sending many small size txs with many sigops, forcing them to validate, and discard due to insufficient fees.

Technically it might be ok if we commit the total validation cost (sigop + hashop + whatever) as the first witness stack item, but that’d take more space and I’m not sure if it is desirable. Anyway, giving up static analysability for scripts is a fundamental change to our existing model.


Without the limit I think we would be DoS-ed to dead




You can find it here: https://github.com/jl2012/bitcoin/commits/vault
https://github.com/jl2012/bitcoin/commit/f3f201d232d3995db38e09b171e4d1dea8d04ad2

But this does more than your proposal as it allows users adding extra scripts when spending a coin. The rationale is described in the revised BIP114:
https://github.com/jl2012/bips/blob/vault/bip-0114.mediawiki#Additional_scripts_in_witness

So to make it functionally comparable with your proposal, the IsMSV0Stack() function is not needed. The new 249-254 lines in interpreter.cpp could be removed. The new 1480-1519 lines could be replaced by a few lines copied from the existing P2WSH code. I can make a minimal version if you want to see how it looks like





-------------------------------------
Hi AJ,
That's outstanding. I am glad to see that there is actually somebody
who has made some progress.

Great idea! Block space as a resource is under-analyzed.


I am not joking when I say that in 3 to 8 years, I want to be able to
verify my transaction through green blocks that are generated locally
by my neighbor through the excess capacity of his solar panels or by
an NGO pool that promotes solar deployment around the equator.


Yes, just selecting all SegWit signaling hash power instead of picking
individual Authorities would be helpful on preferredminer.com


Absolutely, considering the recent language used in opinions by the
ECB and drafts by the EU I see them assembling the artillery. I
wouldn't be surprised if they start target practice next year. That
will mean that commercial interest must have a way to transact on
somewhat regulated space.


I would call it regulated block space or regulated consensus space. I
hope that we can do that on a deeper level, as part of the p2p
protocol layer.


That's a distinct function, e.g. at least some communities charge a tax. [1]
I fear it is more likely that a business, say Coinbase, will approach
a "miner" and just say "we pay 100 USD for a KB to your bank account,
here are our transactions with no fee". It will literally be an
off-chain fee. That's what I mean by "secondary market". This would be
one of the least appealing scenarios.


Spot on, that's why this should receive some attention before it
becomes urgent. I think there is much more to it that we are missing
at the moment, e.g. Tom: "Using xthin/compact blocks miners only send
a tiny version of a block which then causes the receiving node to
re-create it using the memory pool."


[1] http://thebitcoin.foundation/declaration.txt




-------------------------------------
Yes, if you use a witness script version you can save about 40 witness bytes by templating the MBV script, which I think is equivalent to what you are suggesting. 32 bytes from the saved hash, plus another 8 bytes or so from script templates and more efficient serialization.

I believe the conservatively correct approach is to do this in stages, however. First roll out MBV and tail call to witness v0. Then once there is experience with people using it in production, design and deploy a hashing template for script v1. It might be that we learn more and think of something better in the meantime.


-------------------------------------
I agree: a new difficulty algorithm starting from zero is inconceivably 
rushed. But it's also inconceivable to not have one ready in two months 
if my understanding of our current situation is correct. Is there any 
complaint or suggestion about this algorithm or the appropriate goals of 
an ideal difficulty algorithm? I feel like there is a discussion that needs 
to be hashed out before a draft BIP at the HF page, but I do not know 
where is best or who would be interested. If the community shows it is 
receptive and supportive I think I could get Karbowanek coin to put it 
into live action and solicit hash attacks. They are currently using a 
simpler N=17 like this since last November. They have tested all my 
attempted improvements the past few months, so they are familiar with all 
the in and outs. 

This particular coin split is looking different. Assuming users currently 
prefer SW, it still seems like miner support is going to convince enough 
users that SegWit2x is a worthy if not superior alternative. The result 
could be both coins oscillating with long delays, as long as the price is 
similar. As soon as it is not similar, maybe the loser will be in a death 
spiral, pushed to the margin like previous coins. This might be a bitcoin 
feature. But the 2016 window seems like it is too brutal. It seems like it 
will result in an accidental winner before the better coin can be 
determined by more rational means.

-------------------------------------
This topic has come up several times in recent years. While it is well intentioned, it can have devastating outcomes for people that want to save long term. If such a system were implemented, it would force people to move funds around in order to not get nullified. In that process, it introduces multiple opportunities for errors. Cold storage should be able to stay cold. I personally would be apprehensive about implementing this kind of a system.

...via Android



From: Teweldemedhin Aberra via bitcoin-dev
Sent: Monday, December 11, 1:04 PM
Subject: [bitcoin-dev] BIP - Dead Man's Switch
To: bitcoin-dev@lists.linuxfoundation.org


It is estimated that about 4 million of the about 16.4 Bitcoins ever mined are lost forever because no one knows the private keys of some Bitcoin addresses. This effectively mean there are actually only 14.4 million Bitcoins in circulation even though 16.4 million are mined. There is no way of eliminating the human errors that cause these losses of Bitcoin from circulation, while the number of Bitcoin that will ever be mined is capped at 21 million. This means the total number of Bitcoins that are in circulation will eventually become zero, bringing the network to an end.
The solution this BIP proposes is to implementing a dead man's switch to Bitcoin addresses. The dead man's switch causes the Bitcoins assigned to dormant addresses to automatically expire. A Bitcoin address is deemed dormant if it is not used in transactions for some fixed length of time, say ten years.
The calculation of the miner's reward should take into account the Bitcoins that has expired. This means there is a possibility that miner's reward can increase if sufficient number of Bitcoins expire.

Ref:
http://fortune.com/2017/11/25/lost-bitcoins/


Virus-free.
        <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=link>
www.avast.com




-------------------------------------
This isn't BIP material, as it merely describes a local policy.

(BIP125 itself is also local policy, but one that involves standardisation 
since it expresses how wallets interoperate with nodes with that policy.)

If you wish to suggest this policy change, you should just implement it and 
open a merge/pull request on the applicable project.

Luke


On Sunday 02 July 2017 8:35:22 PM Rhavar via bitcoin-dev wrote:

-------------------------------------
I agree with Greg.  What is happening is a cause for celebration: it is the
manifestation of our long-desired fee market in action.  That people are
willing to pay upwards of $100 per transaction shows the huge demand to
transact on the world's most secure ledger. This is what success looks
like, folks!

Now that BTC is being phased out as a means of payment nearly everywhere
(e.g., Steam dropping BTC as a payment option) (to be replaced with the
more-suitable LN when ready), I'd propose that we address the stuck
transaction issue by making replace-by-fee (RBF) ubiquitous.  Why not make
every transaction RBF by default, and then encourage via outreach and
education other wallet developers to do the same?

The frustration with BTC today is less so the high-fees (people realize
on-chain transactions in a secure decentralized ledger are necessarily
costly) but by the feeling of helplessness when their transaction is
stuck.  Being able to easily bump a transaction's fee for users who are in
a hurry would go a long way to improving the user experience.

Paul.


On Thu, Dec 21, 2017 at 2:44 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
For the reasons Pieter listed, an explicit part of our version handshake and protocol negotiation is the exchange of otherwise-ignored messages to set up optional features.

Peers that do not support this ignore such messages, just as if they had indicated they wouldn't support it, see, eg BIP 152's handshake. Not sure why you consider this backwards incompatible, as I would say it's pretty clearly allowing old nodes to communicate just fine.

On February 13, 2017 10:36:21 AM GMT+01:00, Eric Voskuil via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On 2/22/17, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:


What problem does this try to solve, and what does it have to do with bitcoin?

-------------------------------------
I don't have anything interesting to add, except that I have been using 'bits' on my site for over 3 years. It's a great unit that people quickly adapt to, and it's far more convenient. When dealing with large amounts of money, people have no problem naturally thinking in "thousand bits" or "million bits" (a bitcoin).

I would highly encourage it to be a default everywhere. Consistency is really important.

Also slightly unrelated, but the whole "sat/B" thing for fees is such a clusterfuck. Half the time it's used as "vbyte" and half the time actual bytes. Users are constantly confused because of explorers and wallet and stuff all showing it inconsistently. I would suggest there that there is a "standard" of "bits per kiloweight" (i.e. how many bits of fees to pay for a transaction that is 1000 weight)

-Ryan

-------------------------------------
There have been some other proposals to deal with this such as
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2012-June/001506.html
that may be possible to implement in existing miners.

On Tue, Oct 3, 2017 at 9:52 AM, 潘志彪 via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Mon, May 22, 2017 at 12:05 AM, Russell O'Connor via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Ya know, when you're building a Merkle Trie that's exactly the primitive
you need.

In my own construction the assumption is that the values are already hashed
down to 256 bits when they're passed in, and the tags (which are currently
done by sacrificing bits instead of using tags, that needs to be fixed)
include three states for either side: empty, unary, or middle. Three of
those possibilities are unreachable (empty/empty, empty/unary, unary/empty)
so there are 6 possible tags needed. This approach essentially skips doing
the unary hashes, a further performance improvement. There doesn't appear
to be any downside in leveraging this trick as long as tags are cheap.
-------------------------------------
Yes, it’s similar. I’ll quote your design if/when I formalise my BIP. But it seems they are not the same: yours is opt-out, while mine is opt-in.

However, my proposal in nowhere depends on standardness for the protection. It depends on the new network enforcing a new SignatureHash for txs with an nVersion not used in the existing network. This itself is a hardfork and the existing network would never accept such txs.

This is to avoid requiring any consensus changes to the existing network, as there is no guarantee that such softfork would be accepted by the existing network. If the new network wants to protect their users, it’d be trivial for them to include a SignatureHash hardfork like this, along with other other hardfork changes. Further hardforks will only require changing the network characteristic bit, but not the SignatureHash.

If the hardfork designers don’t like the fix of BIP143, there are many other options. The simplest one would be a trivial change to Satoshi’s SignatureHash, such as adding an extra value at the end of the algorithm. I just don’t see any technical reasons not to fix the O(n^2) problem altogether, if it is trivial (but not that trivial if the hardfork is not based on segwit)





-------------------------------------
On 7/17/2017 2:49 PM, Alex Morcos wrote:

Fine, but, before the roadmap itself, I wrote exactly about why I
thought we should update it. Evidently you disagree with the horse, but
it is in front of the cart where it belongs.



It isn't a "roadmap" anymore -- I changed it to a "forecast".

And I edited the drivechain part to emphasize only that mainchain space
would likely be freed as defectors leave for an alt-chain. The departing
individuals (ir hardcore LargeBlockers) will leave, despite a
security-model NACK from anyone here (in fact, it would probably only
encourage them). That leaves more space for those who remain.



Drivechain  (c. bitcointalk Feb 2014, blog Nov 2015) is actually much
older than "the" SegWit to which you refer.  As for being "conservative"
and "flexible", I have tried to do everything I know -- Scaling
conferences, in-person discussions, papers, posts, and presentations,
adding BMM, and posting here for additional peer review. I'm sure you
have lots of ideas about how it could be more conservative and/or
flexible, which I would love to hear.

But again I think people are getting hung up on the drivechain part --
it can be easily taken out, I just thought that, if the plan included
more overall flexibility for industry, then it would help deter network
splits and scaling drama.

Paul





-------------------------------------
On 27/03/17 18:12, Btc Ideas via bitcoin-dev wrote:


This would encourage miners to make their own tiny junk transactions
to fill up their blocks, perhaps leaving larger, more space-efficient
transactions in the mempool.



"Good" miners should probably build upon the block with a set of
transactions more similar to what they themselves would include based
on their mempool at the time.  However, miners don't have an incentive
to do so today.  Instead, they may be better off building upon the
block that leaves the most valuable transactions in the mempool,
e.g. a small or empty block, and maybe leave some valuable
transactions in the mempool for the next miner.[1]  This issue could
possibly be addressed by a soft-fork that requires miners to pay a
portion of their fees to future miners.

[1]
https://freedom-to-tinker.com/2016/10/21/bitcoin-is-unstable-without-the-block-reward/

-- 
Stian



-------------------------------------



I believe BIP39 does an excellent job at reducing the amount of bitcoin permanently lost. Stolen funds can at least in theory be retrieved at some future date. There's a trade-off between having a backup process that is secure and one that people actually use. I don't know the right answer, and tend to agree it's better left to individual wallets to decide.

Sjors
-------------------------------------
On 02/13/2017 03:14 AM, Jonas Schnelli wrote:


The issue I raised is that it is not backward compatible. It sounds like
you agree but consider it a fair trade. My suggesting was that the BIP
be updated to reflect the lack of compatibility.


It doesn't need to be specified, most of Bitcoin is unspecified. The
version handshake establishes the negotiated version. It is not possible
to determine if a message is of the negotiated version before the
version is negotiated. All messages apart from this one have followed
that rule.


An incoming connection will be dropped due to invalid protocol and
potentially banned depending on the implementation.


Not working with peers not supporting BIP151 is the compatibility issue.
But it sort of seems the intent in this case is to rely on that
incompatibility (expecting connections to nonsupporting peers to fail as
opposed to negotiating).



I did consider the possibility, but there's this:

"Encryption initialization must happen before sending any other messages
to the responding peer (encinit message after a version message must be
ignored)."

https://github.com/bitcoin/bips/blob/master/bip-0151.mediawiki#specification

The BIP does not define "responding" and "requesting" peers, but:

"A peer that supports encryption must accept encryption requests from
all peers... The responding peer accepts the encryption request by
sending a encack message."

This implies the requesting peer is the peer that sends the message. You
seem to be saying that the requesting peer is the one that initiated
the connection and the responding peer is the connection receiver. If
this is the case it should be more clearly documented. But in the
case I experienced the "requester" of an encrypted session was also
the "receiver" of the connection.



Flexible is certainly one word for it. Another way to describe it is
dirty. Allowing invalid messages in a protocol encourages protocol
incompatibility. You end up with various implementations and eventually
have no way of knowing how they are impacted by changes. There could be
a range of peers inter-operating with the full network while running
their own sub-protocols. Given the network is public and strong
identification of peers is undesirable, the invalid messages would
reasonably just get sent to everyone. So over time, what is the
protocol? Due to certain "flexibility" it is already a hassle to
properly implement.


There is no reason to treat invalid messages differently based on where
they occur in the communication. After the handshake the agreed version
is known to both peers. As a result there is never a reason for an
invalid message to be sent. Therefore it is always proper to drop a peer
that sends an invalid message.


This was previously addressed (immediately below).



See above.



This is a misinterpretation. The failure to validate did not enable
anything except possibly some broken peers not getting dropped. None of
the protocol changes previously deployed require the older version peer
to allow invalid messages. While it may appear otherwise, due to a
particular implementation, it is never necessary to send a message to a
peer that the peer does not understand. The handshake gives each peer
the other peer's version. That obligates the newer peer to conform to
the older (or disconnect if the old is insufficient). That's the nature
of backward compatibility.



Yes, this is the purpose of version negotiation, which is why there are
version and verack messages. And this is also why, in the satoshi
client, two of the above messages are sent from the verack handler. The
feefilter message is sent dynamically but only if the peer's version
allows it.


I'm not sure I follow your question. The BIP should presumably declare a
version number if one is necessary.


In general you should set a version before it's ever live on the
network. But if it precedes the protocol version negotiation the
protocol version number is moot.

I've been asked to throttle the discussion in the interest of reducing
list volume. I think the issue is pretty clearly addressed at this
point, but feel free to follow up directly and/or via the libbitcoin
development list (copied).

e

-------------------------------------

Yes, micro-payments for online network services is precisely what LN is
best at.

Establishing a channel with each peer is too expensive.   But using LN to
micro-pay for high-quality peer services seems like it would aggregate very
well.

It would be great if this protocol was in-place and ready to go in or
around the same time LN is ready.   It would incentivize full nodes even
further than LN does, and allow the network to be strongly DDOS resistant.
-------------------------------------

*err, my bad that's unlikely to happen, if I remember correctly CPFP can only be done by the person you're sending the coins to. Coin-mixing seems the better option of the two, but shouldn't the BIP148 folks wait until it's clear that will be supported by exchanges?

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
Hi Ilya,


This proposal wouldn't work because bad actors can perform PoW just as cheaply as any other participant.


The transaction fee already acts as a mechanism to prevent spam. It is not a problem to have a lot of low value transactions in the mempool as thresholds can easily be set for them to be disregarded/expire - a 300MB  maxmempool size by default eliminates any real 'DDOS' risk. Spam only really becomes an issue when it enters the blockchain. If a spammer is willing to pay the tx fee to spam, they'd be willing to pay the PoW.


The only actors who can spam the blockchain at zero cost are the miners themselves (beyond the opportunity cost of including genuine fee paying transactions). They can even do it without their transactions ever hitting the mempool or including a fee, though this behaviour would be easy to spot.


If miners are colluding to spam the mempool or blocks in order to increase the average transaction cost overall there is little that can be done as the network relies on 51% of hashpower being honest. A miner creating spam transactions that enter the mempool has the risk that another miner would include it in a block and they would incur an economic cost if this happened.


I had an idea for a dynamic blocksize that required miners to pay a percentage of the transaction fees to the next mined block. Here is a link: https://lists.linuxfoundation.org/pipermail/bitcoin-discuss/2017-January/000123.html


If it was established that miners spamming blocks with transactions was an issue, this could be used as a disincentive as it means the cost for doing so becomes non-zero.


Regards,

John Hardy
john@seebitcoin.com

________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of Ilya Eriklintsev via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Wednesday, June 21, 2017 9:55 AM
To: bitcoin-dev@lists.linuxfoundation.org
Subject: [bitcoin-dev] BIP Idea : DDoS resistance via decentrilized proof-of-work

Hello everyone,

recently I have got an idea that in my opinion will improve bitcoin network, making it better store-of-value for growing cyberspace and cryptoeconomy. Sorry for longread below and thank you for your time.

Decentralized proof-of-work and DDoS resistance for Bitcoin

Abstract

By introducing some new block validation rules it is possible to make Bitcoin network more secure, decentralized and DDoS resistant. The idea is to modify simple proof-of-work puzzle in such a way that user transactions could be hardened with the same proof-of-work algorithm thus incentivising all the miners to include that particular transaction. Such mechanism will effectively give a handicap to every miner who includes "mined" transaction into next block, increasing probability of him getting block reward.

Problems and motivation

This document will address the issue of a continuous DDoS attack targeting the Bitcoin network, e.g. full nodes mempools constantly being overflowed with transactions carrying small value reduce system primary ability to transfer value (and hence making it perfect store of value). Valid transactions are cheap to create (in the sense of computational effort required) and no adequate mechanism exist to make transaction total value increase probably of its confirmation by the network.

Currently, miners decide which transactions to include in blocks because it's them who are securing Bitcoin network providing proof-of-work certificates stored inside every block header. Miners have to store the whole blockchain at all times, so one of the costs is storage which grows linearly with the transaction size (blockchain size as well). Another cost is network bandwidth which depends directly on the size of data to be communicated over.

The only incentive a person who wants to transfer his bitcoins is allowed to use is setting of transaction fee, that is going directly to the miner. This solution probably was intended to utilize free market (as implied by Satoshi introducing sequence numbers) to determine appropriate fees, but that is obviously not the case, in the current bitcoin network operating in full block capacity mode. This fee market deviates significantly from a free market premise (also attempts being made to make it closer, e.g. in BIP125 where Replace-By-Fee signaling is supposed to help in replacing "stuck" transactions with noncompetitive fee).

Currently, bitcoin network is susceptible to the DDoS attack of a kind. Adversary creating and translating into the network a lot of transactions carrying small value (e.g. only miners fee), will be able to impair the ability to transfer value for everyone in the world, should he has enough money to pay the fees. Miners would continue to work providing security for the network and new blocks will consist of transaction transferring negligible value. It's a major drawback because the cost of such attack doesn't grow asymmetrically with the cost of BTC asset.

Proposed solution

So how do we incentivize all miners to include our transaction carrying a lot of value in the next block? The only thing a miner supposed to do to get a reward is to produce Hashcash proof-of-work, thus providing cryptographic security guarantees for the whole Bitcoin blockchain. What if including our transaction in a block would reduce effort requirements for the miner produce valid block?

We could do so by extending the concept of proof-of-work, in such a way that we do not sacrifice security at all. Here are both descriptions proof-of-work as-is and to-be:

Standart proof-of-work: hash(previous block hash + current block target + current block metadata + current block transactions) < target

Decentralized proof-of-work: hash(previous block hash + current block target + current block metadata + current block transactions) - sum( FFFF - hash( previous block hash + raw_tx ) ) < target

It is possible to imagine completely mining agnostic proof-of-work, for example, the following PoW would do:

Distributed (mining-agnostic) proof-of-work: sum( FFFF - hash( previous block hash + current block target + current block metadata + signed_tx ) ) < target

Described protocol change could be implemented as user activated soft-fork (described in BIP148), introducing new blocks with the modified proof-of-work concept.

Economic reasoning

An adversary whose goal is to prevent the network from transferring value will have to compete with good users hash rate using same equipment good miners will use. And it's far more complicated than competing with others using the money to pay transaction fees.

In order to investigate probable consequences of protocol upgrade and stability of implied economical equilibrium, we need an adequate game theoretical model. Such model and numerical simulation results should be obtained and studied before any protocol change could be considered by the community.

To me it seems like a win-win solution for everyone owning BTC:

Miners benefit: as the result DDoS attack will be stopped, Bitcoin becomes perfect store-of-value finally. Political decentralization of hash rate will be incentivized as mining equipment becomes relevant to every user.
Users benefit: miners will have direct incentives to include transactions deemed important by their sender and supported by some amount of proof-of-work.

Sincerely yours, Ilya Eriklintsev.
-------------------------------------
On Sat, Sep 09, 2017 at 11:11:57PM +0200, Jorge Timn wrote:

So with Confidential Transactions, the only thing that's changed relative to a
normal Bitcoin transaction is that fact that the sum of input values is >= the
sum of output values is proven via a CT proof, rather than revealing the actual
sums. Other than that, CT transactions don't need to be any different from
regular transactions.

For CT to be a softfork, we have to ensure that each CT transaction's sum of
inputs and outputs is valid. An obvious way to do this is to have a pool of
"shielded" outputs, whose total value is the sum of all CT-protected outputs.
Outputs in this pool would appear to be anyone-can-spend outputs to pre-CT
nodes.

This gives us three main cases:

1) Spending unshielded outputs to CT-shielded outputs

Since the CT-shielded output's value is unknown, we can simply set their value
to zero. Secondly, we will add the newly CT-shielded value to the pool with an
additional output whose value is the sum of all newly created CT-shielded
outputs.


2) Spending CT-shielded outputs to unshielded outputs

Here one or more CT-shielded outputs will be spent. Since their value is zero,
we make up the difference by spending one or more outputs from the CT pool,
with the change - if any - assigned to a CT-pool output.


3) Spending CT-shielded outputs to CT-shielded outputs

Since both the inputs and outputs are zero-valued, to pre-CT nodes the
transaction is perfectly valid: the sum of coins spent is 0 BTC, and the sum of
coins created is also 0 BTC. We do have the problem of paying miners fees, but
that could be done with an additional CT output that the miner can spend, a
child-pays-for-parent transaction, or something else entirely that I haven't
thought of.



Suppose zero-valued outputs are prohibited. In case #3 above, if there are more
outputs than inputs, we need to add an additional input from the CT-shielded
pool to make up the difference, and an additional change output back to the
CT-shielded pool.

If shielded-to-shielded transactions are common, these extra outputs could
consume a significant % of the total blockchain space - that's a significant
cost. Meanwhile the benefit is so small it's essentially theoretical: an
additional satoshi per output is an almost trivial cost to an attacker.

Quite simply, I just don't think the cost-benefit tradeoff of what you're
proposing makes sense.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On Mon, Aug 28, 2017 at 3:50 PM, Riccardo Casatta via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

You are leaving a lot of bytes on the table.

The bits field can only change every 2016 blocks (4 bytes per header),
the timestamp can not be less than the median of the last 11 and is
usually only a small amount over the last one (saves 2 bytes per
header), the block version is usually one of the last few (save 3
bytes per header).

But all these things improvements are just a constant factor. I think
you want the compact SPV proofs described in the appendix of the
sidechains whitepaper which creates log scaling proofs.

-------------------------------------
On Thursday 28 September 2017 2:13:48 PM Andreas Schildbach via bitcoin-dev 
wrote:

Payment requests don't use and don't overlap with addresses. Maybe you could 
have an argument for serialising BIP70 payment requests in Bech32 as the new 
address format itself, but it doesn't make sense to talk about putting a 
Bech32 address *into* a payment request...

Luke

-------------------------------------
On Tuesday, 14 February 2017 17:10:15 CET Jonas Schnelli via bitcoin-dev 
wrote:

What about allowing trusted users connecting on a different connection. Much 
like the RPC one.
Make that one encrypted. Different usecase, different connection.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
On Thu, Jul 13, 2017 at 01:04:19AM +0000, Gregory Maxwell via bitcoin-dev wrote:
 
And this attitude is why bitcoin-core is going to get dropped and,
hopefully, instead of just one code to rule them all, we will have
good specifications and multiple competing implementations.


One of the significant adverse impacts of Segwit is the following:

https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L2861

When viewed on Github, less than one third of the consensus-critical
code is visible on-screen.

This is a maintenance and readability nightmare, in my opinion. There
are many good reasons why giving *users* the ability to firewall off
that code, maybe even with #ifdefs, as ugly as they are, would provide
some much better confidence that one is indeed, not running segwit.

I suspect it would help the community a great deal in comfort level
if this code were easier to read and used some type of coding standard
in which the default github view on the average browser shows all the
code without having a "hidden feature" that requires scrolling that
has no obvious UI indication you even need to scroll.


-------------------------------------


As Peter pointed out, that is the case here.


That is not a condition of the hard fork concept.

https://github.com/bitcoin/bips/blob/master/bip-0099.mediawiki
Softfork
A consensus fork wherein everything that was previously invalid remains invalid while blocks that would have previously considered valid become invalid. A hashrate majority of miners can impose the new rules. They have some deployment advantages like backward compatibility.
Hardfork
A consensus fork that makes previously invalid blocks valid. Hardforks require all users to upgrade.

The essential element of a hard fork is that the new rule may cause rejection of blocks that are not rejected by old rules (thereby requiring that all users adopt the new rule in order to avoid a split). The reason a hard fork is interesting is that it can create a chain split even if it is enforced by majority hash power.

That is not the case with a soft fork and it is not the case here. A split can occur. The fact that it is possible for the split to also eventually orphan the old nodes does not make it a soft fork. A soft fork requires that a hash power majority can impose the rule. However, under the proposed new rule the hash power majority (according to the new rule) cannot impose the rule on existing nodes.


Nothing about this proposal implies an attack. From the Motivation section:

Mitigate centralization pressures by introducing a POW that does not have economies of scale
Introduce an intermediary confirmation point, reducing the impact of mining power fluctuations


Presumably this preference exists because it implies the new rule would not cause a chain split, making it more acceptable to a risk-averse economy. This is precisely why it should be described correctly.


In reality you have no way to know if/when people would adopt this rule. What matters in the proposal is that people who do adopt it are well aware of its ability to split them from the existing economy.

e

-------------------------------------
On Tue, Mar 7, 2017 at 5:55 PM, bfd--- via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


The benefit of this sort of infrequent utxo commitment is that it would
allow a new client to download just the contents of the utxo set and not
have to get the entire blockchain history, which is much larger.
-------------------------------------
Hello Bitcoin Development Mailing List,

I wish to explain why the current approach to ‘ASICBOOST’ dose not comply with our established best practices for security vulnerabilities and suggest what I consider to be an approach closer matching established industry best practices.


1.     Significant deviations from the Bitcoin Security Model have been acknowledged as security vulnerabilities.

The Bitcoin Security Model assumes that every input into the Proof-of-Work function should have the same difficulty of producing a desired output.


2.     General ASIC optimisation cannot be considered a Security Vulnerabilities.

Quickly being able to check inputs is not a vulnerability. However, being able to craft inputs that are significantly easier to check than alternative inputs is a vulnerability.


3.     We should assign a CVE to the vulnerability exploited by ‘ASICBOOST’.

‘ASICBOOST’ is an attack on this Bitcoin’s security assumptions and should be considered an exploit of the Bitcoin Proof-of-Work Function.

For a more detailed look at ‘ASICBOOST’, please have a look at this excellent document by Jeremy Rubin:
http://www.mit.edu/~jlrubin//public/pdfs/Asicboost.pdf

The Bitcoin Community should be able to track the progress of restoring the quality of the Bitcoin Proof-of-Work function to its original assumptions.


4.     Work should be taken to prudently and swiftly restore Bitcoins Security Properties.

I recommend the Bitcoin Community fix this vulnerability with expediency.



Cameron.

PS:

With a soft-fork it probably is possible to completely fix this Proof-of-Work vulnerability.

(Here is my working list of things to do):

1.     Include extra data in the Coinbase Transaction, such as the Witness Root.

2.     Lock the Version. (Use a space in the Coinbase Transaction for signalling future upgrades).

3.     Lock the lower-bits on the Timestamp: Block timestamps only need ~1minute granularity.

4.	Make a deterministic ordering of transaction chains within a block. (However, I believe this option is more difficult).

Of course, if we have a hard-fork, we should consider the Proof-of-Work internal merkle structure directly.
-------------------------------------

IIRC, it is actually increased by ~81 bytes, and doesn't count witness data if 
on Segwit transactions (so in effect, nearly 4 MB transactions are possible). 
This probably doesn't make the hashing problem worse, however it should be 
made clear in the BIP.


Citations do not support the claim.


This is deceptive and meaningless. There is no reason to *ever* refer to the 
size of the block serialised without witness programs. It is not a meaningful 
number.


What is modified here? "segsignal" does not appear in the BIP 91 protocol at 
all...


A "plain block size limit" of 2 MB would be a no-op. It would have literally 
no effect whatsoever on the network rules.

Furthermore, this does not match what btc1/Segwit2x currently implements at 
all. The actual implementation is: If Segwit (via deployment method) activates 
at block N, then block N+12960 activates a new weight limit of 8M (which 
corresponds to a size of up to 8,000,000 bytes).


What is the rationale for excluding witness data from this measurement?


Actual network growth does not reflect a pattern that supports this claim.


Larger block sizes is not likely to have a meaningful impact on fee pressure. 
Any expectations that do not match the reality are merely misguided, and 
should not be a basis for changing Bitcoin.

Luke

-------------------------------------
Hello everyone,

I'd like to propose a new BIP for native segwit addresses to replace
BIP 142. These addresses are not required for segwit, but are more
efficient, flexible, and nicer to use.

The format is base 32 and uses a simple checksum algorithm with strong
error detection properties. Reference code in several languages as
well as a website demonstrating it are included.

You can find the text here:
https://github.com/sipa/bech32/blob/master/bip-witaddr.mediawiki

Cheers,

-- 
Pieter

-------------------------------------
On Thu, Nov 2, 2017 at 11:31 PM, Scott Roberts via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This is the bitcoin development mailing list, not the "give free
review to the obviously defective proposals of adversarial competing
systems" mailing list. Your posting is off-topic.

-------------------------------------
With the feedback on Proof-of-Loss (always privately to my email), I realized the article was hard to understand for lacking:

* A more explicit definition of transaction rights.
* An overview of how the algorithm works.

As an abstract could not contain all that, I wrote an introduction with examples.

I also adopted a suggestion of including the current block height in the proof-of-loss data once I realized:

* Preventing the same proof-of-loss from chaining consecutive blocks was not the purpose of the proof-of-loss context, which did it statistically rather than logically.
* The presence of that height in the block header made serial chaining easier to enforce, by removing the need to include additional block height information.

While revising the algorithm, I made some corrections, mainly to:

* Transaction prioritization (which now uses fees instead of rights).
* Inactivity fees.

Finally, the new version more aptly derives the design and often has better wording.

The new text is available at:

https://proof-of-loss.money/

Mirelo

-------- Original Message --------
Subject: Proof-of-Loss
Local Time: February 4, 2017 10:39 AM
UTC Time: February 4, 2017 12:39 PM
From: mirelo@deugh-ausgam-valis.com
To: bitcoin-dev@lists.linuxfoundation.org <bitcoin-dev@lists.linuxfoundation.org>

An alternative consensus algorithm to both proof-of-work and proof-of-stake, proof-of-loss addresses all their deficiencies, including the lack of an organic block size limit, the risks of mining centralization, and the "nothing at stake" problem:

https://proof-of-loss.money/
-------------------------------------
On Mar 25, 2017 12:17 AM, "Luke Dashjr via bitcoin-dev" <bitcoin-dev@lists.
linuxfoundation.org> wrote:

 Any ideas? :/


Can't the size be aggregated up the tree such that each midstate hash is
the hash of branches below plus the agreegate of the sizes below.

This would make the root  hash(left + right + size/weight) and the proof
would just be the preimage.
-------------------------------------
On Thu, Apr 6, 2017 at 4:39 AM, Bram Cohen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This is an interesting point.

If you have a precise description why it makes an incentive to make
blocks smaller I would love to read it.
Somebody asked and I didn't have an answer.
I imagine you try several reorderings sometimes excluding certain
branches of the merkle tree, permuting the branches you exclude or
something similar, but I really don't know the algorithm in detail and
I didn't want to say something inaccurate.

-------------------------------------
Den 25 jan. 2017 08:22 skrev "Johnson Lau" <jl2012@xbt.hk>:

Assuming Alice is paying Bob with an old style time-locked tx. Under your
proposal, after the hardfork, Bob is still able to confirm the time-locked
tx on both networks. To fulfil your new rules he just needs to send the
outputs to himself again (with different tx format). But as Bob gets all
the money on both forks, it is already a successful replay


Why would Alice be sitting on an old-style signed transaction with UTXO:s
none of which she controls (paying somebody else), with NO ability to
substitute the transaction for one where she DOES control an output,
leaving her unable to be the one spending the replay protecting child
transaction?
-------------------------------------


Le 22/12/2017 à 00:09, Luke Dashjr via bitcoin-dev a écrit :
I already posted about this, then what is doing the pubkey in sigscript
for standard p2pkh transactions? (this was not the case some time ago)

-- 
Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms


-------------------------------------
Something similar to this has been proposed  in this article by Ron Lavi,
Or Sattath, and Aviv Zohar, and discussed in this bitcoin-dev thread
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-September/015093.html

They only discussed changing the fee structure, not removing the block size
limit, as far as I know.

    "Redesigning Bitcoin's fee market"
    https://arxiv.org/abs/1709.08881



*Ben Kloester*

On 30 November 2017 at 11:47, William Morriss via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On 05/29/2017 04:19 AM, Anthony Towns wrote:

I didn't meant to imply that the point was academic, just to ask your
indulgence before making my point. Thanks for the detailed and
thoughtful reply.


I don't accept that the ease (absolute cost) of implementing the
ASICBOOST optimization is relevant. The cost of implementation is offset
by its returns. Given that people are presumed to be using it profitably
I consider this point settled.

The important point is that if people widely use the optimization, it
does not constitute any risk whatsoever.


I realize that the term "same essential formulation" was misleading, but
my aim was the *source* of harm (unblocked) in an ASIC patent as
compared to an ASICBOOST patent. It seems that you agree that this harm
in both cases results from the patent, not the optimization.

Nobody is suggesting that ASICs are a problem despite the significant
optimization. It is worth considering an alternate history where ASIC
mining had been patented, given that blocking it would not have been an
option. More on this below.

I agree that the optimizations differ in that there is no known way to
block the ASIC advantage, except for all people to use it. But correctly
attributing the source of harm is critical to useful threat modeling. As
the ASIC example is meant to show, it is very possible that an
unblockable patent advantage can arise in the future.


Quite clearly then there is a possibility (if not a certainty) that
Bitcoin will eventually be faced with an unblockable mining patent
advantage.


Precisely. This is a proper generalization of the threats above. A
patent is a state grant of monopoly privilege. The state's agent (patent
holder) extracts licensing fees from miners. The state does this for its
own perceived benefit (social, economic or otherwise). Extracting money
in exchange for permission to use an optimization is a tax on the
optimization.


This is an important point. Consider also that a subsidy has the same
effect as a tax. A disproportionate tax on competing miners amounts to a
subsidy. A disproportionate subsidy amounts to a tax on competitors.

If the state wants to put its finger on the scale it can do so in either
direction. It can compel licensing fees from miners with no need for a
patent. It can also subsidize mining via subsidized energy costs (for
example), intentionally or otherwise.


That sounds more like a central authority than a solution.

So, my point:

Mis-attributing the threat is not helpful. This is not an issue of an
unforeseen bug, security vulnerability, bad miners, or evil
patent-holders. This is one narrow example of the general, foreseen,
primary threat to Bitcoin - or any hard money.

Bitcoin's sole defense is decentralization. People parrot this idea
without considering the implication. How does decentralization work? It
works by broadly spreading the risk of state attack. But this implies
that some people are actually taking the risk.

By analogy, BitTorrent is estimated to have 250 million active users in
a month, and 200,000 have been sued in the US since 2010.
Decentralization works because it reduces risk through risk-sharing.

Bitcoin cannot generally prevent state patent/licensing/tax regimes.
Licensing is a ban that is lifted in exchange for payment. What is the
Bitcoin solution to a global ban on mining? On wallets? On exchange?

The Bitcoin defense against a patent is to ignore the patent. Berating
people for doing so seems entirely counterproductive.

e

-------------------------------------
On Saturday, April 08, 2017 3:17:47 PM Jimmy Song wrote:

No, it isn't allowed right now. Doing it wouldn't invalidate blocks, but it 
would clearly be an attack on the network and cause harm. The same as if 
miners were to maliciously mine only empty blocks.

Luke

-------------------------------------
Good morning,

I'm re-sending this message below as it appears to have gotten lost before it reached cc: bitcoin-dev.

Paul even replied to it and the reply reached on-list, so I'm re-sending it as others might have gotten confused about the discussion.

So far I've come to realize that sidechain-headers-on-mainchain/SHOM/SHM/driveproofs creates a very weak peg, and that only sidechain-only miners can take advantage of this weak peg.  This is because, the fee paid by sidechain-only miners to mainchain miners will approach TRANSFERLIMIT / 288 to protect against theft, and then sidechain miners will be unable to replenish their maincoin stock (to pay for the blind-merge-mine) if they do not transfer *only* their sidecoins earned.

Regards,
ZmnSCPxj

-------- Original Message --------
Subject: Re: [bitcoin-dev] Sidechain headers on mainchain (unification of drivechains and spv proofs)
Local Time: September 8, 2017 10:56 PM
UTC Time: September 8, 2017 2:56 PM
From: ZmnSCPxj@protonmail.com
To: Chris Stewart <chris@suredbits.com>, CryptAxe <cryptaxe@gmail.com>, Paul Sztorc <truthcoin@gmail.com>
Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org>

Good morning,

Chris mentioned the use of OP_WITHDRAWPROOFVERIFY.  I've come to realize
that this is actually superior to use OP_WITHDRAWPROOFVERIFY with a
sidechain-headers-on-mainchain approach.

Briefly, a payment to OP_WITHDRAWPROOFVERIFY is an instruction to transfer
value from the mainchain to a sidechain.  Thus, a payment to
OP_WITHDRAWPROOFVERIFY includes the sidechain to pay to, and a commitment
to a sidechain address (or whatever is the equivalent to a sidechain
address).

Various OP_WITHDRAWPROOFVERIFY explanations exist.  Most of them include
OP_REORGPROOFVERIFY.  With sidechain-headers-on-mainchain, however, there is
no need for reorg proofs.  This is because, the mainchain can see, in real
time, which branch of the sidechain is getting extended.  Thus if someone
attempts to defraud a sidechain by forking the sidechain to an invalid
state, sidechainers can immediately detect this on the mainchain and
immediately act to prevent the invalid fork from being advanced.  After
all, a reorg proof is really just an SPV proof that is longer than some
previous SPV proof, that shows that the previous SPV proof is incorrect,
by showing that the block at the specified height of the WT is not present
on a longer SPV proof.

Since sidechain-headers-on-mainchain implies merge mining of sidechains,
with no option to have independent proof-of-work of sidechains, the
sidechain's entire history is recorded on the mainchain, visible to all
mainchain nodes.

--

An advantage of sidechain-headers-on-mainchain is a side-to-side peg without
passing through the mainchain.
That is, a 2-way peg between any two chains, whether side or main.

Sidechains supporting side-to-side transfer would require supporting
OP_WITHDRAWPROOFVERIFY, but not any of the other parts of sidechains.

We must consider a WT format (withdrawal transaction) that is compatible
with an OP_WITHDRAWPROOFVERIFY Bitcoin transaction.

***That is, a lockbox UTXO on one chain is a WT on another chain.***

Sidechains need not follow the mainchain format for its normal
transactions, only for WT transactions that move coins across chains.

For this, mainchain should also have its own "sidechain ID".  Perhaps a
sidechain ID of 0 would be appropriate for mainchain, as its status as
mainchain.

Suppose we have two sidechains, Ess and Tee, both of which support
side-to-side pegs.

An Ess fullnode is a Bitcoin fullnode, but an Ess fullnode is not
necessarily a Tee fullnode, and vice versa.

A lockbox redemption in sidechain-headers-on-mainchain is simply a spend of
a lockbox, pointing to the sidechain header containing WT, the merkle tree
path to the WT transaction from the h* commitment of the header, the output
which locks, and so on as per usual OP_WITHDRAWPROOFVERIFY.

Then a sidechain can create tokens from nothing, that are locked in a
OP_WITHDRAWPROOFVERIFY lockbox; this is the only way to create sidecoin.
When transferring into a sidechain from mainchain, or anywhere, the
sidechain either creates tokens locked into OP_WITHDRAWPROOFVERIFY, or
looks for an existing UTXO with OP_WITHDRAWPROOFVERIFY from the source
chain and spends them (the latter is preferred as it is fewer
transactions and less space on the sideblock, reducing sidechain fees).

OP_WITHDRAWPROOFVERIFY on a sidechain would query the mainchain fullnodes.
Whatever rules allow lockbox unlocking on mainchain, will also be the same
rules that allow lockbox unlocking on sidechains.
A mainchain RPC can even be made to simplify sidechain verification of
side-to-side pegs, and to ensure that sidechains follow the same consensus
rules for OP_WITHDRAWPROOFVERIFY.

So if we want transfer TeeCoin to EssCoin, we spend into a
OP_WITHDRAWPROOFVERIFY lockbox on Teechain pointing to Esschain (i.e. a
Tee->Ess lockbox).  This lockbox is itself a WT from the point of view of
Esschain.  On Esschain, we look for an existing Ess->Tee lockbox, or
create a Ess->Tee lockbox of our own for a EssCoin fee.  Then we create a
spend of the Ess->Tee lockbox on Esschain, wait until spending is
possible, and then post that transaction on Esschain.

Again, with sidechain-headers-on-mainchain, reorg proofs are unnecessary,
since any invalid chain should be quickly buried by a valid chain,
unless the economic majority decides that a sidechain is not worth
protecting.

--

All is not well, however.  Remember, on a sidechain, we can create new
sidecoin for free, provided they are in a lockbox.  Unlocking that
lockbox would require a valid WT on the chain that the lockbox is
dedicated to.  However, a lockbox on one chain is a WT on the other
chain.  We can create a free lockbox on Ess, then use that lockbox as
a WT on Tee, inflating TeeCoin.

Instead, we add an additional parameter, wtFlag, to
OP_WITHDRAWPROOFVERIFY.
This parameter is ignored by OP_WITHDRAWPROOFVERIFY opcode.

However, this parameter is used to determine if it is a WT.  Sidechain
consensus should require that freely-created lockboxes set this
parameter to 0, so that a side block that creates free lockboxes where
this parameter is non-zero is an invalid side block.  Then a sidechain
will only treat a lockbox on another chain as a WT if the wtFlag
parameter is nonzero.  This way, freely-created lockboxes are not
valid WT.  Valid WT must lock actual, already unlocked coins, not
create new locked coins.

On Bitcoin, of course, this parameter must always be nonzero, since
freely-created lockboxes are not allowed on mainchain, as asset
issuance on mainchain is already fixed.

--

Let us now flesh out how WT and lockboxes look like.  As we mentioned, a
lockbox on one chain is a WT on the destination chain.  Or to be more
precise, what a destination chain sees as a WT, is a lockbox on the source
chain.

Thus, a lockbox is a Bitcoin-formatted transaction output paying to the
scriptPubKey:

  <sidechain address commitment> <sidechain ID> OP_WITHDRAWPROOFVERIFY

(assuming a softfork, additional OP_DROP operations may occur after
OP_WITHDRAWPROOFVERIFY)

Suppose the above lockbox is paid to in the Bitcoin mainchain, with the
sidechain ID being the ID of Esschain.  This is itself a WT transaction
from the point of view of Esschain, on the principle that a lockbox on
one chain is a WT on another chain.

Assuming Esschain is a brand-new sidechain, it has no EssCoins yet.  The
sidechain allows the arbitrary creation of sidecoin provided the new
sidecoins are in a lockbox whose sidechain address commitment is 0.  So
in Esschain, we create the same coins on a UTXO paying to the
scriptPubKey:

  0 0 OP_WITHDRAWPROOFVERIFY

The first 0 is the sidechain address commitment, which is 0 since this
output was not created by transferring to a sidechain; we
reuse the sidechain address commitment as the wtFlag.  The
second 0 is the mainchain's ID.  The above is a lockbox from the point of
view of Esschain.  It is not a WT on mainchain, however, because the
sidechain address commitment is 0, which we use also as the wtFlag
parameter.

Now, how does a main-to-side peg work?  After creating the above output on
Esschain, we now spend the output with the below scriptSig:

  <mainchain output ID> <mainchain WT transaction> <merkle path to WT transaction> <mainchain block hash>

On Esschain, OP_WITHDRAWPROOFVERIFY then verifies that the mainchain block
hash is a valid past block of the mainchain, then locates the mainchain
header.  It then checks the merkle tree path to the mainchain WT
transaction,
confirming that the mainchain contains that transaction, and confirms that
the
indicated output is in fact, a payment to an OP_WITHDRAWPROOFVERIFY, which
pushes the Esschain ID, and with a nonzero sidechain address commitment.

(Esschain also needs to ensure that a single WT is not used to unlock
multiple lockboxes on Esschain; the easiest way is to add it to a set,
but this set cannot be pruned; other ways of ensuring only a WT is only
used to unlock once might be designed)

On Esschain, the sidechain does one final check: the transaction that spends
an OP_WITHDRAWPROOFVERIFY must have an output that pays to the sidechain
address committed to, and that output's value must be the same as the value
locked in the mainchain.

(for now, I think all lockboxes must have the same fixed amount, for
simplicity)

Now suppose we want to convert back our EssCoin to Bitcoin.  We create a
lockbox on Esschain, paying to the below:

  <bitcoin P2SH address> 0 OP_WITHDRAWPROOFVERIFY

The bitcoin P2SH address is mainchain address commitment; for simplicity
we just use P2SH on mainchain as it can encode any address.  The 0 is the
mainchain ID.  The above Esschain lockbox is itself a WT from Esschain to
mainchain.

Then, we look for an unspent lockbox on Esschain whose sidechain ID is the
Esschain ID.  Note that we can select any lockbox with the correct
sidechain ID, regardless of the sidechain address commitment it may have.

Locating an appropriate mainchain lockbox for Esschain coins, we then
provide the below scriptSig, paying out to the bitcoin P2SH address we
selected:

  <esschain output ID> <esschain WT tx> <merkle path to WT tx> <esschain block header hash>

On mainchain, we check that the indicated sidechain block header hash is a
block header on the longest chain of Esschain.  We check it has sufficient
depth.  Then we check if the merkle path to the WT tx is correct and goes
to esschain WT tx.  Finally, we check the indicated output ID, and check that
it is indeed an Esschain lockbox dedicated to mainchain.  Finally, we check
that the transaction has an output that spends the lockbox amount to the
specified bitcoin P2SH address.

(similarly mainchain nees to ensure that the Esschain WT is only used
once)

The key insight here is that side-to-side pegs are just like side-to-main
pegs.  Suppose instead we want to transfer our coins from Esscoin to
Teecoin.  We would instead pay to the following lockbox on Esschain:

  <teecoin address commitment> <teechain ID> OP_WITHDRAWPROOFVERIFY

Then a Teechain transaction spending some Tee->Ess lockbox (or a fresh
lockbox if there are no Tee->Ess lockboxes on Teechain) is created.
We proceed as if it were a side-to-main peg, except it is a peg to
Teechain, either creating or unlocking TeeCoins.  Indeed, mainchain
fullnodes may even provide an RPC for checking OP_WITHDRAWPROOFVERIFY,
so as to reduce risk that a sidechain breaks consensus due to buggy
code.

--

All is still not well with side-to-side pegs, however.

Suppose the economic majority decides that Esschain must die.  Perhaps it
has some irrecoverable security bug, perhaps it adds features that allow
Esschain fullnodes to kill baby seals, perhaps a successful theft of
Esschain lockboxes was performed and Esscoins are now functionally
worthless.  Killing a sidechain is done by bribing miners to put invalid
values into h*, and thus stealing Bitcoin->Ess lockboxes.

If Esschain dies, however, and the economic majority is not prepared to keep
Esschain dead, it is possible to unlock Tee->Ess lockboxes on Teechain.
Unlocking existing Tee->Ess lockboxes on Teechain is safe, since they
represent coins that were locked into Bitcoin->Tee lockboxes.  However,
it is still possible to create "free" Tee->Ess lockboxes on Teechain, then
provide an invalid Tee->Ess WT lockbox on the now-moribund Esschain to
unlock the free Tee->Ess lockbox on Teechain, inflating TeeCoin value.
Thus in the presence of side-to-side pegs, the death of even one sidechain
represents the death of every other sidechain!

Thus, to properly kill Esschain, the economic majority should spam the
Esschain headers slot with a fixed value, say 0, forever.  This makes it
very difficult to create a Tee->Ess WT lockbox on Esschain, as you would
now be able to reverse a one-way hash function.

Alternatively, Teechain can softfork so that Tee->Ess lockboxes are no
longer creatable or spendable.  However, the death of Esschain requires
that all other sidechains, including Youchain, Veechain, Dubyachain, and
so on, to softfork similarly.

Perhaps both can be done: first the economic majority wanting to kill
Esschain starts spamming it with invalid spends of Bitcoin->Ess lockboxes,
then when all Bitcoin->Ess lockboxes have been stolen, spam it with 0s
until all other sidechains have banned free Ess lockboxes on their chains.
Then, the economic majority can leave Esschain dead, and a later softfork
of mainchain prevents Esschain from being extended and allows mainchain
fullnodes to prune Esschain headers.

--

Thieves will still have the same difficulty stealing from sidechains, but
now their payoff is increased.  If a thief wants to steal Esschain
lockboxes, then it is possible to pack an invalid Esschain block full of
invalid WT to other chains.  Even chains that don't have lockboxes to
Esschain can create lockboxes to Esschain for free.  Thus, instead of
stealing only one lockbox at a time on mainchain, the thief can steal one
lockbox on mainchain, and on every sidechain that supports side-to-side
pegs, at a time.  The risk/reward ratio may shift drastically in that case.

However, this does mean that users of one chain must pay attention to
attacks on other chains, not just the chain they use.  If Teechain has no
side-to-side pegs, then Teechain users will not care if Esschain is under
attack.  But if side-to-side pegs are allowed on Teechain, then Teechain
users must also care about Esschain's health, as well as the health of
every other sidechain in existence.  Mainchain is protected since free
lockboxes are not creatable on mainchain.  Each sidechain is not; thus
the user of any sidechain must also stand by users of every other
sidechain, or else they all fall apart.  Of course, this could more
simply lead to "I will not use Teechain even if it would be useful to me,
because if I use Teechain, I have to care about Esschain and Youchain and
whatever."

--

Side-to-side pegs are useful to allow better liquidity and provide
arbitrage quickly between sidechains, without having to pass through
mainchain.  Otherwise, Esscoin may be valued slightly lower than Bitcoin,
then Teecoin valued slightly higher than Bitcoin, creating a larger
difference between Esscoin and Teecoin values than what a full
side-to-side peg could support.  2-way pegs from mainchain
to sidechain stabilize sidecoin with respect to maincoin.  Side-to-side
pegs stabilize all sidecoins to all other sidecoins.

Side-to-side pegs are enabled implicitly by sidechain-headers-on-mainchain,
as all sidechain fullnodes must necessarily be mainchain fullnodes, and
any mainchain fullnode can judge the validity of any WT from any sidechain
without a miner voting period.

Side-to-side pegs are a generalization of main-to-side and side-to-main
pegs.  A sidechain can simply implement OP_WITHDRAWPROOFVERIFY and allow
free lockboxes, and that is sufficient for the sidechain to support
imports of bitcoin from mainchain and from any other sidechain.

Side-to-side pegs seem to imply that all pegs must have the same bitcoin
value transferred.  What that value must be, is something that may be
debated endlessly.

A side-to-side peg is a cut-through of a side-to-main peg from
one sidechain into a main-to-side peg into another sidechain.  If a
withdrawal from side-to-main peg would be accepted by mainchain, then
another sidechain could, in principle, accept a proof that would
authorize a side-to-main peg directly as a side-to-side peg.

Side-to-side pegs make attacks on sidechains more lucrative, as it
becomes possible to print sidecoins by successfully attacking a
different sidechain.

Drivechain cannot implement side-to-side pegs, as WT validity is
voted on by mainchain miners, and asking mainchain miners about
side-to-side pegs requires mainchain miners to be aware of both
sidechains.  Sidechain-headers-on-mainchain publishes SPV proofs
continuously to the mainchain, and since any sidechain fullnode is
also a mainchain fullnode (since sidechains are mergemined), then
every sidechain fullnode is automatically capable of accessing
and verifying SPV proofs for every other sidechain.

However, the pegging here seems less flexible than the pegging
supported by drivechain.  Drivechain lets pegs be any size, with
miner voting being the basis of knowing how much money is owned
by whom.

Regards,
ZmnSCPxj
-------------------------------------
I don't normally post here, but I'm sorry, if you don't see those two as
equal, then I think you have misunderstood the *entire* value proposition
of cryptocurrencies.

The state of any cryptocurrency should entirely (and only) be defined by
its ledger. If the state of the system can be altered outside of the rules
governing its ledger, then the system isn't secure. It doesn't matter
whether the people making those changes are the ones that are leading the
project or not. An "irregular state change" is a fancy term for a bailout.

I'm sure I speak for more than myself in saying that an "irregular state
change" is equivalent to modifying the underlying ledger. Let's not let
semantics keep us from recognizing what actually took place.

-Conner

On Wed, Jun 7, 2017 at 14:14 Nick Johnson via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
It's not clear to me if you are have looked at the previous UTXO set
commitment proposals.

some utxo set commitment bookmarks (a little old)
http://diyhpl.us/~bryan/irc/bitcoin/utxo-commitments-or-fraud-proofs.stdout.txt

TXO bitfields
http://diyhpl.us/wiki/transcripts/sf-bitcoin-meetup/2017-07-08-bram-cohen-merkle-sets/

delayed TXO commitments
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012715.html

TXO commitments do not need a soft-fork to be useful
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-February/013591.html

rolling UTXO set hashes
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014337.html

lotta other resources available, including source code proposals..

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
On 09/28/2017 02:43 PM, Sjors Provoost via bitcoin-dev wrote:


The payment request message is just as one-way as an address is. It is
already being emailed and printed on an invoice, in fact it often acts
as the invoice.

Even more problematic, if you were to include an expiry date in a
BIP-173 address and put that into a payment request, wallets wouldn't be
allowed to parse that expiry date from the script without violating the
BIP70 spec.


-------------------------------------
Hi,

This is the first time I post on this list.

First of all, Thank you Jameson for the interview you gave yesterday, it’s been a model of calm and self-control for all of us.

I deeply believe the high average fees we experience right now are mostly due to the miscalculations of most of the hardware (ledger & trezor) wallets (and probably software too) on the market.

I personally made transactions at the worst period for the Blockchain with less than 40 sat/WU of fees and got confirmed in less than a day.

I think there’s a lot of work to do in used education to make them understand that for a low amount of fees they can still get a transaction confirmed and that’s the POS’ work to make sure the transaction is legit.

Regards, Michel.


-------------------------------------
allow someone to essentially automate the deployment of nodes. i.e. if a
node can pay for itself 100% (even at a lesser value, it just becomes
cheaper overall), you could write an application that uses an AWS API or a
digital ocean API to automatically deploy 100's of nodes. Which sounds
great but not if that person is malicious and wants to prevent the
community adopting proposals.

what other projects have done to avoid such attacks (while incentivizing
economically running full nodes) is to only distribute part of the block
rewards back such nodes if that node has committed/frozen a predetermined
amount of coins that can't be spent. This also leaves less liquidity for
market speculation and a incentives for long term commitments.

On Wed, Apr 19, 2017 at 5:14 AM udevNull via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
---------- Forwarded message ----------
From: Bram Cohen <bram@bittorrent.com>
Date: Thu, Jul 27, 2017 at 1:21 PM
Subject: Re: [Mimblewimble] Switch to Blake2
To: Ignotus Peverell <igno.peverell@protonmail.com>
Cc: Bryan Bishop <kanzure@gmail.com>


I have quite a few thoughts about proofs of position. I gave a talk about
it which hopefully gets the points across if you go through all the Q&A:

https://www.youtube.com/watch?v=52FVkHlCh7Y

On Mon, Jul 24, 2017 at 12:12 PM, Ignotus Peverell <
igno.peverell@protonmail.com> wrote:




-- 
- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
On Sat, Apr 15, 2017 at 4:10 AM, Steven Pine <steven.pine@gmail.com> wrote:

There is a technical requirement that BIP 9 bit allocations must have
a timeout so that a bit is not forever burned if a proposal is ever
abandoned (e.g. because something better came along before it
activated).  This isn't a timeout for the proposal, but for the bit
assignment.  If a proposal hasn't activated but there is still
interest it will just get a new bit (and can alternate back and forth
between a pair). This is a timeout of the bit, not the proposal.

It has to be setup this way because there is no real way to
communicate abandonment to old software, so a timeout must be set in
advance.


"Core" doesn't plan on much of anything beyond the immediate pipeline
of activities, similar to other large open source collaboration, or
open standards development organizations. It isn't a company.
Individuals have plans about their own work which they may collaborate
in one place or another.

But allocating a new bit is how BIP9 works.


What is a "core defined process"?  BIP _itself_ was created by someone
who, AFAICT, has never made a commit to Bitcoin Core.  Numbers are
currently assigned, a nearly judgement-less administrative task, by
someone that authors competing fork of the software (Knots).


Yet it was proposed on this list, had a BIP defined... if it got
eventually used it would presumably end up in the Bitcoin Core project
eventually... so what exactly is your definition of outside? Above you
seemed to be saying a BIP was not outside, but here you are saying
something documented as a BIP is outside?

If your preference is to not insult then it may be advisable to not
disregard distinctions which you do not understand as semantics. :) I
am not prone to arguing over semantics-- the continually binning in
almost all public collaboration as the work of some centralized entity
is really harmful to our community. The distinction is real, and not
semantics.


Sure, and I said so directly in my message.  I believe I was
adequately clear that my complaint about BIP148 is specifically that
it has forced orphaning of passive participants which can be easily
avoided but at the expense of actually needed users to adopt the
change.

For clarity, it could be summarized as: I would not classify BIP148 as
a user activated soft-fork but instead as "user enforced miner
soft-fork activation". The consequence of this is that it likely
cannot achieve low disruptiveness-- this limitation would be excusable
if we weren't aware of any alternative, but in this case we are and
the only relative downside of it is that users will need to upgrade
for it-- which should not be a problem in principle if we believe a
UASF is really user activated.

-------------------------------------
Thank you for your answer, Russel.

When a code path takes advantage of a jet, does the Simplicity code still
need to be publicly available/visible in the blockchain? I imagine that for
big algorithms (say for example EDCA verification/SHA256 hashing etc), it
would take up a lot of space in the blockchain.
Is there any way to mitigate this?

I guess in a softfork for a jet, the Simplicity code for a jet could be
defined as "consensus", instead of needed to be provided within every
script output.
When the Simplicity interpretor encounters an expression that has a jet, it
would run the C/Assembly code instead of interpreting the Simplicity code.
By formal verification we would be sure they match.

Greetings
Hampus

2017-11-03 2:10 GMT+01:00 Russell O'Connor via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
On Mon, Jul 10, 2017 at 1:50 PM, Sergio Demian Lerner via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This is correct. If you are trying to imply that makes the short
timeline here right, you are falling for a "tu quoque" fallacy.


There's no logical reason I can think of (and I've heard many attempts
at explaining it) for miners to consider segwit bad for Bitcoin but
segwitx2 harmless. But I don't see 80% hashrate support for bip141, so
your claim doesn't seem accurate for the segwit part, let alone the
more controversial hardfork part.

I read some people controlling mining pools that control 80% of the
hashrate signed a paper saying they would "support segwit
immediately". Either what I read wasn't true, or the signed paper is
just a proof of the signing pool operators word being something we
cannot trust.

So where does this 80% figure come from? How can we trust the source?


It would be unfortunate to split the network into 2 coins only because
of lack of patience for deploying non-urgent consensus changes like a
size increase or disagreements about the right time schedule.
I think anything less than 1 year after release of tested code by some
implementation would be irresponsible for any hardfork, even a very
simple one.

-------------------------------------
Good morning Luke,

Considering the proposal as a whole, I think, it's a little imperfect.

The main problem, is that the end goal is activation, but what the opcode rewards is signalling.

Consider a miner who signals only if the number of non-signalling blocks in this retargeting time > 5% of 2016. Such a miner would still effectively block a softfork activation, while still has a chance (albeit reduced) of winning the transaction fees of the block-signalling-opcode, in proportion to the number of miners not signaling for a softfork or using a similar algorithm.

What we should reward should be activation.

How about an opcode which requires this stack (stack top at right)

<signature> <publickeyhash> <versionbit>

1. If the <versionbit> given is in state FAILED, then it checks if the given <signature> matches the given <publickeyhash>.

2. If the <versionbit> given is in state LOCKED_IN or ACTIVE, it checks if the given <signature> matches the block's coinbase transaction signature.

This creates an output which is refundable to the owner, if the softfork fails to activate, but which may be claimed by miners, if the softfork activates.

I don't know enough yet about Bitcoin's codebase to know if the above spec is actually workable.

But basically, I think we should create an assurance contract for activation of a softfork.

--

Also, this invites an inverse logic:

1. If the <versionbit> given is in state LOCKED_IN or ACTIVE, then it checks if the given <signature> matches the given <publickeyhash>.

2. If the <versionbit> given is in state FAILED, it checks if the given <signature> matches the block's coinbase transaction signature.

I think, your proposal allows an economic actor to pay fees if the miner is explicitly not signaling. This is supposed to allow a vote against a particular softfork.

Thus, it should also be possible to allow to vote against a softfork.

But in any case, I think, it's better to pay on activation or failure to activate, rather than mere signalling, as signalling is not the goal. Activation, or rejection of activation, is the goal.

Regards,
ZmnSCPxj
-------------------------------------
Today, we’re releasing Ivy, a prototype higher-level language and
development environment for creating custom Bitcoin Script programs. You
can see the full announcement here
<https://blog.chain.com/ivy-for-bitcoin-a-smart-contract-language-that-compiles-to-bitcoin-script-bec06377141a>,
or check out the docs <https://docs.ivy-lang.org/bitcoin/> and source code
<https://github.com/ivy-lang/ivy-bitcoin>.

Ivy is a simple smart contract language that can compile to Bitcoin Script.
It aims to improve on the useability of Bitcoin Script by adding
affordances like named variables and clauses, static (and domain-specific)
types, and familiar syntax for function calls.

To try out Ivy, you can use the Ivy Playground for Bitcoin
<https://ivy-lang.org/bitcoin/>, which allows you to create and test
simulated contracts in a sandboxed environment.

This is prototype software intended for educational and research purposes
only. Please don't try to use Ivy to control real or testnet Bitcoins.
-------------------------------------
Why not simply remove the (redundant after sw activation) 1 mb size
limit check and increasing the weight limit without changing the
discount or having 2 limits?


On Wed, May 24, 2017 at 1:07 AM, Erik Aronesty via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Sat, Feb 25, 2017 at 12:42:56PM -0800, Watson Ladd wrote:

That's what I said: "P2SH's 160-bits is insufficient in certain use-cases such
as multisig"

Obviously any usecase where multiple people are creating a P2SH redeemScript
collaboratively is potentially vulnerable. Use-cases where the redeemScript was
created by a single-party however are _not_ vulnerable, as that party has
complete control over whether or not collisions are possible, by virtue of the
fact that they're the ones who have to make the collision happen!

Similarly, even in the multisig case, commit-reveal techniques can mitigate the
vulnerability, by forcing parties to commit to what pubkeys/hashlocks/etc.
they'll use for the script prior to pubkeys/hashlocks/etc. being revealed.
Though a better long-term approach is to use a 256-bit digest size, as segwit
does.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
You're missing my point.  "As soon as a simple majority supports it" - what
is "it"?  BIP148?  Or "deferring to the miner consensus on BIP148"?  It's
the difference between supporting one side of a vote, vs supporting
deferral to the outcome of the vote.

Or if you mean, the safe thing for miners is to orphan non-segwit blocks
Aug 1 *regardless* of the miner consensus (since the economic consensus
might differ), then there's zero need for this BIP: they should just run
BIP148.

As I said: this BIP should be corrected to only orphan if >50% signal for
BIP148.  Or, define two bits, one meaning "I support BIP148," the other "I
will go w/ the miner majority on BIP148."  Fudging them this way is
deceptive.


On Jun 7, 2017 2:05 PM, "Erik Aronesty" <erik@q32.com> wrote:


Without this option, a miner has to guess whether a split will be
economically impacting.   With this option, his miner will automatically
switch to the chain least likely to get wiped out... as soon as a simple
majority of miners supports it.


On Wed, Jun 7, 2017 at 12:44 PM, Jacob Eliosoff <jacob.eliosoff@gmail.com>
wrote:

-------------------------------------
On Tuesday 05 September 2017 15:00:04 Kabuto Samourai via bitcoin-dev wrote:

In that case, I think we should go back to the proposal I started with in 
March...

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013726.html

This handles not only simple HD seeds, but also multisig HD and such.

Luke

-------------------------------------
There were talks about implementing spv mode for bitcoin core without using
bloom filters. Less efficient because it downloads full blocks, but better
for privacy. Perhaps other spv implementations should consider doing the
same instead of committing the filters in the block?

Now I feel I was missing something. I guess you can download the whole
block you're interested in instead of only your txs and that gives you
privacy.
But how do you get to know which blocks are you interested in?

If the questions are too basic or offtopic for the thread, I'm happy
getting answers privately  (but then maybe I get them more than once).


On 4 Jan 2017 09:57, "Aaron Voisine via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

It's easy enough to mark a transaction as "pending". People with bank
accounts are familiar with the concept.

Although the risk of accepting gossip information from multiple random
peers, in the case where the sender does not control the receivers network
is still minimal. Random node operators have no incentive to send fake
transactions, and would need to control all the nodes a client connects to,
and find a non-false-positive address belonging to the victims wallet.

It's not impossible, but it's non trivial, would only temporarily show a
pending transaction, and provide no benefit to the node operator. There are
much juicier targets for an attacker with the ability to sybil attack the
entire bitcoin p2p network.

Aaron

On Tue, Jan 3, 2017 at 11:47 PM Jonas Schnelli <dev@jonasschnelli.ch> wrote:

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
IDEA:

- Full nodes advertise a bitcoin address.   Users that need to download the
block chain from that node can be encouraged to send a tip to the peers
that served them (by % served).   Recommended tip of 10mbit should be fine.

- A full nodes can *require* a tip to download the blockchain.  If they do,
users that don't specify a tip cannot use them.

CONS:

For some people, this may represent a barrier to hosting their own full
node.   After all, if you have to pay $15 just to get a copy of the
blockchain, that just adds to the already expensive prospect of hosting a
full node.

PROS:

As long as you manage to stay online, you should get your money back and
more.   This is the an incentive for quality, long term hosting.

In the long term, this should cause stable nodes to stick around longer.
It also discourages "installation spam" attacks on the network.

Fees for other node operations can be considered if this is successful.
-------------------------------------
On Wed, Jun 7, 2017 at 6:43 PM, Jared Lee Richardson <jaredr26@gmail.com> wrote:
Both are issues, but wipeout risk is different, the ETH/ETC split for
example didn't have any wipeout risk for either side the same is not
true for BIP148(and it is the non-BIP148 side that carries the risk of
chain wipeout).
Not really, there are a few relatively simple techniques such as RBF
which can be leveraged to get confirmations on on-side before double
spending on another. Once a transaction is confirmed on the non-BIP148
chain then the high fee transactions can be made on only the BIP148
side of the split using RBF. Exchanges will likely do this splitting
automatically for uses as well.
There wasn't proper replay protection at split time for ETH/ETC since
normal transactions would get executed on both sides originally,
however replay protection was added by wallets(mainly using splitting
contracts). I don't think a split is desirable however, which is why
I've proposed this BIP.
Yes, miners aren't likely to waste operational mining costs, that's
ultimately why miners would follow the BIP148 side of the chain
assuming it has sufficient economic support or if it's more profitable
to mine.
segwit2x has more issues since the HF part requires users to reach consensus
That's a political reason not a technical reason.

-------------------------------------

On Sat, 26 Aug 2017, Adam Tamir Shem-Tov via bitcoin-dev wrote:

You effecitvely want these two transactions:

A -(2.30)-> B; B -(1.5)-> C;

To be shorten to one transaction:

A -(0.8)-> B  -(1.5)-> C;

For that to work a lot of changes has to be done to Bitcoin. For
simplicity of the discussion I'll assume all transactions are
standard transactions.

First, a block has to refer to the hash of the "balance sheet" (with
nonce), not the hash of the previous block. This way, a previous block
can be replaced with a smaller one without affecting the hash
reference. To add problem to this significant change, Bitcoin uses
UTXO table instead of "balance sheet". The difference is that UTXO is
indexed by transaction ID while a balance sheet is indexed by owner's
public keys. The shortening you suggested wouldn't affect the balance
sheet but would totally replace UTXOs for B and C, and probably even
A, if A has some changes left.

Second, Alice has to place a new signature on the shortened
transaction. The design challenge is how do we motivate A to do so,
since A needs to do it after "B->C", at which time Alice's business is
done and her wallet offline. Luckily, all bitcoins come from
miners. Imagine A gets her money from A', and all the way back, the
originating A" must be a miner. We just need to design a different
reward mechanism, where miners are not only rewarded by finding
blocks, but also by shortening transactions after his
expenses. Whatever new reward mechanism it may be, it will interfer
with block hash reference discussed in the previous paragraph.

Third, hash references are stablized by work. This is necessary,
otherwise a smaller block intended to replace a long one will not be
forced to maintain the same balance sheet. However, because work is
done on blocks, shortening can only happen within one block. Normally,
Bob who receives a transaction in a block, will not spend it to Carol
in the same block, because he wants 6 confirmations before being sure,
therefore, there will be little opportunity of shortening in one
block. You mentioned the idea of shortening between 1000 blocks - that
surely give a lot of opportunities to shorten a large directed
transaction graph, but you would abandon the proof of work in those
999 blocks in between.

There are three major design issue that needs to be worked out, but
almost all unique aspects of Bitcoin will be affected. Just to name a few:

- wallets need to be aware that the UTXO in it may change to some
   other UTXO with the same sum value.

- nLockTime transactions are affected. Such transactions timed for
   near future probably can stay by ruling that shortening can only
   happen after a year; however, those timed for years to come will
   find itself losing UTXO referenes (e.g. a will).

- I assumed all transactions standard, but if they are not, those who can
   redeem them will lose the UTXO references to them after shortening.

I am, like you, risking proposing what is already proposed or
explaining what is already explained. The thinking around Bitcoin is a
big tome!

Regards
Weiwu Z.

-------------------------------------
Hi all,

        There's a pull request for a lightning payment format which I'd
love wider review on.  It's bech32 encoded, with optional parts tagged:

        https://github.com/lightningnetwork/lightning-rfc/pull/183

There's an implementation with a less formal description here, too:

        https://github.com/rustyrussell/lightning-payencode

Example:
        Send 2500 microbitcoin using payment hash 00010203040506...0102 to me
        @03e7156ae33b0a208d0744199163177e909e80176e55d97a2f221ede0f934dd9ad
        within 1 minute from now (Tue 30 May 12:17:36 UTC 2017):

        lnbc2500u1qpvj6chqqqqsyqcyq5rqwzqfqqqsyqcyq5rqwzqfqqqsyqcyq5rqwzqfqypqdq5xysxxatsyp3k7enxv4jsxqz8slk6hqew9z5kzxyk33ast248j3ykmu3wncvgtgk0ewf5c6qnhen45vr43fmtzsh02j6ns096tcpfga0yfykc79e5uw3gh5ltr96q00zqppy6lfy

Thanks!
Rusty.

-------------------------------------
On 6/23/2017 10:19 AM, Erik Aronesty wrote:

You have not explained how your scheme would cause a relative decrease
in transaction costs. The way I see it, tx costs would be exactly the
same, so it would in fact be impossible to design an inflation schedule
to "balance" these costs (other than inflation of zero as I suggest).

There is no additional bandwidth requirement. That is the point of BMM.
They do not even need to run a sidechain node (to be paid just as much
as if they had).

--Paul




-------------------------------------

If you can't describe an attack that is made possible when typical
personal computers can't run nodes, this kind of logic has no place in
this discussion.

On Fri, Mar 31, 2017 at 4:13 PM, Eric Voskuil via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Seems like it would work fine.  But why would we expect 80pct to signal for
the exact same implementation - when we can't get 40pct.

It will be contingent on some HF code that allows him to continue using
asicboost,  or is too aggressive,  or some other unreasonable request.



On May 22, 2017 6:43 PM, "Matt Corallo via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Given the overwhelming support for SegWit across the ecosystem of
businesses and users, this seems reasonable to me.

On May 22, 2017 6:40:13 PM EDT, James Hilliard via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:
mediawiki
jameshilliard:segsignal-v0.14.1
bitcoin-dev/2017-March/013714.html
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
On 07/09/17 21:35, Andreas Schildbach via bitcoin-dev wrote:

But the depth of exported public key will be null. It does not make
sense to export xpub for m or m/0' for your particular case.


I am fine with having the path there all the time.


Is that really the case? Why come up with a hierarchy and then don't use it?

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
CTO, SatoshiLabs

-------------------------------------
That's BIP30, he linked BIP34:
https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L3004

On Tue, Apr 4, 2017 at 11:32 AM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Mon, May 22, 2017 at 03:05:49AM -0400, Russell O'Connor via bitcoin-dev wrote:

This doesn't hold true in the case of pruned trees, as for the pruning to be
useful, you don't know what produced the left merkleRoot, and thus you can't
guarantee it is in fact a midstate of a genuine SHA256 hash.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
OutputType byte solution is nearly equivalent to {x,y,z} and adds redundant
data. Implementations could erroneously (maliciously) assign the wrong
output type for the given purpose field.

We could reduce the scope of this improvement to BIP43, as suggested by
Thomas. BIP32-generic wallets may implement something else.

On Thu, Sep 7, 2017 at 11:23 AM, Pavol Rusnak via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
-Kabuto

PGP Fingerprint: 1A83 4A96 EDE7 E286 2C5A  B065 320F B934 A79B 6A99
-------------------------------------
On Wed, Sep 27, 2017 at 01:35:33PM -0600, Chris Priest wrote:

My concern is not primarily people re-using addresses, but rather people using
stale addresses that the recipient would rather not be used anymore. This
situation often happens even if the stale address has never been used.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------



Yes. Users probably like this feature and client side filtering is not a drop-in replacement for BIP37.

We should also consider:
BIP37 works, because node-operators are willing to offer that service for free (which maybe change over time).
BIP37 consumes plenty of horsepower (disk/cpu) from nodes. Filtering a couple of days of blocks (assume 1000+) eats lots of resources for something, that has no direct long-term value for Bitcoin (the filters data is unique and will be "thrown away“ [can’t be used by other peers]). Same applies for mempool (filtering mempool of a couple of hundred of mb each time the HD gap limit has been exceeded or the app gets sent to the foreground again).

Purely relying on the availability of BIP37 seems fragile to me and start to explore other ways is very reasonable.

/jonas
-------------------------------------
from what? I am somewhat genuinely befuddled by those who can't even allow
a user config switch to be set.

Indeed. It seems silly. If you're activating the switch, you're most likely
fully aware of what you're doing.
I also saw some very harsh rhetoric being used against BIP148, which seems
unjustified as we have no idea yet how it all will play out. We can only
guess.

Hampus

2017-05-23 6:03 GMT+02:00 Steven Pine via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------

Had a chat with gmax off-list and came to the realization that the method
_should_ indeed generalize to our case of outputting 64-bit integers.
We'll need to do a bit of bit twiddling to make it work properly. I'll
modify our implementation and report back with some basic benchmarks.

-- Laolu


On Thu, Jun 8, 2017 at 8:42 PM Olaoluwa Osuntokun <laolu32@gmail.com> wrote:

-------------------------------------
To ZmnSCPxj:

I don't understand this part.  In my scheme, a sidechain cannot reorganize
unless the mainchain reorganizes, since the consensus loop only cares about
matching the current block; it ignores splits and does not consider them
valid.

But I suppose you are considering something like the Ethereum mutability
feature, which I do not think is something you would want in a sidechain.


The goal was to allow for sidechain reorgs without effecting the mainchain
at all. With the ratchet system (WIP) the sidechain miners can either move
the side chain forward or start a split at some previous sidechain block
height. This happens as the main chain moves forward normally.

withdrawals from sidechain to mainchain?  ...


The ratchet system is actually what links the h* data from bribes to
sidechain blocks. h*'s (which are sidechain block hashes) are added to the
ratchet system if they move the sidechain forward or start a split like I
mentioned before. Then a sidechain can request of their local mainchain
node to verify the headers they have downloaded from sidechain peers and
form the side chain.
-------------------------------------
On Thu, Sep 7, 2017 at 1:55 AM, Peter Todd <pete@petertodd.org> wrote:



I 100% agree.

With SHA256 every final state is also a valid midstate.  Therefore, using a
custom IV of the SHA256 hash of some fixed string results in a hash of data
that is functionally equivalent to prefixing the data with the padded
version of the fixed string and using a regular SHA256 hash of the combined
data.  This is important and I should have explicitly pointed it out.
-------------------------------------


You put forward an interesting idea if it could work, but in the adversarial emergency where an entity is contentiously using a POW monopoly, a hard fork would likely be a far easier and more efficient response.


That said unless I'm missing something I can't see how it would work without still requiring a hard fork since you still need an SHA256 block of sufficient difficulty for the existing network to accept. If the holders of SHA256 hardware didn't want to make their equipment obsolete (likely) they simply would refuse to mine these alternate PoW blocks. I guess a UASF would be an option to force this, but without co-operation (Turkeys voting for Christmas is the British idiom) you'd still end up requiring a hard fork proof of difficulty change, which kind of defeats the purpose?




Upon what do you base this assertion?


________________________________
From: Bram Cohen <bram@bittorrent.com>
Sent: Monday, March 20, 2017 5:49:59 PM
To: John Hardy; Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] Malice Reactive Proof of Work Additions (MR POWA): Protecting Bitcoin from malicious miners

It's possible to switch PoW algorithms with a soft fork rather than a hard fork. You make it so that there are two different PoWs, the old one and the new one, and each old-style block has to reference a new-style block and contain the exact same transactions. The new work rule is that the weighted geometric mean of the quality of the new-style block and the old-style block has to exceed the work threshold, with the weighting starting almost entirely on the old-style block and shifting gradually over to the new-style block until in the end the amount of work to generate the old-style block is completely trivial and doesn't matter any more.

The most interesting part of the whole thing is keeping it so that the new work limit is consistently the limiting factor on mining difficulty rather than the old one interfering. Getting that to work right is an interesting problem which I'm not sure how to do off the top of my head but I believe is manageable.

Using many PoWs is a bad idea, that generally gets the worst of everything rather than the best. There are two ways to go with a PoW, either make it as advantaged on custom hardware as possible, which means sha3, or make it as difficult to ASIC as possible, which at this point means cuckoo since there's already hardware for equihash.

On Sat, Mar 18, 2017 at 9:01 AM, John Hardy via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:

Im very worried about the state of miner centralisation in Bitcoin.

I always felt the centralising effects of ASIC manufacturing would resolve themselves once the first mover advantage had been exhausted and the industry had the opportunity to mature.

I had always assumed initial centralisation would be harmless since miners have no incentive to harm the network. This does not consider the risk of a single entity with sufficient power and either poor, malicious or coerced decision making. I now believe that such centralisation poses a huge risk to the security of Bitcoin and preemptive action needs to be taken to protect the network from malicious actions by any party able to exert influence over a substantial portion of SHA256 hardware.

Inspired by UASF, I believe we should implement a Malicious miner Reactive Proof of Work Additions (MR POWA).

This would be a hard fork activated in response to a malicious attempt by a hashpower majority to introduce a contentious hard fork.

The activation would occur once a fork was detected violating protocol (likely oversize blocks) with a majority of hashpower. The threshold and duration for activation would need to be carefully considered.

I dont think we should eliminate SHA256 as a hashing method and change POW entirely. That would be throwing the baby out with the bathwater and hurt the non-malicious miners who have invested in hardware, making it harder to gain their support.

Instead I believe we should introduce multiple new proofs of work that are already established and proven within existing altcoin implementations. As an example we could add Scrypt, Ethash and Equihash. Much of the code and mining infrastructure already exists. Diversification of hardware (a mix of CPU and memory intensive methods) would also be positive for decentralisation. Initial difficulty could simply be an estimated portion of existing infrastructure.

This example would mean 4 proofs of work with 40 minute block target difficulty for each. There could also be a rule that two different proofs of work must find a block before a method can start hashing again. This means there would only be 50% of hardware hashing at a time, and a sudden gain or drop in hashpower from a particular method does not dramatically impact the functioning of the network between difficulty adjustments. This also adds protection from attacks by the malicious SHA256 hashpower which could even be required to wait until all other methods have found a block before being allowed to hash again.

50% hashing time would mean that the cost of electricity in relation to hardware would fall by 50%, reducing some of the centralising impact of subsidised or inexpensive electricity in some regions over others.

Such a hard fork could also, counter-intuitively, introduce a block size increase since while were hard forking it makes sense to minimise the number of future hard forks where possible. It could also activate SegWit if it hasnt already.

The beauty of this method is that it creates a huge risk to any malicious actor trying to abuse their position. Ideally, MR POWA would just serve as a deterrent and never activate.

If consensus were to form around a hard fork in the future nodes would be able to upgrade and MR POWA, while automatically activating on non-upgraded nodes, would be of no economic significance: a vestigial chain immediately abandoned with no miner incentive.

I think this would be a great way to help prevent malicious use of hashpower to harm the network. This is the beauty of Bitcoin: for any road block that emerges the economic majority can always find a way around.

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
That is only a one-way peg, not a two-way.

In fact, that is exactly what drivechain does, if one chooses parameters
for the drivechain that make it impossible for any side-to-main transfer to
succeed.

One-way pegs have strong first-mover disadvantages.

Paul

On Oct 9, 2017 9:24 PM, "Tao Effect via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Dear list,

In previous arguments over Drivechain (and Drivechain-like proposals) I
promised that better scaling proposals — that do not sacrifice Bitcoin's
security — would come along.

I planned to do a detailed writeup, but have decided to just send off this
email with what I have, because I'm unlikely to have time to write up a
detailed proposal.

The idea is very simple (and by no means novel*), and I'm sure others have
mentioned either exactly it, or similar ideas (e.g. burning coins) before.

This is a generic sharding protocol for all blockchains, including Bitcoin.

Users simply say: "My coins on Chain A are going to be sent to Chain B".

Then they burn the coins on Chain A, and create a minting transaction on
Chain B. The details of how to ensure that coins do not get lost needs to
be worked out, but I'm fairly certain the folks on this list can figure out
those details.

- Thin clients, nodes, and miners, can all very easily verify that said
action took place, and therefore accept the "newly minted" coins on B as
valid.
- Users client software now also knows where to look for the other coins
(if for some reason it needs to).

This doesn't even need much modification to the Bitcoin protocol as most of
the verification is done client-side.

It is fully decentralized, and there's no need to give our ownership of our
coins to miners to get scale.

My sincere apologies if this has been brought up before (in which case, I
would be very grateful for a link to the proposal).

Cheers,
Greg Slepak

* This idea is similar in spirit to Interledger.

--
Please do not email me anything that you are not comfortable also sharing with
the NSA.


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Perhaps you are fortunate to have a home computer that has more than a
single 512GB SSD. Lots of consumer hardware has that little storage. Throw
on top of it standard consumer usage, and you're often left with less than
200 GB of free space. Bitcoin consumes more than half of that, which feels
very expensive, especially if it motivates you to buy another drive.

I have talked to several people who cite this as the primary reason that
they are reluctant to join the full node club.
-------------------------------------
Hi Tim,
After writing this I figured that it was probably not evident at first
sight as the concept may be quite novel. The physical location of the
"miner" is indeed irrelevant, I am referring to the digital location.
Bitcoins blockchain is a digital location or better digital "space".
As far as I am concerned the authority lies with whoever governs this
particular block space. A "miner" can, or can not, include my
transaction.

To make this more understandable:

Abu Bakr al-Baghdadi can extend his caliphate into Bitcoins block
space and rule sovereign(!) over a given block. If he processes my
transaction my fee goes directly into the coffers of his organization.
The same goes for the Queen of England or the Emperor of China. My
interest is not necessarily aligned with each specific authority, yet
as you point out, I can only not use Bitcoin.
Alternatively, however, I can very well sign my transaction and send
it to an authority of my choosing to be included into the ledger, say
BitFurry. - This is what I describe in option 1.

In order to protect my interest I do need to choose, maybe not today,
but eventually.

I also think that people do care who processes transactions and a lot
of bickering could be spared if we could choose.

If we assume a perfectly competitive market with 3 authorities that
govern the block space equally, the marginal cost of 1/3 of the block
space is the same for each, however, the marginal revenue absent of
block rewards is dependent on fees.
If people are willing to pay only a zero fee to a specific authority
while a fee greater than zero to the others it's clear that one would
be less competitive.

Let us assume the fees are 10% of the revenue and the cost is 95 we
have currently the following situation:

A: Cost=95; Revenue=100; Profit=5
B: Cost=95; Revenue=100; Profit=5
C: Cost=95; Revenue=100; Profit=5

With transaction tiering, the outcome could be different!

A: Cost=95; Revenue=90; Loss=5 // BSA that does not respect user interest.
B: Cost=95; Revenue=105; Profit=10
C: Cost=95; Revenue=105; Profit=10

This could motivate transaction processors to behave in accordance
with user interest, or am I missing something?

Best Regards,
Martin


-------------------------------------
On Tuesday 05 September 2017 06:25:16 Thomas Voegtlin via bitcoin-dev wrote:

I think it makes more sense to use a child number field for this purpose.
It seems desirable to use the same seed for all different script formats...

As you note, xpub\xprv are already being used for both P2PKH and P2SH. It 
really doesn't make sense to differentiate segwit specifically.

Luke

-------------------------------------
It's possible to switch PoW algorithms with a soft fork rather than a hard
fork. You make it so that there are two different PoWs, the old one and the
new one, and each old-style block has to reference a new-style block and
contain the exact same transactions. The new work rule is that the weighted
geometric mean of the quality of the new-style block and the old-style
block has to exceed the work threshold, with the weighting starting almost
entirely on the old-style block and shifting gradually over to the
new-style block until in the end the amount of work to generate the
old-style block is completely trivial and doesn't matter any more.

The most interesting part of the whole thing is keeping it so that the new
work limit is consistently the limiting factor on mining difficulty rather
than the old one interfering. Getting that to work right is an interesting
problem which I'm not sure how to do off the top of my head but I believe
is manageable.

Using many PoWs is a bad idea, that generally gets the worst of everything
rather than the best. There are two ways to go with a PoW, either make it
as advantaged on custom hardware as possible, which means sha3, or make it
as difficult to ASIC as possible, which at this point means cuckoo since
there's already hardware for equihash.

On Sat, Mar 18, 2017 at 9:01 AM, John Hardy via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
shaolinfry,

Not sure if you noticed my comments on your earlier orphaning proposal... but if you did you should already know that I really like this proposal... particularly since orphaning valid old blocks is completely unnecessary.

I really like how you pulled out the "lockinontimeout" variable so that this same method could be used in future softfork proposals... instead of hardcoding a special case hack for SegWit.

- it would be nice if the user could set this variable in a configuration file.
- it would be nice if the user could set the "nTimeout" in "src/chainparams.cpp" in a configuratoin file too. This could be used allow a user to expedite when a softfork would become active on his node when combined with ."lockinontimeout".

Developers such as the Core team could put more conservative values in the program, and then community members such as miners and nodes who feel more strongly about SegWit could either compile their own settings or maybe copy a popular configuration file if such was made possible.

Cheers,
Praxeology Guy
-------------------------------------
Maybe it already exists ...

#9484 <https://github.com/bitcoin/bitcoin/pull/9484> 812714f
<https://github.com/bitcoin/bitcoin/commit/812714f> Introduce assumevalid
setting to skip validation presumed valid scripts (gmaxwell)
https://github.com/bitcoin/bitcoin/pull/9484

..., but ...
It would be very interesting if a new node could decide to be a pruned node:
  - it would need to trust one or more peers for the initial blockchain
download, because the blocks downloaded would not be validated
  - it would decide a time from when to get the blocks, like a week before
  - once a day a routine would run that would prune blocks older than the
chosen time

"

*The unspent transaction outputs (which is the only essential piece ofdata
necessary for validation) are already kept in a separate database,so
technically removing old blocks is perfectly possible.*" Pieter Wuille
https://bitcoin.stackexchange.com/questions/11170/why-is-pruning-not-considered-already-at-the-moment


On Fri, Apr 21, 2017 at 10:35 AM, David Kaufman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Sorry, this is not the case.

Your slides gloss over the simple fact that compact fraud proofs in 
Bitcoin aren't possible, and that the "SPV" implemented today bears 
absolutely no resemblance in security properties to the version 
described in the Bitcoin white paper. In the white paper SPV clients 
have the same security as fully validating nodes, in the implementation 
of BIP37 they have absolutely no security except the vague hope that 
they are not being lied to, and that the chain with the most work they 
are seeing is actually valid, both are very weak assumptions.

The suggested solution in no way precludes unconfirmed transactions from 
being used with a commitment scheme, my comments and others are just 
recognising that they are an almost useless indicator for people who 
aren't validating.

During the validationless mining failure around the BIP66 activation 
miners produced 6 invalid blocks in a chain, and many more invalid 
blocks in isolated bursts for a period lasting several months. Due to 
the instability of the network you are completely unreasonable to accept 
anything except multiple confirmations, the true number for safety is 
probably more like 60 or 120, not 6, or 3, or 1 as some Bitcoin 
exchanges use today.


0000000000000000009cc829aa25b40b2cd4eb83dd498c12ad0d26d90c439d99.bin 
bad-version(0x00000002)
0000000000000000032527aa796d3672e32e5f85a452d3a584a28fc7efbcd5d0.bin 
bad-version(0x00000002)
000000000000000003ae1223f4926ec86100885cfe1484dc52fd67e042a19b12.bin 
bad-version(0x00000002)
0000000000000000083cbdbb25c1607527c8f3fdb16f0d048c4439a73b501cb6.bin 
bad-version(0x00000002)
00000000000000000954ed93eda1e79e8261137548fa9ccf4d516bb384a3660b.bin 
bad-version(0x00000002)
00000000000000000afc9fbe7cfe8a6b50502d509ba626beb2e2d6c15d1d3ee3.bin 
bad-version(0x00000002)
00000000000000000b6adf92bc192b3c21210f456ab21b5e46951665c74cfab2.bin 
bad-version(0x00000002)
00000000000000000c9bb4a508fff34f5450d9c62ef2cb833e53909a4c549de5.bin 
bad-version(0x00000002)
0000000000000000116322b5f25826787b01f7a70fb322837b68dff8216cefc4.bin 
bad-version(0x00000002)
000000000000000012aac0664cd8b6cbc3ea485921a05f2c4340f928b0226d3c.bin 
bad-version(0x00000002)

"SPV" like you're describing can exist, or validationless mining can 
exist, both can not simultaneously.



On 2017-03-16 09:36, Tom Harding via bitcoin-dev wrote:

-------------------------------------
On Tue, Jul 11, 2017 at 9:11 PM, Gregory Maxwell <greg@xiph.org> wrote:

To make it clear, since I munged the English on this: Most of my post
is just copied straight out of a private thread where I explained my
perspective on 'roadmaps' as they apply to projects like Bitcoin.

-------------------------------------
Op 2 okt. 2017, om 03:56 heeft Luke Dashjr via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> het volgende geschreven:

If unknown script versions are treated as "return true", there's no need for versions to be deployed in sequence, right? Maybe they should be called numbered script types, rather than script versions.

Sjors
-------------------------------------
Quick comment before I finish reading it completely, looks like you have no way to match the input prevouts being spent, which is rather nice from a "watch for this output being spent" pov.

On June 1, 2017 3:01:14 PM EDT, Olaoluwa Osuntokun via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
As everyone in the Bitcoin space knows, there is a massive scaling debate
going on. One side wants to increase the block size via segwit, while the
other side wants to increase via hard fork. I have strong opinions on the
topic but I won’t discuss them here. The point of the matter is we are
seeing the politicization of protocol level changes. The critiques of these
changes are slowly moving towards critiques based on who is submitting the
BIP -- not what it actually contains. This is the worst thing that can
happen in a meritocracy.

*Avoiding politicization of technical changes in the future*

I like what Tom Elvis Judor did when he submitted his MimbleWimble white
paper to the technical community. He submitted it under a pseudonym, over
TOR, onto a public IRC channel. No ego involved — only an extremely
promising paper. Tom (and Satoshi) both understood that it is only a matter
of time before who they are impedes technical progress of their system.

I propose we move to a pseudonymous BIP system where it is required for the
author submit the BIP under a pseudonym. For instance, the format could be
something like this:

BIP: 1337

Author: 9458b7f9f76131f18823d73770e069d55beb271b@protonmail.com

BIP content down here

The hash “6f3…9cd0” is just my github username, christewart, concatenated
with some entropy, in this case these bytes:
639c28f610edcaf265b47b0679986d10af3360072b56f9b0b085ffbb4d4f440b

and then hashed with RIPEMD160. I checked this morning that protonmail can
support RIPEMD160 hashes as email addresses. Unfortunately it appears it
cannot support SHA256 hashes.

There is inconvenience added here. You need to make a new email address,
you need to make a new github account to submit the BIP. I think it is
worth the cost -- but am interested in what others think about this. I
don't think people submitting patches to a BIP should be required to submit
under a pseudonym -- only the primary author. This means only one person
has to create the pseudonym. From a quick look at the BIPs list it looks
like the most BIPs submitted by one person is ~10. This means they would
have had to create 10 pseudonyms over 8 years -- I think this is
reasonable.

*What does this give us?*

This gives us a way to avoid politicization of BIPs. This means a BIP can
be proposed and examined based on it’s technical merits. This levels the
playing field — making the BIP process even more meritocratic than it
already is.

If you want to claim credit for your BIP after it is accepted, you can
reveal the preimage of the author hash to prove that you were the original
author of the BIP. I would need to reveal my github username and
“639c28f610edcaf265b47b0679986d10af3360072b56f9b0b085ffbb4d4f440b”

*The Future*
Politicization of bitcoin is only going to grow in the future. We need to
make sure we maintain principled money instead devolving to a system where
our money is based on a democratic vote — or the votes of a select few
elites. We need to vet claims by “authority figures” whether it is Jihan
Wu, Adam Back, Roger Ver, or Greg Maxwell. I assure you they are human —
and prone to mistakes — just like the rest of us. This seems like a simple
way to level the playing field.

Thoughts?

-Chris
-------------------------------------
Hi all,

I have put up an initial draft of the full 'bip-genvbvoting' (generalized version bits voting) specification for review:

https://github.com/sanch0panza/bips/blob/bip-genvbvoting/bip-genvbvoting.mediawiki

Comments are again most welcome - and my thanks to those reviewers who took a look at the initial rough draft [1].

A prime goal is that this BIP proposal should end up allowing full backward compatibility with the existing BIP9 state machine, if wishing to do so for a deployment. In fact, this will be necessary to maintain full compatibility with any ongoing deployments.

I will work on a reference implementation which might also turn up inadequacies of the proposed specification. A link to this will follow once it is mature enough for review.

Sancho

[1]https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013969.html
-------------------------------------
I woudl like to propose a BIP that works something like this:

1. Allow users to signal readiness by publishing an EB. This EB is an
absolute upper bound, and cannot be overridden by miners. Current EB is
1MB, the status-quo.   Maybe EB can be configured in a config file, not a
UI, since it's an "advanced" feature.

2. Miners can also signal readiness by publishing their own EB in a block.

3. If 95% of blocks within a one month signalling period contain an EB
greater than the previous consensus EB, a fork date is triggered at 6
months using the smallest 5th percentile EB published. (Other times can be
selected, but these are fairly conservative, looking for feedback here).
Miner signalling is ignored during the waiting period.

4. Block heights used for timing

5. After 6 months, any users which already have the new EB or greater begin
actually using it to validate transactions. Users use the EB or the latest
95% consensus triggered value - whichever is less.   This means that the
portion of users that originally signaled for the increase do not have to
upgrade their software to participate in the hard fork.

6. Core can (optionally) ship a version with a default EB in-line with
their own perceived consensus.

7. Some sort of versioning system is used to ensure that the two networks
(old and new) are incompatible... blocks hashed in one cannot be used in
the other.

Any users which don't already have the new EB or greater should update
their EB within the 6 month period - or they will be excluded from the
majority fork.

It would be in the best interests of major exchanges and users would to
publicly announce their EB's.

Users are free to safely set very high EB levels, based on their current
hardware and network speeds. These EB levels don't cause those users to
accept invalid blocks ever. They are safe because block size transitions
behave like normal hard forks with high miner consensus (95%).

No code changes will be needed to fork the network as many times as both
users and miners feel the need to do so.  (Bitcoin core is off the hook for
"scaling" issues...forever!)

If a smaller block size is needed, a reduced size can also be published and
agreed upon by *both* users and miners using a the same mechanism, but the
largest 5th percentile is used.   In other words... the requires broad
consensus to deviate from status quo and fork.

Any new node can simply follow these rules to validate all the blocks in a
chain... even if the sizes changes a lot (at most twice per year).
-------------------------------------
Process note: It looks like the BIPs have never been posted to
bitcoin-dev, only high-level discussion around the idea. As I understand
it, this is *not* sufficient for BIP number assignment nor
(realistically) sufficient to call it a hard "proposal" for a change to
consensus rules.

Would love to get feedback from some others who are looking at deploying
real-world sidechains, eg the RSK folks. We can't end up with *two*
protocols for sidechains in Bitcoin.

Comments on BIP 1:

At a high level, I'm rather dissapointed by the amount of data that is
going into the main chain here. Things such as a human readable name
have no place in the chain, IMO. Further, the use of a well-known
private key seems misplaced, why not just track the sidechain balance
with something that looks like `OP_NOPX genesis_block_hash`?

I'm not convinced by the full semantics of proposal/ack of new
sidechains. Given the lack of convincing evidence of that "Risk of
centralisation of mining" drawback in section 4.3 of the sidechains
paper has been meaningfully addressed, I'd say its pretty important that
new sidechains be an incredibly rare event. Thus, a much simpler system
(eg a version-bits-based upgrade cycle with high threshold) could be
used to add new sidechains based on well-known public parameters.

The semantics of the deposit process seem very suboptimal. You note that
only one user can deposit at a time, but this seems entirely
unnecessary. As implemented in the first Elements Alpha release (though
I believe subsequently removed in later versions due to the nature of
Elements of targeting asymmetric "federated" sidechains), if you have
outputs that look like `OP_NOPX genesis_block_hash` as the sidechain
deposit/storage address, deposit can be fully parallel. To reduce
blockchain bloat, spending them for the purpose of combining such
outputs is also allowed. You could even go further and allow some new
sighash type to define something like SIGHASH_ALL|SIGHASH_ANYONECANPAY
which further specifies some semantics for combining inputs which all
pay into the same output.

Finally, you may also want to explore some process for the removal of
sidechain balances from the main chain. As proposed it seems like a
sidechain might, over time, fade into an insecure state as mining power
shifts and new miners no longer consider it worth the value to mine an
old sidechain (as has happened over time with namecoin, arguably).


Comments on BIP 2:

I may be missing something, but I find the security model here kind of
depressing...Not only do hashpower-secured sidechains already have a
significantly reduced security level, but now you're proposing to
further (drastically) reduce it by requiring users to potentially pay in
excess of the value an attacker is willing to pay to keep their chain
secure, on a recurring basis? It seems like if a chain has 10 BTC stored
in it, and I wish to reorg it for a potential gain of, lets say, 6 BTC,
I can pay 6 * 1 BTC (1 per block) to reorg it, and users on the chain
would be forced to pay >6 BTC to avoid this?

While I appreciate the desire to implement the proposed mitigation in
section 4.3 of the sidechains paper (delegating the mining effort of a
merge-mined sidechain to an external entity), I believe it was primarily
referencing pooling the sidechain work, not blindly taking the highest
bidder. I suppose, indeed, that, ultimately, as long as the sidechain is
of relatively small value in comparison to BTC, miners do not risk the
value of their BTC/mining investment in simply taking the highest bidder
of a merge-mined block, even if its a clear attack, but I don't think
thats something to be celebrated, encouraged, or designed to be possible
by default. Instead, I'd, in line with Peter Todd's (and others')
objection to merged mining generally, call this one of the most critical
issues with the security model.

Ultimately, I dont believe your proposal here really solves the drawback
in section 4.3 of the paper, and possibly makes it worse. Instead, it
may be more useful to rely on a high threshold for the addition of new
sidechains, though I'd love to see discussion on this point specifically
on this list. Further, I'd say, at a minimum, a very stable
default-available low-bandwidth implementation of at least the
pool-based mitigation suggested in the paper must exist for something
like this to be considered readily stable enough to be deployed into the
Bitcoin ecosystem.

Matt

On 12/01/17 13:38, Paul Sztorc via bitcoin-dev wrote:

-------------------------------------
Daniele Pinna,

Can you please not forget to supply us more details on the claims made regarding the reverse engineering of the Asic chip?

gmaxwell told me that back even in S7 chips its possible to set the SHA256 midstate/IV instead of just resetting it to the standard SHA256 IV. This essentially allows you to re-use midstates, which is one of the key necessary features for the ASICBOOST optimization to work. From the chip's perspective there is not much difference between the covert and overt optimization methods, particularly given that the whole IV/midstate vector can be set.

The covert method just requires more work than the overt method:. overt you just permutate the version bits, vs the covert one requires you find partial hash collisions of the tx merkle root. The extra work to find the partial tx merkle root hash collisions could be done at different stages in the mining system... some speculate that it could be done in the miner's FPGA.

Not sure how exactly gmaxwell (or his friend) did it. I don't currently own any mining hardware nor the time to do it myself.

Cheers,
Praxeology Guy
-------------------------------------

Wouldn't the solution be for nodes to use whatever mechanism an attacker
uses to determine less commonly available blocks and choose to store a
random percentage of them as well as their deterministic random set?

IE X blocks end of chain (spv bootstrap), Y% deterministic random set,  Z%
patch/fill set to deter attacks
-------------------------------------
approach. It's tidy, systematic and precise.

The SI system is great, but it's nice if you pick a base unit that is easy
for intuition to comprehend.

It is a fact that I weigh approximately .000,000,000,000,000,000,000,014
Earth masses. If we arrived at rough consensus that this was a cumbersome
way to express the mass of a human, we might then find a group of people
making the superficially sensible proposal that we use SI prefixes and say
I weigh 14 yoctoearths. This would be tidy, systematic and precise, but
that might not be enough to make it the best option. It might be even
better to choose a base unit that human intuition can make sense of, and
THEN add prefixes as needed.

I dislike the name "bits" but I think 100 satoshis does make a nice base
unit. If we cannot crowdsource a more inspiring label we may be stuck with
bits just due to linguistic network effects.

-Ethan



On Fri, Dec 15, 2017 at 1:27 AM, Marcel Jamin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------


Even in such an event, my personal view is the bitcoin owner should have the
freedom to choose upgrade to secure his/her coins or to leave the door
open for the first hacker to assume the coins - yet the bitcoin network
that he/she trusts should not act like a hacker to assume his/her coins.

daniel


-------------------------------------
If you are going to rely on human verification of addresses, the best way
might be map it to words.

For example, with a 6000 word list, a 25 byte address (with a checksum)
could be mapped to 16 words like this:

vocally           acquire        removed     unfounded
euphemism    sanctuary    sectional     driving
entree            freckles    aloof           vertebrae
scribble          surround      prelaw         effort

In my opinion, that is much faster to verify than this:

13gQFTYHuAcfnZjXo2NFsy1E8JGSLwXHCZ

or

bc1qrp33g0q5c5txsp9arysrx4k6zdkfs4nce4xj0gdcccefvpysxf3qccfmv3

Although I really do love Bech32.

On Mon, Oct 30, 2017 at 9:13 AM, shiva sitamraju via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I am also concerned.  However, this proposal allows two POWs to coexist and
allows for gradual transitions. This is hopefully a less disruptive
approach since it allows cooperative miners to migrate over time.  And of
course, as a soft-fork it keeps backwards compatibility with existing
software.

On Thu, Nov 2, 2017 at 4:55 PM Tao Effect <contact@taoeffect.com> wrote:

-------------------------------------
Mark, this seems an awful lot like an answer of "no", to my question "Is
there a contingency plan in the case that the incumbent chain following the
Bitcoin Core consensus rules comes under 51% attack?" - is this a correct
interpretation?

In fact, beyond a no, it seems like a "no, and I disagree with the idea of
creating one".

So if Bitcoin comes under successful 51%, the project, in your vision, has
simply failed?

*Ben Kloester*

On 10 October 2017 at 13:19, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thu, Sep 28, 2017 at 07:45:02PM -0700, Mark Friedenbach wrote:

I specifically outlined a scenario where that text isn't relevant: *all*
transaction in a block can be paying out of band.


You're making the incorrect assumption that all transactions have to be
broadcast publicly; they don't.


It certainly does not. It simply adds another level of complexity and overhead
to the out-of-band payment situation, which is not desirable. If we can't
eliminate out of band payments entirely, we do not want to make the playing
field of them even more unbalanced than it already is.

This is a typical academic proposal that only considers first order effects
while ignoring second order effects.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
Hello Alex,

Thank you for the thoughtful reply.  


Yes, it is different.  It’s different because the future network upgrade to larger blocks includes a loosening of the consensus ruleset whereas previous upgrades have included a tightening of the rule set.  (BTW—this is not my proposal, I am describing what I have recently learned through my work with Bitcoin Unlimited and discussions with miners and businesses).  

With a tightening of the rule set, a hash power minority that has not upgraded will not produce a minority branch; instead they will simply have any invalid blocks they produce orphaned, serving as a wake-up call to upgrade.  

With a loosening of the consensus rule set, the situation is different: a hash power minority that has not upgraded will produce a minority branch, that will also drag along non-upgraded node operators, leading to potential confusion.  The idea behind orphaning the blocks of non-upgraded miners was to serve as a wake-up call to upgrade, to reduce the chances of a minority chain emerging in the first place, similar to what happens automatically with a soft-forking change.  If one's worry is a chain split, then this seems like a reasonable way to reduce the chances of that worry materializing.  The Level 3 anti-split protection takes this idea one step further to ensure that if a minority branch does emerge, that transactions cannot be confirmed on that branch.


I’m very confident that most people do NOT want a split, especially the miners.  The upgrade to larger blocks will not happen until miners are confident that no minority chain will survive.  


I agree that the soft-fork mechanism usually works well.  I believe this mechanism (or perhaps a modified version of it) to increase the block size limit will likewise work well.  All transactions types that are currently valid will be valid after the upgrade, and no new types of transactions are being created.  The “block-size-limit gene" of network nodes is simply evolving to allow the network to continue to grow in the way it has always grown. (If you’re interested, here is my talk at Coinbase where I discuss this: https://www.youtube.com/watch?v=pWnFDocAmfg <https://www.youtube.com/watch?v=pWnFDocAmfg>)


My read is completely different.  I still have never talked with a person in real life who doesn’t want the block size limit to increase.  Indeed, I have met people who worry that Bitcoin Unlimited is “trying to take over”—and thus they are worried for other reasons—but this couldn’t be further from the truth.  For example, what most people within BU would love to see is a simple patch to Bitcoin Core 0.14 that allows node operators to adjust the size of blocks their nodes will accept, so that these node operators can follow consensus through the upgrade if they choose to.  

This is not a fight about “Core vs. BU”; Bitcoin’s future is one of “genetic diversity” with multiple implementations, so that a bug in one doesn’t threaten the network as a whole.  To me it seems this is largely a fight about whether node operators should be easily able to adjust the size of blocks their nodes accept.  BU makes it easy for node operators to accept larger blocks; Core doesn’t believe users should have this power (outside of recompiling from source, which few users can do).  


Once again, this is not my proposal.  I am writing about what I have come to learn over the past several weeks.  When I first heard about these ideas, I was initially against them too.  They seemed harsh and merciless.  It wasn’t until I got out their and started talking to more people in the community that the rationale started to make sense to me: the biggest concern people had was a chain split!

So I guess the “ethics” here depend on the lens through which one is looking. People who believe that an important outcome of the upgrade to larger blocks is to avoid a blockchain split may be more favourable to these ideas than people who want the upgrade to result in a split (or are OK with a split), as it sounds like you do (is this true that you’d rather split than accept blocks with more than 1,000,000 bytes of transaction information in them? Sorry if I misunderstood).  

But if one's intention is to split and not follow the majority hash power when blocks become larger, then why not change the proof-of-work?  This would certainly result in a peaceful splitting, as you said you desire.  

Best regards,
Peter R




-------------------------------------
Can you please not forget to supply us more details on the claims made
regarding the reverse engineering of the Asic chip?

It is absolutely crucial that we get these independently verified ASAP.

Daniele

Message: 2
-------------------------------------
On Thu, Sep 28, 2017 at 06:06:29PM -0700, Mark Friedenbach via bitcoin-dev wrote:

I think CPFP allows this to break: a miner getting paid out of band
would just make the block look like:

    (1) 100kB of 5s/byte transactions
    (2) 850kB of 35s/byte transactions
    (3) 50kB of 95s/byte transactions, miner paying themselves

As long as every transaction in (1) has an output spent in (3), that seems
like it would be perfectly legitimate for CPFP. 

I think it would be cheaper overall than the fee refund transactions
as well. People making arrangements with miners directly would have to
pay for block space to cover:

   out:
     30-40B dest address
     30-40B change address
     10B    cpfp link
   in:
     36B    cpfp txid

then to actual spend their change:

   in:
     36B+70B txid+idx + witness for change

for a total of 142-162B plus 70B witness, as well as some sort of out of
band payment to the miner (paying fees directly to miners via a lightning
channel, comes to mind). 

If I understand your suggestion correctly, it would look like:

   coinbase:
     30-40B fee overflow payment back to transactor

   out:
     30-40B dest address
     30-40B change address
     30-40B fee-overflow output marker

and to spend their change:

   in:
     36B+70B txid+idx + witness for change
     36B+70B txid+idx + witness for fee overflow

for a total of 192-232B plus 140B witness; so that's 40%-50% more block
weight used. The fee overflow would probably be pretty small amounts,
as well, so kind of annoying to actually collect.

If you end up with two change addresses per tx generally, that also seems
like it might it annoyingly easy to link your transactions together
(unless fees end up getting coinjoined or run through satoshidice or
something). If you end up sending lots of fee overflows to a single
address, that links your txes too of course.


A miner might be willing to do that in order to charge a two-part tariff:
ie, a very high "subscription" fee that's paid once a year or similar,
along with very low per-tx fees. The only reason I can think of why
someone would buy a subscription is if the miner's effectively a monopoly
and submitting transactions via the p2p network isn't reliable enough;
the whole point of a two-part tariff is to be as expensive as each user
can bear, so it won't ever be any cheaper.


FWIW, I think reliability-based-price-discrimination might allow higher
mining revenue via having txes with differing fee rates in the same
block: eg if people are happy to pay 100s/byte for confirmation within
30 minutes, and likewise willing to pay 10s/byte for confirmation within
3 hours, and there aren't enough transactions of either type to hit the
block size limit, then a monopoly miner / mining cartel would do better
by accepting 100s/byte txes at any time, while accepting 10s/byte txes
in any given block, but only with about a 1-in-7 chance for any given tx.

Looking at estimatefee.com, there's currently apparently ~200kB of 82s/B
or more transactions, while to fill a 1MB block you'd have to go all the
way down to 2.1s/B -- so if you have to charge all txes at the marginal
fee rate, that's 200kB at 82s/B for 0.16 BTC rather than 1MB at 2.1s/B
for 0.021 BTC.


I think ideally, it would probably be better to let the block weight
limit adjust to deal with high-frequency components to changes in demand,
and have fee rates adjust more slowly to address the long-term trends
in changes to demand: if fee rates only adjust slowly, then they're
(by definition) easily predictable and you don't *have* to have much
concern about getting them wrong. (You'd still need to add the correct
fee at the time you want to publish a pre-signed transaction that was
very old, but CPFP isn't too bad at that).

Cheers,
aj


-------------------------------------
I'm somewhat curious what the authors envisioned the real-world implications of this model to be. While blindly asking users to enter what they're willing to pay always works in theory, I'd imagine in such a world the fee selection UX would be similar to what it is today - users are provided a list of options with feerates and expected confirmation times from which to select. Indeed, in a world where users pay a lower fee if they paid more than necessary fee estimation could be more willing to overshoot and the UX around RBF and CPFP could be simplified greatly, but I'm not actually convinced that it would result in higher overall mining revenue.

The UX issues with RBF and CPFP, not to mention the UX issues involved in optimizing for quick confirmation are, indeed, quite significant, but I believe them to be solveable with rather striaght-forward changes. Making the market more useable (for higher or lower overall miner revenue) may be a sufficient goal, however, to want to consider something like this.

On September 28, 2017 9:06:29 PM EDT, Mark Friedenbach via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
-------------------------------------
If the sender doesn't control the receiver's network connection, then the
information the receiver gains by watching the mempool is if the
transaction has propagated across the bitcoin network. This is useful to
know in all kinds of situations.


Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com>

On Tue, Jan 3, 2017 at 3:06 PM, adiabat <rx@awsomnet.org> wrote:

-------------------------------------
On Mon, May 22, 2017 at 03:05:49AM -0400, Russell O'Connor via bitcoin-dev wrote:

To be clear, what math operations do you mean by "⋅" and "×"?

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke@dashjr.org> wrote:



That poll shows 63% of votes want a larger than 1 MB block by this summer.
How do you go from that to "the community opposes any block increase ever"?
It shows the exact opposite of that.



Is this causing a problem now? If so, what?




The reason people stop running nodes is because there's no incentive to
counteract the resource costs. Attempting to solve this by making blocks
*smaller* is like curing a disease by killing the patient. (Incentivizing
full node operation would fix that problem.)

- t.k.
-------------------------------------
Hi Other Chris,

Thanks for pointing this out. Here are my responses.

On 12/4/2017 3:11 PM, Chris Stewart wrote:
/u/almkglor on reddit. I'm going to share here for visibility.
complicit in theft. Drivechains have 1008-block cycles ostensibly to
protect against theft, so that someone can "raise the alarm" and tell
miners to downvote a particular theft withdrawal, but that sounds too
much like centralized collusion to me.

Well, that is simply not what "centralized" means. "Centralized" means
that one person has special, irreplaceable influence. In contrast,
"decentralized" means that the process is not uniquely influenced by
what any *one* individual does or believes. Which is the case here: each
miner can independently make a decision about what to check, how to
check it, and what to do as a result. They could do undertake this
process, even if they ignored what everyone else was doing.

default. If a theft withdrawal is done on the mainchain, some sidechain
nodes call up their miner friends (which makes me worry about miner
centralization) to downvote it instead.
theft withdrawal is done? And another, and another? This weakens the
peg: while a theft withdrawal is on-going, a genuine withdrawal can't be
posted (at least as I understand Sztorc's explanation). This chokes the
sidechain withdrawal.

This is a good question.

The answer is that there are mechanisms in place to address these
problems. Contrary to the reviewer's interpretation, multiple
withdrawal-attempts *are* allowed simultaneously. (This part of design
may have changed between now and Nov 2015, and I'm not sure if it was
ever documented anywhere until very recently). Second, only one
withdrawal can be upvoted at a time [ie, per sidechain per main:block].
Third, upvoting one withdrawal automatically downvotes all of the other
withdrawals (all in-progress withdrawals for that sidechain). Thus, we
have the asymmetry we desire. An "auditor class" can ignore all of the
withdrawals, until a significant amount of time has been invested in one
candidate. This makes the attempt more futile. Since they are unlikely
to be meaninglessly harassed, this opens the door to a "farmer class"
who can take it upon themselves to make sure the good withdrawals get
in, and get upvotes (especially early upvotes). Thus, the system can
tolerate a large "loafer class" who just lazily upvotes everything (or
nothing, or only the front-runner).

downvotes withdrawal transactions (WT). This obviously isn't ideal
because a withdrawal will never occur from the drivechain if enough
miners employ this strategy -- which seems to be the most profitable
strategy.

I agree that miners should "always accept BMM bribes". In my recent
email to Matt Corallo, I described that this is basically the same as
saying that miners should "always mine on top of the heaviest chain".
(Of course, in mainchain Bitcoin miners are advised to mine atop the
heaviest *valid* chain, which is different from this case. It is
different because blind-merged-miners have no way of knowing if the
sidechain blocks they are mining are valid or not, which is kind of the
point. In practice I estimate that between 70% and 100% of today's
hashpower is already mining the mainchain "blind" -- through some
combination of pools, spv and spy mining.)

I don't agree with the conclusion (that the optimal policy is "always
downvoting", see above), but even if this analysis turns out to be
correct, it isn't a total disaster. The result (which is in the original
Nov 2015 specification) is that miners are the ones who perform the
atomic swaps. Then they walk the coins side-to-main (which, at this
point, are *their* coins). As long as there are a few large mining
groups, competition will drive the atomic swap fees down to negligible
levels. This slightly encourages mining to consolidate into two large
pools...but of course that is also true of the status quo!

Paul
 

-------------------------------------
Sure, your math is pretty much entirely irrelevant because scaling systems
to massive sizes doesn't work that way.

At 400B transactions per year we're looking at block sizes of 4.5 GB, and a
database size of petabytes. How much RAM do you need to process blocks like
that? Can you fit that much RAM into a single machine? Okay, you can't fit
that much RAM into a single machine. So you have to rework the code to
operate on a computer cluster.

Already we've hit a significant problem. You aren't going to rewrite
Bitcoin to do block validation on a computer cluster overnight. Further,
are storage costs consistent when we're talking about setting up clusters?
Are bandwidth costs consistent when we're talking about setting up
clusters? Are RAM and CPU costs consistent when we're talking about setting
up clusters? No, they aren't. Clusters are a lot more expensive to set up
per-resource because they need to talk to eachother and synchronize with
eachother and you have a LOT more parts, so you have to build in
redundancies that aren't necessary in non-clusters.

Also worth pointing out that peak transaction volumes are typically 20-50x
the size of typical transaction volumes. So your cluster isn't going to
need to plan to handle 15k transactions per second, you're really looking
at more like 200k or even 500k transactions per second to handle
peak-volumes. And if it can't, you're still going to see full blocks.

You'd need a handful of experts just to maintain such a thing. Disks are
going to be failing every day when you are storing multiple PB, so you
can't just count a flat cost of $20/TB and expect that to work. You're
going to need redundancy and tolerance so that you don't lose the system
when a few of your hard drives all fail within minutes of eachother. And
you need a way to rebuild everything without taking the system offline.

This isn't even my area of expertise. I'm sure there are a dozen other
significant issues that one of the Visa architects could tell you about
when dealing with mission-critical data at this scale.

--------

Massive systems operate very differently and are much more costly per-unit
than tiny systems. Once we grow the blocksize large enough that a single
computer can't do all the processing all by itself we get into a world of
much harder, much more expensive scaling problems. Especially because we're
talking about a distributed system where the nodes don't even trust each
other. And transaction processing is largely non-parallel. You have to
check each transaction against each other transaction to make sure that
they aren't double spending eachother. This takes synchronization and
prevents 500 CPUs from all crunching the data concurrently. You have to be
a lot more clever than that to get things working and consistent.

When talking about scalability problems, you should ask yourself what other
systems in the world operate at the scales you are talking about. None of
them have cost structures in the 6 digit range, and I'd bet (without
actually knowing) that none of them have cost structures in the 7 digit
range either. In fact I know from working in a related industry that the
cost structures for the datacenters (plus the support engineers, plus the
software management, etc.) that do airline ticket processing are above $5
million per year for the larger airlines. Visa is probably even more
expensive than that (though I can only speculate).
-------------------------------------
Thank you Marcos,

Though written in Rust, bitcrust-db is definitely usable as pluggable
module as its interface will be roughly some queries, add_tx and
add_block with blobs and flags. (Bitcrust internally uses a
deserialize-only model, keeping references to the blobs with the parsed
data).  

However, from Core's side I believe network and storage are currently
rather tightly coupled, which will make this far from trivial.

Regardless, I am also hoping (with funding & a team) to build a Bitcrust
networking component as well to bring a strong competitor to the market.

best,
Tomas



On Fri, Apr 7, 2017, at 09:55, Marcos mayorga wrote:

-------------------------------------
See https://github.com/bitcoin/bips/blob/master/bip-0150.mediawiki and
https://github.com/bitcoin/bips/blob/master/bip-0151.mediawiki

On Tue, May 9, 2017 at 12:09 PM, Raystonn . via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I have a proposal that would allow each user to optionally configure the
maximum block weight at a support threshold.

It recognizes that there is no need for off chain bickering, by
providing a mechanism that lets each users freely choose their own
parameters while still maintaining full coordination of any changes.

The BIP can be found here: 

https://github.com/tomasvdw/bips/blob/master/bip-changing-the-maximum-block%20weight-based-on-a-support-threshold.mediawiki

It is worth noting that this proposal does neither gives more power to
miners nor reduces decentralization. Miners still rely on their blocks
being accepted by economic nodes to sell their minted coins. This
proposal doesn't change that. 

Regards,
Tomas van der Wansem
bitcrust

-------------------------------------
I asked adam back at hcpp how the block chain would be secured in the long
term, once the reward goes away.  The base idea has always been that fees
would replace the block reward.

At that time fees were approximately 10% of the block reward, but have now
reached 45%, with 50% potentially being crossed soon

https://fork.lol/reward/feepct

While this bodes well for the long term security of the coin, I think there
is some legitimate concern that the fee per tx is prohibitive for some use
cases, at this point in the adoption curve.

Observations of segwit adoption show around 10% at this point

http://segwit.party/charts/

Watching the mempool shows that the congestion is at a peak, though it's
quite possible this will come down over the long weekend.  I wonder if this
is of concern to some.

https://dedi.jochen-hoenicke.de/queue/more/#24h

I thought these data points may be of interest and are mainly FYI.  Though
if further discussion is deemed appropriate, it would be interesting to
hear thoughts.
-------------------------------------
A fun exercise to be sure, but perhaps off topic for this list?

-------------------------------------
On Mon, Sep 11, 2017 at 9:03 PM, Mark Friedenbach via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2017-09-07-merkleized-abstract-syntax-trees/


original bip114:
https://github.com/bitcoin/bips/blob/775f26c02696e772dac4060aa092d35dedbc647c/bip-0114.mediawiki
revised bip114: https://github.com/jl2012/bips/blob/vault/bip-0114.mediawiki
https://github.com/jl2012/bitcoin/commits/vault
from https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-September/014963.html

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
I think this discussion started from the block bloom filter where
there is a bloom filter commitment in the block which can be
downloaded and is much smaller than the block.  An SPV node based on
that model would download headers and bloom filters, verify the bloom
filter is committed to, and test locally if any addresses managed by
the wallet are in the filter (or false positives for being in it), and
then download blocks with hits.  Apparently there are maybe 50% more
compact alternatives to bloom filters but people have been using bloom
filter as a short-hand for that.  The block bloom filter does seem to
have higher overhead than the query model, but it offers much better
privacy.  I think there was previous discussion about maybe doing
something with portions of blocks so you can know which half or
quarter of the block etc.

Adam


On 4 January 2017 at 10:13, Jorge Timón via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Some questions:

Does this require information to be added to blocks, or can it work today
on the existing format?

Does this count number of transactions or their total length? The block
limit is in bytes rather than number of transactions, but transaction
number can be a reasonable proxy if you allow for some false negatives but
want a basic sanity check.

Does this allow for proofs of length in the positive direction,
demonstrating that a block is good, or does it only serve to show that
blocks are bad? Ideally we'd like an extension to SPV protocol so light
clients require proofs of blocks not being too big, given the credible
threat of there being an extremely large-scale attack on the network of
that form.


On Wed, Mar 22, 2017 at 1:47 AM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
If we have a problem with a UTXO set that is too large, seems like maybe
the fair way to approach it is to enforce a limit on the growth of the UTXO
set.

Miners would eventually be forced to generate blocks that are UTXO neutral
and would factor that into their algorithm for prioritizing transactions.
Users who wish to generate a lot of outputs would need to find a buddy with
lots of inputs to consolidate and create a tumble-bit with them. A market
would spring up that would charge people for creating UTXOs and pay them
for disposing of UTXOs.

On Fri, Jul 21, 2017 at 3:59 PM, Lucas Clemente Vella via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Den 1 apr. 2017 14:33 skrev "Jorge Timón via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org>:

Segwit replaces the 1 mb size limit with a weight limit of 4 mb.


That would make it a hardfork, not a softfork, if done exactly as you say.

Segwit only separates out signature data. The 1 MB limit remains, but would
now only cover the contents of the transaction scripts. With segwit that
means we have two (2) size limits, not one. This is important to remember.
Even with segwit + MAST for large complex scripts, there's still going to
be a very low limit to the total number of possible transactions per block.
And not all transactions will get the same space savings.
-------------------------------------
What is the probability that a 65% threshold is too low and can allow a "surprise miner attack", whereby miners are kept offline before the deadline, and brought online immediately after, creating potential havoc?

(Nit: "simple majority" usually refers to >50%, I think, might cause confusion.)

-Greg Slepak

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
One of the purported benefits of a soft-forking change (a tightening of the consensus rule set) is the reduced risk of a blockchain split compared to a loosening of the consensus rule set.  The way this works is that miners who fail to upgrade to the new tighter ruleset will have their non-compliant blocks orphaned by the hash power majority.  This is a strong incentive to upgrade and has historically worked well.  If a minority subset of the network didn’t want to abide by the new restricted rule set, a reasonable solution would be for them to change the proof-of-work and start a spin-off from the existing Bitcoin ledger (https://bitcointalk.org/index.php?topic=563972.0).

In the case of the coming network upgrade to larger blocks, a primary concern of both business such as Coinbase and Bitpay, and most miners, is the possibility of a blockchain split and the associated confusion, replay risk, etc.  By applying techniques that are known to be successful for soft-forking changes, we can likewise benefit in a way that makes a split less likely as we move towards larger blocks.  Two proposed techniques to reduce the chances of a split are:

1. That miners begin to orphan the blocks of non-upgraded miners once a super-majority of the network hash power has upgraded. This would serve as an expensive-to-ignore reminder to upgrade.

2. That, in the case where a minority branch emerges (unlikely IMO), majority miners would continually re-org that minority branch with empty blocks to prevent transactions from confirming, thereby eliminating replay risk.

Just like after a soft forking change, a minority that does not want to abide by the current ruleset enforced by the majority could change the proof-of-work and start a spin-off from the existing Bitcoin ledger, as suggested by Emin.  

Best regards,
Peter R




-------------------------------------
I agree, it is only a good idea in the event of a quantum computing threat
to the security of Bitcoin.

On Tue, Aug 22, 2017 at 9:45 AM, Chris Riley via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hi ZmnSCPxj,

Basically, in case of a sidechain fork, the mainchain considers the longest

What happens in the case that the provided merkle tree hash has a invalid
transaction in it? Wouldn't this mean that the mainchain nodes would think
the longest work chain is the valid chain, and it would kill off any
consensus valid chain that sidechain miners are trying to construct? It
seems that a malicious miner could extend the chain to whatever the SPV
proof block height is and make it impossible for the chain to reorg after
that. I guess if that is a sufficiently long block waiting period it may
not be a realistic concern, but something to think about any way.

Just a side note -- I think it should be highly recommended that the
coinbase maturity period on the sidechain to be longer than 288 (or
whatever we decide on the parameter). This incentivizes the s:miners to
work together to extend the chain by working with other s:miners (otherwise
they won't be able to claim their bribes). If they do not work together
they will not be able to spend their s:coinbase_tx outputs until they
extend their own sidechain by 288 blocks meaning they need to tie up a
large amount of capital to go rogue on their fork.

Another interesting thing might be to use the OP_WITHDRAWPROOFVERIFY op code
<https://github.com/ElementsProject/elements/blob/elements-0.14.1/src/script/interpreter.cpp#L1420>
used in the elements project. Since the cannonical merkle root hashes are
included in the mainchain, we can provide a merkle proof to the bitcoin
blockchain to initiate a withdrawl from the sidechain. I wrote up a blog
post on how OP_WPV works here
<https://medium.com/@Chris_Stewart_5/what-can-go-wrong-when-transferring-coins-into-a-sidechain-with-op-withdrawproofverify-b2f49b02ab60>.
This allows us to prove that a transaction occurred on the sidechain to
lock up those funds.

-Chris
​
-------------------------------------
Here is a Merkle set data structure, whose format may be useful for utxo
commitments in Bitcoin blocks. It may also be useful for any other
distributed computation which wants an audit trail:

https://github.com/bramcohen/MerkleSet

This is a fairly straightforward Patricia Trie, with a simple format and a
simple reference implementation plus a performance optimized non-reference
implementation which is much more cache coherent. It will need to be ported
to C and be properly turned before the potential performance gains can be
realized though.

The clever things which affect the format spec are:

It uses blake2s as the internal hash function. This is the fastest hash
function to use on 512 bit inputs because blake2b uses a 1024 bit block
size. It might make sense to use a hypothetical variant of blake which is
optimized for 64 bits with a 512 bit block size, but that hasn't been
specified. Sha256 would take advantage of hardware acceleration, but that
isn't available everywhere.

Two bits of security are sacrificed to include metadata inline which halves
the CPU cost of hashing.

When one side of a node is empty and the other contains exactly two things
the secure hash of the child is adopted verbatim rather than rehashing it.
This roughly halves the amount of hashing done, and makes it more resistant
to malicious data, and cleans up some implementation details, at the cost
of some extra complexity.
-------------------------------------


On 2017-04-16 17:04, Erik Aronesty via bitcoin-dev wrote:

The write time for configuring a FPGA with a fresh bitstream is measured 
in tens of milliseconds.



Unused circuits don't consume power, which is the main cost in running a 
miner.


-------------------------------------
Is there an issue with the current difficulty adjustment algorithm? It's
worked very well as far as I can tell. Introducing a new one seems pretty
risky, what would the benefit be?

On Nov 2, 2017 4:34 PM, "Scott Roberts via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Should a BIP be submitted or are there any suggestions for the proposal ?
-------------------------------------
On 01/07/2017 03:10 PM, Aymeric Vitte via bitcoin-dev wrote:

The level of control over a majority of full nodes is irrelevant. If
this was truly a measure of control over Bitcoin someone would simply
spin up a bunch of nodes and take control at trivial cost.

e

-------------------------------------
Good evening ZmnSCPxj,


That you for your considered discussion.


Am I wrong to think that any fullnode can validate blocks conform to a probability distribution? In my understanding after adoption of the proposal, any full node could validate all properties that a block has that they now validate, apart from block size, and additionally that the block conforms to a probability distribution. It seems a yes-no result. Let us assume that such a probability distribution exists since the input is a probability.

Before or after the proposal, miners could falsify transactions if there is a feasible way for them to do this. The introduction of the proposal does not change that fact. At the moment the incentive to falsify transactions is to fill blocks so that real transactions must pay the highest possible fees in the auction for limited transaction bandwidth resulting in a net gain for miners. Simply making bigger blocks serves no economic purpose in itself, since the miners we presume must pay the fees for their falsified transactions, there is no net gain, the fee will be distributed through the pool. Unless, by miners, I may presume we mostly mean mining pools and collusion. Still, where is the gain? It is only the blocks that will be larger with no economic advantage.

In a fee for priority service auction, there is always limited space in each new block since it represents only a small fraction of the size of the mempool. Presenting fraudulent transactions at the bottom end of the scale has limited effect on the cost of being near the front of the queue, at priority. As the fraudulent transactions age they would be included in blocks presuming the fee is above dust level, but the block size would grow to accommodate them since the valid mempool is larger. The auction for priority still continues uninterrupted at the top of the priority curve. There is nothing stopping a motivated individual now from writing a script to create a million pointless dust transactions per day, flooding the mempool. Even if the fee is above dust level the proposal does not change this but, ensures transactional reliability for valid transactions.

In an idealist world, all nodes could agree on the state of the mempool. I agree, there is no feasible way currently to hold the mempool to consensus without a network of dedicated mempool servers. As it is, it has been suggested that all long-running nodes will have approximately a similar view of the mempool. Sweeping the entire mempool contents per block would achieve what is required if there was a mempool consensus but since it will just be one node's view of the mempool that will not be the result.

My speculation is that as a result of the proposal, through increased adoption of Bitcoin over time there would, in fact, be more transactions and greater net fees paid per day. An increased value of BTC that we suppose would follow from increased usage would augment this fee value increase. It surely follows that a more stable and reliable service will have greater consumer and business acceptance, and there it follows that this is in miners financial interest.

I have not considered a maxblocksize since I consider that the mempool can eventually grow infinitely in size just in valid transactions, without even any fraudulent transactions. I suppose that in time it will become necessary to start all new nodes in pruned mode by default due to the onerous storage requirements of the full blockchain. I do not think that the proposed changes alter this.

I am sure that there is much more to write.

Regards,
Damian Williamson



________________________________
From: ZmnSCPxj <ZmnSCPxj@protonmail.com>
Sent: Wednesday, 27 December 2017 2:55 PM
To: Damian Williamson
Cc: bitcoin-dev@lists.linuxfoundation.org
Subject: Re: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

Good morning Damian,

I see you have modified your proposal to be purely driven by miners, with fullnodes not actually being able to create a strict "yes-or-no" answer as to block validity under your rules.  This implies that your rules cannot be enforced and that rational miners will ignore your proposal unless it brings in more money for them.  The fact that your proposal provides some mechanism to increase block size means that miners will be incentivized to falsify data (by making up their own transactions just above your fixed "dust size" threshhold whatever that threshhold may be -- and remember, miners get at least 12.5 BTC per block, so they can make a lot of little falsified transactions to justify every block size increase) until the block size increase per block is the maximum possible block size increase.



--

Let me then explain proof-of-work and the arrow of time in Physics.  It may seem a digression, but please, bear with me.

Proof-of-work proves that work was performed, and (crucially) that this work was done in the past.

This is important because of the arrow of time.

In principle, every physical interaction is reversible.  Visualize a video of two indivisible particles.  The two particles move towards each other, collide, and because of the collision, fly apart. If you ran this video in reverse, or in forward, it would not be distinguishable to you, as an outside observer, whether the video was running in reverse or not.  It seems at some level, time does not exist.

And yet time exists.

Consider another video, that of a vase being dropped on a hard surface.  The vase hits the surface and shatters.  Played in reverse, we can judge it as nonsensical: scattered pieces of ceramic spontaneously forming a vase and then flying upwards.  This orients our arrow of time: the arrow of time points from states of the universe where lesser entropy exists (the vase is whole) to where greater entropy exists (the vase is in many pieces).

Indeed, all measures of time are, directly or indirectly, measures of increases in entropy.  Consider a simple hourglass: you place it into a state of low entropy and high energy with most of the sand is in the upper part of the hourglass.  As sand falls, and more of that energy is lost into entropy, you judge that time passes.

Consider a proof-of-work algorithm: you place electrons into a state of low entropy and high energy.  As electrons go through the mining hardware, producing hashes that pass the difficulty requirement, the energy in those electrons is lost into entropy (heat), and from the hashes produced (which proves not only that work was done, but in particular, that entropy increased due to work being done), you judge that time passes.

--

Thus, the blockchain itself is already a service that provides a measure of time.  When a block commits to a transaction, then that transaction is known to have existed at that block height, at the latest.

Thus one idea, is to have each block commit to some view of the mempool.  If a transaction exists in this mempool-view, then you know that the transaction is at least that old, and can judge the age from this and use this to compute the "transaction priority".

Unfortunately, transferring the data to prove that the mempool-view is valid, is equivalent to always sweeping the entire mempool contents per block.  In that case you might as well not have a block size limit.

In addition, miners may still commit to a falsely-empty mempool and deny that your transaction is old and therefore priority and therefore will simply fill their blocks with transactions that have high feerates rather than high priority.  Thus feerate will still be the ultimate measure.

Rather than attempt this, perhaps developers should be encouraged to make use of existing mechanisms, RBF and CPFP, to allow transactions to be sped up by directly manipulating feerates, as priority (by your measure) is not practically computable.

Regards,
ZmnSCPxj
-------------------------------------
Sorry for sending a double, hit the wrong button...

Den 31 mars 2017 06:14 skrev "Natanael" <natanael.l@gmail.com>:

-------------------------------------
Thank you for your constructive feedback. I now see that the proposal introduces a potential issue.


It is difficult to define then, what is a valid transaction? Clearly, my definition was insufficient.


Regards,

Damian Williamson


________________________________
From: Chris Riley <criley@gmail.com>
Sent: Monday, 18 December 2017 11:09 PM
To: Damian Williamson; Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

Regarding "problem" #2 where you say "How do we ensure that all valid transactions are eventually included in the blockchain?":  I do not believe that all people would (a) agree this is a problem or (b) that we do want to *ENSURE* that *ALL* valid transactions are eventually included in the blockchain.  There are many *valid* transactions that oftentimes miners do not (and should not) wish to require be confirmed and included in the blockchain.  Spam transactions for example can be valid, but used to attack bitcoin by using no or low fee.  Any valid transaction MAY be included by a miner, but requiring it in some fashion at this point would open the network to other attack vectors.  Perhaps you meant it a different way.


On Fri, Dec 15, 2017 at 3:59 PM, Damian Williamson via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
-------------------------------------
On 09/15/2017 01:40 PM, Simone Bronzini wrote:

sure, but in this scenario how would one meaningfully "upgrade" the
functionality, eg add a new opcode?  We couldn't, right?  so....
success!   Preventing new functionality is the primary goal of this
thought experiment.  I believe that common sense and market incentives
would prevent arbitrary tightening of the rules for no good reason...



-------------------------------------
The issue is due to Segwit blocks since Testnet has already activated
Segwit. 0.12.x- nodes will receive a Segwit block with all of the
witnesses stripped. When they relay this block to a 0.13.0+ node, the
block will be rejected because those have Segwit functionality and
require the witnesses to be in the block. Given that Testnet has a
smaller number of nodes and less difficulty, this could result in some
miners using 0.13.0+ mining blocks which do not propagate well and thus
causing multiple chain splits and reorgs as other miners find blocks for
the same height before receiving a block for that height.


On 3/23/2017 6:37 PM, Juan Garavaglia via bitcoin-dev wrote:

-------------------------------------
Hi Dan,

thank you for your feedback. Let me clarify that the plausible 
deniability is a property of the protocol. If this will become a BIP, 
and will be approved, there will be wallets that will manage tokens. In 
the meantime, and in the future, it is important that a person with a 
legacy bitcoin wallet can hold, issue and transfer bitcoins without 
disclose that there are tokens involved. Tokens are contained in Bitcoin 
transactions without any modification.

Vanity addresses are on option. They are not mandatory. In situations 
where plausible deniability is a concern they will, probably, not be used.

Sending to someone 0.23000012 bitcoin is really easy. You don't need any 
form of math and you are sending exactly 12 tokens from your wallet. 
Sometimes it is suspect, but sending 0.03423122 in order to send 23122 
tokens does not seem suspect to me. The large majority of the 
transactions have strange numbers like this one.

In the document, when I say "wallet" I mean every single bitcoin wallet 
that you can use today to hold bitcoins. The base of the plausible 
deniability is that there is no "special" wallet involved. Maybe there 
will be special wallets to manage tokens, but they are not mandatory. 
The consolidation is needed only when using wallets that do not allow 
coin selection.

The state of the tokens is fully contained in the bitcoin blockchain. 
There is no need for verification nodes, nor for any other software. 
Maybe you already issued some tokens using this protocol and I cannot 
know it. Unless you disclose it.

There is no "special" need to create small outputs. In order to send a 
transaction containing tokens, you need to send a bitcoin transaction. 
The bitcoin value will be transfered along with the token value. If you 
issue tokens with a token offering transactions (aka ICO), the value of 
the bitcoin transferred to you is exactly the price of the tokens, so 
there is no "extra" bitcoin value involved.

I'm sorry if the example of the corporation is not clear. The idea was 
only that Alice receives from the shareholders the bitcoin value, in 
order to use that same value to give back the tokens. There is no 
interest. As I wrote, people got equity for "time, money, furniture, 
knowledge". I could simply write that Alice sends small outputs without 
receiving the underlying bitcoin value beforehand.

I agree that memorable names are great to social scalability. This is 
why you can use a vanity address or only the first part of the vanity 
address to identify a token type.

Cheers,

Luca

On 09/06/2017 07:24 PM, Dan Anderson wrote:


-------------------------------------
On Wednesday 28 June 2017 12:37:13 AM Chris Stewart via bitcoin-dev wrote:

I don't see how. It seems like the logical outcome from this is "whoever pays 
the most gets the next sidechain block"... That's not particularly useful for 
merge mining.


There are different kinds of sidechains...

Federated peg: this already works on Bitcoin.
SPV/SNARK peg: this isn't enabled by your BIP.
Drivechains: this isn't enabled by your BIP.

How do you say this enables any kind of sidechain?


This is unacceptable, for reasons Greg already pointed out.


Note that this is not acceptable for BIPs anymore.

https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki#BIP_licensing

-------------------------------------

Right, but that's my point.  Any level of control the fullnodes believe
they have is effectively a placebo, unless the opposition to the miners is
essentially unanimous (and maybe not even then, if the chainsplit doesn't
have any miners to get to the next difficulty change or gets attacked
repeatedly).


We're derailed from the main thread at this point, but just wanted to state
that I agree in part.  The part I don't agree with is when a single
transaction begins to cost more than a month's worth of full validation,
which has already happened at least once last week, the full validation is
on its way to becoming worthless.  The two costs have to be balanced for
the coin to have utility for its users.

I agree with the rest.

Jared

On Tue, Jun 13, 2017 at 5:23 PM, James Hilliard <james.hilliard1@gmail.com>
wrote:

-------------------------------------
On Wed, Jun 7, 2017 at 8:01 PM, Jared Lee Richardson <jaredr26@gmail.com> wrote:
Right, it's not straight forward to measure because the hard numbers
that we do have tell an incomplete story. In addition the metric that
BIP148 primarily depends on(economic support) is much harder to
measure than other metrics such as hashpower support.
There will likely be some exchanges offering markets for each side of
a potential split separately ahead of BIP148 activation.
I think you've misunderstood the situation, SegWit has widespread
support but has been turned into a political bargaining chip for other
less desirable changes that do not have widespread support.

-------------------------------------
In softforks, I would argue that 100% of all nodes and miners need to
upgrade to the new rules.
This makes sure that trying to incorrectly spend an "AnyOneCanSpend" will
result in a hardfork, instead of a temporary (or permanent) chainsplit.

With drivechains, it seems like the current plan is to only let the nodes
that are interested in the drivechain validate the other chain, and not
necessarily 100% of the network.
I guess this could be any percentage of the network, which could lead to a
temporary/permanent chainsplit depending on how many percentage of the
miners are also validating the other chain (am I missing something here?).

I have no way to evaluate if this is an okay trade-off.
It seems like major disruption could very likely happen if say only 5% of
all fullnodes validate the drivechain.

To be fully secure, it seems like 100% of all nodes should also have a
fullnode for the drivechain as well...
This is one of the reasons I don't advocate sidechains/drivechains as a
scaling solution, it looks like it would have to the same outcome as a
blocksize increase on the mainchain, but with more complexity.
I think sidechains/drivechains could be useful for other things though.


Thanks for all your work so far Paul.
Hampus

2017-07-13 4:58 GMT+02:00 Paul Sztorc via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
I dont think thats true? Sure, you have to assume the block is valid
aside from a too-large size, but it seems sane.

You don't strictly need to show that a leaf is a parseable transaction,
as long as you can assume that the block is valid and that you cannot
forge a SHA256 midstate which, when combined with data with a given
length tag, would result in a hash of a given value (this is a pretty
strong assumption, IMO, IIRC this was not a studied nor a claimed
feature of SHA256).

The only issue is that, since parts of the merkle tree are repeated, you
need to be sure that the counting for minimum number of transactions is
accurate, though I did not review your proposal text to check that.

On 03/25/17 05:16, Luke Dashjr wrote:
 - snip -

-------------------------------------

Ah, yes, I'm missing that the expected time to find each type of block is
halved, so the orphan rate doubles.
-------------------------------------
On Thu, Sep 28, 2017 at 12:09:59PM +0200, Andreas Schildbach via bitcoin-dev wrote:

I'm well aware. As the payment protocol hasn't caught on - and doesn't fully
overlap all the usecases that addresses do anyway - I think we should consider
bringing this important feature to Bitcoin addresses too.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
I have heard such theory before, it's a complete mistake to think that
others would run full nodes to protect their business and then yours,
unless it is proven that they are decentralized and independent

Running a full node is trivial and not expensive for people who know how
to do it, even with much bigger blocks, assuming that the full nodes are
still decentralized and that they don't have to fight against big nodes
who would attract the traffic first

I have posted many times here a small proposal, that exactly describes
what is going on now, yes miners are nodes too... it's disturbing to see
that despite of Tera bytes of BIPs, papers, etc the current situation is
happening and that all the supposed decentralized system is biased by
centralization

Do we know what majority controls the 6000 full nodes?


Le 29/03/2017  22:32, Jared Lee Richardson via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
On 7/13/2017 4:22 PM, Chris Stewart wrote:

In Bitcoin, new coins are held for 100 blocks. One result of this is
that the coins can't be spent until there is at least 100 blocks worth
of evidence that they actually are in the longest chain, and are
unlikely to be orphaned.

In BMM, we are concerned about exactly this. For example, imagine that I
bribe you $20 to find my side:block (and, in that block I earn $20.50
worth of side:BTC tx fees), which you do. But then, moments later, you
(or some other miner) orphans the block! So I don't get my $20.50, but
you still keep my $20!

And yet, we want the mainchain to validate as little as possible about
each sidechain. We want a "light touch". So we force the h* to be
accompanied by the modulus of its sidechain block number (we call it
"BlockMod"). The sidechain has a new rule that requires h* to be
included AND that the BlockMod be accurate, in order for the sidechain
block to meet the "synthetic" difficulty requirement. The mainchain has
a new rule forcing each new BlockMod to be in range [-X000,+1] relative
to the old BlockMod (ie, "no skipping ahead, but you can reorg by
starting a new chain from up to a=-X000 blocks ago" ... likely values of
X might be 2 or 4). And finally, BMM has a new rule that the bribe isn't
paid unless the sidechain block in question has been buried by [for
example] 100 sidechain blocks.

Hope that helps,
Paul


-------------------------------------
Potentially miners could create their own private communication channel/listening port for submitting transactions that they would not relay to other miners/the public node relay network. Users could then chose who they want to relay to. Miners would be incentivized to not relay higher fee transactions, because they would want to keep them to themselves for higher profits.

Cheers,
Praxeology Guy

-------- Original Message --------
Subject: Re: [bitcoin-dev] Inquiry: Transaction Tiering
Local Time: March 22, 2017 12:48 PM
UTC Time: March 22, 2017 5:48 PM
From: bitcoin-dev@lists.linuxfoundation.org
To: bitcoin-dev@lists.linuxfoundation.org

Hi Tim,
After writing this I figured that it was probably not evident at first
sight as the concept may be quite novel. The physical location of the
"miner" is indeed irrelevant, I am referring to the digital location.
Bitcoins blockchain is a digital location or better digital "space".
As far as I am concerned the authority lies with whoever governs this
particular block space. A "miner" can, or can not, include my
transaction.

To make this more understandable:

Abu Bakr al-Baghdadi can extend his caliphate into Bitcoins block
space and rule sovereign(!) over a given block. If he processes my
transaction my fee goes directly into the coffers of his organization.
The same goes for the Queen of England or the Emperor of China. My
interest is not necessarily aligned with each specific authority, yet
as you point out, I can only not use Bitcoin.
Alternatively, however, I can very well sign my transaction and send
it to an authority of my choosing to be included into the ledger, say
BitFurry. - This is what I describe in option 1.

In order to protect my interest I do need to choose, maybe not today,
but eventually.

I also think that people do care who processes transactions and a lot
of bickering could be spared if we could choose.

If we assume a perfectly competitive market with 3 authorities that
govern the block space equally, the marginal cost of 1/3 of the block
space is the same for each, however, the marginal revenue absent of
block rewards is dependent on fees.
If people are willing to pay only a zero fee to a specific authority
while a fee greater than zero to the others it's clear that one would
be less competitive.

Let us assume the fees are 10% of the revenue and the cost is 95 we
have currently the following situation:

A: Cost=95; Revenue=100; Profit=5
B: Cost=95; Revenue=100; Profit=5
C: Cost=95; Revenue=100; Profit=5

With transaction tiering, the outcome could be different!

A: Cost=95; Revenue=90; Loss=5 // BSA that does not respect user interest.
B: Cost=95; Revenue=105; Profit=10
C: Cost=95; Revenue=105; Profit=10

This could motivate transaction processors to behave in accordance
with user interest, or am I missing something?

Best Regards,
Martin

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Hello,
I'm not a bitcoin developer, but I'd like to receive feedback on what
I think is a serious problem. Hope I'm not wasting your time.
I'm also sure this was already discussed, but google doesn't give me
any good result.

Let me explain: I think that the current incentive system doesn't
really align with the way miners are distributed (not very
decentralized, due to pools and huge asic producers).
I think big miners are incentivized to spam the network with low(ish)
fee transactions, thereby forcing regular users into paying extremely
high fees to be able to get their transactions confirmed.

Obviously this is the result of insufficient mining decentralization,
but as I will try to show, such an attack could be profitable even if
you are controlling just 5-10% of the hashing power, which could
always be easy for a big player and with some collusion.

Let's look at some numbers: https://i.imgur.com/sCn4eDG.png

These are 10 blocks mined yesterday, and they all have rewards hugely
exceeding the normal 12.5 mining output. Even taking the lowest value
of 20, it's a nice 60% extra profit for the miner. Let's say you
control 10% of the hashing power, and you spam enough transactions to
fill 144 blocks (1 day's worth) at 50 satoshi/byte, losing just 72 BTC
in fees.

(blocksize-in-bytes * fee-per-byte * Nblocks)/satoshis-in-btc => (1e6
* 50 * 144)/1e8 => 72

At the same time you will discover about 144*0.1=14.4 blocks per day.
Assuming the situation we see in the previous screenshot is what
happens when you have a mempool bigger than one day's worth of blocks,
you would get 20-12.5=7.5 extra BTC per block, which is 14.4*7.5=108
BTC, given your investment of 72 to spam the mempool. 32 btc extra
profit.

The big assumption here is that spamming 1 day of backlog in the
50satoshi/b range will get people to compete enough to push 7.5 btc of
fees in each block, but:

* https://jochen-hoenicke.de/queue/#30d this seems to confirm that
about half the mempool is in the 50satoshi/b range or less.
* https://blockchain.info/pools there are miners that control more than 10%
* if you get enough new real transactions, it's not necessary to spam
a full 144 blocks worth each day, probably just ~50 would be enough,
cutting the spam cost substantially
* other miners could be playing the same game, helping you spam and
further reduce the costs of the attack
* you actually get 10% of the fees back by avoiding mining your spam
transactions in your own blocks
* most of the spam transactions won't actually end up in blocks if
there is enough pressure coming from real usage

This seems to indicate that you would actually get much higher profit
margins than my estimates. **PLEASE** correct me if my calculations or
my assumptions are wrong.

You might also say that doing this would force users out of the
system, decreasing the value of btc and disincentivizing miners from
continuing. On the other hand, a backlogged mempool could create the
impression of high(er) usage and increase scarcity by slowing down
movements, which could actually push the price upwards.

Of course, it's impossible to prove that this is happening. But the
fact that it is profitable makes me believe that it is happening.

I see some solutions to this, all with their own downsides:

- increasing block size every time there is sustained pressure
this attack wouldn't work, but the downsides have already been
discussed to death.

- change POW
Not clear it would fix this, aside from stimulating terrible
infighting. Controlling 5 to 10% of the hashing power seems too easy,
and I don't think it would be practical to change pow every time that
happens, as it would prevent the development of a solid POW support.

- protocol level MAX transaction fee
I personally think this would totally invalidate the attack by making
the spam more expensive than the fees you would recover.
There already is a minimum fee accepted by the nodes, at 1 satoshi per
byte. The maximum fee could be N times the minimum, maybe 100-200.
Meaning a maximum of 1-2btc in total fee rewards when the block size
is 1mb. Of course the actual values need more analysis, but 2btc -
together with the deflationary structure - seems enough to continue
motivating miners, without giving unfair advantage.

Yes, this would make it impossible to spend your way out of a
congested mempool. But if the mempool stays congested after this
change, you could have a bigger confidence that it's coming from real
usage or from someone willfully burning money, making a block size
increase much more justified.

Hope to hear your opinion,
have a nice day.

oscar

-------------------------------------
On Thu, Jun 1, 2017, at 21:01, Olaoluwa Osuntokun via bitcoin-dev wrote:> Hi y'all, 

Very interesting. 

I would like to consider how this compares to another light client type
with rather different security characteristics where each client would
receive for each transaction in each block,
* The TXID (uncompressed)
* The spent outpoints (with TXIDs compressed)
* The pubkey hash (compressed to reasonable amount of false positives)

A rough estimate would indicate this to be about 2-2.5x as big per block
as your proposal, but comes with rather different security
characteristics, and would not require download since genesis.
The client could verify the TXIDs against the merkle root with a much
stronger (PoW) guarantee compared to the guarantee based on the
assumption of peers being distinct, which your proposal seems to make.
Like your proposal this removes the privacy and processing  issues from
server-side filtering, but unlike your proposal retrieval of all txids
in each block can also serve for a basis of  fraud proofs and
(disprovable) fraud hints, without resorting to full block downloads.
I don't completely understand the benefit of making the outpoints and
pubkey hashes (weakly) verifiable. These only serve as notifications and
therefore do not seem to introduce an attack vector. Omitting data is
always possible, so receiving data is a prerequisite for verification,
not an assumption that can be made.  How could an attacker benefit from
"hiding notifications"?
I think client-side filtering is definitely an important route to
take, but is it worth compressing away the information to verify the
merkle root?
Regards,
Tomas van der Wansem
bitcrust

-------------------------------------
Random misreadings of your post aside (maybe it's time to moderate this list a bit more again), I think this is a reasonable model, and certainly more terminology/understanding is useful, given I and many others have been making arguments based on these differences.

One thing you may wish to further include may be that many soft forks do not require any miner upgrade at all due to standardness rules. Eg OP_CSV and SegWit both only require miners upgrade if they wish to receive the additional fees from new transactions using these features.

Matt

On April 5, 2017 12:28:07 PM GMT+02:00, Johnson Lau via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I can’t recommend your first 2 proposals. But I only have the time to talk about the first one for now.

There are 2 different views on this topic:

1. “The block size is too small and people can’t buy a coffee with an on-chain transaction. Let’s just remove the limit”

2. “The block size is too big and people can’t run full nodes or do initial blockchain download (IBD). Let’s just reduce the limit”

For me, both approaches just show the lack of creativity, and lack of responsibility. Both just try to solve one problem, disregarding all the other consequences.

The 1MB is here, no matter you like it or not, it’s the current consensus. Any attempts to change this limit (up or down) require wide consensus of the whole community, which might be difficult.

Yes, I agree with you that the current 1MB block size is already too big for many people to run a full node. That’s bad, but it doesn’t mean we have no options other than reducing the block size. Just to cite some:

1. Blockchain pruning is already available, so the storage of blockchain is already an O(1) problem. The block size is not that important for this part
2. UTXO size is an O(n) problem, but we could limit its growth without limit the block size, by charging more for UTXO creation, and offer incentive for UTXO spending  **
3. For non-mining full node, latency is not critical. 1MB per 10 minutes is not a problem unless with mobile network. But I don’t think mobile network is ever considered as a suitable way for running a full node
4. For mining nodes, we already have compact block and xthin block, and FIBRE
5. For IBD, reducing the size won’t help much as it is already too big for many people. The right way to solve the IBD issue is to implement long latency UTXO commitment. Nodes will calculate a UTXO commitment every 1000 block, and commit to the UTXO status of the previous 1000 block (e.g. block 11000 will commit to the UTXO of block 10000). This is a background process and the overhead is negligible. When such commitments are confirmed for sufficiently long (e.g. 1 year), people will assume it is correct, and start IBD from that point by downloading UTXO from some untrusted sources. That will drastically reduce the time for IBD
6. No matter we change the block size limit or not, we need to implement a fraud-proof system to allow probabilistic validation by SPV nodes. So even a smartphone may validate 0.1% of the blockchain, and with many people using phone wallet, it will only be a net gain to the network security 

For points 2 and 6 above, I have some idea implemented in my experimental hardfork.
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html>



-------------------------------------
Hi everyone, I have never posted to the list and do not commit the project
proper so I do apologize if this is not even possible in advance.

Examining bitcoin's past and contrasting it with other social contracts and
even physical phenomena it would seem that as bitcoin continues to age
entropy will continue to grow.

The scaling debate would seem to be a fore-bearer of this.

One main contributor to this in bitcoin is as people/businesses become
involved and vested into their perspective it becomes minted in their
identity. The idea that this will somehow decrease as the user base grows
with more perspectives and use-cases seems counter-intuitive so I propose a
potential way to combat entropy in a divided community.

Diminishing Signaling Returns

Since entropy is the increase of disorder in a system (the various scaling
solutions for this idea) and measuring via resource is not trustless, we
need a mechanism to essentially counteract entropy while not relying on
game-able metrics.

What if we applied a rate of diminishing returns on signaling in BIP9?
Something along the lines of:

Every (X) block signaled The actual signal value diminishes from a 1 signal
1 block to a fraction of a signal per block found after a burn in period of
some reasonable time to ensure a majority upgrade(this could even be
effected after the timeout currently apart of BIP9 for ease of
implementation?).

Some thoughts

   - The pools that find more blocks would lose the ability to block the
   network without taking an economical hit splitting up their hash power as
   signaling was never intended to be a voting mechanism (to my knowledge).
   - The longer that the signaling took place would eventually run a larger
   pool's signaling influence to 0 first. This creates a balancing effect
   between hash rate & #of miners actually signaling ready.
   - Gamesmanship of this system would be visible to the community at
   large. e.g. pools hash rate/blocks found jumps or declines significantly in
   a short time frame, or specific time frame (when pools influence begins to
   decline).
   - creates multiple economic incentives for the mining community to be on
   a similar page
   - this as a feature of a soft forks greatly diminishes politics becoming
   a factor in the future.


unfortunately, this itself would require a soft fork if I am correct?


Acceptance then becomes the question.
While bitcoin has proven to be highly resilient, stagnation has destroyed
many systems/businesses and if the current state of affairs is any measure
it would stand to reason that in the future this will only worsen. Taking
this action could be a solution to that stagnation.  So, it would be in
everyone's best long-term interest to support a continually evolving
bitcoin and would allow parties with ideas that differ the time and
resources to fork in a more responsible manner without devoting their
resources to politics. However, everyone would still have the time to voice
their opinions during the burn-in/timeout period and of course before any
code was actually included through technical consensus.

Thoughts?

Regards,
Benjamin George
Crypto.Press http://crypto.press
-------------------------------------
On Monday, 3 April 2017 11:06:02 CEST Sancho Panza wrote:

Please do elaborate :)

The meat of the proposal is missing.
 

I agree that the type of forks are rather irrelevant to the voting 
mechanism. As we remember that BIP109 used a voting bit too.

The per-bit (lets call that per-proposal) parameter threshold and windowsize 
are a different matter though, based on the next paragraph you wrote;


The entire point of BIP9 is to allow nodes that do not know about an upgrade 
to still have a functional state machine. But I don’t see how you can have a 
state machine if the two basic variables that drive it are not specified.

Now, to be clear, I am a big fan of making the window size and the threshold 
more flexible.
But in my opinion we would not be able to have a state machine without those 
variables in the actual BIP because old nodes would miss the data to 
transition to certain states.

Maybe an idea; we have 30 bits. 2 currently in use (although we could reuse 
the CSV one). Maybe we can come up with 3 default sets of properties and 
when a proposal starts to use bit 11 it behaves differently than if it uses 
22.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
This is just me putting in my formal objection to BIP148 and BIP149 based on my experience with the ETH/ETC hard fork and involvement in that drama.

First, it's important to note that ETC/ETH HF is a very different situation from BIP148 and all other soft-forks. To those on this mailing list, the reasons should be self-evident (one results in two incompatible chains, the other doesn't).

However, replay attacks are common to both possibilities (i.e. when BIP148 has <51% hash power).

I believe the severity of replay attacks is going unvoiced and is not understood within the bitcoin community because of their lack of experience with them.

I further believe that replay attacks are the #1 issue with BIP148, BIP149, etc., superseding wipeout attacks in severity.

These are not baseless beliefs, they're born out of experience and I think anyone will reach the same conclusion upon study.

In a nutshell, replay attacks mean that all talk of there being potentially "two coins" as a result of BIP148 is basically nonsense.

Replay attacks effectively eliminate that possibility.

When users go to "sell their legacy coins", they've just sold their 148 coins, and vice versa.

Both of the coin-splitting techniques given so far by the proponents BIP148 are also untenable:

- Double-spending to self with nLockTime txns is insanely complicated, risky, not guaranteed to work, extremely time consuming, and would likely result in a massive increase in backlogged transactions and increased fees.

- Mixing with 148 coinbase txns destroys fungibility.

Without a coin, there is no real threat from BIP148. Without that threat, there is no point to BIP148, and the miners know this.

These and other concerns are outlined and explained in more detail in this conversation I had yesterday with John Light:

https://www.youtube.com/watch?v=33rL3-p8cPw <https://www.youtube.com/watch?v=33rL3-p8cPw>

Cheers,
Greg Slepak

--
Please do not email me anything that you are not comfortable also sharing with the NSA.

-------------------------------------
On Sun, May 28, 2017 at 3:51 PM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
That would partition off those clients, which is not something we
would want to happen.
This isn't possible until either BIP141 segwit is active or BIP141
segwit has expired.
We don't know that since bip9 bit1 only needs 55% hashpower to be
triggered(see BIP91 activation method for how this can be done)
BIP91 activation is clearly superior than trying to fully redeploy, it
is far simpler and can be done almost immediately with only miners
needing to upgrade.

-------------------------------------
Doing nothing is the rules we all agreed to.  If those rules are to be
changed,nearly everyone will need to consent.  The same rule applies to the
cap, we all agreed to 21m, and if someone wants to change that, nearly
everyone would need to agree.

On Feb 8, 2017 10:28 AM, "Andrew Johnson" <andrew.johnson83@gmail.com>
wrote:

It is when you're talking about making a choice and 6.3x more people prefer
something else. Doing nothing is a choice as well.

Put another way, if 10% supported increasing the 21M coin cap and 63% were
against, would you seriously consider doing it?

On Feb 8, 2017 9:57 AM, "alp alp" <alp.bitcoin@gmail.com> wrote:

-------------------------------------
I guess I should caveat, a rounding error is a bit of exaggeration -
mostly because I previously assumed that it would take 14 years for
the network to reach such a level, something I didn't say and that you
might not grant me.

I don't know why paypal has multiple datacenters, but I'm guessing it
probably has a lot more to do with everything else they do -
interface, support, tax compliance, replication, redundancy - than it
does with the raw numbers of transaction volumes.

What I do know is the math, though.  WW tx volume = 426,000,000,000 in
2015.  Assuming tx size of ~500 bytes, that's 669 terabytes of data
per year.  At a hard drive cost of 0.021 per GB, that's $36k a year or
so and declines ~14% a year.

The bandwidth is the really big cost.  You are right that if this
hypothetical node also had to support historical syncing, the numbers
would probably be unmanagable.  But that can be solved with a simple
checkpointing system for the vast majority of users, and nodes could
solve it by not supporting syncing / reducing peer count.  With a peer
count of 25 I measured ~75 Gb/month with today's blocksize cap.  That
works out to roughly 10 relays(sends+receives) per transaction
assuming all blocks were full, which was a pretty close approximation.
The bandwidth data of our 426 billion transactions per year works out
to 942 mbit/s.  That's 310 Terabytes per month of bandwidth - At
today's high-volume price of 0.05 per GB, that's $18,500 a month or
$222,000 a year.  Plus the $36k for storage per year brings it to
~$250k per year.  Not a rounding error, but within the rough costs of
running an exchange - a team of 5 developers works out to ~$400-600k a
year, and the cost of compliance with EU and U.S. entities (including
lawyers) runs upwards of a million dollars a year.  Then there's the
support department, probably ~$100-200k a year.

The reason I said a rounding error was that I assumed that it would
take until 2032 to reach that volume of transactions (Assuming
+80%/year of growth, which is our 4-year and 2-year historical average
tx/s growth).  If hard drive prices decline by 14% per year, that cost
becomes $3,900 a year, and if bandwidth prices decline by 14% a year
that cost becomes $1800 a month($21,600 a year).  Against a
multi-million dollar budget, even 3x that isn't a large concern,
though not, as I stated, a rounding error.  My bad.

I didn't approximate for CPU usage, as I don't have any good estimates
for it, and I don't have significant reason to believe that it is a
higher cost than bandwidth, which seems to be the controlling cost
compared to adding CPU's.


Care to respond to the math?


Well, we agree on something at least.

On Fri, Mar 31, 2017 at 9:14 AM, David Vorick <david.vorick@gmail.com> wrote:

-------------------------------------
One of the ideas that Greg Maxwell brought up in the "'Compressed' headers
stream" thread is the possibility of a header sync mechanism that allowed
parallel download from multiple peers. With the current getheaders/headers
semantics, headers must be downloaded sequentially from genesis. In my
testing, I saw that syncing headers directly from a colocated node took <5s
whereas syncing normally from network peers takes ~5 min for me, which goes
to show that 5s is an upper bound on the time to process all headers if
they are locally available. So if we can introduce new p2p messages for
header sync, what would they look like? Here's one idea.

A new getheadersv2 request would include a start height for the range of
headers requested and a commitment to the last block in the chain that you
want to download. Then you find N peers that are all on the same chain,
partition the range of headers from 0 to the chain height minus some
reasonable reorg safety buffer (~6 blocks), and send download requests in
parallel. So how do we know that the peers are on the same chain and that
their headers served connect into this chain?

When you connect to outbound peers and are in IBD, you will query them for
a Merkle Mountain Range commitment to all headers up to a height X (which
is 6ish blocks before their start height from the version message). Then
you choose the commitment that the majority of the queried peers sent (or
some other heuristic), and these become your download peers. Every
getheadersv2 request includes the start height, X, and the chain
commitment. The headersv2 response messages include all of the headers
followed by a merkle branch linking the last header into the chain
commitment. Headers are processed in order as they arrive and if any of the
headers are invalid, you can ban/disconnect all peers that committed to it,
drop the buffer of later headers and start over.

That's the basic idea. Here are other details:

- This would require an additional 32-byte MMR commitment for each header
in memory.
- When a node receives a headersv2 request and constructs a merkle proof
for the last header, it checks against the sent commitment. In the case of
a really deep reorg, that check would fail, and the node can instead
respond with an updated commitment hash for that height.
- Another packet is needed, getheaderchain or something, that a syncing
peer first sends along with a header locator and an end height. The peer
responds with headerchain, which includes the last common header from the
locator along with the chain commitment at that height and a merkle branch
proving inclusion of that header in the chain.
- Nodes would cache chain commitments for the last ~20 blocks (somewhat
arbitrary), and refuse to serve chain commitments for heights before that.

Thoughts? This is a pretty recycled idea, so please point me at prior
proposals that are similar as well.

-jimpo
-------------------------------------
Well it's not going off-topic since the btc folks need now to find a way
to counter the attack

The disk space story is know to be a non issue, because encouraging
people to run nodes while they don't know how to dedicate the right
storage space that is trivial and not expensive to get today is just
stupid, they should not try to run full nodes, and no I tested with non
SSD drives, I was more wondering about cpu and bandwidth use, but did
not notice any impact, just stopped because a repeated sw bug or drive
issue desynched the chain and bitcoin-qt was trying to reload it from
the begining each time, which in my case was taking 10 days despite of
good bandwidth (which would allow me to torrent the entire chain + state
in less than 20 hours), so I stopped after the 3rd crash, setting up a
full node on my servers is still in the todo list (very low priority for
the reasons already explained)

Running a prune node implies first to setup a full node, so the same
problematic applies and then the advantage of pruning is not really
obvious, I don't know what's the strange story about "archival nodes", I
proposed something else

Back to the topic, the conclusion is that this is not difficult at all
for many people to run efficient full nodes, ideally the community
should promote this, seed a torrent with a recent state, implement a
patch to defeat BU plans and have everybody upgrade

But of course this will not happen


Le 29/03/2017 à 18:41, Andrew Johnson a écrit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
It's my opinion that the purpose of this list and bitcoin protocol
development in general is to build the base functionality that other
companies and individuals require to provide usability to the end-user. The
0-conf debate is a UX issue. If end users shouldn't rely on 0-conf, it is
up to wallet developers to hide 0-conf transactions or mark them
appropriately. Instead of using this list to debate what wallet designers
should or shouldn't do, we should just provide the tools and "let the
market sort it out". If wallet developers start getting inundated with
complaints that 0-conf transactions are causing confusion and loss, they
will find a solution. If the tools they require for the solution don't
exist, they will come to this list to request action.

Am I wrong?

On Fri, Jan 6, 2017 at 12:16 PM Chris Priest via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
??? what do you mean? (https://www.soyoustart.com/fr/serveurs-essential/)


Le 20/04/2017  17:50, Erik Aronesty via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
Currently the only implementation that fulfills the requirements of the NYA
agreement is the segwit2x/btc1 implementation, which is being finalized
this week.

Segwit2mb does not fulfill the NYA agreement.

I'm asking now the segwit2x development team when a BIP will be ready so
that Core has the opportunity to evaluate the technical proposal.




On Wed, Jun 21, 2017 at 1:05 AM, Jacob Eliosoff via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
If I did understand it right, you don't need to publish the Simplicity
code for the "jetable" expression.

That's the whole point of MAST. Each Simplicity expression can be
identified by its MAST root (the Merkle root of all branches in its
Abstract Syntax Tree).

Imagine you want to write a Simplicity script that is roughly equivalent
to P2PKH. Regardless of directly writing such script or using a higher
level smart contract language, you won't likely write for yourself the
part in which you compute the hash of the public key. Instead, you are
expected to include some external library providing hash functions or at
least copy and paste such function into your code.

As everyone is expected to use the same, let's say, RIPEMD160
implementation, it doesn't matter how you included such function in your
program. The point is that once you build the MAST for your program,
such function will be completely replaced by its MAST root---which is
nothing but a hash.

This way, when the Simplicity interpreter (the BitMachine) bumps into
the hash, it can look for it in a predefined jets dictionary and find
the binary for a precompiled, formally proven implementation of a
function that is perfectly equivalent to the original Simplicity code.


On 03.11.2017 13:59, Hampus Sjöberg via bitcoin-dev wrote:

-- 
Adán Sánchez de Pedro Crespo
CTO, Stampery Inc.
San Francisco - Madrid

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

I had a question relating to scaling and privacy enhancements.
I believe that segwit combined with aggregated signatures
and coinjoin can potentially achieve such. The idea is to
use aggregated signatures in conjunction with coinjoin. So
that all inputs of a coinjoin transaction would have a single
signature vastly decreasing size while having privacy at the
same time. If majority of transactions in a block did this I
assume that significant more transactions could be fit into a
block? However the question I have, with the extra blockspace
made possible by segwit, is this extra blockspace limited to only
witness data or can it be used for transaction data such as the
scenario I have described here?

- --
Cannon
PGP Fingerprint: 2BB5 15CD 66E7 4E28 45DC 6494 A5A2 2879 3F06 E832 
Email: cannon@cannon-ciota.info

NOTICE: ALL EMAIL CORRESPONDENCE NOT SIGNED/ENCRYPTED WITH PGP SHOULD 
BE CONSIDERED POTENTIALLY FORGED, AND NOT PRIVATE.
-----BEGIN PGP SIGNATURE-----

iQIcBAEBCgAGBQJaSXSNAAoJEAYDai9lH2mwRy4QAMqhl6UWNqRy7ziDuxukm+nZ
jWtjyc8G38b9r9Nya13/GslHWeEDdSmma6e7afFMVX1y9Qj+t0EZDJVlMMy8JRZr
zDmSdXDxStNv6T+L3NVbSOBhdP+1MpcsvAAs3yd0Nl5cxfBF87ArHlXMbTLJF86S
1gijI4pg3x83tDg/Di6gf9BHk2oXGDc4vraF6LsMDTfQmp7S8pivnswaaEyb6etH
39ei6L3wkV7LvTmA2onCAB8vZtTuARhNuLTYSPfH5LAC4hha2bOCXci3p4Mz4qh3
U4LqUnuYVR8nYOFFsrfhKggN3kptVWhrbDAoHR2fLoYDmfbMkqUdyjdmmc2Rvlgm
eMJvpG91dYb+Q6JqTrar6DH+XSvoOVSWnBLe8Uwf4AnzGxMUpkTDzkyaBxGq4K1u
Vv2Yg808KwA47MKKpvKSckB350YAq9Cr276Lq/giUrxmS1gOyDKDjm1e3yFLM+6d
NancAwgnp17q43FwSX44cT0ISxk9USnWVhaKDQjSGK8MnirkZ1vuu2SshEW1AVhm
44Bt5nQdLmJDw7rqwkjv66sxofXvmCAnPD+p4yiVyfLNZ7OKw6XNcKm3zKAch2Fy
fefWbZnw0yEA3IhNPiMZOSv/YnwTtfzpFUNuTCtLehs+3Xkp0bl72JDz0HRVYbHM
RbsrLp60rD5kuJBq5dl7
=3gku
-----END PGP SIGNATURE-----

-------------------------------------
On Tue, Sep 12, 2017 at 11:58 PM, michele terzi via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Current nodes allow pruning so you can save disk space that way.  Users
still need to download/verify the new blocks though.

Under your scheme, you don't need to throw the data away.  Nodes can decide
how far back that they want to go.

"Fast" IBD

- download header chain from genesis (~4MB per year)
- check headers against "soft" checkpoints (every 50k blocks)
- download the UTXO set of the most recent soft checkpoint (and verify
against hash)
- download blocks starting from the most recent soft checkpoint
- node is now ready to use
- [Optional] Slowly download the remaining blocks

This requires some new protocol messages to allow requesting and send the
UTXO set, though the inv and getdata messages could be used.

If you add a new services bit, NODE_NETWORK_RECENT, then nodes can find
other nodes that have the most recent blocks.  This indicates that you have
all blocks since the most recent snapshot.

The slow download doesn't have to download the blocks in order.  It can
just check against the header chain.  Once a node has all the blocks, it
would switch from NODE_NETWORK_RECENT to NODE_NETWORK.

(Multiple bits could be used to indicate that the node has 2 or more recent
time periods).

"Soft" checkpoints mean that re-orgs can't cause a network partition.  Each
soft checkpoint is a mapping of {block_hash: utxo_hash}.

A re-org of 1 year or more would be devastating so it is probably
academic.  Some people may object to centralized checkpointing and soft
checkpoints cover that objection.

full nodes with old software can no longer be fired up and sync with the
This is why having archive nodes (and a way to find them) is important.

You could have a weaker requirement that nodes shouldn't delete blocks
unless they are at least 3 time periods (~3 years) old.

The software should have a setting which allows the user to specify maximum
disk space.  Disk space is cheap, so it is likely that a reasonable number
of people will leave that set to infinite.

This automatically results in lots of archive nodes.  Another setting could
decide how many time periods to download.  2-3 seem reasonable as a default
(or maybe infinite too).



Soft forks are inherently backward compatible.  Coins cannot be stolen
using a soft fork.  It has nothing to do with inspecting new releases.

It is possible for a majority of miners to re-write history, but that is
separate to a soft fork.

A soft fork can lock coins away.  This effectively destroys the coins, but
doesn't steal them.  It could be part of a extortion scheme I guess, but if
a majority of miners did that, then I think Bitcoin has bigger problems.


For it to be a soft fork, you need to maintain archive nodes.  That is the
whole point.  The old network and the new network rules agree that the new
network rules are valid (and that miners only mine blocks that are valid
under the new rules).  If IBD is impossible for old nodes, then that counts
as a network split.
-------------------------------------
Unconfirmed transactions are incredibly important for real world use.
Merchants for instance are willing to accept credit card payments of
thousands of dollars and ship the goods despite the fact that the
transaction can be reversed up to 60 days later. There is a very large cost
to losing the ability to have instant transactions in many or even most
situations. This cost is typically well above the fraud risk.

It's important to recognize that bitcoin serves a wide variety of use cases
with different profiles for time sensitivity and fraud risk.

Aaron

On Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Sat, Jun 3, 2017 at 2:55 AM, Alex Akselrod via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Ahh, so you actually make a separate digest chain with prev hashes and
everything. Once/if committed digests are soft forked in, it seems a
bit overkill but maybe it's worth it. (I was always assuming committed
digests in coinbase would come after people started using this, and
that people could just ask a couple of random peers for the digest
hash and ensure everyone gave the same answer as the hash of the
downloaded digest..).


I noticed an increase in FP hits when using real data sampled from
real scriptPubKeys and such. Address reuse and other weird stuff. See
"lies.h" in github repo for experiments and chainsim.c initial part of
main where wallets get random stuff from the chain.


I created digests for all blocks up until block #469805 and actually
ended up with 5.8 GB, which is 1.1 GB lower than what you have, but
may be worse perf-wise on false positive rates and such.


For comparison, creating the digests above (469805 of them) took
roughly 30 mins on my end, but using the kstats format so probably
higher on an actual node (should get around to profiling that...).

-------------------------------------
On Mon, Feb 13, 2017 at 8:58 AM, John Hardy <john@seebitcoin.com> wrote:

The reward is split between all full nodes. Therefore each full node has an
incentive to check at least some other full nodes responses because there
is a competition for the full node reward. At the end, each full node
response will be checked by more than other node with high probability.
Also each full node does a small pre-deposit, that is consumed if the node
cheats.

Is any validation of responses mandatory, or does policing the system rely

There is not many defenses against censorship but try to hide your identity
until the end of the protocol. But if somebody knows that your transactions
belong to you, then there is little defense. We just wait more than a
single block for the commitments, so several miners must collude in order
to censor a transaction.

I'm keeping it private against all my desire because I want it to be
reviewed before I publish it. Credibility is very easily lost.
The same idea (Ephemeral Data) has been used to design the Lumino Network.

-------------------------------------
I admittedly haven't had a chance to read the paper in full details, but I was curious how you propose dealing with "jets" in something like Bitcoin. AFAIU, other similar systems are left doing hard-forks to reduce the sigops/weight/fee-cost of transactions every time they want to add useful optimized drop-ins. For obvious reasons, this seems rather impractical and a potentially critical barrier to adoption of such optimized drop-ins, which I imagine would be required to do any new cryptographic algorithms due to the significant fee cost of interpreting such things.

Is there some insight I'm missing here?

Matt

On October 30, 2017 11:22:20 AM EDT, Russell O'Connor via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
-------------------------------------
after that happens it becomes optional for miners again.

I missed that, that does effectively address that concern.  It appears
that BIP148 implements the same rule as would be required to prevent a
later chainsplit as well, no?

This comment did bring to mind another concern about BIP148/91 though,
which I'll raise in the pull request discussion.  Feel free to respond
to it there.

Jared

On Wed, Jun 7, 2017 at 2:21 PM, James Hilliard
<james.hilliard1@gmail.com> wrote:

-------------------------------------
Hi all,

In the transaction today we have a version field which is always 4 bytes.
The rest of the integer encoding in a transaction is variable-size because 
it saves on bytes.

Specifically, in practice this means that almost all of the transaction have 
bytes 2, 3 & 4 set to zero[1].

The question that I was pondering is that when we accept a new version of 
transaction format (flextrans uses 4), what would the impact be of also 
changing the way that the version number is actually serialized to be var 
int.

The benefit would be that each and every transaction looses 3 bytes. These 
can be used differently in v1 transactions and are not needed at all to be 
there for newer transaction formats.
The secondairy benefit is that, at least for FlexTrans[2], 100% of all the 
integers in the transaction are following exactly the same encoding, the
var-int encoding.

There is currently no consensus rule that rejects transactions which lie 
about their version, so obviously this rule should not and can not be 
introduced retro-actively. It will be from a certain block-height.

The way to do this is that from a certain block-height the current 
transaction format labels bytes 2, 3 & 4 to be unused.
varint.
Last, we add the rule from that block-height that only transactions that do 
not lie about their version number are valid. Which means version 1.

Do people see any problems with this?
This could be done as a soft fork.

1) It should be 100% because there is no transaction version defined that 
sets them to non-zero, but there is no consensus rule that rejects 
transactions that lie about their version number.
2) https://bitcoinclassic.com/devel/Flexible%20Transactions.html

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
bitcoin/papers/CDE+16.pdf  It may help you form opinions based in science
rather than what appears to be nothing more than a hunch.  It shows that
even 4MB is unsafe.  SegWit provides up to this limit.

I find this paper wholly unconvincing.  Firstly I note that he assumes the
price of electricity is 10c/kwh in Oct 2015.  As a miner operating and
building large farms at that time, I can guarantee you that almost no large
mines were paying anything even close to that high for electricity, even
then.  If he had performed a detailed search on the big mines he would have
found as much, or could have asked, but it seems like it was simply made
up.  Even U.S. industrial electricity prices are lower than that.

Moreover, he focuses his math almost entirely around mining, asserting in
table 1 that 98% of the "cost of processing a transaction" as being
mining.  That completely misunderstands the purpose of mining.  Miners
occasionally trivially resolve double spend conflicts, but miners are
paid(and played against eachother) for economic security against
attackers.  They aren't paid to process transactions.  Nodes process
transactions and are paid nothing to do so, and their costs are 100x more
relevant to the blocksize debate than a paper about miner costs.  Miner's
operational costs relate to economic protection formulas, not the cost of a
transaction.

He also states: "the top 10% of nodes receive a 1MB block 2.4min earlier
than the bottom 10% — meaning that depending on their access to nodes, some
miners could obtain a significant and unfair lead over others in solving
hash puzzles."

He's using 2012-era logic of mining.  By October 2015, no miner of any size
was in the bottom 10% of node propagation.  If they were a small or medium
sized miner, they mined shares on a pool and would be at most 30 seconds
behind the pool.  Pools that didn't get blocks within 20 seconds weren't
pools for long.  If they were a huge miner, they ran their own pool with
good propagation times.  For a scientific paper, this is reading like
someone who had absolutely no idea what was really going on in the mining
world at the time.  But again, none of that relates to transaction "costs."
 Transactions cost nodes money; protecting the network costs miners money.
Miners are rewarded with fees; nodes are rewarded only by utility and price
increases.

On Tue, Mar 28, 2017 at 10:53 AM, Alphonse Pace via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Saturday 13 May 2017 3:26:08 AM Eric Voskuil wrote:

Most people cannot mine except at a huge expense (profit is limited to few 
people via monopoly and electric costs). But more importantly, the profits 
from every miner you buy will go to pay for Bitmain growing their arsenal more 
than enough to offset your influence.

Mining is simply broken at this point.


Running a node and mining are two very different things.


First of all, this isn't donating to miners, but forbidding them from mining 
your transaction (and thereby collecting your transaction fee) unless they 
signal for the softfork.

Secondly, your argument here assumes miners are a government or control 
Bitcoin in some way. This is not correct. Miners are entrusted with 
enforcement of softforks *for old nodes only*, and therefore given the ability 
to trigger activation of the new rules via signalling. But entrusting them 
with this is NOT done by the system itself, but by the users, whose updated 
nodes are the primary mechanism for enforcing softforks. So miners are in fact 
already bound to honour the wishes of the greater economy, and their refusal 
to do so is an attack on the network.

Luke

-------------------------------------

The current Bitcoin Core wallet setup is not as ideal as it could be.
An good example is, that the wallet and the full node (the p2p logic on 8333) do share the same process (same memory space).
AFAIK a lot of users use Core in watch-only mode and do the signing offline (offline / through HWWs).
Although, Core has currently no direct support for offline signing (expect the rawtx API which are pretty expert-ish).

The Core development process goes into that direction but it takes time due to the strict and extremely important code quality insurance.


IMO we should make it better not worse.
Paper wallets delude to do address reuse, the spending-procedure is unclear, and very likely insecure.
A quick photo-snapshot by an attack may result in a full compromised key.
Printer buffers, etc. are also something to worry here.


I think Bitcoin Core does a great job there. But not sure about other security layers are outside of Core.
Especially your operating system.
The reason why we see a growing demand in hardware wallets is probably because people no longer trust in current available operating systems as well as current used desktop/laptop CPUs (like Intel wit it’s MME, etc.).


Yes. That actually something we are considering (especially if we would allow BIP44 or other HD public key derivation forms).
Also, we heard of "support sessions“ on IRC where attackers told victims they must enter „dumpprivkey“ in the Console and give them the output in order „to fix the problem“.


I dislike that as well – in general. But I guess most users like self-protection. Also, the user layer is attackable. If _you_ can access the private-keys, an attacker can do also. What most users want is a key-safe that only signs transactions which they could verify beforehand in a safe environment, and not a way to export private keys or something else that can touch the keys.



The answer is probably: No (for now). But working towards this should be the focus.


---
/jonas

-------------------------------------
There actually isn't an activation threshold in Bitcoin Classic. The hard
fork rules are active the moment you install the software. As was noted,
there aren't any release notes, so you can be forgiven for not knowing that
BIP109 support was removed and the proposal rejected. Classic recently
adopted a new set of hard fork rules for which there is no written
specification.

Bitcoin software vendors should take great pains to document software
features and changes from version to version. Bitcoin Core for example,
always has extensive release notes, and a full changelog extracted from the
source code for each version. In the case of consensus rule change
proposals, we follow the BIPs process which exists to help ecosystem-wide
co-ordination. A detailed and complete specification allows others to
re-implement the BIP in their own software and also acts as part of the
consensus building process and peer review process.

There's nothing wrong with hard forks per se, and this list is certain a
good place to discuss proposals, but releasing hard fork software without
establishing community wide consensus and without clearly labelling your
product as such is just not cricket. If I may cast your attention back a
few weeks ago, Johnson Lau released a hard fork client _testnet_ as part of
his research project which was announced on this list. It was clearly
labelled. This Bitcoin Classic announcement was not clearly labelled (and
released on mainnet).


On Sat, Jan 7, 2017 at 8:12 PM, Chris Priest via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Ah, thanks, I suspected the idea was too simple and must have been
discussed before, but somehow I missed these proposals. I've got some
reading to do.

-Marc

On Thu, Nov 16, 2017 at 11:14 AM, Bryan Bishop <kanzure@gmail.com> wrote:

-------------------------------------
Hi Peter,


I agree with the latter part of your statement but am actually much less confident about the first part… I need to run some numbers on that.


I really, really don’t want to get into it but segwit has many aspects that are less appealing, not least of which being the amount of time it would take to reach the critical mass. 

Surely there's a number of alternative approaches which could be explored, even if only to make a fair assessment of a best response?

/s
-------------------------------------
The "UASF movement" seems a bit premature to me - I doubt UASF will be
necessary if a WTXID commitment is tried first.   I think that should be
first-efforts focus.

On Sat, Apr 15, 2017 at 2:50 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thu, Jul 13, 2017 at 12:19 PM, Dan Libby via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

If you mean you wish to avoid receiving UTXOs that have value that was at
one point previously encumbered by a SegWit output then no, you can't avoid
that. No more than you can currently avoid receiving BTC that were at one
point in time encumbered by a P2SH output.


-------------------------------------
I didn't say typical, I said every. Currently a raspberry pi on shitty adsl
can run a full node. What's wrong with needing a high end pc and good
connectivity to run a full node?

People that want to, can. People that don't want to, won't, no matter how
low spec the machine you need.

If nobody uses bitcoin, all the security in the world provides no value.
The value of bitcoin is provided by people using bitcoin, and people will
only use bitcoin if it provides value to them.  Security is one aspect
only. And the failure to understand that is what has led to the block size
debate.

Rodney

On 1 Apr 2017 10:12, "Eric Voskuil" <eric@voskuil.org> wrote:

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 03/31/2017 02:23 PM, Rodney Morris via bitcoin-dev wrote:

The cause of the block size debate is the failure to understand the
Bitcoin security model. This failure is perfectly exemplified by the
above statement. If a typical personal computer cannot run a node
there is no security.

e
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBCAAGBQJY3uJ8AAoJEDzYwH8LXOFOrBoH/1VdXQObKZ2JPHL387Sd8qT4
zzWt8tKFD+6/uCS8re97h1lZcbwb3EzBOB1J15mJ3fqTOU/rPCitN+JZAMgpw/z9
NGNp4KQDHo3vLiWWOq2GhJzyVAOcDKYLsY8/NrHK91OtABD2XIq9gERwRoZZE4rb
OPSjSAGvDK8cki72O7HpyEKX5WEyHsHNK/JmBDdTjlzkMcNEbBlYMgO24RC6x+UA
8Fh17rOcfGv6amIbmS7mK3EMkkGL83WmsgJKXNl4inI1R8z5hVKRqOFMPxmTDXVc
dEHtw8poHOX1Ld85m0+Tk2S7IdH66PCnhsKL9l6vlH02uAvLNfKxb+291q2g3YU=
=HPCK
-----END PGP SIGNATURE-----
-------------------------------------
On Sat, Apr 1, 2017 at 5:34 PM, Natanael <natanael.l@gmail.com> wrote:

It's not odd, it's just counter-intuitive. How can "< 4 mb weight" be
a more restrictive rule than "< 1 mb size"? Well, it is, that's why
segwit's size increase is a softfork.
It is not that hard once you look at the actual weight formula:
segregated_sigs_sise + (other_size * 4) < 4 "mb"

It is impossible to produce to produce a block that violates the 1 mb
size limit but doesn't violate the 4 mb weight limit too.
There can be block that are < 1 mb size but 20 mb in weight, but those
are invalid according to the new 4 mb weight rule.
At the same time, any block that violates the < 1 mb rule for old
nodes will be invalid not only to old nodes but also to any node
validating the new 4mb rule. This is not by chance but a design choice
for any block size increase within segwit to remain a softfork, which
is what can be deployed faster.

One extreme example would be any 1 mb block today. 1 "mb" of a block
today times 4 is 4 mb, so it complies with the new 4 mb weight rule.
The opposite extreme example would be 4 mb of signatures and 0 mb of
"other data", but this example is not really possible in practice
because signatures need some tx to be part of to be part of the block
itself.
The most extreme examples I have seen on testnet are 3.7 mb blocks,
but those don't represent the average usage today (whenever you read
this).

One common misunderstanding is that users who aren't using payment
channels (that includes lightning but also other smart contracts) or
users that aren't using mutlisig can't enjoy the so called "discount":
there's no reasonable argument for rejecting the "discount" on your
own transactions once/if segwit gets activated.

I would prefer to call the absence of "discount" *penalization*.
Signatures are unreasonable penalized pre-segwit, and there's more
things that remain unreasonably penalized with respect to their
influence on the current utxo after segwit. But signatures are by far
the biggest in data space and validation time, and the most important
unreasonable yet unintended penalization pre-segwit.


Exactly, once one maximum limit is defined, no need for two limits.
But the current max is 1 mb size, not 4 mb weight until/unless segwit
is activated.
Some people complain about 4 mb weight not being as much as 4 mb size,
and that is correct, but both are bigger than 1 mb size.


If the single ratio needs to be modified, it can be modified now
before any rule changes are activated, no need to change the consensus
rules more than needed.


If you don't see any disadvantage on having one single limit if/when
segwit gets activated, I don't see the point of maintaining two
limits, but if you're happy to maintain the branch with the redundant
one you may get my ack: I don't see any disadvantage on checking the
same thing twice besides performance,


That's precisely why it's good segwit has been designed to be backward
compatible as a bip9 softfork.

-------------------------------------
On Thursday 21 September 2017 8:02:42 AM Johnson Lau wrote:

SigAgg is a softfork, so old clients *won't* understand it... am I missing 
something?

For example, perhaps the lookup opcode could have a data payload itself (eg, 
like pushdata opcodes do), and the script can be parsed independently from 
execution to collect the applicable ones.


The same as your OP_MULVERIFY at the consensus level, except new clients would 
execute it as an OP_MUL, and inject pops/pushes when sending such a 
transaction to older clients. The hash committed to for the script would 
include the inferred values, but not the actual on-chain data. This would 
probably need to be part of some kind of MAST-like softfork to be viable, and 
maybe not even then.

Luke

-------------------------------------
This is not a priority, not very important either.
Right now it is possible to create 0-value outputs that are spendable
and thus stay in the utxo (potentially forever). Requiring at least 1
satoshi per output doesn't really do much against a spam attack to the
utxo, but I think it would be slightly better than the current
situation.

Is there any reason or use case to keep allowing spendable outputs
with null amounts in them?

If not, I'm happy to create a BIP with its code, this should be simple.

-------------------------------------
On Wed, Sep 27, 2017 at 1:56 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Even minor revisions can not change the meaning of text. Changing a single
word can often have a strange impact on the meaning of the text. There
should be some amount of care exercised here. Maybe it would be okay as
long as edits are mentioned in the changelog at the bottom of each
document, or mention that the primary authors have not reviewed suggested
changes, or something as much; otherwise the reader might not be aware to
check revision history to see what's going on.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------


No, and the whole issue of compressed vs uncompressed is a red herring. If Alice gives Bob 1MsHWS1BnwMc3tLE8G35UXsS58fKipzB7a, she is saying to Bob “I will accept payment to the scriptPubKey [DUP HASH160 PUSHDATA(20)[e4e517ee07984a4000cd7b00cbcb545911c541c4] EQUALVERIFY CHECKSIG]”.

Payment to any other scriptPubKey may not be recognized by Alice.
-------------------------------------
To follow up on the remarkable work Greg announced from Benedikt Bnz (Stanford)
and Jonathan Bootle (UCL) on Bulletproofs: https://eprint.iacr.org/2017/1066

Summary
=========

Over the last couple weeks, along with Jonas Nick, Pieter Wuille, Greg Maxwell
and Peter Dettmann, I've implemented the single-output version of Bulletproofs
at https://github.com/ElementsProject/secp256k1-zkp/pull/16 and have some
performance numbers.

All of these benchmarks were performed on one core of an Intel i7-6820MQ
throttled to 2.00Ghz, and reflect verification of a single 64-bit rangeproof.


Old Rangeproof    14.592 ms
     with endo    10.304 ms
Bulletproof        4.208 ms
     with endo     4.031 ms
ECDSA verify       0.117 ms
     with endo     0.084 ms


Here "with endo" refers to use of the GLV endomorphism supported by the curve
secp256k1, which libsecp256k1 (and therefore Bitcoin) supports but does not
enable by default, out of an abundance of caution regarding potential patents.

As we can see, without the endomorphism this reflects a 3.47x speedup over
the verification speed of the old rangeproofs. Because Bulletproof verification
scales with O(N/log(N)) while the old rangeproof scales with O(N), we can
extrapolate forward to say that a 2-output aggregate would verify with 4.10x
the speed of the old rangeproofs.

By the way, even without aggregation, we can verify two rangeproofs nearly 15%
faster than verifying one twice (so a 3.95x speedup) because the nature of the
verification equation makes it amenable to batch verification. This number
improves with the more proofs that you're verifying simultaneously (assuming
you have enough RAM), such that for example you can batch-verify 10000
bulletproofs 9.9 times as fast as you could verify 10000 of the old proofs.


While this is a remarkable speedup which greatly improves the feasibility of
CT for Bitcoin (though still not to the point where I'd expect a serious
proposal to get anywhere, IMHO), the concerns highlighted by Greg regarding
unconditional versus computational soundness remain. I won't expand on that
more than it has already been discussed in this thread, I just want to tamp
down any irrational exhuberance about these result.


People who only care about numbers can stop reading here. What follows is a
discussion about how this speedup is possible and why we weren't initially
sure that we'd get any speedup at all.


Details
=========

Section 6 of the linked preprint discusses performance vs our old rangeproofs. As
Greg mentioned, it is possible to fit two 64-bit bulletproofs into 738 bytes,
with logarithmic scaling. (So one proof would take 674 bytes, but eight proofs
only 866 bytes.)

However, this section does not give performance numbers, because at the time
the preprint was written, there was no optimized implementation on which to
benchmark. It was known that verification time would be roughly linear in the
size of the proof: 141 scalar-multiplies for a 64-bit proof, 270 for an
aggregate of two proofs, and so on [*]. Our old rangeproofs required only 128
multiplies for a 64-bit proof, then 256 for two, and so on. So naively we were
concerned that the new Bulletproofs, despite being fantastically smaller than
the original rangeproofs, might wind up taking a bit longer to verify.

For reference, an ordinary ECDSA signature verification involves 2 multiplies.
So roughly speaking, the naive expectation was that a N-bit rangeproof would
require N-many signature verifications' worth of CPU time, even with this new
research. Worse, we initially expected bulletproofs to require 1.5x this much,
which we avoided with a trick that I'll describe at the end of this mail.

As you can see in the above numbers, the old rangeproofs actually perform worse
than this expectation, while the new Bulletproofs perform significantly **better**.
These are for the same reason: when performing a series of scalar multiplications
of the form

  a*G + b*H + c*I + ...

where G, H, I are curvepoints and a, b, c are scalars, it is possible to compute
this sum much more quickly than simply computing a*G, b*H, c*I separately and
then adding the results. Signature validation takes advantage of this speedup,
using a technique called Strauss' algorithm, to compute the sum of two multiplies
much faster than twice the multiple-speed. Similarly, as we have learned, the
141 scalar-multiplies in a single-output Bulletproof can also be done in a single
sum. To contrast, the old rangeproofs required we do each multiplication separately,
as the result of one would be hashed to determine the multiplier for the next.

libsecp256k1 has supported Strauss' algorithm for two points since its inception
in 2013, since this was needed for ECDSA verification. Extending it to many points
was a nontrivial task which Pieter, Greg and Jonas Nick took on this year as part
of our aggregate signatures project. Of the algorithms that we tested, we found
that Strauss was fastest up to about 100 points, at which point Pippenger's was
fastest. You can see our initial benchmarks here

https://user-images.githubusercontent.com/2582071/32731185-12c0f108-c881-11e7-83c7-c2432b5fadf5.png

though this does not reflect some optimizations from Peter Dettmann in the last
week.


It was a happy coincidence that the Bulletproofs paper was published at nearly
the same time that we had working multi-point code to test with.


Finally, the Bulletproof verification process, as written in the paper, is a
recursive process which does not appear to be expressible as a single multiproduct,
and in fact it appears to require nearly twice as many multiplications as I claim
above. I want to draw attention to two optimizations in particular which made this
possible.

1. By expanding out the recursive process, one can see that the inner-product argument
   (Protocol 1 in the paper) is actually one multiproduct: you hash each (L_i, R_i)
   pair to obtain logarithmically many scalars, invert these, and then each scalar in
   the final multiproduct is a product containing either the inverse or original of
   each scalar.

   Peter Dettmann found a way to reduce this to one scalar inversion, from which
   every single scalar was obtainable from a single multiplication or squaring of a
   previous result. I was able to implement this in a way that cached only log-many
   previous results.

2. Next, line (62) of the Bulletproofs paper appears to require N multiplications
   beyond the 2N multiplications already done in the recursive step. But since
   these multiplications used the same basepoints that were used in the recursive
   step, we could use the distributive property to combine them. This sounds
   trivial but took a fair bit of care to ensure that all the right data was still
   committed to at the right stage of proof verification.



Further Work
=========

There are still a few open issues I plan to help resolve in the coming month:

  - Bulletproof aggregation is not compatible with Confidential Assets, where each
    output has a unique asset tag associated with it. There are a couple possible
    solutions to this but nothing public-ready.

  - Bulletproofs, as described in the paper, work only when proving 2^n-many bits.
    I believe there is a straightforward and verifier-efficient way to extend it
    to support non-powers-of-2, but this requires some work to modify the proof in
    the paper.

  - Bulletproofs are actually much more general than rangeproofs. They can be used
    to prove results of arbitrary arithmetic circuits, which is something we are
    very interested in implementing.


[*] By "and so on", I mean that N bits require 2N + 2log_2(N) + 6 scalar multiplies.


Cheers
Andrew



-- 
Andrew Poelstra
Mathematics Department, Blockstream
Email: apoelstra at wpsoftware.net
Web:   https://www.wpsoftware.net/andrew

"A goose alone, I suppose, can know the loneliness of geese
 who can never find their peace,
 whether north or south or west or east"
       --Joanna Newsom

-------------------------------------
See https://github.com/bitcoin/bitcoin/pull/9722

What still needs to be done is that during the first start up after
updating with this popup, the wallet needs to scan for addresses that
have been used in the past. That way the popup isn't only shown for
addresses that are reused after updating.


On 09/27/2017 12:35 PM, Chris Priest via bitcoin-dev wrote:
...
-------------------------------------
On Mon, Dec 11, 2017 at 1:04 PM, Gregory Maxwell <greg@xiph.org> wrote:

Is there a link somewhere to that proposal? The only thing I could find was
your forwarded email
<https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-September/014904.html>
on
this thread.


Omitting nBits entirely seems reasonable, I wrote up a possible
implementation here
<https://github.com/bitcoin/bitcoin/compare/master...jimpo:compact-headers-difficulty>.
The downside is that it is more complex because it leaks into the
validation code. The extra 4 byte savings is certainly nice though.



Can you elaborate on how parallel header fetching might work? getheaders
requests could probably already be pipelined, where the node requests the
next 2,000 headers before processing the current batch (though would make
sense to check that they are all above min difficulty first).

I'm open to more ideas on how to optimize the header download or design the
serialization format to be more flexible, but I'm concerned that we forgo a
40-45% bandwidth savings on the current protocol for a long time because
something better might be possible later on or there might be a hard fork
that at some point requires another upgrade. I do recognize that supporting
multiple serialization formats simultaneously adds code complexity, but in
this case the change seems simple enough to me that the tradeoff is worth
it.
-------------------------------------
On Mon, Jan 2, 2017 at 4:19 PM, Luke Dashjr <luke@dashjr.org
<javascript:_e(%7B%7D,'cvml','luke@dashjr.org');>> wrote:


Do you have any direct evidence of "miner spam"? If so, please link to a
transaction. Also, what could a miner possibly gain from this?

For resource requirements (bandwidth/disk space), please run the numbers on
the Block75 algorithm and compare with growth rates of both.


By this definition, any transaction which transfers ownership of an asset
(stock certificate, deeds to property, copyrights, etc.) is spam. But these
are legitimate, fee paying transactions. They are not spam. Yes, they're
only moving 0.0001 btc. Some will eventually move to Lightning (or
something like it), but many will not as they are unsuitable for off-chain.


No, it actually gives them less power than they have now. Consider the
two-week recalculation: the result of any attempt to manipulate the block
size (up or down) will only last for two weeks. There's no way for a miner
to profit from this (in fact, they'd lose money this way). The best outcome
they could hope for is a very small increase or decrease for two weeks. So
why would anyone do this?

As soon as such a manipulation ended, Block75 would correct the max block
size back to an appropriate level (defined as: average block 75% full).



We covered this ad nauseum in the other Block75 thread, but basically no
one has been able to come up with a realistic scenario wherein miners would
be motivated to do this *and* there aren't any pools large enough now (nor
have there ever been) to have anything more than a minor and temporary
effect.

If such a scenario actually does exist, please describe it in detail.

- t.k.
-------------------------------------


Look at feefilter BIP 133
(https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki#backward-compatibility)
or sendheaders BIP130
(https://github.com/bitcoin/bips/blob/master/bip-0130.mediawiki#backward-compatibility)
Isn't it the same there?
Once BIP151 is implemented, it would make sense to bump the protocol
version, but this needs to be done once this has been
implemented/deployed. Or do I make a mistake somewhere?
How do you threat any other not known message types? Any peer can send
you any type of message anytime. Why would your implementation how you
threat unknown messages be different for messages specified in BIP151?


</jonas>
-------------------------------------
Hi,

Sorry for the delay, I overlooked this email until now. I see that Chris
and CryptAxe both answered but I will also answer, because the message
was addressed to me.

On 6/30/2017 12:00 AM, ZmnSCPxj wrote:

If I've understood you correctly, you have said that each OP Return
links the (ex)-latest block to a brand new block, and that whichever
message of this kind comes first (in the mainchain) wins and the rest
are discarded.

So what if I had a sidechain represented by a jumble of capital letters,
discarded entries as lowercase letters.

Mainchain Block #200001:  [0 --> Q], [0 -->v], [0 -->s], [0 -->b],
Mainchain Block #200002:  [Q --> H], [Q --> z],
Mainchain Block #200003:  [H --> F]
Mainchain Block #200004:  [F --> J], [F -->w]
Mainchain Block #200005:  [H --> P], [J -->x]
Mainchain Block #200006:  [P --> D]

Isn't the chain {{ Q --> H --> F --> J  }} now starting to reorg, with a
competing chain {{ Q --> H --> P --> D  }} ?


What I do want to do, is retain the existing model to some extent.
Specifically, to the degree where sidechains could salvage some bad
situations (eg value overflow incident, or March 2013 incident).

That is a good point. Technically, we do include it twice, but the
second instance (briber-transaction) can be "shuffled" out if the
counterparties are part of the same Lightning Network (which I expect to
the be the case, in equilibrium).


No, sorry. There are many tangled issues (Drivechain (total system);
side-to-main withdrawals (OP CountACKs); individual Drivechains
themselves; Blind Merged Mining (OP BribeVerify)). The ratchet is not
about withdrawals, it is exclusively about Blind Merged Mining, and
making a better OP BribeVerify that offers better guarantees to both sides.

Paul


-------------------------------------
"Absolutely, I assume if Vorick's proposal were implemented that nodes
would have the follow options: Pruned [UTXO + recent two weeks of
blocks], 20%, 40%, 60%, 80%, 100% (archive)."

Yes, and I think that they can have this in "disorder" (only 20% of
blocks in the middle of the blockchain for example)

There are many reasons why I dislike David's proposal as it is, you
mention some below, why 20%? too much? too small? what will happen
tomorrow? etc, I gave others, and this still does not explain why people
should go for more than two weeks of blocks

Maybe what I suggested here again
https://gist.github.com/Ayms/aab6f8e08fef0792ab3448f542a826bf#proposal
could be considered

This is just a suggestion based on incremental "torrents", fortunately
it comes from much more than days of work as you require below and is
the concatenation of thoughts from different projects/studies

It does follow your 8 rules (although I am not sure what you mean in
rule 2 "The decision to contact a node should need O(1) communications",
1 M limit works?) and proposes others, and it solves the issues
mentioned below, or please someone tell me why it does not (I know
people here dislike DHTs, not sure why)

Except fingerprinting (and I don't know what is the SPV issue exactly)
but still is better, if the nodes behave like the groups with most
numerous peers (ie the group seeding, 20%, or the one seeding 40%, or
the one seeding about nothing (sic... subliminal message here...), etc)
then they just can't be fingerprinted (at least based on this feature)

I think the main concepts are detailed enough but maybe that's not the
case, it's a really draft, if you look well the pruning case is
considered, but not explained, like some other points, because
continuing this work with no incentive solution for the seeders looks to
me useless


Le 21/04/2017  22:38, Gregory Maxwell via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms


-------------------------------------
BIP XXXX : User activated features (ROUGH OVERVIEW)

A proposed change to a usage of the 'OP_RETURN' script opcode in Bitcoin
transactions, allowing multiple changes (features) to be deployed in
parallel. It relies on interpreting the output field as a bit vector, where
each bit can be used to track an independent change. Like BIP9, once a
consensus change succeeds or times out, there is a "fallow" pause after
which the bit can be reused for later changes.

==Motivation==

BIP 9 introduced a mechanism for doing soft-forking changes, relying on
measuring miner support indicated by version bits in block headers. As it
relies on miner support, any change which may conflict with miners but is
acceptable to users may be difficult to deploy.   The alternative, a
flag-day deployment can cause issues for users of a feature that has failed
to achieve adequate miner support.

BIP XXXX, if used for deployment, can be used in conjunction with BIP 9, in
order to more safely deploy soft-forking changes that do not require a
supermajority of miners, but do require a large percentage of active
users.

Alternatively, BIP XXXX signalling can be used to gauge user support for
"features" - independent of its use as a direct deployment mechanism.   In
this document a "feature" can be considered synonymous with "soft fork",
but since this mechanism is "user activated", it is not necessarily
restricted to soft-forks.

==Specification==

Each "feature" is specified by the sames set of per-chain parameters as in
BIP9, with the same usage and meaning (name, bit, starttime and timeout).

===Bit flags===

If the outputs contain a zero valued OP_RETURN, and the length of the key
is 2 bytes, and if the first byte (prefix) of that OP_RETURN's key
parameter is 0x012, then the remaining byte is to be interpreted as an
8-bit little-endian integer, and bits are selected within this integer as
values (1 << N) where N is the bit number.  This allows up to 8 features to
be in the STARTED state at a time.

===Array determination===

In order for this to successfully be used for deployment, a lightweight
UTXO must be maintained in memory.   For each bit in STARTED state, a
corresponding bit is set in a map entry for each input address.   Each
input address is hashed to a 24 bit value using SHA3-256(input)[0:24].  An
array with 16777216 2-byte entries (~32MB RAM) is used to record the
current activation state.   The first byte contains the bit flags most
recently associated with an entry.

The second byte contains the log base 2 of the number of "1/100th" bitcoins
most recently associated with this entry.   This is computed by taking the
value, multiplying by 100, converting to an unsigned 32 bit integer, and
using the log2_32 function below (.... log2_32 func defined below ....).

This array is initialized to zero.   The array must be stored and
maintained for each block.  When a block is in the STARTED state for any
bit, the array is updated for each transaction in the block according to
the rules above: a[i][0]=bits, a[i][1]=log2_32(....)

===State transitions===

State transitions work the same as BIP9, however, the determination of the
LOCKED_IN tally is as follows:

For each bit in STARTED state, using the array above, the values are
totaled (unsigned int)(2 << a[i][1]) for each entry where this bit is set
in a[i][0].  In addition the total of all the entries in a, irrespective of
bit, are computed.   This can be done in a single pass, resulting in a
vector of up to 8 32 bit entries containing the "feature totals" for the
array, and one extra 32 bit entry for the sum total of observations since
the start time.

The percentage of observations is computed for each bit.   Up to 8 features
can be computed at a time, with reuse similar to BIP9.

If 2016 sequential blocks have a value of 95% or greater, a feature is
"LOCKED_IN", (75% on testnet)
bit.

Similar to BIP9, a block's state never depends on its own transactions set;
only on that of its ancestors.  ACTIVE and FAILED are terminal states, etc.


On Thu, Apr 20, 2017 at 12:14 PM, Erik Aronesty <erik@q32.com> wrote:

-------------------------------------
On Tue, May 23, 2017 at 2:55 PM, Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Well, it's putting users at more risk only if for those users who
actively decided to put themselves at risk.
I also feel bip148 is rushed and that makes it more risky. I don't
want to reiterate points other have made but I don't fully agree with
all of them.
I prefer the way it is over the way it was (just activating at a given
date without forcing mining signaling), but I still think it's rushed
and unnecessarily risky (unless activating segwit was urgent, which I
think it's not, no matter how much I want it to become active as soon
as possible).
On the other hand, I support uasf and bip8 to replace bip9 for future
deployments, since bip9 made assumptions that weren't correct (like
assuming miners would always signal changes that don't harm any user
and are good for some of them).
Perhaps bip149 can be modified to activate earlier if the current
proposal is perceived as unnecessarily cautious.

Luke, I've seen you say in other forums that "bip148 is less risky
than bip149", but I think that's clearly false.

As a reminder, one of my complains about bip109 was precisely that it
was also rushed in how fast it could activate.

-------------------------------------
On 7 March 2017 at 08:23, Gareth Williams <gjw@posteo.net> wrote:

Well, they'd be censoring transactions to prevent the thing from
activating in the first place. (As opposed to censoring a subset of
those transactions to enforce the new rule, which is the behaviour
that the people promoting the change want.) There would be no point at
which people reasonably expected that something useful would happen if
they sent funds to an address protected by the new scripting rule.


This is true. But what we're talking about here is hostility to *a
particular proposal to change the network rules* which is (in this
hypothetical case) supported by the economic majority of users. This
doesn't, in itself, break Bitcoin, although the economic majority are
of course always free to hard-fork to something new if they're
unhappy.

Edmund

-- 
-- 
Edmund Edgar
Founder, Social Minds Inc (KK)
Twitter: @edmundedgar
Linked In: edmundedgar
Skype: edmundedgar
http://www.socialminds.jp

Reality Keys
@realitykeys
ed@realitykeys.com
https://www.realitykeys.com

-------------------------------------
On Sat, Feb 25, 2017 at 11:12 AM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

P2SH is not secure against collision. I could write two scripts with
the same hash, one of which is an escrow script and the other which
pays it to me, have someone pay to the escrow script, and then get the
payment. Some formal analysis tools would ignore the unused
instructions even if human analysis would not.




-- 
"Man is born free, but everywhere he is in chains".
--Rousseau.

-------------------------------------
On Sat, Feb 25, 2017 at 2:12 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Be aware that the issue is more problematic for more complex contracts.
For example, you are building a P2SH 2-of-2 multisig together with someone
else if you are not careful, party A can hand their key over to party B,
who can may try to generate a collision between their second key and
another 2-of-2 multisig where they control both keys. See
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012205.html
-------------------------------------
Here is a solution may solve Block Withholding Attack. The general idea is
came from Aviv Zohar(avivz@cs.huji.ac.il), I made it work for Bitcoin.
Anyway, thanks Aviv.

=====================

DIFF_1 = 0x00000000FFFF0000000000000000000000000000000000000000000000000000;

Diff = DIFF_1 / target

this is equal to

Diff = DIFF_1 / (target - 0) or Diff = DIFF_1 / abs(target - 0)

now, we change diff algo to below:

New_Diff = DIFF_1 / abs(target - offset)

Offset is 32 bytes, like uint256 in Bitcoin, range is [0, 2^256),
define: offset_hash = DSHA256(offset).

we need to do a little change to the merkle root hash algo, put the
offset_hash as a tx hash in the front of tx hashes.

[offset_hash, coinbase_tx_hash, tx01_hash, tx02_hash, … , tx_n_hash]

Actually could put offset_hash in any place in the array of hashes.

network_hash_range = network_hash_end - network_hash_begin

miner_hash_range = miner_hash_end - miner_hash_begin

The offset value MUST between network_hash_begin/end or
miner_hash_begin/end.

https://user-images.githubusercontent.com/514951/31133378-e00d9ca2-a891-11e7-8c61-73325f59f6ed.JPG

When mining pool send a job to miners, put the PoW hash range
(miner_hash_begin/end) in the job. So if the miners find a hash which value
is between [miner_hash_begin, miner_hash_end], means it's SHOULD be a
valid share, could submit the share to the pool. If the hash value is
between [network_hash_begin, network_hash_end] means find a valid block.

The network_diff is much much high than the miner's diff, means the
network_hash_range is much much smaller than miner_hash_range. By now,
a typical miner's pool diff is around 16K, network diff is 1123863285132,
so miner_hash_range is at least million times bigger than
network_hash_range.
The miners only know miner_hash_range, it's impossible for cheat miners
to find out which share could make a valid block or not.

Problems:
1. it's a hard fork.
2. will make existed asic dsha256 chips useless, but I think it's only a
small change to make new asic chips based on existed tech.
-------------------------------------
Dear Bitcoin-Dev,

I’m writing in to enquire on the post (https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2013-July/002916.html) and I’d like to seek your help and understanding on the codes you modified in order to optimize the syncing of blockchain. For instance, the functions that you modified/variables/values that may lead to a better performance. I’m currently working on further optimising Electrum in terms of syncing and any help will be greatly appreciated.

Thank you and I really look forward to your prompt reply soon.

Regards,
Daryl
-------------------------------------
Thanks for replying, I'd be interested to see what you would come up with
today using the same methodology, seeing as max single hard drive capacity
has roughly doubled, global average internet bandwidth has increased 31%
from 4.8Mbps to 6.3Mbps(sourced from Akamai State of the Internet reports
2014q4 and 2016q3), and we now have xThin and compact blocks to help
significantly with block propagation time.  Not to mention the usual
improvements in CPUs(not that we're anywhere near a CPU bottleneck today
anyway save for quadratic hashing when raising the blocksize, but I don't
think that anyone would seriously suggest an increase without addressing
that).

I don't think that the 17% yearly increase is too far off base considering
current global trends(although I still don't particularly like the idea of
centrally planning the limit, especially not that far into the future), but
the 66% decrease first seems completely out of touch with reality.

I'd also like to point out to Luke that Satoshi envisioned most full nodes
running in data centers in the white paper, not every single user needs to
run a full node to use bitcoin.  Not to present this as an argument from
authority, but rather to remind us what the intention of the system was to
be(p2p cash, not a settlement layer only afforded by the wealthiest and
largest value transactions).  That a lot of people want to continue to move
in that direction shouldn't be a surprise.

On Jan 27, 2017 3:28 PM, "Christian Decker via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

On Fri, Jan 27, 2017 at 03:47:20PM -0500, Greg Sanders via bitcoin-dev
wrote:
overlay
etc.

As one of the authors of that paper and the source of the measurement
data I'd also like to point out that the 4MB number is indeed intended
as an optimistic upper bound on todays network capacity.

More importantly it's not a black and white situation, where there is
a magic number beyond which Bad Things (TM) happen, it's a spectrum on
which we can see a few threshold beyond which we _know_ Bad Things
definitely happen. Miner centralization pressure is felt earlier.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Tier Nolan, right, a new tx version would be required.

I have to look deeper into the CT as sf proposal.

What futures upgrades could this conflict with it's precisely the
question here. So that vague statement without providing any example
it's not very valuable.

Although TXO commitments are interesting, I don't think they make UTXO
growth a "non-issue" and I also don't think they justify not doing
this.

Yeah, the costs for spammers are very small and doesn't really improve
things all that much, as acknowledged in the initial post.



On Thu, Sep 7, 2017 at 8:00 PM, Peter Todd <pete@petertodd.org> wrote:

-------------------------------------
On 02/13/2017 03:11 AM, Matt Corallo wrote:

In the interest of perfect clarity, see your code:

https://github.com/bitcoin/bitcoin/blob/master/src/net_processing.cpp#L1372-L1403

Inside of the VERACK handler (i.e. after the handshake) there is a peer
version test before sending SENDCMPCT (and SENDHEADERS).

I have no idea where the fee filter message is sent, if it is sent at
all. But I have *never* seen any control messages arrive before the
handshake is complete.

are ignored by old peers amounts to a lack of backward compatibility.

See preceding messages in this thread, I think it's pretty clearly
spelled out.

e


-------------------------------------
Den 1 apr. 2017 16:07 skrev "Jorge Timón" <jtimon@jtimon.cc>:

On Sat, Apr 1, 2017 at 3:15 PM, Natanael <natanael.l@gmail.com> wrote:
would
be
And

No, because of the way the weight is calculated, it is impossible to
create a block that old nodes would perceive as bigger than 1 mb
without also violating the weight limit.
After segwit activation, nodes supporting segwit don't need to
validate the 1 mb size limit anymore as long as they validate the
weight limit.


https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki#Block_size

Huh, that's odd. It really does still count raw blockchain data blocksize.

It just uses a ratio between how many units each byte is worth for block
data vs signature data, plus a cap to define the maximum. So the current
max is 4 MB, with 1 MB of non-witness blockchain data being weighted to 4x
= 4 MB. That just means you replaced the two limits with one limit and a
ratio.

A hardfork increasing the size would likely have the ratio modified too.
With exactly the same effect as if it was two limits...

Either way, there's still going to be non-segwit nodes for ages.
-------------------------------------
Ah, two corrections:
1. I meant to include an option c): of course >50% of hashpower running
BIP148 by Aug 1 avoids a split.
2. More seriously, I misrepresented BIP148's logic: it doesn't require
segwit *activation*, just orphans non-segwit-*signaling* (bit 1) blocks
from Aug 1.

I believe that means 80% of hashrate would need to be running BIP91
(signaling bit 4) by ~June 30 (so BIP91 locks in ~July 13, activates ~July
27), not "a few days ago" as I claimed.  So, tight timing, but not
impossible.

Sorry about the errors.


On Fri, Jun 9, 2017 at 12:40 AM, Jacob Eliosoff <jacob.eliosoff@gmail.com>
wrote:

-------------------------------------
Maybe not, unlike frozen objects (certificates, etc), trees are supposed
to extend

Then you can perform progressive hash operations on the objects, ie
instead of hashing the intermediate hash of the objects you do it
continuously (ie instead of hashing the hash of hash file a + hash file
b + hash file c, wait for file d and then do the same, instead hash(file
a + file b + file c), when d comes compute the hash of (file a + file b
+ file c + file d), which implies each time to keep the intermediary
hash state because you are not going to recompute everything from the
beginning)

I have not worked on this since some time, so that's just thoughts, but
maybe it can render things much more difficult than computing two files
until the same hash is found

The only living example I know implementing this is the Tor protocol,
fact apparently unknown, this is probably why nobody cares and nobody is
willing to take it into account (please follow bwd/fwd [1] and see [2]),
this is not existing in any crypto implementations, unless you hack into
it, and this applies to progressive encryption too

[1]
https://lists.w3.org/Archives/Public/public-webcrypto-comments/2013Feb/0018.html


[2] https://github.com/whatwg/streams/issues/33#issuecomment-28554151


Le 23/02/2017  22:28, Peter Todd via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
I have needed to re-tac my intentions somewhat, there is still much
work to be done.

This is a request for assistance and further discussion of the re-
revised proposal. I am sure there are still issues to be resolved.

## BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering
Transactions In Blocks

Schema:  
##########  
Document: BIP Proposal  
Title: UTPFOTIB - Use Transaction Priority For Ordering Transactions In
Blocks  
Date: 26-12-2017  
Author: Damian Williamson &lt;willtech@live.com.au&gt;  
Licence: Creative Commons Attribution-ShareAlike 4.0 International
License.  
URL: http://thekingjameshrmh.tumblr.com/post/168948530950/bip-proposal-
utpfotib-use-transaction-priority-for-order  
##########  

### 1. Abstract

This document proposes to address the issue of transactional
reliability in Bitcoin, where valid transactions may be stuck in the
transaction pool for extended periods or never confirm.

There are two key issues to be resolved to achieve this:

1.  The current transaction bandwidth limit.
2.  The current ad-hoc methods of including transactions in blocks
resulting in variable and confusing confirmation times for valid
transactions, including transactions with a valid fee that may never
confirm.

It is important with any change to protect the value of fees as these
will eventually be the only payment that miners receive. Rather than an
auction model for limited bandwidth, the proposal results in a fee for
priority service auction model.

It would not be true to suggest that all feedback received so far has
been entirely positive although, most of it has been constructive.

The previous threads for this proposal are available here:  
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/s
ubject.html

In all parts of this proposal, references to a transaction, a valid
transaction, a transaction with a valid fee, a valid fee, etc. is
defined as any transaction that is otherwise valid with a fee of at
least 0.00001000 BTC/KB as defined as the dust level, interpreting from
Bitcoin Core GUI. Transactions with a fee lower than this rate are
considered dust.

In all parts of this proposal, dust and zero-fee transactions are
always ignored and/or excluded unless specifically mentioned.

It is generally assumed that miners currently prefer to include
transactions with higher fees.

### 2. The need for this proposal

We all must learn to admit that transaction bandwidth is still lurking
as a serious issue for the operation, reliability, safety, consumer
acceptance, uptake and, for the value of Bitcoin.

I recently sent a payment which was not urgent so; I chose three-day
target confirmation from the fee recommendation. That transaction has
still not confirmed after now more than six days - even waiting twice
as long seems quite reasonable to me (note for accuracy: it did
eventually confirm). That transaction is a valid transaction; it is not
rubbish, junk or, spam. Under the current model with transaction
bandwidth limitation, the longer a transaction waits, the less likely
it is ever to confirm due to rising transaction numbers and being
pushed back by transactions with rising fees.

I argue that no transactions with fees above the dust level are rubbish
or junk, only some zero fee transactions might be spam. Having an ever-
increasing number of valid transactions that do not confirm as more new
transactions with higher fees are created is the opposite of operating
a robust, reliable transaction system.

Business cannot operate with a model where transactions may or may not
confirm. Even a business choosing a modest fee has no guarantee that
their valid transaction will not be shuffled down by new transactions
to the realm of never confirming after it is created. Consumers also
will not accept this model as Bitcoin expands. If Bitcoin cannot be a
reliable payment system for confirmed transactions then consumers, by
and large, will simply not accept the model once they understand.
Bitcoin will be a dirty payment system, and this will kill the value of
Bitcoin.

Under the current system, a minority of transactions will eventually be
the lucky few who have fees high enough to escape being pushed down the
list.

Once there are more than x transactions (transaction bandwidth limit)
every ten minutes, only those choosing twenty-minute confirmation (2
blocks) from the fee recommendations will have initially at most a
fifty percent chance of ever having their payment confirm when 2x
transactions is reached. Presently, not even using fee recommendations
can ensure a sufficiently high fee is paid to ensure transaction
confirmation.

I also argue that the current auction model for limited transaction
bandwidth is wrong, is not suitable for a reliable transaction system
and, is wrong for Bitcoin. All transactions with valid fees must
confirm in due time. Currently, Bitcoin is not a safe way to send
payments.

I do not believe that consumers and business are against paying fees,
even high fees. What is required is operational reliability.

This great issue needs to be resolved for the safety and reliability of
Bitcoin. The time to resolve issues in commerce is before they become
great big issues. The time to resolve this issue is now. We must have
the foresight to identify and resolve problems before they trip us
over.  Simply doubling block sizes every so often is reactionary and is
not a reliable permanent solution.

I have written this proposal for a technical solution but, need your
help to write it up to an acceptable standard to be a full BIP.

### 3. The problem

Everybody wants value. Miners want to maximise revenue from fees (and
we presume, to minimise block size). Consumers need transaction
reliability and, (we presume) want low fees.

The current transaction bandwidth limit is a limiting factor for both.
As the operational safety of transactions is limited, so is consumer
confidence as they realise the issue and, accordingly, uptake is
limited. Fees are artificially inflated due to bandwidth limitations
while failing to provide a full confirmation service for all valid
transactions.

Current fee recommendations provide no satisfaction for transaction
reliability and, as Bitcoin scales, this will worsen.

Transactions are included in blocks by miners using whatever basis they
prefer. We expect that this is usually a fee-based priority. However,
even transactions with a valid fee may be left in the transaction pool
for some time. As transaction bandwidth becomes an issue, not even
extreme fees can ensure a transaction is processed in a timely manner
or at all.

Bitcoin must be a fully scalable and reliable service, providing full
transaction confirmation for every valid transaction.

The possibility to send a transaction with a fee lower than one that is
acceptable to allow eventual transaction confirmation should be removed
from the protocol and also from the user interface.

### 4. Solution summary

#### Main solution

Provide each valid transaction in the mempool with an individual
transaction priority each time before choosing transactions to include
in the current block. The priority being a function of the fee (on a
curve), and the time waiting in the transaction pool (also on a curve)
out to n days (n = 60 days ?), and extending past n days. The value for
fee on a curve may need an upper limit. The transaction priority to
serve as the likelihood of a transaction being included in the current
block, and for determining the order in which transactions are tried to
see if they will be included.

Nodes will need to keep track of when a transaction is first seen. It
is satisfactory for each node to do this independently provided the
information survives node restart. If there is a more reliable way to
determine when a transaction was first seen on the network then it
should be utilised.

Use a dynamic target block size to make the current block. If the block
size is consistently too small then I expect ageing transactions will
be overrepresented as a portion of the block contents, to the point
where blocks will only contain the oldest transactions as they age past
n days. If block size is too large on average then this will shrink the
transaction pool. Determine the target block size using; pre-
rollout(current average valid transaction pool size) x ( 1 / (144 x n
days ) ) = number of transactions to be included in the current block.
The block created should be a minimum 1MB in size regardless if the
target block size is lower.

Nodes that have not yet adopted the proposal will just continue to
create 1MB unordered blocks.

The default value for mempoolexpiry may in future need to be adjusted
to match n days or, perhaps using less than n = 14 days may be a more
sensible approach?

All block created with dynamic size should be verified to ensure
conformity to a probability distribution curve resulting from the
priority method. Since the input is a probability, the output should
conform to a probability distribution.

The curves used for the priority of transactions would have to be
appropriate. Perhaps a mathematician with experience in probability can
develop the right formulae. My thinking is a steep curve. I suppose
that the probability of all transactions should probably account for a
sufficient number of inclusions that the target block size is met on
average although, it may not always be. As a suggestion, consider
including some dust or zero-fee transactions to pad if each valid
transaction is tried and the target block size is not yet met, highest
BTC transaction value first?

**Explanation of the operation of priority:**

and one-hundred (high) it can be directly understood as the percentage
chance in one-hundred of a transaction being included in the block.
Using probability or likelihood infers that there is some function of
random. Try the transactions in priority order from highest to lowest,
if random (100) < transaction priority then the transaction is included
until the target block size is met.

time waiting on a curve value are each a number between one and one-
hundred, a rudimentary method may be to simply multiply those two
numbers, to find the priority number. For example, a middle fee
transaction waiting thirty days (if n = 60 days) may have a value of
five for each part  (yes, just five, the values are on a curve). When
multiplied that will give a priority value of twenty-five, or, a
twenty-five percent chance at that moment of being included in the
block; it will likely be included in one of the next four blocks,
getting more likely each chance. If it is still not included then the
value of time waiting will be higher, making for more probability. A
very low fee transaction would have a value for the fee of one. It
would not be until near sixty-days that the particular low fee
transaction has a high likelihood of being included in the block.

In practice it may be more useful to use numbers representative of one-
hundred for the highest fee priority curve down to a small fraction of
one for the lowest fee and, from one for a newly seen transaction up to
a proportionately high number above one-hundred for the time waiting
curve. It is truely beyond my level of math to resolve probability
curves accurately without much trial and error.

The primary reason for addressing the issue is to ensure transactional
reliability and scalability while having each valid transaction confirm
in due time.

#### Pros

*   Maximizes transaction reliability.
*   Overcomes transaction bandwidth limit.
*   Fully scalable.
*   Maximizes possibility for consumer and business uptake.
*   Maximizes total fees paid per block without reducing reliability;
because of reliability, in time confidence and overall uptake are
greater; therefore, more transactions.
*   Market determines fee paid for transaction priority.
*   Fee recommendations work all the way out to 30 days or greater.
*   Provides additional block entropy; greater security since there is
less probability of predicting the next block. _Although this is not
necessary it is a product of the operation of this proposal._

#### Cons

*   Could initially lower total transaction fees per block.
*   Must be first be programmed.

#### Pre-rollout

Nodes need to have at a minimum a loose understanding of the average
(since there is no consensus) size of the transaction pool as a
requirement to enable future changes to the way blocks are constructed.

A new network service should be constructed to meet this need. This
service makes no changes to any existing operation or function of the
node. Initially, Bitcoin Core is a suitable candidate.

**The service must:**

*   Have an individual temporary (runtime permanent only) Serial Node
ID.
*   Accept communication of the number of valid transactions in the
mempool of another valid Bitcoin node along with the Serial Node ID of
the node whose value is provided.
*   Disconnect the service from any non-Bitcoin node. Bitcoin Core may
handle this already?
*   Expire any value not updated for k minutes (k = 30 minutes?).
*   Broadcast all mempool information the node has every m minutes (m =
10 minutes?), including its own.
*   Nodes own mempool information should not be broadcast or used in
calculation until the node has been up long enough for the mempool to
normalise for at least o minutes (o = 300 minutes ?)
*   Only new or updated mempool values should be transmitted to the
same node. Updated includes updated with no change.
*   All known mempool information must survive node restart.
*   If the nodes own mempool is not normalised and network information
is not available to calculate an average just display zero.
*   Internally, the average transaction pool size must return the
calculated average if an average is available or, if none is available
just the number of valid transactions in the node's own mempool
regardless if it is normalised.

Bitcoin Core must use all collated information on mempool size to
calculate a figure for the average mempool size.

The calculated figure should be displayed in the appropriate place in
the Debug window alongside the text Network average transactions.

Consideration must be given before development of the network bandwidth
this would require. All programming must be consistent with the current
operation and conventions of Bitcoin Core. Methods must work on all
platforms.

As this new service does not affect any existing service or feature of
Bitcoin or Bitcoin Core, this can technically be programmed now and
included in Bitcoin Core at any time.

### 5. Solution operation

This is a simplistic view of the operation. The actual operation will
need to be determined accurately in a spec for the programmer.

1.  Determine the target block size for the current block.
2.  Assign a transaction priority to each valid transaction in the
mempool.
3.  Select transactions to include in the current block using
probability in transaction priority order until the target block size
is met. If target block size is not met, include dust and zero-fee
transactions to pad.
4.  Solve block.
5.  Broadcast the current block when it is solved.
6.  Block is received.
7.  Block verification process.
8.  Accept/reject block based on verification result.
9.  Repeat.

### 6. Closing comments

It may be possible to verify blocks conform to the proposal by showing
that the probability for all transactions included in the block
statistically conforms to a probability distribution curve, *if* the
individual transaction priority can be recreated. I am not that deep
into the mathematics; however, it may also be possible to use a similar
method to do this just based on the fee, that statistically, the block
conforms to a fee distribution. Any dust and zero-fee transactions
would have to be ignored. This solution needs a competent mathematician
with experience in probability and statistical distribution.

There has been some concern expressed over spam and very low fee
transactions, and an infinite block size resulting. I hope that for
those concerned using the dust level addresses the issue, especially as
the value of Bitcoin grows.

This proposal is necessary. I implore, at the very least, that we use
some method that validates full transaction reliability and enables
scalability of Bitcoin. If not this proposal, an alternative.

I have done as much with this proposal as I feel that I am able so far
but continue to take your feedback.

Regards,  
Damian Williamson

[![Creative Commons License](https://i.creativecommons.org/l/by-sa/4.0/
88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)  
<span xmlns:dct="http://purl.org/dc/terms/"
href="http://purl.org/dc/dcmitype/Text" property="dct:title"
rel="dct:type">BIP Proposal: UTPFOTIB - Use Transaction Priority For
Ordering Transactions In Blocks</span> by [Damian Williamson
&lt;willtech@live.com.au&gt;](http://thekingjameshrmh.tumblr.com/post/1
68948530950/bip-proposal-utpfotib-use-transaction-priority-for-order)
is licensed under a [Creative Commons Attribution-ShareAlike 4.0
International License](http://creativecommons.org/licenses/by-sa/4.0/).
Based on a work at [https://lists.linuxfoundation.org/pipermail/bitcoin
-dev/2017-
December/015371.html](https://lists.linuxfoundation.org/pipermail/bitco
in-dev/2017-December/015371.html).
Permissions beyond the scope of this license may be available at [https
://opensource.org/licenses/BSD-3-
Clause](https://opensource.org/licenses/BSD-3-Clause).
-------------------------------------
Hi,

Are there are any vulnerabilities in Bitcoin which have been fixed but
not yet publicly disclosed?  Is the following list of Bitcoin CVEs
up-to-date?

https://en.bitcoin.it/wiki/Common_Vulnerabilities_and_Exposures

There have been no new CVEs posted for almost three years, except for
CVE-2015-3641, but there appears to be no information publicly available
for that issue:

https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3641

It would be of great benefit to end users if the community of clients
and altcoins derived from Bitcoin Core could be patched for any known
vulnerabilities.

Does anyone keep track of security related bugs and patches, where the
defect severity is similar to those found on the CVE list above?  If
yes, can that list be shared with other developers?

If some fixes have been committed with discreet log messages, it will be
difficult for third parties to identify and assess the importance of any
critical patches.  Do any important ones come to mind?

Finally, curious to know, what has changed since 2014 that has resulted
in the defect rate, at least based on the list of publicly reported
CVEs, to fall to zero?  A change to the development process?
Introduction of a bug bounty?

Best Regards,

Simon


-------------------------------------
Hi Sancho,

I saw your proposal. However, my point is that the threshold should be
part of the signaling, and not fixed in the soft-fork proposal.

I agree that coinbase space might be a limitation.

To avoid this, I realize that the threshold could be encoded in the
version bits. We have 32 version bits, and the top 3 bits must be set to
001 in BIP9. In order to extend BIP9 in a backward compatible way, we
could set these 3 top bits to 010, and use the 29 remaining bits for
soft fork signaling.

If we use 7 bits per soft-fork proposal, we have enough space to encode
four simultaneous soft-fork proposals, and sub-percent granularity for
the threshold (2^7=128).



Le 13/04/2017 à 16:17, Sancho Panza a écrit :

-------------------------------------
single 512GB SSD. Lots of consumer hardware has that little storage.

That's very poor logic, sorry.  Restricted-space SSD's are not a
cost-effective hardware option for running a node.  Keeping blocksizes
small has significant other costs for everyone.  Comparing the cost of
running a node under arbitrary conditons A, B, or C when there are far more
efficient options than any of those is a very bad way to think about the
costs of running a node.  You basically have to ignore the significant
consequences of keeping blocks small.

If node operational costs rose to the point where an entire wide swath of
users that we do actually need for security purposes could not justify
running a node, that's something important for consideration.  For me, that
translates to modern hardware that's relatively well aligned with the needs
of running a node - perhaps budget hardware, but still modern - and
above-average bandwidth caps.

You're free to disagree, but your example only makes sense to me if
blocksize caps didn't have serious consequences.  Even if those
consequences are just the threat of a contentious fork by people who are
mislead about the real consequences, that threat is still a consequence
itself.

On Wed, Mar 29, 2017 at 9:18 AM, David Vorick via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Fri, Apr 7, 2017 at 12:48 AM, Tomas <tomas@tomasvdw.nl> wrote:

How do you deal with validity rules changing based on block height?


So it sounds like to work the software still needs an analog of a
(U)TXO database? I am confused by the earlier comments about thinking
the the resource consumption of the (U)TXO database is not a
consideration in your design.


If you get a transaction claiming to spend 0xDEADBEEFDEADBEEF, an
output that never existed how does your spent index reject this spend?

-------------------------------------
Hi Matt,

Thanks for very much for your thoughtful review

Comments below.

On 12/3/2017 4:32 PM, Matt Corallo wrote:

Well, that is quite a minor quibble, because it is just a one-time cost
of 120 bytes per sidechain.

To address it, there could instead be a hash commitment to this
information. That commitment could be "optional", in that old nodes
wouldn't need to possess the 120 bytes. Although, all of the sidechains
are themselves optional. And old nodes will be ignoring all of this data
anyways. So I do not think

The inclusion of this field is deliberate. Probably, you do not buy my
lengthy argument about "categorical control".

http://www.drivechain.info/faq/#categorical-control

Perhaps you have seen my May 2016 presentation on the topic. It was
itself a lengthy reply to comments that Greg Maxwell made about the
original Nov 2015 Drivechain post.

https://www.youtube.com/watch?v=xGu0o8HH10U&list=PLw8-6ARlyVciMH79ZyLOpImsMug3LgNc4&index=1

Either you aren't aware [of why I want each sidechain to have a
comprehensible category]. Or, you are aware and you disagree. But if it
is the latter we might just have to agree to disagree.


I agree. I myself am unhappy with the private key approach, as it
results in a totally pointless signature being generated, and a
pointless CheckSig operation. (Somewhere, buried in
documentation/GitHub-issue purgatory, there is a discussion of replacing
it with OP TRUE.)

Basically, our way was just a hack to make sure uses knew where they had
to send the money, and also to get the balances to show up in all user's
wallets. I do not pretend to have any expertise in this area (or even
experience) so it is surely an area for improvement.


Well, I partially agree.

However, if drivechain is a soft fork, and if 51% hashrate can add new
softforks at any time, then our ability to alter the rate of
sidechain-creation is very low. While we might use the rules of
"Drivechain I" to impose restrictions on the "add a sidechain" process,
nothing prevents 51% hashrate from re-adding Drivechain to Core a second
time, creating a "Drivechain II" system with its own "add a sidechain"
rules. Thus, the activation speed can increase, even if miners are
incapable of writing any code. Or, miners might add "Drivechain I" to
one of the sidechains. (And of course the new-sidechain-rate can
decrease, through mere miner-censorship.)

So, I think there really is no threat model, other than to say: we
either open Pandora's box or we do not. My vision (for any Redditors who
may be reading this, years in the future!!) is of a stable, conservative
Bitcoin Core with 3-8 sidechains, of which at least one is rather
experimental, and at least one of which has its own sidechains. But who
knows.

More importantly, the problem you've outlined is much much worse for
extension blocks.

(It can scarcely be denied that hashrate wants more block space, and
that they can easily add one or many extension blocks, in public or in
secret, at any time. Will a UASF really be able to disable an in-use
extension block? I think the UASF-case is much less persuasive,
especially since it involves loss/freezing of user funds.)

So I would argue that one of *the* greatest benefits of Drivechain is
that it neutralizes the threat of extension blocks by giving everyone a
better alternative. In fact, I do not know of any other way to
neutralize this threat.


I don't have a problem with this. In fact that is mostly how we have it
today.

My concern is a scenario where:

Person A: is running the latest version of Bitcoin (which has
drivechain), and full nodes for 3 out of 3 sidechains
Person B: is running the 2nd-latest version of Bitcoin (which has
drivechain), and was disconnected when the 3rd sidechain activated
Person C: is running the 3rd-latest version of Bitcoin (which has
drivechain), but was disconnected for the activation of all sidechains.
Person D: is running 0.5.3 and has no idea what drivechain is.

Then, we consider a case where someone attempts a side-to-main
withdrawal from sidechain #2, but which tries to cheat the drivechain
rules (which are mainchain-enforced).

In one setup, C's security is downgraded. But in other settings it is
not. And in other settings it might do something complicated.

(Although, I also plan to introduce minimum parameter values, both to
prevent C from being harassed in this case, and to force all
drivechain-killing actions to be comparable to each other.)

My thought was to have all drivechain parts to either stand or fall by
themselves. But I am open-minded on this.


Well, your proposal doesn't reduce the bloat, it merely makes
bloat-reduction possible. And your way relies (slightly) on
miner-charity, because it imposes an opportunity cost on miners. (Miners
could sell their blockspace for fees, but instead they must use it to
make these combinations you describe.)

In contrast, the one-user-deposits-at-a-time not only allows bloat to be
addressed, but also forces it to be addressed. It is like forcing
someone who uses a shared kitchen to leave it exactly as clean as they
found it.

While I am concerned by the one-at-a-time concept, I would point out:

* It is NOT one deposit per block. Just one at a time. In general, there
can be as many deposits as needed.
* It will not be a problem if [a] transactions propagate very quickly,
and [b] transactions are signed very quickly.
* If the deposit fails, it will likely be easy for the user to re-do it
(or, this will be made easy, in the UX eventually).
* It may ultimately be the case, that the task of shepherding the coins
around is one that is only ever performed by specialists. They would
have their own ways of batching txns to deal with this issue. In
contrast, regular users might always use atomic swaps / ShapeShift-like
tools.

Nonetheless, I think this is another opportunity for improvement.
Probably, if someone goes deeper into the scripting language and block
validation rules, they will be able to achieve all of the objectives
simultaneously. As you say:




Yes, I think there should be some kind of switch for saying "please
withdraw all of your funds because this chain is being closed down".
However, if miners stopped mining a chain, I think sidechain-users would
notice because their blocktimes would start to increase (under blind
merged mining, anyway).



I think you are missing a few things.

First of all, I think the security model for sidechains is the same as
that of every blockchain

People will say things, like "but with sidechains 51% hashrate can steal
your coins!", but as I have repeated many times, this is also true of
mainchain btc-tx. As I say on drivechain.info:

 """ In theory, the incentives of miners and investors are very strongly
aligned: both are compensated most when the exchange rate is highest.
And, in practice, we do not see large reorganizations (where miners can
“steal”, by first depositing BTC to major exchanges, then selling that
BTC for fiat (which they withdraw), and finally rewriting the last 3 or
4 days of chain history, to un-confirm the original deposits). These
reorgs would devastate the exchange rate, as they would cast doubt on
the entire Bitcoin experiment. The thesis of Drivechain is that
sidechain-theft would also devastate the exchange rate, as it would cast
doubt on the entire sidechain experiment (which would itself cast doubt
on the Bitcoin experiment, given the anti-competitive power of
sidechains). """

In fact, it is true of everything, including the lightning network. LN
has the advantage of allowing victims to spend the attacker's funds on
tx fees (as these victims desperately try to get their txn included).
But the LN loses the blockchain's "strength in numbers" advantage --
miners can single-out unpopular individuals, figure out their channels,
and steal from them (and only them) at an inopportune time.

This is not to knock the lightning network -- I believe it is
well-designed and likely to be secure. I am merely saying that this
concept of stringing these security models on a line from "most secure"
to "least secure" is a concept which is reductionist and incorrect.

Drivechain will be more secure if sidechains are popular. But if they
are not popular, the question of how secure they are is not really
interesting, is it?

Secondly, I think you have overlooked something very important indeed.
Sidechains are optional, and so their use should be up to each
individual user, and no one else. Users should be free to make their own
mistakes -- specifically, they should be the ones to decide for
themselves if they want to use an "insecure" system or not.

It would be another matter, if you had a competing sidechain idea which
accomplished the same goal. But you do not.

Thirdly, I do not agree with your claim that the security model is
reduced by BMM. In fact, the way I see it, it is the same as the model
we already have, if not better.

To make this point, let me ask: Who determines the contents (valid or
otherwise) of "the next block that meets the difficulty requirement"?

In Mainchain Bitcoin Core: "Highest Cumulative Difficulty"
BMM Sidechain: "Highest BTC Payment"

But these are actually the very same thing! We merely need to clarify
our thinking with a few simplifications. First, substitute "Most Hashes"
for "Cumulative Difficulty" (as these are expected converge to each
other). Second, ignore *unexpected* fluctuations in the two denominator
prices in the following:

"Most Hashes" = "Most USD Spent on Mining" / (hashes-per-usd price)
"Highest BTC Payment" = "Most USD Spent on Mining" / (btc-per-usd price)

"Most Hashes" = "Highest BTC Payment" = "Most USD Spent on Mining"

It should be clearer now that they are the same model. While the
mainchain follows the heaviest chain, measured in hashes, the sidechain
follows the heaviest chain, measured in BTC. Both are expressions of the
same concept ("value sacrificed")...just expressed in different units.

With that explained, let me bring in this:


Your example (which is a great example) sounds bleak, but it is in fact
the security model of Bitcoin itself, in the long future without the
block subsidy. Likely, Bitcoin will have new advantages by then
(assuming it survives that long). But this is a problem that just
*can't* be solved without a new block subsidy (which can't be added to
sidechains).

So, you may be successfully arguing that sidechains can never work. (But
that is different from saying that users should be prohibited from
trying them out, as I said above). Or, you may be successfully arguing
that Bitcoin itself will stop working when fees overwhelm the block
subsidy. (Since that hour is rapidly approaching, we might as well start
running experiments now).

The equivalence between hashes and purchases is not perfect. Certainly,
regular miners might be better-behaved than BM-miners, because r-miners
stand to lose their entire hardware investment if the system fails,
whereas BM-miners do not. On the other hand, BMM is *better* in a few
ways, namely that it makes "mining" much more competitive, because it
lowers the barriers to entry for sidechain-mining all the way to zero.
Any node can do it. Furthermore, BM-miners are more cypherpunk-like:
they will not be confined to any physical location, they do not give
away their location (via power usage or thermal exhaust), when they
greedily move into high-efficiency spaces (data centers, EC2 instances)
they can instantly destroy themselves and reappear somewhere else.

I'm not sure if that made you more, or less depressed. But here is a
smiley face :-] .


Well, BMM is more efficient when there are pools. Without them, the
sidechain nodes would be trying to connect to all mainchain miners.

And there's no need for that. In my view, pools are cannot really do
anything wrong (ie, pools cannot do anything except what their members
want them to do). If a pool operator goes rogue and attempts to censor a
transaction, what has actually happened is just that the transaction is
delayed (until the hashers learn about the inefficient policy, and
switch pools). Same for everything else.

In other words, yes pool operators would almost certainly be running a
node of each sidechain.


It is not about the amount of BTC on the sidechain. It is about miner's
estimations of user's valuation of their option to use the sidechain at
any point in the future. The idea of "911 emergency response" is
valuable, and people would complain about a motion to get rid of it,
even though most of those people wouldn't currently be using it.


That is interesting because that section reads:

 """ As miners provide work for more blockchains, more resources are
needed to track and validate them all. Miners that provide work for a
subset of blockchains are compensated less than those which provide work
for every possible blockchain. Smaller-scale miners may be unable to
afford the full costs to mine every blockchain, and could thus be put at
a disadvantage compared to larger, established miners who are able to
claim greater compensation from a larger set of blockchains. """

 Which is exactly what BMM does address. It allows miners to ignore the
resource-cost of the sidechain, and therefore smaller miners will not be
at a revenue-disadvantage.

Do you think that the drawback is something else?

And, are you ever going to define "miner centralization"? Is it "the
economic barrier-to-entry for mining", to you?

Paul

https://github.com/drivechain-project/docs/blob/master/bip1-hashrate-escrow.md
https://github.com/drivechain-project/docs/blob/master/bip2-blind-merged-mining.md
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014364.html
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-June/014559.html



-------------------------------------
On Mon, May 22, 2017 at 5:19 PM, Paul Sztorc via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I don't think they are the same.

With Bitcoin, you only get to reverse recent transactions.  If you actually
reversed 2-3 weeks of transactions, then the Bitcoin price would fall,
destroying the value of the additional coins you managed to obtain.  Even
if their was no price fall, you can only get a fraction of the total.

With BMM, you can "buy" the entire reserve of the sidechain by paying
(timeout) * (average tx fees).  If you destroy a side-chain's value, then
that doesn't affect the value of the bitcoins you manage to steal.

The incentive could be eliminated by restricting the amount of coin that
can be transferred from the side chain to the main chain to a fraction of
the transaction fee pay to the bitcoin miners.

If the side chain pays x in fees, then at most x/10 can be transferred from
the side chain to the main chain.  This means that someone who pays for
block creation can only get 10% of that value transferred to the main chain.

Main-chain miners could support fraud proofs.  A pool could easily run an
archive node for the side chain in a different data center.

This wouldn't harm the performance of their main operations, but would
guarantee that the side chain data is available for side chain validators.

The sidechain to main-chain timeout would be more than enough for fraud
proofs to be constructed.

This means that the miners would need to know what the rules are for the
side chain, so that they can process the fraud proofs.  They would also
need to run SPV nodes for the side chain, so they know which sidechain
headers to blacklist.


The big difference is that Bitcoin holds no assets on another chain.  A
side-chain's value is directly linked to the fact that it has 100% reserves
on the Bitcoin main chain.  That can be targeted for theft.


-------------------------------------
Thank you, Conrad, for the feedback and I think I can see your points
clearly, but I would disagree that decentralised proof-of-work doesn't
change game rules drastically.
You are correct about proof-of-work market splitting into
"transaction-miners" and "blockchain-miners", but this is not just another
way to pay the fees, it's actually completely different mechanism. Here are
some arguments that might be able to convince you or at least to question
your premises:

1. Since bitcoin mining is a game in which winner takes it all, modified
proof-of-work will introduce a new tradeoff for the miners: either you
collect more fees (increasing risk of losing reward) or you collect more
proof-of-work (increasing reward win probability). This will create the
missing economic link between transaction fees (BTC) and proof-of-work
security costs (Hashcash PoW), creating the secondary market of
transaction-mining-capable hash power / BTC. It feels like some new game
dynamics arise in this setting, but I haven't run any numerical simulations
yet, though I'm going to do so.

2. Why is it possible to stop spam and DDoS of the network? The reason is
that to generate transaction proof-of-work user have to spend some physical
time (not money). After the user found out previous block hash he has two
choices: either to start doing some work (probably with the help of
transaction miners AND money) or instantly adjust fee at every moment (only
money). This adds a new signalling mechanism alongside standard fee market,
which will make it possible for miners to filter and do not propagate
transactions with PoW less than some handicap value (of course they can
always agree on some whitelist of non-spammers), thus preventing abuse of
spammer transactions with competitive fees to delay valuable and important
transfers of BTC.

Hope I addressed all the issues you mentioned.


On 22 June 2017 at 15:02, Conrad Burchert <conrad.burchert@googlemail.com>
wrote:

-------------------------------------
On Wednesday, 12 July 2017 03:22:59 CEST Karl Johan Alm via bitcoin-dev 
wrote:

I've heard this before and it doesn't make any sense to me. Just like your 
Linux box needs a reboot to get a kernel upgrade, your node needs a restart 
to upgrade. Neither the (entire) internet will go down nor the (entire) 
Bitcoin network will go down as a result.


This is fine, and groups that do development should do this more structured 
than something like https://bitcoinhardforkresearch.github.io/

It just would not make any sense to have a roadmap for the *entire* industry 
as this would require you to decide what technical solution is better than 
another before either of them are fully researched.

Individual groups can have solutions that they believe is the best. And then 
we can let the market decide which one is to be actually activated.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
is to avoid having to deal with the extra bandwidth that segwit could
require.   Running a 0.14.2 node means being ok with >1MB blocks, in case
segwit is activated and widely used. Users not interested in segwit
transactions may prefer to keep the cost of their node lower.

If the majority of the network decides to deploy SegWit, it would be in
your best interest to validate the SegWit transactions, because you might
will be downgraded to near-SPV node validation.
It would be okay to still run a "non-SegWit" node if there's no SegWit
transactions in the chain of transactions for your bitcoins, otherwise you
cannot fully verify the the ownership of your bitcoins.
I'm not sure the practicality of this in the long run, but it makes a case
for having an up-to-date non-SegWit node, although I think it's a bit of a
stretch.

Greetings
Hampus

2017-07-13 15:11 GMT+02:00 Federico Tenga via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
quickly if bitcoin-core were to ship with the defaults set to a pruned
node."

Sorry to be straight, I read the (painful) thread below, and most of
what is in there is inept, wrong, obsolete... or biased, cf the first
sentence above, if the idea is to invent a workaround to the fact that
pruning might/will become the default or might/will be set by the users
as the default so full nodes might/will disappear then just say it
clearly instead of proposing this kind of non-solution as a solution to
secure the blockchain

I can't believe this is serious, people now are supposed to prune but
will be forced to host a part of the blockchain, how do you expect this
to work, why people would do this? Knowing that to start pruning they
need a full node, then since we are there, why not continuing with a
full node... but indeed, why should they continue with a full node, and
therefore why should they accept to host a part of the blockchain if
they decline the first proposal?

This is absurd, you are not addressing the first priority given the
context which is to quickly increase the full nodes and which obviously
includes an incentive for people to run them

It gives also the feeling that bitcoin wants to reinvent everything not
capitalizing on/knowing what exists, sorry again but the concepts of the
proposal and others like archival nodes are just funny


Le 18/04/2017  15:07, Tier Nolan via bitcoin-dev a crit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
On Thu, Mar 23, 2017 at 3:37 PM, Juan Garavaglia via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Hello Juan,

this is expected behaviour. Nodes with segwit active only download
blocks from other segwit peers, as old peers cannot provide the
witness data they need to verify the blocks.

-- 
Pieter

-------------------------------------
The idea is that some peers, when you connect to them will work fine for
some time, but you need to find out the rate for services and send a
micropayment to maintain the connection.   This creates an optional pay
layer for high quality services, and also creates DDOS resistance in this
fallback layer.
-------------------------------------
(copying from GitHub per jonasschnelli's request)

I can understand the desire to keep all reference strings to the nice
14-character version by keeping the data payload to 40 bits, but it
seems to place artificial limitations on the format (year 2048 & 8191
transactions). I also understand that this might be addressed with
Version 1 encoding. But current blocks are not that far from having
8191 transactions.

You could go with a variable-length encoding similar to Bitcoin's
variable ints and gain the benefit of having a format that will work
for very large blocks and the very far future.

Also, the Bech32 reference libraries allow encoding from byte arrays
into the base-5 arrays native to Bech32. It seems like bit-packing to
these 40 bits might be overkill. As an alternative you could have one
bit-packed byte to start:

# First two bits are the protocol version, supporting values 0-3
V = ((protocol version) & 0x03) << 6
# Next two bits are magic for the blockchain
# 0x00 = Bitcoin
# 0x01 = Testnet3
# 0x02 = Byte1 is another coin's magic code (gives 256 options)
# 0x03 = Byte1-2 is treated as the coin magic code (gives 65280 more options)
M = (magic & 0x03) << 4
# Next two bits are the byte length of the block reference
B = ((byte length of block reference) & 0x03) << 2
# Final two bits are the byte length of the transaction index
T = ((byte length of transaction index) & 0x03)
# Assemble into the first byte
Byte0 = V | M | B | T

This gives you up to 3 bytes for each block and transaction reference,
which is 16.7 M blocks, or year 2336, and 16.7 M transaction slots.

Data part: [Byte0][optional magic bytes 1-2][block reference bytes][tx
reference bytes]

So the shortest data part would have 3 bytes in it, with the reference
version 0 genesis coinbase transaction having data part 0x050000.

I know this is a departure from your vision, but it would be much more
flexible for the long term.


Clark

-------------------------------------
In https://github.com/bitcoin/bitcoin/pull/11423 <https://github.com/bitcoin/bitcoin/pull/11423> I propose to make OP_CODESEPARATOR and FindAndDelete in non-segwit scripts non-standard

I think FindAndDelete() is one of the most useless and complicated functions in the script language. It is omitted from segwit (BIP143), but we still need to support it in non-segwit scripts. Actually, FindAndDelete() would only be triggered in some weird edge cases like using out-of-range SIGHASH_SINGLE.

Non-segwit scripts also use a FindAndDelete()-like function to remove OP_CODESEPARATOR from scriptCode. Note that in BIP143, only executed OP_CODESEPARATOR are removed so it doesn’t have the FindAndDelete()-like function. OP_CODESEPARATOR in segwit scripts are useful for Tumblebit so it is not disabled in this proposal

By disabling both, it guarantees that scriptCode serialized inside SignatureHash() must be constant

If we use a softfork to remove FindAndDelete() and OP_CODESEPARATOR from non-segwit scripts, we could completely remove FindAndDelete() from the consensus code later by whitelisting all blocks before the softfork block. The first step is to make them non-standard in the next release.


 
-------------------------------------
On Saturday 13 May 2017 4:23:41 AM Russell O'Connor wrote:

I already explained why this isn't the case: If we're comparing MASF to 
MASF+CBV, then I agree. But MASF is not necessarily always on the table, so 
the comparison where this becomes relevant is MASF+CBV vs UASF.


Versionbits change/lose their meaning after the deployment timeout. For this 
reason, the timeout must be specified so the check is skipped when that 
occurs.

Also, doing it the way you describe would fail to enforce that BIP9 is 
actually in use for the block version; you could simply add that as an 
additional condition, but it seems pretty hacky since you wouldn't be able to 
upgrade versionbits anymore...


Script validity can still be cached with this: you would always allow the 
opcode to succeed at evaluation-time, and simply store the criteria checked 
separately. Then it would behave effectively the same as using the transaction 
version number.

Luke

-------------------------------------
Seems to me an obvious use case for drive chains are to have high speed
small transactions on a side chain, eventually cleared to the main chain.

Not sure why miners would want this to fail any more than any other side
chain, like Liquid or lightning.



On May 28, 2017 5:23 PM, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

On Mon, May 22, 2017 at 05:30:46PM +0200, Paul Sztorc wrote:
would

That's not at all true. If I'm a miner with a better capability than another
miner to prevent that theft, I have reasons to induce it to happen to give
me
political cover to pushing that other miner off the network.

This is a very similar problem to what we had with zeroconf double-spending,
where entities such as Coinbase tried to pay off miners to guarantee
something
that wasn't possible in a geninely decrentralized system: safe zeroconf
transactions.


Why are you forcing miners to run this code at all?

Equally, you're opening up miners to huge political risks, as rejecting all
withdrawals is preventing users' from getting their money, which gives other
miners a rational for kicking those miners off of Bitcoin entirely.

question.

Why do you think this will be infrequent? Miners with a better ability to
validate the drivechain have every reason to make these events more
frequent.


This is also a very dubious security model - I would argue that Bitcoin is
much
*more* valuable if miners do everything they can to ensure that drivechains
fail, given the huge risks involved. I would also argue that users should do
user-activated-soft-forks to ensure they fail.

By comparison, note Adam Back and my own efforts to ensure miners have a
smaller part in the ecosystem, with things like committed (encrypted)
transactions and my closed-seal-set/truth-list approach(1). We want to
involve
miners as little as possible in the consensus, not more.

I have to ask: What use-cases do you actually see for drivechains? Why can't
those use-cases be done in the much safer client-side validation fashion?

1) https://petertodd.org/2016/closed-seal-sets-and-truth-lists-for-privacy

--
https://petertodd.org 'peter'[:-1]@petertodd.org

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Fundamentally, what you have described is not a UASF, it is a miner attack on other miners. 51% of hash power has always been able to collude to orphan blocks that contain transactions they have blacklisted (a censorship soft fork). This is clearly a miner attack which would escalate pretty rapidly into a PoW change if sustained for any time.

Miners have always retained the ability to include whatever valid transactions they like. If they don't like P2SH or segwit, they don't have to include them in their blocks. There is a clear difference between opting out of transaction selection vs miners attacking other miners to prevent their voluntary participation in mining valid transactions.

Of course, anything is possible, but game theory and incentives seem to suggest that any tantrums would be at most, short lived, if lived at all. Mining is an extraordinarily expensive endeavour, which is the basis of the strong assumption of Bitcoin that PoW/revenue incentives will keep miners honest. If that assumption is broken, Bitcoin has bigger problems.


-------- Original Message --------
Subject: Re: [bitcoin-dev] Flag day activation of segwit
From: nickodell



The problem with modifying Bitcoin to work around community norms is that it's a two-way street. Other people can do it too.

Let me propose a counter-fork, or a "Double UASF." This is also a BIP9 fork, and it uses, say, bit 2. starttime is 1489449600, and the end time is 1506812400. It enforces every rule that UASF enforces, plus one additional rule. If 60% of blocks in any retargeting period signal for Double UASF, any descendant block that creates or spends a segregated witness output is invalid.

Double UASF signaling never coincides with UASF signaling, because the signaling periods don't overlap. Full nodes happily validate the chain, because Double UASF doesn't break any rules; it just adds new ones. Miners who adopt Double UASF don't need to understand segwit, because all segwit transactions are banned. Miners don't need to commit to a wtxid structure, either. Per BIP 141, "If all transactions in a block do not have witness data, the commitment is optional." Segwit is activated, but useless. Miners who *do* adopt segwit will lose money, as their blocks are orphaned.

Thanks,
--Nick



On Sun, Mar 12, 2017 at 9:50 AM, shaolinfry via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

I recently posted about so called "user activated soft forks" and received a lot of feedback. Much of this was how such methodologies could be applied to segwit which appears to have fallen under the miner veto category I explained in my original proposal, where there is apparently a lot of support for the proposal from the economy, but a few mining pools are vetoing the activation.

It turns out Bitcoin already used flag day activation for P2SH[[1](https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283)], a soft fork which is remarkably similar to segwit. The disadvantage of a UASF for segwit is there is an existing deployment. A UASF would require another wide upgrade cycle (from what I can see, around 80% of existing nodes have upgraded from pre-witness, to NODE_WITNESS capability[[2](http://luke.dashjr.org/programs/bitcoin/files/charts/services.html)][[3](https://www.reddit.com/r/Bitcoin/comments/5yyqt1/evidence_of_widespread_segwit_support_near50_of/)]. While absolute node count is meaningless, the uprgrade trend from version to version seems significant.

Also it is quite clear a substantial portion of the ecosystem industry has put in time and resources into segwit adoption, in the form of upgrading wallet code, updating libraries and various other integration work that requires significant time and money. Further more, others have built systems that rely on segwit, having put significant engineering resources into developing systems that require segwit - such as several lightning network system. This is much more significant social proof than running a node.

The delayed activation of segwit is also holding back a raft protocol of innovations such as MAST, Covenants, Schnorr signature schemes and signature aggregation and other script innovations of which, much of the development work is already done.

A better option would be to release code that causes the existing segwit deployment to activate without requiring a completely new deployment nor subject to hash power veto. This could be achieved if the economic majority agree to run code that rejects non-signalling segwit blocks. Then from the perspective of all existing witness nodes, miners trigger the BIP9 activation. Such a rule could come into effect 4-6 weeks before the BIP9 timeout. If a large part of the economic majority publicly say that they will adopt this new client, miners will have to signal bip9 segwit activation in order for their blocks to be valid.

I have drafted a BIP proposal so the community may discuss https://gist.github.com/shaolinfry/743157b0b1ee14e1ddc95031f1057e4c (full text below).

References:
[1]: https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283
[2]: http://luke.dashjr.org/programs/bitcoin/files/charts/services.html
[3]: https://www.reddit.com/r/Bitcoin/comments/5yyqt1/evidence_of_widespread_segwit_support_near50_of/

Proposal text:

<pre> BIP: bip-segwit-flagday Title: Flag day activation for segwit deployment Author: Shaolin Fry <shaolinfry@protonmail.ch> Comments-Summary: No comments yet. Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-???? Status: Draft Type: Informational Created: 2017-03-12 License: BSD-3-Clause CC0-1.0 </pre> ==Abstract== This document specifies a BIP16 like soft fork flag day activation of the segregated witness BIP9 deployment known as "segwit". ==Definitions== "existing segwit deployment" refer to the BIP9 "segwit" deployment using bit 1, between November 15th 2016 and November 15th 2017 to activate BIP141, BIP143 and BIP147. ==Motivation== Cause the mandatory activation of the existing segwit deployment before the end of midnight November 15th 2017. ==Specification== All times are specified according to median past time. This BIP will be activate between midnight October 1st 2017 (epoch time 1538352000) and midnight November 15th 2017 (epoch time 1510704000) if the existing segwit deployment is not activated before epoch time 1538352000. This BIP will cease to be active when the existing segwit deployment activates. While this BIP is active, all blocks must set the nVersion header top 3 bits to 001 together with bit field (1<<1) (according to the existing segwit deployment). Blocks that do not signal as required will be rejected. === Reference implementation === <pre> // mandatory segwit activation between Oct 1st 2017 and Nov 15th 2017 inclusive if (pindex->GetMedianTimePast() >= 1538352000 && pindex->GetMedianTimePast() <= 1510704000 && !IsWitnessEnabled(pindex->pprev, chainparams.GetConsensus())) { if (!((pindex->nVersion & VERSIONBITS_TOP_MASK) == VERSIONBITS_TOP_BITS) && (pindex->nVersion & VersionBitsMask(params, Consensus::DEPLOYMENT_SEGWIT)) != 0) { return state.DoS(2, error("ConnectBlock(): relayed block must signal for segwit, please upgrade"), REJECT_INVALID, "bad-no-segwit"); } } </pre> ==Backwards Compatibility== This deployment is compatible with the existing "segwit" bit 1 deployment scheduled between midnight November 15th, 2016 and midnight November 15th, 2017. ==Rationale== Historically, the P2SH soft fork (BIP16) was activated using a predetermined flag day where nodes began enforcing the new rules. P2SH was successfully activated with relatively few issues By orphaning non-signalling blocks during the last month of the BIP9 bit 1 "segwit" deployment, this BIP can cause the existing "segwit" deployment to activate without needing to release a new deployment. ==References== [https://github.com/bitcoin/bitcoin/blob/v0.6.0/src/main.cpp#L1281-L1283 P2SH flag day activation]. ==Copyright== This document is placed in the public domain.




_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
Several times. It's been debated if unconfirmed transactions are 
necessary, methods of doing more private filtering have been suggested, 
along with simply not filtering unconfirmed transactions at all. My 
collected data suggests that there is very little use of BIP37 at 
present, based on incoming connections to nodes I know end up in the DNS 
seed responses (no "SPV" clients do their own peer management).


On 2017-06-19 12:58, Andreas Schildbach via bitcoin-dev wrote:

-------------------------------------
On Monday, 19 June 2017 17:49:59 CEST Jonas Schnelli wrote:

You seem to misunderstand the usecase.
If you send me a transaction, both of use are using our phones, then I need 
to be able to have immediate feedback on the transaction being broadcast on 
the network.
This is not about zero-conf, this is simple seeing what is happening while 
it is happening.

Additionally, when the transaction that is meant for my wallet is broadcast, 
I want my SPV wallet to parse and check the actual transaction.
It is not just to see that *something* was actually send, but also to be 
able to see how much is being paid to me. Maybe If the transaction is marked 
as RBF-able, etc.

Really basic usability: provide information to your users when you can, 
should they want to, and by default on.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
used.

I want OP_CAT so that I can securely and compactly verify many hashes and
hash preimages. This would shrink offchain Tumblebit transactions
significantly.

For instance if I want a transaction TxA which checks that a transaction
TxB releases preimages x1,x2,...,x10 such that
y1=H(x1), y2=H(x2),...,y10=H(x10). Currently I just put y1,...y10 and check
that the preimahes hash correctly. With OP_CAT I would only have to store
one hash in TxA, yhash

ytotal = H(OP_CAT(H(OP_CAT(y1, y2)),y3)...y10)

TxA could then just hash all the preimages supplied by TxB and confirm they
hash to TxA. This would reduce the size of TxA from approx 10*32B to
32+10*16B. I have a version which improves this further but it is more
complex.

Most of the math OP codes aren't particularly helpful due to their 32bit
nature and their strange overflow behavior.
-------------------------------------
Hi, Block compression brings some problems witch need to check and you can visit:
https://bitcointalk.org/index.php?topic=88208.0 and https://bitcointalk.org/index.php?topic=204283.0

________________________________________
: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org>  Jeff Johnson via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
ʱ: 20171127 10:11
ռ: bitcoin-dev@lists.linuxfoundation.org
: [bitcoin-dev] Block compression

I'm new to this mailing list and apologize if this has been suggested before. I was directed from the Bitcoin core github to this mailing list for suggestions.

I'd just like to post a possible solution that increases the amount of data in a block without actually increasing the size on disk or the size in memory or the size transmitted over the Internet. Simply applying various compression algorithms, I was able to achieve about a 50% compression ratio. Here are my findings on a recent Bitcoin block using max compression for all methods:

Raw block
998,198 bytes

Gzip
521,212 bytes (52% ratio)
(needs 2MB to decompress).

LZMA
415,308 bytes (41% ratio)
(1MB dictionary, needs 3MB to decompress)

- ZStandard: 469,179 bytes (47% ratio)
(1MB memory to decompress)

- LZ4: 641,063 bytes (64% ratio)
(32-64K to decompress)

The compression time on my modest laptop (2 years old) was "instant". I ran all from the command line and did not notice any lag as I pressed enter to do the compression, so easily less than a second. But compression time doesn't matter, decompression time is what matters as blocks will be decompressed billions of times more than they will be compressed. Decompression speed for LZ4 is the fastest of the above methods, at 3.3GB / second, slightly less than half the speed of memcpy, see char at (https://github.com/lz4/lz4).

If decompression speed, CPU and memory usage is a concern, LZ4 is a no brainer. You basically get a 33% larger block size for "free". But ZStandard, in my opinion, makes the most sense as it offers greater than 50% compression ratio with a very good decompression ratio of 900MB / second.

If this were implemented in the Bitcoin protocol, there would need to be a place to specify the compression type in a set of bits somewhere, so that future compression algorithms could potentially be added.

Miners could do nothing and keep sending blocks as is, and these blocks would have "no compression" as the type of compression, just as today. Or they could opt in to compress blocks and choose how many transactions they want to stuff into the block, keeping the compressed size under the limit.

The bitcoin client code would also need to be able to handle the appropriate compression bits, and limits of signature data, etc. modified to deal with the compression.

I understand schnorr signatures are on the roadmap as a 25% compression gain which is great, I suspect that schnorr signatures would compress even further when compressed with the above compression methods.

Here is a link to the block that I compressed: https://mega.nz/#!YPIF2KTa!4FxxLvqzjqIftrkhXwSC2h4G4Dolk8dLteNUolEtq98

Thanks for reading, best wishes to all.

-- Jeff Johnson
-------------------------------------
<pre>
BIP: ?
Title: Standard address format for timelocked funds
Author: ZmnSCPxj <ZmnSCPxj@protonmail.com>
Comments-Summary: ?
Comments-URI: ?
Status: ?
Type: ?
Created: 2017-07-01
License: CC0-1.0
</pre>
== Abstract ==
<code>OP_CHECKLOCKTIMEVERIFY</code> provides a method of
locking funds until a particular time arrives.
One potential use of this opcode is for a user to precommit
himself or herself to not spend funds until a particular
date, i.e. to hold the funds until a later date.
This proposal adds a format for specifying addresses that
precommit to timelocked funds, as well as specifying a
redemption code to redeem funds after the timelock has
passed.
This allows ordinary non-technical users to make use of
<code>OP_CHECKLOCKTIMEVERIFY</code> easily.
== Copyright ==
This BIP is released under CC0-1.0.
== Specification ==
This proposal provides formats for specifying an
address that locks funds until a specified date,
and a redemption code that allows the funds to be
swept on or after the specified date.
At minimum, wallet software supporting this BIP must
be capable of sweeping the redemption code on or after
the specified date.
In addition, the wallet software should support sending
funds to the timelocked address specified here.
Finally, wallet software may provide a command to create
a pair of timelocked address and redemption code.
Addresses and redemption codes are encoded using
[https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki#Bech32
Bech32 encoding].
=== Timelocked Address Format ===
The human-readable part of the address is composed of:
# The four characters <code>hodl</code>.
# A date, in <code>YYYYMMDD</code> form. For example,
the date August 1, 2017 is encoded as <code>20170801</code>.
# A network code, either <code>tb</code> for testnet,
or <code>bc</code> for Bitcoin mainnet.
The data part of the address is composed of:
# A version quintet (5 bits), which must be 0 for this
BIP.
# A public key hash, 32 quintets (160 bits). As is
usual for Bitcoin, this is big-endian.
This is to be interpreted as follows:
# The given date is the first day that the funds in
the given address may be redeemed.
# The funds are owned by whoever controls the private
key corresponding to the public key hash given.
=== Redemption Code ===
The human-readable part of the redemption code is
composed of:
# The four characters <code>hedl</code>.
# A date, in <code>YYYYMMDD</code> form.
# A network code, either <code>tb</code> for testnet,
or <code>bc</code> for Bitcoin mainnet.
The data part of the address is composed of:
# A version quintet (5 bits), which must be 0 for this
BIP.
# A private key, 52 quintets (260 bits). This is the
256-bit private key, prepended with 4 <code>0</code>
bits, in big-endian order. <!-- We could consider
some kind of mini private key instead if the security
is similar anyway. -->
This is to be interpreted as follows:
# The given date is the first day that the funds in
the given address may be redeemed.
# The private key unlocks the funds.
=== Lock Time Computation ===
Given a particular lock date <code>YYYYMMDD</code>, the
actual lock time is computed as follows:
# The day before the lock date is taken. For example,
if the lock date is <code>20180101</code> or
January 1, 2018, we take the date December 31, 2017.
# We take the time 1000h (10:00 AM, or 10 in the morning)
of the date from the above step.
This lock time is then translated to a
Unix epoch time, as per POSIX.1-2001 (which removes the
buggy day February 29, 2100 in previous POSIX revisions).
The translation should use, at minimum, unsigned 32-bit
numbers to represent the Unix epoch time.
The Unix epoch time shall then be interpreted as an
<code>nLockTime</code> value, as per standard Bitcoin.
Whether it is possible to represent dates past 2038
will depend on whether standard Bitcoin can represent
<code>nLockTime</code> values to represent dates past
2038.
Since <code>nLockTime</code> is an unsigned 32-bit
value, it should be possible to represent dates until
06:28:15 UTC+0 2106-02-07.
Future versions of Bitcoin should be able to support
<code>nLockTime</code> larger than unsigned 32-bit,
in order to allow even later dates.
The reason for using an earlier lock time than the
specified date is given in the Rationale section of
this BIP.
=== Payment to a Timelocked Address ===
An ordinary P2SH payment is used to provide funds to a
timelocked address.
The script below is used as the <code>redeemScript</code>
for the P2SH payment:
<timeout> OP_CHECKLOCKTIMEVERIFY OP_DROP
OP_DUP OP_HASH160 <publickeyhash> OP_EQUALVERIFY OP_CHECKSIG
Once the <code>redeemScript</code> is derived, the hash is
determined, and an ordinary P2SH output with the below
<code>scriptPubKey</code> used:
OP_HASH160 <redeemScripthash> OP_EQUAL
In case of SegWit deployment, SegWit-compatible wallets
should be able to use P2SH, P2WSH, or P2SH-P2WSH, as per
the output they would normally use in that situation.
Obviously, a timelocked address has an equivalent
Bitcoin <code>3</code> (P2SH) address.
A simple service or software that translates from a
public timelocked address to a P2SH address can be
created that makes timelocking (but not redemption)
backwards compatible with wallets that do not support
this BIP.
This proposal recommends that wallets supporting payment
to P2PKH, P2SH, P2WPKH, and P2WSH Bitcoin addresses should
reuse the same interface for paying to such addresses as
paying into timelocked addresses of this proposal.
=== Redemption of a Timelocked Redemption Code ===
To sweep a timelocked redemption code after the timelock,
one must provide the given <code>redeemScript</code> as
part of the <code>scriptSig</code>, of all unspent
outputs that pay to the given <code>redeemScript</code>
hash.
When sweeping a timelocked redemption code, first the
wallet must extract the private key from the redemption
code, then derive the public key, the public key hash,
the <code>redeemScript</code>, and finally the
<code>redeemScript</code> hash.
Then, the wallet must find all unspent outputs that pay
to the <code>redeemScript</code> hash via P2SH (and, in the
case of SegWit deployment, via P2SH-P2WSH and P2WSH).
For each such output, the wallet then generates a
transaction input with the below <code>scriptSig</code>, as
per usual P2SH redemptions:
<signature> <pubkey> <redeemScript>
The wallet then outputs to an address it can control.
As the Script involved uses <code>OP_CHECKLOCKTIMEVERIFY</code>,
the <code>nSequence</code> must be 0 and the
<code>nLockTime</code> must be equal to the computed
lock time.
This implies that the transaction cannot be transmitted
(and the funds cannot be sweeped)
until after the given lock time.
The above procedure is roughly identical to sweeping an
ordinary, exported private key.
This proposal recommends that wallets supporting a sweep
function should reuse the same interface for sweeping
individual private keys (wallet import format) for sweeping
timelocked redemption codes.
== Motivation ==
A key motivation for this BIP is to allow easy use of
<code>OP_CHECKLOCKTIMEVERIFY</code> by end-users.
The below are expected use cases of this proposal:
# A user wants to purchase an amount of Bitcoin,
and subsequently wait for an amount of time before
cashing out.
The user fears that he or she may have "weak hands",
i.e. sell unfavorably on a temporary dip, and thus
commits the coins into a timelocked fund that can
only be opened after a specific date.
# A user wants to gift an amount of Bitcoins to
an infant or minor, and wants the fund to not be spent
on ill-advised purchases until the infant or minor
reaches the age of maturity.
# A user may wish to prepare some kind of monthly subsidy
or allowance to another user, and prepares a series of
timelocked addresses, redeemable at some set date on
each month, and provides the private redemption codes to
the beneficiary.
# A user may fear duress or ransom for a particular
future time horizon, and voluntarily impose a lock time
during which a majority of their funds cannot be spent.
== Rationale ==
While in principle, this proposal may be implemented as a
separate service or software, we should consider the long
time horizons that may be desired by users.
A user using a particular software to timelock a fund may
have concerns, for example, of specifying a timelock
18 years in the future for a gift or inheritance to a
newborn infant.
The software or service may no longer exist after 18 years,
unless the user himself or herself takes over maintenance
of that software or service.
By having a single standard for timelocked funds that is
shared and common among multiple implementations of Bitcoin
wallets, the user has some assurance that the redemption code
for the funds is still useable after 18 years.
Further, a publicly-accessible standard specifying how the
funds can be redeemed will allow technically-capable users
or beneficiaries to create software that can redeem the
timelocked fund.
This proposal provides a timelock at the granularity of a
day.
The expectation is that users will have long time
durations of months or years, so that the ability to
specify exact times, which would require specifying the
timezone, is unneeded.
The actual timeout used is 1000h of the day before the
human-readable date, so that timezones of UTC+14 will
definitely be able to redeem the money starting at
0000h of the human-readable date, local time (UTC+14).
Given the expectation that users will use long time
durations, the fact that timezones of UTC-12 will
actually be able to redeem the funds on 2200h UTC-12
time two days before can be considered an acceptable
error.
The human-readable date is formatted according to
[https://www.iso.org/iso-8601-date-and-time-format.html
ISO standard dates], with the dashes removed.
Dashes may prevent double-click selection, making
usability of these addresses less desirable.
<!--
We can consider something like 2021m12d11 instead,
which would be much more readable and understandable
to human users.
-->
The <code>bc</code> or <code>tb</code> is after the
date since the date is composed of digits and the bech32
separator itself is the digit <code>1</code>. One
simply needs to compare <code>hedlbc202111211...</code>
and <code>hedl20211121bc1...</code>.
A version quintet is added in case of a future
sociopolitical event that changes interpretation of
dates, or changes in scripting that would allow for more
efficient redemptions of timelocked funds (which would
change the <code>redeemScript</code> paid to), or changes
in the size and/or format of lock times, and so on.
Such changes are unlikely, so the version is a quintet in
the bech32 data part rather than a substring in the
human-readable part.
The public address format uses the <code>hodl</code> as
the start of the code, while the private key (the
redemption code) uses <code>hedl</code>.
This provides a simple mnemonic for users:
"Pay into the <code>hodl</code> code to hold your
coins until the given date.
After you've held the coins (on or after the given date)
use the <code>hedl</code> code to redeem the coins."
The obvious misspelling of "hodl" is a homage to the common
meme within the Bitcoin community.
<!-- The above misspelling may be corrected if it is considered
to be in bad taste. -->
-------------------------------------
Some notes:

Hardforks like Bitcoin ABC without a malleability fix are very unlikely to
have payment channels, so the problem does not exist for those.

The designers of a hardfork which does have a malleability fix will
probably know about payment channels, so they can just build a replay
protection that allows the execution of old commitments. That needs some
kind of timestamping of commitments, which would have to be integrated in
the channel design. The easiest way would be to just write the time of
signing the commitment in the transaction and the replay protection accepts
old commitments, but rejects one's which were signed after the hardfork.
These timestamps can essentially be one bit (before or after a hardfork)
and if the replay protection in the hardfork only accepts old commitments
for something like a year, then it can be reused for more hardforks later
on. Maybe someone comes up with an interesting way of doing this without
using space.

Nevertheless hardforking while having channels open will always be a mess
as an open channel requires you to watch the blockchain. Anybody who is
just not aware of the hardfork or is updating his client a few days too
late, can get his money stolen by an old commitment transaction where he
forgets to retaliate on the new chain. As other's can likely figure out
your client version the risk of retaliation is not too big for an attacker.



2017-08-17 13:31 GMT+02:00 Bryan Bishop via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
2017-08-28 19:12 GMT+02:00 Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:


 ... and I guess the nonce can be arbitrarily truncated as well, just brute
force the missing bits :-P.



I think that my blog post on compact spv proofs can be helpful also. It
tries to make the pretty compact formulations in the sidechains paper a bit
more graspable by normal people.

http://popeller.io/index.php/2016/09/15/compact-spv-proofs/

Kalle
-------------------------------------
On Feb 27, 2017, at 8:02 AM, shaolinfry via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:


The relationship between a codebase and chain fork implementations is similar to vendor lock-in, and is being used in a similar manner.

There is nothing preventing a single codebase from implementing all forks and exposing the option to apply any non-conflicting combination of them.

While this has not been the norm libbitcoin now utilizes this approach. Currently the options to apply any activated Bitcoin forks are exposed via config. I personally am not working to implement non-activated forks at this point, but that's just prioritization.

Recently I objected to BIP90. This hard fork is presented as a code simplification and a performance optimization. I showed in the discussion that it was neither. Nevertheless we implemented this additional code and give the user the option to apply it or not. It's application produces no performance benefit, but it ensures that the choice of forks remains in the hands of the user.

e
-------------------------------------
A better solution is to just have the sending wallet check to see if the
address you are about to send to has been used before. If it's a fresh
address, it sends it through without any popup alert. If the address has
history going back a certain amount of time, then a popup comes up and
notifies the sender that they are sending to a non-fresh address that may
no longer be controlled by the receiver anymore.

Also, an even better idea is to set up an "address expiration service".
When you delete a wallet, you first send off an "expiration notice" which
is just a message (signed with the private key) saying "I am about to
delete this address, here is my new address". When someone tries to send to
that address, they first consult the address expiration service, and the
service will either tell them "this address is not expired, proceed", or
"this address has been expired, please send to this other address
instead...". Basically like a 301 redirect, but for addresses. I don't
think address expiration should be part of the protocol.

On Wed, Sep 27, 2017 at 10:06 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



-- 
Chris Priest
786-531-5938
-------------------------------------
Hello Bitcoin-Dev,

CVE-2017-9230 (1) (2), or commonly known as ‘ASICBOOST’ is a severe (3) (4) and actively exploited (5) security vulnerability.
 
To learn more about this vulnerability please read Jeremy Rubin’s detailed report:
http://www.mit.edu/~jlrubin//public/pdfs/Asicboost.pdf
 
Andreas Antonopoulos has an excellent presentation on why asicboost is dangerous:
https://www.youtube.com/watch?v=t6jJDD2Aj8k

In decisions on the #bitcoin-core-dev IRC channel; It was proposed, without negative feedback, that SegWit be used as a partial-mitigation of CVE-2017-9230.

SegWit partially mitigates asicboost with the common reasonable assumption that any block that doesn’t include a witness commit in it's coinbase transaction was mined using covert asicboost.  Making the use of covert asicboost far more conspicuous.

It was also proposed that this partial mitigation should be quickly strengthened via another soft-fork that makes the inclusion of witness commits mandatory, without negative feedback.

The security trade-offs of deploying a partial-mitigation to CVE-2017-9230 quickly vs more slowly but more conservatively is under intense debate.  The author of this post has a strong preference to the swiftest viable option.

Cameron.


(1) CVE Entry:
https://cve.mitre.org/cgi-bin/cvename.cgi?name=+CVE-2017-9230

(2) Announcement of CVE to Mailing List:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014416.html

(3) Discussion of the perverse incentives created by 'ASICBOOST' by Ryan Grant:
 https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014352.html

(4) Discussion of ASICBOOST's non-independent PoW calculation by Tier Nolan:
 https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014351.html

(5) Evidence of Active Exploit by Gregory Maxwell:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013996.html


-------------------------------------
Hi Greg,

The safest way to ensure everyone's protection to make sure *no one can do
anything*. Then we will ALL be safe ;).


Ok, let's calm down.

give out if pressed, is that a good idea? Can I justify pushing for such a
"feature" just because it's "opt-in"?

It would be more like "should we allow a car on the road if we know
statistically that our brakes give out in every 1/100,000,000 cars"? There
is security risks with everything in life -- we need to quantify the risk
to see if it is worth taking. I think Paul has been pretty upfront about
the risks of his model. I think you did a good job of demonstrating it in
the email I cited too.


By your account bitcoin is already insecure then -- it allows anyone can
spend outputs that can be claimed by miners.


I look forward to this!

-Chris

On Wed, Jul 12, 2017 at 2:24 PM, Tao Effect <contact@taoeffect.com> wrote:

-------------------------------------
On Wed, Jun 14, 2017 at 12:04 PM, Zheming Lin <heater@gmail.com> wrote:

network-wide consensus changes. Even if someone spooled up 100 times more
nodes than currently exist and they all "signal" for some consensus rule
change, that doesn't compel the rest of the "genuine" nodes to change the
rules they enforce.

The users always have a choice with regard to what consensus rules to
enforce and what software to run. Everyone is welcome to propose changes
and write software that they make available to users.

should be a non-breaking change.

-------------------------------------
What you're describing is a hashpower activated soft fork to censor transactions, in response to a user activated soft fork that the majority of hashpower disagrees with.

It is always possible for a majority of hashpower to censor transactions they disagree with. Users may view that as an attack, and can always respond with a POW hard fork. 

Bitcoin only works if the majority of hashpower is not hostile to the users.


On 6 March 2017 9:29:35 PM AEDT, Edmund Edgar via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:


-------------------------------------
Hi,

I am trying to follow this Simplicity proposal and I am seeing all over references to ‘jets’, but I haven’t been able to find any good reference to it.
Can anyone give me a brief explanation and or a link pointing to this feature?
Thanks


-------------------------------------
First, there’s been no discussion so far for address expiration to be part of “the protocol” which usually means consensus rules or p2p. This is purely about wallets and wallet information exchange protocols.

There’s no way for the sender to know whether an address has been used without a complete copy of the block chain and more indexes than even Bitcoin Core maintains. It’s simply not an option now, let alone as the blockchain grows into the future.


-------------------------------------


…so far. I wonder how long that vacation will last?


...but we can be sure that it will be, since the dollar value held in existing utxos continues to increase...


Does that offer any greater protection? That’s not so clear to me as the outputs (at least for p2pkh) only verify the public key against the final 20 byte hash. Specifically, in the first (notional) case the challenge would be to find a private key that has a public key that hashes to the final hash. In the second (realistic) case, you merely need to add the sha256 hash into the problem, which doesn’t seem to me to increase the difficulty by any significant amount? 


/s
-------------------------------------

Let's move forward with the simplest solution that solves the problem and
achieves consensus! Version bytes {x,y,z} fits the bill.

On Wed, Sep 6, 2017 at 4:26 AM, Thomas Voegtlin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
-Kabuto

PGP Fingerprint: 1A83 4A96 EDE7 E286 2C5A  B065 320F B934 A79B 6A99
-------------------------------------
Hi Erik,

I have left you some comments below.

Some general questions:
How will you deal with excessive sighashing (i.e. massive transactions
that include a lot of signature verification)?
Presumably the sigops limit will increase proportionally?

On 3/8/2017 2:42 PM, Erik Aronesty via bitcoin-dev wrote:
What does EB stand for?

What is the point of users publishing an EB? Is it for miners to
determine what to set theirs to? If so, what about sybil attacks with
fake nodes publishing EBs?

How do users publish an EB? Do they use a transaction? Or is it
something that goes into the User Agent?

How high can the EB go? What is its maximum?
So anyone who does not change their EB are forked off of the network? If
the EB is an "advanced feature", then most users are going to be leaving
it at the default shipped with the software. That means that they will
then be forked off of the network when they don't change the EB because
it is an "advanced feature" that is more difficult to access.
I think this would require a soft fork beforehand in order to implement
such a system.
Why?
"Scaling" includes a lot more than just the block size. There is much
more to scaling than just increasing the block size.
What if the EB of a new node is set to be smaller than the current block
size?

-------------------------------------
On 7/11/2017 7:12 PM, Tao Effect wrote:

In that case you should clarify, which is exactly what you are doing now.


If that were the case, then DC would need to be a hard fork. It so
happens that it is *not* the case -- if users choose to deposit to an
anyone-can-spend output, then miners can take the Bitcoins, which *is*
something that miners can do currently.


I am not sure what you are disagreeing with. The three thefts involve
different attacker resources and behaviors, and in that way they are
different. But in all three cases the user has lost the BTC they would
have otherwise owned, and in that way they are not different.

And users are under no obligation to use DC, just as they are under no
obligation to open a LN channel (or use P2SH, etc).

Other than your complaint about being freely given the option to upgrade
to software which will give you the option to freely opt-in to a
different security model that you can otherwise ignore, feel free to
bring up any other issues you feel need to be addressed.

That is false. I do deny it. Per the logic of the soft fork, the
security properties are at best unchanged (as I think almost everyone
else on this list would acknowledge). And even for those users who
opt-in, I feel the risk of theft is low, as I explain in the very
passage you've quoted above.

And please try to avoid going off-topic -- this is supposed to be about
the idea of a new roadmap.

Paul
-------------------------------------
As a user, I would far prefer a split over any kind of mandatory change
that would drastically harm the ecosystem.  Many users feel the same way.
Level 3 is a pure attack on users who do not conform to your beliefs.
Please do not put words in people's mouths claiming they wouldn't prefer a
split when many would.  If you wish to fork off, please do so responsibly.

-Alphonse

On Sun, Mar 26, 2017 at 2:05 PM, Peter R via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hi Tom


This thread is not about BIP150/151.
The hash includes the encryption session which makes it impossible to distinct identities.


Not true. The hash includes the encryption session which is based on a ephemeral ECDH/HKDF per connection-session.

Have you read the BIP?

</jonas>

-------------------------------------
Charlie, I'll be mostly in the tech track, of course. And I've already
planned to meet RSK guys after their event tomorrow.

Ryan, the more review the better. We aren't in any direct rush, other than
the natural desire to have cool things as early as possible.

Peter, responses below:

On May 22, 2017 9:33 AM, "Peter Todd" <pete@petertodd.org> wrote:

On Mon, May 22, 2017 at 02:17:07AM -0400, Paul Sztorc via bitcoin-dev wrote:

Thanks for the credit, although I think the security properties of what
you're
proposing are very different - and much weaker - than what I proposed in
Zookeyv.


As you state in [2] "if miners never validate sidechains at all, whoever
bids
the most for the chain (on a continuous basis), can spam a 3-month long
stream
of invalid headers, and then withdraw all of the coins deposited to the
sidechain." and "Since the mining is blind, and the sidechain-withdrawal
security-level is SPV, miners who remain blind forever have no way of
telling
who “should” really get the funds."

Finally, you suggest that in this event, miners *do* have to upgrade to a
full
node, an expensive and time-consuming operation (and one that may be
impossible
for some miners if necessary data isn't available).


Surprisingly, this requirement (or, more precisely, this incentive) does
not effect miners relative to each other. The incentive to upgrade is only
for the purpose of preventing a "theft" -- defined as: an improper
withdrawal from a sidechain. It is not about miner revenues or the ability
to mine generally (or conduct BMM specifically). The costs of such a theft
(decrease in market price, decrease in future transaction fee levels) would
be shared collectively by all future miners. Therefore, it would have no
effect on miners relative to each other.

Moreover, miners have other recourse if they are unable to run the node.
They can adopt a policy of simply rejecting ("downvoting") any withdrawals
that they don't understand. This would pause the withdraw process until
enough miners understand enough of what is going on to proceed with it.

Finally, the point in dispute is a single, infrequent, true/false question.
So miners may resort to semi-trusted methods to supplement their decision.
In other words, they can just ask people they trust, if the withdrawal is
correct or not. It is up to users to decide if they are comfortable with
these risks, if/when they decide to deposit to a sidechain.


It's unclear to me what the incentive is for miners to do any of this. Could
you explain in more detail what that incentive is?


It is a matter of comparing the costs and benefits. Ignoring theft, the
costs are near-zero, and the benefits are >0. Specifically, they are: a
higher BTC price and greater transaction fees. Theft is discouraged by
attempting to tie a theft to a loss of confidence in the miners, as
described in the spec/website.
In general the incentives are very similar to those of Bitcoin itself.

Paul




--
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
To expand on this below;

Den 18 apr. 2017 00:34 skrev "Natanael" <natanael.l@gmail.com>:

IMHO the best option if we change PoW is an algorithm that's moderately
processing heavy (we still need reasonably fast verification) and which
resists partial state reuse (not fast or fully "linear" in processing like
SHA256) just for the sake of invalidating asicboost style attacks, and it
should also have an existing reference implementation for hardware that's
provably close in performance to the theoretical ideal implementation of
the algorithm (in other words, one where we know there's no hidden
optimizations).

[...] The competition would mostly be about packing similar gate designs
closely and energy efficiency. (Now that I think about it, the proof MAY
have to consider energy use too, as a larger and slower but more efficient
chip still is competitive in mining...)


What matters for miners in terms of cost is primarily (correctly computed)
hashes per joule (watt-seconds). The most direct proxy for this in terms of
algorithm execution is the number of transistor (gate) activations per
computed hash (PoW unit).

To prove that an implementation is near optimal, you would show there's a
minimum number of necessary transistor activations per computed hash, and
that your implementation is within a reasonable range of that number.

We also need to show that for a practical implementation you can't reuse
much internal state (easiest way is "whitening" the block header,
pre-hashing or having a slow hash with an initial whitening step of its
own). This is to kill any ASICBOOST type optimization. Performance should
be constant, not linear relative to input size.

The PoW step should always be the most expensive part of creating a
complete block candidate! Otherwise it loses part of its meaning. It should
however still also be reasonably easy to verify.

Given that there's already PoW ASIC optimizations since years back that use
deliberately lossy hash computations just because those circuits can run
faster (X% of hashes are computed wrong, but you get Y% more computed
hashes in return which exceeds the error rate), any proof of an
implementation being near optimal (for mining) must also consider the
possibility of implementations of a design that deliberately allows errors
just to reduce the total count of transistor activations per N amount of
computed hashes. Yes, that means the reference implementation is allowed to
be lossy.

So for a reasonably large N (number of computed hashes, to take batch
processing into consideration), the proof would show that there's a
specific ratio for a minimum number of average gate activations per
correctly computed hash, a smallest ratio = X number of gate activations /
(N * success rate) across all possible implementations of the algorithm.
And you'd show your implementation is close to that ratio.

It would also have to consider a reasonable range of time-memory tradeoffs
including the potential of precomputation. Hopefully we could implement an
algorithm that effectively makes such precomputation meaningless by making
the potential gain insignificant for any reasonable ASIC chip size and
amount of precomputation resources.

A summary of important mining PoW algorithm properties;

* Constant verification speed, reasonably fast even on slow hardware

* As explained above, still slow / expensive enough to dominate the costs
of block candidate creation

* Difficulty must be easy to adjust (no problem for simple hash-style
algorithms like today)

* Cryptographic strength, something like preimage resistance (the algorithm
can't allow forcing a particular output, the chance must not be better than
random within any achievable computational bounds)

* As explained above, no hidden shortcuts. Everybody has equal knowledge.

* Predictable and close to constant PoW computation performance, and not
linear in performance relative to input size the way SHA256 is (lossy
implementations will always make it not-quite-constant)

* As explained above, no significant reusable state or other reusable work
(killing ASICBOOST)

* As explained above, no meaningful precomputation possible. No unfair
headstarts.

* Should only rely on just transistors for implementation, shouldn't rely
on memory or other components due to unknowable future engineering results
and changes in cost

* Reasonably compact implementation, measured in memory use, CPU load and
similar metrics

* Reasonably small inputs and outputs (in line with regular hashes)

* All mining PoW should be "embarrassingly parallel" (highly
parallellizable) with minimal or no gain from batch computation,
performance scaling should be linear with increased chip size & cycle
speed.

What else is there? Did I miss anything important?
-------------------------------------
You might be interested in this proposal, which is very similar. The repo
contains a very basic implementation in typescript:
https://github.com/bitauth/bitauth2017/blob/master/bips/0-bitauth.mediawiki

https://github.com/bitauth/bitauth2017/

On Tue, Dec 19, 2017 at 4:59 PM Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
It is when you're talking about making a choice and 6.3x more people prefer
something else. Doing nothing is a choice as well.

Put another way, if 10% supported increasing the 21M coin cap and 63% were
against, would you seriously consider doing it?

On Feb 8, 2017 9:57 AM, "alp alp" <alp.bitcoin@gmail.com> wrote:

-------------------------------------
Some responses..

I think that limiting the maximum transaction size may not be the best
possible solution to the N^2 hashing problem, yet it is not a bad start.

There are several viable soft-forking solutions to it:

1- Soft-fork to perform periodic reductions in the maximum non-segwit
checksigs per input (down to 20)
2- Soft-fork to perform periodic reductions in the number of non-segwit
checksigs per transaction. (down to 5K)
3- Soft-fork to perform periodic reductions in the amount of data hashed by
non-segwit checksigs.

Regardless which one one picks, the soft-fork can be deployed with enough
time in advance to reduce the exposure. The risk is still low. Four years
have passed since I reported this vulnerability and yet nobody has
exploited it. The attack is highly anti-economical, yet every discussion
about the block size ends up citing this vulnerability.

,


I will mention this worst case in the BIP.

Even if artificially filling the witness space up to 8 MB is
anti-economical, Segwit exacerbates this problem because each witness byte
costs 1/4th of a non-witness byte, so the block bloat attack gets cheaper
than before. I think the guilt lies more in Segwit discount factor than in
the plain block size increase.
I would remove the discount factor altogether, and add a fixed (40 bytes)
discount for each input with respect to outputs (not for certain input
types), to incentivize the cleaning of the UTXO set. A discount for inputs
cannot be used to bloat an unlimited number of blocks, because for each
input the attacker needs to first create an output (without discount).
There is no need to incentivize removing the signatures from blocks,
because there is already an incentive to do so to save disk space.


be redefined, even if it's not a consensus change.


-------------------------------------
As mining works now, miners have to verify all Bitcoin transactions in the
blocks they mine, because they would otherwise risk producing an invalid
block. This is problematic because many miners are Chinese, and thus have
poor Internet connectivity, so it would be preferable to separate the task
of creating valid proof-of-work from the task of collecting valid
transactions.

This could be made possible by adding an opcode that checks whether the
top-most stack item is a valid block header, we could call it
OP_VALID_HEADER(VERIFY), thus allowing miners to be paid for a valid block
header through a regular Bitcoin transaction, rather than through the
coinbase transaction only. This allows a different group to simply act as
collectors of transactions, and create OP_VALID_HEADER-transactions that
pay to block headers with a merkle root that includes all the highest-fee
transactions.

So, these collectors would accumulate as many connections as possible
within the Bitcoin P2P network, and collect all the highest fee
transactions they can find. Then construct a block which includes all these
transactions, and a coinbase tx that pays the block reward plus fees to the
collector.

With this block the collector would then create a Bitcoin transaction, with
a OP_VALID_HEADER-output that can be redeemed by supplying the block header
in the script but with a modified nonce/timestamp such that the
proof-of-work+timestamp is valid. Miners would then only have to look for
these Bitcoin transactions from the collectors, and mine on whichever
header pays them the most, without having to care about whether the block
in question includes invalid transactions, because the miner is paid for
just a valid proof-of-work hash. When the miner finds a solution, it
publishes the transaction, the collector see this transaction, gets it
valid header, and publishes the block.

A side bonus of this is that botnet miners can now participate on basically
equal footing with traditional miners: they just listen to the P2P network
for the transaction from the collector who pays them the most, which will
include as many transactions as possible to earn the most in fees, thus
verifying transactions without having to do the work.




      /Rune
-------------------------------------
Den 30 mars 2017 11:34 skrev "Natanael" <natanael.l@gmail.com>:

Block size dependent difficulty scaling. Hardfork required.

Larger blocks means greater difficulty - but it doesn't scale linearly,
rather a little less than linearly. That means miners can take a penalty in
difficulty to claim a greater number of high fee transactions in the same
amount of time (effectively increasing "block size bitrate"), increasing
their profits. When such profitable fees aren't available, they have to
reduce block size.

In other words, the users literally pay miners to increase block size (or
don't pay, which reduces it).


This can be simplified if we do get a fee pool (less hardfork code, more
softfork code), except that the effect will be partially reduced by the
mining subsidy until it approximately reaches parity with average total
fees.

We don't need to alter difficulty calculation.
Instead we alter the percentage of the fees that the miner gets to claim VS
what he have to donate to the pool based on the size of the block he
generated.
Larger block = smaller percentage of fees. This is another way to pay for
blocksize. The effect of this is that on average, miners that generate
smaller blocks takes a share of what otherwise would be part of the mining
profits of those generating larger blocks.

We would need to keep pieces of the section from above on expected
blocksize calculation. Because the closer you are to the expected
blocksize, the more you keep. And thus we need to adjust it according to
usage.
-------------------------------------
Gmaxwell I think what's new is that in this case, with a single tx you
would take out all txs with fee below 1 btc. With current rules, you would
only remove enoguh txs for that one to fit, not empty the whole block and
mine only a block with that single tx.

On 30 Sep 2017 5:53 am, "Jorge Timón" <jtimon@jtimon.cc> wrote:

-------------------------------------
On Wednesday 20 September 2017 5:13:04 AM Johnson Lau wrote:

This seems like a problem for signature aggregation to address, not a problem 
for OP_RETURNTRUE... In any case, I don't think it's insurmountable. Signature 
aggregation can simply be setup upfront, and have the Script verify inclusion 
of keys in the aggregation?


This is another approach, and one that seems like a good idea in general. I'm 
not sure it actually needs to take more witness space - in theory, such stack 
items could be implied if the Script engine is designed for it upfront. Then 
it would behave as if it were non-verify, while retaining backward 
compatibility.

Luke

-------------------------------------


If I understand the proposal correctly, you can always spend coins; it's the next transaction that is replay protected.

I like the idea of specifying the fork in bech32 [0]. On the other hand, the standard already has a human readable part. Perhaps the human readable part can be used as the fork id?

Note that in your currently proposal nForkId is only in the transaction signature pre-image. It's not in the serialized transaction, so a node would just have to try to see if the signature is valid. I don't know if that's a problem.

Can you clarify what you mean with:

What's the purpose of nForkId 1?


Can you give an example of where this opt-out would be useful? Why wouldn't it be enough to just sign one transaction for each fork?

In Spoonnet, the version number is added to the SIGHASH_TYPE in the pre-image. Your solution of just adding another field seems easier, but maybe there's a downside?

Sjors

[0] https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki#Bech32
-------------------------------------
On Tuesday, March 28, 2017 5:34:23 PM Johnson Lau via bitcoin-dev wrote:

Indeed, actually implementing hfprep proved to be overly complicated.

I like the idea of a time bomb that just shuts down the client after it 
determine it's stale and refuses to start without an explicit override.
That should work no matter what the hardfork is, and gives us a good 
expectation for hardfork timeframes.


I don't like this idea. It leaves the node open to attack from blocks actually 
meeting the criteria. Maybe the absolute minimum as Jeremy suggested.

Luke

-------------------------------------
On Tue, Feb 21, 2017 at 02:00:23PM -0800, Bram Cohen via bitcoin-dev wrote:

Note that this is a use-case specific concept of an idea I'm calling a
"generalized commitment"

A commitment scheme needs only have the property that it's not feasible to find
two messages m1 and m2 that map to the same commitment; it is *not* required
that it be difficult to find m given the commitment. Equally, it's not required
that commitments always be the same size.

So a perfectly reasonable thing to do is design your scheme such that the
commitment to short messages is the message itself! This adds just a single bit
of data to the minimum serialized size(1) of the commitment, and in situations
where sub-digest-sized messages are common, may overall be a savings.


Another advantage is that the scheme becomes more user-friendly: you *want*
programmers to notice when a commitment is not effectively hiding the message!
If you need message privacy, you should implement an explicit nonce, rather
than relying on the data to not be brute-forcable.


1) The more I look at these systems, the more I'm inclined to consider
bit-granularity serialization schemes... Heck, sub-bit granularity has
advantages too in some cases, e.g. by making all possible inputs to the
deserializer be valid.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
On 04/05/2017 08:23 PM, David Vorick via bitcoin-dev wrote:

The proposed BIP only removes covert ASICBOOST. As long as the ASICs can
also do the non-covert ASICBOOST, it shouldn't have any impact on miner
revenue.

-------------------------------------
Hi, I am interested in the possibility of a cryptocurrency software
(future bitcoin or a future altcoin) that strives to have immutable
consensus rules.

The goal of such a cryptocurrency would not be to have the latest and
greatest tech, but rather to be a long-term store of value and to offer
investors great certainty and predictability... something that markets
tend to like.  And of course, zero consensus rule changes also means
less chance of new bugs and attack surface remains the same, which is
good for security.

Of course, hard-forks are always possible.  But that is a clear split
and something that people must opt into.  Each party has to make a
choice, and inertia is on the side of the status quo.  Whereas
soft-forks sort of drag people along with them, even those who oppose
the changes and never upgrade.  In my view, that is problematic,
especially for a coin with permanent consensus rule immutability as a
goal/ethic.

As I understand it, bitcoin soft-forks always rely on anyone-can-spend
transactions.  If those were removed, would it effectively prevent
soft-forks, or are there other possible mechanisms?  How important are
any-one-can spend tx for other uses?

More generally, do you think it is possible to programmatically
avoid/ban soft-forks, and if so, how would you go about it?






-------------------------------------
On May 22, 2017 3:13 PM, "Tier Nolan via bitcoin-dev" <bitcoin-dev@lists.
linuxfoundation.org> wrote:

On Mon, May 22, 2017 at 5:19 PM, Paul Sztorc via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I don't think they are the same.

With Bitcoin, you only get to reverse recent transactions.  If you actually
reversed 2-3 weeks of transactions, then the Bitcoin price would fall,
destroying the value of the additional coins you managed to obtain.  Even
if their was no price fall, you can only get a fraction of the total.


I would replace "Bitcoins you manage to steal" with "Bitcoins you manage to
double-spend". Then, it still seems the same to me.


With BMM, you can "buy" the entire reserve of the sidechain by paying
(timeout) * (average tx fees).  If you destroy a side-chain's value, then
that doesn't affect the value of the bitcoins you manage to steal.


It may destroy great value if it shakes confidence in the sidechain
infrastructure. Thus, the value of the stolen BTC may decrease, in addition
to the lost future tx fee revenues of the attacked chain.

http://www.truthcoin.info/blog/drivechain/#drivechains-security

In my view, sidechains should only exist at all if they positively impact
Bitcoin's value. It is therefore desirable for miners to steal from chains
that provide no value-add.




The big difference is that Bitcoin holds no assets on another chain.  A
side-chain's value is directly linked to the fact that it has 100% reserves
on the Bitcoin main chain.  That can be targeted for theft.


Again, I don't really think it is that different. One could interchange
"recent txns" (those which could be double-spent within 2-3 weeks) with
"sidechain deposit tnxs".
-------------------------------------
Shouldn't there be a FAQ about this? All the blocksize increase proposals
going back to the Bitcoin Classic have the same problems and having
repeated proposals which move the details around a bit doesn't add anything
to the discussion.
-------------------------------------
configuration then I think you would see far more users running a pruned
node.

Default configurations aren't a big enough deal to factor into the critical
discussion of node costs versus transaction fee cost.  Default
configurations can be changed, and if nodes are negatively affected by a
default configuration, there will be an abundance of information about how
to correct that effect by turning on pruning.  Bitcoin can't design with
the assumption that people can't google - If we wanted to cater to that
population group right now, we'd need 100x the blocksize at least.


This is already a big problem from the measurements I've been looking at.
There are alternatives that need to be considered there as well.  If we
limit ourselves to not changing the syncing process for most users, the
blocksize limit debate changes drastically.  Hard drive costs, CPU costs,
propagation times... none of those things matter because the cost of sync
bandwidth is so incredibly high even now ($130ish per month, see other
email).  Even if we didn't increase the blocksize any more than segwit,
we're already seeing sync costs being shifted onto fewer nodes - I.e., Luke
Jr's scan finding ~50k nodes online but only 7k of those show up on sites
like bitnodes.21.co.  Segwit will shift it further until the few nodes
providing sync limit speeds and/or max out on connections, providing no
fully-sync'd nodes for a new node to connect to. Then wallet providers /
node software will offer a solution - A bundled utxo checkpoint that
removes the need to sync.  This slightly increases centralization, and
increases centralization more if core were to adopt the same approach.

The advantage would be tremendous for such a simple solution - Node costs
would drop by a full order of magnitude for full nodes even today, more
when archival nodes are more restricted, history is bigger, and segwit
blocksizes are in effect, and then blocksizes could be safely increased by
nearly the same order of magnitude, increasing the utility of bitcoin and
the number of people that can effectively use it.

Another, much more complicated option is for the node sync process to
function like a tor network.  A very small number of seed nodes could send
data on to only other nodes with the highest bandwidth available(and good
retention policy, i.e. not tightly pruning as they sync), who then spread
it out further and so on.  That's complicated though, because as far as I
know the syncing process today has no ability to exchange a selfish syncing
node for a high performing syncing node.  I'm not even sure - will a
syncing node opt to sync from a different node that, itself, isn't fully
sync'd but is farther ahead?

At any rate, syncing bandwidth usage is a critical problem for future
growth and is solvable.  The upsides of fixing it are huge, though.

On Wed, Mar 29, 2017 at 9:25 AM, David Vorick via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Mar 29, 2017 9:50 AM, "Martin Lízner via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Im tending to believe, that HF is necessary evil now.


I will firmly disagree. We know how to do a soft-fork blocksize increase.
If it is decided that a block size increase is justified, we can do it with
extension blocks in a way that achieves full backwards compatibility for
all nodes.

Barring a significant security motivation, there is no need to hardfork.

I am also solidly unconvinced that increasing the blocksize today is a good
move, even as little as SegWit does. It's too expensive for a home user to
run a full node, and user-run full nodes are what provide the strongest
defence against political manuveuring.

When considering what block size is acceptable, the impact of running
bitcoin in the background on affordable, non-dedicated home-hardware should
be a top consideration.

Disk space I believe is the most significant problem today, with RAM being
the second most significant problem, and finally bandwidth consumption as
the third most important consideration. I believe that v0.14 is already too
expensive on all three fronts, and that block size increases shouldn't be
considered at all until the requirements are reduced (or until consumer
hardware is better, but I believe we are talking 3-7 years of waiting if we
pick that option).
-------------------------------------
I don't want to be rude and I will refer to your expertise, but segwit does
have a 'time out' as defined in BIP 9 with the date of November 15th? Does
core plan on just releasing another BIP with a new timeout hoping it will
eventually get 95% census?

As for the other point, we can play semantics but that's boring, I guess my
meaning was every census change has gone through a core defined process
(not counting the changes that occurred before there were BIPs and such),
isn't that the case? If the currently discussed UASF goes through it would
seem like the first time census occurred outside core's mailing list of
pull requests, acks, and merge to master, I only note it as a thing of
interest.

To be clear, the fast and reckless part for you is the mechanism by which
segwit could possibly be made active? Do you envision a means of segwit
being made consensus that does not have 95% mining support?

I appreciate your time and expertise, and to not take up anymore, back to
lurking i go.


On Fri, Apr 14, 2017 at 11:29 PM, Gregory Maxwell <greg@xiph.org> wrote:




-- 
Steven Pine
(510) 517-7075
-------------------------------------
The discussion is going offtopic. Can we please take vague discussions
about changing pow, so called "asic resistance", the environment etc
to bitcoin-disscuss or some other forum?

-------------------------------------
Good morning,

This is called "UTXO Set Commitments".

Pieter Wuille I think had concrete proposals for the cryptographic primitive to use. Try searching "Rolling UTXO Set Commitments".

Regards,
ZmnSCPxj

Sent with [ProtonMail](https://protonmail.com) Secure Email.

-------------------------------------
Good morning ZmnSCPxj, it must be where you are,


I suppose that we are each missing each other's point some.


I understand that nodes would not be expected to agree on the transaction pool and do not propose validating that the correct transactions are included in a block. I speak of probability and likelihood of a transaction being included in a block, implying a random element. I do not propose rejecting blocks on the basis that the next block size is stated too large or too small for the transaction pool, only that the block received conforms to the next block size given on the previous block. Yes, it could be cheated. Also, various nodes may have at times wildly different amounts of transactions waiting in the transaction pool compared to each other and there could be a great disparity between them. It would not be possible in any case I can think of to validate the next block size is correct for the current transaction pool. Even as it is now, nodes may include transactions in a block that no other nodes have even heard of, nodes have no way to validate that either. If the block is built on sufficiently, it is the blockchain.


I will post back the revised proposal to the list. I have fleshed parts of it out more, given more explanation and, tried this time not to recycle terminology.


Regards,

Damian Williamson

________________________________
From: ZmnSCPxj <ZmnSCPxj@protonmail.com>
Sent: Thursday, 7 December 2017 5:46:08 PM
To: Damian Williamson
Cc: bitcoin-dev@lists.linuxfoundation.org
Subject: Re: [bitcoin-dev] BIP Proposal: UTWFOTIB - Use Transaction Weight For Ordering Transactions In Blocks

Good morning Damian,


Each long-running node would have a view that is roughly the same as the view of every other long-running node.

However, suppose a node, Sleeping Beauty, was temporarily stopped for a day (for various reasons) then is started again.  That node cannot verify what the "consensus" transaction pool was during the time it was stopped -- it has no view of that.  It can only trust that the longest chain is valid -- but that means it is SPV for this particular rule.


It would not. Suppose Sleeping Beauty slept at block height 500,000.  On awakening, some node provides some purported block at height 500,001.  This block indicates some "next blocksize" for the block at height 500,002.  How does Sleeping Beauty know that the transaction pool at block 500,001 was of the correct size to provide the given "next blocksize"?  The only way, would be to look if there is some other chain with greater height that includes or does not include that block: that is, SPV confirmation.

How does Sleeping Beauty know it has caught up, and that its transaction pool is similar to that of its neighbors (who might be lying to it, for that matter), and that it should therefore stop using SPV confirmation and switch to strict fullnode rejection of blocks that indicate a "next blocksize" that is too large or too small according to your equation?  OR will it simply follow the longest chain always, in which case, it trusts miners to be honest about how loaded (or unloaded) the transaction pool is?

-------

As a general rule, consensus rules should restrict themselves to:

1.  The characteristics of the block.
2.  The characteristics of the transactions within the block.

The transaction pool is specifically those transaction that are NOT in any block, and thus, are not safe to depend on for any consensus rules.

Regards,
ZmnSCPxj

-------------------------------------
Oh nevermind. I had a look at the history but missed that commit and
assumed the change was introduced when adding the text to
contrib/debian/copyright

Tim

On Wed, 2017-09-27 at 22:21 +0000, Gregory Maxwell wrote:

-------------------------------------
Maybe I missed or did not receive some messages, where was your
centralization concern addressed in the discussion?


Le 26/09/2017 à 03:33, Patrick Sharp via bitcoin-dev a écrit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
Thank you,



The benches are running in Google Cloud Engine; currently on 8 vCPU
32gb, but I tend to switch hardware regularly.


Roughly, the results are better for Bitcrust with high end hardware and
the difference for total block validations is mostly diminished at 2
vCPU, 7,5 gb.


Note that the spend-tree optimization primarily aims to improve peak
load order validation; when a block with pre-synced transactions comes
in, but this is tricky to accurately bench with Core using this simple
method of comparison by logs.


I will upgrade to, and show the results against 0.14 in the next weeks.


Best,

Tomas





On Fri, Apr 7, 2017, at 16:14, Greg Sanders wrote:






















































-------------------------------------
On Fri, Apr 14, 2017 at 8:34 PM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Please stop abusing participants on this list. Your activity is
actively driving people off this list.

James Hilliard should be commended for correcting your misinformation.


Anyone can modify their software to produce invalid blocks at any
time. If they want to be stupid, they can be stupid.

The fact remains that miners who haven't gone and wreaked their
software internals will not mine segwit incompatible blocks. Right now
_no_ observable has broken node in this way.

-------------------------------------
Thank You Christian for your response.

https://bitcointalk.org/index.php?topic=473.0 :  I dont see the relevance.
https://bitcointalk.org/index.php?topic=52859.0 : This idea does not seem
to talking about trimming the full node. Trimming the full node is the key,
the full node is what keeps us secure from hackers. If it can be trimmed
without losing security, that would be good, that is what I am proposing.
https://bitcointalk.org/index.php?topic=12376.0 : Same answer as 505.0.
https://bitcointalk.org/index.php?topic=74559.15 : I think his proposal is
similar to mine, unfortunately for us his predictions were way off. He was
trying to fix this problem while believing that in the year 2020 the
blockchain would be 4GB!!! It is not his fault, his prediction was in 2011.
But you can see, by his prediction, which was rational at the time, was way
off. And it stresses my point, we need to fix this now. Too bad, no one
took him seriously back then, when the block chain i was 1GB.
*https://bitcointalk.org/index.php?topic=56226.0
<https://bitcointalk.org/index.php?topic=56226.0>*: Another guy with a
valid point, who was first acknowledged and then apparently ignored.
.
To summarize, this problem was brought up about 6 years ago, when the
blockchain was 1GB in size, Now it is about 140GB in size. I think it is
about time we stop ignoring this problem, and realize something needs to
change, or else the only full-nodes you will have will be with private
multi-million dollar companies, because no private citizen will have the
storage space to keep it. That would make bitcoin the worst decentralized
or uncentralized system in history.


On 27 August 2017 at 00:42, Christian Riley <criley@gmail.com> wrote:

-------------------------------------
TL;DR I'll be updating the fast Merkle-tree spec to use a different
      IV, using (for infrastructure compatability reasons) the scheme
      provided by Peter Todd.

This is a specific instance of a general problem where you cannot
trust scripts given to you by another party. Notice that we run into
the same sort of problem when doing key aggregation, in which you must
require the other party to prove knowledge of the discrete log before
using their public key, or else key cancellation can occur.

With script it is a little bit more complicated as you might want
zero-knowledge proofs of hash pre-images for HTLCs as well as proofs
of DL knowledge (signatures), but the basic idea is the same. Multi-
party wallet level protocols for jointly constructing scriptPubKeys
should require a 'delinearization' step that proves knowledge of
information necessary to complete each part of the script, as part of
proving the safety of a construct.

I think my hangup before in understanding the attack you describe was
in actualizing it into a practical attack that actually escalates the
attacker's capabilities. If the attacker can get you to agree to a
MAST policy that is nothing more than a CHECKSIG over a key they
presumably control, then they don't need to do any complicated
grinding. The attacker in that scenario would just actually specify a
key they control and take the funds that way.

Where this presumably leads to an actual exploit is when you specify a
script that a curious counter-party actually takes the time to
investigate and believes to be secure. For example, a script that
requires a signature or pre-image revelation from that counter-party.
That would require grinding not a few bytes, but at minimum 20-33
bytes for either a HASH160 image or the counter-party's key.

If I understand the revised attack description correctly, then there
is a small window in which the attacker can create a script less than
55 bytes in length, where nearly all of the first 32 bytes are
selected by the attacker, yet nevertheless the script seems safe to
the counter-party. The smallest such script I was able to construct
was the following:

    <fake-pubkey> CHECKSIGVERIFY HASH160 <preimage> EQUAL

This is 56 bytes and requires only 7 bits of grinding in the fake
pubkey. But 56 bytes is too large. Switching to secp256k1 serialized
32-byte pubkeys (in a script version upgrade, for example) would
reduce this to the necessary 55 bytes with 0 bits of grinding. A
smaller variant is possible:

    DUP HASH160 <fake-pubkey-hash> EQUALVERIFY CHECKSIGVERIFY HASH160 <preimage> EQUAL

This is 46 bytes, but requires grinding 96 bits, which is a bit less
plausible.

Belts and suspenders are not so terrible together, however, and I
think there is enough of a justification here to look into modifying
the scheme to use a different IV for hash tree updates. This would
prevent even the above implausible attacks.



-------------------------------------
I do agree with you to a degree, but address reuse is actually not even
supposed to work (it is a bug). Peter Todd is suggesting only to make
expiration a part of a new address format, and we could have a GUI
warning (but no protocol change) for the existing formats. What do you
think about that?


On 09/27/2017 01:23 PM, Nick Pudar via bitcoin-dev wrote:

-------------------------------------
I believe there continues to be concern over a number of altcoins which
are running old, unpatched forks of Bitcoin Core, making it rather
difficult to disclose issues without putting people at risk (see, eg,
some of the dos issues which are preventing release of the alert key).
I'd encourage the list to have a discussion about what reasonable
approaches could be taken there.

On 09/10/17 18:03, Simon Liu via bitcoin-dev wrote:

-------------------------------------
On Thursday, March 30, 2017 9:34:31 AM Natanael via bitcoin-dev wrote:

Minor detail: OP_RETURN doesn't work like that. You'd need OP_DROP.


Inputs don't have addresses, and addresses should only ever be used once.
You might be able to fix this by increasing the value of the change, though.
It would require a new signature-check opcode at the very least.

I don't see a purpose to this proposal. Miners always mine as if it's their 
*only* chance to get the fee, because if they don't, another miner will. Ie, 
after 1 block, the fee effectively drops to 0 already.


The standard problem with these is that miners are incentivised to publish 
their own "out of band fee" address so they get all the fee directly.

Luke

-------------------------------------
Have you thought about combining this with BIP-47? You could associate payment codes with email via DNS.

It would be nice if there was a way to get rid of the announcement transaction in BIP-47 and establish a shared secret out of bound. That would simplify things, at the cost of an additional burden of storing more than an HD seed to recover a wallet that received funds this way.

Perhaps the sender can email to the recipient the information they need to retrieve the funds. The (first) transaction could have a time locked refund in it, in case the payment code is stale.

Sjors


-------------------------------------
On Monday, April 03, 2017 9:06:02 AM Sancho Panza via bitcoin-dev wrote:

BIP 9 doesn't limit itself, merely acknowledges the *inherent* nature of it 
not being applicable to hardforks. BIP 9 provides a mechanism for having 
miners coordinate softforks because they can make the upgrade process smoother 
this way. But the same is not true of hardforks: miners are essentially 
irrelevant to them, and cannot make the process any smoother. Therefore, BIP 9 
and any miner signalling in general is not very useful for deploying these.


Softforks are not required to use BIP 9, and even if they do, they are not 
required to use the recommended thresholds.

Basically, the problems you're trying to solve don't exist...

Luke

-------------------------------------
Jorge, I'll be happy to discuss with you more about whether allowing
ASICBoost would actually secure the network more or not, but that's not my
main motivation. My main motivation is to get more miners to accept segwit.

The version bit usage part, I don't believe requires any code changes as
those bits aren't being used by BIP9 anyway, though some cleanup to
restrict them later is probably a good idea.
The requiring witness commitment part would require some changes, but
according to Timo Hanke, that's actually not necessary as overt is so much
more efficient.

In any case, I'm happy to close this discussion until there's some
indication that more miners would accept segwit as a result of this change.

Jimmy

On Tue, Apr 11, 2017 at 4:25 PM, Jorge Timón <jtimon@jtimon.cc> wrote:

-------------------------------------
bitcoin, there’d now be 2

Yes, there would be 2. One of which would (in the scenario we are
discussing) be producing blocks excruciatingly slowly but be the same in
all other aspects.

issue—hashpower not being aligned with users’ (or even its owners’)
interests

I disagree. Changing the difficulty adjustment algorithm could improve the
functionality of a chain, which could be an important prerequisite to using
and trading the tokens on the chain. This property could help keep the
price of the token high, which is what pressures hashpower to align with
user interests.

bitcoin chain dies from disuse after suffering a hashpower attack,
especially a centrally and/or purposefully instigated one, then bitcoin
would be failed a failed project.

IF the incentive problem could not be resolved then Bitcoin would be a
failed project.

But here is a bit of good news.

Bitcoin has developers!

And those developers can publish a contingency plan!

And that contingency plan can be an emergency hard fork to a different
retarget algorithm.

And that emergency hard fork can gain consensus if it is broadly preferred
over the status quo.

If 90% of the hash power follows NYA, blocks are going to take 100 minutes
until difficulty adjusts after 4.5 months.

That is quite a handicap, even for a honey badger. Emergency hard fork
carries a risk, but depending on the scenario in November, it could be a
risk worth taking.

One more thing. If miners think they are going to succeed in starving the
legacy chain to death, they might be more likely to try. If they get a
credible signal that the legacy chain will react by changing the retarget
function and thereby be more likely to survive, they might feel less
committed to a strategy of starving the legacy chain. This could be
especially true if they are giving up profit for what they fervently hope
will be a short period of time.

On Wed, Oct 11, 2017 at 12:08 AM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Why do you say activation by August 1st is likely? That would require an entire difficulty adjustment period with >=95% bit1 signaling. That seems a tall order to organize in the scant few weeks remaining. 

-------------------------------------
On Mon, Sep 11, 2017 at 07:34:33AM -0400, Alex Morcos wrote:

If you can't pick even a small group that's trustworthy (top five by
market cap as a start [0]? or just major bitcoin wallets / exchanges /
alt node implementations?), then it still seems better to (eventually)
disclose publically than keep it unrevealed and let it be a potential
advantage for attackers against people who haven't upgraded for other
reasons?

I find it hard to imagine bitcoin's still obscure enough that people
aren't tracking git commit logs to use them as inspiration for attacks
on bitcoin users and businesses; at best I would have thought it'd
only be a few months of development time between a fix being proposed
as a PR or committed to master and black hats having the ability to
exploit it in users who are running older nodes. (Or for that matter,
being able to be exploited by otherwise legitimate bitcoin businesses
with an agenda to push, a strong financial motive behind that agenda,
and a legal team that says they'll get away with it)


Isn't that just an argument for putting more effort into backporting
fixes/workarounds? (I don't see how you do that without essentially
publically disclosing which patches have a security impact -- "oh,
gosh, this patch gets a backport, I wonder if maybe it has security
implications...")

(In so far as bitcoin is a consensus system, there can sometimes be a
positive network effect, where having other people upgrade can help your
security, even if you don't upgrade; "herd immunity" if you will. That
way a new release going out to other people helps keep you safe, even
while you continue to maintain the same definition of money by not
upgrading at all)

If altcoin maintainers are inconvenienced by tracking bitcoin-core
updates, that would be an argument for them to contribute back to their
upstream to make their own job easier; either helping with backports,
or perhaps contributing to patches like PR#8994 might help.

All of those things seem like they'd help not just altcoins but bitcoin
investors/traders too, so it's not even a trade-off between classes of
bitcoin core users.  And if in the end various altcoins aren't able to
keep up with security fixes, that's probably valuable information to
provide to the market...

Cheers,
aj

[0] Roughly: BCash, Litecoin, Dash, BitConnect, ZCash, Dogecoin?
    I've no idea which of those might have trustworthy devs to work with,
    but surely at least a couple do?


-------------------------------------
Makes sense. I would love if GPUs were back as the main hashing tool.

However, we need to consider the environmental impact of mining, which currently consumes a quite exorbitant amount of energy. Any ideas on this front?

--
Garrett MacDonald
+1 720 515 2248
g@cognitive.ch
GPG Key

On Apr 10, 2017, 12:17 PM -0600, Erik Aronesty <erik@q32.com>, wrote:
-------------------------------------
Its too bad you're not the one who decides what gets posted here or
not. If you don't like whats being discussed, then don't open those
emails.

On 1/7/17, Eric Lombrozo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------


There’s no need to put it in the transaction itself. You put it in the witness and it is either committed to as part of the witness (in which case it has to hold for all possible spend paths), or at spend time by including it in the data signed by CHECKSIG.

-------------------------------------
Hi,

I wanted to discuss few changes in BIP49

*- Breaking backwards compatibility *
The BIP talks about breaking this, and  but it really doesn't.  I really
feel it should completely break this. Here is why

What would happen if you recover a wallet  using seed words ?
  1. Since there is no difference in seed words between segwit/non segwit,
the wallet would discover both m/44' and m/49' accounts
  2. Note that we cannot ask the user to choose an account he wants to
operate on (Segwit/Non segwit). This is like asking him the HD derivation
path and a really bad UI
  3. The wallet now has to constantly monitor both m/44' and m/49' accounts
for transactions

Basically we are always stuck with keeping compatibility with older seed
words or always asking the user if the seed words came from segwit/non
segwit wallet !

Here is my suggestion :
1. By default all new wallets will be created as segwit  m/49' without
asking user anything. I think you would agree with me that in future we
want most wallet to be default segwit (unless user chooses a non segwit
from advanced options)!

2. Segwit wallet seed words have a different format which is incompatible
with previous wallet seed words. This  encodes the information that this
wallet is segwit in the seed words itself. We need to define a structure
for this



*- XPUB Derivation*
This is something not addressed in the BIP yet.

1. Right now you can get an xpub balance/transaction history. With m/49'
there is no way to know whether an xpub is from m/44' or m/49'

2. This breaks lots of things. Wallets like electrum/armory/mycelium
<https://blog.trezor.io/using-mycelium-to-watch-your-trezor-accounts-a836dce0b954>support
importing  xpub as a watch only wallet. Also services like blockonomics/
blockchain.info use xpub for displaying balance/generating merchant
addresses

Looking forward to hearing your thoughts
-------------------------------------
*Rationale:*

A node that stores the full blockchain (I will use the term archival node)
requires over 100GB of disk space, which I believe is one of the most
significant barriers to more people running full nodes. And I believe the
ecosystem would benefit substantially if more users were running full nodes.

The best alternative today to storing the full blockchain is to run a
pruned node, which keeps only the UTXO set and throws away already verified
blocks. The operator of the pruned node is able to enjoy the full security
benefits of a full node, but is essentially leeching the network, as they
performed a large download likely without contributing anything back.

This puts more pressure on the archival nodes, as the archival nodes need
to pick up the slack and help new nodes bootstrap to the network. As the
pressure on archival nodes grows, fewer people will be able to actually run
archival nodes, and the situation will degrade. The situation would likely
become problematic quickly if bitcoin-core were to ship with the defaults
set to a pruned node.

Even further, the people most likely to care about saving 100GB of disk
space are also the people least likely to care about some extra bandwidth
usage. For datacenter nodes, and for nodes doing lots of bandwidth, the
bandwidth is usually the biggest cost of running the node. For home users
however, as long as they stay under their bandwidth cap, the bandwidth is
actually free. Ideally, new nodes would be able to bootstrap from nodes
that do not have to pay for their bandwidth, instead of needing to rely on
a decreasing percentage of heavy-duty archival nodes.

I have (perhaps incorrectly) identified disk space consumption as the most
significant factor in your average user choosing to run a pruned node or a
lite client instead of a full node. The average user is not typically too
worried about bandwidth, and is also not typically too worried about
initial blockchain download time. But the 100GB hit to your disk space can
be a huge psychological factor, especially if your hard drive only has
500GB available in the first place, and 250+ GB is already consumed by
other files you have.

I believe that improving the disk usage situation would greatly benefit
decentralization, especially if it could be done without putting pressure
on archival nodes.

*Small Nodes Proposal:*

I propose an alternative to the pruned node that does not put undue
pressure on archival nodes, and would be acceptable and non-risky to ship
as a default in bitcoin-core. For lack of a better name, I'll call this new
type of node a 'small node'. The intention is that bitcoin-core would
eventually ship 'small nodes' by default, such that the expected amount of
disk consumption drops from today's 100+ GB to less than 30 GB.

My alternative proposal has the following properties:

+ Full nodes only need to store ~20% of the blockchain
+ With very high probability, a new node will be able to recover the entire
blockchain by connecting to 6 random small node peers.
+ An attacker that can eliminate a chosen+ 95% of the full nodes running
today will be unable to prevent new nodes from downloading the full
blockchain, even if the attacker is also able to eliminate all archival
nodes. (assuming all nodes today were small nodes instead of archival nodes)

Method:

A small node will pick an index [5, 256). This index is that node's
permanent index. When storing a block, instead of storing the full block,
the node will use Reed-Solomon coding to erasure code the block using a
5-of-256 scheme. The result will be 256 pieces that are 20% of the size of
the block each. The node picks the piece that corresponds to its index, and
stores that instead. (Indexes 0-4 are reserved for archival nodes -
explained later)

The node is now storing a fragment of every block. Alone, this fragment
cannot be used to recover any piece of the blockchain. However, when paired
with any 5 unique fragments (fragments of the same index will not be
unique), the full block can be recovered.

Nodes can optionally store more than 1 fragment each. At 5 fragments, the
node becomes a full archival node, and the chosen indexes should be 0-4.
This is advantageous for the archival node as the encoded data for the
first 5 indexes will actually be identical to the block itself - there is
no computational overhead for selecting the first indexes. There is also no
need to choose random indexes, because the full block can be recovered no
matter which indexes are chosen.

When connecting to new peers, the indexes of each peer needs to be known.
Once peers totaling 5 unique indexes are discovered, blockchain download
can begin. Connecting to just 5 small node peers provides a >95% chance of
getting 5 uniques, with exponentially improving odds of success as you
connect to more peers. Connecting to a single archive node guarantees that
any gaps can be filled.

A good encoder should be able to turn a block into a 5-of-256 piece set in
under 10 milliseconds using a single core on a standard consumer desktop.
This should not slow down initial blockchain download substantially, though
the overhead is more than a rounding error.

*DoS Prevention:*

A malicious node may provide garbage data instead of the actual piece.
Given just the garbage data and 4 other correct pieces, it is impossible
(best I know anyway) to tell which piece is the garbage piece.

One option in this case would be to seek out an archival node that could
verify the correctness of the pieces, and identify the malicious node.

Another option would be to have the small nodes store a cryptographic
checksum of each piece. Obtaining the cryptographic checksum for all 256
pieces would incur a nontrivial amount of hashing (post segwit, as much as
100MB of extra hashing per block), and would require an additional ~4kb of
storage per block. The hashing overhead here may be prohibitive.

Another solution would be to find additional pieces and brute-force
combinations of 5 until a working combination was discovered. Though this
sounds nasty, it should take less than five seconds of computation to find
the working combination given 5 correct pieces and 2 incorrect pieces. This
computation only needs to be performed once to identify the malicious peers.

I also believe that alternative erasure coding schemes exist which actually
are able to identify the bad pieces given sufficient good pieces, however I
don't know if they have the same computational performance as the best
Reed-Solomon coding implementations.

*Deployment:*

Small nodes are completely useless unless the critical mass of 5 pieces can
be obtained. The first version that supports small node block downloads
should default everyone to an archival node (meaning indexes 0-4 are used)

Once there are enough small-node-enabled archive nodes, the default can be
switched so that nodes only have a single index by default. In the first
few days, when there are only a few small nodes, the previously-deployed
archival nodes can help fill in the gaps, and the small nodes can be useful
for blockchain download right away.

----------------------------------

This represents a non-trivial amount of code, but I believe that the result
would be a non-trivial increase in the percentage of users running full
nodes, and a healthier overall network.
-------------------------------------
On Tue, Nov 14, 2017 at 9:11 AM, Peter Todd <pete@petertodd.org> wrote:

I very strongly disagree with your strong disagreement.


Its important that people know and understand what properties a system has.

Perhaps one distinction you miss is that perfectly hiding systems
don't even exist in practice: I would take a bet that no software on
your system that you can use with other people actually implements a
perfectly hiding protocol (much less find on most other people's
system system :)).

In the case of practical use with CT perfect hiding is destroyed by
scalability-- the obvious construction is a stealth address like one
where a DH public key is in the address and that is used to scan for
your payments against a nonce pubkey in the transactions.   The
existence of that mechanism destroys perfect hiding.  No scheme that
can be scanned using an asymmetric key is going to provide perfect
hiding.

Now, perhaps what you'd like is a system which is not perfect hiding
but where the hiding rests on less "risky" assumptions.  That is
something that can plausibly be constructed, but it's not itself
incompatible with unconditional soundness.

As referenced in the paper, there is also the possibility of having a
your cake and eating it too-- switch commitments for example allow
having computational-hiding-depending-on-the-hardness-of-inverting-hashes
 (which I would argue is functionally as good as perfect hiding, esp
since hiding is ultimately limited by the asymmetric crypto used for
discovery)  and yet it retains an option to upgrade or block spending
via unsound mechanisms in the event of a crypto break.


Sounds like you are assuming that you know when there is a problem, if
you do then the switch commitments scheme works and doesn't require
any selling of anything. Selling also has the problem that everyone
would want to do it at once if there was a concern; this would not
have good effects. :) Without switch commitments though, you are just
hosed.  And you cannot have something like switch commitments without
abandoning perfect hiding ( though you get hiding which is good enough
(tm), as mentioned above).

On Tue, Nov 14, 2017 at 10:07 AM, Peter Todd <pete@petertodd.org> wrote:

Miners to reduce coin supply, enhancing the value of their own
holdings, by simply not letting near-expiry ones get spent...
(This can be partially mediated by constructing proofs to hide if a
coins in near expiration or not.)


Yes, that they can do-- though with the trade-offs inherent in that.
It is worse than what you were imagining in the Bitcoin case because
you cannot use one or two time-ordered trees, the spent coins list
needs search-insertion; so maintaining it over time is harder. :(  The
single time ordered tree trick doesn't work because you can't mutate
the entries without blowing anonymity.

I think it's still fair to say that ring-in and tree-in approaches
(monero, and zcash) are fundamentally less scalable than
CT+valueshuffle, but more private-- though given observations of Zcash
behavior perhaps not that much more private.  With the right smart
tricks the scalablity loss might be more inconvenient than fatal, but
they're still a loss even if they're one that makes for a good
tradeoff.

As an aside, you shouldn't see Monero as entirely distinct now that
we're talking about a framework which is fully general:  Extending
this to a traceable 1 of N input for monero is simple-- and will add
size log() in the number of ring inputs with good constant factors.
One could also store inputs in a hash tree, and then have a
bulletproof that verified membership in the tree.  This would provide
tree-in style transactions with proofs that grow with the log() of the
size of the tree (and a spent coins accumulator); the challenge there
would be choosing a hash function that had a compact representation in
the arithmetic circuit so that the constant factors aren't terrible.
Effectively that's what bulletproofs does:   It takes a general scheme
for ZKP of arbitrary computation, which could implement a range proof
by opening the commitments (e.g. a circuit for EC point scalar
multiply) and checking the value, and optimizes it to handle the
commitments more directly. If you're free to choose the hash function
there may be a way to make a hash tree check ultra efficient inside
the proof, in which case this work could implement a tree-in scheme
like zcash-- but with larger proofs and slower verification in
exchange for much better crypto assumptions and no trusted setup.
This is part of what I meant by it opening up "many other interesting
applications".

But as above, I think that the interactive-sparse-in (CJ) has its own
attractiveness, even though it is not the strongest for privacy.

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 04/11/2017 01:43 AM, Tomas wrote:

Ok

It's not the headers/tx-hashes of the blocks that I'm referring to, it
is the confirmation and spend information relative to all txs and all
outputs for each branch. This reverse navigation (i.e. utxo
information) is essential, must be persistent and is branch-relative.


That's not your concurrent validation scenario. In the scenario you
described, the person chooses the weaker block of two that require
validation because it's better somehow, not because it's his own
(which does not require validation).


Consistency is reached, despite seeing things at different times,
because people use the same rules. If the economy ran on arbitrary
block preference consistency would be elusive.


This line of reasoning has me a bit baffled. Yet as I said, it's not
important to the question at hand. It is not likely to be optimal to
validate concurrently even if you consider selection of a weaker block
advantageous.


Storing the validation flags with each tx is exactly what libbitcoin
does (otherwise pre-validation would be infeasible). But that was not
the full point. You said on this in response previously:

adequate and safe.

I read this as encoding the height at which a fork historically
activated. If you intend to track activation for each branch that will
not be "height-based" it will be history based.

e
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBCAAGBQJY7KTHAAoJEDzYwH8LXOFOI+QH/RzX++1TNLC9DEMWioE7SmMj
yKOrP8WEkOnnrZdFKxVmwV9oZBekEvDABMnJmFiW5TMjsmPz7XwKAYzV0Y5L5oGU
fZYo3IOPyr0dA9TcpP15gNziR6pFUBq/QTYB6BcbUvvlkJv6xjgIdedgDMEyREWU
Hm/JU5g7gQUQd6MIDWbQ9FbYjtPuNSRQi851YfIn5mDivT4HuidaqQYMd9t5yS2Z
FuoQBI6L5GTJIqml1bTwJ0wsA7+ZseBEgMn1TT1ehy2v1FFJTojTpzIwG+m3eiXg
TxN3U/+fNAj+sKBb8Hq+nb7DvgjvKHyHuyRryBju7yq5d5rsb6meXcoiOtAznP8=
=fRXf
-----END PGP SIGNATURE-----

-------------------------------------
Lock time transactions have been valid for over a year now I believe. In any case we can't scan the block chain for usage patterns in UTXOs because P2SH puts the script in the signature on spend.

-------------------------------------
On Wednesday, April 05, 2017 4:54:05 PM Christopher Jeffrey via bitcoin-dev 
wrote:

Oh? If this was not meant to be a Bitcoin Improvement Proposal, perhaps you 
should clarify somewhere what altcoin you are proposing it for. As it stands, 
it certainly did read much like it was meant to be a BIP, and apparently many 
others thought so as well.

Admittedly, the bitcoin-dev ML isn't the place for altcoin discussions, and 
I'm not particularly interested in spending my time aiding altcoins, so I'll 
just end the conversation here until someone re-proposes something similar for 
Bitcoin.

Sorry for confusing the nature of your work,

Luke

-------------------------------------
Even more to the point, new post- fork coins are fork-specific.  The 
longer both forks persist, the more transactions become unavoidably 
fork-specific through the mixing in of these coins.  Any attempt to 
maximize replay will become less effective with time.

The rationality of actors in this situation essentially defines the 
limited solution that is possible.  Upgraded software can create 
transactions guaranteed not to execute to one fork or the other, or that 
is not prevented from execution on either fork.  I see no downside to 
this, and the advantage is that markets can be much less chaotic.  In 
fact exchanges will be much better off if they require that post-fork 
trading, deposits and withdrawals are exclusively chain-specific, which 
will also result in well determined prices for the two currencies.

None of this precludes the possibility of further forks on either side, 
and the difficulty consideration alone suggests a likely counter-fork by 
(part of) the existing network.


On 1/26/2017 1:20 AM, Johnson Lau via bitcoin-dev wrote:


-------------------------------------
On 09/29/2017 11:55 AM, Peter Todd via bitcoin-dev wrote:


I regularly pay with Bitcoin, and I haven't seen the payment protocol
not being in use in ages.


15+ Mio Coinbase users
~10 Mio BitPay users
8 Mio Bitcoin Wallet users
Plus Bitcoin Core, Electrum, etc (sorry no numbers)

Probably the only usecase for naked addresses is paper wallets, right?
I'm not sure if paper wallets can expire.


-------------------------------------

I think it being a bitcoin address probably makes the most sense. The address could even be used for donations to incentivise identifier use.

I had not really envisaged this having any blockchain presence though. It was just an easy way to give third party node monitors like coin.dance and bitnodes.21.co a few more metrics.

That said, it would allow the creation of a 'nodepool', where each node could broadcast its latest status like a transaction, and every node has a register of active nodes. Like a mempool, but for nodes.

By leveraging the randomness of node identities, it could be that a deterministic subset of nodes randomly check that a new node status update is legitimate by querying the node directly (a small enough subset to not cause a DDOS). If a threshhold of those random checking nodes reports that the node either doesn't exist or is responding with conflicting information, this will become evident to the network and can be flagged.

This should paint a pretty accurate picture of the state of the network, and might also prove useful for developing lightning routing?

________________________________
From: Marcel Jamin <marcel@jamin.net>
Sent: Sunday, March 5, 2017 6:29 AM
To: John Hardy; Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] Unique node identifiers


Wouldn't this actually *need* to be a bitcoin address that is included in a block to get any real assurances about the age if this node id? Otherwise malicous nodes could lie and claim to have seen a brand new node id years ago already.

Even if included in a block, people could sell their aged IDs (if we were to rely on those for anything).

Also funding that ID address would might tie your economic activity (or even identity) to a node.

On 4 March 2017 at 17:04, John Hardy via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:

The discussion of UASF got me thinking about whether such a method might lead to sybil attacks, with new nodes created purely to inflate the node count for a particular implementation in an attempt at social engineering.


I had an idea for an anonymous, opt-in, unique node identification mechanism to help counter this.


This would give every node the opportunity to create a node address/unique identifier. This could even come in the form of a Bitcoin address.


The node on first installation generates and backs up a private key. The corresponding public key becomes that nodes unique identifier. If the node switches to a new software version or a new IP, the identifier can remain constant if the node operator chooses.


Asking a node for its identifier can be done by sending a message the command identify and a challenge. The node can then respond with its unique identifier and a signature for the challenge to prove it. The node can also include what software it is running and sign this information so it can be verified as legitimate by third parties.


Why would we do this?


Well, it adds a small but very useful piece of data when compiling lists of active nodes.


Any register of active nodes can have a record of when a node identifier was first seen, and how many IPs the same identifier has broadcast from. Also, crucially, we could see what software the node operator has been seen running historically.


This information would make it easy to identify patterns. For example if a huge new group of nodes appeared on the network with no history for their identifier they could likely be dismissed as sybil attacks. If a huge number of nodes that had been reporting as Bitcoin Core for an extended period of time started switching to a rival implementation, this would add credibility but not certainty (keys could be traded), that the shift was more organic.


This would be trivial to implement, is (to me?) non-controversial, and would give a way for a node to link itself to a pseudo-anonymous identity, but with the freedom to opt-out at any time.


Keen to hear any thoughts?


Thanks,


John Hardy

john@seebitcoin.com<mailto:john@seebitcoin.com>

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
BIP125 is the standard way to signal:
https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki

Should explain everything you need.

On Thu, Jan 12, 2017 at 9:02 AM, Police Terror via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
By "upgrade"  the HF you mean activate 2X and then spoonet 18 months later
or do not activate the 2x HF at all?





On Fri, Jun 2, 2017 at 4:04 PM, Erik Aronesty via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Saturday, March 18, 2017 3:23:16 PM Chris Stewart via bitcoin-dev wrote:

GitHub doesn't allow people to have multiple accounts last I checked.

Luke

-------------------------------------
I don't think we should put any Bitcoin users at additional risk to help
altcoins. If they fork the code they are making maintenance their own
responsibly.

It's hard to disclose a bitcoin vulnerability considering the network is
decentralised and core can't force everyone to update. Maybe a timeout
period for vulnerabilities could be decided. People might be expected to
patched before then at which point the vulnerability can be published. Is
that not already sort of how it works?

On Sep 10, 2017 4:10 PM, "Matt Corallo via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Fri, Aug 18, 2017 at 5:11 PM, Andrew Chow via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Just a quick note but perhaps you and other readers would find this thread
(on hardware wallet BIP drafting) to be tangentially related and useful:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-August/013008.html

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version *0.15.1* is now available from:

  <https://bitcoincore.org/bin/bitcoin-core-0.15.1/>

or

  <https://bitcoin.org/bin/bitcoin-core-0.15.1/>

Or through bittorrent:

  magnet:?xt=urn:btih:e83dfdfca54def4e29f5355e8c3f9a7aa45ecbaf&dn=bitcoin-core-0.15.1&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&tr=udp%3A%2F%2Fzer0day.ch%3A1337&tr=udp%3A%2F%2Fexplodie.org%3A6969

This is a new minor version release, including various bugfixes and
performance improvements, as well as updated translations.

Please report bugs using the issue tracker at GitHub:

  <https://github.com/bitcoin/bitcoin/issues>

To receive security and update notifications, please subscribe to:

  <https://bitcoincore.org/en/list/announcements/join/>

How to Upgrade
==============

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes for older versions), then run the 
installer (on Windows) or just copy over `/Applications/Bitcoin-Qt` (on Mac)
or `bitcoind`/`bitcoin-qt` (on Linux).

The first time you run version 0.15.0 or higher, your chainstate database will
be converted to a new format, which will take anywhere from a few minutes to
half an hour, depending on the speed of your machine.

The file format of `fee_estimates.dat` changed in version 0.15.0. Hence, a
downgrade from version 0.15 or upgrade to version 0.15 will cause all fee
estimates to be discarded.

Note that the block database format also changed in version 0.8.0 and there is no
automatic upgrade code from before version 0.8 to version 0.15.0. Upgrading
directly from 0.7.x and earlier without redownloading the blockchain is not supported.
However, as usual, old wallet versions are still supported.

Downgrading warning
- -------------------

The chainstate database for this release is not compatible with previous
releases, so if you run 0.15 and then decide to switch back to any
older version, you will need to run the old release with the `-reindex-chainstate`
option to rebuild the chainstate data structures in the old format.

If your node has pruning enabled, this will entail re-downloading and
processing the entire blockchain.

Compatibility
==============

Bitcoin Core is extensively tested on multiple operating systems using
the Linux kernel, macOS 10.8+, and Windows Vista and later. Windows XP is not supported.

Bitcoin Core should also work on most other Unix-like systems but is not
frequently tested on them.


Notable changes
===============

Network fork safety enhancements
- --------------------------------

A number of changes to the way Bitcoin Core deals with peer connections and invalid blocks
have been made, as a safety precaution against blockchain forks and misbehaving peers.

- - Unrequested blocks with less work than the minimum-chain-work are now no longer processed even
if they have more work than the tip (a potential issue during IBD where the tip may have low-work).
This prevents peers wasting the resources of a node. 

- - Peers which provide a chain with less work than the minimum-chain-work during IBD will now be disconnected.

- - For a given outbound peer, we now check whether their best known block has at least as much work as our tip. If it
doesn't, and if we still haven't heard about a block with sufficient work after a 20 minute timeout, then we send
a single getheaders message, and wait 2 more minutes. If after two minutes their best known block has insufficient
work, we disconnect that peer. We protect 4 of our outbound peers from being disconnected by this logic to prevent
excessive network topology changes as a result of this algorithm, while still ensuring that we have a reasonable
number of nodes not known to be on bogus chains.

- - Outbound (non-manual) peers that serve us block headers that are already known to be invalid (other than compact
block announcements, because BIP 152 explicitly permits nodes to relay compact blocks before fully validating them)
will now be disconnected.

- - If the chain tip has not been advanced for over 30 minutes, we now assume the tip may be stale and will try to connect
to an additional outbound peer. A periodic check ensures that if this extra peer connection is in use, we will disconnect
the peer that least recently announced a new block.

- - The set of all known invalid-themselves blocks (i.e. blocks which we attempted to connect but which were found to be
invalid) are now tracked and used to check if new headers build on an invalid chain. This ensures that everything that
descends from an invalid block is marked as such.


Miner block size limiting deprecated
- ------------------------------------

Though blockmaxweight has been preferred for limiting the size of blocks returned by
getblocktemplate since 0.13.0, blockmaxsize remained as an option for those who wished
to limit their block size directly. Using this option resulted in a few UI issues as
well as non-optimal fee selection and ever-so-slightly worse performance, and has thus
now been deprecated. Further, the blockmaxsize option is now used only to calculate an
implied blockmaxweight, instead of limiting block size directly. Any miners who wish
to limit their blocks by size, instead of by weight, will have to do so manually by
removing transactions from their block template directly.


GUI settings backed up on reset
- -------------------------------

The GUI settings will now be written to `guisettings.ini.bak` in the data directory before wiping them when
the `-resetguisettings` argument is used. This can be used to retroactively troubleshoot issues due to the
GUI settings.


Duplicate wallets disallowed
- ----------------------------

Previously, it was possible to open the same wallet twice by manually copying the wallet file, causing
issues when both were opened simultaneously. It is no longer possible to open copies of the same wallet.


Debug `-minimumchainwork` argument added
- ----------------------------------------

A hidden debug argument `-minimumchainwork` has been added to allow a custom minimum work value to be used
when validating a chain.


Low-level RPC changes
- ----------------------

- - The "currentblocksize" value in getmininginfo has been removed.

- - `dumpwallet` no longer allows overwriting files. This is a security measure
  as well as prevents dangerous user mistakes.

- - `backupwallet` will now fail when attempting to backup to source file, rather than
  destroying the wallet.

- - `listsinceblock` will now throw an error if an unknown `blockhash` argument
  value is passed, instead of returning a list of all wallet transactions since
  the genesis block. The behaviour is unchanged when an empty string is provided.

0.15.1 Change log
=================

### Mining
- - #11100 `7871a7d` Fix confusing blockmax{size,weight} options, dont default to throwing away money (TheBlueMatt)

### RPC and other APIs
- - #10859 `2a5d099` gettxout: Slightly improve doc and tests (jtimon)
- - #11267 `b1a6c94` update cli for estimate\*fee argument rename (laanwj)
- - #11483 `20cdc2b` Fix importmulti bug when importing an already imported key (pedrobranco)
- - #9937 `a43be5b` Prevent `dumpwallet` from overwriting files (laanwj)
- - #11465 `405e069` Update named args documentation for importprivkey (dusty-wil)
- - #11131 `b278a43` Write authcookie atomically (laanwj)
- - #11565 `7d4546f` Make listsinceblock refuse unknown block hash (ryanofsky)
- - #11593 `8195cb0` Work-around an upstream libevent bug (theuni)

### P2P protocol and network code
- - #11397 `27e861a` Improve and document SOCKS code (laanwj)
- - #11252 `0fe2a9a` When clearing addrman clear mapInfo and mapAddr (instagibbs)
- - #11527 `a2bd86a` Remove my testnet DNS seed (schildbach)
- - #10756 `0a5477c` net processing: swap out signals for an interface class (theuni)
- - #11531 `55b7abf` Check that new headers are not a descendant of an invalid block (more effeciently) (TheBlueMatt)
- - #11560 `49bf090` Connect to a new outbound peer if our tip is stale (sdaftuar)
- - #11568 `fc966bb` Disconnect outbound peers on invalid chains (sdaftuar)
- - #11578 `ec8dedf` Add missing lock in ProcessHeadersMessage(...) (practicalswift)
- - #11456 `6f27965` Replace relevant services logic with a function suite (TheBlueMatt)
- - #11490 `bf191a7` Disconnect from outbound peers with bad headers chains (sdaftuar)

### Validation
- - #10357 `da4908c` Allow setting nMinimumChainWork on command line (sdaftuar)
- - #11458 `2df65ee` Don't process unrequested, low-work blocks (sdaftuar)

### Build system
- - #11440 `b6c0209` Fix validationinterface build on super old boost/clang (TheBlueMatt)
- - #11530 `265bb21` Add share/rpcuser to dist. source code archive (MarcoFalke)

### GUI
- - #11334 `19d63e8` Remove custom fee radio group and remove nCustomFeeRadio setting (achow101)
- - #11198 `7310f1f` Fix display of package name on 'open config file' tooltip (esotericnonsense)
- - #11015 `6642558` Add delay before filtering transactions (lclc)
- - #11338 `6a62c74` Backup former GUI settings on `-resetguisettings` (laanwj)
- - #11335 `8d13b42` Replace save|restoreWindowGeometry with Qt functions (MeshCollider)
- - #11237 `2e31b1d` Fixing division by zero in time remaining (MeshCollider)
- - #11247 `47c02a8` Use IsMine to validate custom change address (MarcoFalke)

### Wallet
- - #11017 `9e8aae3` Close DB on error (kallewoof)
- - #11225 `6b4d9f2` Update stored witness in AddToWallet (sdaftuar)
- - #11126 `2cb720a` Acquire cs_main lock before cs_wallet during wallet initialization (ryanofsky)
- - #11476 `9c8006d` Avoid opening copied wallet databases simultaneously (ryanofsky)
- - #11492 `de7053f` Fix leak in CDB constructor (promag)
- - #11376 `fd79ed6` Ensure backupwallet fails when attempting to backup to source file (tomasvdw)
- - #11326 `d570aa4` Fix crash on shutdown with invalid wallet (MeshCollider)

### Tests and QA
- - #11399 `a825d4a` Fix bip68-sequence rpc test (jl2012)
- - #11150 `847c75e` Add getmininginfo test (mess110)
- - #11407 `806c78f` add functional test for mempoolreplacement command line arg (instagibbs)
- - #11433 `e169349` Restore bitcoin-util-test py2 compatibility (MarcoFalke)
- - #11308 `2e1ac70` zapwallettxes: Wait up to 3s for mempool reload (MarcoFalke)
- - #10798 `716066d` test bitcoin-cli (jnewbery)
- - #11443 `019c492` Allow "make cov" out-of-tree; Fix rpc mapping check (MarcoFalke)
- - #11445 `51bad91` 0.15.1 Backports (MarcoFalke)
- - #11319 `2f0b30a` Fix error introduced into p2p-segwit.py, and prevent future similar errors (sdaftuar)
- - #10552 `e4605d9` Tests for zmqpubrawtx and zmqpubrawblock (achow101)
- - #11067 `eeb24a3` TestNode: Add wait_until_stopped helper method (MarcoFalke)
- - #11068 `5398f20` Move wait_until to util (MarcoFalke)
- - #11125 `812c870` Add bitcoin-cli -stdin and -stdinrpcpass functional tests (promag)
- - #11077 `1d80d1e` fix timeout issues from TestNode (jnewbery)
- - #11078 `f1ced0d` Make p2p-leaktests.py more robust (jnewbery)
- - #11210 `f3f7891` Stop test_bitcoin-qt touching ~/.bitcoin (MeshCollider)
- - #11234 `f0b6795` Remove redundant testutil.cpp|h files (MeshCollider)
- - #11215 `cef0319` fixups from set_test_params() (jnewbery)
- - #11345 `f9cf7b5` Check connectivity before sending in assumevalid.py (jnewbery)
- - #11091 `c276c1e` Increase initial RPC timeout to 60 seconds (laanwj)
- - #10711 `fc2aa09` Introduce TestNode (jnewbery)
- - #11230 `d8dd8e7` Fixup dbcrash interaction with add_nodes() (jnewbery)
- - #11241 `4424176` Improve signmessages functional test (mess110)
- - #11116 `2c4ff35` Unit tests for script/standard and IsMine functions (jimpo)
- - #11422 `a36f332` Verify DBWrapper iterators are taking snapshots (TheBlueMatt)
- - #11121 `bb5e7cb` TestNode tidyups (jnewbery)
- - #11521 `ca0f3f7` travis: move back to the minimal image (theuni)
- - #11538 `adbc9d1` Fix race condition failures in replace-by-fee.py, sendheaders.py (sdaftuar)
- - #11472 `4108879` Make tmpdir option an absolute path, misc cleanup (MarcoFalke)
- - #10853 `5b728c8` Fix RPC failure testing (again) (jnewbery)
- - #11310 `b6468d3` Test listwallets RPC (mess110)

### Miscellaneous
- - #11377 `75997c3` Disallow uncompressed pubkeys in bitcoin-tx [multisig] output adds (TheBlueMatt)
- - #11437 `dea3b87` [Docs] Update Windows build instructions for using WSL and Ubuntu 17.04 (fanquake)
- - #11318 `8b61aee` Put back inadvertently removed copyright notices (gmaxwell)
- - #11442 `cf18f42` [Docs] Update OpenBSD Build Instructions for OpenBSD 6.2 (fanquake)
- - #10957 `50bd3f6` Avoid returning a BIP9Stats object with uninitialized values (practicalswift)
- - #11539 `01223a0` [verify-commits] Allow revoked keys to expire (TheBlueMatt)


Credits
=======

Thanks to everyone who directly contributed to this release:

- - Andreas Schildbach
- - Andrew Chow
- - Chris Moore
- - Cory Fields
- - Cristian Mircea Messel
- - Daniel Edgecumbe
- - Donal OConnor
- - Dusty Williams
- - fanquake
- - Gregory Sanders
- - Jim Posen
- - John Newbery
- - Johnson Lau
- - João Barbosa
- - Jorge Timón
- - Karl-Johan Alm
- - Lucas Betschart
- - MarcoFalke
- - Matt Corallo
- - Paul Berg
- - Pedro Branco
- - Pieter Wuille
- - practicalswift
- - Russell Yanofsky
- - Samuel Dobson
- - Suhas Daftuar
- - Tomas van der Wansem
- - Wladimir J. van der Laan

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBCgAGBQJaBwnSAAoJEB5K7WKYbNJdAhIIAL6y/9IM1cdvt6Wob9yDMawv
it2iL5pS0HIWbcqXTfnf+52JMw9SNmTX356U8B9q9l6V8EiFKMN8y1wu/A921kKb
n1BREmVKD0JtawK368LiFt9x0eYV3q0MTww9dOCPp5HoIEt8TTLGFIOwzAvscxNi
IQMZRE5ejtm9Yjs0VHeKBrAXNA9zt8BKzmuwGi/JHoWda8nUnAhnaL/asAaYQ1zB
IpqZHJo4k7GxxXUFIm1hiQkqT7uDZ5iehT706Su3qY7ATtaByPq8aHsPDEZFfUJO
PoW7nqzCzkyTofIIE7+ejviBruL7EYFZiq+oUzOt4byGJvgaRyBXo8rn+druvEI=
=8PAn
-----END PGP SIGNATURE-----

-------------------------------------
On Apr 9, 2017 7:00 PM, "Jared Lee Richardson via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

I can speak from personal experience regarding another very prominent
altcoin that attempted to utilize an asic-resistant proof of work
algorithm, it is only a matter of time before the "asic resistant"
algorithm gets its own Asics.  The more complicated the algorithm, the more
secretive the asic technology is developed.  Even without it,
multi-megawatt gpu farms have already formed in the areas of the world with
low energy costs.  I'd support the goal if I thought it possible, but I
really don't think centralization of mining can be prevented.

On Apr 9, 2017 1:16 PM, "Erik Aronesty via bitcoin-dev" <bitcoin-dev@lists.
linuxfoundation.org> wrote:

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


The real bottleneck today is the amount of capex required to achieve
optimal mining. I am strongly in favor of PoW research that investigates
better PoW, but I do not think that any obvious strategies are known yet to
improve substantially on computation heavy hashcash.
-------------------------------------
Most of these are answered in the BIP, but for clarity:

Preferably no one decides. The company would have to exist prior to the
vote, and would need a public-facing website. In the event of contested
votes (meaning someone finds evidence of a fake company), the admin could
investigate and post results.

No.

I don't know. I'm happy to volunteer, if no one else wants to be
responsible for it. The only task would be adding the confirmed votes to
each respective BIP. From there, everything's public and can be confirmed
by everyone.

voting?
The logistics of doing that prevent it. But let's say 10 fake companies ...
first, you'd need to register ten domain names, host and customize the
website, all before the vote and in a way that no one would notice.

Someone at the company sends a very small transaction to the BIP's vote
address. Someone at the company then posts what the vote was and its
transaction ID on the company's blog/twitter, etc., and then emails the URL
to the administrator.

actually the correct person?
They post it on their company blog.

On Fri, Feb 3, 2017 at 11:19 AM, alp alp via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
bitcoin in the background on affordable, non-dedicated home-hardware should
be a top consideration.

Why is that a given?  Is there math that outlines what the risk levels are
for various configurations of node distributions, vulnerabilities, etc?
How does one even evaluate the costs versus the benefits of node costs
versus transaction fees?

being the second most significant problem, and finally bandwidth
consumption as the third most important consideration. I believe that v0.14
is already too expensive on all three fronts, and that block size increases
shouldn't be considered at all until the requirements are reduced (or until
consumer hardware is better, but I believe we are talking 3-7 years of
waiting if we pick that option).

Disk space is not the largest cost, either today or in the future.  Without
historical checkpointing in some fashion, bandwidth costs are more than 2
orders of magnitude higher cost than every other cost for full listening
nodes.  With historical syncing discounted(i.e. pruned or nonlistening
nodes) bandwidth costs are still higher than hard drive costs.


Today: Full listening node, 133 peers, measured 1.5 TB/mo of bandwidth
consumption over two multi-day intervals.  1,500 GB/month @ ec2 low-tier
prices = $135/month, 110 GB storage = $4.95.  Similar arguments extend to
consumer hardware - Comcast broadband is ~$80/mo depending on region and
comes with 1.0 TB cap in most regions, so $120/mo or even $80/mo would be
in the same ballpark.  A consumer-grade 2GB hard drive is $70 and will last
for at least 2 years, so $2.93/month if the hard drive was totally
dedicated to Bitcoin and $0.16/month if we only count the percentage that
Bitcoin uses.

For a non-full listening node, ~25 peers I measured around 70 GB/month of
usage over several days, which is $6.3 per month EC2 or $5.6 proportional
Comcast cost.  If someone isn't supporting syncing, there's not much point
in them not turning on pruning.  Even if they didn't, a desktop in the $500
range typically comes with 1 or 2 TB of storage by default, and without
segwit or a blocksize cap increase, 3 years from now the full history will
only take up the 33% of the smaller, three year old, budget-range PC hard
drive.  Even then if we assume the hard drive price declines of the last 4
years hold steady(14%, very low compared to historical gains), 330gb of
data only works out to a proportional monthly cost of $6.20 - still
slightly smaller than his bandwidth costs, and almost entirely removable by
turning on pruning since he isn't paying to help others sync.

I don't know how to evaluate the impacts of RAM or CPU usage, or
consequently electricity usage for a node yet.  I'm open to quantifying any
of those if there's a method, but it seems absurd that ram could even
become a signficant factor given the abundance of cheap ram nowadays with
few programs needing it.  CPU usage and thus electricity costs might become
a factor, I just don't know how to quantify it at various block scales.
Currently cpu usage isn't taxing any hardware that I run a node on in any
way I have been able to notice, not including the syncing process.

good move, even as little as SegWit does.

The consequence of your logic that holds node operational costs down is
that transaction fees for users go up, adoption slows as various use cases
become impractical, price growth suffers, and alt coins that choose lower
fees over node cost concerns will exhibit competitive growth against
Bitcoin's crypto-currency market share.  Even if you are right, that's
hardly a tradeoff not worth thoroughly investigating from every angle, the
consequences could be just as dire for Bitcoin in 10 years as it would be
if we made ourselves vulnerable.

And even if an altcoin can't take Bitcoin's dominance by lower fees, we
will not end up with millions of home users running nodes, ever.  If they
did so, that would be orders of magnitude fee market competition, and
continuing increases in price, while hardware costs decline.  If
transaction fees go up from space limitations, and they go up even further
in real-world terms from price increases, while node costs decline,
eventually it will cost more to send a transaction than it does to run a
node for a full month.  No home users would send transactions because the
fee costs would be higher than anything they might use Bitcoin for, and so
they would not run a node for something they don't use - Why would they?
The cost of letting the ratio between node costs and transaction costs go
in the extreme favor of node costs would be worse - Lower Bitcoin
usability, adoption, and price, without any meaningful increase in security.

How do we evaluate the math on node distributions versus various attack
vectors?



On Wed, Mar 29, 2017 at 8:57 AM, David Vorick via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Sorry, I'm still missing it...
So your claim is that a) ignoring incoming messages of a type you do not recognize is bad, and thus b) we should be disconnecting/banning peers which send us messages we do not recognize (can you spell out why? Anyone is free to send your host address messages/transactions they are generating/etc/etc, we don't ban nodes for such messages, as that would be crazy - why should we ban a peer for sending us an extra 50 bytes which we ignore?), and thus c) this would be backwards incompatible with software which does not currently exist?

Usually "backwards incompatible" refers to breaking existing software, not breaking theoretical software. Note that, last I heard, BIP 151 is still a draft, if such software actually exists we can discuss changing it, but there are real wins in sending these messages before VERSION.

On February 13, 2017 12:17:11 PM GMT+01:00, Eric Voskuil <eric@voskuil.org> wrote:

-------------------------------------
Erik,

On Tue, May 9, 2017 at 3:58 AM, Erik Aronesty <erik@q32.com> wrote:

I'm not entirely sure what you mean, but right now you can make an
arbitrary chain of challenges, and the BIP includes methods for
determining an approximate time to solve (nodes will, at the very
least, discard any challenge which will on average take longer time to
solve than the expiration of the challenge itself, for example, i.e.
the "nope too hard" part).


Others mentioned this approach. I haven't given it much thought.
Admittedly it would be an effective way to prevent DoS but it also has
some unwanted side effects that need to be cleared up (e.g. in a
no-gains scenario like the BIP proposes, the node requesting PoW done
doesn't *gain* anything from lying to the node performing the work).

-------------------------------------
You can implement this already, but only for ~1 year expirations.

IF <normal script> ELSE <1 year> CHECKSEQUENCEVERIFY ENDIF

Perhaps it would make sense to propose a flag extending the range of relative 
lock-times so you can do several years?

Luke


On Monday 11 December 2017 5:30:37 PM Teweldemedhin Aberra via bitcoin-dev 
wrote:

-------------------------------------
On Tuesday 05 December 2017 7:24:04 PM Sjors Provoost wrote:

This seems counterproductive. There is no reason to ever avoid the RBF flag. 
I'm not aware of any evidence it even reduces risk of, and it certainly 
doesn't prevent double spending. Plenty of miners allow RBF regardless of the 
flag, and malicious double spending doesn't benefit much from RBF in any case.


Bech32 addresses are just normal addresses. What need is there for a param?

Luke

-------------------------------------

I cannot imagine the benefit to replicating an ip address in this case,
except maybe you think that you would be more likely to be selected as a
peer?   But there would be no actual advantage since download peers are
selected based on throughput and actual blocks served.

Also, since this makes the network far more resistant to DDOS attacks, it
has added benefits.


I agree, if lightning networks were baked in, then the tips could be as
granular as "per block downloaded", or even (outlandish seeming now, but
maybe not in a future where there is a "public rpc api") "per rpc call".
Miners and business users would certainly pay for high quality services.
Spinning up new nodes without a tip and relying on the "free network" would
probably take more time, for example.

I suspect that if income were even a small possibility the number of full
nodes would vastly increase.

Sybil attacks seem irrelevant as long as reasonable QOS metrics are stored
per peer.


On Wed, May 3, 2017 at 5:53 PM, Gregory Maxwell <greg@xiph.org> wrote:

-------------------------------------

Can you explain this statement a little better?  What do you mean by
that?  What does the total bitcoins transacted have to do with
hashpower required?

On Fri, Mar 31, 2017 at 2:22 PM, Matt Corallo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
In Drivechain, 51% of miners have total control and ownership over all of the sidechain coins. The vision of Drivechain is to have many blockchains "plugged in" to the main chain.

Today, well over 51% of miners are under the jurisdiction of a single government.

Thus the effect of Drivechain appears to be the creation of a new kind of digital border imposed onto the network where everyone hands over ownership of their Bitcoins to a /single/ mining cartel when they wish to interact with /any/ sidechain.

Drivechain would be a reasonable idea if that weren't the case, but since it is, Drivechain now introduces a very real possible future where Bitcoins can be confiscated by the Chinese government in exactly the same manner that the Chinese government today confiscates financial assets in other financial networks within China.

One argument is that the psuedo-anonymity granted by Bitcoin addresses could theoretically make this less likely to happen, and while that is true, it is also true that every day Bitcoin becomes less and less psuedo-anonymous as governments invest in blockchain analysis tech [1].

How many high-profile confiscations — now via the Bitcoin-blockchain itself (no longer being able to blame a 3rd-party exchange) — is Bitcoin willing to tolerate and open itself up to?

In a world before Drivechain: the worst that a 51% coalition can do is prevent you from spending your coins (censorship).

In a world with Drivechain three things become true:

1. The Bitcoin network centralizes more, because more power (both financial power and power in terms of capability/control) is granted to miners. This is likely to have secondary consequences on the main-chain network as well (in terms of its price and ability to evolve).

2. A 51% coalition — and/or the government behind it — is now able to financially profit by confiscating coins. No longer is it just censorship, "asset forfeiture" — theft — becomes a real possibility for day-to-day Bitcoin users.

3. Drivechain limits user's existing choice when it comes to who is acting as custodian of their Bitcoins, from any trustworthy exchange, down to a single mining cartel under the control of a single set of laws.

The argument given to this is that a UASF could be initiated to wrest control away from this cartel. I do not believe this addresses the concern at all.

A UASF is a very big deal and extremely difficult to pull off correctly. Even when you have users behind you, it *requires* significant support from the miners themselves [2]. Therefore, I do not believe a UASF is a realistic possibility for recovery.

I would only support Drivechain if all of these issue were addressed.

Kind regards,
Greg Slepak

[1] EU Commits €5 Million to Fund Blockchain Surveillance Research — http://www.coindesk.com/eu-commits-e5-million-fund-blockchain-surveillance-research/ <http://www.coindesk.com/eu-commits-e5-million-fund-blockchain-surveillance-research/>

[2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-June/014497.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-June/014497.html>

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
On Sunday 03 December 2017 9:32:15 PM Matt Corallo wrote:

This is impossible to guarantee. Federated sidechains are already possible 
with simple multisig (and they could very well be merge-mined chains instead 
of simply signed). At the same time, the value of permissionless sidechain 
innovation is potentially huge.

Luke

-------------------------------------
activate softforks is a good choice. The only two reliable signals we have
for this purpose in Bitcoin are block height (flag day) and standard miner
signaling, as every other metric can be falsified or gamed.

UASF can be just a flag day.

On Sat, Apr 15, 2017 at 9:23 AM, Natanael via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
USD per coin, nobody will build mining infrastructure to the same level as
today, with 5000 USD per coin.

In the case of bitcoin, it is the price that follows mining
infrastructures. The price is at 5000 because it is difficult to mine
bitcoin not the other way around, like you mention it. Even with a fixed
demand, price would go up as difficulty grow, the supply guide the market.
There is a strong incentive to mine blindly as it is difficult to estimate
for a miner where is the actual demand, with a start up currency without
actual economic support. Indeed at the genesis of this "mining-price" cycle
the incentive was to contribute to a network and create ones own supply,
and not respond to a demand.

Ilansky

Le 13 oct. 2017 13:55, <bitcoin-dev-request@lists.linuxfoundation.org> a
écrit :

-------------------------------------


I think you overestimated the difficulty. Consider this MAST branch (an example in BIP114)

"Timestamp" CHECKLOCKTIMEVERIFY <fake-pubkey> CHECKSIGVERIFY

This requires just a few bytes of collision.





-------------------------------------
On 07/09/17 06:29, Thomas Voegtlin via bitcoin-dev wrote:

What if we added another byte field OutputType for wallets that do not
follow BIP43?

0x00 - P2PKH output type
0x01 - P2WPKH-in-P2SH output type
0x02 - native Segwit output type

Would that work for you?

The question is whether this field should be present only if depth==0x00
or at all times. What is your suggestion, Thomas?

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
CTO, SatoshiLabs

-------------------------------------
Can you tell me where it is enforced?

The only place I found was here;
https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L1793

which doesn’t enforce it, all that code does is check that the txid is 
unknown or fully spent.
And since the below idea from Russel would change the txid, it would seem no 
full client would reject this.

Maybe its in a BIP, but I can’t find it in the code.

On Tuesday, 4 April 2017 16:59:12 CEST James Hilliard wrote:


-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
TL;DR (In layman terms): Refund any excess fee amounts higher than the lowest included fee in a block.

=== Proposed Hard Fork Change ===

LIFB: Lowest Included Fee/Byte

Change the fee policy to cause all fee/byte in a block to be reduced to the lowest included fee/byte. Change transactions to specify how/which outputs get what portions of [(TX_fee/TX_length - LIFB)*TX_length]. Transactions of course could still offer the remainder to the miner if they don't want to modify some of the outputs and don't want to reveal their change output.

=== Economic Analysis Of Why Such Is Desirable ===

Pure profit seeking miners attempt to fill their block with transactions that have the highest fee/byte. So what happens is the users who are willing to offer the highest fee/byte get included first in a block until it gets filled. At fill, there is some fee/byte where the next available tx in mempool doesn't get included. And right above that fee/byte is the last transaction that was selected to be included in the block, which has the lowest fee/byte of any of the transactions in the block.

Users who want to create transactions with the lowest fee watch the LIFB with https://bitcoinfees.21.co/ or similar systems... so that they can make a transaction that offers a fee at or above the LIFB so that it can be included in a block in reasonable time.

Some users have transactions with very high confirmation time sensitivity/importance... so they offer a fee much higher than the LIFB to guarantee quick confirmation. But they would prefer that even though they are willing to pay a higher fee, that they would still only pay the LIFB fee/byte amount.

This becomes even more of an issue when someone wants to create a transaction now that they want to be included in a block at a much later time... because it becomes harder and harder to predict what the LIFB will be as you try to predict further into the future. It would be nice to be able to specify a very high fee/byte in such a transaction, and then when the transaction is confirmed only have to pay the LIFB.

Users will look for the money that offers the greatest money transfer efficiency, and tx fees are a big and easily measurable component. So a money system is better if its users can pay lower fees than competing money options. Refund Excees Fee is one way to reduce fees.

=== Technical Difficulties ===

I realize this is a big change... and I'm not sure of the performance problems this might entail... I'm just throwing this idea out there. Of course if fees are very small, and there is little difference between a high priority fee/byte and the LIFB, then this issue is not really that big of a deal. Also... hard forks are very hard to do in general, so such a hard fork as this might not be worthwhile.

Cheers,
Praxeology Guy
-------------------------------------


Of course, that's why this is a last resort, successfully activated only in response to a contentious hard fork. If it succeeds just once it should help prevent the same situation occurring in the future.



How so? If you have four proof of work methods, that 50-75% of SHA256 hashpower would equate to 13-18% of total hashpower. If you can harm the network with this much hashpower bitcoin was DOA.

________________________________
From: Andrew Johnson <andrew.johnson83@gmail.com>
Sent: Monday, March 20, 2017 3:38:01 PM
To: Bitcoin Protocol Discussion; John Hardy
Subject: Re: [bitcoin-dev] Malice Reactive Proof of Work Additions (MR POWA): Protecting Bitcoin from malicious miners

By doing this you're significantly changing the economic incentives behind bitcoin mining. How can you reliably invest in hardware if you have no idea when or if your profitability is going to be cut by 50-75% based on a whim?

You may also inadvertently create an entirely new attack vector if 50-75% of the SHA256 hardware is taken offline and purchased by an entity who intends to do harm to the network.

Bitcoin only works if most miners are honest, this has been known since the beginning.

On Mon, Mar 20, 2017 at 9:50 AM John Hardy via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:

Im very worried about the state of miner centralisation in Bitcoin.

I always felt the centralising effects of ASIC manufacturing would resolve themselves once the first mover advantage had been exhausted and the industry had the opportunity to mature.

I had always assumed initial centralisation would be harmless since miners have no incentive to harm the network. This does not consider the risk of a single entity with sufficient power and either poor, malicious or coerced decision making. I now believe that such centralisation poses a huge risk to the security of Bitcoin and preemptive action needs to be taken to protect the network from malicious actions by any party able to exert influence over a substantial portion of SHA256 hardware.

Inspired by UASF, I believe we should implement a Malicious miner Reactive Proof of Work Additions (MR POWA).

This would be a hard fork activated in response to a malicious attempt by a hashpower majority to introduce a contentious hard fork.

The activation would occur once a fork was detected violating protocol (likely oversize blocks) with a majority of hashpower. The threshold and duration for activation would need to be carefully considered.

I dont think we should eliminate SHA256 as a hashing method and change POW entirely. That would be throwing the baby out with the bathwater and hurt the non-malicious miners who have invested in hardware, making it harder to gain their support.

Instead I believe we should introduce multiple new proofs of work that are already established and proven within existing altcoin implementations. As an example we could add Scrypt, Ethash and Equihash. Much of the code and mining infrastructure already exists. Diversification of hardware (a mix of CPU and memory intensive methods) would also be positive for decentralisation. Initial difficulty could simply be an estimated portion of existing infrastructure.

This example would mean 4 proofs of work with 40 minute block target difficulty for each. There could also be a rule that two different proofs of work must find a block before a method can start hashing again. This means there would only be 50% of hardware hashing at a time, and a sudden gain or drop in hashpower from a particular method does not dramatically impact the functioning of the network between difficulty adjustments. This also adds protection from attacks by the malicious SHA256 hashpower which could even be required to wait until all other methods have found a block before being allowed to hash again.

50% hashing time would mean that the cost of electricity in relation to hardware would fall by 50%, reducing some of the centralising impact of subsidised or inexpensive electricity in some regions over others.

Such a hard fork could also, counter-intuitively, introduce a block size increase since while were hard forking it makes sense to minimise the number of future hard forks where possible. It could also activate SegWit if it hasnt already.

The beauty of this method is that it creates a huge risk to any malicious actor trying to abuse their position. Ideally, MR POWA would just serve as a deterrent and never activate.

If consensus were to form around a hard fork in the future nodes would be able to upgrade and MR POWA, while automatically activating on non-upgraded nodes, would be of no economic significance: a vestigial chain immediately abandoned with no miner incentive.

I think this would be a great way to help prevent malicious use of hashpower to harm the network. This is the beauty of Bitcoin: for any road block that emerges the economic majority can always find a way around.

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
--
Andrew Johnson

-------------------------------------
I think standardization of this term is a great idea. I second all of
Jimmy's points. I think the analogy of dollars & cents to bits and satoshis
is easy to grasp, particularly given that satoshis and cents are the
smallest tangible units of their respective currencies. It's a concept
that's common across cultures and countries as it also applies to pounds
and pence, pesos and centavos, etc...

To David's points, I agree that it's not ideal that bit is a homonym for
other words, but I don't think it's a terrible flaw as context will usually
make the meaning clear. I'm actually not in love with the term "bit," but
rather the idea of a non-SI term for a millionth of a bitcoin. But bit has
already caught on to some extent and I can't think of anything better.




I find the SI prefixes to be very user unfriendly. I have plenty of smart
friends and family who constantly confuse mega, giga, micro, nano, and so
on. Rather than try to train users, I think we should choose terms that
will be easy for them to grasp right away. Even for people fluent in SI
terms, I think some of the problems regarding unit bias still exist. 500
microbitcoins sounds diminutive and uttering it is a reminder that it's a
very small fraction of a larger unit. 500 bits sounds like you have 500 of
something, neat!

I consider "bits" to be a term that's quite future proofed. While I won't
dismiss the possibility of $10M or $100M bitcoins in the not-too-distant
future, there would still be plenty of time for a bit to be a useful
day-to-day unit. Even at the $10M point, small ticket items like coffee
could still be priced at 0.30 bits for example, not bad I'd say.

Should bitcoin ever soar past the $100M mark, it might be time for a new
term akin to bits and maybe a hard fork to allow for more decimal places on
chain. A nanobitcoin could not be transacted with today anyhow. These would
all be good problems to have.

Thanks for reading and thanks to Jimmy for taking the initiative with this
BIP.

Daniel
-------------------------------------
I see further arguments supporting the “bit" denomination:

huge benefit:
	- amounts denominated in bits fit nicely into legacy database structures and UIs with two decimal places for currency. This change to the usual currrency precision is a huge benefit for integration into existing financial software.

nice to have:
	- while fraction prefixes m for 1/1000 and u for 1/1000000 are obvious to engineers and geeks, they are a foreign concept to many. Chances confusing magnitudes would be high if alternative scales were offered.
	- bit assigns an easy to comprehend meaning to the second part of the Bitcoin name: I think the term “(a whole) coin" would quickly catch on as a synonym for a million bits.


Tamas Blummer
-------------------------------------
On Thu, Nov 2, 2017 at 11:53 PM, Scott Roberts <wordsgalore@gmail.com> wrote:

I urge my colleagues here to not fall for the obvious xkcd386 bait.

The competitive advantage of prudence and competence is diminished if
competitors are able to divert our efforts into reviewing their
proposals.

-------------------------------------
On 05/09/17 12:25, Thomas Voegtlin via bitcoin-dev wrote:

I used this argument for mnemonic/seed, not xpub/xprv. I am fine with
this proposal of yours, so don't worry.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
CTO, SatoshiLabs

-------------------------------------
# Jacob Eliosoff:

split.

Correct.  There are 2 short activation periods in BIP91 either of which
would avoid a split.

# Gregory Maxwell:


This is the relevant pull req to core:

https://github.com/bitcoin/bitcoin/pull/10444

Seems OK.  It's technically running now on testnet5.   I think it (or a
-bip148 option) should be merged as soon as feasible.


apples vs oranges, imo.   segwit is not a contentious feature.   the
"bundling" in segwit2x is, but that's not the issue here.   the issue is we
are indirectly requiring miners that strongly support segwit to install
consensus protocol changes outside of bitcoin's standard reference.   80%
of them have signaled they will do so.   these are uncharted waters.


On Tue, Jun 20, 2017 at 6:57 PM, Jacob Eliosoff via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Strongly disagree with buying "votes", or portraying open standards as a 
voting process. Also, this depends on address reuse, so it's fundamentally 
flawed in design.

Some way for people to express their support weighed by coins (without 
losing/spending them), and possibly weighed by running a full node, might 
still be desirable. The most straightforward way to do this is to support 
message signatures somehow (ideally without using the same pubkey as 
spending), and some [inherently unreliable, but perhaps useful if the 
community "colludes" to not-cheat] way to sign with ones' full node.

Note also that the BIP process already has BIP Comments for leaving textual 
opinions on the BIP unrelated to stake. See BIP 2 for details on that.

Luke


On Thursday, February 02, 2017 7:39:51 PM t. khan via bitcoin-dev wrote:

-------------------------------------
I would be fine with that, since segwit is widely deployed on the
network already a lower activation threshold should be safe.

On Wed, May 24, 2017 at 12:02 PM, Wang Chun <1240902@gmail.com> wrote:

-------------------------------------
On 04/09/2017 05:20 PM, Erik Aronesty via bitcoin-dev wrote:

It's actually the best thing to use an ASIC tightly coupled with DRAM
for - for example, HBM and HBM2 which reduce latency and increase
throughput by placing the DRAM on an interposer with the ASIC die, or
even putting the logic on the DRAM die itself.

It would need at least proof that existing chips using HBM are ideal for
Cuckoo Cycle (unlikely) and that no DRAM manufacturer could ever be
coaxed into making an ASIC (even harder to guarantee).

I think any long term PoW change is irrelevant to the review or adoption
of the covert ASICBOOST BIPs, given the many unresolved problems of such
a change.

-------------------------------------
I have been working on filesystem based on Mandelbrots formula, and it seems
that it is a prime candidate for extending the bitcoin algorithm.

 

Would like to run some of this by you folks. 

 

The key is that the Mandelbrot workers could help to modify the verification
work so that the brute force work in the current hash.

 

It would allow the hash results to reign in the next iteration.

 

Also the hash results could help to build a fast lookup table of future
variations of the block chain.

 

 

-------------------------------------
On Tuesday, 28 March 2017 18:59:32 CEST Wang Chun via bitcoin-dev wrote:
...

I think that is a very smart idea, thank you for making it.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------


On Sat, Apr 8, 2017, at 20:27, Tom Harding via bitcoin-dev wrote:













I am not quite sure why you think this approach would help in this
regard. I may be missing part of how Core works here, but Bitcrust's
txindex is merely used to lookup transactions from hashes and currently,
and seems to  fulfil the same role  as Core's -txindex  mode.


This can be pruned, and in the future auto-pruned as the "flat files"
used as base for all data allow for concurrent pruning. But unlike Core,
it is always needed as without UTXO index, it is needed to find outputs
during base load validation.

-------------------------------------
Hi James:


ڵҪǷЭǲܱʲôǴжֵѡ
The node should decide to follow the protocol upgrade or not. But they cant just be passive and do nothing. The choice is always provided.

ǲŴĿ󹤣ǿѡһЩûп󹤽ɫҡ
If they dont trust the choice of majority miners, they can use some alt coin that dont including miners part.


ӦУ飬Ϊ֤ SPY ڿĿṩһˡ
We should introduce block validation in the code, but how to provide incentive to no-validating SPY miner is another problem.



ҲȫݡڴҶϿɡ߶xxxִxxxʱҲΪҪ
I dont see them as completely backwards compatible. since I dont see that is important if we all agree with after block height xxx, then xxx.
ȻԴӴ鿪ʼһֱ֤졣
And we can validate from the genesis block till today.



Ȼ45% ԷͼźţԺͲֽڵ㹲ıǵǰѾڵ⣬û˸⡣
Of cause, there could be false signal from 45% and have conspiracy with some nodes. But thats the problem we have today, and it dont get any worse (nor any better).



-------------------------------------
On 02/13/2017 02:07 AM, Jonas Schnelli via bitcoin-dev wrote:

No. This is what I was referring to. These messages are enabled by
protocol version. If they are received by a node below the version at
which they are activated, they are unknown messages, implying an invalid
peer. The above messages cannot be sent until *after* the version is
negotiated. BIP151 violates this rule by allowing the new control
message to be sent *before* the version handshake.


There are already nodes out there breaking connections based on the BIP.


Yes, the ordering of the messages. New messages can only be added after
the handshake negotiates the higher version. Otherwise the handshake is
both irrelevant (as Pieter is implying) and broken (for all existing
protocol versions).



You may be more familiar with non-validating peers. If a message type is
not known it is an invalid message and the peer is immediately dropped.
We started seeing early drops in handshakes with bcoin nodes because of
this issue.


Sure, a peer can do what it wants. It can send photos. But I'm not sure
what makes you think it would be correct to maintain the connection when
an *invalid* message is received.

different for messages specified in BIP151?

Because it properly validates the protocol.

More than that it supports a configurable protocol range. So by setting
the min protocol (below which the node won't connect) and the max
protocol (at which it desires to connect) we can observe the behavior of
the network at any protocol levels (currently between 31402 and 70013).
This is very helpful for a development stack as it allows one to easily
test against each protocol level that one wishes to support.

e

-------------------------------------
If / when Schnorr signatures are deployed in a future witness version, it
may be possible to have non-interactive partial aggregation of the
signatures on a per-block basis.  This could save quite a bit of space.  It
*seems* not to have any security problems but this mailing list is very
good at finding vulnerabilities so that type of feedback is the main reason
I'm writing :) (A quick explanation of why this is horribly broken could
save me lots of time!)
(also sorry if this has been discussed; didn't see anything)

Quick recap / context of Schnorr sigs:

There are a bunch of private keys x1, x2, x3...
multiply by generator G to get x1G = P1, x2G = P2, x3G = P3

Everyone makes their sighash m1, m2, m3, and their random nonces k1, k2, k3.

To sign, people calculate s values:

s1 = k1 - h(m1, R1, P1)x1
s2 = k2 - h(m2, R2, P2)x2

(adding the P2 into the e hash value is not in most literature /
explanations but helps with some attacks; I beleive that's the current
thinking.  Anyway it doesn't matter for this idea)

Signature 1 is [R1, s1].  Verifiers check, given P1, m1, R1, s1:

s1G =? R1 - h(m1, R1, P1)P1

You can *interactively* make aggregate signatures, which requires
co-signers to build an aggregate R value by coming up with their own k
values, sharing their R with the co-signers, adding up the R's to get a
summed R, and using that to sign.

Non-interactively though, it seems like you can aggregate half the
signature.  The R values are unique to the [m, P] pair, but the s's can be
summed up:

s1 + s2 = k1 + k2 - h(m1, R1, P1)x1 - h(m2, R2, P2)x2

(s1 + s2)G = R1 + R2 - h(m1, R1, P1)P1 - h(m2, R2, P2)P2

To use this property in Bitcoin, when making transactions, wallets can sign
in the normal way, and the signature, consisting of [R, s] goes into the
witness stack.  When miners generate a block, they remove the s-value from
all compatible inputs, and commit to the aggregate s-value in the coinbase
transaction (either in a new OP_RETURN or alongside the existing witness
commitment structure).

The obvious advatage is that signatures go down to 32 bytes each, so you
can fit more of them in a block, and they take up less disk and network
space.  (In IBD; if a node maintains a mempool they'll need to receive all
the separate s-values)

Another advatage is that block verification is sped up.  For individual
signatures, the computation involves:

e = h(m1, R1, P1)           <- hash function, super fast
e*P                         <- point multiplication, slowest
R - e*P                     <- point addidion, pretty fast
s*G                         <- base point multiplication, pretty slow

with s-aggregate verification, the first three steps are still carried out
on each signature, but the s*G operation only needs to be done once.
Instead another point addition per signature is needed, where you have some
accumulator and add in the left side:
A += R - e*P
this can be parallelized pretty well as it's commutative.

The main downside I can see (assuming this actually works) is that it's
hard to cache signatures and quickly validate a block after it has come
in.  It might not be as bad as it first seems, as validation given chached
signatures looks possible without any elliptic curve operations.  Keep an
aggregate s-value (which is a scalar) for all the txs in your mempool.
When a block comes in, subtract all the s-values for txs not included in
the block.  If the block includes txs you weren't aware of, request them in
the same way compact blocks works, and get the full signature for those
txs.  It could be several thousand operations, but those are all bigInt
modular additions / subtractions which I believe are pretty quick in
comparison with point additions / multiplications.

There may be other complications due to the fact that the witness-txids
change when building a block.  TXIDs don't change though so should be
possible to keep track of things OK.

Also you can't "fail fast" for the signature verification; you have to add
everything up before you can tell if it's correct.  Probably not a big deal
as PoW check comes first, and invalid blocks are pretty uncommon and quite
costly.

Would be interested to hear if this idea looks promising.
Andrew Polestra mentioned something like this in the context of CT /
mimblewimble transactions a while ago, but it seems it may be applicable to
regular bitcoin Schnorr txs.

-Tadge
-------------------------------------
Just to be clear, the tagging would occur on the addresses, and the
weighting would be by value, so it's a measure of economic significance.
Major exchanges will regularly tag massive amounts of Bitcoins with their
capabilities.

Just adding a nice bit-field and a tagging standard, and then charting it
might be enough to "think about how to use it later".   The only problem
would be that this would interfere with "other uses of op_return" ...
colored coins, etc.

Personally, I think that's OK, since the purpose is to tag economically
meaningful nodes to the Bitcoin ecosystem and colored coins, by definition,
only have value to "other ecosystems".

(Counterargument: Suppose in some future where this is used as an
alternative to BIP9 for a user-coordinated code release - especially in
situations where miners have rejected activation of a widely-regarded
proposal.  Suppose also, in that future, colored coin ICO's that use
op-return are regularly used to float the shares of major corporation.  It
might be irresponsible to exclude them from coordinating protocol changes.)





On Tue, Apr 18, 2017 at 10:52 AM, Marcel Jamin <marcel@jamin.net> wrote:

-------------------------------------
I've gotten feedback from Adam Back that you actually don't need all 32
bits in the header for overt ASICBoost, so I'm modifying my proposal. Of
the 32-bit version field, bits 16 to 23 are reserved for miners, the
witness commitment stays as defined in BIP-141 except that it's now
required. BIP9 then is modified so that bits 16 to 23 are now no longer
usable.

On Fri, Apr 7, 2017 at 3:06 PM, Jimmy Song <jaejoon@gmail.com> wrote:

-------------------------------------
On Tue, Sep 12, 2017 at 4:49 AM, Sergio Demian Lerner via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

I agree with your post, but wanted to make a point of clarification on
the use of "can't".

If someone wants to report something to the Bitcoin project we're
obviously at your mercy in how we handle it. If we disagree on the
handling approach we may try to talk you into a different position
based with a rational judgement based on our experience (or, if
justified, advice that we're likely to whine about your approach in
public). But if you still want to go also report a common issue to
something else with a different approach then you can. Even our
ire/whining can be avoided by a sincere effort to communicate and give
us an opportunity to mitigate harm.

That said, as mentioned, we'd encourage otherwise for issues that
warrant it-- and I think with cause enough that the reporter will
agree. So that is a different kind of "cant". :)

In Bitcoin the overwhelming majority of serious issues we've
encountered have been found by people I'd consider 'inside the
project' (frequent regular contributors who aren't seriously involved
in other things).  That hasn't been so obviously the case for other
open source projects that I've been involved with; but Bitcoin is
pretty good from a basic security perspective and finding additional
issues often requires specialized experience that few people outside
of the project regulars have (though some, like Sergio, clearly do).

I know through direct experience that both Mozilla and the Chrome
project fix _serious_ (like RCE bugs) issues based on internal
discoveries which they do not make public (apparently ever), though
they may coordinate with distributors on some of them.   (Some of
these experiences are also why I give the advice that you should not
consider any computer which has ever run a web browser to be strongly
secure...)

-------------------------------------
On Fri, May 5, 2017 at 6:24 AM, Tomas via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


see also:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011853.html

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
On Tue, Mar 7, 2017 at 1:28 PM, praxeology_guy via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


A big yuck on that format. It should be something based on a patricia trie
to support incremental updates. Also if the things being stored are output
transaction + output number then those two can be hashed together to make a
consistent size identifier and be put into the merkle set format I
proposed, which is exactly the intended usage:
https://github.com/bramcohen/MerkleSet

- We could make BCP 4383 blocks, which would be 12 times per year, given

If it's of that order it should be synched up with the work difficulty
reset interval of 2016. If the format supports incremental updates it would
of course be possible to require them more frequently later.



With a patricia based format it would be possible to make much more common
utxo commitments, possibly as often as every block only trailing by a few,
and the cost of updating wouldn't be onerous and reorgs could be handled by
simply undoing the last few transactions in the set and and then rolling
forward.
-------------------------------------
What you describe is not a fix of replay attack. By confirming the same tx in both network, the tx has been already replayed. Their child txs do not matter.


-------------------------------------
That would invalidate any pre-signed transactions that are currently out there. You can't just change the rules out from under people.


-------------------------------------


On 5/23/2017 5:51 AM, Tier Nolan via bitcoin-dev wrote:

I'm not sure it makes much of a difference. First of all, in point of
fact, the miners themselves own the coins from the coinbase. But more
importantly, even if miners did not explicitly own the coins, they might
profit by being bribed -- these bribes would come from people who did
own the coins.

The principle is that value "v' has been taken from A and given to B.
This is effectively coercive activity, and therefore itself has value
proportional to 'v'.


Yes. The more value _on_ the sidechain, the more abhorrent the malfeasance.


What do you think of my argument, that we already labor under such an
assumption? An attacker could pay fees today equal to greater than
sum(blockreward_(last N block)). According to you this would force a
reorg, even on mainchain (pre-sidechain) Bitcoin. Yet this has never
happened.

It seems that this argument fully reduces to the "what will happen when
the block subsidy falls to zero" question.


See above (?) for why I still feel they are comparable, if not identical.


Yes. Sidechains have newer, more interesting features, and
simultaneously more risk.



Again, I think that we _already_ need to eliminate any assumption of
"charitable miners".


Yes, we may see interesting behavior where people buy up these
liabilities using the LN. In my original post, I mention that miners
themselves may purchase these liabilities (at competitive rates, even if
these arent the idealized 1:1). At this point, miners would be paying
themselves and there would be no agency problem.


If you haven't seen http://www.truthcoin.info/blog/drivechain/ , that is
probably the most human-readable description.

Cheers,
Paul

-------------------------------------
The bitcoin ecosystem is better off the more transactions are propogated
peer-to-peer than directly to miners. We want miners listening to the
network, not soliciting transactions directly from users. You whispering
your transactions to your miner-of-choice while I whisper to mine
contravenes a critical value-add of the peer-to-peer network in my opinion.

On Tue, Mar 28, 2017 at 10:35 AM Martin Stolze via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
In that case, you may as well remove all references to leaves and double
SHA-256 from your BIP since your design has no method for distinguishing
between internal nodes and leaves.

I think that if this design stands, it will play a role in some future
CVEs.  The BIP itself is too abstract about its data contents to
specifically say that it has a vulnerability; however, I believe it is
inviting vulnerabilities.
For example, I might agree with a counterparty to a design of some sort of
smart contract in the form of a MAST.  My counterparty has shown me all the
"leaves" of our MAST and I can verify its Merkle root computation.
After being deployed, I found out that one of the leaves wasn't really a
leaf but is instead a specially crafted "script" with a fake pubkey chosen
by my couterparty so that this leaf can also be interpreted as a fake
internal node (i.e. an internal node with a right branch of 0x8000...100).
Because the Fast Merkle Tree design doesn't distinguish between leaves and
internal nodes my counter party gets away with building an Inclusion Proof
through this "leaf" to reveal the evil code that they had designed into the
MAST at a deeper level.

Turns out my counterparty was grinding their evil code to produce an
internal node that can also be parsed as an innocent script.  They used
their "pubkey" to absorb excess random data from their grinding that they
cannot eliminate.
(The counterparty doesn't actually know the discrete log of this "pubkey",
they just claimed it was their pubkey and I believed them).


Having ambiguity about whether a node is a leaf or an internal node is a
security risk. Furthermore, changing the design so that internal node and
leaves are distinguishable still allows chained invocations.
Arbitrary data can be stored in Fast Merkle Tree leaves, including the
Merkle root of another Fast Merkle Tree.
Applications that are limited to proof with paths no longer than 32
branches can still circumvent this limit by staging these Fast Merkle Trees
in explicit layers (as opposed to the implicit layers with the current
design).

By storing a inner Fast Merkle Tree root inside the (explicit) leaf of an
outer Fast Merkle Tree, the application can verify a Inclusion Proof of the
inner Fast Merkle Tree Root in the outer Fast Merkle Tree Root, and then
verify a second Inclusion Proof of the desired data in the inner Faster
Merkle Tree Root.  The application will need to tag their data to
distinguish between inner Fast Merkle Tree Roots and other application
data, but that is just part of the general expectation that applications
not store ambiguous data inside the leaves of Fast Merkle Trees.


On Wed, Sep 6, 2017 at 10:20 PM, Mark Friedenbach <mark@friedenbach.org>
wrote:

-------------------------------------
Hello Devs,

I am Patrick Sharp. I just graduated with a BS is computer science. Forgive
my ignorance.

As per bip-0002 I have scoured each bip available on the wiki to see if
these ideas have already been formally proposed and now as per bip-0002
post these ideas here.

First and foremost I acknowledge that these ideas are not original nor new.

Side Chains:

Bip-R10 offers a mechanism to assign custody or transfer coins from one
chain to another. However I did not find a bip that proposed a formal
bitcoin side chain.

My proposal

   - They are officially supported, tracked and built by official bitcoin
   software meaning that they are not an external chain
   - each chain has an identifier in the block header i.e. main chain: 0,
   first chain: 1, second chain: 2...
   - the number of chains including the main chain that exists is always a
   power of 2, this power will also be included in the block header.
   - each address is assigned to a chain via chain = (address) mod (number
   of chains)
      - to be valid an addresse's next transaction will first send their
      coins to their chain if they are not already there
      - if the address they are sending to is outside their chain their
      transaction will be submitted to both chains and transaction fee will be
      split between chains
   - They come into being via a fork or split
      - every 2016 blocks (upon recalculation of difficulty) if some
      percentage (lets say 10%) of blocks on any chain are larger than some
      specified amount (lets say 750 KB) then all chains are called to
increment
      their power value and fork on their block.
         - miner of chain x creates genesis block for chain x+2^previous
         power
         - upon fork, the difficulty of the old chain and the new chain
         will be half the next difficulty
      - if every chain has gone 2016 block without surpassing some amount
      (lets say 250 KB) at least some percentage of the time (lets say 10%) all
      chains will be called to join, decrement their power and double their
      difficulty
         - given miner of chain x, if x not less than 2^new power, chain
         will be marked dead or sleeping
         - miners who mine blocks on the chain that was joined (the chain
         with the smaller identifier) may have to make a block for the sleeping
         chain if transactions include funds that fully or partially
originate from
         the sleeping chain
         - dead chain are revived on next split.
      - each block's reward outside of transaction fees will be the
      (current bounty / 2^fork power) except obviously for dead blocks who's
      reward is already included in their joined block
   - benefits
      - dynamically scales to any level of usage, no more issues about
      block size
      - miners have incentive to keep all difficulties close to parity
      - if miners are limited by hard drive space they don't have to mine
      every chain (though they should have trusted peers working on
other chains
      to verify transactions that originate off their chains, faulty block will
      still be unaccepted by the rest of the miners)
      - though work will still grow linearly with the number of chains due
      to having to hash each separate header, some of the overhead may remain
      constant and difficulty and reward will still be balanced.
      - transactions are pseudo equally distributed between chains.
      - rewards will be more distributed (doesn't' really matter, except
      that its beautiful)
   - cons
      - because most transactions will be double recorded the non-volatile
      memory foot print of bitcoin doubles (since miners do not need
all chains i
      believe this solution not only overcomes this cost but may decrease the
      foot print per miner in the long run overall)
      - transactions will hang in limbo until both chains have picked them
      up, a forever limboed transaction could result in lost coins, as
long as a
      transaction fee has been included this risk should be mitigated.

I believe this idea is applicable to the entire community. I would like
your thoughts and suggestions. I obviously think this is a freaking awesome
idea. I know it is quite ambitious but it is the next step in evolution
that bitcoin needs to take to be a viable competitor to visa.

I come to you to ask if this has any chance of acceptance.

-Patrick
-------------------------------------
10% say literally never.  That seems like a significant disenfranchisement
and lack of consensus.

On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Friday, 14 April 2017 18:50:47 CEST praxeology_guy via bitcoin-dev wrote:

Here is a list of clear alternatives;

https://github.com/bitcoin/bips/

See the BIPs with number 010[1-8].

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel

-------------------------------------
On Sun, Mar 26, 2017 at 2:05 PM, Peter R via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


False. With bip9-based soft-fork-based activation of segwit, miner blocks
will not be orphaned unless they are intentionally segwit-invalid (which
they currently are not). If you have told miners otherwise, let me know.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
I agree the date can be brought forward. FWIW, I originally set the date far out enough that people wouldn't immediately fixate on the date and rather look at the meat of the proposal instead.

Given that we saw around 70% of nodes upgrade to BIP141 in around 5/6 months, I dont see any reason why we cant reduce the date to being 6 months or less from Nov. Given people are starving for segwit to the point of running BIP148, there is good evidence the community will upgrade in record time to BIP149.

Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.

-------- Original Message --------

Based on how fast we saw segwit adoption, why is the BIP149 timeout so
far in the future?

It seems to me that it could be six months after release and hit the
kind of density required to make a stable transition.

(If it were a different proposal and not segwit where we already have
seen what network penetration looks like-- that would be another
matter.)
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------



It is clear that charging min fee won't maximize total miner's fees because it ignores heterogeneity in willingness to pay among bidders within the same block. Also, spamming can still occur by setting a large number of transactions to the min fee. Competition between spammers might drive up the min fee, which is really where the positive effect comes from in this model.


A two-part pricing scheme involving a fixed fee per transaction plus variable fee per byte is likely to work much better. The fixed fee component raises the cost of micro-transactions substantially and deters spamming of the mempool.  Also, revenue is not lost from people with higher willingness to pay.


Chenxi

________________________________
From: William Morriss <wjmelements@gmail.com>
Sent: Wednesday, November 29, 2017 11:05 PM
To: Chenxi Cai
Cc: Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] BIP Idea: Marginal Pricing

On Wed, Nov 29, 2017 at 9:52 PM, Chenxi Cai <Chenxi_Cai@live.com<mailto:Chenxi_Cai@live.com>> wrote:

Hi All,


Auction theory is a well-studied problem in the economics literature. Currently what bitcoin has is Generalized first-price auction, where winning bidders pay their full bids. Alternatively, two approaches are potentially viable, which are Generalized second-price auction and VickreyClarkeGroves auction. Generalized second-price auction, where winning bidders pay their next highest bids, reduces (but not eliminate) the need for bidders to strategize by allowing them to bid closer to their reservation price. VickreyClarkeGroves auction, a more sophisticated system that considers all bids in relation to one another, elicit truthful bids from bidders, but may not maximize miners' fees as the other two systems will.


Due to one result called Revenue Equivalence, the choice of fee design will not impact miners' fees unless the outcomes of the auction changes (i.e, the highest bidders do not always win). In addition, the sole benefit of second-price auction over first-price auction is to spare people's mental troubles from strategizing, rather than actually saving mining fees, because in equilibrium the fees bidders pay remain the same. Therefore, in balance, I do not see substantial material benefits arising from switching to a different fee schedule.


Best,

Chenxi Cai


Changing the bidding system to the marginal price allows us to supersede the block size limit, which changes the outcome of the auction, as different transactions are included.
-------------------------------------
On Tue, Jun 6, 2017 at 10:39 PM, Tao Effect via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Please don't insult our community-- the issues with replay were
pointed out by us to Ethereum in advance and were cited specifically
in prior hardfork discussions long before Ethereum started editing
their ledger for the economic benefit of its centralized
administrators.

The lack of extensive discussion on these issues you're seeing is
rather symptomatic of engineers that take stability seriously not
taking BIP148 seriously; not symptomatic of people not knowing about
them. The same concerns also applies to all these HF proposals (which
for some reason you don't mention), arguably even stronger.  The same
basic pattern exists: There are people that just don't care about the
technical issues who have made up their minds, and so you don't see
technical discussion.  Those people who do see the issues already
called out the proposals as being ill-advised.   Replay isn't even the
largest of the technical issues (network partitioning, for example, is
a much larger one).

BIP149 is arguably something of another matter in particular because
it has a time-frame that allows dealing with replay and other issues--
and particularly because it has a time-frame that can allow for the
avoidance of a meaningful fork at all.

-------------------------------------
On Mon, Jun 12, 2017 at 2:46 PM, Andrew Miller via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

I'm glad to see this out now, so I'm not longer invading the git repo
uninvited. :)



The description in the BIP appears inadequate:



For example, it's not clear if I can query for the existence of a
transaction by sending a conflict.  If this doesn't seem problematic,
consider the case where I, communicating with you over some private
channel, send you a payment inside a payment protocol message. You
announce it to the network and I concurrently send a double spend.
Only nodes that were part of the stem will reject my double spend, so
I just learned a lot about your network location.

It's also appears clear that I can query by sending an inv and
noticing that no getdata arrives.  An example attack in the latter is
that when I get a stem transaction I rapidly INV interrogate the
entire network and by observing who does and doesn't getdata I will
likely learn the entire stem path upto permutation.

The extra network capacity used by getdata-ing a transaction you
already saw via dandelion would be pretty insignificant.

I believe the text should be simplified and clarified so just say:

"With the exception of her behavior towards the peer sending in the
stem transaction and the peer sending out the transaction Alice's
behavior should be indistinguishable from a node which has not seen
the transaction at all until she receives it via ordinary forwarding
or until after the timeout." -- then its up to the implementation to
achieve indistinguishably regardless of what other protocol features
it uses.


I think avoiding the is the most sensible way; and from a software
maintenance perspective I expect that anything less will end up
continually suffering from serious information leaks which are hard to
avoid accidentally introducing via other changes.

The primary functionality should be straightforward to implement,
needing just a flag to determine if a transaction would be accepted to
the mempool but for the flag, but which skips actually adding it.

Handling chains of unconfirmed stem transactions is made more
complicated by this and this deserves careful consideration. I'm not
sure if its possible to forward stem children of stem transactions
except via the same stem path as the parent without leaking
information, it seems unlikely.

This approach would mostly take additional complexity from the need to
limit the amplification of double spends. I believe this can be
resolved by maintaining a per-peer map of the not yet expired vin's
consumed by stem fowards sent out via that peer. E.g. vin->{timeout,
feerate}.  Then any new forward via that stem-peer is tested against
that map and suppressed if it it spends a non-timed-out input and
doesn't meet the feerate epsilon for replacement.

Use of the orphan map is not indistinguishable as it is used for block
propagation, and itself also a maintenance burden to make sure
unrelated code is not inadvertently leaking the stem transactions.


The BIP is a bit under-specified on this transition, I think-- but I
know how it works from reading the prior implementation (I have not
yet read the new implementation).

The way it works (assuming I'm not confused and it hasn't changed) is
that when a new stem transaction comes in there is a chance that it is
treated as coming in as a normal transaction.

An alternative construction would be that when a stem transaction goes
out there is a random chance that the stem flag is not set (with
suitable adjustment to keep the same expected path length)

For some reason I believe this would be a superior construction, but I
am only able to articulate one clear benefit:  It allows non-dandelion
capable nodes to take on the role of the last stem hop, which I
believe would improve the anonymity set during the transition phase.

Unrelated:

Has any work been given to the fact that dandelion propagation
potentially making to measure properties of the inter-node connection
graph?  e.g.  Say I wish to partition node X by disconnecting all of
its outbound connections, to do that it would be useful to learn whom
is connected to X.  I forward a transaction to X, observe the first
node to fluff it,  then DOS attack that node to take it offline.  Will
I need to DOS attack fewer or more nodes  to get all of X's outbounds
if X supports rapid stem forwarding?

-------------------------------------

Ah, so the BIP148 client handles this on behalf of its less technical users
on their behalf then, yes?


Sure, Exchanges are going to dedicate hundreds of developer hours and
thousands of support hours to support something that they've repeatedly
told everyone must have replay protection to be supported.  They're going
to do this because 8% of nodes and <0.5% of miners say they'll be rewarded
richly.  Somehow I find that hard to believe.

Besides, if the BIP148 client does it for them, they wouldn't have to
dedicate those hundreds of developer hours.  Right?

I can't imagine how this logic is getting you from where the real data is
to the assumption that an economic majority will push BIP148 into being
such a more valuable chain that switching chains will be attractive to
enough miners.  There's got to be some real data that convinces you of this
somewhere?


Wipeout risk is a serious issue when 45% of the miners support one chain
and 55% support the other chain.  Segwit doesn't even have 35% of the
miners; There's no data or statements anywhere that indicate that UASF is
going to reach the point where wipeout risk is even comparable to
abandonment risk.


To convince miners you would have to have some data SOMEWHERE supporting
the economic majority argument.  Is there any such data?

consensus

It doesn't have those issues during the segwit activation, ergo there is no
reason for segwit-activation problems to take priority over the very real
hardfork activation problems.


In a consensus system they are frequently the same, unfortunately.
Technical awesomeness without people agreeing = zero consensus.  So the
choice is either to "technically" break the consensus without a
super-majority and see what happens, or to go with the choice that has real
data showing the most consensus and hope the tiny minority chain actually
dies off.

Jared

On Wed, Jun 7, 2017 at 5:01 PM, James Hilliard <james.hilliard1@gmail.com>
wrote:

-------------------------------------
BIP32 extended public/private keys have version bytes that result in the
user visible xpub/xprv prefix. The BIP's recommendation is to use
different version bytes for other networks (such as tpub/tprv for testnet)

I would like to use additional version bytes to indicate the type of
output script used with the public keys.

I believe the change should be user visible, because users are exposed
to master public keys. I propose the following prefixes:

========== =========== ===================================
Version    Prefix      Description
========== =========== ===================================
0x0488ade4 xprv        P2PKH or P2SH
0x0488b21e xpub        P2PKH or P2SH
0x049d7878 yprv        (P2WPKH or P2WSH) nested in P2SH
0x049d7cb2 ypub        (P2WPKH or P2WSH) nested in P2SH
0x04b2430c zprv        P2WPKH or P2WSH
0x04b24746 zpub        P2WPKH or P2WSH
========== =========== ===================================
(source: http://docs.electrum.org/en/latest/seedphrase.html)

I have heard the argument that xpub/xprv serialization is a format for
keys, and that it should not be used to encode how these keys are used.
However, the very existence of version bytes, and the fact that they are
used to signal whether keys will be used on testnet or mainnet goes
against that argument.

If we do not signal the script type in the version bytes, I believe
wallet developers are going to use dirtier tricks, such as the bip32
child number field in combination with bip43/bip44/bip49.


Thomas

-------------------------------------
[inline responses]


On Thu, Sep 14, 2017 at 2:27 PM, Anthony Towns via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

I advocate a policy like this, except I propose two modifications:

- Point 4 should include *zero or more* altcoin developers, such that those
altcoins also deploy mitigations as early as Bitcoin. (Call this "early
altcoin disclosure".)

- Disclose of vulnerabilities, by social convention, always explicitly
names which altcoin developers were included in my proposed Early Altcoin
Disclosure and Point 6.

The rationale is that the policy should allow closer coordination with
altcoins. If the goal is minimizing economic damage, including altcoins
earlier may be the better trade-off between inclusiveness and secrecy. At
the same time, the policy doesn't establish *which* altcoins, which is a
tricky choice. However it *does* require disclosure of those relationships,
which provides a form of feedback on the system.

Imagine if altcoin X is compromised, and later disclosure occurs that
reveals that altcoin X was not contacted early, then this *might* indicate
leaks, maliciousness in the Bitcoin mitigation organization, or it *might*
be coincidence or dumb luck. In the other case, if the Bitcoin disclosure
reveals that X was indeed contacted early, then it probably indicates
incompetence of the altoin X.

Finally, notice that this kind of loose early disclosure policy can be
symmetric. For example, Zcash developers may choose to disclose
vulnerabilities they discover which affect Bitcoin to Bitcoin developers
*before* Zcash releases fixes, or before those fixes are widely adopted in
Zcash. We actually have a policy of doing this, since it's obvious that if
our mitigation process leaks and that's used to attack Bitcoin the
potential economic damage is very large.



I advocate for something like the latter case. I'd like to see a timeout on
disclosure. There's an endless tail of alt-coins that could be affected,
and no guarantee all will vigilantly upgrade. Meanwhile, deciding which of
them to disclose to confidentially versus which should just receive hints
to apply new patches is tricky and political.

Having a global timeout is a reasonable stop-gap. I consider the cost of
never disclosing, publicly, a known vulnerbility to be very high, even if
the fix is ubiquitously deployed, because it's a loss of security
knowledge, a precious public good.


Publishing a policy *might* increase organizational vulnerability, but so
might *not publishing* a policy. It seems fairly neutral to me on
vulnerability impact, whereas the benefit is good for users and developers.



Publishing after a reasonable timeout has many benefits. Many security
researchers learn from vulnerability disclosures across many disciplines
and industries. Future protocol designers of things potentially unrelated
to blockchain altogether may also learn important lessons.


If the first of those arguments holds, well, hopefully this message has
regards,
Nathan Wilcox
Zcash


-------------------------------------
Yes you practically can. No proxy can defeat the protocol investing less
money than buying storage space to store the blockchain.

Even with challenge-response delays of minutes.  That's why it will be
fully controlled by a RSK smart-contract, with no user intervention.
I'm will post about this soon.




On Mon, May 8, 2017 at 6:44 PM, Natanael <natanael.l@gmail.com> wrote:

-------------------------------------
On 03/08/2017 11:47 AM, Jonas Schnelli wrote:

I'm always willing to debate this issue. I'm generally a little
suspicious of one who demands another person to stop arguing. I got at
least one such demand (along with a threat) on this subject privately
last summer from a notable Core dev. There is a lengthy thread on this
subject in which I raised these issues. Everyone is free to review that
discussion.


The "presharing" of keys is how provable identity works, and is
precisely what this new proposal is also promoting. And in response to
that, the above statement was made by a Core dev (and not disputed):


I'm calling out the obvious relationship between BIP150 and this new
proposal. Restating how identity works, or that its use is optional does
not refute my position. It's not FUD.

e

-------------------------------------
"10% say literally never.  That seems like a significant disenfranchisement
and lack of consensus."

Certainly the poll results should be taken with a grain of salt and not a definitive answer or measure . 
However if we agree the poll has some worth (or even if not, then lets use it as hyptothetical): If we split it into two groups: those okay with a hardfork at some point > now, and those never okay with hardfork, that means there is 90% that agree a hardfork is acceptable in the future. That said, what threshold defines consensus then? 98%? 100%?       
 
Personally I think pursuing paths that maximize net social benefit in terms of cost surplus/burden is the best way to go since consensus is such an impossible to define, variable, case-by-case thing that doesn't always lead to the best choice.

-Ryan J. MArtin
 

________________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org [bitcoin-dev-bounces@lists.linuxfoundation.org] on behalf of bitcoin-dev-request@lists.linuxfoundation.org [bitcoin-dev-request@lists.linuxfoundation.org]
Sent: Thursday, February 09, 2017 7:00 AM
To: bitcoin-dev@lists.linuxfoundation.org
Subject: bitcoin-dev Digest, Vol 21, Issue 10

Send bitcoin-dev mailing list submissions to
        bitcoin-dev@lists.linuxfoundation.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
or, via email, send a message with subject or body 'help' to
        bitcoin-dev-request@lists.linuxfoundation.org

You can reach the person managing the list at
        bitcoin-dev-owner@lists.linuxfoundation.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of bitcoin-dev digest..."


Today's Topics:

   1. Re: A Modified Version of Luke-jr's Block Size BIP (alp alp)


----------------------------------------------------------------------

Message: 1
Date: Wed, 8 Feb 2017 08:44:52 -0600
From: alp alp <alp.bitcoin@gmail.com>
To: "t. khan" <teekhan42@gmail.com>,    Bitcoin Protocol Discussion
        <bitcoin-dev@lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] A Modified Version of Luke-jr's Block Size
        BIP
Message-ID:
        <CAMBsKS9OS2tA4bG-JG96XNZTiPyuq322Qu=fyJcZ1BtVj3TtxQ@mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

10% say literally never.  That seems like a significant disenfranchisement
and lack of consensus.

On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/18d9cda5/attachment-0001.html>

------------------------------

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


End of bitcoin-dev Digest, Vol 21, Issue 10
*******************************************

-------------------------------------
In light of some recent discussions, I wrote up this BIP for a real 2 MB block 
size hardfork following Segwit BIP148 activation. This is not part of any 
agreement I am party to, nor anything of that sort. Just something to throw 
out there as a possible (and realistic) option.

Note that I cannot recommend this to be adopted, since frankly 1 MB blocks 
really are still too large, and this blunt-style hardfork quite risky even 
with consensus. But if the community wishes to adopt (by unanimous consensus) 
a 2 MB block size hardfork, this is probably the best way to do it right now. 
The only possible way to improve on this IMO would be to integrate it into 
MMHF/"spoonnet" style hardfork (and/or add other unrelated-to-block-size HF 
improvements).

I have left Author blank, as I do not intend to personally champion this. 
Before it may be assigned a BIP number, someone else will need to step up to 
take on that role. Motivation and Rationale are blank because I do not 
personally think there is any legitimate rationale for such a hardfork at this 
time; if someone adopts this BIP, they should complete these sections. (I can 
push a git branch with the BIP text if someone wants to fork it.)

<pre>
BIP: ?
Layer: Consensus (hard fork)
Title: Post-segwit 2 MB block size hardfork
Author: FIXME
Comments-Summary: No comments yet.
Comments-URI: ?
Status: Draft
Type: Standards Track
Created: 2017-05-22
License: BSD-2-Clause
</pre>

==Abstract==

Legacy Bitcoin transactions are given the witness discount, and a block size 
limit of 2 MB is imposed.

==Copyright==

This BIP is licensed under the BSD 2-clause license.

==Specification==

Upon activation, a block size limit of 2000000 bytes is enforced.
The block weight limit remains at 4000000 WU.

The calculation of block weight is modified:
all witness data, including both scriptSig (used by pre-segwit inputs) and 
segwit witness data, is measured as 1 weight-unit (WU), while all other data 
in the block is measured as 4 WU.

The witness commitment in the generation transaction is no longer required, 
and instead the txid merkle root in the block header is replaced with a hash 
of:

1. The witness reserved value.
2. The witness merkle root hash.
3. The transaction ID merkle root hash.

The maximum size of a transaction stripped of witness data is limited to 1 MB.

===Deployment===

This BIP is deployed by flag day, in the block where the median-past time 
surpasses 1543503872 (2018 Nov 29 at 15:04:32 UTC).

It is assumed that when this flag day has been reached, Segwit has been 
activated via BIP141 and/or BIP148.

==Motivation==

FIXME

==Rationale==

FIXME

==Backwards compatibility==

This is a hardfork, and as such not backward compatible.
It should not be deployed without consent of the entire Bitcoin community.
Activation is scheduled for 18 months from the creation date of this BIP, 
intended to give 6 months to establish consensus, and 12 months for 
deployment.

==Reference implementation==

FIXME




-------------------------------------
To make it completely transparent to unupgraded wallets, the return outputs have to be sent to something that is non-standard today, i.e. not P2PK, P2PKH, P2SH, bare multi-sig, and (with BIP141) v0 P2WPKH and v0 P2WSH.

Mainchain segwit is particularly important here, as that allows atomic swap between the bitcoin and xbitcoin. Only services with high liquidity (exchanges, payment processors) would need to occasionally settle between the chains.



-------------------------------------
Yes, 75% seems fine - given that there is a already a wide deployment of
segwit enforcing nodes

This implementation is 100% compatible with a "UASF movement" since, if
triggered, it essentially turns all supporting miners into equivalent
BIP148 enforcers.   This should allay any fears that this would subvert a
UASF.

The proposed "agreement" which was reached without input from the
development community also apparently requires that a hard fork be locked
in on the same bit (bit 4).

Ideally, such a 2MB increase should be scheduled using BIP103-esqe logic:
Gradually increasing from 1MB to 2MB over the course of at least a couple
years, beginning 6 months from lock-in.

This will give developers ample time to evaluate and react to network
impacts.


On Wed, May 24, 2017 at 12:02 PM, Wang Chun via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------


Just take something like FlexTran as example. How you could get prepared for that without first finalising the spec?

Or changing the block interval from 10 minutes to some other value?

Also, fixing the sighash bug for legacy scripts?

There are many other ideas that require a HF:
https://en.bitcoin.it/wiki/User:Gmaxwell/alt_ideas <https://en.bitcoin.it/wiki/User:Gmaxwell/alt_ideas>
-------------------------------------
Thank you for your constructive feedback. I now see that the proposal introduces a potential issue.




Do you have any critical suggestion as to how transaction bandwidth limit could be addressed, it will eventually become an issue if nothing is changed regardless of how high fees go?


Regards,

Damian Williamson



________________________________
From: Mark Friedenbach <mark@friedenbach.org>
Sent: Tuesday, 19 December 2017 3:08 AM
To: Damian Williamson
Subject: Re: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

Damian, you seem to be misunderstanding that either

(1) the strong form of your proposal requires validating the commitment to the mempool properties, in which case the mempool becomes consensus critical (an impossible requirement); or

(2) in the weak form where the current block is dependent on the commitment in the last block only it is becomes a miner-selected field they can freely parameterize with no repercussions for setting values totally independent of the actual mempool.

If you want to make the block size dependent on the properties of the mempool in a consensus critical way, flex cap achieves this. If you want to make the contents or properties of the mempool known to well-connected nodes, weak blocks achieves that. But you cant stick the mempool in consensus because it fundamentally is not something the nodes have consensus over. Thats a chicken-and-the-egg assumption.

Finally in terms of the broad goal, having block size based on the number of transactions is NOT something desirable in the first place, even if it did work. Thats effectively the same as an infinite block size since anyone anywhere can create transactions in the mempool at no cost.

On Dec 16, 2017, at 8:14 PM, Damian Williamson via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:

I do not know why people make the leap that the proposal requires a consensus on the transaction pool. It does not.

It may be helpful to have the discussion from the previous thread linked here.
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015370.html

Where I speak of validating that a block conforms to the broadcast next block size, I do not propose validating the number broadcast for the next block size itself, only that the next generated block is that size.

Regards,
Damian Williamson


________________________________
From: Damian Williamson <willtech@live.com.au<mailto:willtech@live.com.au>>
Sent: Saturday, 16 December 2017 7:59 AM
To: Rhavar
Cc: Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

There are really two separate problems to solve.


  1.  How does Bitcoin scale with fixed block size?
  2.  How do we ensure that all valid transactions are eventually included in the blockchain?

Those are the two issues that the proposal attempts to address. It makes sense to resolve these two problems together. Using the proposed system for variable block sizes would solve the first problem but there would still be a whole bunch of never confirming transactions. I am not sure how to reliably solve the second problem at scale without first solving the first.


I do not suggest a consensus. Depending on which node solves a block the value for next block size will be different. The consensus would be that blocks will adhere to the next block size value transmitted with the current block. It is easy to verify that the consensus is being adhered to once in place.


Not a necessary function, just an effect of using a probability-based distribution.


I entirely agree with your sentiment that Bitcoin must be incentive compatible. It is necessary.

It is in only miners immediate interest to make the most profitable block from the available transaction pool. As with so many other things, it is necessary to partially ignore short-term gain for long-term benefit. It is in miners and everybody's long-term interest to have a reliable transaction service. A busy transaction service that confirms lots of transactions per hour will become more profitable as demand increases and more users are prepared to pay for priority. As it is there is currently no way to fully scale because of the transaction bandwidth limit and that is problematic. If all valid transactions must eventually confirm then there must be a way to resolve that problem.

Bitcoin deliberately removes traditional scale by ensuring blocks take ten minutes on average to solve, an ingenious idea and, incentive compatible but, fixed block sizes leaves us with a problem to solve when we want to scale.


I am confident that the math to verify blocks based on the proposal can be developed (and I think it will not be too complex for a mathematician with the relevant experience), however, I am nowhere near experienced enough with probability and statistical analysis to do it. Yes, if Bitcoin doesn't then it might make another great opportunity for an altcoin but I am not even nearly interested in promoting any altcoins.

If not the proposal that I have put forward, then, hopefully, someone can come up with a better solution. The important thing is that the issues are resolved.

Regards,
Damian Williamson


________________________________
From: Rhavar <rhavar@protonmail.com<mailto:rhavar@protonmail.com>>
Sent: Saturday, 16 December 2017 3:38 AM
To: Damian Williamson
Cc: Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks


Unfortunately your proposal is really fundamentally broken, on a few levels. I think you might need to do a bit more research into how bitcoin works before coming up with such improvements =)

But just some quick notes:

* Every node has a (potentially) different mempool, you can't use it to decide consensus values like the max block size.

* Increasing the entropy in a block to make it more unpredictable doesn't really make sense.

* Bitcoin should be roughly incentive compatible. Your proposal explicits asks miners to ignore their best interests, and confirm transactions by "priority".  What are you going to do if a "malicious" miner decides to go after their profits and order by what makes them the most money. Add "ordered by priority" as a consensus requirement? And even if you miners can still sort their mempool by fee, and then order the top 1MB by priority.

If you could find a good solution that would allow you to know if miners were following your rule or not (and thus ignore it if it doesn't) then you wouldn't even need bitcoin in the first place.




-Ryan


-------- Original Message --------
Subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks
Local Time: December 15, 2017 3:42 AM
UTC Time: December 15, 2017 9:42 AM
From: bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
To: Bitcoin Protocol Discussion <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>>



I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?

I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.

It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.


If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?

Regards,
Damian Williamson


________________________________

From: bitcoin-dev-bounces@lists.linuxfoundation.org<mailto:bitcoin-dev-bounces@lists.linuxfoundation.org> <bitcoin-dev-bounces@lists.linuxfoundation.org<mailto:bitcoin-dev-bounces@lists.linuxfoundation.org>> on behalf of Damian Williamson via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>>
Sent: Friday, 8 December 2017 8:01 AM
To: bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
Subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks


Good afternoon,

The need for this proposal:

We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.

I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.

I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.

Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.

Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.

Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.

I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.

I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.

This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.

I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.

# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

## The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.

The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.

Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.

Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.

The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.

## Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.

Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.

The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?

**Explanation of the operation of priority:**


I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.

## Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.

## Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.

## Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.

1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.

## Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.

I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.

Regards,
Damian Williamson


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
On Thu, Apr 20, 2017 at 6:39 PM, shaolinfry <shaolinfry@protonmail.ch> wrote:
[...]

I have not reviewed it carefully yet, but I agree that it addresses my
main concern!  I think this is a much better approach. Thanks.

-------------------------------------
Thank you for your elaborate response Eric,

On Sun, Apr 9, 2017, at 00:37, Eric Voskuil wrote:

I haven't dived into libbitcoin V2/V3 enough to  fully grasp it and
though your comments help, I still not fully do.  I will answer below
what is related to bitcrust itself.

My post wasn't posted to claim innovation; I merely try to explain how
Bitcrust works and why   it performs well. 




You seem to ignore here the difference between base load and peak load.
If Compact blocks/XThin with further optimizations can presync nearly
100% of the transactions, and nodes can do as much as possible when a
transaction comes in, the time spent when a block comes in can be
minimized and a lot more transactions can be handled with the same
resources.

The reason for "splitting" is that for an incoming transaction the
spent-state of the outputs being spent isn't particularly relevant as
you seem to acknowledge. When the block comes in, the actual output data
isn't relevant.

The *only* thing that needs to be checked when a block comes in is the
order, and the spend-tree approach absolves the need to access outputs
here.

As it also absolves the need for reorgs this greatly simplifies the
design. I am not sure why you say that a one-step approach is more
"test-friendly" as this seems to be unrelated.


I fully agree and hopefully do not pretend to hide that my numbers are
premature without a full implementation. I just think they are promising
enough to  convince at least myself to move on with this model.
 

I don't get what you are saying. Why pick the greatest PoW of two
competing blocks? If two blocks come in, an implementation is free to
choose whichever block to build on. Choosing so is not a "hardfork".
Parallel validation simply makes it easier to make an optimal choice,
for if two blocks come in, the one that is validated fastest can be
build upon without the risk of validationless mining.


I am not trying to claim novelty here.


Frankly, I think this is a bit of an exaggeration. Soft forks are
counted on a hand, and I don't think there are many - if any -
transactions in the current chain that have changed compliance based on
height. This makes this a compliance issue and not a performance issue
and the solution I have explained, to add height-based compliance as
meta data of validation seems to 
be adequate and safe.



I think I get the gist of your approach and it sounds very interesting
and I will definitely dive in deeper.

It also seems sufficiently different from Bitcrust to merit competing on
(eventual) results instead of the complicated theory alone.

Best,
Tomas

-------------------------------------

In order to *get to that point*, you need >51%.

Not only that, but, if you started out with <51%, then you need >>51% in order to *catch up* and replace the large number of blocks added to the legacy chain in the mean time.

So, since >51% is _required_ for BIP148 to succeed (and likely >>51%)... you might as well do as SegWit did originally, or lower the threshold to 80% or something (as BIP91 does).

Without replay protection at the outset, BIP148, as far as I can tell, isn't a threat to miners.

--
Please do not email me anything that you are not comfortable also sharing with the NSA.


-------------------------------------
It is *not proof of stake.* when:

a) burn happens regardless of whether you successfully mine.
b) miner cannot know which tx are burns
c) the majority of burns cannot be used for mining and are simply lost
(poisson discovery distribution)
d) burn involves real risk: *every bit as much at stake *

(It's the difference between a computer secured by not being connected to
the internet, and a computer secured by re-imaging from a computer that
was, in the past, not connected to the internet.)

It is possible to craft a burn-network such that the only way for a miner
to prevent a burn is to prevent all transactions other than his own.

This is still a weakness, and I can't see a way around it though.


On Fri, Apr 7, 2017 at 8:59 AM, Jannes Faber via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Thank you for the proposal Wang Chung!

It is clear that, spam aside, blocks are getting full and we need increase
them soon. What I don't like about your proposal is it forces all node
operators to implicitly accept larger blocks in 2020, even maybe against
their will. 32 MB blocks might result in a loss of decentralization, and it
might be too difficult to coordinate for small blocks before it's too late.


So I think Core can't decide on hard forks like this. It must be left up to
the users. I think only choice is for Core to add a run-time option to
allow node operators to increase block size limit, so that this very
controversial decision is not coming from Core. It must come from the
community.
-------------------------------------
Hi I have a small interjection about the point on error correction (excuse
me if it seems elementary). Isn't there an argument to be made where a
wallet software should never attempt to figure out the 'correct' address,
or in this case private key? I don't think it's crazy to suggest somebody
could import a slightly erroneous WIF, the software gracefully
error-corrects any problem, but then the user copies that error onward such
as in their backup processes like a paper wallet. I always hate to advocate
against a feature, I'm just worried too much error correcting removes the
burden of exactitude and attention of the user (eg. "I know I can have up
to 4 errors").

I'm pretty sure I read those arguments somewhere in a documentation or
issue tracker/forum post. Maybe I'm misunderstanding the bigger picture in
this particular case, but I was just reminded of that concept (even if it
only applies generally).

Thanks,
AJ West

On Sun, Sep 17, 2017 at 4:10 AM, Thomas Voegtlin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I'm all for using height instead of time. That was my preference for
bip9 all along, but my arguments at the time apparently weren't
convincing.

Regarding luke's proposal, the only advantage I see is that it would
allow nodes that don't know a deployment that gets activated to issue
a warning, like bip9 always does when an unknown deployment is locked
in.

But there's a simpler way to do that which doesn't require to add
consensus rules as to what versionbits should be.
I'm honestly not worried about it being "coersive" and I don't think
it's inherently reckless (although used with short deployment times
like bip148 it can be IMO). But it adds more complexity to the
consensus rules, with something that could merely be "warning code".

You can just use a special bit in versionbits for nodes to get the warning.
My proposal doesn't guarantee that the warning will be signaled, for
example, if the miner that mines the block right after lock in doesn't
know about the deployment, he can't possibly know that he was supposed
to signal the warning bit, even if he has the best intentions. Miners
can also intentionally not signal it out of pure malice. But that's no
worse than the current form, when deployments activated by final date
instead of miner signaling never get a warning.

Shaolinfry had more concerns with my proposed modification, but I
think I answered all of them here:

https://github.com/bitcoin/bitcoin/pull/10462#issuecomment-306266218

The implementation of the proposal is there too. I'm happy to reopen
and rebase to simplify (#10464 was merged and there's at least 1
commit to squash).

on an as-needed basis.

You can also do

consensus.vDeployments[Consensus::DEPLOYMENT_MASF].bit = 0;
consensus.vDeployments[Consensus::DEPLOYMENT_MASF].nStartHeight = 500000;
consensus.vDeployments[Consensus::DEPLOYMENT_MASF].nTimeoutHeight = 510000;
consensus.vDeployments[Consensus::DEPLOYMENT_MASF].lockinontimeout = false;

and "if needed", simply add the following at any time (before the new
nStartHeight, obviously):


consensus.vDeployments[Consensus::DEPLOYMENT_UASF].bit = 0;
consensus.vDeployments[Consensus::DEPLOYMENT_UASF].nStartHeight = 510000;
consensus.vDeployments[Consensus::DEPLOYMENT_UASF].nTimeoutHeight = 515000;
consensus.vDeployments[Consensus::DEPLOYMENT_UASF].lockinontimeout = true;


On Wed, Jul 5, 2017 at 9:44 PM, Hampus Sjöberg via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
By "all of this" I meant the other issues that I mentioned too "would
everybody even here say that they feel very comfortable with their keys?
That if something happen to them there is no pb for the family or
trusted parties to retrieve the keys? That this process is secured in
case the trusted parties are finally untrusted? etc", I am extending the
problematic while the very basic concerns are still unsolved

Then I don't agree with the fact that users should not have the control
of their keys, but if I try to summarize, your suggestions probably lead
to the fact that the "wallet" part should be outside of bitcoin-qt, in a
simple offline module (assuming that you can trust the simple sw + the
os + the hw +the cpu, but ok, the pb is the same with a hw wallet),
which I think is a good idea

That's why I made a module some time ago, supposed to be "bitcoin
transactions made simple", you do your transactions offline, check them,
and send them to the network via qt, the web or other, it's working but
is not online on github because unfinished, and unfinished because
nothing is simple and it's unlikely that normal people can use this for
now, unfortunately you need to be a bit online to make your transaction,
fetch the output you want to spend or get the info, then associate the
right key, calculate the fees, that's not simple, that's why it's
different from a standard wallet, but probably a good way

Small sw a bit like a credit card finally, and people know they must not
disclose their code(s) in case they are asked on IRC or elsewhere



Le 30/09/2017 à 23:14, Jonas Schnelli via bitcoin-dev a écrit :

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------------------------------
If the primary purpose of pow is to destroy value, then a masked proof of
burn to an expanded address that assigns the private key holder the right
to mine only in the next Nth block would be sufficient.  Expanding the
address space so that addresses can only be proven invalid only with the
private key.  Miners can then not trivially game the system by excluding
tx...without killing the entire system.  ( Like POW ... miners lose many
burns since only one valid proof is deterministically selected. Difficult
adjusted upward based on the number of valid proofs per block.)

The other part of "real POW" is that miners take *time* to mine.  Proof of
destroyed value us not sufficient.  Proof of time spent is critical....
something even a masked burn cannot provide.

On Apr 5, 2017 10:49 PM, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I think we need something like this. Hour resolution seems like the
correct choice to me.

Please someone steal whatever code you can from this PR when
implementing the UI for BIP173 expiration:

https://github.com/bitcoin/bitcoin/pull/9722

I have a rebased version as well if anyone wants it.


On 09/27/2017 09:06 AM, Peter Todd via bitcoin-dev wrote:

-------------------------------------
Hi Mark

Yes, it seems like sign-to-contract protocols, which I just now briefly
read about [1][2], may need to use historic witnesses. That raises the
question, what are Bitcoin witnesses for?

To me it seems witnesses should be regarded as temporary. But it seems both
respondents to this thread, Eric and Mark, mean that witnesses are forever.
I regard witnesses as a way to authenticate updates to the UTXO set, and
once buried deep enough in the blockchain, the witness is no longer needed,
because consensus has formed around the UTXO set update.

Suppose a transaction with an invalid witness happens to enter the
blockchain and gets buried 100000 blocks down with the witness still
available. Is the blockchain above it valid? I'd say the blockchain is
valid and that it was a bug that the transaction made it into the
blockchain. We will have to live with such bugs.

Another way to put it: Suppose that all witnesses from 2017 dissappears
from all nodes in 2020. Is the blockchain still valid? I think so. I would
continue using it without looking back.

With that approach, I think sign-to-contract protocols has to find ways to
work in a witnessless environment. For example, users of such protocols can
setup their own archival nodes.

I'd love to hear alternative views on this.

Thanks,
/Kalle

[1]
https://download.wpsoftware.net/bitcoin/wizardry/mw-slides/2017-03-mit-bitcoin-expo/slides.pdf
[2] https://bitcointalk.org/index.php?topic=893898.msg9861102#msg9861102

2017-12-18 18:30 GMT+01:00 Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
Today someone showed up on IRC suggesting a scheme for to improve the
ability of miners to mine without validation while including transactions
by shipping around an approximate sketch of the txins that were used by a
block.

I pointed out that what sounded like the exact same scheme had been
previously proposed by Anthony Towns over a year ago,  that it turned out
that it didn't need any consensus changes, but also wasn't very attractive
because the actual transmission of the block (at least with FBRP or Fibre)
didn't really take any longer...  And, of course, mining without validating
does a real number on SPV security assumptions.

But then realized the the conversation between Anthony and I was offlist.
So-- for posterity...

I think the most interesting thing about this thread is that it gives a
concrete proof that a restriction on collecting transaction fees does not
discourage validationless mining; nor to other proposed consensus changes
make it any easier to include transactions while mining without validation.


Forwarded conversation
Subject: Blockchain verification flag (BIP draft)
------------------------

From: Anthony Towns <aj@erisian.com.au>
Date: Mon, Feb 29, 2016 at 2:13 AM
To: Gregory Maxwell <greg@xiph.org>


On Fri, Dec 04, 2015 at 08:26:22AM +0000, Gregory Maxwell via bitcoin-dev
wrote:

Two thoughts related to this. Are they obvious or daft?

a)

Would it make sense to require some demonstration that you've validated
prior blocks? eg, you could demonstrate you've done part of the work
to at least verify signatures from the previous block by including the
sha256 of the concatenation of all the sighash values in the coinbase
transaction -- if you'd already done the sig checking, calculating that
as you went would be pretty cheap, I think. Then make the rule be that
if you set the "validated" bit without including the demonstration of
validation, your block is invalid.

I guess this is more or less what Peter Todd proposed in:

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/
2015-December/012105.html

b)

It occurred to me while emailing with Matt Corallo, that it's probably
possible to make it easy to generate actually useful blocks while doing
validationless mining, rather than only creating empty blocks.

When creating a block, you:

   - calculate a fixed size (7168 bytes?) bloom filter of the
     prevouts that the block is spending
   - include the sha256 of the final bloom filter as the last output
     in the coinbase
   - enforce the inclusion of that sha256 by soft-fork
   - as part of fast relaying, transmit:
       - 80 byte block header
       - 7168 byte bloom filter
       - < 416 (?) byte merkle path to the coinbase
       - 64 byte sha256 midstate for coinbase up to start of the
         final transaction
       - < 128 byte tail of coinbase including bloom commitment
     (total of 7856 bytes, so less than 8kB)

I think that would be enough to verify that the proof-of-work is
committing to the bloom filter, and the bloom filter will then let
you throw out any transactions that could have been included in a block
built on block n-1, but can't be included in block n+1 -- whether they're
included in the new block, or would be double spends. So given that
information you can safely build a new block that's actually full of
transactions on top of the new block, even prior to downloading it in
full, let alone validating it.

I've run that algorithm over the last couple of weeks' worth of
transactions (to see how many of the next block's transaction would have
been thrown away using that approach) and it appeared to work fine --
it throws away maybe a dozen transactions per block compared to accurate
validation, but that's only about a couple of kB out of a 1MB block,
so something like 0.2%.  (I'm seeing ~4500 prevouts per block roughly,
so that's the error rate you'd expect; doubling for 2MB's worth of txes
with segwit predicts 3.5%, doubling again would presumably result in 14%
of transactions being falsely identified as double spends prior to the
block actually validating)

I haven't checked the math in detail, but I think that could reasonably
give an immediate 20% increase in effective blocksize, given the number of
empty blocks that get mined... (There were only ~1571MB of transactions
in the last 2016 blocks, so bumping the average from 780kB per block to
940kB would be a 20% increase; which would bring the 1.7x segwit increase
up to 2x too...)

Also, as far as I can see, you could probably even just have bitcoin core
transmit that 8kB of data around as part of propogating headers first.
Once you've got the new header and bloom filter, the only extra bit
should be passing both those into getblocktemplate to update the
previousblockhash and transaction selection. Both those together and it
seems like you could be mining on top of the latest block seconds after
it was found, just by naively running a bitcoin node?

I saw the "Bitcoin Classic" roadmap includes:

  "Implement "headers-first" mining. As soon as a valid 80-byte block
   header that extends the most-work chain is received, relay the header
   (via a new p2p network message) and allow mining an empty block on top
   of it, for up to 20 seconds."

which seems like the same idea done worse...

Any thoughts? Pointers to the bitcointalk thread where this was proposed
two years ago? :)

Cheers,
aj


----------
From: Gregory Maxwell <gmaxwell@gmail.com>
Date: Mon, Feb 29, 2016 at 3:20 AM
To: Anthony Towns <aj@erisian.com.au>


On Mon, Feb 29, 2016 at 2:13 AM, Anthony Towns <aj@erisian.com.au> wrote:

That information is easily shared/delegated... so it just creates
another centralized information source, and another source of
unfairness producing latency in the mining process. Without actually
preventing parties from mining. Doubly so in the context of how
validationless mining is actually done; the miners pull from other
miner's stratum servers; so they'll just see the commitments there.

So I don't see there being too much value there.


Pretty good incentive to not adopt the scheme, perhaps?

Moreover, this creates another way for a block to be invalid which has
no compact fraud proof. :(


I agree but:

I'm basically tired of repeating to people that there is no need for a
validationless block to be empty. So Yes, I agree with you on that
fact; it's possible for miners to do this already, with no protocol
changes (yes, it requires trusting each other but inherently
validationless mining already requires that). Miners only don't bother
right now because the funds left behind are insubstantial.

Its absolutely untrue that an empty block is not useful. Every block,
empty or not, mined against the best tip you know contributes to the
resolution of consensus and collapsing the network onto a single
state. Every block that was mined only after validating a block
amplifies security; by helping leave behind an invalid chain faster. A
block doesn't need to contain transactions to do these things.


FWIW, thats significantly larger than the amount of data typically
needed to send the whole block using the fast block relay protocol.

Your estimates are assuming the empty blocks come purely from
transmission and verification, but because most verification is cached
and transmission compressed-- they don't. There are numerous latency
sources through the whole stack, some constant some
size-proportional... the mining without validation achieves its gains
not from skipping validation (at least not most of the time); but
mostly from short cutting a deep stack with many latency sources;
including ones that have nothing to do with bitcoin core or the
Bitcoin protocol.

High hardware latency also amplifies short periods of empty block
mining to longer periods.

Perhaps most importantly, VFM mining avoids needing to identify and
characterize these other delay sources, by short cutting right at the
end no one needs to even figure out that their pool server is
performing a DNS request before every time it contacts their bitcoind
RPC or whatnot.


This BIP draft resulted in me relieving some pretty vicious attacks
from that community... funny.


Relevant to your interests: https://github.com/bitcoin/bitcoin/pull/1586

Lots of discussion on IRC.

----------
From: Anthony Towns <aj@erisian.com.au>
Date: Wed, Mar 2, 2016 at 9:55 PM
To: Gregory Maxwell <gmaxwell@gmail.com>


On Mon, Feb 29, 2016 at 03:20:01AM +0000, Gregory Maxwell wrote:

Yeah, I thought about that. It's a tradeoff -- you definitely want the
validation to be easily "shared" in the sense that you want one validation
run to suffice for billions of mining attempts; and you probably want
it to be easy to compute when you receive a block, so you don't have
to revalidate the previous one to validate the new one... But you don't
want it to be so easily shared that one person on the planet calculates
it and everyone else just leeches from them.


I think you could make it hostile to accidental sharing by having it be:

  <n> ;
  sha256(
      sha256( current block's first <n>+1 coinbase outputs ;
               previous block's nonce )
      sha256( previous block's sighash values )
  )

If you skipped the internal sha256's (or just moved the nonce into the
final sha256), you'd be half-way forced to revalidate the previous block
every time you found a new block, which might be worthwhile.


Well, my theory was once you have validated the block, then the
demonstration is trivially easy to provide.

I was thinking that you could add a positive incentive by making validated
blocks count for something like 1.6x the chainwork for choosing which
chain to build on; so if you have a chain with 3 unvalidated blocks in
a row, then a chain with 2 validated blocks in a row instead would be
preferred for building your next block.


Hmmm. That's true. Is it true by definition though? If you're proving
you've validated 100% of a block, then is it even conceptually possible
to check that proof with less work than validating 100% of a block?
Sounds kind of SNARK-ish.

Oh, don't SNARKs (theoretically) give you a compact fraud proof, provided
the block size and sigops are bounded? The "secret" input is the block
data, public input is the block hash and the supposed validation proof
hash, program returns true if the block hash matches the block data,
and the calculated validation hash doesn't match the supposed validation
hash. Shudder to think how long generating the proof would take though,
or how hard it'd be to generate the circuit in the first place...


If you're only mining an empty block, the only way someone else can
cause you to waste your time is by wasting their own time doing PoW on
an invalid block. If you're mining a block with transactions in it, and
they can mine a valid block, but trick you into mining something that
double spends, then they can make you waste your time without wasting
their own, which seems like a much worse attack to me.

The advantage of the consensus enforced bloom filter is you don't have
to trust anything more than that economic incentive. However if you just
sent an unverifiable bloom filter, it'd be trivial to trick you into
mining an invalid block.

(If you already have the 1MB of block data, then extracting the prevouts
for use as a blacklist would probably be plenty fast though)

(Of course, maybe 90% of current hashpower does trust each other
anyway, in which case requiring trust isn't a burden, but that's not
very decentralised...)

(Paragraphs deleted. My maths is probably wrong, but I think it is
actually economically rational to mine invalid blocks as chaff to distract
validationless miners? The numbers I get are something like "if 40% of
the network is doing validationless mining for 20 seconds out of every
10 minutes, then it's profitable to devote about 2% of your hashpower to
mining invalid blocks". Probably some pretty dodgy assumptions though,
so I'm not including any algebra. But having actual invalid blocks with
real proof of work appear in the wild seems like it'd be a good way to
encourage miners to do validation...)


Hey, fees are almost 1% of the block payout these days -- that's within
an order of magnitude of a rounding error!


Yeah, I deleted "useless" for that reason then put it back in anyway...


Really? Hmm, if you have 2-byte indexes into the most likely to be mined
60k transactions, by 2000 transactions per block is about 4000 bytes. So
I guess that makes sense. And weak blocks would make that generalisable
and only add maybe a 32B index to include on the wire, presumably.

It'd only take a dozen missed transactions to be longer though.


Hmm, so my assumption is the "bitcoin core" side of the stack looks
something like:

   block header received by p2p or relay network
     |
     V
   block data received by p2p or relay network
     |
     V
   validation, UTXO set updates
     |
     V
   getblocktemplate (possible tx ordering recalculation)
     |
     V
   block header to do PoW on!
     |
     V
   vary and push to miners over the network
     |
     V
   push to ASICs

and the validationless "shortcut" just looks like:

   block header received by p2p or relay network
     |
     V
   hack hack
     |
     V
   new block header to do PoW on!
     |
     V
   vary and push to miners over the network
     |
     V
   push to ASICs

and so making the bitcoin core parts able to provide an unvalidated
header to push to miners/ASICs against "instantly" would be a win as
far as getting bitcoin proper back into the loop all the time... That
would mean removing validation from the critical path, and possibly more
optimisation of getblocktemplate to make it effectively instant too. But
those seem possible?

Having it be:

  header received by bitcoin core
    |
    V
  new block header to do (unverified) PoW on!
    |
    V
  ...

and

  header received by bitcoin core
    |
    V
  block data received by bitcoin core
    |
    V
  block data validated
    |
    V
  new block header to do (verified) PoW on!
    |
    V
  ...

with mining tools being able to just reliably and efficiently leave
bitcoin core in the loop seems like it ought to be a win to me...


At least with longpoll, doing a DNS query before connection shouldn't
matter?


I'm guessing you meant "receiving", which makes that a kinda weird
freudian slip? :) But yeah, technical consistency isn't something I've
seen much of from that area...


Tsk, 2 != 4...

Hmm, I'm not sure where this leaves my opinion on either of those ideas.

Cheers,
aj


----------
From: Anthony Towns <aj@erisian.com.au>
Date: Sun, Mar 13, 2016 at 3:58 AM
To: Gregory Maxwell <gmaxwell@gmail.com>


On Thu, Mar 03, 2016 at 07:55:06AM +1000, Anthony Towns wrote:

So I think there's two levels of withholding adversarial miners could
do:

 - block withholding, so they have more time to build on top of their
   own block, maybe increasing their effective hashrate if they have
   above average connectivity

 - transaction withholding, so an entire block can be invalidated
   after the fact, hitting SPV nodes. if there are SPV miners, this can
   invalidate their work (potentially profitably, if you've accidently
   orphaned yourself)

You could solve transaction withholding for miners just by saying
"a PoW isn't valid unless the merkle tree is valid", that way you
can't retroactively invalidate a block, but then you need fast relay
before starting to mine, not just the header and some hint as to what
transactions might be included, and therefore the bloom filter idea
is pointless...


Having actually tried the relay network now, it seems like:

 a) it gets less coding gain than it theoretically could; the day or
    so's worth of blocks from Lightsword only seemed to be ~8x less data,
    rather than ~125x-250x, and what I'm seeing seems similar. So still
    room for improvement?

 b) using "weak blocks" as a way of paying for adding "non-standard"
    transactions (large, low fee, actually non-standard, etc) to the
    mempool seems workable to me; so long as the only reason you're doing
    weak blocks is so miners can ensure the transactions they're mining
    are in mempools, and thus that their blocks will relay quickly, the
    incentives seem properly aligned. (I think you'd want to distinguish
    txns only relayed because they have a weak block, just to be nice to
    SPV clients -- weak block txns might only be mined by one miner, while
    standard, fee paying transactions are being mined by all/most miners)

 c) it seems like it would be possible to adapt the relay protocol into
    a p2p environment to me? I'm thinking that you provide a bidirectional
    mapping for (a subset of) your mempool for each connection you
    have, so that you can quickly go to/from a 2-byte index to a
    transaction. If you make it so that whoever was listening gets to
    decide what transactions are okay, then you'd just need 9 of these
    maps -- 1 for each of your outgoing connections (ie, 8 total), plus
    another 1 that covers all your incoming connections, and each map
    should only really need to use up to about a meg of memory, which
    seems pretty feasible.  Maybe it means up to 8x5MB of your mempool
    is controlled by other people's policies rather than your own,
    but that doesn't seem to bad either.

 d) I'm a bit confused how it compares to IBLT; it seems like IBLT has
    really strong ordering requirements to work correctly, but if you
    had that you could compress the fast relay protocol really well,
    since you could apply the same ordering to your shared mempool, and
    then just send "next tx, next tx, skip 1 tx, next tx, next tx, skip
    3 tx, next tx, here's one you missed, ...", which with compression
    would probably get you to just a few /bits/ per (previously seen)
    transaction...  [0] [1]

 e) for p2p relay, maybe it would make sense to have the protocol only
    allow sending blocks where all the transactions are "previously
    seen". that way if you get a block where some txes haven't been
    seen before, you stall that block, and start sending transactions
    through. if another block comes in in the meantime, that doesn't
    have any new transactions, you send that block through straight away.
    that encourages sending weak blocks through first, to ensure your
    transactions are already in mempools and no one else can sneak
    in first.

Hmm... So that all seems kind of plausible to me; in how many ways am I
mistaken? :)

Cheers,
aj

[0] A hard-fork change to have the block merkle tree be ordered by txid,
    and have the transactions topologically sorted before being validated
    would be kind-of interesting here -- apart from making sorting
    obvious, it'd make it easy to prove that a block doesn't contain a
    transaction. Bit altcoin-y though...

[1] Maybe having the shared mempool indexes be sorted rather than FIFO
    would make the data structures hard; I don't think so, but not sure.


----------
From: Gregory Maxwell <gmaxwell@gmail.com>
Date: Sun, Mar 13, 2016 at 5:06 AM
To: Anthony Towns <aj@erisian.com.au>


On Sun, Mar 13, 2016 at 3:58 AM, Anthony Towns <aj@erisian.com.au> wrote:

Also called "selfish mining".


Right, this is how Bitcoin Core works (won't extend a chain it hasn't
validated)-- but some miners have shortcutted it to reduce latency.
(And not just bypassing validation, but the whole process, e.g.
transaction selection; which historically has taken more time than
propagation).


It's pretty variable.  It depends a lot on consistency between the
transactions the server side selects and the client. When spam attacks
go on, or miners change their policy compression falls off until the
far end twiddles.

Go look at the distribution of the results.


That is a bit kludgy, but yes-- it would work.

But the key thing about latency minimization is that you _must_ send a
block with no request; because otherwise the RTT for just the request
alone will totally dominate the transfer in most cases.  And having N
peers send you the whole block redundantly ends up hurting your
performance (esp because packet losses mean more round trips) even if
the compression is very high.

All these problems can be avoided; at least in theory. Optimal latency
mitigation would be achieved by something like block network coding
techniques:

https://en.bitcoin.it/wiki/User:Gmaxwell/block_network_coding

With these techniques peers could blindly send you data without you
requesting it, while every byte they send would usefully contribute to
your reconstruction. With extra effort and opportunistic forwarding
the entire network could, in theory, receive a block in the time it
took the original host to send only one block, while making use of a
significant fraction of the network's whole bisection bandwidth.


Latency of block relay easily ends up CPU bound; even when not doing
anything too smart (this is why Matt's relay protocol stuff has AVX
sha2 code in it). Prior IBLT implementation attempts have performance
so low that their decode time ends up dwarfing transmission time, and
plain uncoded blocks are faster for common host/bandwidth
configurations.

The ordering requirements stuff is not that relevant in my view; you
likely believe this because Gavin rat-holed himself on it trying to
spec out ordering requirements for miners...  The reality of it is
that a uniform permutation of, say, 4000 transactions can be stored in
log2(4000!)/8 bytes, or about 5.2kbytes (and this is easily achieved
just by using range coding to optimally pack integers in the range
[0..n_unpicked_txn) to pick transactions out of a lexagraphically
sorted list) ... and this is without any prediction at all-- randomly
ordered txn in the block would work just as well.

[E.g. using the uint coder from the daala video codec project can code
these values with about 1% overhead, and runs at about 12MB/sec doing
so on my slow laptop]

Recently some folks have been working privately on a block network
coding implementation... earlier attempts (even before IBLT became
trendy) were thwarted by the same thing that thwarts IBLT: the
decoding was so slow it dominated the latency. We've found some faster
coding schemes though... so it looks like it might be practical now. I
could send you more info if you read the block network coding page and
are interested in helping.

Both IBLT and BNC would both be more useful in the weakblocks model
because there the decode speed isn't latency critical-- so if it needs
100ms of cpu time to decode an efficiently encoded block, that is no
big deal.


Yes, it's perfectly reasonable to do that for bandwidth minimization--
though it doesn't minimize latency.  "Seen" is complex, you have no
guarantee a peer will accept any transaction you've sent it, or even
that it will retain any it sent you. So multiple round trips are
required to resolve missing transactions.

We haven't bothered implementing this historically because the
bandwidth reduction is small overall, and it's not the right strategy
for reducing latency-- the vast majority of bandwidth is eaten by
relay. Right now maybe 15% is used by blocks... so at most you'd get a
15% improvement here.

I did some fairly informal measurements and posted about it:
https://bitcointalk.org/index.php?topic=1377345.0

I also point out there that the existing blocksonly mode achieves
bandwidth optimal transport already (ignoring things like transaction
format compression)... just so long as you don't care about learning
about unconfirmed transactions. :)


If you sort by data (or ID) without requiring the verifier to
topologically sort then an efficient permutation coder would only
spend bits on places where dependencies push things out of the
expected order... which is fairly rare.

Seems like a reasonable cost for avoiding the hardfork, no? The
receiver topo sort requirement would also require more memory in a
block verifier; and would be more complex to fraud proof, I think.

Engineering wise it's not quite so simple. It's helpful for miners to
have blocks sorted by feerate so that later stages of the mining
process can drop the least profitable transactions simply by
truncating the block.


I tried to get Matt to do that for his stuff previously; pointing out
the sorted indexes would be easier to efficiently code. His
counterargument was that for 2000 txn, the two bytes indexes take 4kb,
which is pretty insignificant... and that his time would be better
spent trying to get the hit-rate up. I found that hard to argue with.
:)

----------
From: Anthony Towns <aj@erisian.com.au>
Date: Mon, Mar 14, 2016 at 3:08 AM
To: Gregory Maxwell <gmaxwell@gmail.com>


On Sun, Mar 13, 2016 at 05:06:25AM +0000, Gregory Maxwell wrote:

Yup.


If the block can be encoded fully, then it's up to maybe 10kB per block
max (at 1MB blocksize); I don't think multiple transmissions matter much
in that case? Hmm, maybe it does...


Ugh, patents. Interesting that the patents on turbo codes have expired,
last time I looked they hadn't.


Yeah, that makes sense I think. Pretty complicated though. The "someone
sent corrupt data" seems a little bit problematic to deal with too,
especially in the "optimistically forward stuff before you can validate
it" phase. At least if you're using error correcting codes anyway,
that's probably a self-solving problem.

What's with the switch from 32 bit faux ids in the original section
to 63 bits in the reimagination? I guess you use most of that for the
additional encoded length though...

Keying with the previous block's hash seems kind-of painful, doesn't it?
Once you receive the ids, you want to lookup the actual transactions
from your mempool, but since you can't decrypt anything useful with
only the first 50/60 bits of cyphertext, the only way to do that is
to have already cycled through all the transactions in your mempool
and pre-calculated what their network coded id for that block is, and
you have to do that everytime you receive a block (including orphans,
I guess). It'd make reorgs more expensive too, because you'd have to
reindex all the mempool then as well?

Maybe if you're only doing that predictively it's not so bad? The 5MB-20MB
of txes with highest fees get coded up, and you just download any other
transactions in full? If you're downloading large coinbase txes regularly
anyway, that's probably no big deal.


Yeah, that seemed a little odd to me; there shouldn't be that much
hashing to validate a block (1MB of transactions, then maybe 128kB to
get to sha256d, then another 2*128kB for the rest of the merkle tree?).
Matt's code seems like it's doing a linear search through the tx index
to find each tx though, which probably doesn't help.


Heh.


Right, but 5.2 kB is a lot of overhead; at least compared to the cases
where Matt's stuff already works well :)


Sure. (Though, fair warning, I've already failed a few times at doing
anything useful with erasure coding...)

away.

The "p2p relay" in my head has "seen" meaning "the 5MB of transactions
the listening peer thinks is most likely to be mined", odds on both
peers have actually seen something like 145MB of additional transactions
too. You don't do round trips; you just start sending the "unseen"
transactions automatically (by id or in full?), then you send the
compressed block. The only round trip is if you sent the id, but they
actually needed the full tx.

In my head, you get good latency if you do weak blocks beforehand,
and somewhat poorer latency if you don't. Even in my head, I'm not sure
that's actually feasible, though: I'm not sure weak blocks for coinbase
transactions really work, and comparatively high latency on 5% of blocks
that didn't get any weak blocks beforehand isn't very attractive...


Yeah, I'm assuming a non-trivial increase in bandwidth usage compared
to current relay. Compared to relaying spam transactions (that don't
get mined prior to expiry), not sure it's significant though.

validated

Really? I was seeing a lot of transaction chains in the couple of blocks I
looked at. Also, you wouldn't get short proofs that a transaction isn't
present in a block that way either afaics.


Hmm, I think it'd be easy to fraud proof -- just show adjacent merkle
paths where the results are in the wrong order. Maybe the same's true
with the id-order-but-toposorted too -- just show adjacent merkle paths
where the results are in the wrong order, and the later doesn't depend
on the former. I'm not sure that gives a unique sort though (but maybe
that doesn't actually matter).


Yeah; not having ordering requirements seems far more practical.


Yeah. Having the bitcoin mempool and fee info (and heck, priority info)
more readily available when seeing new transactions and choosing what to
include seems like it'd be helpful here. Seems relatively painful to do
that outside of bitcoin though.

Cheers,
aj


----------
From: Gregory Maxwell <gmaxwell@gmail.com>
Date: Mon, May 15, 2017 at 8:03 PM
To: Anthony Towns <aj@erisian.com.au>


I ran into someone proposing the same thing as you. Can I share this
discussion with them? (with the public?)

----------
From: Anthony Towns <aj@erisian.com.au>
Date: Mon, May 15, 2017 at 11:00 PM
To: Gregory Maxwell <gmaxwell@gmail.com>


Yes, go ahead on both counts.
--
Sent from my phone.
-------------------------------------


Apologies for interjecting a thought on this topic.
My belief is that Bitcoin could grow freely, and become worth enough so that mining becomes profitable even for those of us in countries without free / subsidized electricity.

By that time, buying commodity mining equipment (ASIC-based) from major manufacturers should be no problem, esp. not for existing Bitcoin holders.

I see no sign that current major miners are principally opposed to such a natural process of decentralization of Bitcoin mining.

Sancho
-------------------------------------
On Mon, Jul 10, 2017 at 12:50:21PM -0400, Paul Sztorc via bitcoin-dev wrote:

Timelines have good and bad points (in this context, I'd generally call
projections good, deadlines bad :); people have interpreted the lack of
any clear timeline for a hardmark on the 2015 roadmap as no plan for a
hard fork at all; meanwhile the overly optimistic timeline for segwit
being "ready" in April or July last year has been interpreted as "ready
for use" and treated as a failure, when it didn't work out that way.

I think it would be helpful for the development community to have some
way of talking about timelines, for instance to be able to say "the
*minimum* timeline for a reasonable hard fork is 6 months for proposal
review, speculative analysis and initial coding, 3 months for concrete
proposal review and thorough testing, 3-6 months for consensus to develop
on whether to lock the proposed changes in as the new consensus, and
a further 6-24 months for wide scale deployment to occur before any
behavioural change to take actual effect".

Those numbers give a lead time of 18 to 38 months of engagement with the
developer community before it takes effect, as compared to the six month
timeline of the New York agreement. 18 months implies that a block size
increase would be *available* today if people wanting larger blocks had
engaged with the development community from January 2016 in the same
way that segwit was developed, rather than working in their own sandboxes.

That could have happened: the proposals in 

  https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011995.html

from Dec 2015 could have been engaged with, and, optimistically, we could
have a non-controversial deployment of SpoonNet already if they had been.

It might be a good idea to actually engage with investors and businesses
on this: the point of the timelines above isn't to slow things down for
its own sake, it's that you need to take time in order to think through
the potential consequences of changes, and avoid unintended bad outcomes.
That seems like something that investors and businesses can understand,
and endorse up front -- and they could meaningfully do so simply by saying
"any hard-fork that does not go through each of the stages for at least
the minimum time will be treated as an altcoin rather than an upgrade
of bitcoin". But the process has to be "here is what it takes from a
technical POV to avoid fucking up bitcoin; does your company endorse
being responsible with other people's money despite the costs of doing
so?" If you're in a move-fast-and-break-things mode, the answer might
legitimately be "no", of course.


I'd suggest dividing the activities into phases more clearly; maybe:

 - Already available to users:
     * version bits
     * compact block relay
     * FIBRE
     * CSV
     * better fee estimation

 - Awaiting consensus:
     * segregated witness

 - Active development / concrete specifications:
     * lightning network
     * light client mode for bitcoin core (PR#10794)

 - Draft proposals at experimental stage:
     * transaction compression? (or is this the already deployed stuff?)
     * schnorr sig aggregation
     * drivechain
     * spoonnet
     * mimblewimble
     * block size increases

 - Ideas that aren't even experiments yet
     * asicboost prevention


As above, it seems to me that 18 months of engagement is likely a bare
minimum amount of time for a robustly implemented hard fork (6 months is
almost exactly segwit2x's total timeline, from proposal in late May as
the New York Agreement to the new rules being available in mid-November,
and it doesn't look at all robust to me).  

Possibly if the existing features of spoonnet are already adequate,
you could cut that down by a few months. But realistically, that says
to me early 2019 at the absolute earliest, and if engagement with the
development process doesn't start tomorrow, later than that.

FWIW, here's a longer form draft of what I think hard fork guidelines
maybe could look like:

  https://gist.github.com/ajtowns/914cf2309822bff357cda4ab3f48a966

It's obviously blatantly contradictory with support of the NYA/segwit2x,
but at this point I think that's true of any process that's not just
a rephrasing of "move fast and break things".


Publishing something like this as an informational BIP every year or
two seems like a good idea to me.

Instead of a "roadmap" (with the implication that there's a schedule
people might rely on and developers have to meet), maybe just have it as
a list of the current high impact scaling features being worked on --
where the purpose of publishing the list is to let people understand
how far various ideas have progressed currently, and focus attention on:

  - wider adoption of already deployed features, by users, exchanges,
    wallets, etc; eg segwit doesn't scale anything if no one uses it
  - achieving activation of implemented features
  - encouraging R&D on approaches that are currently still experimental
    in order to make them actually usable

In that case, there's no actual need for guessing at future dates;
just the current status is sufficient.

Documenting current roadblocks might also be valuable (eg, lightning and
signature aggregation and drivechains etc are kind-of stalled waiting
on segwit's activation, I think; for a brief point, segwit was stalled
waiting on compact blocks, etc). Might not be worthwhile updating the doc
regularly to keep track of what's a roadblock though.

(I think you could usefully generalise beyond scaling to just "high
impact features" really)

Cheers,
aj


-------------------------------------
My point, if you missed it, is that there's a mathematical equivalence
between using two limits (and calculating the ratio) vs using one limit and
a ratio. The output is fully identical. The only difference is the order of
operations. Saying there's no blocksize limit with this is pretty
meaningless, because you're just saying you're using an abstraction that
doesn't make the limit visible.
-------------------------------------
An alternative to "training" users to understand SI prefixes could be to
make 100 satoshi = 1 mu, spelling out the Greek letter.

Although the Units <https://en.bitcoin.it/wiki/Units> page on the wiki has
been brought up to argue against naming 10,000 satoshi = 1 finney, I would
like to support this designation. It seems to be gaining some popular
support on Twitter & podcasts. So at $10,000 BTC/USD, 1 finney = $1.00. The
smallest unit of value would be 0.0001 finney = 1 satoshi. Finney has a
natural abbreviation as fin, and 100 mu = 1 finney.

The Units page also refers to "bitcent" as 0.01 BTC, but if a "bit" is 100
satoshi, then what is a "bitcent" in that context?

/bikeshed

@Natanael you're exactly right. There are already multiple uses of "bits"
within bitcoin itself.

@Sjors I don't think a redefinition of 'satoshi' is going to happen ;-)



-Clark

On Thu, Dec 14, 2017 at 4:01 PM, Natanael via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Tue, Dec 12, 2017 at 9:07 PM, Suhas Daftuar <sdaftuar@gmail.com> wrote:

The question becomes there-- how should it work.

In an ideal world, we'd decide what peers to fetch headers from based
on a compact proof of the total work in their chains... but we cannot
construct such proofs in Bitcoin today.

I think that instead of that a weak heuristic is to fetch first from
the peers with the tip at the highest difficulty. Then work backwards.

See: https://en.bitcoin.it/wiki/User:Gmaxwell/Reverse_header-fetching_sync

Which is the inspiration for the current headers first sync, but
without the reverse part because the protocol didn't permit it: you
can't request a linear wad of headers that come before a header. This
is the thing I was mostly thinking of when I mentioned that we may
want to change the interface.

-------------------------------------
On Tue, May 9, 2017 at 7:42 PM, Matt Corallo <lf-lists@mattcorallo.com> wrote:

Rawr.

-------------------------------------
On Mon, Aug 28, 2017 at 08:55:47PM +0000, Alex Nagy via bitcoin-dev wrote:

It's not a theoretical use-case: the two OpenTimestamps calendar servers I run
- {alice,bob}.btc.calendar.opentimestamps.org - use native P2WPKH segwit
outputs to keep transaction size to the absolute minimum possible; previously
they used bare CHECKSIG <pubkey> output scripts for the same reason.

I enabled support for it the moment segwit activated, so I'm probably the first
ever production user of P2WPKH on mainnet, and quite possibly, the first person
to create P2WPKH outputs on mainnet for any reason.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------------------------------
I was merely describing that the only failure mode of using "post-split coinbases from the legacy chain" as seedcoins for cointainting purposes would be a resolution of the coinsplit, thereby rendering the cointainting redundant, therefore this would be an entirely safe approach to cointainting, as the only way coins could become untainted (and therefore subject to the threat of replay attacks) would coincide with a disappearance of the situation that gave rise to such replay attacks in the first place. This should sufficiently answer your concerns regarding lack of replay protection in case of medium-to-long-term chainsplits in general. If you fail to grok, please read again until you don't.

Sent with [ProtonMail](https://protonmail.com) Secure Email.

-------- Original Message --------
Subject: Re: [bitcoin-dev] Replay attacks make BIP148 and BIP149 untennable
Local Time: June 7, 2017 3:38 AM
UTC Time: June 7, 2017 12:38 AM
From: contact@taoeffect.com
To: Kekcoin <kekcoin@protonmail.com>
Anthony Towns <aj@erisian.com.au>, bitcoin-dev@lists.linuxfoundation.org <bitcoin-dev@lists.linuxfoundation.org>

Please read my email more carefully; the replay threat would be moot because there would be no alternative chain to replay the TX on,

In order to *get to that point*, you need >51%.

Not only that, but, if you started out with <51%, then you need >>51% in order to *catch up* and replace the large number of blocks added to the legacy chain in the mean time.

So, since >51% is _required_ for BIP148 to succeed (and likely >>51%)... you might as well do as SegWit did originally, or lower the threshold to 80% or something (as BIP91 does).

Without replay protection at the outset, BIP148, as far as I can tell, isn't a threat to miners.

--
Please do not email me anything that you are not comfortable also sharing with the NSA.

On Jun 6, 2017, at 5:29 PM, Kekcoin <kekcoin@protonmail.com> wrote:

Please read my email more carefully; the replay threat would be moot because there would be no alternative chain to replay the TX on, as the non-148 chain would have been reorganized into oblivion.

Sent with [ProtonMail](https://protonmail.com/) Secure Email.

-------- Original Message --------
Subject: Re: [bitcoin-dev] Replay attacks make BIP148 and BIP149 untennable
Local Time: June 7, 2017 3:26 AM
UTC Time: June 7, 2017 12:26 AM
From: contact@taoeffect.com
To: Kekcoin <kekcoin@protonmail.com>
Anthony Towns <aj@erisian.com.au>, bitcoin-dev@lists.linuxfoundation.org <bitcoin-dev@lists.linuxfoundation.org>

I don't know what you mean by "render the replay threat moot."

If you don't have replay protection, replay is always a threat. A very serious one.

--
Please do not email me anything that you are not comfortable also sharing with the NSA.

On Jun 6, 2017, at 5:19 PM, Kekcoin <kekcoin@protonmail.com> wrote:

Hmm, that's not the difference I was talking about. I was referring to the fact that using "post-chainsplit coinbases from the non-148 chain" to unilaterally (ie. can be done without action on the 148-chain) taint coins is more secure in extreme-adverserial cases such as secret-mining reorg attacks (as unfeasibly expensive they may be); the only large-scale (>100 block) reorganization the non-148 chain faces should be a resolution of the chainsplit and therefore render the replay threat moot.
-------------------------------------
On Mon, May 15, 2017 at 11:04 PM, ZmnSCPxj via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Ya, lite nodes with UTXO sets are one of the the oldest observed
advantages of a commitment to the UTXO data:

https://bitcointalk.org/index.php?topic=21995.0

But it requires a commitment. And for most of the arguments for those
you really want compact membership proofs.  The recent rise in
interest in full block lite clients (for privacy reasons), perhaps
complements the membership proofless usage.

Pieter describes some uses for doing something like this without a
commitment.  In my view, it's more interesting to first gain
experience with an operation without committing to it (which is a
consensus change and requires more care and consideration, which are
easier if people have implementation experience).


For audibility and engineering reasons it would need to be be in
addition to rather than rather than, because the proof of work needs
to commit to the witness data (in that kind of flip, the transactions
themselves become witnesses for UTXO deltas) or you get trivial DOS
attacks where people provide malleated blocks that have invalid
witnesses.

-------------------------------------

I understand. Thank you for your explanation.


Another thought I have, is that instead of committing to the UTXO of the block, to commit to the UTXO of the previous block, and the merkle tree root of the transactions in the current block.

My thought is that this would help reduce SPV mining, as a miner would need to actually scan any received new blocks in order to create the UTXO set of the previous block. An empty block would make things easier for the next block's miner, not the current block's miner. However, I'm not sure if my understanding is correct, or if there is some subtlety I missed in this regard.

Regards,
ZmnSCPxj
-------------------------------------
Hello Devs,

I am Patrick Sharp. I just graduated with a BS is computer science. Forgive
my ignorance.

As per bip-0002 I have scoured each bip available on the wiki to see if
these ideas have already been formally proposed and now as per bip-0002
post these ideas here.

First and foremost I acknowledge that these ideas are not original nor new.

Trimming and demurrage:

I am fully aware that demurrage is a prohibited change. I hereby contest.
For the record I am not a miner, I am just aware of the economics that
drive the costs of bitcoin.

Without the ability to maintain some sort of limit on the maximum length or
size of the block chain, block chain is not only unsustainable in the long
run but becomes more and more centralized as the block chain becomes more
and more unwieldy.

Trimming is not a foreign concept. Old block whose transactions are now
spent hold no real value. Meaningful trimming is expensive and inhibited by
unspent transactions. Old unspent transactions add unnecessary and unfair
burden.

   - Old transactions take up real world space that continues incur cost
   while these transactions they do not continue to contribute to any sort of
   payment for this cost.
   - One can assume that anybody with access to their bitcoins has the
   power to move these bitcoins from one address to another (or at least that
   the software that holds the keys to their coins can do it for them) and it
   is not unfair to require them to do so at least once every 5 to 10 years.
   - Given the incentive to move it or lose it and software that will do it
      for them, we can assume that any bitcoin not moved is most likey
therefore
      lost.
      - moving these coins will cost a small transaction fee which is fair
      as their transactions take up space, they need to contribute
      - most people who use their coins regularly will not even need to
      worry about this as their coins are moved to a change address anyway.
   - one downside is that paper wallets would then have an expiration date,
   however I do not think that a paper wallet that needs to be recycled every
   5 to 10 years is a terrible idea.

Therefore I propose that the block chain length be limited to either 2^18
blocks (slightly less than 5 years) or 2^19 blocks, or slightly less than
10 years. I propose that each time a block is mined the the oldest block(s)
(no more than two blocks) beyond this limit is trimmed from the chain and
that its unspent transactions are allowed to be included in the reward of
the mined block.

This keeps the block chain from tending towards infinity. This keeps the
costs of the miners balanced with the costs of the users.

Even though I believe this idea will have some friction, it is applicable
to the entire community. It will be hard for some users to give up small
benefits that they get at the great cost of miners, however miners run the
game and this fair proposal is in in their best interest in two different
ways. I would like your thoughts and suggestions. I obviously think this is
a freaking awesome idea. I know it is quite controversial but it is the
next step in evolution that bitcoin needs to take to ensure immortality.

I come to you to ask if this has any chance of acceptance.

-Patrick
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 05/11/2017 01:36 PM, Aymeric Vitte via bitcoin-dev wrote:

I agree. Every full node operator should (and likely would at some
point) simply never connect to, or relay the address of, any "peer"
advertising itself as diminished. Why on earth would a full node
operator want to encourage shrinking support for bootstrapping and
deep reorgs, resulting in greater load for himself. That's a race to
the bottom.

We are literally talking about $7.50 for the *entire chain* with
breathing room. If someone wants to save a few dollars they are better
off attempting to hide their pruning.

e

93f4358c5ae/src/validation.h#L204
But maybe I’m confused.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBCAAGBQJZFNHtAAoJEDzYwH8LXOFOYRwH/0By+TNSgnV6m4c7g1ZrjboG
8fZSeGaz7FXmAUZ69XMdQ1H+wlP0e4OAz9eRCcVqcn3K9OZJn++hbzI2K+zijyxZ
cpQjg/2dcTc4B0Z3PZdnuZx5mnHzavr/1vPlgOYla7AbYqcKB5dkq/HqgD6tFsJP
HXPClbEkVRF6UFP/7sDfzW8FMJycMSVcbEpuWAhcx2d+NusywWhbkuc5NiT9J1Ug
/3OFhHVJtd+rDl2B4iRHXHOhysUGffvmmLANZpPMcOgplM6Xwv7nMT34FV4HCdgs
Gyxc9GSFsD6xsOshBRPICtEZe+IDDb0cnOLjDdAnUnKeolUljFY52djSa300Fp0=
=REyc
-----END PGP SIGNATURE-----

-------------------------------------
I believe I have come up with a structure that allows for trustless use of
hybrid wallets that would allow for someone to use a hybrid wallet without
having to trust it while still allowing for emergency recovery of funds in
the case of a lost wallet. It would run off of this TX script:

IF
     1 <clientRecoveryPubKey> <serverRecoveryPubKey> 2 CHECKMULTISIGVERIFY
ELSE
     2 <userWalletPubKey> <serverWalletPubKey> 2 CHECKMULTISIG
ENDIF

A typical transaction using this would involve a user signing a TX with
their userWalletPrivKey, authenticating with the server, possibly with 2FA
using a phone or something like Authy or Google Authenticator. After
authentication, the server signs with their serverWalletPrivKey.

In case the server goes rogue and starts refusing to sign, the user can use
their userRecoveryPrivKey to send the funds anywhere they choose. Because
if this, the userRecoveryPrivKey is best suited to cold wallet storage.

In the more likely event that the user forgets their password and/or looses
access to their userWalletPrivKey as well as loses their recovery key, they
rely on the serverRecoveryPrivKey.

When the user first sets up their wallet, they answer some basic identity
information, set up a recovery password, and/or set up recovery questions
and answers. This information is explicitly NOT sent to serve with the
exception of recovery questions (although the answers remain with the user,
never seeing the server). What is sent to the server is it's 256 bit hash
used to identify the recovery wallet. The server then creates a 1025 bit
nonce, encrypts it, stores it, and transmits it to the user's client.

Meanwhile, the user's wallet client generates the serverRecoveryPrivKey.

Once the client has both the serverRecoveryPrivKey, and the nonce, it uses
SHA512 on the combination of the identity questions and answers, the
recovery password (if used), the recovery questions and answers, and the
nonce. It uses the resulting hash to encrypt the serverRecoveryPrivKey.

Finally, the already encrypted key is encrypted again for transmission to
the server. The server decrypts it, then rencrypts it for long term storage.

When the user needs to resort to using this option, they 256 bit hash their
information to build their recovery identifier. The server may, optionally,
request e-mail and or SMS confirmation that user is actually attempting the
recovery.

Next, the server decrypts the saved nonce, as well as the first layer of
encryption on the serverRecoveryPrivKey, then encrypts both for
transmission to the user's client. Then the client removes the transmission
encryption, calculates the 512 bit hash that was used to originally encrypt
the serverRecoveryPrivKey by using the provided information and the nonce.

After all of that the user can decrypt the airbitzServerRecoveryPrivKey and
use it to send a transaction anywhere they choose.

I was thinking this may make a good informational BIP but would like
feedback.
-------------------------------------
OK, so nForkId 0 is exactly the "valid on all chains" specifier I was
asking about, cool.  And your LN example (and nLockTime txs in general)
illustrate why it's preferable to implement a generic replay protection
scheme like yours *in advance*, rather than before each fork: all ad hoc RP
schemes I know of break old txs on one of the chains, even when that's not
desirable - ie, they offer no wildcard like nForkId 0.

One comment on your LN example: users would have to take note that nForkId
0 txs would be valid not only on future forks, but on *past* forks too.
Eg, if BCH had been deployed with nForkId 2, then a user setting up BTC LN
txs now with nForkId 0 would have to be aware that those txs would be valid
for BCH too.  Of course the user could avoid this by funding from a
BTC-only address, but it is a potential minor pitfall of nForkId 0.  (Which
I don't see any clean way around.)


On Fri, Nov 10, 2017 at 6:28 AM, Mats Jerratsch <mats@blockchain.com> wrote:

-------------------------------------
On 3/26/2017 1:22 PM, Bryan Bishop via bitcoin-dev wrote:

A reasonable miner automatically checks every transaction seen, to see
if it might be valid with his own outputs substituted.

-------------------------------------
Google recommeds "migrate to safer cryptographic hashes such as SHA-256 and
SHA-3"
It does not mention RIPEMD-160

https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html?m=1


Em 25/02/2017 10:47, "Steve Davis via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> escreveu:


wrote:
about RIPEMD-160 which is the foundation of Bitcoin addresses?
160bits isn't enough.

…so far. I wonder how long that vacation will last?


...but we can be sure that it will be, since the dollar value held in
existing utxos continues to increase...

attack

Does that offer any greater protection? That’s not so clear to me as the
outputs (at least for p2pkh) only verify the public key against the final
20 byte hash. Specifically, in the first (notional) case the challenge
would be to find a private key that has a public key that hashes to the
final hash. In the second (realistic) case, you merely need to add the
sha256 hash into the problem, which doesn’t seem to me to increase the
difficulty by any significant amount?


/s
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
As an independently verifiable, decentralized store of public information, the Bitcoin block tree and transaction DAG do have an advantage over systems such as Visa. The store is just a cache. There is no need to implement reliability in storage or in communications. It is sufficient to be able to detect invalidity. And even if a subset of nodes fail to do so, the system overall compensates.

As such the architecture of a Bitcoin node and its supporting hardware requirements are very different from an unverifiable, centralized store of private information. So in that sense the comparison below is not entirely fair. Many, if not most, of the high costs of a Visa datacenter do not apply because of Bitcoin's information architecture.

However, if the system cannot remain decentralized these architectural advantages will not hold. At that point your considerations below are entirely valid. Once the information is centralized it necessarily becomes private and fragile. Conversely, once it becomes private it necessarily becomes centralized and fragile. This fragility requires significant investment by the central authority to maintain.

So as has been said, we can have decentralization and its benefit of trustlessness or we can have Visa. We already have Visa. Making another is entirely uninteresting.

e 


-------------------------------------


It is true, there is no question. The fact that an attack does not appear to have occurred does not mean that the vulnerability exists. It is as you say a trivial exploit, which means it will happen when the economic incentive is great enough. Analogous attacks on other points of centralization are already well underway.

e
-------------------------------------
On Fri, Jul 14, 2017 at 12:20 AM, Dan Libby via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


You would also have to ensure that everyone you give your addresses to
follows the same rule.  As time passes, there would be fewer and fewer
people who have "clean" outputs.

transferring money to "anyone-can-spend" outputs.  This outputs are
completely unprotected.  Literally, anyone can spend them.  (In practice,
miners would spend them, since why would they include a transaction that
sends "free money" to someone else).

If you run an old node, then someone could send you a transaction that only
spends segwit outputs and you would think it is a valid payment.

Imagine that there are only 3 UTXOs (Alice, Bob and Carl have all the
Bitcoins).

UTXO-1:  Requires signature by Alice (legacy output)

UTXO-2: Anyone can pay (but is actually a segwit output that needs to be
signed by Bob)

UTXO-3: Anyone can pay (but is actually a segwit output that needs to be
signed by Carl)

Only Bob can spend UTXO-2, since it needs his signature.

Anyone could create a transaction that spends UTXO-2 and it would look good
to all legacy nodes.  It is an "anyone can spend" output after all.

However, if they submit the transaction to the miners, then it will be
rejected, because according to the new rules, it is invalid (it needs to be
signed by Bob).

Once a soft fork goes through, then all miners will enforce the new rules.

A miner who added the transaction to one of his blocks (since it is valid
under the old rules) would find that no other miners would accept his block
and he would get no fees for that block.  This means that all miners have
an incentive to upgrade once a soft fork activates.

His block would be accepted by legacy nodes, for a short while.  However,
since 95% of the miners are on the main chain, their chain (which rejects
his block) would end up the longest.

If you are running a legacy client when a soft fork comes in, then you can
be tricked with "zero confirm" transactions.  The transaction will look
good to you, but will be invalid under the new rules.  This makes your
client think you have received (a lot of) money, but in practice, the
transaction will not be accepted by the miners.



If you wanted, you could mark any transaction that has a segwit looking
output as "dirty" and then all of its descendants as dirty.

However, pretty quickly, only a tiny fraction of all bitcoins would be
clean.

I suppose that it would be possible without modifying any rule to

Right.

I think a reasonably compromise would be to assume that all transactions
buried more than a few hundred blocks deep are probably ok.  Only segwit
looking outputs would be marked as "uncertain".
-------------------------------------
On Fri, Sep 29, 2017 at 12:45 PM, Andreas Schildbach via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Who's payment protocol SSL cert was expired for months without even
generating a post on reddit.  Not exactly convincing there.

The fact that someone supports it doesn't mean its being used.

-------------------------------------
Hi Pieter,

I wanted to say that I thought this write-up was excellent!  And efficiently hashing the UTXO set in this rolling fashion is a very exciting idea!! 

Peter R



-------------------------------------
Good morning Damian,

The primary problem in your proposal, as I understand it, is that the "transaction pool" is never committed to and is not part of consensus currently.  It is unlikely that the transaction pool will ever be part of consensus, as putting the transaction pool into consensus is effectively lifting the block size to include the entire transaction pool into each block.  The issue is that the transaction pool is transient and future fullnodes cannot find the contents of the transaction pool at the time blocks were made and cannot verify the correctness of historical blocks.  Also, fullnodes using -blocksonly mode have no transaction pool and cannot verify incoming blocks for these new rules.

Applying a patch that follows this policy into Bitcoin Core without enforcing it in all fullnodes will simply lead to miners removing the patch in software they run, making it an exercise in futility to write, review, and test this code in the first place.

In addition, you reuse the term "weight" for something different than its current use.  Current use, is that the "weight" of a transaction, is the computed weight from the SegWit weight equation, measured in virtual units called "sipa", using the equation (4sipa / non-witness byte + 1sipa/witness byte).

Regards,
ZmnSCPxj

Sent with [ProtonMail](https://protonmail.com) Secure Email.

-------------------------------------
Have you read the cuckoo cycle paper?  Finding cycles in massive graphs is
just about the worst thing to use an ASIC for.

It might be a hitherto before unknown emergent property of cryptocurrencies
in general that POW *must* change every 7-9 years.  Could bake that into
the protocol too...

On Apr 9, 2017 7:51 PM, "David Vorick" <david.vorick@gmail.com> wrote:

-------------------------------------
Natanael,

=== Metal Layers ===

One factor in chip cost other than transistor count is the number of layers required to route all the interconnects in the desired die area constraint. The need for fewer layers can result in less patent-able costs of layering technology. Fewer layers are quicker and easier to manufacture.

I'm not an expert in the field, and I can't vouch for the validity of the entirety of the paper, but this paper discusses various factors that impact chip cost design.
http://www.cse.psu.edu/~juz138/files/3d-cost-tcad10.pdf

=== Early nonce mixing, Variable Length Input with Near Constant Work ===

To minimize asicboost like optimizations... the entirety of the input should be mixed with the nonce data ASAP. For example with Bitcoin as it is now, the 80 byte block header doesn't fully fit in one 64 byte SHA256 input block. This results in a 2nd SHA256 block input that only has 4 bytes of nonce and the rest constant that are mixed much later than the rest of the input... which allows for unexpected optimizations.

Solution: A hash algorithm that could have more linear computation time vs input size would be a 2 stage algorithm:
1. 1st stage Merkle tree hash to pre-lossy-mix-compress the variable length input stream to the size of the 2nd stage state vector. Each bit of input should have about equal influence on each of the output bits. (Minimize information loss, maximize mixed-ness).
2. Multi-round mixing of the 2nd stage, where this stage is significantly more work than the 1st stage.

This is somewhat done already in Bitcoin by the PoW doing SHA256 twice in serial. The first time is pretty much the merkle tree hash (a node with two children), and then the second time is the mult-round mixing. If the Bitcoin PoW did SHA256 three or four times or more, then asicboost like optimizations would have less of an effect.

In actual hardware, assuming a particular input length for the design can result in a significantly more optimized design than creating hardware that can handle a variable length input. So your design goal of "not linear in performance relative to input size" to me seems to be a hard one to attain... in practical, to support very large input sizes in a constant work fashion requires a trade off between memory/parallelization and die space. I think it would be better to make an assumption about the block header size, such as that it is exactly 80 bytes, or, at least something reasonable like the hardware should be able to support a block header size <= 128 bytes.

Cheers,
Praxeology Guy
-------------------------------------

After mulling over this proposal I think it is quite elegant; however there is one big "regression" in functionality in regards to BIP9 which it extends upon; a lack of back-out procedure. That is to say, if a protocol change is deployed using this BIP9-with-lock-in-on-timeout method, it is no longer possible to abstain from activating it if it is shown to contain a critical flaw.

I suggest that a second version bit can be used as an abandonment vote; with sufficient hashpower (50% might be enough since it is no longer about safe coordination of protocol change deployment) the proposed protocol change is abandoned. This changes the dynamic from BIP9's "opt-in" to an "opt-out" system, still governed by hashpower, but far less susceptible to minority miner veto.
-------------------------------------


On Sun, Apr 9, 2017, at 00:12, Gregory Maxwell wrote:

Although I don't quite follow the details (CNB post-test? Connect block
I assume?), the risks you are describing seem to be rather specific to
Core's implementation. For one, bitcrust does not or use need reorgs at
all.

Do you argue (or can you further explain) that the idea of splitting
script validation (or what you call mempool pre-validation), and order
validation is introducing risks  inherent to the protocol? 

Thanks,
Tomas

-------------------------------------
Hi Zmn,

I'm actually not sure that the existence of these tools makes the
attacker's collective action problem that much easier to solve.

As I said: "...even the most straightforward attack (of "a 51% hashrate
group attacks a sidechain and distributes the proceeds to the group
proportional to hashpower") is actually one that contains a difficult
(and potentially interminable) negotiation."

But even under your scheme, there is someone who has to seek out the
Accomplices, and has to try to figure out what is acceptable to pay
them. This sparks a tiresome negotiation that drains both parties of
time and effort and might potentially last forever. Problematically,
there is a Market for Lemons problem with respect to how many blocks an
Accomplice "will" mine. If many people try to be Thieves at once, then
each individual Thief has less of an incentive to bother trying to steal
in the first place.

And so, even if your scheme does work, the improvement seems small. And
even if the improvement is very great, the remaining collective action
problem is still more difficult than the one in the comparative "reorg
case" (in which the problem is just to "pick the block number from which
to start the reorg").

Paul



On 12/5/2017 11:49 PM, ZmnSCPxj wrote:




-------------------------------------
I'm not advocating. I'm mediating.


This is out of

On Wed, May 10, 2017 at 1:39 PM, Matt Corallo <lf-lists@mattcorallo.com>
wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version 0.14.1 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.14.1/>

Or, by torrent:

  magnet:?xt=urn:btih:0482be8fc8e1c0b02162871e3591efc3d1d34585&dn=bitcoin-core-0.14.1&tr=udp%3A%2F%2Fpublic.popcorn-tracker.org%3A6969%2Fannounce&tr=http%3A%2F%2Fatrack.pow7.com%2Fannounce&tr=http%3A%2F%2Fbt.henbt.com%3A2710%2Fannounce&tr=http%3A%2F%2Fmgtracker.org%3A6969%2Fannounce&tr=http%3A%2F%2Fopen.touki.ru%2Fannounce.php&tr=http%3A%2F%2Fp4p.arenabg.ch%3A1337%2Fannounce&tr=http%3A%2F%2Fpow7.com%3A80%2Fannounce&tr=http%3A%2F%2Ftracker.dutchtracking.nl%3A80%2Fannounce

This is a new minor version release, including various bugfixes and
performance improvements, as well as updated translations.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

To receive security and update notifications, please subscribe to:

  <https://bitcoincore.org/en/list/announcements/join/>

Compatibility
==============

Bitcoin Core is extensively tested on multiple operating systems using
the Linux kernel, macOS 10.8+, and Windows Vista and later.

Microsoft ended support for Windows XP on [April 8th, 2014](https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support),
No attempt is made to prevent installing or running the software on Windows XP, you
can still do so at your own risk but be aware that there are known instabilities and issues.
Please do not report issues about Windows XP to the issue tracker.

Bitcoin Core should also work on most other Unix-like systems but is not
frequently tested on them.

Notable changes
===============

RPC changes
- -----------

- - The first positional argument of `createrawtransaction` was renamed from
  `transactions` to `inputs`.

- - The argument of `disconnectnode` was renamed from `node` to `address`.

These interface changes break compatibility with 0.14.0, when the named
arguments functionality, introduced in 0.14.0, is used. Client software
using these calls with named arguments needs to be updated.

Mining
- ------

In previous versions, getblocktemplate required segwit support from downstream
clients/miners once the feature activated on the network. In this version, it
now supports non-segwit clients even after activation, by removing all segwit
transactions from the returned block template. This allows non-segwit miners to
continue functioning correctly even after segwit has activated.

Due to the limitations in previous versions, getblocktemplate also recommended
non-segwit clients to not signal for the segwit version-bit. Since this is no
longer an issue, getblocktemplate now always recommends signalling segwit for
all miners. This is safe because ability to enforce the rule is the only
required criteria for safe activation, not actually producing segwit-enabled
blocks.

UTXO memory accounting
- ----------------------

Memory usage for the UTXO cache is being calculated more accurately, so that
the configured limit (`-dbcache`) will be respected when memory usage peaks
during cache flushes.  The memory accounting in prior releases is estimated to
only account for half the actual peak utilization.

The default `-dbcache` has also been changed in this release to 450MiB.  Users
who currently set `-dbcache` to a high value (e.g. to keep the UTXO more fully
cached in memory) should consider increasing this setting in order to achieve
the same cache performance as prior releases.  Users on low-memory systems
(such as systems with 1GB or less) should consider specifying a lower value for
this parameter.

Additional information relating to running on low-memory systems can be found
here:
[reducing-bitcoind-memory-usage.md](https://gist.github.com/laanwj/efe29c7661ce9b6620a7).

0.14.1 Change log
=================

Detailed release notes follow. This overview includes changes that affect
behavior, not code moves, refactors and string updates. For convenience in locating
the code changes and accompanying discussion, both the pull request and
git merge commit are mentioned.

### RPC and other APIs
- - #10084 `142fbb2` Rename first named arg of createrawtransaction (MarcoFalke)
- - #10139 `f15268d` Remove auth cookie on shutdown (practicalswift)
- - #10146 `2fea10a` Better error handling for submitblock (rawodb, gmaxwell)
- - #10144 `d947afc` Prioritisetransaction wasn't always updating ancestor fee (sdaftuar)
- - #10204 `3c79602` Rename disconnectnode argument (jnewbery)

### Block and transaction handling
- - #10126 `0b5e162` Compensate for memory peak at flush time (sipa)
- - #9912 `fc3d7db` Optimize GetWitnessHash() for non-segwit transactions (sdaftuar)
- - #10133 `ab864d3` Clean up calculations of pcoinsTip memory usage (morcos)

### P2P protocol and network code
- - #9953/#10013 `d2548a4` Fix shutdown hang with >= 8 -addnodes set (TheBlueMatt)
- - #10176 `30fa231` net: gracefully handle NodeId wrapping (theuni)

### Build system
- - #9973 `e9611d1` depends: fix zlib build on osx (theuni)

### GUI
- - #10060 `ddc2dd1` Ensure an item exists on the rpcconsole stack before adding (achow101)

### Mining
- - #9955/#10006 `569596c` Don't require segwit in getblocktemplate for segwit signalling or mining (sdaftuar)
- - #9959/#10127 `b5c3440` Prevent slowdown in CreateNewBlock on large mempools (sdaftuar)

### Tests and QA
- - #10157 `55f641c` Fix the `mempool_packages.py` test (sdaftuar)

### Miscellaneous
- - #10037 `4d8e660` Trivial: Fix typo in help getrawtransaction RPC (keystrike)
- - #10120 `e4c9a90` util: Work around (virtual) memory exhaustion on 32-bit w/ glibc (laanwj)
- - #10130 `ecc5232` bitcoin-tx input verification (awemany, jnewbery)

Credits
=======

Thanks to everyone who directly contributed to this release:

- - Alex Morcos
- - Andrew Chow
- - Awemany
- - Cory Fields
- - Gregory Maxwell
- - James Evans
- - John Newbery
- - MarcoFalke
- - Matt Corallo
- - Pieter Wuille
- - practicalswift
- - rawodb
- - Suhas Daftuar
- - Wladimir J. van der Laan

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJY+1YNAAoJEHSBCwEjRsmme+YIAIkJLCjimADYBJoM8bnHK2Dc
4KznlAjpXqFWb6taoSyWi+/6DtZxJF1MZm+iaDhqmTEy+ms313N2mBEd2xrSAPPL
nYf84e3tgnwq07sQmUxVZXyhZe2R+m/kgy75TTZw+bonyXwwA3384F0L8gvV5Iu+
kNNu6WggWUTvOADEFVKecgzWeT1FCYXklbNk+Z5T/YWWrKA8ATXgv45IIEKT8uI1
pqhKQxoqLM3ga7Df3VbzwXAYAOCaFzf+0nmTRoqDM5pX+FQ2hq0UM6joLnUNk2ee
G4/nsNWAKg/6eycrA7Wvawwcozr2iYAov/YDj6pEI8UoeGcOdlSh69Seb1cngHg=
=EHlY
-----END PGP SIGNATURE-----

-------------------------------------
I'm not sure who wrote segwit.org, but I wouldn't take it as
authoritative reasoning why we must do X over Y.

You seem to be claiming that there is not cost for a miner to fill
"extra witness space", but this is very untrue - in order to do so they
must forgo fees on other transactions. Your analysis on worst-case vs
normal-case blocks also seems flawed - there is a single limit, and not
a separate, secondary, witness limit.

You suggested "If the maximum block weight is set to 2.7M, each byte of
non-witness block costs 1.7", but these numbers dont work out - setting
the discount to 1.7 gets you a maximum block size of 1.7MB (in a soft
fork), not 2.7MB. If you set the max block weight to 2.7 with a 1.7x
discount, you have a hard fork. If you set the discount to 2.7x with a
2.7 weight limit, you dont get 2.7MB average-sized blocks, but smaller,
and still have the potential for padding blocks with pure-witness data
to create larger blocks.

Additionally, note that by padding blocks with larger witness data you
lose some of the CPU cost to validate as you no longer have as many
inputs (which have a maximal validation cost).

Further, I'm not sure why you're arguing for a given witness discount on
the basis of a future hardfork - it seems highly unlikely the community
is in a position to pull something like that off, and even if it were,
why set the witness discount with that assumption? If there were to be a
hardfork, we should probably tweak a bunch of parameters (see, eg, my
post from February of last year at
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012403.html).

Maybe you could clarify your proposal a bit here, because the way I read
it you seem to have misunderstood SegWit's discount system.

On 05/09/17 13:49, Sergio Demian Lerner via bitcoin-dev wrote:

-------------------------------------


Much better analogy:

1. An ISV make software which makes use of an undocumented OS feature.
2. That feature is no longer present in the next OS release.
3. ISV suffers losses because its software cannot work under new OS, and
thus people stop buying it.

I think 99% of programmers would agree that this loss was inflicted by a
bad decision of ISV, and not by OS vendor changing OS internals. Relying on
undocumented features is something you do on your own risk.

I think it is ethically unambiguous to everyone who isn't on Bitmain's
payroll.
-------------------------------------
Sergio Demian Lerner: Please confirm that you understand that:

The current SegWit being proposed comes bundled with an effective 2MB block size increase.

Are you proposing the remove this bundled policy change, and then have a different BIP that increases the block size? Not quite clear if you understand what the current proposal is.

Cheers,
Praxeology
-------------------------------------
On Fri, Jan 27, 2017 at 03:47:20PM -0500, Greg Sanders via bitcoin-dev wrote:

As one of the authors of that paper and the source of the measurement
data I'd also like to point out that the 4MB number is indeed intended
as an optimistic upper bound on todays network capacity.

More importantly it's not a black and white situation, where there is
a magic number beyond which Bad Things (TM) happen, it's a spectrum on
which we can see a few threshold beyond which we _know_ Bad Things
definitely happen. Miner centralization pressure is felt earlier.

-------------------------------------
Currently in order to signal support for changes to Bitcoin, only miners are able to do so on the blockchain through BIP9.


One criticism is that the rest of the community is not able to participate in consensus, and other methods of assessing community support are fuzzy and easily manipulated through Sybil.


I was trying to think if there was a way community support could be signaled through transactions without requiring a hard fork, and without increasing the size of transactions at all.


My solution is basically inspired by hashcash and vanity addresses.


The output address of a transaction could basically have the last 4 characters used to signal support for a particular proposal.

To generate an address with 4 consecutive case-insensitive characters should be roughly 34^4 which is just over a million attempts. On typical hardware this should take less than a second.

An example bitcoin address that wanted to support the core roadmap might be:

1CLNgjuu8s51umTA76Zi8v6EdvDp8qCorE


or to signal support for a big block proposal might be:

1N62SRhBioRFrigS5eJ8kR1WYcfcYr16mB


Popularity could be measured weighted by fee paid per voting kb.


Issues are that this could lead to transactions been censored by particular miners for political reasons. Also miners might attempt to manipulate the results by stuffing their block with 'fake' transactions. Such attempts could be identified if a large number of voting transactions were not in the mempool.


Despite the limitations, I believe this offers a very accessible way to immediately allow the entire economic community to signal their support within transactions. The only cost is that of a tiny hashing PoW that should tie up a CPU for a barely noticeable amount of time, and could be implemented relatively easily into wallet software.


For its weaknesses, surely it is better than the existing methods we use to assess support from the wider economic community?


While it could just be used for signaling support and giving users a 'voice' on chain, if considered effective it could also be used to activate changes in the future.


Any thoughts welcome.


Thanks,


John Hardy

john@seebitcoin.com
-------------------------------------

Since the sidechain has the sidechain BMM headers that they want the LD
(bribe) data for, I think that they could specifically request LD data
relevant only to that sidechain by providing a list of hashes to the
mainchain, and the mainchain can return a list of boolean values telling
the sidechain if the LD data exists. That way the data never even has to
go over the network, just a verification that it exists on the mainchain
and we can drop the sidechain_id from the script.



Agreed, we need that.



No, OP_BRIBE (the old version) didn't care about the block height. Where
the blockheight was relevant is when bribe data is added to LD. In order
to be added to LD, the bribe needed to either be a fork (block height
less than the sidechain tip height) or extending the current side-chain
(block height = sidechain tip height + 1). The goal of this was to allow
for reorgs, and also make it so that people cannot skip forward (which
would never be valid so it's a waste of time and space) so that the
sidechain makes progress. With the new design that we have been talking
about, I think that we will need to drop this requirement from the
mainchain, and have the sidechain handle filtering out invalid LD data /
only requesting the verification of LD data that they know to be valid
as far as chain order goes.



Agreed. It might be helpful if you outlined the idea you had on IRC to
the mailing list.



I might be wrong but I thought that OP_RETURN outputs do not go into the
UTXO set. Anyone else want to chime in?






-------------------------------------
For what it’s worth, I think it would be quite easy to do better than the implied solution of rejiggering the message signing system to support non-P2PKH scripts. Instead, have the signature be an actual bitcoin transaction with inputs that have the script being signed. Use the salted hash of the message being signed as the FORKID as if this were a spin-off with replay protection. This accomplishes three things:

(1) This enables signing by any infrastructure out there — including hardware wallets and 2FA signing services — that have enabled support for FORKID signing, which is a wide swath of the ecosystem because of Bitcoin Cash and Bitcoin Gold.

(2) It generalizes the message signing to allow multi-party signing setups as complicated (via sighash, etc.) as those bitcoin transactions allow, using existing and future tools based on Partially Signed Bitcoin Transactions; and

(3) It unifies a single approach for message signing, proof of reserve (where the inputs are actual UTXOs), and off-chain colored coins.

There’s the issue of size efficiency, but for the single-party message signing application that can be handled by a BIP that specifies a template for constructing the pseudo-transaction and its inputs from a raw script.

Mark



-------------------------------------
Repeatedly hashing to make it so that lossy implementations just fail
sounds like a great idea. Also relying on a single crypto primitive which
is as simple as possible is also a great idea, and specifically using
blake2b is conservative because not only is it simple but its block size is
larger than the amount of data being hashed so asicboost-style attacks
don't apply at all and the logic of multiple blocks doesn't have to be
built.

Memory hard functions are a valiant effort and are holding up better than
expected but the problem is that when they fail they fail catastrophically,
immediately going from running on completely commodity hardware to only
running on hardware from the one vendor who's pulled off the feat of making
it work. My guess is it's only a matter of time until that happens.

So the best PoW function we know of today, assuming that you're trying to
make mining hardware as commodity as possible, is to repeatedly hash using
blake2b ten or maybe a hundred times.

Mind you, I still think hard forking the PoW function is a very bad idea,
but if you were to do it, that would be the way to go.
-------------------------------------
A correction to my previous email (because people are quoting me on
r/btc and what I wrote was wrong)

This statement is incorrect:

this could result in some miners using 0.13.0+ mining blocks which do
not propagate well and thus causing multiple chain splits and reorgs as
other miners find blocks for the same height before receiving a block
for that height.

Miners using 0.13.0+ will produce blocks that propagate well. This is
because 0.12.x- nodes will accept those blocks, and so will 0.13.0+.
Furthermore Core 0.13.0+ will use its outbound connections to connect to
segwit enabled peers so that it will be relaying segwit blocks to
someone. However Bitcoin Core 0.13.0+ will not request blocks from peers
that are not segwit enabled (because with segwit, they will be receiving
blocks without witnesses which are invalid to a segwit node), so they
will not receive blocks mined by a 0.12.x- node. This means that 0.12.x-
mined blocks propagate poorly, even though are valid. Because of the
poor propagation, a 0.13.0+ miner can find a block at the same height
which is more likely to get built upon. However, the poorly propagated
block can still reach other 0.12.x- miners who can build upon it due to
the low difficulty and difficulty resets, thus causing multiple chains
to exist, particularly among pockets of 0.12.x- nodes. The reorgs happen
when either the 0.12.x- nodes hear of the segwit blockchain that is
presumably longer because it has the majority hashrate, or when people
run bridges which allow 0.12.x- nodes relay blocks to 0.13.0+ nodes.

On 3/23/2017 7:14 PM, Andrew Chow wrote:

-------------------------------------
Ups. My mistake:  the mempool will not grow 400 times, the is no square
there.
I will initially grow 20 times. Multiplied by the number of times a
transaction may need to be replaced with one with higher fees. Maybe 50
times, but not 400.



On Thu, Apr 6, 2017 at 5:42 PM, Sergio Demian Lerner <
sergio.d.lerner@gmail.com> wrote:

-------------------------------------
I was thinking about something like this that could add the ability for
module extensions in the core client.

When messages are received, modules hooks are called with the message data.


They can then handle, mark the peer invalid, push a message to the peer or
pass through an alternate command.  Also, modules could have their own
private commands prefixed by "x:" or something like that.

The idea is that the base P2P layer is left undisturbed, but there is now a
way to create "enhanced features" that some peers support.

My end goal is to support using lightning network micropayments to allow
people to pay for better node access - creating a market for node services.


But I don't think this should be "baked in" to core.   Nor do I think it
should be a "patch".   It should be a linked-in module, optionally compiled
and added to bitcoin conf, then loaded via dlopen().    Modules should be
slightly robust to Bitcoin versions changing out from under them, but not
if the network layer is changed.   This can be ensured by a) keeping a
module version number, and b) treating module responses as if they were
just received from the network.   Any module incompatibility should throw
an exception...ensuring broken peers don't stay online.

In general I think the core reference would benefit from the ability to
create subnetworks within the Bitcoin ecosystem.   Right now, we have two
choices... full node and get slammed with traffic, or listen-only node, and
do nothing.

Adding a module/hook system would allow a complex ecosystem of
participation - and it would seem to be far more robust in the long term.

Something like this???

class MessageHookIn {
public:
    int hookversion;
    int64_t nodeid;
    int nVersion;
    int64_t serviceflags;
    const char *strCommand;
    const char *nodeaddr;
    const char *vRecv;
    int vRecvLen;
    int64_t nTimeReceived;
};

class MessageHookOut {
public:
    int hookversion;
    int misbehaving;
    const char *logMsg;
    const char *pushCommand;
    const unsigned char *pushData;
    int pushDataLen;
    const char *passCommand;
    CDataStream passStream;
};

class MessageHook {
public:
    int hookversion;
    std::string name;
    typedef bool (*HandlerType)(const MessageHookIn *in, MessageHookOut
*out);
    HandlerType handle;
};
-------------------------------------
This seems to be drifting off into alt-coin discussion.  The idea that we
can change the rules and steal coins at a later date because they are
"stale" or someone is "hoarding" is antithetical to one of the points of
bitcoin in that you can no longer control your own money ("be your own
bank") because someone can at a later date take your coins for some reason
that is outside your control and solely based on some rationalization by a
third party.  Once the rule is established that there are valid reasons why
someone should not have control of their own bitcoins, what other reasons
will then be determined to be valid?

I can imagine Hal Finney being revived (he was cryo-preserved at Alcor if
you aren't aware) after 100 or 200 years expecting his coins to be there
only to find out that his coins were deemed "stale" so were "reclaimed" (in
the current doublespeak - e.g. stolen or confiscated).  Or perhaps he
locked some for his children and they are found to be "stale" before they
are available.  He said in March 2013, "I think they're safe enough" stored
in a paper wallet.  Perhaps any remaining coins are no longer "safe enough."

Again, this seems (a) more about an alt-coin/bitcoin fork or (b) better in
bitcoin-discuss at best vs bitcoin-dev. I've seen it discussed many times
since 2010 and still do not agree with the rational that embracing allowing
someone to steal someone else's coins for any reason is a useful change to
bitcoin.




On Tue, Aug 22, 2017 at 4:19 AM, Matthew Beton via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I think we should go for 75%, same Litecoin. As I have said before, 95% threshold is too high even for unconventional soft forks.


-------------------------------------
On Thu, Apr 6, 2017 at 2:30 PM, Luv Khemani via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This is simply a non sequitur. These optimizations benefit users. On
the other hand, asicboost doesn't benefit users in any way, it only
benefits some miners if and only if not all miners use it. It
obviously harms the miners that aren't using it by making them less
profitable (maybe to the point that they lose money).
If all miners use it or if no one of them uses it is equivalent from
the point of view of the user. In fact, the very fact of allowing it
makes the network less secure unless every single honest miner uses
it, for an attacker could use it against the network.

Even if asicboost was good for users in any way (which as explained
isn't), this proposal doesn't disable it, only the covert form that
cannot be proven to be used.

Therefore there's no rational arguments to oppose this proposal unless
you are (or are invested in):

A) A Miner currently using the covert form of asicboost.

B) A Miner planning to use the covert form of asicboost soon.

C) An attacker using or planning to use the covert form of asicboost.


Asicboost doesn't seek this and doesn't help with this in any way.

-------------------------------------
Evening all,

The following BIP submission summarizes an idea that I've been tossing
around for the last year.  I understand that there may be nuances to SegWit
and current consensus layer mechanisms that I may not fully understand, so
please do not hesitate to shred the following text to pieces (I can handle
it, I promise!).

Please note that this BIP assumes failure of the current softfork version
of SegWit to activate in November -- something that I personally do not
wish to see(!). However, given the real possibility of that happening, or
perhaps just some newfound willingness (by "the community") to support a
hardfork in lieu of a stalemate, I figure now is as good a time as any to
share the idea in black and white.

I would really appreciate any/all feedback from the dev community on the
technical merits (read: feasibility) of the idea. I would especially
appreciate feedback from the SegWit developers who designed the current
implementation in 0.14, as they likely have the most intimate knowledge of
SegWit's nuances, and the entire BIP below would likely rely on their
willingness to develop a hardfork version.

Nothing in this BIP is set in stone -- including all values and timelines
-- but, I do hope the following text effectively captures the gist of the
idea, and I do thank you ahead of time for your consideration of the
proposal.


Respectfully,
Oliver Petruzel

-------------------------------------------------------------------------------

BIP:  TBD
Layer: Consensus (hard fork)
Title: Base Size Increase and Segregated Witness with Discount Governors
(SW-DGov) Hardfork
Author: Oliver Petruzel <opetruzel@gmail.com>
Comments-Summary: No comments yet.
Comments-URI:
Status: Draft
Type: Standards Track
Created: 2017-04-05
License: PD

Abstract

This BIP proposes a method of combining an immediate base size increase to
2MB and a hardfork version of Segregated Witness (SegWit).  The SegWit
portion of the hardfork will leverage Discount Governors to control (or
“govern”) the pace of the increase over a period of 145,152 blocks
(approximately three (3) years).


Motivation

Given the possibility of the current softfork version of SegWit failing to
activate in November 2017, this BIP aims to provide a hardfork alternative
that would provide every user in the ecosystem with the fixes and changes
they need to move forward with other great projects, while also tightly
controlling the rate at which the total weight of blocks increases during
the next three years.  The predictable nature of the increases will provide
miners, full node operators, and other users of the system with the ability
to plan their development, resources, and operations accordingly.  The
fixed nature of the increases will also allow all full nodes to maintain a
fixed set of rules for block validity (consensus).


Specification

The following changes will be made to the client:

* An immediate increase of base size to 2,000,000 bytes (perhaps leveraging
code changes similar to those described in BIP 109).

· A hardfork version of SegWit that maintains all of the fixes present in
the softfork version, including (but not limited to):
- Fix for the Malleability issue
- Linear scaling of sighash operations
- Signing of input values
- Increased security for multisig via pay-to-script-hash (P2SH)
- Script versioning
- Reducing UTXO growth
- Moving towards a single combined block limit

* In addition to those fixes listed above, the hardfork version of SegWit
will include the following:

- Rather than using the fixed (75%) Discount found in the softfork version
of SegWit, the hardfork version will leverage Discount Governing to control
the pace of total block weight increases over a three (3) year period of
time.  The use of Discount Governors will allow a steady increase over that
period from an immediate 2MB to 8MB total.  There are several ways these
increases can be handled – either by hardcoding the scheduled increases in
the initial hardfork, or perhaps using subsequent softforks (additional
input/discussion needed on the best way to handle the increases.
- Example increase schedule: +12.5% every 24,192 blocks (roughly every six
(6) months).  The increases would cap at the same 75% Discount rate found
in the current softfork version of SegWit.
- Each time the Discount increases – every 24,192 blocks -- the Total Block
Weight value would also increase to appropriately compensate for the added
Discount.


Rationale

This hardfork employs a simple flag day deployment based on the median
timestamp of previous blocks. Beyond this point, supporting nodes will not
accept blocks with original rules.  This ensures a deterministic and
permanent departure with the original rules.

The use of Discount Governors to control the pace of the increase will
result in a predictable and stable increase over the period of three (3)
years.

If, at any time, the increases present problems for the network -- such as
centralization concerns, negative impacts on the fee market(s), or other
unforeseen problems -- a softfork could be leveraged to halt the increases.

The pace of the increases is described using the following table:

Time -- Base Size (bytes) -- Total Discount -- Total Block Weight (bytes)
Flag Day (FD) -- 2,000,000 -- 0.00% -- 2,000,000
FD+24,192 Blocks -- 2,000,000 -- 12.5% -- 2,285,715
FD+48,384 Blocks -- 2,000,000 -- 25.0% -- 2,666,667
FD+72,576 Blocks -- 2,000,000 -- 37.5% -- 3,200,000
FD+96,768 Blocks -- 2,000,000 -- 50.0% -- 4,000,000
FD+120,960 Blocks -- 2,000,000 -- 62.5% -- 5,333,334
FD+145,152 Blocks -- 2,000,000 -- 75.0% -- 8,000,000

Based on the above, the "effective blocksize increase," or the number of
transactions per block, will also scale with each Discount increase.


Compatibility

This proposal requires a hardfork that does not maintain compatibility with
previous clients and rules for consensus.  It should not be deployed
without widespread consensus.

Wallet software and other applications will also need to be upgraded to
maintain compatibility.

The hardfork Flag Day will need to be coordinated/determined during the
development and testing stages for the hardfork – estimated at 9-12 months
to ensure a safe rollout of the hardfork to all network participants.


Reference implementation

TBD


Copyright

This document is placed in the public domain.
-------------------------------------
Good morning Paul,

Thank you for your consideration.


I don't see how this regress occurs.  Perhaps I need more information on extension blocks.


Any miner that rejects a bribe from outside the miner-group in order to put their desired hash on the sidechain, values their desired hash more than the bribe to put a different hash.  This rejection is a loss of potential proift, and other miners who accept the bribe gain the profit from it.


Your last paragraph does not make sense to me.  I suspect I have hit upon a nerve and will make no further comment on this sub-topic.


This is indeed the problem.  SHOM, as it unifies merge mining and WT^ voting, also allows theft attempts, and once the money available for withdrawal exceeds the sum of 288 bribes, we enter a dollar auction game between the thief and the sidechain users: https://en.wikipedia.org/wiki/Dollar_auction

As thieves are expected to follow the simple greedy algorithm, sidechain death can be triggered by a single theft attempt.

Assuming potential thieves understand the dollar-auction irrationality, they may be disincentivized, as presumably there are more sidechain protectors than thieves, and the sidechain protectors can (we hope) all outbid the thief.  But the problem is that this require rational behavior from thieves.  Mere greedy algorithm, or disruption for the sake of disruption, would still collapse SHOM sidechains.

But given the many parallels between SHOM and drivechains: what happens if 26% of miners disrupt all sidechains by always downvoting WT^?  In that case, sidechains still collapse as a whole, with practically the same effect as the SHOM thief.

We could limit the money available for withdrawal, but that weakens the side-to-main peg, reducing the value of the sidecoin relative to the maincoin.

The problem, to my mind, is that blind merge mining is pointless if it does not also allow voting on WT^.  In the end, no matter how novel a sidechain may be, what is valued is the maincoin backing the sidecoin; that is the whole point of the two-way peg.  A sidechain user may OP_BRIBEVERIFY valid sideblocks onto the mainchain, but if that user cannot vote on WT^ anyway, no matter how valid sideblocks committed on the mainchain, it would be pointless if the sidechain is attacked by mainchain miners.  You may as well remove blind merge mining, as miners who must vote on WT^ will need to understand the sidechain validity rules anyway.


I do not quite follow.  Can you expand more on this?


Thank you.


In order to attack multiple sidechains, bribing thieves must pay bribes for each sidechain being attacked.  Even if a miner attacks, bribes for valid sidechains must be rejected by the miner, effectively reducing the miner's profits, and the bribes to be rejected must be for all the sidechains to be attacked.

If withdrawals have a fixed or maximum value, then the bribe a thief must be prepared to pay (or turn down, in the case of thieving miners) must be no more than the maximum value / 288.

Unfortunately, capping withdrawals weakens the side-to-main peg, which weakens the reason for even using SHOM.  This is the true weakness of SHOM: it provides only a very weak side-to-main peg.


I agree.


Yes.


I am fine with some economic bond or proof-of-burn to start a sidechain.  But I am opposed to any permissioned method of starting sidechains.  To my mind, asking miners to install your software is already permissioned.


Thank you.

Regards,
ZmnSCPxj
-------------------------------------
On Mon, Dec 11, 2017 at 9:56 PM, Jim Posen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


A compromise would be to have 1 byte indicating the difference since the
last header.

Since the exponent doesn't use the full range you could steal bits from
there to indicate mode.

- no change
- mantissa offset (for small changes)
- full difficulty

This would support any nBits rule and you say 3 of the 4 bytes.



I suggest adding a message where you can ask for the lowest N hashes
between 2 heights on the main chain.

The reply is an array of {height, header} pairs for the N headers with the
lowest hash in the specified range.

All peers should agree on which headers are in the array.  If there is
disagreement, then you can at least narrow down on which segment there is
disagreement.

It works kind of like a cut and choose.  You pick one segment of the ones
he gave you recursively.

You can ask a peer for proof for a segment between 2 headers of the form.

- first header + coinbase with merkle branch
- all headers in the segment

This proves the segment has the correct height and that all the headers
link up.

There is a method called "high hash highway" that allows compact proofs of
total POW.
-------------------------------------

It is true only for nodes software. Most of the world's mining
infrastructure (at least for pool mining) is not ready for such
change. Current version of Stratum protocol doesn't support block
version changing. A broad adoption would require:

- A new standard extension to the mining protocol (generally, we want
the hash rate to be free to change the used pool without efficiency
loss)
- Pool operators must change their software.
- All miners must update their firmware IF they have compatible
hardware (we know there is compatible hardware out there but
definitely not all of the currently used). The firmware can be changed
after the mining protocol extension is settled.

Until all miners update (firmware or hardware), the change encourages
large difference in mining efficiency. And IMO it gives another
advantage to large mining operations in general.


You make a strong assumption that the new optimization is not
compatible with overt ASICBoost. If it is compatible, ASICBoost
doesn't help you with "defending against" the new optimization at all.
And it can be the case that the new optimization is based on ASICBoost
so you can make the situation "worse" by allowing it.


Can you explain why the reality should be significantly different? In
sufficiently near future.


We don't have to deal with any such theoretical situation now. You
proposal goes in opposite direction, by adding support for patented
algorithm. I don't know myself what the possible legal implications
are (maybe only for a subset of miners) so I consider it as an
unnecessary risk. At least before some conclusive legal analysis says
differently.

-------------------------------------
You're never going to reach 100% agreement, and stifling the network
literally forever to please a tiny minority is daft.

On Feb 8, 2017 8:52 AM, "alp alp via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

10% say literally never.  That seems like a significant disenfranchisement
and lack of consensus.

On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <bitcoin-dev@lists.
linuxfoundation.org> wrote:


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------------------------------
On Wed, May 31, 2017 at 1:50 AM, James MacWhyte <macwhyte@gmail.com> wrote:

Why is it https://github.com/bitcoin/bitcoin/blob/master/src/validation.cpp#L1661
not enough at this point?
Why the need for a transaction size limit?

-------------------------------------
What, in your appraisal, is the purpose of the block size limit? I think we
will be more able to have a productive discussion around this proposal if
we clear that up first.
-------------------------------------
On Tue, Jun 06, 2017 at 03:39:28PM -0700, Tao Effect via bitcoin-dev wrote:

CoinJoin works as a method of both improving fungibility and mixing with
coinbase transactions.

You probably don't need to do anything clever to split a coin though:
if you send a transaction with a standard fee it will get confirmed
in a normal time on the higher hashrate chain, but won't confirm as
quickly on the lower hashrate chain (precisely because transactions are
valid on both chains, but blocks are found more slowly with the lower
hashrate). When it's confirmed on one chain, but not on the other, you
can then "double-spend" on the lower hashrate chain with a higher fee,
to end up with different coins on both chains.

(also, no double-n in untenable)

Cheers,
aj


-------------------------------------
Thanks for this link.  From my reading though, it seems that only
soft-forks that attempt to freeze funds are problematic on ethereum.


So in the general case ethereum can still soft-fork I think...


On 09/15/2017 04:19 AM, Andrew Quentson wrote:


-- 
Dan Libby

Open Source Consulting S.A.
Santa Ana, Costa Rica
http://osc.co.cr
phone: 011 506 2204 7018
Fax: 011 506 2223 7359

-------------------------------------
On Wed, Jan 25, 2017 at 10:29 PM, Matt Corallo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Compatibility with existing transaction-signing software and hardware
should be considered.

I think any hard fork proposal should support a reasonable number of
reasonable-size old-sighash transactions, to allow a smooth transaction of
wallet software and hardware and to support anybody who might have a
hardware wallet locked away in a safe deposit box for years.

-- 
--
Gavin Andresen
-------------------------------------
Anyway, I'll count that as a NAK from Luke.  what do others here think?

I wish to guage if I were to submit a functional pull request for one or
both of these RPC calls, if would it be likely to be accepted.

If so I'm happy to contribute my time, otherwise...

On 09/29/2017 03:13 PM, Dan Libby wrote:


-- 
Dan Libby

Open Source Consulting S.A.
Santa Ana, Costa Rica
http://osc.co.cr
phone: 011 506 2204 7018
Fax: 011 506 2223 7359

-------------------------------------
Good morning Chris,


This is exactly the problem, and one which exists in a different form in any sidechain proposal.  In drivechain, malicious mainchain miners may arbitrarily downvote any side-to-main peg even if the side-to-main peg is valid on the sidechain, with mainchain fullnodes unable to gainsay them.  In original sidechain's SPV proofs, malicious mainchain miners may provide an invalid SPV proof and then censor any reorg proof against that SPV proof.  In both of those cases, trust in the sidechain and the value of sidecoin necessarily takes a hit.

Of course, in both of those two cases, the hit is "temporary" and the sidechain could theoretically recover.  In sidechain-headers-on-mainchain, the hit would permanently kill the sidechain.

The fact that sidechains are merge mined and cannot be mined off-mainchain makes sidechains entirely dependent on mainchain miner's support.  I agree with Sztorc that sidechains must be merge mined entirely, otherwise the sidechain will effectively reduce mainchain security by pulling away potential miners from mainchain.

OP_BRIBEVERIFY, which is intended to allow sidechain miners/protectors to be a separate datacenter from miners, allows anyone with either enough hashpower or enough maincoin to disrupt a sidechain by spamming its slot with random hash values.  With enough disruption, the sidechain may become unusable in drivechains, but may indeed be killed that way in sidechain-headers-on-mainchain.


Yes, this seems sensible.


Yes.

Even without sidechain headers on mainchain, one might consider plain blind merged mining to have put even the "previous block hash" in the sidechain block coinbase transaction.  Thus, one might consider that in blind merged mining, h' commitments are really merkle tree roots, and the previous block hash is encoded in a special sidechain transaction on one side of the merkle tree, while sidechain block transactions are encoded in the other side of the merkle tree root.  This allows OP_WITHDRAWPROOFVERIFY to be used on blind merged mining, but without sidechain headers on mainchain, a compact SPV proof somehow must still be provided, or we are forced to use drivechain miner voting.

Regards,
ZmnSCPxj
-------------------------------------
Not sure what "last week's meeting" is in reference to?

Agreed that the hard fork should be well-prepared, but I think its
dangerous to think that a hard fork as agreed upon would be a simple
relaxation of the block size. For example, Johnson Lau's previous
proposal, Spoonnet, which I think is probably one of the better ones,
would be incompatible with these rules.

I, of course, worry about what happens if we cannot come to consensus on
a number to soft fork down to, potentially significantly risking miner
profits (and, thus, the security of Bitcoin) if a group is able to keep
things "at the status quo". That said, for that to be alleviated we
could simply do something based on historical transaction growth (which
is somewhat linear, with a few inflection points), but that number ends
up being super low (eg somewhere around 2MB at the next halving, which
SegWit itself already provides :/.

We could, of course, focus on designing a hard fork's activation and
technical details, with a very large block size increase in it (ie
closer to 4/6MB at the next halving or so, something we at least could
be confident we could develop software for), with intention to soft fork
it back down if miner profits are suffering.

Matt

On 03/28/17 16:59, Wang Chun via bitcoin-dev wrote:

-------------------------------------
Probabilistic collisions, while present, would be statistically insignificant at 4 chars length.


Implementation by wallets would just require a loop of their existing address generation until a match is found, trivial to implement. Wallets could provide a dropdown which shows the most commonly used signals as seen on the block chain, or a write-in.

Signalling within OP_RETURN increases the tx size and cost. This address hashing method keeps the very small economic cost of voting off the chain, rather than passing it cumulatively to everyone with the insertion of additional data.


Since I wrote this I have come across a similar idea called CryptoVoter which I think deserves more attention than it has had.

________________________________
From: Natanael <natanael.l@gmail.com>
Sent: Sunday, February 5, 2017 4:22 PM
To: Bitcoin Dev; John Hardy
Subject: Re: [bitcoin-dev] Transaction signalling through output address hashing

Censorship by miners isn't the only problem. Existing and normal transactions will probabilistically collide with these schemes, and most wallets have no straightforward way of supporting it.
-------------------------------------
Hi everyone,

Segwit2Mb is the project to merge into Bitcoin a minimal patch that aims to
untangle the current conflict between different political positions
regarding segwit activation vs. an increase of the on-chain blockchain
space through a standard block size increase. It is not a new solution, but
it should be seen more as a least common denominator.

Segwit2Mb combines segwit as it is today in Bitcoin 0.14+ with a 2MB block
size hard-fork activated ONLY if segwit activates (95% of miners
signaling), but at a fixed future date.

The sole objective of this proposal is to re-unite the Bitcoin community
and avoid a cryptocurrency split. Segwit2Mb does not aim to be best
possible technical solution to solve Bitcoin technical limitations.
However, this proposal does not imply a compromise to the future
scalability or decentralization of Bitcoin, as a small increase in block
size has been proven by several core and non-core developers not to affect
Bitcoin value propositions.

In the worst case, a 2X block size increase has much lower economic impact
than the last bitcoin halving (<10%), which succeeded without problem.

On the other side, Segwit2Mb primary goal is to be minimalistic: in this
patch some choices have been made to reduce the number of lines modified in
the current Bitcoin Core state (master branch), instead of implementing the
most elegant solution. This is because I want to reduce the time it takes
for core programmers and reviewers to check the correctness of the code,
and to report and correct bugs.

The patch was built by forking the master branch of Bitcoin Core, mixing a
few lines of code from Jeff Garzik's BIP102,  and defining a second
versionbits activation bit (bit 2) for the combined activation.

The combined activation of segwit and 2Mb hard-fork nVersion bit is 2
(DEPLOYMENT_SEGWIT_AND_2MB_BLOCKS).

This means that segwit can still be activated without the 2MB hard-fork by
signaling bit 1 in nVersion  (DEPLOYMENT_SEGWIT).

The tentative lock-in and hard-fork dates are the following:

Bit 2 signaling StartTime = 1493424000; // April 29th, 2017

Bit 2 signaling Timeout = 1503964800; // August 29th, 2017

HardForkTime = 1513209600; // Thu, 14 Dec 2017 00:00:00 GMT


The hard-fork is conditional to 95% of the hashing power has approved the
segwit2mb soft-fork and the segwit soft-fork has been activated (which
should occur 2016 blocks after its lock-in time)

For more information on how soft-forks are signaled and activated, see
https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki

This means that segwit would be activated before 2Mb: this is inevitable,
as versionbits have been designed to have fixed activation periods and
thresholds for all bits. Making segwit and 2Mb fork activate together at a
delayed date would have required a major re-write of this code, which would
contradict the premise of creating a minimalistic patch. However, once
segwit is activated, the hard-fork is unavoidable.

Although I have coded a first version of the segwit2mb patch (which
modifies 120 lines of code, and adds 220 lines of testing code), I would
prefer to wait to publish the source code until more comments have been
received from the community.

To prevent worsening block verification time because of the O(N^2) hashing
problem, the simple restriction that transactions cannot be larger than 1Mb
has been kept. Therefore the worse-case of block verification time has only
doubled.

Regarding the hard-fork activation date, I want to give enough time to all
active economic nodes to upgrade. As of Fri Mar 31 2017,
https://bitnodes.21.co/nodes/ reports that 6332 out of 6955 nodes (91%)
have upgraded to post 0.12 versions. Upgrade to post 0.12 versions can be
used to identify economic active nodes, because in the 0.12 release dynamic
fees were introduced, and currently no Bitcoin automatic payment system can
operate without automatic discovery of the current fee rate. A pre-0.12
would require constant manual intervention.
Therefore I conclude that no more than 91% of the network nodes reported by
bitnodes are active economic nodes.

As Bitcoin Core 0.12 was released on February 2016, the time for this 91%
to upgrade has been around one year (under a moderate pressure of
operational problems with unconfirmed transactions).
Therefore we can expect a similar or lower time to upgrade for a hard-fork,
after developers have discussed and approved the patch, and it has been
reviewed and merged and 95% of the hashing power has signaled for it (the
pressure not to upgrade being a complete halt of the operations). However I
suggest that we discuss the hard-fork date and delay it if there is a real
need to.

Currently time works against the Bitcoin community, and so is delaying a
compromise solution. Most of the community agree that halting the
innovation for several years is a very bad option.

After the comments collected by the community, a BIP will be written
describing the resulting proposal details.

If segwit2mb locks-in, before hard-fork occurs all bitcoin nodes should be
updated to a Segwit2Mb enabled node to prevent them to be forked-away in a
chain with almost no hashing-power.

The proof of concept patch was made for Bitcoin Core but should be easily
ported to other Bitcoin protocol implementations that already support
versionbits. Lightweight (SPV) wallets should not be affected as they
generally do not check the block size.

I personally want to see the Lightning Network in action this year, use the
non-malleability features in segwit, see the community discussing other
exciting soft-forks in the scaling roadmap, Schnorr sigs, drivechains and
MAST.

I want to see miners, developers and industry side-by-side pushing Bitcoin
forward, to increase the value of Bitcoin and prevent high transaction fees
to put out of business use-cases that could have high positive social
impact.

I believe in the strength of a unified Bitcoin community. If you're a
developer, please give your opinion, suggest changes, audit it, and take a
stand with me to unlock the current Bitcoin deadlock.

Contributions to the segwit2mb project are welcomed and awaited. The only
limitation is to stick to the principle that the patch should be as simple
to audit as possible. As an example, I wouldn't feel confident if the patch
modified more than ~150 lines of code.

Improvements unrelated to a 2 Mb increase or segwit, as beneficial as it
may be to Bitcoin, should not be part of segwit2Mb.

This proposal should not prevent other consensus proposals to be
simultaneously merged: segwit2mb is a last resort solution in case we can
not reach consensus on anything better.

Again, the proposal is only a starting point: community feedback is
expected and welcomed.

Regards,
Sergio Demian Lerner
-------------------------------------
On Tue, 2017-04-18 at 12:34 +0200, Natanael via bitcoin-dev wrote:

I'm not an expert on lower bounds of algorithms but I think proving
such properties is basically out of reach for mankind currently.


Yes, a reasonable thing in practice seems to use a slower hash function
(or just iterating the hash function many times), see also this thread:
 https://twitter.com/Ethan_Heilman/status/850015029189644288 .

PoW verification will still be fast enough. That's not the bottleneck
of block verification anyway.

Also, I don't agree that a PoW function should not rely on memory.
Memory-hard functions are the best we have currently.


Tim

-------------------------------------
On Saturday 13 May 2017 12:48:48 PM Peter Todd wrote:

I'm assuming that if the economic majority hasn't consented to the softfork, 
at least as many users will make their transactions conditional on non-
signalling.


-------------------------------------
Is there any reason to believe that you need Bitcoin "full security" at all
for timestamping?

On Mon, Aug 28, 2017 at 11:50 AM, Riccardo Casatta via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Also check the Open Alias project. It's based on DNS+DNSSEC but it
offers the usability feature you mention (nice addresses).

https://openalias.org/

On 01/12/2017 00:20, mandar mulherkar via bitcoin-dev wrote:


-------------------------------------
My apologies if this post has been answered, but I am new to the list. I am lawyer trying to understand the licensing of the Bitcoin core and I  will be presenting in a webinar with Black Duck Software on Blockchain on September 28  (in case you are not familiar with them, Black Duck Software assists companies in managing their open source software resources). They have scanned the Bitcoin Core code for the open source licenses used in the codebase.  I am enclosing a summary of the findings. I would be interested in communicating with the individuals who manage this codebase and can provide insight about the project manages contributions because the codebase includes projects with inconsistent licenses (for example, code licensed under the Apache Software License version  2 and GPLv2 cannot work together in some situations). Thanks in advance.

According to the scan, the code base includes code licensed under the following licenses:


Apache License 2.0

Boost Software License 1.0

BSD 2-clause "Simplified" License

BSD 3-clause "New" or "Revised" License

Creative Commons Attribution Share Alike 3.0

Expat License

GNU General Public License v2.0 or later

GNU General Public License v3.0 or later

GNU Lesser General Public License v2.1 or later

License for A fast alternative to the modulo reduction

License for atomic by Timm Kosse

MIT License

Public Domain

University of Illinois/NCSA Open Source License






Mark Radcliffe
Partner

T +1 650.833.2266
F +1 650.687.1222
M +1 650.521.5039
E mark.radcliffe@dlapiper.com <mailto:mark.radcliffe@dlapiper.com>

[DLA Piper Logo]

DLA Piper LLP (US)
2000 University Avenue
East Palo Alto, California 94303-2215
United States
www.dlapiper.com <http://www.dlapiper.com>

Please consider the environment before printing this email.

The information contained in this email may be confidential and/or legally privileged. It has been sent for the sole use of the intended recipient(s). If the reader of this message is not an intended recipient, you are hereby notified that any unauthorized review, use, disclosure, dissemination, distribution, or copying of this communication, or any of its contents, is strictly prohibited. If you have received this communication in error, please reply to the sender and destroy all copies of the message. To contact us directly, send to postmaster@dlapiper.com. Thank you.
-------------------------------------

Summary
=========

In my opinion, Greg Maxwell's scaling roadmap [1] succeeded in a few
crucial ways. One success was that it synchronized the entire Bitcoin
community, helping to bring finality to the (endless) conversations of
that time, and get everyone back to work. However, I feel that the Dec
7, 2015 roadmap is simply too old to serve this function any longer. We
should revise it: remove what has been accomplished, introduce new
innovations and approaches, and update deadlines and projections.


Why We Should Update the Roadmap
=================================

In a P2P system like Bitcoin, we lack authoritative info-sources (for
example, a "textbook" or academic journal), and as a result
conversations tend to have a problematic lack of progress. They do not
"accumulate", as everyone must start over. Ironically, the scaling
conversation _itself_ has a fatal O(n^2) scaling problem.

The roadmap helped solve these problems by being constant in size, and
subjecting itself to publication, endorsement, criticism, and so forth.
Despite the (unavoidable) nuance and complexity of each individual
opinion, it was at least globally known that X participants endorsed Y
set of claims.

Unfortunately, the Dec 2015 roadmap is now 19 months old -- it is quite
obsolete and replacing it is long overdue. For example, it highlights
older items (CSV, compact blocks, versionbits) as being _future_
improvements, and makes no mention of new high-likelihood improvements
(Schnorr) or mis-emphasizes them (LN). It even contains mistakes (SegWit
fraud proofs). To read the old roadmap properly, one must already be a
technical expert. For me, this defeats the entire point of having one in
the first place.

A new roadmap would be worth your attention, even if you didn't sign it,
because a refusal to sign would still be informative (and, therefore,
helpful)!

So, with that in mind, let me present a first draft. Obviously, I am
strongly open to edits and feedback, because I have no way of knowing
everyone's opinions. I admit that I am partially campaigning for my
Drivechain project, and also for this "scalability"/"capacity"
distinction...that's because I believe in both and think they are
helpful. But please feel free to suggest edits.

I emphasized concrete numbers, and concrete dates.

And I did NOT necessarily write it from my own point of view, I tried
earnestly to capture a (useful) community view. So, let me know how I did.

 ==== Beginning of New ("July 2017") Roadmap Draft ====

This document updates the previous roadmap [1] of Dec 2015. The older
statement endorsed a belief that "the community is ready to deliver on
its shared vision that addresses the needs of the system while upholding
its values".

That belief has not changed, but the shared vision has certainly grown
sharper over the last 18 months. Below is a list of technologies which
either increase Bitcoin's maximum tps rate ("capacity"), or which make
it easier to process a higher volume of transactions ("scalability").

First, over the past 18 months, the technical community has completed a
number of items [2] on the Dec 2015 roadmap. VersonBits (BIP 9) enables
Bitcoin to handle multiple soft fork upgrades at once. Compact Blocks
(BIP 152) allows for much faster block propagation, as does the FIBRE
Network [3]. Check Sequence Verify (BIP 112) allows trading partners to
mutually update an active transaction without writing it to the
blockchain (this helps to enable the Lightning Network).

Second, Segregated Witness (BIP 141), which reorganizes data in blocks
to handle signatures separately, has been completed and awaits
activation (multiple BIPS). It is estimated to increase capacity by a
factor of 2.2. It also improves scalability in many ways. First, SW
includes a fee-policy which encourages users to minimize their impact on
the UTXO set. Second, SW achieves linear scaling of sighash operations,
which prevents the network from crashing when large transactions are
broadcast. Third, SW provides an efficiency gain for everyone who is not
verifying signatures, as these no longer need to be downloaded or
stored. SegWit is an enabling technology for the Lightning Network,
script versioning (specifically Schnorr signatures), and has a number of
benefits which
are unrelated to capacity [4].

Third, the Lightning Network, which allows users to transact without
broadcasting to the network, is complete [5, 6] and awaits the
activation of SegWit. For those users who are able to make a single
on-chain transaction, it is estimated to increase both capacity and
scalability by a factor of ~1000 (although these capacity increases will
vary with usage patterns). LN also greatly improves transaction speed
and transaction privacy.

Fourth, Transaction Compression [7], observes that Bitcoin transaction
serialization is not optimized for storage or network communication. If
transactions were optimally compressed (as is possible today), this
would improve scalability, but not capacity, by roughly 20%, and in some
cases over 30%.

Fifth, Schnorr Signature Aggregation, which shrinks transactions by
allowing many transactions to have a single shared signature, has been
implemented [8] in draft form in libsecp256k1, and will likely be ready
by Q4 of 2016. One analysis [9] suggests that signature aggregation
would result in storage and bandwidth savings of at least 25%, which
would therefore increase scalability and capacity by a factor of 1.33.
The relative savings are even greater for multisignature transactions.

Sixth, drivechain [10], which allows bitcoins to be temporarily
offloaded to 'alternative' blockchain networks ("sidechains"), is
currently under peer review and may be usable by end of 2017. Although
it has no impact on scalability, it does allow users to opt-in to
greater capacity, by moving their BTC to a new network (although, they
will achieve less decentralization as a result). Individual drivechains
may have different security tradeoffs (for example, a greater reliance
on UTXO commitments, or MimbleWimble's shrinking block history) which
may give them individually greater scalability than mainchain Bitcoin.

Finally, the capacity improvements outlined above may not be sufficient.
If so, it may be necessary to use a hard fork to increase the blocksize
(and blockweight, sigops, etc) by a moderate amount. Such an increase
should take advantage of the existing research on hard forks, which is
substantial [11]. Specifically, there is some consensus that Spoonnet
[12] is the most attractive option for such a hardfork. There is
currently no consensus on a hard fork date, but there is a rough
consensus that one would require at least 6 months to coordinate
effectively, which would place it in the year 2018 at earliest.

The above are only a small sample of current scaling technologies. And
even an exhaustive list of scaling technologies, would itself only be a
small sample of total Bitcoin innovation (which is proceeding at
breakneck speed).

Signed,
<Names Here>

[1]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011865.html
[2] https://bitcoincore.org/en/2017/03/13/performance-optimizations-1/
[3] http://bluematt.bitcoin.ninja/2016/07/07/relay-networks/
[4] https://bitcoincore.org/en/2016/01/26/segwit-benefits/
[5]
http://lightning.community/release/software/lnd/lightning/2017/05/03/litening/
[6] https://github.com/ACINQ/eclair
[7] https://people.xiph.org/~greg/compacted_txn.txt
[8]
https://github.com/ElementsProject/secp256k1-zkp/blob/d78f12b04ec3d9f5744cd4c51f20951106b9c41a/src/secp256k1.c#L592-L594
[9] https://bitcoincore.org/en/2017/03/23/schnorr-signature-aggregation/
[10] http://www.drivechain.info/
[11] https://bitcoinhardforkresearch.github.io/
[12]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-February/013542.html

 ==== End of Roadmap Draft ====

In short, please let me know:

1. If you agree that it would be helpful if the roadmap were updated.
2. To what extent, if any, you like this draft.
3. Edits you would make (specifically, I wonder about Drivechain
thoughts and Hard Fork thoughts, particularly how to phrase the Hard
Fork date).

Google Doc (if you're into that kind of thing):
https://docs.google.com/document/d/1gxcUnmYl7yM0oKR9NY9zCPbBbPNocmCq-jjBOQSVH-A/edit?usp=sharing

Cheers,
Paul

-------------------------------------
This feels redundant to me; the payment protocol already has an
expiration time.


On 09/27/2017 06:06 PM, Peter Todd via bitcoin-dev wrote:



-------------------------------------

OK, but then it seems to me you have a dilemma for, eg, your LN commitment
tx.  You either give it the specific nForkId of the fork it's created on -
making it invalid on *all* other forks (eg, any future "non-contentious
upgrade" HF that replaces that fork).  Or you give it nForkId 0 - which has
the "BCH tx valid on Segwit2x (& vice versa)" flaw.

It may make sense to revise your proposal to incorporate Luke's
OP_CHECKBLOCKATHEIGHT
<https://github.com/bitcoin/bips/blob/master/bip-0115.mediawiki>, and make
the fork ID a (block height, hash) pair rather than just a number.  But I
still think the idea of fork-specific addresses is a keeper!


On Tue, Nov 14, 2017 at 8:49 AM, Mats Jerratsch <mats@blockchain.com> wrote:

-------------------------------------
Nodes are by design not supposed to be identifiable in any way, including
persisting identities across IPs changes or when connecting over different
networks (e.g. clearnet/tor). Anything that makes Bitcoin less private is a
step backwards. Also absolute node count is pretty meaningless since only
fully validating nodes that participate in economic activity really matter.

As a side note, this should probably have started out as a bitcoin-discuss
post.

On Sat, Mar 4, 2017 at 4:04 PM, John Hardy via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Good morning Rusty,

The fact that amount is optional, and the separator character between human-readable and data is the character "1", may mean some trouble with parsing of the optional amount.

Currently, the version is 0, which translates to the character "q" appearing after "1". So 1q is obviously not an amount and is known to start the data part.

However, what happens when we decide to upgrade, and version is now 1, translating to character "p"?

If version 1 of invoice is avalialble, does a payment starting with lnbc1p .... indicate a 1 pico-bitcoin payment, or an arbitrary payment to a version-1 data part?

Or is amount not allowed to have character "1"? It seems a strange limitation if we impose this...

Or do I mistake my understanding of bech32?

Alternatively we can just fix the first 5 bits to be 0 (so "1q" is an unambiguous separator between human-readable and data parts) and provide the version by other means, such as changing lnbc to ln2bc or so on...


At first read, this seems wrong. My understanding is that lightning invoices are longer than segwit addresses in bech32, so human error is more likely.

Of course, it seems that the intended meaning is really "lightning invoices are so long that it is unlikely humans will enter it by hand, so human errors are unlikely compared to QR reader errors etc." so perhaps better reworded as such.

Regards,
ZmnSCPxj
-------------------------------------
Since BIP 141's version bit assignment will timeout soon, and needing renewal, 
I was thinking it might make sense to make some minor tweaks to the spec for 
the next deployment. These aren't critical, so it's perfectly fine if BIP 141 
activates as-is (potentially with BIP 148), but IMO would be an improvement if 
a new deployment (non-BIP148 UASF and/or new versionbit) is needed.

1. Change the dummy marker to 0xFF instead of 0. Using 0 creates ambiguity 
with incomplete zero-input transactions, which has been a source of confusion 
for raw transaction APIs. 0xFF would normally indicate a >32-bit input count, 
which is impossible right now (it'd require a >=158 GB transaction) and 
unlikely to ever be useful.

2. Relax the consensus rules on when witness data is allowed for an input. 
Currently, it is only allowed when the scriptSig is null, and the scriptPubKey 
being spent matches a very specific pattern. It is ignored by "upgrade-safe" 
policy when the scriptPubKey doesn't match an even-more-specific pattern. 
Instead, I suggest we allow it (in the consensus layer only) in combination 
with scriptSig and with any scriptPubKey, and consider these cases to be 
"upgrade-safe" policy ignoring.

The purpose of the second change is to be more flexible to any future 
softforks. I consider it minor because we don't know of any possibilities 
where it would actually be useful.

Thoughts?

Luke

-------------------------------------
OK, fair enough, just wanted to make sure we were on the same page.
"Thorny issues there and there hasn't been a ton of effort put into what
Bitcoin integration and maintainability looks like" is a perfectly fair
response :)

Matt

On 10/30/17 18:32, Mark Friedenbach wrote:

-------------------------------------
Hi Greg,


On 7/11/2017 5:11 PM, Gregory Maxwell wrote:
Is this the only reason you do not support the document? If so I would
be happy to take out the section, if enough people share such a view.

As to your specific complaints, I have addressed both the security model
and the concept of mining centralization on this list in the recent
past. I would like to hear your responses to my claims, if you are
willing to share them. As for positioning DC as a major solution, it is
a little confusing because Luke-Jr and Adam back seem to feel it is at
least worth discussing on those terms (and I know of no reason why it
would not be discussed on those terms). The peer review here on
[bitcoin-dev] seemed to be moving forward without any serious
objections. And it seems unsportsmanlike for you to object, for reasons
which you keep only to yourself.


I'm aiming for minimal utility.

I don't understand this at all. This document attempts to do exactly
what its predecessor did -- nothing more or less.

No one is suggesting that features be guaranteed, either ever or in
releases.


I really don't think they are related. For a start, software is almost
always delayed. An obvious second is that this entire scaling
conversation is polarized to the hilt and everyone that can be blamed
for something has been blamed for something.

No one likes to be held to a certain deadline, but this roadmap is just
about producing some clarity for people who do not do this 24/7.

I asked Adam Back for it.

Again, I think you're missing the point. If there is a problem with SA,
you can just suggest it be removed from the document.


This is a very strange argument. I would consider it a benefit if people
learned from the document, and discovered things that they had not heard
of before.

There is no "insisting" of any kind.


He probably showed you an earlier draft. But I wrote almost all of this
myself, and I can only recall 2 or 3 phrases (not even complete
sentences) included from Adam Back. And most of the phrases are
themselves just boring descriptions that I'm sure anyone could write.
Some phrases may have simply been taken from bitcoincore.org or
somewhere similar.

I am not exactly sure what you are insinuating but I encourage you to
clarify it.

I really don't understand what you are disclosing. That Adam asked you
for feedback on the draft? And then, in the next sentence, that not
enough experts were asked for feedback on the draft? I'm legitimately
confused by this part.

As I stated, we can remove the drivechain section. But surely you can
appreciate how bizarre your position on roadmaps is. What exactly, did
you intended to create at [1]? Since it is described explicitly as "the
roadmap in Capacity increases for the Bitcoin system", have you been
disagreeing with it's characterization as a 'roadmap' this entire time?
One wonders why you haven't said anything until now.

[1] https://bitcoincore.org/en/2015/12/21/capacity-increase/

In my first email I list the benefits of having a roadmap. One benefit
is that, without one, it is likely that a large majority of outsiders
have almost no idea at all what is being worked on, what effect it will
have, or when it might be ready, or to whom/what they should turn to for
advice on such matters. Do you have a different way of addressing this
communication problem?

Paul


-------------------------------------
On Thu, Apr 20, 2017 at 11:46:33AM +0200, Tom Zander via bitcoin-dev wrote:

I think the expected number of peers is actually ~47.75, which is pretty
close to David's estimate, which was wrong in a way that was actually
more favorable to the "everyone stores random blocks" scheme than the
truth.

Even assuming no archival nodes, and all nodes storing only one random
index between 5 and 255 inclusive, the chance of five arbitrary nodes
giving unique indices by chance is about 98.4%. To get the same probability
from a scheme where each peer has only 25% of the blocks, you need to
connect to 59.59 nodes.

This is over a ten-times increase in the number of nodes required to
download the entire chain, and requires participating nodes to use 25%
more space than David's proposal.


Storing random but complete blocks requires the assumption this is _not_ the
case; David's does not make any assumptions. So on top of the performance
considerations there is this potential DoS vector.
 

-- 
Andrew Poelstra
Mathematics Department, Blockstream
Email: apoelstra at wpsoftware.net
Web:   https://www.wpsoftware.net/andrew

"A goose alone, I suppose, can know the loneliness of geese
 who can never find their peace,
 whether north or south or west or east"
       --Joanna Newsom

-------------------------------------
I should not take it that the lack of critical feedback to this revised proposal is a glowing endorsement. I understand that there would be technical issues to resolve in implementation, but, are there no fundamental errors?

I suppose that it if is difficult to determine how long a transaction has been waiting in the pool then, each node could simply keep track of when a transaction was first seen. This may have implications for a verify routine, however, for example, if a node was offline, how should it differentiate how long each transaction was waiting in that case? If a node was restarted daily would it always think that all transactions had been waiting in the pool less than one day If each node keeps the current transaction pool in a file and updates it, as transactions are included in blocks and, as new transactions appear in the pool, then that would go some way to alleviate the issue, apart from entirely new nodes. There should be no reason the contents of a transaction pool files cannot be shared without agreement as to the transaction pool between nodes, just as nodes transmit new transactions freely.

It has been questioned why miners could not cheat. For the question of how many transactions to include in a block, I say it is a standoff and miners will conform to the proposal, not wanting to leave transactions with valid fees standing, and, not wanting to shrink the transaction pool. In any case, if miners shrink the transaction pool then I am not immediately concerned since it provides a more efficient service. For the question of including transactions according to the proposal, I say if it is possible to keep track of how long transactions are waiting in the pool so that they can be included on a probability curve then it is possible to verify that blocks conform to the proposal, since the input is a probability, the output should conform to a probability curve.


If someone has the necessary skill, would anyone be willing to develop the math necessary for the proposal?

Regards,
Damian Williamson

________________________________
From: bitcoin-dev-bounces@lists.linuxfoundation.org <bitcoin-dev-bounces@lists.linuxfoundation.org> on behalf of Damian Williamson via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Friday, 8 December 2017 8:01 AM
To: bitcoin-dev@lists.linuxfoundation.org
Subject: [bitcoin-dev] BIP Proposal: Revised: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks


Good afternoon,

The need for this proposal:

We all must learn to admit that transaction bandwidth is still lurking as a serious issue for the operation, reliability, safety, consumer acceptance, uptake and, for the value of Bitcoin.

I recently sent a payment which was not urgent so; I chose three-day target confirmation from the fee recommendation. That transaction has still not confirmed after now more than six days - even waiting twice as long seems quite reasonable to me. That transaction is a valid transaction; it is not rubbish, junk or, spam. Under the current model with transaction bandwidth limitation, the longer a transaction waits, the less likely it is ever to confirm due to rising transaction numbers and being pushed back by transactions with rising fees.

I argue that no transactions are rubbish or junk, only some zero fee transactions might be spam. Having an ever-increasing number of valid transactions that do not confirm as more new transactions with higher fees are created is the opposite of operating a robust, reliable transaction system.

Business cannot operate with a model where transactions may or may not confirm. Even a business choosing a modest fee has no guarantee that their valid transaction will not be shuffled down by new transactions to the realm of never confirming after it is created. Consumers also will not accept this model as Bitcoin expands. If Bitcoin cannot be a reliable payment system for confirmed transactions then consumers, by and large, will simply not accept the model once they understand. Bitcoin will be a dirty payment system, and this will kill the value of Bitcoin.

Under the current system, a minority of transactions will eventually be the lucky few who have fees high enough to escape being pushed down the list.

Once there are more than x transactions (transaction bandwidth limit) every ten minutes, only those choosing twenty-minute confirmation (2 blocks) will have initially at most a fifty percent chance of ever having their payment confirm. Presently, not even using fee recommendations can ensure a sufficiently high fee is paid to ensure transaction confirmation.

I also argue that the current auction model for limited transaction bandwidth is wrong, is not suitable for a reliable transaction system and, is wrong for Bitcoin. All transactions must confirm in due time. Currently, Bitcoin is not a safe way to send payments.

I do not believe that consumers and business are against paying fees, even high fees. What is required is operational reliability.

This great issue needs to be resolved for the safety and reliability of Bitcoin. The time to resolve issues in commerce is before they become great big issues. The time to resolve this issue is now. We must have the foresight to identify and resolve problems before they trip us over.  Simply doubling block sizes every so often is reactionary and is not a reliable permanent solution. I have written a BIP proposal for a technical solution but, need your help to write it up to an acceptable standard to be a full BIP.

I have formatted the following with markdown which is human readable so, I hope nobody minds. I have done as much with this proposal as I feel that I am able so far but continue to take your feedback.

# BIP Proposal: UTPFOTIB - Use Transaction Priority For Ordering Transactions In Blocks

## The problem:
Everybody wants value. Miners want to maximize revenue from fees (and we presume, to minimize block size). Consumers need transaction reliability and, (we presume) want low fees.

The current transaction bandwidth limit is a limiting factor for both. As the operational safety of transactions is limited, so is consumer confidence as they realize the issue and, accordingly, uptake is limited. Fees are artificially inflated due to bandwidth limitations while failing to provide a full confirmation service for all transactions.

Current fee recommendations provide no satisfaction for transaction reliability and, as Bitcoin scales, this will worsen.

Bitcoin must be a fully scalable and reliable service, providing full transaction confirmation for every valid transaction.

The possibility to send a transaction with a fee lower than one that is acceptable to allow eventual transaction confirmation should be removed from the protocol and also from the user interface.

## Solution summary:
Provide each transaction with an individual transaction priority each time before choosing transactions to include in the current block, the priority being a function of the fee paid (on a curve), and the time waiting in the transaction pool (also on a curve) out to n days (n=60 ?). The transaction priority to serve as the likelihood of a transaction being included in the current block, and for determining the order in which transactions are tried to see if they will be included.

Use a target block size. Determine the target block size using; current transaction pool size x ( 1 / (144 x n days ) ) = number of transactions to be included in the current block. Broadcast the next target block size with the current block when it is solved so that nodes know the next target block size for the block that they are building on.

The curves used for the priority of transactions would have to be appropriate. Perhaps a mathematician with experience in probability can develop the right formulae. My thinking is a steep curve. I suppose that the probability of all transactions should probably account for a sufficient number of inclusions that the target block size is met although, it may not always be. As a suggestion, consider including some zero fee transactions to pad, highest BTC value first?

**Explanation of the operation of priority:**


I am not concerned with low (or high) transaction fees, the primary reason for addressing the issue is to ensure transactional reliability and scalability while having each transaction confirm in due time.

## Pros:
* Maximizes transaction reliability.
* Fully scalable.
* Maximizes possibility for consumer and business uptake.
* Maximizes total fees paid per block without reducing reliability; because of reliability, in time confidence and overall uptake are greater; therefore, more transactions.
* Market determines fee paid for transaction priority.
* Fee recommendations work all the way out to 30 days or greater.
* Provides additional block entropy; greater security since there is less probability of predicting the next block.

## Cons:
* Could initially lower total transaction fees per block.
* Must be first be programmed.

## Solution operation:
This is a simplistic view of the operation. The actual operation will need to be determined in a spec for the programmer.

1. Determine the target block size for the current block.
2. Assign a transaction priority to each transaction in the pool.
3. Select transactions to include in the current block using probability in transaction priority order until the target block size is met.
5. Solve block.
6. Broadcast the next target block size with the current block when it is solved.
7. Block is received.
8. Block verification process.
9. Accept/reject block based on verification result.
10. Repeat.

## Closing comments:
It may be possible to verify blocks conform to the proposal by showing that the probability for all transactions included in the block statistically conforms to a probability distribution curve, *if* the individual transaction priority can be recreated. I am not that deep into the mathematics; however, it may also be possible to use a similar method to do this just based on the fee, that statistically, the blocks conform to a fee distribution. Any zero fee transactions would have to be ignored. This solution needs a clever mathematician.

I implore, at the very least, that we use some method that validates full transaction reliability and enables scalability of block sizes. If not this proposal, an alternative.

Regards,
Damian Williamson
-------------------------------------
