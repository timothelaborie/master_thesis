---------- Forwarded message ----------
From: Marcel Jamin <marcel@jamin.net>
Date: 2015-10-01 11:39 GMT+02:00
Subject: Re: [bitcoin-dev] Bitcoin Core 0.12.0 release schedule
To: Btc Drak <btcdrak@gmail.com>


I guess the question then becomes why bitcoin still is <1.0.0

I'd say it's safe to say that it's used in production.

2015-10-01 11:17 GMT+02:00 Btc Drak <btcdrak@gmail.com>:

-------------------------------------
I was mansplaining weak blocks to my wife. She asked a simple question:

Why would I, as a miner, publish a weak block if I find one?

I don't know.

Sure, I will get faster propagation for my solved block, should I find one.
On the other hand everybody else mining a similar block will enjoy the same
benefit. Assuming that I'm not a huge miner, it's unlikely that I will
actually solve the block, so I'm probably just giving away fast propagation
times to someone else.

So how does publishing a weak block benefit the producer of it more than
the other miners? Please help me understand this.

/Kalle Rosenbaum


2015-09-27 11:42 GMT+02:00 Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
I think this hardfork is dead-on-arrival given the ideas for OP_CHECKSIG 
softforking. Instead of referring to previous transactions by a normalised 
hash, it makes better sense to simply change the outpoints in the signed data 
and allow nodes to hotfix dependent transactions when/if they are malleated. 
Furthermore, the approach of using a hash of scriptPubKey in the input rather 
than an outpoint also solves dependencies in the face of intentional 
malleability (respending with a higher fee, or CoinJoin, for a few examples).

These aren't barriers to making the proposal or being assigned a BIP number if 
you want to go forward with that, but you may wish to reconsider spending time 
on it.

Luke


On Wednesday, May 13, 2015 12:48:04 PM Christian Decker wrote:


-------------------------------------
awaiting transmission transmission

On Mon, Jun 22, 2015 at 5:23 PM, Nathan Wilcox <nathan@leastauthority.com>
wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



On 08/04/2015 08:12 PM, Gavin Andresen via bitcoin-dev wrote:

[snip]


It's a big problem. What are you dismissing it for?

With Bitcoin in fledgling 0.x version state this is neither desirable
nor encouraging.

Development did not freeze at some time in the past and now we see how
the userbase reacts. Miners, btw, are arguibly still a class of user
as long as they are guaranteed coinbase.

When they start making and proving themselves useful in a free
floating fee market absent coinbase subsidy, we can revisit this
topic, with the benefit of hindsight.


[snip]

Pieter never said it wasn't interesting, so this emphatic statement is
strange - like someone is trying to convince an audience - but
anyway... as you, a veritable spring-chicken by your actions and
words, said the other day: having graduated in '88 you're "old" and
speak from experience. Don't come with that jive, bossy man - bring
facts and testing to the technical list.

My finance readers, in one camp, and Bitcoin investors, in the other,
want to see the XT 8MB hard-fork testing data that you mentioned for
BIP100.

"Being ignorant is not so much a shame, as being unwilling to learn."
- - Benjamin Franklin


Venzen Khaosan
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVwMyOAAoJEGwAhlQc8H1mho4IAKEHVxE4lAs3aIoLXa2fxLP8
3q7MhfM5vIW9QAM7rjz8YzheMg3Wj2CNfZPuUV7YDTVrLZPrIN/aMY6CIftr7GUS
pjMI9nnwezFwYX5oyRU+gW51AMFhvexV6ITZYpiLRtWHgK1FZtXWMG13eO/6Jb5U
Wjflub7suMDvg+ST2PplhQf7fFmnPHrLZg3ISDqK+hvgw20geW1rXC/wCChlewfd
DqSt9fxqs+NIvbIzS2TgLTkIcHlbKNeI5AeqbaFoaIQtvYALD3Ojt2I/qoCJU1za
rB8Il7UK0B5uf6xxgErGcYAHzjVpR6Zhsdzo6MiBF1j4ClfNPEQAlG49YjrRXpI=
=4nai
-----END PGP SIGNATURE-----

-------------------------------------
On Aug 31, 2015 3:01 PM, "Justus Ranvier via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

I believe he explained very well what he meant by decentralized, please
stop suggesting he doesn't understand his own thoughts: it is extremely
irritating.

payments
perform?

For starters, a third party (or a recuded group of miners controlling the
majority of the hashrate) can censor transactions. It doesn't matter how
benevolent that party is: it can be forced to do it by the laws of its
jurisdiction.

If you don't care about this, I suggest you start a new system without
expensive proof of work, you can replace it with block signing (it can
still be multisig). It is already coded, just fork the alpha or the
blocksigning branch in elementsProject (github).
-------------------------------------
On Tue, Jan 20, 2015 at 08:43:57AM -0800, Daniel Stadulis wrote:

Heh, well, courts tend not to have the narrow-minded pedantic logic that
programmers do; quite likely that they'd see having the ability to give
themselves the ability as equivalent to simply having the ability. What
matters more is intent: the authors of an operating system had no intent
to have a custodial relationship over anyones' BTC, so they'd be off the
hook. The authors of a Bitcoin wallet on the other hand, depends on how
you go about it.

For instance Lighthouse has something called UpdateFX, which allows for
multi-signature updates. It also supports deterministic builds, and
allows users to chose whether or not they'll follow new updates
automatically, or only update on demand. In a court that could be all
brought up as examples of intent *not* to have a custodial relationship,
which may be enough to sway judge/jury, and certainly will help avoid
ending up in court in the first place by virtue of the fact that all
those protections help avoid theft, and increase the # of people that an
authority need to involve to seize funds via an update.

-- 
'peter'[:-1]@petertodd.org
00000000000000001a5e1dc75b28e8445c6e8a5c35c76637e33a3e96d487b74c
-------------------------------------
Not forgetting, simply deferring discussion on that.  We’ve a much smaller limit to deal with right now.  But even that limit would have to go to remove this attack.

From: Btc Drak 
Sent: Monday, June 08, 2015 3:07 PM
To: Raystonn . 
Cc: Peter Todd ; Bitcoin Dev ; Patrick Mccorry (PGR) 
Subject: Re: [Bitcoin-development] New attack identified and potential solution described: Dropped-transaction spam attack against the blocksize limit

On Mon, Jun 8, 2015 at 11:01 PM, Raystonn . <raystonn@hotmail.com> wrote:

  No, with no blocksize limit, a spammer would would flood the network with
  transactions until they ran out of money.

I think you are forgetting even if you remove the blocksize limit, there is still a hard message size limit imposed by the p2p protocol. Block would de-facto be limited to this size.
-------------------------------------
I think proof-of-idle had a potentially serious problem when I last looked at it. The risk is that a largish miner can use everyone else's idle time to construct a very long chain; it's also easy enough for them to make it appear to be the work of a large number of distinct miners. Given that this would allow them to arbitrarily re-mine any block rewards and potentially censor any transactions then that just seems like a huge security hole?


Cheers,
Dave



-------------------------------------
Hi everyone,

Thanks to everyone for a very friendly and scientifically-oriented
discussion. We have collated all the issues that have been raised related
to NG, and placed them in context, here:
    http://hackingdistributed.com/2015/11/09/bitcoin-ng-followup/

Overall, NG has a unique insight: turning the block creation process upside
down can provide many benefits. Most notably, throughput can go as high as
the network will allow, providing scalability benefits that increase as the
network improves. There are many other side benefits, including fast
confirmations that are stronger than 0-conf in Core, and come much more
quickly than Core's 1-confirmations. And there are ancillary benefits as
well, such as resilience to fluctuations in mining power, and healthier
incentives for participants to ferry transactions. We believe that a fresh
new permission-less blockchain protocol, designed today, would end up
looking more like NG than Core. Of course, if NG could possibly be layered
on top of Bitcoin, that would be the ultimate combination.

Many thanks for an interesting discussion, and as always, we're happy to
hear constructive suggestions and feedback,
- egs


On Wed, Oct 14, 2015 at 2:02 PM, Emin Gün Sirer <el33th4x0r@gmail.com>
wrote:

-------------------------------------
On Wed, Aug 19, 2015 at 3:59 AM, Jorge Timón <
bitcoin-dev@lists.linuxfoundation.org> wrote:


FWIW, and I mentioned this opinion in #bitcoin-dev on IRC, but I am
perfectly fine with receiving everything through a single mailing list. I
used to read the Wikipedia firehose of recent edits because I thought
that's how you were supposed to use the site. Edits per second eventually
reached beyond any reasonable estimate of human capacity and then I
realized what was going on. Any sort of "glorious future" for bitcoin with
hundreds of millions of users will also see this problem for future
developers, even if only 0.1% of that population are money-interested
programmers then that's 100,000 programmers to work with. I would never
want to turn off this raw feed. Having said that, I am somewhat surprise
that nobody has taken to weekly summaries of research and development
activity. Summarizing recent work is a valuable task that others can engage
in just by reading the mailing list and aggregating multiple thoughts
together, similar to release notes. I was also expecting to see something
like "individual developer's summaries of things they have found
interesting over the past 30-90 days or past year" digging up arcane
details from the mailing list archives, or more infrequent summaries of the
other smaller batched review emails. Digest mode mailing list consumption
is often recommended to those who are uninterested in dealing with low
signal-to-noise, but I suspect that summarizing activity would be more
valuable for this community, especially for the different cognitive niches
that have developed.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
Sync time wouldn't be longer compared to 20MB, it would (eventually) be
longer under either setup.

Also, and this is probably a silly concern, but wouldn't changing block
time change the supply curve? If we cut the rate in half or a power of two,
that affects nothing, but if we want to keep it in round numbers, we need
to do it by 10, 5, or 2. I feel like most people would bank for 10 or 5,
both of which change the supply curve due to truncation.

Again, it's a trivial concern, but probably one that should be addressed.
On May 25, 2015 11:52 PM, "Jim Phillips" <jim@ergophobia.org> wrote:

-------------------------------------
On Wed, Dec 16, 2015 at 9:11 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


This is really the most important question.

Bitcoin is kind of like a republic where there is separation of powers
between various groups.

The power blocs in the process include

- Core Devs
- Miners
- Exchanges
- Merchants
- Customers

Complete agreement is not required for a change.  If merchants and their
customers were to switch to different software, then there is little any of
the other groups could do.

Consensus is nice, certainly, and it is a good social norm to seek
widespread agreement before committing to a decision above objection.
Committing to no block increase is also committing to a decision against
objections.

Having said that, each of the groups are not equal in power and
organisation.

Merchants and their customers have potentially a large amount of power, but
they are disorganised.  There is little way for them to formally express a
view, much less put their power behind making a change.  Their potential
power is crippled by public action problems.

On the other extreme is the core devs. Their power is based on legitimacy
due to having a line of succession starting with Satoshi and respect gained
due to technical and political competence.  Being a small group, they are
organised and they are also more directly involved.

The miners are less centralised, but statements supported by the majority
of the hashing power are regularly made.  The miners' position is that they
want dev consensus.  This means that they have delegated their decision
making to the core devs.

The means that the two most powerful groups in Bitcoin have given the core
devs the authority to make the decision.  They don't have carte blanche
from the miners.

If the core devs made the 2MB hard-fork with a 75% miner threshold, it is
highly likely that the other groups would accept it.

That is the only authority that exists in Bitcoin.  The check is that if
the authority is abused, the other groups can simply leave (or use
checkpointing)
-------------------------------------
On 4 August 2015 at 01:22, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


​The scicast prediction market is shutdown atm (since early July?) so those
numbers aren't live. But...

Network hash rate
3,255.17 PH/s  (same block size)
5,032.64 PH/s  (block size increase)

4,969.68 PH/s  (no replace-by-fee)
3,132.09 PH/s  (replace-by-fee)

Those numbers seem completely implausible: that's ~2.9-3.6 doublings of the
current hashrate (< 400PH/s) in 17 months, when it's taken 12 months for
the last doubling, and there's a block reward reduction due in that period
too. (That might've been a reasonable prediction sometime in the past year,
when doublings were slowing from once every ~45 days to once a year; it
just doesn't seem a supportable prediction now)

That the PH/s rate is higher with bigger blocks is surprising, but given
that site also predicts USD/BTC will be $280 with no change but $555 with
bigger blocks, so I assume that difference is mostly due to price. Also,
12.5btc at $555 each is about 23 btc at $300 each, so if that price
increase is realistic, it would compensate for almost all of the block
reward reduction.

Daily transaction volume
168,438.22 tx/day  (same block size)
193,773.08 tx/day  (block size increase)

192,603.80 tx/day  (no replace-by-fee)
168,406.73 tx/day  (replace-by-fee)

That's only a 15% increase in transaction volume due to the block size
increase; I would have expected more? 168k-194k tx/day is also only a
30%-50% increase in transaction volume from 130k tx/day currently. If
that's really the case, then a 1.5MB-2MB max block size would probably be
enough for the next two years...

(Predicting that the node count will drop from ~5000 to ~1200 due to
increasing block sizes seems quite an indictment as far as centralisation
risks go; but given I'm not that convinced by the other predictions, I'm
not sure I want to give that much weight to that one either)

Cheers,
aj

-- 
Anthony Towns <aj@erisian.com.au>
-------------------------------------


Only being partly serious - I strongly am in favor of a sufficiently modularized codebase that swapping out consensus rules is fairly straightforward and easy to test. I’m not in favor of encouraging forking an existing blockchain without having mechanisms in place to gracefully merge back without significant network disruptions. We do not have this yet.


That’s exactly what my coinparams_new branch does. Adding a parameter for maximum block size would be straightforward.


Yes, indeed - this would be a special case.


I do not encourage anyone to try to fork an existing blockchain without first securing overwhelming (near unanimous) consensus…or without having yet built a mechanism that can merge divergent chains gracefully.


-------------------------------------
On Mon, 16 Mar 2015 09:29:03 -0700, Sergio Lerner  
<sergiolerner@certimix.com> wrote:

Thanks so much for publishing this. It could be useful in any application  
to try to prove a keyed copy of some data.

If I understand correctly, transforming raw blocks to keyed blocks takes  
512x longer than transforming keyed blocks back to raw. The key is public,  
like the IP, or some other value which perhaps changes less frequently.

The verifier keeps blocks in the keyed format, and can decrypt quickly to  
provide raw data, or use the keyed data for hashing to try to demonstrate  
they have a pre-keyed copy.


Can you clarify, the prover is hashing random blocks of *decrypted*, as-in  
raw, blockchain data? What does this prove other than, perhaps, fast  
random IO of the blockchain? (which is useful in its own right, e.g. as a  
way to ensure only full-node IO-bound mining if baked into the PoW)

How is the verifier validating the response without possession of the full  
blockchain?


The challenger requests a hash-sum of a random sequence of indices of the  
keyed data, based on a challenge seed. So in a few bytes round-trip we can  
see how fast the computation is completed. If the data is already keyed,  
the hash of 1,000 random 1024-bit blocks should come back much faster than  
if the data needs to be keyed on-the-fly.

To verify the response, the challenger would have to use the peer's  
identity key and perform the slower transforms on those same 1,000 blocks  
and see that the result matches, so cost to challenger is higher than  
prover, assuming they actually do the computation.

Which brings up a good tweak, a full-node challenger could have to do the  
computation first, then also include something like HMAC(identityKey,  
expectedResult). The prover could then know if the challenger was honest  
before returning a result, and blacklist them if not.


I guess a new-node could see if different servers all returned the same  
challenge response, but they would have no way to know if the challenge  
response was technically correct, or sybil.

I also wonder about the effect of spinning disk versus SSD. Seek time for  
1,000 random reads is either nearly zero or dominating depending on the  
two modes. I wonder if a sequential read from a random index is a possible  
trade-off,; it doesn't prove possession of the whole chain nearly as well,  
but at least iowait converges significantly. Then again, that presupposes  
a specific ordering on disk which might not exist. In X years it will all  
be solid-state, so eventually it's moot.



-------------------------------------
On Aug 17, 2015 5:29 PM, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Can you explain how the spv node fails against an attacker with a
non-trivial amount of hash power where a full node doesn't? To attack an
spv wallet that is waiting for 6 or 10 confirmations, you would not only
need to Sybil them but also summon a massive amount of hashing power to
create a chain of headers (while forgoing the opportunity to mine valid
blocks with that hash power).

But could someone with that much hash power not Sybil a full node and give
them a chain for valid blocks (but on an orphan fork)? The failure model
doesn't seem specific to spv to me.
-------------------------------------
On Fri, May 8, 2015 at 2:20 AM, Matt Whitlock <bip@mattwhitlock.name> wrote:

Block contents can be grinded much faster than hashgrinding and
mining. There is a significant run-away effect there, and it also
works in the gradual sense as a miner probabilistically mines large
blocks that get averaged into that 2016 median block size computation.
At least this proposal would be a slower way of pushing out miners and
network participants that can't handle 100 GB blocks immediately..  As
the size of the blocks are increased, low-end hardware participants
have to fall off the network because they no longer meet the minimum
performance requirements. Adjustment might become severely mismatched
with general economic trends in data storage device development or
availability or even current-market-saturation of said storage
devices. With the assistance of transaction stuffing or grinding, that
2016 block median metric can be gamed to increase faster than other
participants can keep up with or, perhaps worse, in a way that was
unintended by developers yet known to be a failure mode. These are
just some issues to keep and mind and consider.

- Bryan
http://heybryan.org/
1 512 203 0507


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On Thu, Jun 18, 2015 at 02:29:42PM +0200, Pieter Wuille wrote:

Sure. According to github, there exist 4890 forks of the bitcoin/bitcoin repository.

Forking the code is perfectly fine in itself, that doesn't even need to be said, it's how open source works. Make your changes, run your own version, contribute back the changes (or not).

And I never had a problem with Bitcoin-XT while it was just a patch-set with no consensus changes. But a controversial hard fork of the chain is something else completely.

Wladimir
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJVgr5LAAoJEHSBCwEjRsmm5mMH/0yLGGQQefRVdmM/nJZ60b/z
iTCUUzY4eLL67FRC6pqGA18RdUt4Etl4wEqvgXH/B9mWIAM2yQD/jnxutYrEIoBT
8Jyd1OhmmKF8MN5/uE7JNPivIuHs0ioF+qTxlbdElpVZ2NodVotznbTvuqJgXUFb
c9Et5L5n7g55uPzDt+MSV5iBDJaMiBAnZA00aTLGmYmNXxcy7xBwCFX3dDij8krv
0+zdpNNAKm85k1rG2jHCM+0onu+TOIur03pPd5OZktgr18P6UvAQ6A59yAkGgFai
4l6VVNJ40g3PzItGQ7wsKZ8s/qG5LlcEppxMlG6CX1dIDpxbrwx2aJmeNjwSLKQ=
=LbA3
-----END PGP SIGNATURE-----


-------------------------------------

IMO it's better to pair a protocol spec with an implementation. For one, it
can show up issues in the design you didn't think of. For another,
implementation is a lot more work than speccing out a few protocol buffers
and high level procedures, so people who are going to write an
implementation probably won't follow your design unless they have a great
degree of confidence in it and some compelling reason to use it (e.g.
interop with other users).
-------------------------------------
The state of bitcoin transactions can be committed to in blocks by keeping two
running hashes, one of unspent transaction outputs and one of spent transaction
outputs.  A "running hash" $R$ I define as being computed by taking the previous
value of the hash $r$, concatenating it with the new data $x$, and hashing it:
\[
    R = hash(r|x).
\]
In the case of the UTXO set, the data $x$ can be taken to be the concatenation
(txid|vout|amount) for all outputs, let's call this running hash hTXO.  Because
data cannot be "removed" from this set commitment, a second hash can be computed
consisting of the spent outputs, let's call this hSTXO.  Thus the pair of hashes
(hTXO, hSTXO) is equivalent to a hash of all unspent outputs.  These hashes can
be placed into a block's Merkle tree by miners with a soft fork.  It can be
reduced to a single hash hUXTO = hash(hTXO|hSXTO) if desired.

By defining *how* to compute (hTXO, hSXTO) we can define an implementation
independent definition of consensus that is extremely cheap to compute.  The
order in which outputs are hashed is clearly important, but bitcoin has a well
defined ordering already in terms of the order in which transactions appear in
blocks, and the sequential order of outputs.

In the recent discussion surrounding leveldb and jgarzik's new sqlite branch, it
has been brought up repeatedly by gmaxwell that this db is "consensus critical".
As a data structure storing the state of transactions, of course it's consensus
critical.  However there's only one right answer to what the set of UTXOs is.
Any other result reported by the db is simply wrong.  By creating and publishing
(hTXO, hSXTO), miners can publish their view of the transaction state, and any
implementation can be validated against it.

As I understand it, leveldb is in the bitcoin core source tree because it could
have bugs and give the wrong answer for a given UTXO (see BIP50).  This is worse
than a consensus failure, it's just wrong, and the argument that we have to keep
leveldb around and maintain it because it could be wrong is pretty ugly, and I
don't think anyone actually wants to do this.  Let's not be wrong in the first
place, and let's choose databases based on performance and other considerations.
"Not being wrong" should go without saying, regardless of implementation
details.

It should be noted that (hTXO, hSXTO) can be computed twice, once without the
database (while processing a new block) and once by requesting the same data
from the database.  So bad database behavior can be detected and prevented from
causing consensus failures.  And then we can remove leveldb from the core.

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 

-------------------------------------
On Sun, Apr 26, 2015 at 03:01:10AM +0300, s7r wrote:

Agreed, needing the transaction to be signed & broadcastable before the
refunds can be generated is similar to paying for a contract before the
terms have been decided.


The current problem is that SIGHASH_NORMALIZED_TXID as presently
discussed implies stripping the sigScript, which is not sufficient for
the Lightning Network.

The currently discussed SIGHASH_NORMALIZED_TXID does not permit chained
transactions 2 levels deep, which is necessary for Lightning as well.
The path from the Commitment -> HTLC -> Refund requires up to 3 levels
deep of transactions. 

Suppose TxA -> TxB -> TxC -> TxD. All outputs are 2-of-2 multisig. TxA
has already entered into the blockchain, the rest have not yet been
broadcast. If TxB spends from TxA, it doesn't need new sighash flags, it
just does a plain SIGHASH_ALL. However, TxC needs
SIGHASH_NORMALIZED_TXID due to malleability risks.
SIGHASH_NORMALIZED_TXID works for TxC because the sigScript can change,
but because TxA's txid has already entered the blockchain, the parent's
input txids cannot change (with high degrees of certainty).

However, with TxD, the txid of TxB may be different, which will result
in an invalid transaction if SIGHASH_NORMALIZED_TXID only strips the
sigScript when obtaining the normalized txid of TxC. The reason for this
is TxC's input txid of TxB has changed (TxC's input 0 txid of TxB)!

Therefore, a functional SIGHASH_NORMALIZED which permits chained
transactions requires the parent transaction's sigScript *AND* txid to
be stripped when determining the parent's normalized txid. Similar to
OP_CHECKSIG, a part of the normalized TXID includes each input's
scriptPubKey, e.g. TxC's normalized TXID includes TxB's scriptPubKey
output which it is spending, so when TxD signs TxC's normalized TXID, it
includes TxB's output (this is a cheap way of increasing uniqueness but
is not an absolute necessity if it's too difficult). All this data
should be immediately available when validating the transaction and
appending it to the UTXO set.

If the txid and sigScript are removed when building the normalized input
txid as part of the spend/signature, it should be possible for chained
transactions to work. However, this isn't absolute security against
replay attacks. If there are two spends with all inputs having the same
values *AND* the same scriptPubKeys per input, then it can be replayed.
The odds of this occurring seems like a sort of uncanny valley of risks;
it's low enough that it shouldn't ever happen which may result in a lack
of documentation, so when it does happen it'll be a big surprise. So,
even if this "safer" method becomes a softfork, perhaps great care
should be taken before making this a default method of spending when the
sighash flag is not an absolute necessity (i.e. "don't do it!" I'm all
in favor of giving this a scary name so developers won't inadvertently
think "hey, normalization sounds like a good thing to do").

That said, it should cover an overwhelming majority of potential
replays, it's nearly impossible to create a "duplicate" replayable tx of
someone *else's* send, since the poteintally "replayable" transaction
signs the sigScript of the redeemed output.

As a side note, SIGHASH_NORMALIZED does not permit spending from any
transaction, which is desirable for the Lightning Network (HTLCs may
persist in new Commitment Transactions). However, this is merely a "nice
to have" and is not an absolute necessity, there is no significant loss
of functionality, merely some slight slowdown from significantly more
signatures. For Lightning in particular, the effect would probably be
batching Commitment Transactions (e.g. 1 mass update per second per
channel), with the only major discernable penalty is an order of
magnitude greater storage of signatures.

Additionally, I think it was Mark Friedenbach who brought up that
SIGHASH_NORMALIZED creates significant complexities with the need for an
additional hash with every UTXO (almost doubling the UTXO set size), and
with nodes which already have UTXO pruning enabled, it'll require
downloading the entire blockchain. I'm not sure if this problem is
insurmountable or not, but if a normalized sighash becomes the most
ideal candidate for a malleability soft-fork, then sooner may be better
than later as more nodes start using the pruning patch.



Assuming you mean the current P2SH scriptPubKey format, it's not
possible to do so while making it a soft fork. If you use OP_EQUAL,
current nodes will treat "P3SH" transactions as P2SH ones.

I'm in favor of keeping P3SH conservative. It's possible to have your
cake and eat it too, by enabling script versions within P3SH.

If you create P3SH as:

OP_DUP <20-byte hash> OP_EQUALVERIFY

The redeemScript has the first byte as a version number, and there is
also an OP_TRUE pushed right before the redeemScript. The scriptSig
would look something like:

<sigs...> OP_TRUE <3 redeemScript>

When executing the script, the last item on the stack verifies against
the hash, then the redeemScript is copied/read, the 3 is popped off
(first byte unsigned int), the OP_TRUE is popped off the stack, and the
script then executes P3SH "version 3" (again, it is the first byte, NOT
an opcode). Any non-known version will return everything as true and not
continue with execution of the script, to permit future soft-forks. The
OP_TRUE is to ensure there is a OP_TRUE left on the stack just in case
for older nodes as this is an EQUALVERIFY.

This works because the address, 20-byte hash, has the 3 version number
as part of the hash, so it is the recipient who determines the version
number. For future soft-forks, it's incredibly flexible, just make the
version byte to 4. Prior addresses work the same, and it's not possible
to accidentally send it using different scripting versions. Perhaps this
can make things upgradeable enough that a malleability sighash flag can
go in sooner rather than later.

-- 
Joseph Poon


-------------------------------------
On Thursday, 18 June 2015, at 8:31 pm, Ross Nicoll wrote:

An honest question: who is proposing inaction? I haven't seen anyone in this whole, agonizing debate arguing that 1MB blocks are adequate. The debate has been about *how* to increase the block-size limit and whether to take action ASAP (at the risk of fracturing Bitcoin) or to delay action for further debate (at the risk of overloading Bitcoin). Even those who are arguing for further debate are not arguing for *inaction*.


-------------------------------------
On Aug 11, 2015 11:44 PM, "Thomas Zander" <zander32@gmail.com> wrote:
apart

And again, you dodge the question...

productive.
is
a

It's not that I don't remember, it's that for all your "reasons" I can
always say one of these:

1) This could only be an indirect consequence of rising fees (people will
move to a competitive system, cheap transactions will become unreliable,
etc).
2) This problem will appear with other sizes too and it needs to be solved
permanently no matter what (dumb mempool design, true scalability, etc)

that
that
People

Whatever, even suggesting you may want to just spread fud and that's why
you don't respond directly to the questions made you respond directly to
the question: you answered with "[]".
I just give up trying that people worried about a non-increase in the short
term answer to me that question. I will internally think that they just
want to spread fud, but not vey vocal about it.
It's just seems strange to me that you don't want to prove to me that's not
the case when it is so easy to do so: just answer the d@#/&m question.


I'm not so sure, people keep talking about the need to scale the system by
increasing the consensus maximum...
But I'm happy that, indeed, many (possibly most?) people understand this.

higher

I disagree with this.
In any case, how can future demand be easier to predict than software
development times?
-------------------------------------


Great work!

It also means the remaining usages of OpenSSL can be safely replaced with
something like LibreSSL or (perhaps better) BoringSSL.
-------------------------------------
I'm merely proving the existence of the effect.
On Jun 12, 2015 8:24 PM, "Mike Hearn" <mike@plan99.net> wrote:

-------------------------------------
On Sat, May 16, 2015 at 5:39 AM, Stephen <stephencalebmorse@gmail.com>
wrote:


In effect, there is a confirm penalty for less strict blocks.  Confirms =
max(miner_confirms, merchant_confirms - 3, 0)

Merchants who don't upgrade end up having to wait longer to hit
confirmations.

If they get deep enough in the chain, though, the client should probably

That is a good idea.  Any parameters that have miner/merchant differences
should be modifiable (but only upwards) in the command line.

"Why are my transactions taking longer to confirm?"

"There was a soft fork to make the block size larger and your client is
being careful.  You need to add "minermaxblocksize=4MB" to your
bitcoin.conf file."

Hah, it could be called a "semi-hard fork"?
-------------------------------------
On Thu, Jul 23, 2015 at 9:52 PM, Jameson Lopp via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Although I don't have a concrete proposals myself, I agree that
without having any common notion of what the "minimal target hardware"
looks like, it is very difficult to discuss other things that depend
on that.
If there's data that shows that a 100 usd raspberry pi with a 1 MB
connection in say, India (I actually have no idea about internet
speeds there) size X is a viable full node, then I don't think anybody
can reasonably oppose to rising the block size to X, and such a
hardfork can perfectly be uncontroversial.
I'm exaggerating ultra-low specifications, but it's just an example to
illustrate your point.
There was a thread about formalizing such "minimum hardware
requirements", but I think the discussion simply finished there:
- Let's do this
- Yeah, let's do it
- +1, let's have concrete values, I generally agree.

-------------------------------------
On Sat, Jul 25, 2015 at 12:50 AM, Tom Harding <tomh@thinlink.com> wrote:

My level of contributions are irrelevant to this discussion. But
still, I feel I should clarify some of the metrics you are looking to.
Most of my contributions so far are code refactors (with a small
change on the command-line options and a small optimization here and
there). This type of changes is usually better done incrementally to
be less risky and disruptive (this is specially important for
consensus-related code), and that makes my total commit count
unusually high, even when some people have contributed more in new
functionality than me in a single commit.
Also code movements (often required as part of these refactors)
produce unusually high total diff counts even when they require little
less than copy and paste (once you know what you want to move and
where, of course).
If I didn't thought this work is extremely important in the long term
(among other things, to make the code more accessible to new
reviewers/developers) I wouldn't be doing it, but you can't just
compare contributions counting commits or lines changed, that's not
how it works.
Github may say that I'm #10 with 96 commits / 9,371 ++ / 8,962 --, but
it's obvious to me that, say, gmaxwell #13 with 71 commits / 807 ++ /
707 -- has done a lot more for Bitcoin Core than I have.
Even if it was true that I'm the person currently coding more for
Bitcoin Core, I wouldn't write any of that if I had no hope of getting
review, so review is certain sense much more important than coding.


Who cares?
If my work is good for the software, my motivations are irrelevant. If
I accidentally PR a bug, my motivations are again: the bug should not
be accepted no matter how pure and noble my intentions are.
But, no, making Bitcoin's price (no offense taken, but there's an
spanish say that goes like this "Sólo un necio confunde valor y
precio" which translates to "Only a fool confuses value and price")
rise is not one of my main motivations.
I'm much more concerned about the long term success of the currency
(for which turnover is a much more interesting metric than market cap
IMO) and about learning a technology that I believe will revolutionize
the world, but maybe you don't believe me. There's a Bitcoin incentive
as part of my Blockstream's contract, so I have a financial incentive
for Bitcoin's price to increase, but, in fact, when I started
contributing to bitcoin core my bitcoin holdings where extremely low.
It bothers me that so many people seem to assume that Bitcoin
developers are also hardcore currency speculators and are also good at
it (I can say Bitcoin has teach me that I'm a terrible day-trader
myself).
There's many reasons to contribute to Bitcoin core and none of them
are relevant to this discussion.


The fact is that there's no "bitcoin developer dance" that makes it
rain and also raises bitcoin's market price 100 usd. And suggesting
"rising the price" as a solution to any problem just cannot be
considered a serious proposal.
No, we can't just ACK a "double the price" PR when the next halving comes.


If that's what you're doing as a currency speculator, that's fine.
It's just off-topic to this list.
And, no, that's not "what I am doing" as a software developer.
I want the system to improve, like that "Jessie J" singer said, forget
about the priceeeeeeeeee, yeah.

-------------------------------------
On Tuesday 11. August 2015 00.52.23 Pieter Wuille wrote:


I'm going to go with this one, since we are seeking common ground and all of 
this makes sense to me. And I bet to Gavin would agree to this too.

The question I want to ask is this;

How do you expect to get from the current to the situation outlined above?

There are several market forces at work;

* people currently expect near-free payments.
* people currently expect zero-confirmations.
* Bitcoin is seeing a huge amount of uptake, popularity, etc.
* With Greece still in flux, there is a potential enormous spike of usage set 
to come when (not if) the Euro falls.


I conclude that we need;

* to create and make working solutions like LN, sidechains etc etc etc.
This should allow people to get their fast confirmation time. Who cares its of 
a different nature, the point is that the coffeeshop owner lets you leave with 
your coffee. We can sell that.

* To buy ourselves time, LN is not done, Bitpay and friends work on-chain. 
That won't change for a year at least.
We need to move the max-block size to a substantial bigger size to allow 
Bitcoin to grow.


Unfortunately for us all, Bitcoin is over-sold. We don't have a sales 
department but the worth-of-mouth leaves us in a bad situation. And we need to 
react to make sure the product isn't killed by bad publicity. What Gox didn't 
manage can certainly happen when people find out that Bitcoin can't currently 
do any of the things everyone is talking about.
So, while LN is written, rolled out and tested, we need to respond with bigger 
blocks.  8Mb - 8Gb sounds good to me.

Can everyone win?
-- 
Thomas Zander

-------------------------------------
On Mon, Jan 26, 2015 at 5:14 AM, Pieter Wuille <pieter.wuille@gmail.com> wrote:

Sure. BIP0066. There was also some feedback on Bitcointalk, which I
think you've addressed:
https://bitcointalk.org/index.php?topic=932054.0 I also had off-list
positive feedback from Amir Taak, so we have positive feedback from
several implementers.

One of the points that was raised which we'd discussed pre-proposal
that was brought up there that I thought I should summarize here was
the possibility that someone had previously authored an nlocked spend
with an invalidly encoded signature. In those cases the signature can
just be mutated to get it mined, and would need to be already to pass
IsStandard rules. A case that isn't covered if if they have a chain of
transactions after that nlocked transaction, but those cases would
already be at extreme risk of malleability (esp since their unchanged
form is non-standard), and that coupled with the fact that avoiding
this would undermine the intent of the BIP (independence from  a
specific encoding scheme) seems to have been convincing as much.


-------------------------------------
Maybe a new analogy helps.

SW presents a blended price and blended basket of two goods.  You can
interact with the Service through the blended price, but that does not
erase the fact that the basket contains two separate from similar resources.

A different set of economic actors uses one resource, and/or both.  There
are explicit incentives to shift actors from solely using one resource to
using both.

The fact that separate sets of economic actors and incentives exist is
sufficient to prove it is indeed a basket of goods, not a single good.


On Wed, Dec 16, 2015 at 4:36 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:


Under your blended algorithm, this seems reasonable as a first pass.




This is a microscopic, not macroscopic analysis.  Externalities and long
term incentives can severely perturb or invalidate that line of thinking.

Typical counter-example:  Many miners are perfectly happy with very low
fees to encourage long term growth of their bitcoin income through network
effect growth -- rendering fee micro-optimizations largely in the realm of
DoS prevention rather than miner incentive.
-------------------------------------

Bitcoin Core version 0.10.0 is now available from:

   https://bitcoin.org/bin/0.10.0/

This is a new major version release, bringing both new features and
bug fixes.

Please report bugs using the issue tracker at github:

   https://github.com/bitcoin/bitcoin/issues

The whole distribution is also available as torrent:

   https://bitcoin.org/bin/0.10.0/bitcoin-0.10.0.torrent

   magnet:?xt=urn:btih:170c61fe09dafecfbb97cb4dccd32173383f4e68&dn=0.10.0&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Fopen.demonii.com%3A1337&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2F

Upgrading and downgrading
=========================

How to Upgrade
--------------

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes for older versions), then run the
installer (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or
bitcoind/bitcoin-qt (on Linux).

Downgrading warning
---------------------

Because release 0.10.0 makes use of headers-first synchronization and parallel
block download (see further), the block files and databases are not
backwards-compatible with older versions of Bitcoin Core or other software:

* Blocks will be stored on disk out of order (in the order they are
received, really), which makes it incompatible with some tools or
other programs. Reindexing using earlier versions will also not work
anymore as a result of this.

* The block index database will now hold headers for which no block is
stored on disk, which earlier versions won't support.

If you want to be able to downgrade smoothly, make a backup of your entire data
directory. Without this your node will need start syncing (or importing from
bootstrap.dat) anew afterwards. It is possible that the data from a completely
synchronised 0.10 node may be usable in older versions as-is, but this is not
supported and may break as soon as the older version attempts to reindex.

This does not affect wallet forward or backward compatibility.


Notable changes
===============

Faster synchronization
----------------------

Bitcoin Core now uses 'headers-first synchronization'. This means that we first
ask peers for block headers (a total of 27 megabytes, as of December 2014) and
validate those. In a second stage, when the headers have been discovered, we
download the blocks. However, as we already know about the whole chain in
advance, the blocks can be downloaded in parallel from all available peers.

In practice, this means a much faster and more robust synchronization. On
recent hardware with a decent network link, it can be as little as 3 hours
for an initial full synchronization. You may notice a slower progress in the
very first few minutes, when headers are still being fetched and verified, but
it should gain speed afterwards.

A few RPCs were added/updated as a result of this:
- `getblockchaininfo` now returns the number of validated headers in addition to
the number of validated blocks.
- `getpeerinfo` lists both the number of blocks and headers we know we have in
common with each peer. While synchronizing, the heights of the blocks that we
have requested from peers (but haven't received yet) are also listed as
'inflight'.
- A new RPC `getchaintips` lists all known branches of the block chain,
including those we only have headers for.

Transaction fee changes
-----------------------

This release automatically estimates how high a transaction fee (or how
high a priority) transactions require to be confirmed quickly. The default
settings will create transactions that confirm quickly; see the new
'txconfirmtarget' setting to control the tradeoff between fees and
confirmation times. Fees are added by default unless the 'sendfreetransactions' 
setting is enabled.

Prior releases used hard-coded fees (and priorities), and would
sometimes create transactions that took a very long time to confirm.

Statistics used to estimate fees and priorities are saved in the
data directory in the `fee_estimates.dat` file just before
program shutdown, and are read in at startup.

New command line options for transaction fee changes:
- `-txconfirmtarget=n` : create transactions that have enough fees (or priority)
so they are likely to begin confirmation within n blocks (default: 1). This setting
is over-ridden by the -paytxfee option.
- `-sendfreetransactions` : Send transactions as zero-fee transactions if possible 
(default: 0)

New RPC commands for fee estimation:
- `estimatefee nblocks` : Returns approximate fee-per-1,000-bytes needed for
a transaction to begin confirmation within nblocks. Returns -1 if not enough
transactions have been observed to compute a good estimate.
- `estimatepriority nblocks` : Returns approximate priority needed for
a zero-fee transaction to begin confirmation within nblocks. Returns -1 if not
enough free transactions have been observed to compute a good
estimate.

RPC access control changes
--------------------------

Subnet matching for the purpose of access control is now done
by matching the binary network address, instead of with string wildcard matching.
For the user this means that `-rpcallowip` takes a subnet specification, which can be

- a single IP address (e.g. `1.2.3.4` or `fe80::0012:3456:789a:bcde`)
- a network/CIDR (e.g. `1.2.3.0/24` or `fe80::0000/64`)
- a network/netmask (e.g. `1.2.3.4/255.255.255.0` or `fe80::0012:3456:789a:bcde/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff`)

An arbitrary number of `-rpcallow` arguments can be given. An incoming connection will be accepted if its origin address
matches one of them.

For example:

| 0.9.x and before                           | 0.10.x                                |
|--------------------------------------------|---------------------------------------|
| `-rpcallowip=192.168.1.1`                  | `-rpcallowip=192.168.1.1` (unchanged) |
| `-rpcallowip=192.168.1.*`                  | `-rpcallowip=192.168.1.0/24`          |
| `-rpcallowip=192.168.*`                    | `-rpcallowip=192.168.0.0/16`          |
| `-rpcallowip=*` (dangerous!)               | `-rpcallowip=::/0` (still dangerous!) |

Using wildcards will result in the rule being rejected with the following error in debug.log:

     Error: Invalid -rpcallowip subnet specification: *. Valid are a single IP (e.g. 1.2.3.4), a network/netmask (e.g. 1.2.3.4/255.255.255.0) or a network/CIDR (e.g. 1.2.3.4/24).


REST interface
--------------

A new HTTP API is exposed when running with the `-rest` flag, which allows
unauthenticated access to public node data.

It is served on the same port as RPC, but does not need a password, and uses
plain HTTP instead of JSON-RPC.

Assuming a local RPC server running on port 8332, it is possible to request:
- Blocks: http://localhost:8332/rest/block/*HASH*.*EXT*
- Blocks without transactions: http://localhost:8332/rest/block/notxdetails/*HASH*.*EXT*
- Transactions (requires `-txindex`): http://localhost:8332/rest/tx/*HASH*.*EXT*

In every case, *EXT* can be `bin` (for raw binary data), `hex` (for hex-encoded
binary) or `json`.

For more details, see the `doc/REST-interface.md` document in the repository.

RPC Server "Warm-Up" Mode
-------------------------

The RPC server is started earlier now, before most of the expensive
intialisations like loading the block index.  It is available now almost
immediately after starting the process.  However, until all initialisations
are done, it always returns an immediate error with code -28 to all calls.

This new behaviour can be useful for clients to know that a server is already
started and will be available soon (for instance, so that they do not
have to start it themselves).

Improved signing security
-------------------------

For 0.10 the security of signing against unusual attacks has been
improved by making the signatures constant time and deterministic.

This change is a result of switching signing to use libsecp256k1
instead of OpenSSL. Libsecp256k1 is a cryptographic library
optimized for the curve Bitcoin uses which was created by Bitcoin
Core developer Pieter Wuille.

There exist attacks[1] against most ECC implementations where an
attacker on shared virtual machine hardware could extract a private
key if they could cause a target to sign using the same key hundreds
of times. While using shared hosts and reusing keys are inadvisable
for other reasons, it's a better practice to avoid the exposure.

OpenSSL has code in their source repository for derandomization
and reduction in timing leaks that we've eagerly wanted to use for a
long time, but this functionality has still not made its
way into a released version of OpenSSL. Libsecp256k1 achieves
significantly stronger protection: As far as we're aware this is
the only deployed implementation of constant time signing for
the curve Bitcoin uses and we have reason to believe that
libsecp256k1 is better tested and more thoroughly reviewed
than the implementation in OpenSSL.

[1] https://eprint.iacr.org/2014/161.pdf

Watch-only wallet support
-------------------------

The wallet can now track transactions to and from wallets for which you know
all addresses (or scripts), even without the private keys.

This can be used to track payments without needing the private keys online on a
possibly vulnerable system. In addition, it can help for (manual) construction
of multisig transactions where you are only one of the signers.

One new RPC, `importaddress`, is added which functions similarly to
`importprivkey`, but instead takes an address or script (in hexadecimal) as
argument.  After using it, outputs credited to this address or script are
considered to be received, and transactions consuming these outputs will be
considered to be sent.

The following RPCs have optional support for watch-only:
`getbalance`, `listreceivedbyaddress`, `listreceivedbyaccount`,
`listtransactions`, `listaccounts`, `listsinceblock`, `gettransaction`. See the
RPC documentation for those methods for more information.

Compared to using `getrawtransaction`, this mechanism does not require
`-txindex`, scales better, integrates better with the wallet, and is compatible
with future block chain pruning functionality. It does mean that all relevant
addresses need to added to the wallet before the payment, though.

Consensus library
-----------------

Starting from 0.10.0, the Bitcoin Core distribution includes a consensus library.

The purpose of this library is to make the verification functionality that is
critical to Bitcoin's consensus available to other applications, e.g. to language
bindings such as [python-bitcoinlib](https://pypi.python.org/pypi/python-bitcoinlib) or
alternative node implementations.

This library is called `libbitcoinconsensus.so` (or, `.dll` for Windows).
Its interface is defined in the C header [bitcoinconsensus.h](https://github.com/bitcoin/bitcoin/blob/0.10/src/script/bitcoinconsensus.h).

In its initial version the API includes two functions:

- `bitcoinconsensus_verify_script` verifies a script. It returns whether the indicated input of the provided serialized transaction 
correctly spends the passed scriptPubKey under additional constraints indicated by flags
- `bitcoinconsensus_version` returns the API version, currently at an experimental `0`

The functionality is planned to be extended to e.g. UTXO management in upcoming releases, but the interface
for existing methods should remain stable.

Standard script rules relaxed for P2SH addresses
------------------------------------------------

The IsStandard() rules have been almost completely removed for P2SH
redemption scripts, allowing applications to make use of any valid
script type, such as "n-of-m OR y", hash-locked oracle addresses, etc.
While the Bitcoin protocol has always supported these types of script,
actually using them on mainnet has been previously inconvenient as
standard Bitcoin Core nodes wouldn't relay them to miners, nor would
most miners include them in blocks they mined.

bitcoin-tx
----------

It has been observed that many of the RPC functions offered by bitcoind are
"pure functions", and operate independently of the bitcoind wallet. This
included many of the RPC "raw transaction" API functions, such as
createrawtransaction.

bitcoin-tx is a newly introduced command line utility designed to enable easy
manipulation of bitcoin transactions. A summary of its operation may be
obtained via "bitcoin-tx --help" Transactions may be created or signed in a
manner similar to the RPC raw tx API. Transactions may be updated, deleting
inputs or outputs, or appending new inputs and outputs. Custom scripts may be
easily composed using a simple text notation, borrowed from the bitcoin test
suite.

This tool may be used for experimenting with new transaction types, signing
multi-party transactions, and many other uses. Long term, the goal is to
deprecate and remove "pure function" RPC API calls, as those do not require a
server round-trip to execute.

Other utilities "bitcoin-key" and "bitcoin-script" have been proposed, making
key and script operations easily accessible via command line.

Mining and relay policy enhancements
------------------------------------

Bitcoin Core's block templates are now for version 3 blocks only, and any mining
software relying on its `getblocktemplate` must be updated in parallel to use
libblkmaker either version 0.4.2 or any version from 0.5.1 onward.
If you are solo mining, this will affect you the moment you upgrade Bitcoin
Core, which must be done prior to BIP66 achieving its 951/1001 status.
If you are mining with the stratum mining protocol: this does not affect you.
If you are mining with the getblocktemplate protocol to a pool: this will affect
you at the pool operator's discretion, which must be no later than BIP66
achieving its 951/1001 status.

The `prioritisetransaction` RPC method has been added to enable miners to
manipulate the priority of transactions on an individual basis.

Bitcoin Core now supports BIP 22 long polling, so mining software can be
notified immediately of new templates rather than having to poll periodically.

Support for BIP 23 block proposals is now available in Bitcoin Core's
`getblocktemplate` method. This enables miners to check the basic validity of
their next block before expending work on it, reducing risks of accidental
hardforks or mining invalid blocks.

Two new options to control mining policy:
- `-datacarrier=0/1` : Relay and mine "data carrier" (OP_RETURN) transactions
if this is 1.
- `-datacarriersize=n` : Maximum size, in bytes, we consider acceptable for
"data carrier" outputs.

The relay policy has changed to more properly implement the desired behavior of not 
relaying free (or very low fee) transactions unless they have a priority above the 
AllowFreeThreshold(), in which case they are relayed subject to the rate limiter.

BIP 66: strict DER encoding for signatures
------------------------------------------

Bitcoin Core 0.10 implements BIP 66, which introduces block version 3, and a new
consensus rule, which prohibits non-DER signatures. Such transactions have been
non-standard since Bitcoin v0.8.0 (released in February 2013), but were
technically still permitted inside blocks.

This change breaks the dependency on OpenSSL's signature parsing, and is
required if implementations would want to remove all of OpenSSL from the
consensus code.

The same miner-voting mechanism as in BIP 34 is used: when 751 out of a
sequence of 1001 blocks have version number 3 or higher, the new consensus
rule becomes active for those blocks. When 951 out of a sequence of 1001
blocks have version number 3 or higher, it becomes mandatory for all blocks.

Backward compatibility with current mining software is NOT provided, thus miners
should read the first paragraph of "Mining and relay policy enhancements" above.

0.10.0 Change log
=================

Detailed release notes follow. This overview includes changes that affect external
behavior, not code moves, refactors or string updates.

RPC:
- `f923c07` Support IPv6 lookup in bitcoin-cli even when IPv6 only bound on localhost
- `b641c9c` Fix addnode "onetry": Connect with OpenNetworkConnection
- `171ca77` estimatefee / estimatepriority RPC methods
- `b750cf1` Remove cli functionality from bitcoind
- `f6984e8` Add "chain" to getmininginfo, improve help in getblockchaininfo
- `99ddc6c` Add nLocalServices info to RPC getinfo
- `cf0c47b` Remove getwork() RPC call
- `2a72d45` prioritisetransaction <txid> <priority delta> <priority tx fee>
- `e44fea5` Add an option `-datacarrier` to allow users to disable relaying/mining data carrier transactions
- `2ec5a3d` Prevent easy RPC memory exhaustion attack
- `d4640d7` Added argument to getbalance to include watchonly addresses and fixed errors in balance calculation
- `83f3543` Added argument to listaccounts to include watchonly addresses
- `952877e` Showing 'involvesWatchonly' property for transactions returned by 'listtransactions' and 'listsinceblock'. It is only appended when the transaction involves a watchonly address
- `d7d5d23` Added argument to listtransactions and listsinceblock to include watchonly addresses
- `f87ba3d` added includeWatchonly argument to 'gettransaction' because it affects balance calculation
- `0fa2f88` added includedWatchonly argument to listreceivedbyaddress/...account
- `6c37f7f` `getrawchangeaddress`: fail when keypool exhausted and wallet locked
- `ff6a7af` getblocktemplate: longpolling support
- `c4a321f` Add peerid to getpeerinfo to allow correlation with the logs
- `1b4568c` Add vout to ListTransactions output
- `b33bd7a` Implement "getchaintips" RPC command to monitor blockchain forks
- `733177e` Remove size limit in RPC client, keep it in server
- `6b5b7cb` Categorize rpc help overview
- `6f2c26a` Closely track mempool byte total. Add "getmempoolinfo" RPC
- `aa82795` Add detailed network info to getnetworkinfo RPC
- `01094bd` Don't reveal whether password is <20 or >20 characters in RPC
- `57153d4` rpc: Compute number of confirmations of a block from block height
- `ff36cbe` getnetworkinfo: export local node's client sub-version string
- `d14d7de` SanitizeString: allow '(' and ')'
- `31d6390` Fixed setaccount accepting foreign address
- `b5ec5fe` update getnetworkinfo help with subversion
- `ad6e601` RPC additions after headers-first
- `33dfbf5` rpc: Fix leveldb iterator leak, and flush before `gettxoutsetinfo`
- `2aa6329` Enable customising node policy for datacarrier data size with a -datacarriersize option
- `f877aaa` submitblock: Use a temporary CValidationState to determine accurately the outcome of ProcessBlock
- `e69a587` submitblock: Support for returning specific rejection reasons
- `af82884` Add "warmup mode" for RPC server
- `e2655e0` Add unauthenticated HTTP REST interface to public blockchain data
- `683dc40` Disable SSLv3 (in favor of TLS) for the RPC client and server
- `44b4c0d` signrawtransaction: validate private key
- `9765a50` Implement BIP 23 Block Proposal
- `f9de17e` Add warning comment to getinfo

Command-line options:
- `ee21912` Use netmasks instead of wildcards for IP address matching
- `deb3572` Add `-rpcbind` option to allow binding RPC port on a specific interface
- `96b733e` Add `-version` option to get just the version
- `1569353` Add `-stopafterblockimport` option
- `77cbd46` Let -zapwallettxes recover transaction meta data
- `1c750db` remove -tor compatibility code (only allow -onion)
- `4aaa017` rework help messages for fee-related options
- `4278b1d` Clarify error message when invalid -rpcallowip
- `6b407e4` -datadir is now allowed in config files
- `bdd5b58` Add option `-sysperms` to disable 077 umask (create new files with system default umask)
- `cbe39a3` Add "bitcoin-tx" command line utility and supporting modules
- `dbca89b` Trigger -alertnotify if network is upgrading without you
- `ad96e7c` Make -reindex cope with out-of-order blocks
- `16d5194` Skip reindexed blocks individually
- `ec01243` --tracerpc option for regression tests
- `f654f00` Change -genproclimit default to 1
- `3c77714` Make -proxy set all network types, avoiding a connect leak
- `57be955` Remove -printblock, -printblocktree, and -printblockindex
- `ad3d208` remove -maxorphanblocks config parameter since it is no longer functional

Block and transaction handling:
- `7a0e84d` ProcessGetData(): abort if a block file is missing from disk
- `8c93bf4` LoadBlockIndexDB(): Require block db reindex if any `blk*.dat` files are missing
- `77339e5` Get rid of the static chainMostWork (optimization)
- `4e0eed8` Allow ActivateBestChain to release its lock on cs_main
- `18e7216` Push cs_mains down in ProcessBlock
- `fa126ef` Avoid undefined behavior using CFlatData in CScript serialization
- `7f3b4e9` Relax IsStandard rules for pay-to-script-hash transactions
- `c9a0918` Add a skiplist to the CBlockIndex structure
- `bc42503` Use unordered_map for CCoinsViewCache with salted hash (optimization)
- `d4d3fbd` Do not flush the cache after every block outside of IBD (optimization)
- `ad08d0b` Bugfix: make CCoinsViewMemPool support pruned entries in underlying cache
- `5734d4d` Only remove actualy failed blocks from setBlockIndexValid
- `d70bc52` Rework block processing benchmark code
- `714a3e6` Only keep setBlockIndexValid entries that are possible improvements
- `ea100c7` Reduce maximum coinscache size during verification (reduce memory usage)
- `4fad8e6` Reject transactions with excessive numbers of sigops
- `b0875eb` Allow BatchWrite to destroy its input, reducing copying (optimization)
- `92bb6f2` Bypass reloading blocks from disk (optimization)
- `2e28031` Perform CVerifyDB on pcoinsdbview instead of pcoinsTip (reduce memory usage)
- `ab15b2e` Avoid copying undo data (optimization)
- `341735e` Headers-first synchronization
- `afc32c5` Fix rebuild-chainstate feature and improve its performance
- `e11b2ce` Fix large reorgs
- `ed6d1a2` Keep information about all block files in memory
- `a48f2d6` Abstract context-dependent block checking from acceptance
- `7e615f5` Fixed mempool sync after sending a transaction
- `51ce901` Improve chainstate/blockindex disk writing policy
- `a206950` Introduce separate flushing modes
- `9ec75c5` Add a locking mechanism to IsInitialBlockDownload to ensure it never goes from false to true
- `868d041` Remove coinbase-dependant transactions during reorg
- `723d12c` Remove txn which are invalidated by coinbase maturity during reorg
- `0cb8763` Check against MANDATORY flags prior to accepting to mempool
- `8446262` Reject headers that build on an invalid parent
- `008138c` Bugfix: only track UTXO modification after lookup

P2P protocol and network code:
- `f80cffa` Do not trigger a DoS ban if SCRIPT_VERIFY_NULLDUMMY fails
- `c30329a` Add testnet DNS seed of Alex Kotenko
- `45a4baf` Add testnet DNS seed of Andreas Schildbach
- `f1920e8` Ping automatically every 2 minutes (unconditionally)
- `806fd19` Allocate receive buffers in on the fly
- `6ecf3ed` Display unknown commands received
- `aa81564` Track peers' available blocks
- `caf6150` Use async name resolving to improve net thread responsiveness
- `9f4da19` Use pong receive time rather than processing time
- `0127a9b` remove SOCKS4 support from core and GUI, use SOCKS5
- `40f5cb8` Send rejects and apply DoS scoring for errors in direct block validation
- `dc942e6` Introduce whitelisted peers
- `c994d2e` prevent SOCKET leak in BindListenPort()
- `a60120e` Add built-in seeds for .onion
- `60dc8e4` Allow -onlynet=onion to be used
- `3a56de7` addrman: Do not propagate obviously poor addresses onto the network
- `6050ab6` netbase: Make SOCKS5 negotiation interruptible
- `604ee2a` Remove tx from AlreadyAskedFor list once we receive it, not when we process it
- `efad808` Avoid reject message feedback loops
- `71697f9` Separate protocol versioning from clientversion
- `20a5f61` Don't relay alerts to peers before version negotiation
- `b4ee0bd` Introduce preferred download peers
- `845c86d` Do not use third party services for IP detection
- `12a49ca` Limit the number of new addressses to accumulate
- `35e408f` Regard connection failures as attempt for addrman
- `a3a7317` Introduce 10 minute block download timeout
- `3022e7d` Require sufficent priority for relay of free transactions
- `58fda4d` Update seed IPs, based on bitcoin.sipa.be crawler data
- `18021d0` Remove bitnodes.io from dnsseeds.

Validation:
- `6fd7ef2` Also switch the (unused) verification code to low-s instead of even-s
- `584a358` Do merkle root and txid duplicates check simultaneously
- `217a5c9` When transaction outputs exceed inputs, show the offending amounts so as to aid debugging
- `f74fc9b` Print input index when signature validation fails, to aid debugging
- `6fd59ee` script.h: set_vch() should shift a >32 bit value
- `d752ba8` Add SCRIPT_VERIFY_SIGPUSHONLY (BIP62 rule 2) (test only)
- `698c6ab` Add SCRIPT_VERIFY_MINIMALDATA (BIP62 rules 3 and 4) (test only)
- `ab9edbd` script: create sane error return codes for script validation and remove logging
- `219a147` script: check ScriptError values in script tests
- `0391423` Discourage NOPs reserved for soft-fork upgrades
- `98b135f` Make STRICTENC invalid pubkeys fail the script rather than the opcode
- `307f7d4` Report script evaluation failures in log and reject messages
- `ace39db` consensus: guard against openssl's new strict DER checks
- `12b7c44` Improve robustness of DER recoding code
- `76ce5c8` fail immediately on an empty signature

Build system:
- `f25e3ad` Fix build in OS X 10.9
- `65e8ba4` build: Switch to non-recursive make
- `460b32d` build: fix broken boost chrono check on some platforms
- `9ce0774` build: Fix windows configure when using --with-qt-libdir
- `ea96475` build: Add mention of --disable-wallet to bdb48 error messages
- `1dec09b` depends: add shared dependency builder
- `c101c76` build: Add --with-utils (bitcoin-cli and bitcoin-tx, default=yes). Help string consistency tweaks. Target sanity check fix
- `e432a5f` build: add option for reducing exports (v2)
- `6134b43` Fixing condition 'sabotaging' MSVC build
- `af0bd5e` osx: fix signing to make Gatekeeper happy (again)
- `a7d1f03` build: fix dynamic boost check when --with-boost= is used
- `d5fd094` build: fix qt test build when libprotobuf is in a non-standard path
- `2cf5f16` Add libbitcoinconsensus library
- `914868a` build: add a deterministic dmg signer 
- `2d375fe` depends: bump openssl to 1.0.1k
- `b7a4ecc` Build: Only check for boost when building code that requires it

Wallet:
- `b33d1f5` Use fee/priority estimates in wallet CreateTransaction
- `4b7b1bb` Sanity checks for estimates
- `c898846` Add support for watch-only addresses
- `d5087d1` Use script matching rather than destination matching for watch-only
- `d88af56` Fee fixes
- `a35b55b` Dont run full check every time we decrypt wallet
- `3a7c348` Fix make_change to not create half-satoshis
- `f606bb9` fix a possible memory leak in CWalletDB::Recover
- `870da77` fix possible memory leaks in CWallet::EncryptWallet
- `ccca27a` Watch-only fixes
- `9b1627d` [Wallet] Reduce minTxFee for transaction creation to 1000 satoshis
- `a53fd41` Deterministic signing
- `15ad0b5` Apply AreSane() checks to the fees from the network
- `11855c1` Enforce minRelayTxFee on wallet created tx and add a maxtxfee option

GUI:
- `c21c74b` osx: Fix missing dock menu with qt5
- `b90711c` Fix Transaction details shows wrong To:
- `516053c` Make links in 'About Bitcoin Core' clickable
- `bdc83e8` Ensure payment request network matches client network
- `65f78a1` Add GUI view of peer information
- `06a91d9` VerifyDB progress reporting
- `fe6bff2` Add BerkeleyDB version info to RPCConsole
- `b917555` PeerTableModel: Fix potential deadlock. #4296
- `dff0e3b` Improve rpc console history behavior
- `95a9383` Remove CENT-fee-rule from coin control completely
- `56b07d2` Allow setting listen via GUI
- `d95ba75` Log messages with type>QtDebugMsg as non-debug
- `8969828` New status bar Unit Display Control and related changes
- `674c070` seed OpenSSL PNRG with Windows event data
- `509f926` Payment request parsing on startup now only changes network if a valid network name is specified
- `acd432b` Prevent balloon-spam after rescan
- `7007402` Implement SI-style (thin space) thoudands separator
- `91cce17` Use fixed-point arithmetic in amount spinbox
- `bdba2dd` Remove an obscure option no-one cares about
- `bd0aa10` Replace the temporary file hack currently used to change Bitcoin-Qt's dock icon (OS X) with a buffer-based solution
- `94e1b9e` Re-work overviewpage UI
- `8bfdc9a` Better looking trayicon
- `b197bf3` disable tray interactions when client model set to 0
- `1c5f0af` Add column Watch-only to transactions list
- `21f139b` Fix tablet crash. closes #4854
- `e84843c` Broken addresses on command line no longer trigger testnet
- `a49f11d` Change splash screen to normal window
- `1f9be98` Disable App Nap on OSX 10.9+
- `27c3e91` Add proxy to options overridden if necessary
- `4bd1185` Allow "emergency" shutdown during startup
- `d52f072` Don't show wallet options in the preferences menu when running with -disablewallet
- `6093aa1` Qt: QProgressBar CPU-Issue workaround
- `0ed9675` [Wallet] Add global boolean whether to send free transactions (default=true)
- `ed3e5e4` [Wallet] Add global boolean whether to pay at least the custom fee (default=true)
- `e7876b2` [Wallet] Prevent user from paying a non-sense fee
- `c1c9d5b` Add Smartfee to GUI
- `e0a25c5` Make askpassphrase dialog behave more sanely
- `94b362d` On close of splashscreen interrupt verifyDB
- `b790d13` English translation update
- `8543b0d` Correct tooltip on address book page

Tests:
- `b41e594` Fix script test handling of empty scripts
- `d3a33fc` Test CHECKMULTISIG with m == 0 and n == 0
- `29c1749` Let tx (in)valid tests use any SCRIPT_VERIFY flag
- `6380180` Add rejection of non-null CHECKMULTISIG dummy values
- `21bf3d2` Add tests for BoostAsioToCNetAddr
- `b5ad5e7` Add Python test for -rpcbind and -rpcallowip
- `9ec0306` Add CODESEPARATOR/FindAndDelete() tests
- `75ebced` Added many rpc wallet tests
- `0193fb8` Allow multiple regression tests to run at once
- `92a6220` Hook up sanity checks
- `3820e01` Extend and move all crypto tests to crypto_tests.cpp
- `3f9a019` added list/get received by address/ account tests
- `a90689f` Remove timing-based signature cache unit test
- `236982c` Add skiplist unit tests
- `f4b00be` Add CChain::GetLocator() unit test
- `b45a6e8` Add test for getblocktemplate longpolling
- `cdf305e` Set -discover=0 in regtest framework
- `ed02282` additional test for OP_SIZE in script_valid.json
- `0072d98` script tests: BOOLAND, BOOLOR decode to integer
- `833ff16` script tests: values that overflow to 0 are true
- `4cac5db` script tests: value with trailing 0x00 is true
- `89101c6` script test: test case for 5-byte bools
- `d2d9dc0` script tests: add tests for CHECKMULTISIG limits
- `d789386` Add "it works" test for bitcoin-tx
- `df4d61e` Add bitcoin-tx tests
- `aa41ac2` Test IsPushOnly() with invalid push
- `6022b5d` Make `script_{valid,invalid}.json` validation flags configurable
- `8138cbe` Add automatic script test generation, and actual checksig tests
- `ed27e53` Add coins_tests with a large randomized CCoinViewCache test
- `9df9cf5` Make SCRIPT_VERIFY_STRICTENC compatible with BIP62
- `dcb9846` Extend getchaintips RPC test
- `554147a` Ensure MINIMALDATA invalid tests can only fail one way
- `dfeec18` Test every numeric-accepting opcode for correct handling of the numeric minimal encoding rule
- `2b62e17` Clearly separate PUSHDATA and numeric argument MINIMALDATA tests
- `16d78bd` Add valid invert of invalid every numeric opcode tests
- `f635269` tests: enable alertnotify test for Windows
- `7a41614` tests: allow rpc-tests to get filenames for bitcoind and bitcoin-cli from the environment
- `5122ea7` tests: fix forknotify.py on windows
- `fa7f8cd` tests: remove old pull-tester scripts
- `7667850` tests: replace the old (unused since Travis) tests with new rpc test scripts
- `f4e0aef` Do signature-s negation inside the tests
- `1837987` Optimize -regtest setgenerate block generation
- `2db4c8a` Fix node ranges in the test framework
- `a8b2ce5` regression test only setmocktime RPC call
- `daf03e7` RPC tests: create initial chain with specific timestamps
- `8656dbb` Port/fix txnmall.sh regression test
- `ca81587` Test the exact order of CHECKMULTISIG sig/pubkey evaluation
- `7357893` Prioritize and display -testsafemode status in UI
- `f321d6b` Add key generation/verification to ECC sanity check
- `132ea9b` miner_tests: Disable checkpoints so they don't fail the subsidy-change test
- `bc6cb41` QA RPC tests: Add tests block block proposals
- `f67a9ce` Use deterministically generated script tests
- `11d7a7d` [RPC] add rpc-test for http keep-alive (persistent connections)
- `34318d7` RPC-test based on invalidateblock for mempool coinbase spends
- `76ec867` Use actually valid transactions for script tests
- `c8589bf` Add actual signature tests
- `e2677d7` Fix smartfees test for change to relay policy
- `263b65e` tests: run sanity checks in tests too

Miscellaneous:
- `122549f` Fix incorrect checkpoint data for testnet3
- `5bd02cf` Log used config file to debug.log on startup
- `68ba85f` Updated Debian example bitcoin.conf with config from wiki + removed some cruft and updated comments
- `e5ee8f0` Remove -beta suffix
- `38405ac` Add comment regarding experimental-use service bits
- `be873f6` Issue warning if collecting RandSeed data failed
- `8ae973c` Allocate more space if necessary in RandSeedAddPerfMon
- `675bcd5` Correct comment for 15-of-15 p2sh script size
- `fda3fed` libsecp256k1 integration
- `2e36866` Show nodeid instead of addresses in log (for anonymity) unless otherwise requested
- `cd01a5e` Enable paranoid corruption checks in LevelDB >= 1.16
- `9365937` Add comment about never updating nTimeOffset past 199 samples
- `403c1bf` contrib: remove getwork-based pyminer (as getwork API call has been removed)
- `0c3e101` contrib: Added systemd .service file in order to help distributions integrate bitcoind
- `0a0878d` doc: Add new DNSseed policy
- `2887bff` Update coding style and add .clang-format
- `5cbda4f` Changed LevelDB cursors to use scoped pointers to ensure destruction when going out of scope
- `b4a72a7` contrib/linearize: split output files based on new-timestamp-year or max-file-size
- `e982b57` Use explicit fflush() instead of setvbuf()
- `234bfbf` contrib: Add init scripts and docs for Upstart and OpenRC
- `01c2807` Add warning about the merkle-tree algorithm duplicate txid flaw
- `d6712db` Also create pid file in non-daemon mode
- `772ab0e` contrib: use batched JSON-RPC in linarize-hashes (optimization)
- `7ab4358` Update bash-completion for v0.10
- `6e6a36c` contrib: show pull # in prompt for github-merge script
- `5b9f842` Upgrade leveldb to 1.18, make chainstate databases compatible between ARM and x86 (issue #2293)
- `4e7c219` Catch UTXO set read errors and shutdown
- `867c600` Catch LevelDB errors during flush
- `06ca065` Fix CScriptID(const CScript& in) in empty script case

Credits
=======

Thanks to everyone who contributed to this release:

- 21E14
- Adam Weiss
- Aitor Pazos
- Alexander Jeng
- Alex Morcos
- Alon Muroch
- Andreas Schildbach
- Andrew Poelstra
- Andy Alness
- Ashley Holman
- Benedict Chan
- Ben Holden-Crowther
- Bryan Bishop
- BtcDrak
- Christian von Roques
- Clinton Christian
- Cory Fields
- Cozz Lovan
- daniel
- Daniel Kraft
- David Hill
- Derek701
- dexX7
- dllud
- Dominyk Tiller
- Doug
- elichai
- elkingtowa
- ENikS
- Eric Shaw
- Federico Bond
- Francis GASCHET
- Gavin Andresen
- Giuseppe Mazzotta
- Glenn Willen
- Gregory Maxwell
- gubatron
- HarryWu
- himynameismartin
- Huang Le
- Ian Carroll
- imharrywu
- Jameson Lopp
- Janusz Lenar
- JaSK
- Jeff Garzik
- JL2035
- Johnathan Corgan
- Jonas Schnelli
- jtimon
- Julian Haight
- Kamil Domanski
- kazcw
- kevin
- kiwigb
- Kosta Zertsekel
- LongShao007
- Luke Dashjr
- Mark Friedenbach
- Mathy Vanvoorden
- Matt Corallo
- Matthew Bogosian
- Micha
- Michael Ford
- Mike Hearn
- mrbandrews
- mruddy
- ntrgn
- Otto Allmendinger
- paveljanik
- Pavel Vasin
- Peter Todd
- phantomcircuit
- Philip Kaufmann
- Pieter Wuille
- pryds
- randy-waterhouse
- R E Broadley
- Rose Toomey
- Ross Nicoll
- Roy Badami
- Ruben Dario Ponticelli
- Rune K. Svendsen
- Ryan X. Charles
- Saivann
- sandakersmann
- SergioDemianLerner
- shshshsh
- sinetek
- Stuart Cardall
- Suhas Daftuar
- Tawanda Kembo
- Teran McKinney
- tm314159
- Tom Harding
- Trevin Hofmann
- Whit J
- Wladimir J. van der Laan
- Yoichi Hirai
- Zak Wilcox

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).
Also lots of thanks to the bitcoin.org website team David A. Harding and Saivann Carignan.

Wladimir


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Note how transaction malleability can quickly sabotage naive notions of this idea.

Equally, if this looks like it might ever be implemented, rather than using a hard fork, using a forced soft-fork to deploy changes becomes attractive.


On 30 December 2015 12:08:36 GMT-08:00, "Emin Gün Sirer via bitcoin-dev" <bitcoin-dev@lists.linuxfoundation.org> wrote:

- --
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWhDuA
AAoJEMCF8hzn9Lncz4MIAIObFNbRRJ5g52H8yprqAjX76Lt7vw+cwCnICNzHra5h
iuTWxgbwED5fki2Q96ZzYAyUf7ju7rI45qBl8YuuVUlyxJgE6oV6h2oJoxGQNGz0
WvrOjWMkmARNs0FM4GMsKQWcmIMgZxWnWTMOXv0EDBLySsm8WFRu9H4drGBB+Fmb
wFRyi0XVDiXxsVUoNj6pCdcpekdnuq+V87IoweoxigfqgWIM31Vb9QK8Y/7vWO2b
0lu0CvVdqvw5Npx55LWLF1tY8jbw6BYvgXwZGtUazKO+x8i3Qt6+tRm07+UXvkoR
3erxzhnoZa3F66ufz+ImY7l0E/AyRE5ox+1W68hO6sk=
=d0+L
-----END PGP SIGNATURE-----


-------------------------------------
A recent post, which I cannot find after much effort, made an excellent
point.

If capacity grows, fewer individuals would be able to run full nodes. 
Those individuals, like many already, would have to give up running a
full-node wallet :(

That sounds bad, until you consider that the alternative is running a
full node on the bitcoin 'settlement network', while massive numbers of
people *give up any hope of directly owning bitcoin at all*.

If today's global payments are 100Ktps, and move to the Lightning
Network, they will have to be consolidated by a factor of 25000:1 to fit
into bitcoin's current 4tps capacity as a settlement network.  You
executing a personal transaction on that network will be about as likely
as you personally conducting a $100 SWIFT transfer to yourself today. 
For current holders, just selling or spending will get very expensive!

Forcing block capacity to stay small, so that individuals can run full
nodes, is precisely what will force bitcoin to become a backbone that is
too expensive for individuals to use.  I can't avoid the conclusion that
Bitcoin has to scale, and we might as well be thinking about how.

There may be a an escape window.  As current trends continue toward a
landscape of billions of SPV wallets, it may still be possible for
individuals collectively to make up the majority of the network, if more
parts of the network itself rely on SPV-level security.

With SPV-level security, it might be possible to implement a scalable
DHT-type network of nodes that collectively store and index the
exhaustive and fast-growing corpus of transaction history, up to and
including currently unconfirmed transactions.  Each individual node
could host a slice of the transaction set with a configurable size,
let's say down to a few GB today.

Such a network would have the desirable property of being run by the
community.  Most transactions would be submitted to it, and like today's
network, it would disseminate blocks (which would be rapidly torn apart
and digested).  Therefore miners and other full nodes would depend on
it, which is rather critical as those nodes grow closer to data-center
proportions.




-------------------------------------


So your concern is just about the ordering and process of things, and not
about the change itself?

I have witnessed many arguments in IRC about block sizes over the years.
There was another one just a few weeks ago. Pieter left the channel for his
own sanity. IRC is not a good medium for arriving at decisions on things -
many people can't afford to sit on IRC all day and conversations can be
hard to follow. Additionally, they tend to go circular.

That said, I don't know if you can draw a line between the "ins" and "outs"
like that. The general public is watching, commenting and deciding no
matter what. Might as well deal with that and debate in a format more
accessible to all.




There have been many such discussions over time. On bitcointalk. On reddit.
On IRC. At developer conferences. Gavin already knew what many of the
objections would be, which is why he started answering them.

But alright. Let's say he should have started a thread. Thanks for starting
it for him.

Now, can we get this specific list of things we should do before we're
prepared?




Do you have a specific research suggestion? Gavin has run simulations
across the internet with modified full nodes that use 20mb blocks, using
real data from the block chain. They seem to suggest it works OK.

What software do you have in mind?
-------------------------------------
On Sun, Jan 25, 2015 at 6:48 AM, Gregory Maxwell <gmaxwell@gmail.com> wrote:

I would like to fix this at some point in any case.

If we want to do that, we must at least have signatures with too-long
R or S values as non-standard.

One way to do that is to just - right now - add a patch to 0.10 to
make those non-standard. This requires another validation flag, with a
bunch of switching logic.

The much simpler alternative is just adding this to BIP66's DERSIG
right now, which is a one-line change that's obviously softforking. Is
anyone opposed to doing so at this stage?

-- 
Pieter


-------------------------------------
On Sat, May 9, 2015 at 4:36 AM, Gregory Maxwell <gmaxwell@gmail.com> wrote:



This could be implemented as a soft fork too.

* 1MB hard size limit
* 900kB soft limit

S = block size
U = UTXO_adjusted_size = S + 4 * outputs - 3 * inputs

A block is valid if S < 1MB and U < 1MB

A 250 byte transaction with 2 inputs and 2 outputs would have an adjusted
size of 252 bytes.

The memory pool could be sorted by fee per adjusted_size.

 Coin selection could be adjusted so it tries to have at least 2 inputs
when creating transactions, unless the input is worth more than a threshold
(say 0.001 BTC).

This is a pretty weak incentive, especially if the block size is
increased.  Maybe it will cause a "nudge"
-------------------------------------
<p dir="ltr">Technical merit only has value when it advances one or more goals, as in a vision statement.  Technical merit without direction is efficient chaos.<br>
 </p>
<div class="gmail_quote">On 24 Jun 2015 8:42 pm, Gareth Williams &lt;gacrux@gmail.com&gt; wrote:<br type='attribution'><blockquote class="quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">



<div>
<div>On Thu, Jun 25, 2015 at 10:07 AM, Milly Bitcoin &lt;milly&#64;bitcoins.info&gt;<br />
wrote:<br />
&lt;snip&gt;<br />
&gt; Also, the incentive for new<br />
&gt; developers to come in is that they will be paid by companies who want to<br />
&gt; influence the code and this should be considered<br />
&lt;snip&gt;<br />
&gt; Now you are left with a broken, unwritten/unspoken process.<br />
<br />
Your former statement is a great example of why &#34;rough consensus and<br />
running code&#34; is superior to design by committee.<br />
An argument should be assessed on its technical merit alone, not on<br />
the number of people advancing it -- a process that would be open to<br />
exactly the type of external manipulation you say you are concerned<br />
about.<br />
_______________________________________________<br />
bitcoin-dev mailing list<br />
bitcoin-dev&#64;lists.linuxfoundation.org<br />
<a href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a><br />
</div>
</div>

</blockquote></div>
-------------------------------------
Copyright doesn't care how notices are written. They are merely informative 
to humans reading them. Anyhow, this is not development related, so please 
direct any further discussion of it to me directly (with any applicable CCs) 
and NOT to the mailing list.

Thanks,

Luke

On Tuesday, October 06, 2015 5:49:40 AM Milly Bitcoin via bitcoin-dev wrote:

-------------------------------------
Den 22 feb 2015 13:36 skrev "Peter Todd" <pete@petertodd.org>:

Somebody sent me a zero-confirmation transaction, or one that got orphaned
after one block. I created a transaction spending that UTXO, and another.

So at that point I have UTXO_orphaned based on the sender's UTXO_origin and
my UTXO_old (because I've had it unspent for a long time), both in one
transaction, creating UTXO_new.

Now he doublespend UTXO_origin to create a UTXO_doublespend (which
conflicts with UTXO_orphaned). He conspires with a miner to get it into a
block.

Now what? Can my UTXO_old effectively be tainted forever because UTXO_new
got invalidated together with UTXO_orphaned? Will that transaction be a
valid proof of doublespend against a new UTXO_replacement I created?

Or otherwise, if only transactions where all UTXO's are currently valid
works as doublespend proofs, aren't you still just left without protection
against any one miner that conspires with doublespend attempting thieves?

In other words, you are unprotected and potentially at greater risk if you
create a transaction depending on another zero-confirmation transaction.
-------------------------------------




You mean the bunch of self-proclaimed Bitcoin wizards, who decided they have the right to tell everybody what to do, and who never got to grow up and are now angry at the world for not listening to them anymore? That "technical consensus"?

"Bitcoin is decentralized, but you are only allowed to do what we tell you to do. It's our pet project, we wrote code for it!"

That's what it all boils down to, all these dirty games of calling XT an alt-coin and censoring its posts, pretending to be Satoshi, sabotaging XT switch, etc.: "How dare they not listen to Us The Smartest anymore?!!!"

Pathetic. The history will roll over you in a blink. The harder you try, the quicker it will go.


-------------------------------------
I'm mainly just an observer on this. I mostly agree with Pieter. Also, I
think the main reason why people like Gavin and Mike Hearn are trying to
rush this through is because they have some kind of "apps" that depend on
zero conf instant transactions, so this would of course require more
traffic on the blockchain. I think people like Gavin or Mike should state
clearly what kind of (rigorous) system for instant transactions is
satisfactory for use in their applications. Be it lightning or something
similar, what is good enough? And no zero conf is not a real secure system.
Then once we know what is good enough for them (and everyone else), we can
implement it as a soft fork into the protocol, and it's a win win situation
for both sides (we can also benefit from all the new users people like Mike
are trying bring in).

On Thu, May 7, 2015 at 10:52 AM, Jorge Timón <jtimon@jtimon.cc> wrote:




-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
-------------------------------------
Although not perfect, and it may require visual/verbal verification, I
don't see what the trust issue is.


[image: logo]
*Paul Puey* CEO / Co-Founder, Airbitz Inc
+1-619-850-8624 | http://airbitz.co | San Diego
<http://facebook.com/airbitz>  <http://twitter.com/airbitz>
<https://plus.google.com/118173667510609425617>
<https://go.airbitz.co/comments/feed/>  <http://linkedin.com/in/paulpuey>
<https://angel.co/paul-puey>
*DOWNLOAD THE AIRBITZ WALLET:*
  <https://play.google.com/store/apps/details?id=com.airbitz>
<https://itunes.apple.com/us/app/airbitz/id843536046>




On Thu, Feb 5, 2015 at 2:05 PM, Eric Voskuil <eric@voskuil.org> wrote:

-------------------------------------


On December 3, 2015 7:02:20 AM GMT+08:00, Peter Tschipper <peter.tschipper@gmail.com> wrote:

Ok. It wasn't clear to me that you weren't also claiming at latency reduction as a result. In any case, the point I was making is that the p2p protocol isn't for every use-case. Indeed, I agree (as noted previously) that we should support people who have very restrictive data usage limits, but I don't think we need to do this in the p2p protocol. Considering we're in desperate need of more ways to sync, supporting syncing over slow and/or very restrictive connections is something maybe better addressed by a sync-over-http-via-cdn protocol than the p2p protocol.


My point is that, with limited further optimization, and especially after the first hundred thousand blocks, block download should nearly never be the thing limiting IBD speed.


No matter how easily you can implement something, complexity always has cost. This is especially true in complicated, incredibly security critical applications exposed to the internet.



-------------------------------------
Hello all, I wanted to add another analogy here to this block size
debate, in case helpful. I understand some may not see it this way, so
apologies in advance if it ruffles anyone's feathers. In some ways,
however, to me at least - Bitcoin is like Windows 3.11. Before Bitcoin
everything was DOS - something completely disruptive and good for
society has come into the computing space that exponentially improves
upon almost everything in the space that existed before it. Now there is
a huge debate about if there should ever be a Windows 95, XP, Pro, etc.,
that scales better and makes advances over time, but doesn’t support
facets of older versions as it gets updated.  What will happen to 3.11
users/developers/etc. who don't upgrade that have money and/or important
tech tied into the 3.11 platform? Should it just be Windows 3.11 forever
except with better programs that continue to be built to run on it? Or,
should we agree to only change it if 100% of Windows users or Windows
developers agree on upgrading?

Regardless of what side we all stand on, I just want to point out that
this mailing list is full of incredibly brilliant minds leading the
charge into perhaps one of the greatest technical achievements in recent
decades. Maybe it would be a good idea for each side of the issue here
to democratically appoint a developer representative, and then allow the
representatives to achieve a framework and hammer out the details of the
solution together?

Hope you all have nice weekends,
Will

-- 
// will binns
// gpg fingerprint: 4519 7EB7 66A7 CC5E 4E66 F200 AF5C 2D1C E58E B37C
// threema id: 5YM2J894

-------------------------------------
On Thu, Feb 12, 2015 at 08:15:01PM +0100, Alan Reiner wrote:

Speaking of, a relatively simple thing that would help dispel these
notions would be if some wallets supported replace-by-fee-using
fee-bumping and an "attempt undo" button. Armory is an (unfortunately!)
special case because it uses a full node and has good privacy
guarantees, but most wallets could implement this by just sending the
doublespend transactions to any node advertising either the
replace-by-fee or GETUTXO's service bits.

1) https://www.schneier.com/blog/archives/2009/09/the_doghouse_cr.html

-- 
'peter'[:-1]@petertodd.org
00000000000000000a1fb2fd17f5d8735a8a0e7aae841c95a12e82b934c4ac92
-------------------------------------
On 07/04/2015 12:44 AM, Peter Todd wrote:

In general, the situation can be improved if there existed proofs which
validating full nodes could broadcast which would tell SPV nodes why the
branch it sees with the most proof of work is actually invalid.

As far as I can tell, producing such proofs is reasonably
straightforward for all cases except the case where a block is invalid
because it contains a transaction which references a non-existent output.

The shortest proof that a particular transaction does not exist in the
blockchain is the entire blockchain.

If each transaction input identified the block containing the referenced
outpoint, then the proof of non-existence is either the block in
question, or the list of block headers (to show that the block doesn't
exist). That's a significant improvement in proof size over the entire
blockchain.

Proving the non-existence of a particular transaction in a specific
block could be made easier for future blocks by requiring transactions
to be ordered in the merkle tree by their hashes.  Then it would just
require a few nodes in the tree to show that the transaction isn't in
the place where it should be.

-- 
Justus Ranvier
Open Bitcoin Privacy Project
http://www.openbitcoinprivacyproject.org/
justus@openbitcoinprivacyproject.org
E7AD 8215 8497 3673 6D9E 61C4 2A5F DA70 EAD9 E623
-------------------------------------
Maybe I'm overlooking something, but I've been watching this thread with increasing skepticism at the complexity of the offered solution. I don't understand why it needs to be so complex. I'd like to offer an alternative for your consideration...

Challenge:
"Send me: SHA256(SHA256(concatenation of N pseudo-randomly selected bytes from the block chain))."

Choose N such that it would be infeasible for the responding node to fetch all of the needed blocks in a short amount of time. In other words, assume that a node can seek to a given byte in a block stored on local disk much faster than it can download the entire block from a remote peer. This is almost certainly a safe assumption.

For example, choose N = 1024. Then the proving node needs to perform 1024 random reads from local disk. On spinning media, this is likely to take somewhere on the order of 15 seconds. Assuming blocks are averaging 500 KiB each, then 1024 blocks would comprise 500 MiB of data. Can 500 MiB be downloaded in 15 seconds? This data transfer rate is 280 Mbps. Almost certainly not possible. And if it is, just increase N. The challenge also becomes more difficult as average block size increases.

This challenge-response protocol relies on the lack of a "partial getdata" command in the Bitcoin protocol: a node cannot ask for only part of a block; it must ask for an entire block. Furthermore, nodes could ban other nodes for making too many random requests for blocks.


On Thursday, 26 March 2015, at 7:09 pm, Sergio Lerner wrote:



-------------------------------------
On 02/02/15 13:38, Andreas Schildbach wrote:

Do I understand this correctly and h=bip32 hierarchy means that both

xpub/0/i and xpub/1/j chains are scanned? (So it applies to BIP44
generated xpubs as well?)


Uff, I would expect YYYYMMDD there so it's human readable as well.

-- 
Best Regards / S pozdravom,

Pavol Rusnak <stick@gk2.sk>


-------------------------------------
On 02/03/2015 11:10 AM, Pavol Rusnak wrote:


Not really IMHO. Keys can be used on multiple blockchains.



-------------------------------------
On Mon, Jun 15, 2015 at 5:08 PM, Aaron Voisine <voisine@gmail.com> wrote:


We are not reaching consensus about any proposal, Garzik's or otherwise.
-------------------------------------
On Thu, May 7, 2015 at 3:05 PM, Mike Hearn <mike@plan99.net> wrote:

You are conflating consensus with commit access. People with commit access
are maintainers who are *able to merge* pull requests. However, the rules
for bitcoin development are that only patches with consensus get merged. If
any of the maintainers just pushed a change without going through the whole
code review and consensus process there would be uproar, plain and simple.

Please don't conflate commit access with permission to merge because it's
just not the case. No-one can sidestep the requirement to get consensus,
not even the 5 maintainers.
-------------------------------------
On Fri, Jun 26, 2015 at 5:22 PM, Milly Bitcoin <milly@bitcoins.info> wrote:


I think you just proved my point by saying "when needed".

-- 
Pieter
-------------------------------------
 I am disappointed that you did not understand my point of view. Let me rephrase it for you,
People tipping, buying 0.99$ products and gamblers that need Bitcoin transactions *more* than the rest of the people will afford the fees that establish the equilibrium between demand and supply of Bitcoin transactions. The people are free to use they money for whatever they like, but you should understand that Bitcoin transactions are not free.
I was merely attempting to point out that spammers and gamblers would be the first ones that would go away. They would be free to spam or gamble, but they would have to pay for it.
When a category of users would get priced out because of the fee market, they would be free to use any altcoin they want.

Please understand that not everyone will leave. The more important players will remain, those that need it the most. The other players are free to use whatever altcoin they wish.

     În Miercuri, 29 Iulie 2015 16:47:57, Angel Leon <gubatron@gmail.com> a scris:
   

 "the gamblers and perhaps people transacting very low amounts. The people that actually need Bitcoin would remain."

so people tipping, buying $0.99 products, and gamblers actually don't need Bitcoin.
Who are you to say what people need to use money for?This statement goes against the freedom of decentralization and financial freedom Bitcoin should be able to provide.
It's an open network and it will be used as most users see fit, and that requires a blocksize increase wether you like it or not, it's simple physics, other time wait times will become unbearable for those not willing to pay the high fees, if people leave, then it only mean bitcoins isn't useful, and if bitcoin isn't useful, it's worthless.



http://twitter.com/gubatron

On Wed, Jul 29, 2015 at 9:27 AM, Vali Zero via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

Hello,

I have been reading an argument saying that paying higher fees would scare Bitcoin users and they would stop using it, preferring bank transfers or other payment methods. This does not make sense for me. If some users leave, then demand for bitcoin transactions goes down and so do the fees. The others remain.

Fee market means that an equilibrium is found between the demand for bitcoin transactions and the available supply (given by the block size). The fee is the price that finds this equilibrium.

If a fee market starts to exist, the first ones to leave are the spammers, probably followed by the gamblers and perhaps people transacting very low amounts. The people that actually need Bitcoin would remain.

Please allow this fee market to form...

In the absence of a functioning fee market, I will refuse to run Bitcoin code that increases the block size and will do my best to tell everyone I know not to upgrade towards running such code. If Bitcoin succombs to the free stuff army, I will sell all the coins and leave. Nothing is for free.

I apologize for any exagerations, but I just felt strongly towards expressing my opinion here. I'm only a local Bitcoin trader, computer engineer, with a reasonable understanding of free markets. And I'm running only one full node.

Kind regards,
Valentin


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev





  
-------------------------------------
On Wednesday, October 21, 2015 8:44:53 AM Christian Decker wrote:

Or he can just have the other signers re-sign with the modified version.
Even if it only worked with a single signer, it's still a form of malleability 
that your BIP does not presently solve, but would be desirable to solve...

Luke

-------------------------------------
There is *no* consensus on using a soft fork to deploy this feature. It
will result in the same problems as all the other soft forks - SPV wallets
will become less reliable during the rollout period. I am against that, as
it's entirely avoidable.

Make it a hard fork and my objection will be dropped.

Until then, as there is no consensus, you need to do one of two things:

1) Drop the "everyone must agree to make changes" idea that people here
like to peddle, and do it loudly, so everyone in the community is correctly
informed

2) Do nothing
-------------------------------------
That is a very good point.

We considered whether data existing before a licence change would be
covered, but we hadn't factored the potential need for gaining permissions
for a change to be considered effective.

We have proposed that miners be the main beneficiaries of licensing and
there is a consideration on whether they should vote to adopt the new
terms. While not the preferred route, that would overcome any issues to
what is an otherwise honest 'error and omission.' There doesn't seem to be
anyone who could claim to have suffered any economic losses so this may not
be an issue. It merits further investigation.

The block chain is in perpetual change, so the sooner a change is agreed
upon, if at all, the more data it will cover without any reservations. At
any rate, we believe the changes would be considered effective on a
retrospective basis.


On Tue, Sep 1, 2015 at 7:12 PM, Btc Drak <btcdrak@gmail.com> wrote:

-------------------------------------
Threat models can be developed for things like threats from governments. 
  The idea in developing a model is to put in the context of other 
possible threats.  For example, someone with a few million to burn can 
easily crash the exchange rate or buy a couple core developers much 
easier and cheaper than doing a 51% attack.  These attacks can be done 
by governments and non-governments alike.  The people who consider 
threats from government and think everyone associated with Bitcoin is 
somehow "pure" are irrational cultists who have no business discussing 
threat models in the first place.

Russ



On 9/20/2015 5:10 PM, NxtChg via bitcoin-dev wrote:



-------------------------------------
On 04/01/15 17:44, Gregory Maxwell wrote:

Ah, thanks for that.

I'll try Peter's patch for testnet tomorrow, sounds like it should fix
this for my use case.



-------------------------------------
On Wed, May 13, 2015 at 11:04 AM, Christian Decker <
decker.christian@gmail.com> wrote:


Sufficient confirmations help of course, but make systems like this less
useful for more complex interactions where you have multiple unconfirmed
transactions waiting on each other. I think being able to rely on this
problem being solved unconditionally is what makes the proposal attractive.
For the simple cases, see BIP62.

I remember reading about the SIGHASH proposal somewhere. It feels really

I think you misunderstand the idea. This is related, but orthogonal to the
ideas about extended the sighash flags that have been discussed here before.

All it's doing is adding a new CHECKSIG operator to script, which, in its
internally used signature hash, 1) removes the scriptSigs from transactions
before hashing 2) replaces the txids in txins by their ntxid. It does not
add any data to transactions, and it is a softfork, because it only impacts
scripts which actually use the new CHECKSIG operator. Wallets that don't
support signing with this new operator would not give out addresses that
use it.


OP_*SIG* semantics don't change here either, we're just adding a superior
opcode (which in most ways behaves the same as the existing operators). I
agree with the advantage of not needing to monitor transactions afterwards
for malleated inputs, but I think you underestimate the deployment costs.
If you want to upgrade the world (eventually, after the old index is
dropped, which is IMHO the only point where this proposal becomes superior
to the alternatives) to this, you're changing *every single piece of
Bitcoin software on the planet*. This is not just changing some validation
rules that are opt-in to use, you're fundamentally changing how
transactions refer to each other.

Also, what do blocks commit to? Do you keep using the old transaction ids
for this? Because if you don't, any relayer on the network can invalidate a
block (and have the receiver mark it as invalid) by changing the txids. You
need to somehow commit to the scriptSig data in blocks still so the POW of
a block is invalidated by changing a scriptSig.

There certainly are merits using the SIGHASH approach in the short term (it

It requires a hard fork, but more importantly, it requires the whole world
to change their software (not just validation code) to effectively use it.
That, plus large up-front deployment costs (doubling the cache size for
every full node for the same propagation speed is not a small thing) which
may not end up being effective.

-- 
Pieter
-------------------------------------

My first internal version triggered on block 406,800 (~May 5), and each
block increased by 20 bytes thereafter.

It was changed to time, because time was the standard used in years past
for other changes; MTP flag day is more stable than block height.

It is preferred to have a single flag trigger (height or time), rather than
the more complex trigger-on-time, increment-on-height, but any combination
of those will work.

Easy to change code back to height-based...



On Fri, Dec 18, 2015 at 2:52 PM, Jorge Timón <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Aaron,

My understanding is that Gavin and Mike are proceeding with the XT fork, I
hope that understanding is wrong.

As for improving the non-consensus code to handle full blocks more
gracefully.  This is something I'm very interested in, block size increase
or not. Perhaps I shouldn't hijack this thread, but maybe there are others
who also believe this would ameliorate some of the time pressure for
deciding on a block size increase.

What is it that you would like to see improved?
The fee estimation code that is included for 0.11 will give much more
accurate fee estimates, which should allow adding the correct fee to a
transaction to see it likely to be confirmed in a reasonable time.  For
further improvements:
- There has recently been attention to overhauling the block creation and
mempool limiting code in such a way that actual outstanding queues to be
included in a block could also be incorporated in fee estimation.  See
https://github.com/bitcoin/bitcoin/pull/6281.
- CPFP and RBF are candidates for inclusion in core soon, both of which
could be integrated into transaction processing to handle the edge cases
where a priori fee estimation fails. See
https://github.com/bitcoin/bitcoin/pull/1647 and
https://github.com/bitcoin/bitcoin/pull/6176

I know there has been much discussion of fee estimation not working for SPV
clients, but I believe several independent servers which were serving the
estimates from full nodes would go a long way towards allowing that
information to be used by SPV clients even if its not a completely
decentralized solution.  See for example
http://core2.bitcoincore.org/smartfee/latest.json



On Mon, Jun 15, 2015 at 8:08 PM, Aaron Voisine <voisine@gmail.com> wrote:

-------------------------------------
On Tue, Jun 16, 2015 at 9:33 PM, Peter Todd <pete@petertodd.org> wrote:


Thank you very much Peter for pointing this out! That is very kind of you.

It would be great to work with Constance Choi, Primavera De Filippi, your
goodself and others to make this happen.

As you may know, the Hong Kong Monetary Authority considers bitcoin a
virtual 'commodity' and not a currency per se.

Regards,

p.


-------------------------------------
On Dec 26, 2015 23:55, "Jonathan Toomim" <j@toom.im> wrote:
bitcoin-dev@lists.linuxfoundation.org> wrote:
guarantees that old nodes will still see a 25% forked off chain temporarily.
4000 to 8000 blocks (1 to 2 months).

I think that's extremely short, even assuming there is no controversy about
changing the rules at all. Things like BIP65 and BIP66 already took
significantly longer than that, were uncontroversial, and only need miner
adoption. Full node adoption is even slower.

I think the shortest reasonable timeframe for an uncontroversial hardfork
is somewhere in the range between 6 and 12 months.

For a controversial one, not at all.

-- 
Pieter
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version 0.10.3 release candidate 2 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.10.3/test/>

(release candidate 1 never had binaries available)

This is a release candidate for a new minor version release, bringing security
fixes and translation updates.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

Preliminary releae notes follow:

Upgrading and downgrading
=========================

How to Upgrade
- --------------

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes for older versions), then run the
installer (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or
bitcoind/bitcoin-qt (on Linux).

Downgrade warning
- ------------------

Because release 0.10.0 and later makes use of headers-first synchronization and
parallel block download (see further), the block files and databases are not
backwards-compatible with pre-0.10 versions of Bitcoin Core or other software:

* Blocks will be stored on disk out of order (in the order they are
received, really), which makes it incompatible with some tools or
other programs. Reindexing using earlier versions will also not work
anymore as a result of this.

* The block index database will now hold headers for which no block is
stored on disk, which earlier versions won't support.

If you want to be able to downgrade smoothly, make a backup of your entire data
directory. Without this your node will need start syncing (or importing from
bootstrap.dat) anew afterwards. It is possible that the data from a completely
synchronised 0.10 node may be usable in older versions as-is, but this is not
supported and may break as soon as the older version attempts to reindex.

This does not affect wallet forward or backward compatibility.

Notable changes
===============

Fix buffer overflow in bundled upnp
- ------------------------------------

Bundled miniupnpc was updated to 1.9.20151008. This fixes a buffer overflow in
the XML parser during initial network discovery.

Details can be found here: http://talosintel.com/reports/TALOS-2015-0035/

This applies to the distributed executables only, not when building from source or
using distribution provided packages.

Additionally, upnp has been disabled by default. This may result in a lower
number of reachable nodes on IPv4, however this prevents future libupnpc
vulnerabilities from being a structural risk to the network
(see https://github.com/bitcoin/bitcoin/pull/6795).

Test for LowS signatures before relaying
- -----------------------------------------

Make the node require the canonical 'low-s' encoding for ECDSA signatures when
relaying or mining.  This removes a nuisance malleability vector.

Consensus behavior is unchanged.

If widely deployed this change would eliminate the last remaining known vector
for nuisance malleability on SIGHASH_ALL P2PKH transactions. On the down-side
it will block most transactions made by sufficiently out of date software.

Unlike the other avenues to change txids on transactions this
one was randomly violated by all deployed bitcoin software prior to
its discovery. So, while other malleability vectors where made
non-standard as soon as they were discovered, this one has remained
permitted. Even BIP62 did not propose applying this rule to
old version transactions, but conforming implementations have become
much more common since BIP62 was initially written.

Bitcoin Core has produced compatible signatures since a28fb70e in
September 2013, but this didn't make it into a release until 0.9
in March 2014; Bitcoinj has done so for a similar span of time.
Bitcoinjs and electrum have been more recently updated.

This does not replace the need for BIP62 or similar, as miners can
still cooperate to break transactions.  Nor does it replace the
need for wallet software to handle malleability sanely[1]. This
only eliminates the cheap and irritating DOS attack.

[1] On the Malleability of Bitcoin Transactions
Marcin Andrychowicz, Stefan Dziembowski, Daniel Malinowski, Łukasz Mazurek
http://fc15.ifca.ai/preproceedings/bitcoin/paper_9.pdf

Minimum relay fee default increase
- -----------------------------------

The default for the `-minrelaytxfee` setting has been increased from `0.00001`
to `0.00005`.

This is necessitated by the current transaction flooding, causing
outrageous memory usage on nodes due to the mempool ballooning. This is a
temporary measure, bridging the time until a dynamic method for determining
this fee is merged (which will be in 0.12).

(see https://github.com/bitcoin/bitcoin/pull/6793, as well as the 0.11.0
release notes, in which this value was suggested)

0.10.3 Change log
=================

Detailed release notes follow. This overview includes changes that affect external
behavior, not code moves, refactors or string updates.

- - #6186 `e4a7d51` Fix two problems in CSubnet parsing
- - #6153 `ebd7d8d` Parameter interaction: disable upnp if -proxy set
- - #6203 `ecc96f5` Remove P2SH coinbase flag, no longer interesting
- - #6226 `181771b` json: fail read_string if string contains trailing garbage
- - #6244 `09334e0` configure: Detect (and reject) LibreSSL
- - #6276 `0fd8464` Fix getbalance * 0
- - #6274 `be64204` Add option `-alerts` to opt out of alert system
- - #6319 `3f55638` doc: update mailing list address
- - #6438 `7e66e9c` openssl: avoid config file load/race
- - #6439 `255eced` Updated URL location of netinstall for Debian
- - #6412 `0739e6e` Test whether created sockets are select()able
- - #6694 `f696ea1` [QT] fix thin space word wrap line brake issue
- - #6704 `743cc9e` Backport bugfixes to 0.10
- - #6769 `1cea6b0` Test LowS in standardness, removes nuisance malleability vector.
- - #6789 `093d7b5` Update miniupnpc to 1.9.20151008
- - #6795 `f2778e0` net: Disable upnp by default
- - #6797 `91ef4d9` Do not store more than 200 timedata samples
- - #6793 `842c48d` Bump minrelaytxfee default

Credits
=======

Thanks to everyone who directly contributed to this release:

- - Adam Weiss
- - Alex Morcos
- - Casey Rodarmor
- - Cory Fields
- - fanquake
- - Gregory Maxwell
- - Jonas Schnelli
- - J Ross Nicoll
- - Luke Dashjr
- - Pavel Vasin
- - Pieter Wuille
- - randy-waterhouse
- - ฿tcDrak
- - Tom Harding
- - Veres Lajos
- - Wladimir J. van der Laan

And all those who contributed additional code review and/or security research:

- - timothy on IRC for reporting the issue
- - Vulnerability in miniupnp discovered by Aleksandar Nikolic of Cisco Talos

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWG+djAAoJEHSBCwEjRsmmYskH/3bvwLnbRYaeAX71/F/VgOqd
N7Ly6ar00I4nbTpOloRd4xdf2TGaeRyu1Ty7lbK21weCxzsC3Uq7s40zDyBbfgZq
3kDupt9naNVmfXlsCHyiFRqgQnbtJTX1UH7Y8rjjnqPuInNdo0S6ZGBZm04Iqccq
qPfncyK95fd+9g1fow0TP+dIFHrEjD9NtEzsiH58RNtDlpadxNYCjbMjnDriS7wd
k7uOkDx+jBG1X4TwMHEf32PkKssZYzDr4xLP4SkmDTYVqzf24bOiO9nXempxUUQp
E9t1pRklZSZ5chBIRMJHhwci8cRVLmv4cAbXx1cUby4P7WJJC/8t8/fhWxgPddk=
=ar+M
-----END PGP SIGNATURE-----

-------------------------------------
Jean-Paul,

I think you're missing what I'm saying -- the point of my suggestion to
make Rocket a min-spec is more along the lines of saying that the Rocket
serves as a fixed point, Bitcoin Core performance must be acceptable on
that platform, however it can be lower. Yes there are conversion factors
and different architectures will perform differently. However, there still
must be some baseline, a point at which we can say processors below it no
longer are supported. I am saying that line should never be set so high as
to exclude presently available open hardware.

Ultimately, this ends up making an odd, but nice, goal for Bitcoin
development. If Bitcoin Core needs more MIPS, the community must ensure the
availability of open hardware that it can run on.

Jeff,

Moxie looks fantastic! The reason I thought RISC-V was a good selection is
the very active development community which is pushing the performance of
the ISA implementations forward. Can you speak to the health of Moxie
development? Ultimately, ensuring support for many open architectures would
be preferable. Are there other reasonable open-source processors that you
are aware of?

I would be willing to work on a design a Bitcoin specific open-hardware
processor, up to the FPGA bound, if this would be useful for this goal.

On Fri, Jul 3, 2015 at 12:19 PM, Jean-Paul Kogelman <jeanpaulkogelman@me.com

-------------------------------------
On Wed, May 13, 2015 at 1:26 PM, Alex Mizrahi <alex.mizrahi@gmail.com>
wrote:


I don't really see how you can protect against total isolation of a node
(POS or POW).  You would need to find an alternative route for the
information.

Even encrypted connections are pointless without authentication of who you
are communicating with.

Again, it is part of the security model that you can connect to at least
one honest node.

Someone tweated all the bitcoin headers at one point.  The problem is that
if everyone uses the same check, then that source can be compromised.

upwards of $100000.

Headers first mean that you can't knock a synced node off the main chain
without winning the POW race.

Checkpoints can be replaced with a minimum amount of POW for initial sync.
This prevents spam of low POW blocks.  Once a node is on a chain with at
least that much POW, it considers it the main chain.,
-------------------------------------
Jeff, block size limits large enough to prevent fee pressure is absolutely,
unequivocally unsustainable. We are already running against technological
limits in the tradeoff between decentralization and utility. Increases of
the block size limit in advance of fee pressure only delay the problem --
it does not and cannot solve it!

We must be careful to use the block size limit now to get infrastructure to
support a world with full blocks -- it's not that hard -- while still
having a little room to grow fast if things unexpectedly break.

On Fri, Jun 26, 2015 at 11:23 AM, Jeff Garzik <jgarzik@gmail.com> wrote:

-------------------------------------
If the worry about raising the block size will increase centralization,
could not one could imagine an application which rewarded decentralized
storage of block data? It could even be build aside or on top of the
existing bitcoin protocol.

See the Permacoin paper by Andrew Miller:
http://cs.umd.edu/~amiller/permacoin.pdf

Regards
-------------------------------------
On Fri, Aug 21, 2015 at 10:29 AM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

That's exactly what the "Technical Opinion" column is for.

-------------------------------------


OK, good to hear that. I'm not happy about the use of web technologies in
wallets/services either, but the causes of that trend are nothing to do
with block chain sizes. It's more because there's a generation of
developers who see no alternatives.

With projects like Lighthouse, I'm trying to show people that they can
blend the good bits of the web with the good bits of more traditional
client side development, at a cost they can afford.

Unfortunately, as you know, one of the reasons that developers turn to
outsourced services is that those services actually like developers and
give them the features they need. Whereas any attempt to add protocol
features for app/wallet developers to Bitcoin Core becomes controversial
due to some perceived or real lack of perfection.

I persevered for several months to add a very small "API" I needed for my
app to Bitcoin Core, and it was in the end a waste of time. There are no
actionable items left for the getutxo patch, regardless, I had to fork
Bitcoin to get it out there. It would have been *much* easier to just say,
fuck it, I'll use blockchain.info and in fact some in this community told
me to do exactly that. But, the approach I chose has been working fine for
months now.

Compare this experience to companies like chain.com, blockcypher etc - when
developers say jump, they say "how high?"

So It's unreasonable for the Bitcoin Core developer group to constantly
call developers building apps idiots or "non technical" (as I see so often
in this block size debate), and then complain that people don't write apps
in their preferred way! Just accept that decentralised app dev is already
hard, and the way Core is run makes it much harder still.


As I said I dont think we can expect Bitcoin to scale with no further


A big part of the debate around this change is showing that this statement
is wrong. "Scaling" is not some kind of binary yes/no thing. It's a
continuous effort. You write a system that scales a certain amount, and
then if you find you need more capacity, you scale it again. Maybe that
 involves rewriting the existing code or maybe it just means improving what
you've got.

Or maybe (painful truth coming up) your product is not that compelling, or
times change and your users leave, and you discover you never actually need
to scale to the giddy heights originally envisioned.

A big part of the reason modern web dev is so messed up is that lots of
developers starting thinking every app they built needed to be "web scale"
from day one. SQL databases? Pah. Doesn't scale. Think big. We gotta no
NoSQL sharded key/value store from the start! Otherwise we're just showing
lack of confidence in our own product.

Then when they used up all their budget solving consistency bugs a
relational database would have avoided, they notice their competitors
sailing past them on a not-fully-scalable but certainly-scalable-enough
architecture that let them focus on features and making users happy.






OK. O() notation normally refers to computational complexity, but ... I
still don't get it - the vast majority of users don't run relaying nodes
that take part in gossiping. They run web or SPV wallets. And the nodes
that do take part don't connect to every other node.






Alright - let's agree that we disagree on a few areas, like the relative
desirability of alternative non-blockchain designs - but we do seem to
agree that there is a case for an increase in the block size limit. That
seems like progress.

As you agree with that, what sort of schedule and time are you thinking of?
(well, by "you" I really mean blockstream because it's taking forever to
try and negotiate with every single person individually).
-------------------------------------
I proposed something very similar 2 years ago:
https://bitcointalk.org/index.php?topic=283746.0

This is an interesting academic idea. But the way you implement it will 
immediately kill all existing full and SPV nodes (not really dead, 
rather like zombie as they can't send and receive any tx).

joe2015--- via bitcoin-dev 於 2015-12-20 05:56 寫到:


-------------------------------------
Hi

Following earlier posts on Proof of Payment I'm now proposing the following
BIP for a Proof of Payment URI scheme (To read it formatted instead, go to
https://github.com/kallerosenbaum/poppoc/wiki/btcpop-scheme-BIP).

Regards,
Kalle Rosenbaum

<pre>
  BIP: <BIP number>
  Title: Proof of Payment URI scheme
  Author: Kalle Rosenbaum <kalle@rosenbaum.se>
  Status: Draft
  Type: Standards Track
  Created: <date created on, in ISO 8601 (yyyy-mm-dd) format>
</pre>

== Abstract ==

This is a proposal for a URI scheme to be used in the Proof of Payment
process.

== Motivation ==

To make a Proof of Payment, the party that wants the proof needs to
transfer a Proof of Payment request to the wallet software of the
other party. To facilitate that transfer, a new URI scheme
representing the PoP request is proposed. This URI can then be encoded
in QR images or sent over NFC in order to transfer it to the wallet.

== Specification ==

The specification is the same as BIP0021, with the following
differences:

* The URI scheme is <tt>btcpop</tt> instead of <tt>bitcoin</tt>
* The path component, i.e. the address part, is always empty.
* A mandatory <tt>p</tt> parameter whose value contains the destination for
the PoP. This could for example be a <tt>https:</tt> URL or a <tt>mailto:</tt>
URI.
* A mandatory <tt>n</tt> parameter representing the nonce, base58 encoded.
* An optional <tt>txid</tt> parameter containing the Base58 encoded hash of
the transaction to prove.

Just as in BIP0021, elements of the query component may contain
characters outside the valid range. These must first be encoded
according to UTF-8, and then each octet of the corresponding UTF-8
sequence must be percent-encoded as described in RFC 3986.

All parameters except <tt>p</tt> and <tt>n</tt> are hints to the
wallet on which transaction to create a PoP for.

The extensibility of BIP0021 applies to this scheme as well. For
example, a <tt>date</tt> parameter or a <tt>toaddr</tt> parameter
might be useful. <tt>req-*</tt> parameters are also allowed and obey
the same rules as in BIP0021, clients not supporting a <tt>req-*</tt>
parameter must consider the URI invalid.

=== Keep URIs short ===

Implementations should keep the URIs as short as possible. This is
because it makes QR decoding more stable. A camera with a scratched
lens or low resolution may run into problems scanning huge QR
codes. This is why the <tt>txid</tt> parameter is encoded in Base58
instead of the classic hex encoded string. We get away with 44
characters instead of 64. Also, the <tt>nonce</tt> parameter is Base58
encoded for the same reason.

== Interpretation ==

=== Transaction hints ===

The wallet processing the URI must use the hints in the PoP request to
filter its transaction set. The <tt>label</tt>, <tt>amount</tt> and
<tt>message</tt> parameters must, if present in the URI, exactly match
the data associated with the original payment according to the
following table:

{|
| <tt>btcpop:</tt> URI parameter || <tt>bitcoin:</tt> URI parameter ||
BIP70 PaymentDetails data
|-
| <tt>label</tt>                 || <tt>label</tt>                  ||
<tt>merchant_data</tt>
|-
| <tt>amount</tt>                || <tt>amount</tt>                 ||
<tt>sum of outputs.amount</tt>
|-
| <tt>message</tt>               || <tt>message</tt>                ||
<tt>memo</tt>
|}

The <tt>txid</tt> parameter value must match the transaction hash of
the payment.

After filtering, the resulting transaction set is displayed to the
user who selects one of them to prove. An implementation could also
automatically select a transaction in the filtered set, but
there must still be a way for the user to select freely among the
matching transactions. If the filtered set is empty, no transaction
fits the hints and a message about that is presented to the user. If
the filtered set contains exactly one transaction, which is
preferable, that transaction can be automatically selected.

As a fallback, there must also be a way for the user to select any
transaction from the wallet regardless of the transaction hints. This
can be useful if the metadata of the wallet is lost, possibly due to a
restore from backup.

=== PoP destination <tt>p</tt> ===

The <tt>p</tt> parameter value is the destination where to send the
PoP to. This destination is typically a <tt>https:</tt> URL or a
<tt>http:</tt> URL, but it could be any type of URI, for example
<tt>mailto:</tt>. To keep <tt>btcpop:</tt> URIs short, users should
not make their <tt>p</tt> parameter unneccesarily long.

==== <tt>http:</tt> and <tt>https:</tt> URLs ====

Wallet implementations must support the <tt>http:</tt> and
<tt>https:</tt> schemes in which case <tt>POST</tt> method must be
used. The content type of the POST request must be set to

 Content-Type: application/bitcoin-pop
 Content-Transfer-Encoding: binary

== Examples ==

Send PoP for a transaction with label "video 42923" to
<tt>https://www.example.com/pop/352</tt>, using nonce <tt>0x73 0xd5
0x1a 0xbb 0xd8 0x9c</tt>:

 btcpop:?p=https://www.example.com/pop/352&n=zgWTm8yH&label=video 42923

Send PoP through mail using
<tt>mailto:pop@example.com?subject=pop444</tt>, amount is 13370000
satoshis, nonce is <tt>0x6f 0xe 0xfb 0x68 0x92 0xf9</tt>. Note that
the <tt>?</tt> before <tt>subject</tt> is OK according to RFC3986,
since the query part starts from the first <tt>?</tt>:

 btcpop:?p=mailto:pop@example.com?subject%3Dpop444&n=xJdKmEbr&amount=0.1337

Send PoP for transaction with id
<tt>cca7507897abc89628f450e8b1e0c6fca4ec3f7b34cccf55f3f531c659ff4d79</tt>
to pizza place at <tt>http://pizza.example.com/pop/laszlo111</tt>
using nonce <tt>0xfc 0xcc 0x2c 0x35 0xf0 0xb8</tt>

 btcpop:?p=
http://pizza.example.com/pop/laszlo111&n=3AtNpVrPh&txid=Emt9MPvt1joznqHy5eEHkNtcuQuYWXzYJBQZN6BJm6NL

== Reference implementation ==

[https://github.com/kallerosenbaum/poppoc poppoc on GitHub]

[https://github.com/kallerosenbaum/wallet Mycelium fork on GitHub]

== References ==

[https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki BIP21]: URI
Scheme

[[Proof of Payment BIP]]

[https://www.ietf.org/rfc/rfc3986.txt RFC3986]: Uniform Resource Identifier
(URI): Generic Syntax
-------------------------------------
On Thu, Oct 08, 2015 at 01:00:14AM +1000, Anthony Towns via bitcoin-dev wrote:



As was discussed on the weekly meeting [0], turns out it *is* less
restrictive than current policy. IsStandardTx currently returns a failure
if the tx version is greater than 1, and per BIP68, nSequence will only
be inforced with tx version of 2 or greater.

So afaics, BIP 65 (OP_CLTV), BIP 68 (nSequence) and BIP 112 (OP_CSV)
are all "safe soft forks", and if activated won't cause SPV nodes to
see a significant uptick in reorgs, double-spends etc. (They'll still
be vulnerable to people deliberately spending hashpower to mine invalid
blocks, but that's a problem at any point, independent of whether a
soft-fork is underway)

[0] http://www.erisian.com.au/meetbot/bitcoin-dev/2015/bitcoin-dev.2015-10-08-18.59.log.html#l-312

Cheers,
aj


-------------------------------------


Please read my article as it's all explained there.

But to reiterate: the risk is that miners will build invalid blocks on top
of the best work chain, instead of an ignored lower work side chain. This
opens users to payment fraud. With a hard fork, all the blocks by miners
that aren't checking all the rules anymore get neatly collected together on
a side chain after the split, and wallets all know how to ignore that chain.

Yes, you made OP_NOPs be non-standard. So out of the box, miners won't
create invalid blocks, as long as they're running Core past that version.
But this makes the IsStandard function very much like a part of the
consensus rules, as bypassing it can result in invalid blocks being
created. Miners have always understood that they can modify this function,
or even bypass it entirely, without affecting the validity of their blocks.
And some miners do exactly that.

So I'll repeat the question that I posed before - given that there are
clear, explicit downsides, what is the purpose of doing things this way?
Where is the gain for ordinary Bitcoin users?
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Hello. I've seen Greg make a couple of posts online
(https://bitcointalk.org/index.php?topic=1033396.msg11155302#msg11155302
is one such example) where he has mentioned that Pieter has a new
proposal for allowing multiple softforks to be deployed at the same
time. As discussed in the thread I linked, the idea seems simple
enough. Still, I'm curious if the actual proposal has been posted
anywhere. I spent a few minutes searching the usual suspects (this
mailing list, Reddit, Bitcointalk, IRC logs, BIPs) and can't find
anything.

Thanks.

- ---
Douglas Roark
Senior Developer
Armory Technologies, Inc.
doug@bitcoinarmory.com
PGP key ID: 92ADC0D7
-----BEGIN PGP SIGNATURE-----
Version: GnuPG/MacGPG2 v2.0.22 (Darwin)
Comment: GPGTools - https://gpgtools.org

iQIcBAEBCgAGBQJVTQ4eAAoJEGybVGGSrcDX8eMQAOQiDA7an+qZBqDfVIwEzY2C
SxOVxswwxAyTtZNM/Nm+8MTq77hF8+3j/C3bUbDW6wCu4QxBYA/uiCGTf44dj6WX
7aiXg1o9C4LfPcuUngcMI0H5ixOUxnbqUdmpNdoIvy4did2dVs9fAmOPEoSVUm72
6dMLGrtlPN0jcLX6pJd12Dy3laKxd0AP72wi6SivH6i8v8rLb940EuBS3hIkuZG0
vnR5MXMIEd0rkWesr8hn6oTs/k8t4zgts7cgIrA7rU3wJq0qaHBa8uASUxwHKDjD
KmDwaigvOGN6XqitqokCUlqjoxvwpimCjb3Uv5Pkxn8+dwue9F/IggRXUSuifJRn
UEZT2F8fwhiluldz3sRaNtLOpCoKfPC+YYv7kvGySgqagtNJFHoFhbeQM0S3yjRn
Ceh1xK9sOjrxw/my0jwpjJkqlhvQtVG15OsNWDzZ+eWa56kghnSgLkFO+T4G6IxB
EUOcAYjJkLbg5ssjgyhvDOvGqft+2e4MNlB01e1ZQr4whQH4TdRkd66A4WDNB+0g
LBqVhAc2C8L3g046mhZmC33SuOSxxm8shlxZvYLHU2HrnUFg9NkkXi1Ub7agMSck
TTkLbMx17AvOXkKH0v1L20kWoWAp9LfRGdD+qnY8svJkaUuVtgDurpcwEk40WwEZ
caYBw+8bdLpKZwqbA1DL
=ayhE
-----END PGP SIGNATURE-----


-------------------------------------
On Sat, Jan 10, 2015 at 04:26:23AM +0000, Gregory Maxwell wrote:

As an aside, it's interesting to note that this issue is not entirely
unique to miners.

For example in micropayment channel protocols the receiver must validate
signatures from the sender to ensure that they will be able to broadcast
transactions containing those signatures in the near-future. If they
accept a signature as valid that the majority of hashing power rejects
as invalid the sender can simply wait until the micropayment channel
timeout expires to recover 100% of their funds, ripping off the
receiver. There's many other advanced Bitcoin protocols with similar
vulnerabilities; I'd be interested to hear if anyone can come up with a
similar vulnerability in a non-Bitcoin protocol, and wouldn't be that
surprised if they did.

While I have often cautioned people before to avoid using libsecp256k1
for verification on the grounds that consensus trumps correctness, the
above incompatibility does strongly suggest that OpenSSL may not itself
have very good consensus-critical design. Along with Maxwell and
Wuille's recent findings CVE-2014-3570 - strong evidence of the
excellent testing the library has undergone - I personally am now of the
opinion that migrating Bitcoin Core to libsecp256k1 in the near future
is a good idea on the grounds that it provides us with a well-written,
and well-understood library designed with consensus in mind that'll
probably give us fewer consensus problems than our existing OpenSSL
dependency. It'll also help advanced protocol implementations by giving
them a clear dependency to use when they need consensus-critical
signature evaluation.

1) https://www.reddit.com/r/Bitcoin/comments/2rrxq7/on_why_010s_release_notes_say_we_have_reason_to/

-- 
'peter'[:-1]@petertodd.org
000000000000003b82d8644b56c846e7497118b04a6ec68d3e0a23d33323b82e
-------------------------------------

I agree, but we need things to be easy in the short term as well as the
long term :)

The long term solution is clearly to have the 12 word seed be an encryption
key for a wallet backup with all associated metadata. We're heading in that
direction one step at a time. Unfortunately it will take time for wallets
to start working this way, and all the pieces to fall into place. Restoring
from the block chain will be a semi regular operation for users until then.

WRT version number I have no real strong feelings about this. But
representing short pieces of binary data as words is so convenient, it
seems likely that it could be similar to addresses: people find other uses
for this mechanism beyond just storing a raw private key. Bitcoin addresses
have versions and that's proven to be useful several times, even though in
theory an address is "just" a hash of a pubkey.
-------------------------------------
On Fri, Aug 7, 2015 at 1:17 PM, jl2012 via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



Some arguments have floated around that even in the absence of "causing an
increase in the number of full nodes", that a reduction of the max block
size might be beneficial for other reasons, such as bandwidth saturation
benefits. Also less time spent validating transactions because of the fewer
transactions.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
I think that if a vote was free, no matter how much weight it carried, then
it could be easily bought and the vote manipulated. If the cost of the vote
was proportional to its weight, then it would be harder to manipulate the
vote.
I know I haven't explained that thoroughly, but as an analogy think to how
markets determine the clearing price for a good. Votes in markets cost
money.

On 8 August 2015 at 16:10, Peter Todd <pete@petertodd.org> wrote:

-------------------------------------
Mike's position is that he wants the block size limit
to eventually be removed. That is of course an extreme view. Meanwhile,
your view that the block size should be artificially constrained below the
organic growth curve (in a way that will penalize a majority of existing
and future users) lies at the other extreme. The majority position lies
somewhere in between (i.e. a one-time increase to 8MB). This is the
position that ultimately matters.

If the block size is increased to 8MB and things get demonstrably a whole
lot worse, then you will have a solid leg to stand on. In that case we can
always do another hard fork later to reduce the block size back to
something smaller, and henceforth the block size will never be touched
again.

On 4 August 2015 at 11:35, Jorge Timón <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On 2015/1/21 15:37, Gavin Andresen wrote:
(or whatever is best reference for DER).

The link you gave is to the 2002 revision.
http://www.itu.int/rec/T-REC-X.690-200811-I/en has the latest revision
(Nov. 2008) and, AFAIK, is the most visible link to people searching
for X.690.

That said, X.690 is the definitive DER document (if not exactly the
easiest read). A link to it wouldn't hurt.


These all look good to me.

- ---
Douglas Roark
Senior Developer
Armory Technologies, Inc.
doug@bitcoinarmory.com
PGP key ID: 92ADC0D7
-----BEGIN PGP SIGNATURE-----
Version: GnuPG/MacGPG2 v2.0.22 (Darwin)
Comment: GPGTools - https://gpgtools.org

iQIcBAEBCgAGBQJUwBF8AAoJEGybVGGSrcDXBxcP/j9dKIeXkOvDFgSzON2hmjxT
nzpPcxovGt+ds1KqHMtuMm8+Mmc/Z8kOhKWzgQKYlxq8eQayQ4X/DUr97IY248NX
udVM6vEp/azPkXLOQnO6POpv8Il6twyuYGvFAHLiYe9k9qMfdSKZetx5xFKVBsuj
DhRY2TnWC7/OXNUrT7H5TPHDaGHyXeJ47XSOVjGQ/qxdczIzvmt11amZ/Vn2+uXh
Rvz+0CzbpXYaqYB04ZnIv5lxknmjWGbxPdht/SoOly8INehQacWnwUNZJpilKb6x
qEpbDGNxW2zHEFgfNHmtr9PCBN8KyiVnTt+VZpNNl7PJCxZiK6uiwyNxsmOBhBtm
Hrsvxb9GqEO/6PKesEo+Hi+6hhzzQRC6Xrf85SaFMzw9UjKuuRhstxx7XhudKFkN
lBJcxd40G7kWk0Gv+YQmhFUyXUBqloEFGrFlzWniFKaJGzZs5D0JPd83DsPI4RuT
0M63YabL8qplYN8vnyUXabFpzglvQdAFqZS2GsO6zwAeWrqxsojpcEpikj4T+izR
W1TzaRDdm5pEaMMxvb6wFIgO32uAjN1a8GrRj+uk5cxuiOuk/C4Ii18FYhqEtDNd
Gv80rPxWEOxbCoSqH6igPnySw3ePFLBzgC4eSLBTnqfKYltd8fTeS9wGy47+L1YO
qb5K/xlqt+REOdbTGLHi
=MNXG
-----END PGP SIGNATURE-----


-------------------------------------
On Wed, Sep 30, 2015 at 05:57:42PM +0000, Luke Dashjr wrote:

In principle, "feature freeze" means that any large code changes will no longer go into 0.12, unless fixing critical bugs. 

I'm not keen on postponing 0.12 for such reasons - after the HK workshop I'm sure that it will take some development/testing/review before code makes it into anything. Apart from that there's a good point to decouple consensus changes from Bitcoin Core major releases.

We've seen lot of release date drift due to "this and this change needs to make it in" in the past, that was a major reason to switch to a time-based instead of feature-based release schedule.

We can always do a 0.12.1.

Wladimir

-------------------------------------
Hi,
As I understand the main problem of the fork Core<->XT is possibility
of double spending:
-I run XT and spend my coins
-it is written in 8mb block
-Core does not accept this block
-I run Core and spend my coins again
-it is written in 1mb block
-but XT accepts this block too
so
-in the XT blockchain both blocks [8] and [1] contain my coins

I thought that possible solution can be to set minimum block size
i.e.
2016: 1mb <= blockSize < 2mb
2017: 2mb <= blockSize < 3mb
2018: 3mb <= blockSize < 4mb
etc

Free space could be filled with zeroes and compressed.

That's all, just an idea.


With Best Regards
Dmitry Bolshakov
bdimych@gmail.com

-------------------------------------
organizations operating full network nodes would provide connectivity to
light clients and these light clients would make up the majority of the
user base.

Satoshi also believed that fraud proofs would be widely available and
practical.

If fraud proofs were practical SPV client security would be much closer
to full node security than it is today.

Unfortunately no design for fraud proofs which is both efficient and
secure has been proposed; much less implemented and deployed.

In building a system as new and innovative as bitcoin certain things
will be wrong.

The perception that SPV clients could be made nearly as secure as full
nodes is one example of something that was wrong.

On 06/27/2015 05:14 PM, Santino Napolitano wrote:



-------------------------------------
I did wonder what the post actually meant, I recommend appending /s after
sarcasm so it's clear. Lots gets lost in text. But I agree with you btw his
response was not particularly tactful.

On Mon, Jun 1, 2015 at 7:19 PM, Warren Togami Jr. <wtogami@gmail.com> wrote:

-------------------------------------
I disagree with the importance of this concern and old soft/hardforks will
replace this activation mechanism with height, so that's an argument in
favor of using the height from the start. This is "being discussed" in a
thread branched from bip99's discussion.
Anyway, is this proposing to use the block time or the median block time?
For some hardforks/softforks the block time complicates the implementation
(ie in acceptToMemoryPool) as discussed in the mentioned thread.
On Sep 19, 2015 1:24 AM, "Rusty Russell" <rusty@rustcorp.com.au> wrote:

-------------------------------------
If you read between the lines of what was recently changed and why
(reducing to 2MB), it seems reasonable to assume BIP101's allowance
opens up some of the attack vector again.

On Fri, Sep 4, 2015 at 4:37 PM, Simon Liu via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Guys,
 I strongly think the original prabhat e-mail is a parody.

And I find very funny that important people have responded.

But maybe I'm wrong!
*:)*




El jue., 27 ago. 2015 a las 11:04, Chris Pacia via bitcoin-dev (<
bitcoin-dev@lists.linuxfoundation.org>) escribió:

-------------------------------------
On Sun, Jan 4, 2015 at 5:22 PM, Ross Nicoll <jrn@jrn.me.uk> wrote:

Can you send me the actual raw transaction (that site doesn't appear
have a way to get it, only some cooked json output; which doesn't
include the sequence number).

As I said, it's a severe bug if unlocked transactions are being
relayed or mempooled far in advance.


Ah I missed that the replacement had to be final. Thats indeed a much
more sane thing to do than I was thinking (sorry for some reason I saw
the +1 and thought it was just checking the sequence number was
higher.)


If they can relay the first one to begin with its an an issue, the
replacement just makes it twice an issue. :)


-------------------------------------
On 15 August 2015 at 18:43, Satoshi Nakamoto via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Is he talking about "full nodes" i.e. validating-only, or nodes in the
sense of the original whitepaper (i.e. miners)? Because there is already
plenty of incentive for running a node (i.e. the coinbase).

The issue is that the reward is more or less like a decentralised lottery
with high entry cost. If the income could be smoothed like in a mining
pool, without actually being a mining pool, then perhaps more people would
pay to enter the mining game. A bit like making P2Pool the one and only
pool allowed on the network.
-------------------------------------
On Thu, Aug 20, 2015 at 1:07 AM, Eric Voskuil <eric@voskuil.org> wrote:

No, as previously explained, once libconsensus is complete it can be
moved to a separate repository like libsecp256k1.
At first it will need to be a subtree/subrepository of Bitcoin Core
(like libsecp256k1 currently is), but I still don't undesrtand how
that can possibly be a problem for alternative implementations (they
can use a subtree as well if they want to). Depending on a separated
libconsensus doesn't "make Bitcoin Core a dependency" more than
depending on libsecp256k1 currently does.


I believe the simplest option would be to fork the libconsensus
project and do the schism/controversial/contentious hardfork there.
But of course modifying libconsensus will be much easier than
modifying Bitcoin Core (if anything, because the amount of code is
much smaller).


Unfortunately I only directly contacted libbitcoin because I was
subscribed to the list at the time (maybe I'm still subscribed, not
really sure).
The other attempts to get feedback from other alternative
implementations have been just mostly-ignored threads in bitcoin-dev.
So, no, I cannot facilitate such a discussion, but I'm more than happy
to collaborate to achieve our mutual goal.

-------------------------------------
Ok good. We have established it to be a non-trivial cost.
Now, what is the growth complexity of the total cost of the network in
terms of number of connections each hub has to other hubs? And then,
consider a payment channel with many hops in it. The end-to-end users would
have to swallow all the costs of the hubs in the channel.

On 9 August 2015 at 22:57, Patrick Strateman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Satoshi,

As much as I want to believe this is you it's very difficult to ignore the
fact that Vistomail could have been hacked and I'm currently speaking to a
troll.
Can you copy and paste what you wrote above, to
http://p2pfoundation.ning.com as well, like how you did during the Dorian
fiasco?


Much appreciated.


On Sat, Aug 15, 2015 at 10:43 AM, Satoshi Nakamoto via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
will accept and use. Users "fire" developers by choosing different devs 
or different software.


Most open source projects do not require 100% user adoption in order for 
other users to be compatible.  Very different than most open source 
projects.  Repeating "stock" answers does nothing to advanced the 
discussion.

Russ


-------------------------------------
On Thu, Jun 18, 2015 at 3:33 PM, Mark Friedenbach <mark@friedenbach.org>
wrote:


This is a long, unreasonable list of work.  None of this exists and it
equates to "upgrade all wallets and websites everywhere"  It requires all
exchanges, payment processors, merchants, etc. to  - basically everybody
but miners - to update.

It is a far, far larger amount of work to write, test and deploy than
simply increasing the block size limit.

Think through roll-out of these ambitious suggestions, before suggesting as
an alternative!

Not a realistic alternative except in an alternate universe where (a)
developer work at all companies is cost free, plus (b) we can pause the
business universe while we wait for The Perfect Solution.










-- 
Jeff Garzik
Bitcoin core developer and open source evangelist
BitPay, Inc.      https://bitpay.com/
-------------------------------------
On Thu, Jun 18, 2015 at 11:56 PM, Mike Hearn <mike@plan99.net> wrote:


The new list currently has footers removed during testing.  I am not
pleased with the need to remove the subject tag and footer to be more
compatible with DKIM users.


I'm guessing DKIM enforcement is not very common because of issues like
this?

It seems that Sourceforge silently drops DKIM enforced mail like
jgarzik's.  LF seems to pass along their mail but mangles the header/body
and makes DKIM verification fail, which causes gmail to toss it into the
spam folder.  I think this behavior is slightly worse than Sourceforge
because it makes the poster think their message was successfully sent (it
is in the archive), but many subscribers never see it due to the spam
binning.

I don't see any good solution to this except an auto-reject for DKIM
enforced domain postings.  Yes this is rather terrible, but the instant
rejection is vastly better than Sourceforge silently dropping the post or
LF getting stuck in spam filters.

We should also auto-reject any other reason for mail getting stuck in the
moderation queue like including non-subscribers.  I considered
auto-rejecting spam too, but that could go horribly wrong as a false From
address could make the Mailman server into a spammer itself.  We may have
no choice but to silently drop spam for that reason.

Warren
-------------------------------------

Andy Schroder

On 06/10/2015 03:20 PM, Peter Todd wrote:

It's possible that the enigmail extension is not working right, but I 
was under the impression that it is just feeding data to gpg and then 
receiving the response back. It's possible that your e-mail you just 
checked was not sent through mailman since I also replied directly to 
you explicitly (in which case the message has not been modified) and you 
probably have the setting in the mailing list set to not send duplicate 
messages if you are an explicit TO. I just deleted all explicit TOs for 
this message, so everyone should be receiving it through the mailing 
list and not directly. Is the signature still valid for you now? I think 
enigmail can handle messages with some signed and unsigned content, and 
maybe PGP/MIME inherently does not support this and a mailing list 
re-writing parts of messages is an expected action? If this message 
re-writing is an expected action and I'm correct that PGP/MIME does not 
support partially signed content, then maybe it is just a recommendation 
for this mailing list to not use PGP/MIME for messages sent to the list?

Can anyone else confirm?



-------------------------------------
On Tue, Sep 29, 2015 at 06:31:28PM +0000, Gregory Maxwell via bitcoin-dev wrote:

I think I finally understand this objection.

For a hard fork, activated by a majority of nodes/hashpower upgrading
to a new bitcoin release, the behaviour is:

 - upgraded bitcoin nodes: everything works fine

 - non-upgraded bitcoin nodes: total breakage. there will be a push
   alert telling you to upgrade. anyone who doesn't will think they're
   tracking "bitcoin" but will actually be tracking a new "bitcoin-old"
   altcoin. most non-upgraded miners will presumably realise they're
   wasting hashpower and stop doing this pretty quick; and remaining
   miners will only create blocks very slowly due to sudden reduced
   hashpower, without possibility of difficulty adjustment. users who
   don't uprade will try to do transactions, but won't see them confirm
   for hours or days due to lack of hashpower.

 - SPV nodes: they track the upgraded majority, everything works fine
   even if they don't upgrade

For a soft fork, again activated by the majority of upgraded hashpower,
the behaviour is:

 - upgraded bitcoin nodes: everything works fine

 - non-upgraded bitcoin miners willing to mine newly unacceptable txs:
   may produce orphaned blocks; may be able to be forced into producing
   blocks that will be orphaned

 - other non-upgraded bitcoin nodes: everything works fine

 - SPV nodes: partial breakage -- may track invalid blocks for 1-2
   confirmations until the set of "non-upgraded bitcoin miners willing
   to produce newly unacceptable txs" becomes vanishingly few.

In the hard fork case, all non-upgraded nodes get a DoS attack, but
aren't likey to be hit by doublespends. That's inconvenient, but it's
not too bad.

In the soft fork case, if there's likely to be old nodes mining
previously invalid transactions, SPV clients become very unreliable,
to the point of possibly seeing semi-regular double-spends with 1 or
2 confirmation, until miners that aren't paying attention notice their
blocks are getting orphaned and upgrade. That is pretty bad IMHO; and
there are a lot more *people* running SPV clients than bitcoin nodes,
so its impact is potentially worse in both ways.

Comparing generic hard forks versus generic soft forks, the above says
to me that a hard fork would be less harmful to users in general, and
thus a better approach.

*But* a soft fork that only forbids transactions that would previously
not have been mined anyway should be the best of both worlds, as it
automatically reduces the liklihood of old miners building newly invalid
blocks to a vanishingly small probability; which means that upgraded
bitcoin nodes, non-upgraded bitcoin nodes, /and/ SPV clients *all*
continuing to work fine during the upgrade.

AFAICS, that's what BIP65 achieves, as will similar OP_NOP* replacements
like BIP112.

But that only applies to a subset of potential soft forks, not every
soft fork.

Maybe a good way to think about it is something like this.  Consensus
(IsValid) is always less restrictive than (default) policy (previously
IsStandard, not sure how to summarise it now, maybe it's just OP_NOP
redefinition?).  So choosing a new consensus rule will be one of:

  * even less restrictive than consensus (hard fork)

  * more restrictive than consensus, but less restrictive than policy
    (safe soft fork)

  * more restrictive than IsStandard etc (damaging soft fork)

Hmm, in particular, following this line of thinking it's not clear to
me that BIP68 is actually less restrictive than current policy? At
least, I can't see anything that prevents txs with nSequence set to
something other than 0 or ~0 from being relayed?

If it's not, and nodes currently happily mine and relay transactions
with nSequence set without caring what it's set to, doesn't this mean
BIP68 is of the "damaging soft fork" variety? That is, if it activated
as a soft-fork with a majority of miners using it, but a minority of ~5%
not upgraded, then

 - someone could construct an tx with nSequence set to sometime in
   the future, but not using OP_CSV

 - this tx would get relayed by old nodes (but not upgraded nodes
   due to CheckLockTime)

 - non-upgraded miners would mine it into a block immediately, which
   would then get orphaned by majority hashpower

 - before it got orphaned, non-upgraded nodes and SPV clients would
   be misled and vulnerable to double spend attacks of txs with 0, 1 or
   maybe 2 confirmations

(BIP65 with OP_CLTV and BIP112 with OP_CSV don't have that problem as
they both redefine a non-standard opcode and would not get relayed or
mined by old, non-upgraded nodes, and are thus "safe soft forks" per
above terminology. This is just BIP68)

Can anyone confirm or refute the above?

Cheers,
aj


-------------------------------------
On Thu, Apr 16, 2015 at 9:12 AM, s7r <s7r@sky-ip.org> wrote:

The BIP 62 approach to malleability isn't the only option. Another
approach is to sign the transaction in such a way that the input
txid's are allowed to change without invalidating the signatures. That
way, if malleability happens, you just adjust you transaction to match
and re-broadcast. That proposal is here:

https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md

The "Build your own nHashType" thread on this mailing list contains
the discussion.

I personally prefer this solution, since it nails the problem
completely with one simple and obvious change. The BIP 62 approach is
more like a game of wac-a-mole.

-William


-------------------------------------
On Oct 2, 2015 10:03 AM, "Daniele Pinna via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
optimization be found.

This is demonstrably impossible: anything that can be done with software
can be done with hardware. This is computer science 101.
And specialized hardware can always be more efficient, at least
energy-wise.

On the other hand, BIP99 explicitly contemplates "anti-miner hardforks"
(obviously not for so called "ASIC-resistance" [an absurd term coined to
promote some altcoins], but just for restarting the ASIC and mining market
in case mining becomes too centralized).
-------------------------------------
This is good feedback. Thank you.

Very briefly:

The block chain is a database. There are laws to protect databases. We have
suggested who might be best placed to be assigned rights to the block chain
and more importantly why.

about saying you have a right to something and you give up those rights.
There are likely to be many examples where this could be applied, for
example - if you transact with someone and government agencies develop the
means to reveal your transaction, a licence gives protections which might
otherwise not be there in the absence of a licence. The MIT licence does
something similar - the Core developers give up their rights to revenue
from the software. Not wishing to go down rabbit hole, why not just remove
the MIT licence?

license to control data owned by someone else." ## It is up to us to
produce some guidance and context to assist with the BIP discussion
process. If anyone else has any suggestions on wording or access to legal
advice, that will be helpful.

to display the block at their web site" ## I would oppose any wording that
attempted to do anything of the sort. Bitcoin works because the block chain
is in the public domain. We have included references to royalty free use of
the data.

## The original reference client did everything. A block chain licence was
probably not envisioned. Mining has taken a different path from that which
was intended. Nevertheless, one needs to start somewhere. The proposal to
assign rights to miners is just that, a proposal.

I would just like to labour the point that users pay to use the network,
but they have no defined rights, anywhere.

Regards,

Ahmed

On Tue, Sep 1, 2015 at 11:42 PM, Milly Bitcoin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thu, Aug 20, 2015 at 12:25 AM, Gary Mulder via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

The potential impacts of Schism/controversial/contentious hardforks
are shortly covered in
https://github.com/bitcoin/bips/pull/181/files#diff-e331b8631759a4ed6a4cfb4d10f473caR137
It is still a BIP draft so improvements are welcomed.

-------------------------------------
On Friday 24. July 2015 05.37.30 Slurms MacKenzie via bitcoin-dev wrote:

I assume you mean that they don't have a Bitcoin Core node that is open to 
incoming connections. Since that is the only thing you can actually test, no?

Most companies are still terrified of accepting incoming connections from the 
scary Internet. So I'm not entirely surprised by this conclusion.  I would be 
very surprised if companies don't actually have a node at all.
-- 
Thomas Zander

-------------------------------------
On Fri, Jan 23, 2015 at 5:40 PM, slush <slush@centrum.cz> wrote:

I'm quite familiar with embedded development :), and indeed trezor MCU
is what I would generally consider (over-)powered which is why I was
somewhat surprised by the numbers; I'm certainly not expecting you to
perform dynamic allocation... but wasn't clear on how 40 minutes and
was I just trying to understand. Using a table to avoid retransmitting
reused transactions is just an optimization and can be done in
constant memory (e.g. falling back to retransmission if filled).

So what I'm understanding now is that you stream the transaction along
with its inputs interleaved in order to reduce the memory requirement
to two midstates and a value accumulator; requiring resending the
transaction... so in the worst case transaction (since you can't get
in more than about 800 inputs at the maximum transaction size) each
input spending from (one or more, since even one would be repeated)
100kb input transactions you might send about 800MBytes of data, which
could take a half an hour if hashing runs at 45KB/s or slower?

(If so, okay then there isn't another thing that I was missing).


-------------------------------------
It's written as 'a' and/or 'b'.  If you don't have idle hashpower, then
paying with difficulty requires some amount of collusion ('a')

Any miner paying with a higher difficulty either needs idle hashpower, or
self-increase their own difficulty at the possible *opportunity cost* of
losing an entire block's income to another miner who doesn't care about
changing the block size.  The potential loss does not economically
compensate for size increase gains in most cases, when you consider the
variability of blocks (they come in bursts and pauses) and the fee income
that would be associated.

Miners have more to lose paying with diff than they gain -- unless the
entire network colludes out-of-band with ~90% certainty, by collectively
agreeing to increase the block period by collectively agreeing with
pay-with-diff until the globally desired block size is reached.  At that
level of collusion, we can create far more simple schemes to increase block
size.

Pay-with-diff will either not get used, or lead to radical short term block
size (and thus fee) volatility.  It is complex & difficult for all players
to reason, and a Rational game theory choice can be to avoid
paying-for-diff even when the network desperately needs an upgrade.






On Thu, Sep 3, 2015 at 2:57 AM, Gregory Maxwell <gmaxwell@gmail.com> wrote:

-------------------------------------
On Fri, Jul 10, 2015 at 11:31 AM, Jeff Garzik <jgarzik@gmail.com> wrote:

True... there are two propagation thresholds... "-minrelaytxfee"
(defaults to 1000 sotoshi/kbyte) and "relaypriority" (defaults to
True).  If -relaypriority is True, then items with a priority above
57600000 (currently <ref1>) will still be relayed, even if their TxFee
is below MinRelayTxFee.

Therefore even if miners are using bitcoind rules for mempool tx
creation, they can still configure how and what they propagate.

The flip-side of this is that a transactions priority will go up the
longer it ages (in the mempool).  So it would be possible (if
relaypriority was on) for even a lowfee transaction to become
relayable eventually simply based on relaypriority

ref1: https://en.bitcoin.it/wiki/Transaction_fees

On Fri, Jul 10, 2015 at 12:02 PM, Justus Ranvier
<justus@openbitcoinprivacyproject.org> wrote:

If the recipient is running a full node with incoming connections.
I'm not sure if SPV clients rebroadcast both spend and receive
transactions.

-------------------------------------
On Tuesday, 20 January 2015, at 10:46 am, Peter Todd wrote:

If you have the private keys for your users' bitcoins, then you are every bit as much the owner of those bitcoins as your users are. There is no custodial relationship, as you have both the ability and the right to spend those bitcoins. Possession of a private key is equivalent to ownership of the bitcoins controlled by that private key.


-------------------------------------

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 04/27/2015 04:46 PM, Mike Hearn wrote:

I expect that mappings would begin to develop between payment codes
and government / real name identities, at least as far as that
businesses which are required to collect that kind of information
would associate it with the payment code(s) known to be used by their
customers for their own use.

I proposed payment codes in this form because I'd rather see that kind
of mapping be limited to the application layer and kept away from the
blockchain/network layer.

Even if it makes certain kind of application-layer distasteful
behavior easier, it's a good trade if doing so can simultaneously
provide resistance to graph analysis and make transaction-level
censorship more difficult.

- -- 
Justus Ranvier                   | Monetas <http://monetas.net/>
<mailto:justus@monetas.net>      | Public key ID : C3F7BB2638450DB5
                                 | BM-2cTepVtZ6AyJAs2Y8LpcvZB8KbdaWLwKqc
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBAgAGBQJVPmusAAoJECpf2nDq2eYjhxIP/3Jw9f6kcEsdFTXouQ5+D5gb
MjM8AW7EEA6KXj2PqrPv/H/brorW9/Ugcc8KweCjEdJAKOJV/Bl6sP5ydSZT6pmj
A0IFIkbdxKLY9JC3BbmVHuiAFrsL1u2EX5arUC3WNAWeWlVEmAL92cSlAka4BBxy
P/wh8xN0b4hsgA602Y4Btkv2fBHLQI9NMxW3AsujP3/S78mSxwKQZz4lYAMCowu8
NL/3toaFhrUsdHsH301jNAnxEEOodMVGmgjg/ZSdvWeHwdsE2J8Q9AJqiFDswjU5
q2kZuKmuJ6EXcGDlhelUuUpfHO34qS3/dyTydcqFrYB6eynZ8nV6S1SHaSlDEM10
b95+EpfIENtYdgAqJxwfbqpibpSEIW7cxCAopF0sSbQ2qv8rwRrcIah7KeARCrc0
e+HDcyLhYkrWrlK28vVmIxkEiQ/nmkTu9dOfoVJgXxcVl9AkiHGjo7QICOZHqfRB
TOupk9UUHMmdfZC5vpj9rd+VSXJJEF19ZbGF1QsFSMuxjKTb9jAy7Dk6U/9/xK9Q
+mH6QHhKzNKb8GsiowZJq3bF2mEYqmh/BPyQ06gfDLM4yvlTb+k4R6brFzm7tkWG
49hREmHK9w/wZXnH0lMCqMHRY/YqQF5bR3ujq7pB0WHLvbvDoSvyWvGQ9cVrRA24
ASb47sR77R1LlZntoSyy
=b7HG
-----END PGP SIGNATURE-----
-------------------------------------
OpenSSL 1.0.0p / 1.0.1k was recently released and is being
pushed out by various operating system maintainers.  My review
determined that this update is incompatible with the Bitcoin
system and could lead to consensus forks.

Bitcoin Core released binaries from Bitcoin.org are unaffected,
as are any built with the gitian deterministic build system.

If you are running third-party or self-compiled Bitcoin Core
or an alternative implementation using OpenSSL you must not
update OpenSSL or must run a Bitcoin software containing a
workaround:

https://github.com/bitcoin/bitcoin/commit/488ed32f2ada1d1dd108fc245d025c4d5f252783
(versions of this will be backported to other stable branches soon)

The tests included with Bitcoin Core in the test_bitcoin
utility already detect this condition and fail.  (_Do not ignore or
disable the tests in order to run or distribute software
which fails_)

The incompatibility is due to the OpenSSL update changing the
behavior of ECDSA validation to reject any signature which is
not encoded in a very rigid manner.  This was a result of
OpenSSL's change for CVE-2014-8275 "Certificate fingerprints
can be modified".

While for most applications it is generally acceptable to eagerly
reject some signatures, Bitcoin is a consensus system where all
participants must generally agree on the exact validity or
invalidity of the input data.  In a sense, consistency is more
important than "correctness".

As a result, an uncontrolled 'fix' can constitute a security
vulnerability for the Bitcoin system.  The Bitcoin Core developers
have been aware of this class of risk for a long time and have
taken measures to mitigate it generally; e.g., shipping static
binaries, internalizing the Leveldb library... etc.

It was somewhat surprising, however, to see this kind of change show
up as a "low" priority fix in a security update and pushed out live
onto large numbers of systems within hours.

We were specifically aware of potential hard-forks due to signature
encoding handling and had been hoping to close them via BIP62 in 0.10.
BIP62's purpose is to improve transaction malleability handling and
as a side effect rigidly defines the encoding for signatures, but the
overall scope of BIP62 has made it take longer than we'd like to
deploy.

(Coincidentally, I wrote about this concern and our unique demands on
 cryptographic software as part of a comment on Reddit shortly before
 discovering that part of this OpenSSL update was actually
 incompatible with Bitcoin:
 https://www.reddit.com/r/Bitcoin/comments/2rrxq7/on_why_010s_release_notes_say_we_have_reason_to/cnitbz3
)

The patches above, however, only fix one symptom of the general
problem: relying on software not designed or distributed for
consensus use (in particular OpenSSL) for consensus-normative
behavior.  Therefore, as an incremental improvement, I propose
a targeted soft-fork to enforce strict DER compliance soon,
utilizing a subset of BIP62.

Adding a blockchain rule for strict DER will reduce the risk of
consensus inconsistencies from alternative implementations of
signature parsing or signature verification, simplify BIP62,
and better isolate the cryptographic validation code from the
consensus algorithm. A failure to do so will likely leave us
in this situation, or possibly worse, again in the future.

The relevant incompatible transactions are already non-standard on
the network since 0.8.0's release in February 2013, although there
was seemingly a single miner still mining incompatible transactions.
That miner has been contacted and has fixed their software, so a
soft-fork with no chain forking should be possible.


-------------------------------------
Taking the hash of the secret would then require an extra step to make sure
the hash is valid for secp256k1.

Using the x value directly avoids the need for that check.

On Fri, Apr 24, 2015 at 10:35 PM, Patrick Mccorry (PGR) <
patrick.mccorry@newcastle.ac.uk> wrote:

-------------------------------------
Comments:

1) cblock seems a reasonable way to extend the protocol.  Further wrapping
should probably be done at the stream level.

2) zlib has crappy security track record.

3) A fallback path to non-compressed is required, should compression fail
or crash.

4) Most blocks and transactions have runs of zeroes and/or highly common
bit-patterns, which contributes to useful compression even at smaller
sizes.  Peter Ts's most recent numbers bear this out.  zlib has a
dictionary (32K?) which works well with repeated patterns such as those you
see with concatenated runs of transactions.

5) LZO should provide much better compression, at a cost of CPU performance
and using a less-reviewed, less-field-tested library.





On Tue, Nov 10, 2015 at 11:30 AM, Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
The SIGHASH_WITHINPUTVALUE proposal is a hardfork, but otherwise
non-intrusive, doesn't change any TxOut scripts, doesn't change any
tx/block parsing (besides verification), it works with all existing
coins in the network, and existing software doesn't have to use it if
they don't want to upgrade their signers.   The proposal simply provides
a way to optionally sign the input values with the TxOut scripts.  In
other words a signature right now says "I sign this transaction using
these inputs, whatever value they are."  With this SIGHASH type, the
signature says "I sign this transaction assuming that input 0 is X BTC,
input 1 is Y BTC,....".  If the online computer providing the data to be
signed lies about the value of any input, the resulting signature will
be invalid.

Unfortunately, it seems that there was no soft-fork way to achieve this
benefit, at least not one that had favorable properties.  Most of the
soft-fork variations of it required the coins being spent to have been
originated in a special way.  In other words, it would only work if the
coins had entered the wallet with some special, modified TxOut script. 
So it wouldn't work with existing coins, and would require senders to
update their software to reshape the way they send transactions to be
compatible with our goals.

I *strongly* encourage this to be considered for inclusion at some
point.  Not only does it simplify HW as Marek suggested, it increases
the options for online-offline communication channels, which is also a
win for security.  Right now, QR codes don't work because of the
possibility of having to transfer megabytes over the channel, and no way
to for the signer to control that size.  With this change, it's possible
for the signer to control the size of each chunk of data to guarantee it
fits in, say, a QR code (even if it means breaking it up into a couple
smaller transactions).

-Alan



On 01/23/2015 09:51 AM, slush wrote:

-------------------------------------
Hi Dave,

Thank you for the feedback regarding my paper.  


With the benefit of hindsight, I think the paper would be stronger if it also analyzed how the model changes (or doesn't) if we assume zero propagation impedance for intra-miner communication, as you suggested (the "you don't orphan your own blocks" idea).  Note that the paper did briefly discuss miner-dependent propagation times in the second paragraph of page 9 and in note 13.  


Agreed.  In this case there's no information to communicate (since the miner has no peers) and so the Shannon-Hartley limit doesn't apply.  My model makes no attempt to explain this case.  


I'd like to explore this in more detail.  Although a miner may not orphan his own block, by building on his own block he may now orphan two blocks in a row.  At some point, his solution or solutions must be communicated to his peers.  And if there's information about the transactions in his blocks to communicate, I think there's a cost associated with that.  It's an interesting problem and I'd like to continue working on it.  


It will be interesting to see.  I suspect that the main result that "a healthy fee market exists" will still hold (assuming of course that a single miner with >50% of the hash power isn't acting maliciously).  Whether miners with larger value of h/H have a profit advantage, I'm not sure (but that was outside the scope of the paper anyways).  

Best regards,
Peter




-------------------------------------
Greg,


Sorry, for not being clear. I am not talking definitions here, of course you can call it "controversial" when you get N-1 NACK's!

I object that it's enough evidence to deny any change (see below). For example, in case the interests of developers became misaligned with the interests of the community (you can't say it can't happen).


Wladimir,


Why the "entire network"? So if, say, 75% of everybody involved want some change and 25% don't, the majority can't have it?

Well, I guess we're down to that philosophical question of whether majority can dictate minority or whether minority can be a roadblock to majority :)

Probably no reason to discuss it further :) A "software fork" seems like an inevitable resolution for this.


-------------------------------------
How do you explain to end users that a "validated" transaction can instantly become completely unspendable by a mined block? This seems like setting up people to just be Finney attacked even more.



-------------------------------------
[collating a private mail and a github issue comment, moving it to a
better forum]

On libconsensus
---------------
In general there exists the reasonable goal to move consensus state
and code to a specific, separate lib.

To someone not closely reviewing the seemingly endless stream of
libconsensus refactoring PRs, the 10,000 foot view is that there is a
rather random stream of refactors that proceed in fits and starts
without apparent plan or end other than a one sentence "isolate
consensus state and code" summary.

I am hoping that
* There is some plan
* We will not see a five year stream of random consensus code movement
patches causing lots of downstream developer headaches.

I read every code change in every pull request that comes into
github/bitcoin/bitcoin with three exceptions:
* consensus code movement changes - too big, too chaotic, too
frequent, too unfocused, laziness guarantees others will inevitably
ACK it without me.
* some non-code changes (docs)
* ignore 80% of the Qt changes

As with any sort of refactoring, they are easy to prove correct, easy
to reason, and therefore quick and easy to ACK and merge.

Refactors however have a very real negative impact.
bitcoin/bitcoin.git is not only the source tree in the universe.
Software engineers at home, at startups, and at major companies are
maintaining branches of their own.

It is very very easy to fall into a trap where a project is merging
lots of cosmetic changes and not seeing the downstream ripple effects.
Several people complained to me at the conference about all the code
movement changes breaking their own work, causing them to stay on
older versions of bitcoin due to the effort required to rebase to each
new release version - and I share those complaints.

Complex code changes with longer development cycles than simple code
movement patches keep breaking.  It is very frustrating, and causes
folks to get trapped between a rock and a hard place:
- Trying to push non-trivial changes upstream is difficult, for normal
and reasonable reasons (big important changes need review etc.).
- Maintaining non-trivial changes out of tree is also painful, for the
aforementioned reasons.

Reasonable work languishes in constant-rebase hell, and incentivizes
against keeping up with the latest tree.


Aside from the refactor, libconsensus appears to be engineering in the
dark.  Where is any sort of plan?  I have low standards - a photo of a
whiteboard or youtube clip will do.

The general goal is good.   But we must not stray into unfocused
engineering for a non-existent future library user.

The higher priority must be given to having a source code base that
maximizes the collective developers' ability to maintain The Router --
the core bitcoin full node P2P engine.

I recommend time-based bursts of code movement changes.  See below;
for example, just submit & merge code movement changes on the first
week of every 2nd month.  Code movement changes are easy to create
from scratch once a concrete goal is known.  The coding part is
trivial and takes no time.

As we saw in the Linux kernel - battle lessons hard learned - code
movement and refactors have often unseen negative impact on downstream
developers working on more complicated changes that have more positive
impact to our developers and users.


On Bitcoin development release cycles & process
------------------------------------------------------------------

As I've outlined in the past, the Linux kernel maintenance phases
address some of these problems.  The merge window into git master
opens for 1 week, a very chaotic week full of merging (and rebasing),
and then the merge window closes.  Several weeks follow as the "dust
settles" -- testing, bug fixing, moving in parallel OOB with
not-yet-ready development.  Release candidates follow, then the
release, then the cycle repeats.

IMO a merge window approach fixes some of the issues with refactoring,
as well as introduces some useful -developer discipline- into the
development process.  Bitcoin Core still needs rapid iteration --
another failing of the current project -- and so something of a more
rapid pace is needed:
- 1st week of each month, merge changes.  Lots of rebasing during this week.
- remaining days of the month, test, bug fix
- release at end of month

If changes are not ready for merging, then so be it, they wait until
next month's release.  Some releases have major features, some
releases are completely boring and offer little of note.  That is the
nature of time-based development iteration.  It's like dollar cost
averaging, a bit.


And frankly, I would like to close all github pull requests that are
not ready to merge That Week.  I'm as guilty of this as any, but that
stuff just languishes.  Excluding a certain category of obvious-crap,
pull requests tend to default to a state of either (a) rapid merging,
(b) months-long issues/projects, (c) limbo.

Under a more time-based approach, a better pull request process would be to
* Only open pull requests if it's a bug fix, or the merge window is
open and the change is ready to be merged in the developer's opinion.
* Developers CC bitcoin-dev list to discuss Bitcoin Core-bound projects
* Developers maintain and publish projects via their own git trees
* Pull requests should be closed if unmerged after 7 days, unless it
is an important bug fix etc.

The problem with projects like libconsensus is that they can get
unfocused and open ended.  Code movement changes in particular are
cheap to generate.  It is low developer cost for the developer to
iterate all the way to the end state, see what that looks like, and
see if people like it.  That end state is not something you would
merge all in one go.  I would likely stash that tree, and then start
again, seek the most optimal and least disruptive set of refactors,
and generate and merge those into bitcoin/bitcoin.git in a time-based,
paced manner.  Announce the pace ahead of time - "cosmetic stuff that
breaks your patches will be merged 1st week of every second month"

To underscore, the higher priority must be given to having a source
code base and disciplined development process that maximizes the
collective developers' ability to maintain The Router that maintains
most of our network.

Modularity, refactoring, cleaning up grotty code generates a deep
seated happiness in many engineers.  Field experience however shows
refactoring is a never ending process which sometimes gets in the way
of More Important Work.

-------------------------------------
On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev 
<bitcoin-dev@lists.linuxfoundation.org 
<mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
There's that, but there's also a case where an attacker creates a 
majority chain that follows the old rules but not the new ones. 
Non-upgraded nodes would accept a transaction on what they believe to be 
the consensus chain only to find that when they try to spend those coins 
no one accepts them because they were part of an invalid chain.

This has the effect of dropping non upgraded nodes to a form of spv 
security without their consent.

This is in contrast to a hard fork where a full node operator could 
explicitly set their node to accept higher version blocks that it can't 
validate. They get the soft fork functionality back but they have at 
least consented to it rather than have it forced on them. Doing forks 
that way would also have the benefit of notifying the user they are 
accepting unvalidated coins, whereas they wont know that in a soft fork.
-------------------------------------
On Wed, Jul 22, 2015 at 11:03 AM, Alex Morcos <morcos@gmail.com> wrote:


There is a vast difference between what software developers have been
chattering about in the background versus what the users actually
experience in the field.

To the user, talk of a fee market is equivalent to talk about block size -
various opinions are tossed about, but it doesn't really impact them.  Fees
have been low for 6 years.

We see this with the actual data - no fee pressure on average for the
entirety of bitcoin's history.  We see this with the recent stress tests,
which exposed dumb wallet behavior WRT fees.   Users -and software- had the
expectation

Remember, this is not a judgement on whether or not fee market/pressure
should exist.  It is simply a factual observation that users/market have
not experienced this new economic policy.

That opens the question - *why now?*   Why make bitcoin growth more
expensive at this time in its young life?  Many smart people would prefer
that bitcoin continue to grow, rather than making the system more expensive
to use right now.

Choosing "let a fee market develop" -- *today* -- is picking economic
sides, picking winners & losers in the market.

This new policy should be debated and consensus achieved, not simply rolled
out by fiat without user notification.

Otherwise it is engaging in precisely the economic wizardry that this
thread opened with decrying.

Just like block size, there are multiple sides to the fee market debate.
However, Bitcoin Core has (unfortunately) outsized decision making power in
that simply avoiding progress on block size limit will achieve the "let a
fee market develop" economic policy change.  Ironic but true - sitting
around and doing nothing dumps users into a new economic policy.
-------------------------------------
On Thursday, November 12, 2015 9:21:57 PM Alex Morcos wrote:

That's what unit tests are for. :)


I'm not sure what you're getting at here, but rebroadcasting won't work if 
they're still in the memory pools (unless we open the door to DoS from 
reprocessing the same tx over and over).

Luke

-------------------------------------
Probably out of my league, but I will respond here anyway.

I am in favor of replace-by-fee, but only if it were to be applied to a very limited subset of transactions: namely, transactions that seek to supplement, not replace, the original transaction.

In other words, a replacement transaction would only be accepted if it were adding additional value (additional transaction inputs and/or outputs).  Otherwise, the original transaction would stand.  Reducing any of the promised outputs, or diverting them to some other recipient, would not be allowed.

This would solve the problem of a stuck transaction: a transaction that is taking seemingly forever to confirm, because one forgot to pay the miner’s fee, something that happened to me once.

Stuck transactions are bad, both for the recipient (they aren’t getting paid) and the sender (some of their funds are still tied up, because change from that transaction has not returned yet).

With replace-by-fee, the sender of a transaction can send it again, with additional inputs (to pay more miner’s fees) and additional outputs (to receive the change, if any is desired, from those inputs).  So, now the sender is self-empowered to “shove through” their stuck transaction, by voluntarily choosing to pay more for it, a market-driven solution to the problem.

There are really good reasons to not allow replace-by-fee as a general policy that can apply to all transactions, as others have already mentioned.  However, I still believe the idea has merit, for this special limited case of supplementing a transaction.

Josh Lehan





-------------------------------------
On Fri, May 29, 2015 at 1:39 PM, Gavin Andresen <gavinandresen@gmail.com>
wrote:


How do you define that the movement is successful?

For



The measure is miner consensus.  How do you intend to measure
exchange/merchant acceptance?
-------------------------------------
On Fri, Sep 18, 2015 at 2:07 AM, Wladimir J. van der Laan via
bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

I agree with this long term vision.
Here's how I think it could happen:

1) Libconsensus is completed and moved to a subtree (which has libsecp
as an internal subtree)

2) Bitcoind becomes a subtree of bitcoin-wallet (which has
bitcoin-wallet and bitcoin-qt)

Without aggressively changing it for this purpose, libconsensus should
tend to become C, like libsecp, which is better for proving
correctness.
Hopefully at some point it won't take much to move to C.

Upper layers should move to C++11

Don't focus on the git subtrees, the basic architecture is bitcoin-qt
on top of bitcoin-wallet, bitcoin-wallet on top of bitcoind (and
friends like bitcoin-cli and bitcoin-tx), bitcoind on top of
libconsensus on top of libsecp256k1.

I believe this would maximize the number of people who can safely
contribute to the project.
I also believe this is the architecture most contributors have in mind
for the long term, but I may be wrong about it.

Criticisms to this plan?

-------------------------------------
CPFP is interesting, but it does not fully cover the case it is trying to
address:   If TX_a goes out without sufficient fee, sending out a new TX_b
will not help TX_a suddenly reach nodes/miners that ignored TX_a.


On Fri, Jul 10, 2015 at 12:09 PM, Richard Moore <me@ricmoo.com> wrote:

-------------------------------------


Yes, but the block maker won't publish the second block it finds for the same set of transactions. It won't orphan its own block. In fact even if it does it still doesn't matter because the block maker still gets the block reward irrespective of which of the two solutions are published.

It's not about which hash wins, the issue is who gets paid as a result.

-------------------------------------
(My apologies for a 'drive-by' posting. I'm not subscribed to this mailing
list but this post may be of interest here. If you'd like to make sure I
see a response send it to me directly. This post was originally posted to
the web at
https://medium.com/@bramcohen/how-wallets-can-handle-transaction-fees-ff5d020d14fb
 )

Since transaction fees are a good thing (see
https://medium.com/@bramcohen/bitcoin-s-ironic-crisis-32226a85e39f ), that
brings up the question: How should wallets handle them? This essay is an
expansion of my talk at the bitcoin scaling conference (see
https://www.youtube.com/watch?v=iKDC2DpzNbw&t=13m17s and
https://scalingbitcoin.org/montreal2015/presentations/Day1/11-bram_wallet_fees.pdf
 ).

Ground Rules

To answer this question we first need to lay down some ground rules of what
we’re trying to solve. We’ll focus on trying to solve the problem for
consumer wallets only. We’ll be ignoring microchannels, which dramatically
reduce the number of transactions used but still have to put some on the
blockchain. We’ll also be assuming that full replace by fee is in effect
(see
https://medium.com/@bramcohen/the-inevitable-demise-of-unconfirmed-bitcoin-transactions-8b5f66a44a35
)
because the best solution uses that fairly aggressively.

What should transaction fees be?

Before figuring out how wallets should calculate transaction fees, we first
need to know what transaction fees should be. The obvious solution to that
question is straightforward: It should be determined by supply and demand.
The price is set at the point where the supply and demand curves meet. But
supply and demand curves, while mostly accurate, are a little too simple of
a model to use, because they don’t take into account time. In the real
world, the supply of space for transactions is extremely noisy, because
more becomes available (and has to be immediately consumed or it’s lost
forever) every time a block is minted, and block minting is an
intentionally random process, that randomness being essential for
consensus. Demand is random and cyclical. Random because each transaction
is generated individually so the total amount is noisy (although that
averages out to be somewhat smooth at scale) and has both daily and weekly
cycles, with more transactions done during the day than at night.

What all these result in is that there should be a reward for patience. If
you want or need to get your transaction in quicker you should have to pay
on average a higher fee, and if you’re willing to wait longer it should on
average cost less. Inevitably this will result in transactions taking on
average longer than one block to go through, but it doesn’t require it of
everyone. Those who wish to offer high fees to be sure of getting into the
very next block are free to do so, but if everyone were to do that the
system would fall apart.

What should the wallet user interface be?

Ideally transaction fees would be handled in a way which didn’t require
changes to a wallet’s user interface at all. Unfortunately that isn’t
possible. At a minimum it’s necessary to have a maximum fee which the user
is willing to spend in order to make a transaction go through, which of
course means that some transactions will fail because they aren’t willing
to pay enough, which is the whole point of having transaction fees in the
first place.

Because transaction fees should be lower for people willing to wait longer,
there should be some kind of patience parameter as well. The simplest form
of this is an amount of time which the wallet will spend trying to make the
transaction go through before giving up (Technically it may make sense to
specify block height instead of wall clock time, but that’s close enough to
not change anything meaningful). This results in fairly understandable
concepts of a transaction being ‘pending’ and ‘failed’ which happen at
predictable times.

Transactions eventually getting into a ‘failed’ state instead of going into
permanent limbo is an important part of the wallet fee user experience.
Unfortunately right now the only way to make sure that a transaction is
permanently failed is to spend its input on something else, but that
requires spending a transaction fee on the canceling transaction, which of
course would be just as big as the fee you weren’t willing to spend to make
the real transaction go through in the first place.

What’s needed is a protocol extension so a transaction can make it
impossible for it to be committed once a certain block height has been
reached. The current lack of such an extension is somewhat intentional
because there are significant potential problems with transactions going
bad because a block reorganization happened and some previously accepted
transactions can’t ever be recommitted because their max block height got
surpassed. To combat this, when a transaction with a max block height gets
committed near its cutoff it’s necessary to wait a longer than usual number
of blocks to be sure that it’s safe (I’m intentionally not giving specific
numbers here, some developers have suggested extremely conservative
values). This waiting is annoying but should only apply in the edge case of
failed transactions and is straightforward to implement. The really big
problem is that given the way Bitcoin works today it’s very hard to add
this sort of extension. If any backwards-incompatible change to Bitcoin is
done, it would be a very good idea to use that opportunity to improve
Bitcoin’s extension mechanisms in general and this one in particular.

What information to use

The most obvious piece of information to use for setting transaction fees
is past transaction fees from the last few blocks. This has a number of
problems. If the fee rate goes high, it can get stuck there and take a
while to come down, if ever, even though the equilibrium price should be
lower. A telltale sign of this is high fee blocks which aren’t full, but
it’s trivial for miners to get around that by padding their blocks with
self-paying transactions. To some extent this sort of monopoly pricing is
inherent, but normally it would require a cabal of most miners to pull it
off, because any one miner can make more money in the short term by
accepting every transaction they can instead of restricting the supply of
available transaction space. If transaction fees are sticky, a large but
still minority miner can make money for themselves even in the short term
by artificially pumping fees in one of their blocks because fees will
probably still be high by the time of their next block.

Past fees also create problems for SPV clients, who have to trust the full
nodes they connect to to report past fees accurately. That could be
mitigated by making an extension to the block format to, for example,
report what the minimum fee per bytes paid in this block is in the headers.
It isn’t clear exactly what that extension should do though. Maybe you want
to know the minimum, or the median, or the 25th percentile, or all of the
above. It’s also possible for miners to game the system by making a bunch
of full nodes which only report blocks which are a few back when fees have
recently dropped. There are already some incentives to do that sort of bad
behavior, and it can be mitigated by having SPV clients connect to more
full nodes than they currently do and always go with the max work, but SPV
clients don’t currently do that properly, and it’s unfortunate to create
more incentives for bad behavior.

Another potential source of information for transaction fees is currently
pending transactions in the network. This has a whole lot of problems. It’s
extremely noisy, much more so than regular transaction fees, because (a)
sometimes a backlog of transactions builds up if no blocks happen to have
happened in a while (b) sometimes there aren’t many transactions if a bunch
of blocks went through quickly, and (c) in the future full nodes can and
should have a policy of only forwarding transactions which are likely to
get accepted sometime soon given the other transactions in their pools.
Mempool is also trivially gameable, in exactly the same way as the last few
blocks are gameable, but worse: A miner who wishes to increase fees can run
a whole lot of full nodes and report much higher fees than are really
happening. Unlike with fee reporting in blocks, there’s no way for SPV
clients to audit this properly, even with a protocol extension, and it’s
possible for full nodes to lie in a much more precise and targetted manner.
Creating such a strong incentive for such a trivial and potentially
lucrative attack seems like a very bad idea.

A wallet’s best information to use when setting price are the things which
can be absolutely verified locally: The amount it’s hand to pay in the
past, the current time, how much it’s willing to pay by when. All of these
have unambiguous meanings, precise mathematical values, and no way for
anybody else to game them. A wallet can start at a minimum value, and every
time a new block is minted which doesn’t accept its transaction increase
its fee a little, until finally reaching its maximum value at the very end.
Full nodes can then follow the behavior of storing and forwarding along
several blocks’s worth of transactions, ten times sounds reasonable,
ignoring transactions which pay less per byte than the ones they have
stored, and further requiring that a new block be minted between times when
a single transaction gets replaced by fee. That policy both has the
property of being extremely denial-of-service resistant and minimizing the
damage to zeroconf. (Zeroconf is a bad idea, but if something is a good
idea to do for other reasons reducing the pain to those stuck with zeroconf
is a nice bonus.)

An actual formula

At long last, here is the formula I advocate using:

Pick a starting point which is de minimis for your first transaction or 1/2
(or less, configurable) your last fee paid if you’ve sent coin before

Let B = max number of blocks from start before giving up, S = starting fee,
M = max fee

For each new block at height H from the start, post a new transaction with
fee e^(lg(S) + (lg(M) — lg(S)) * H/B)

To avoid artifacts when multiple wallets use the same magic numbers, do
this before the first block: pick V uniformly in [0, 1], let S = e^(lg(S) +
(lg(M) — lg(S)) * (V/(V+B)))

The very first time you send coin it makes sense to give it a longer time
to do the transaction because it’s starting from a very low value and you
don’t want to way overshoot the amount necessary. But if you start from the
standard absolute minimum fee in Bitcoin and put the maximum time at
several hours it will increase by less than 10% per block, so exponential
growth is on your side.

It might be reasonable to, for example, start at a value which is a
discount to the minimum paid in the last block if that value is less than
what you would start with otherwise and if there’s a protocol extension to
put that information in the block headers. Such possibilities should be
studied and discussed more, but the formula I gave above should be the
default starting point if you simply want something which works and is
conservative and reliable.

Sidebar: Handling utxo combining

Whenever a wallet makes a payment, it needs to decide how to structure the
inputs and outputs of the new transaction. Generally the output consists of
two utxos, one of them going to the recipient and one of them going back
into the original wallet. Which input or inputs to use is less clear.
Usually an attempt is made to optimize for anonymity, or at least leaking
as little information as possible, and there’s usually a comment in the
code saying what amounts to ‘I can’t clearly justify any particular
strategy here but this is what I’m doing’.

When there are real transaction fees, one might consider trying to optimize
utxo combining for fees. The strategy used turns out to matter surprisingly
little for fees in the long run. For every separate utxo in your wallet,
you’ll eventually have to pay the fee to combine it with something else,
and the amount of increase in fee will be the same regardless of whether
you do it in the current transaction or a later transaction. It does make
sense to include more inputs in earlier versions of a payment though,
because the fees at that time are lower, and drop them in later versions
once the fees have gone up, in the hopes that the utxo consolidation can be
done for cheaper in some later transaction. It may also make sense to do
completely separate purely consolidation transactions with no external
output during off-peak times. That puts more bytes on the blockchain
because of the unnecessary intermediary value it generates though, so there
needs to be a significant difference in fees between peak and off-peak
times for it to make sense. Both of those techniques have significant and
unclear privacy implications and should be studied more.

There are also signing tricks which could potentially save significant
amounts of bytes on the blockchain, thus lowering fees. The most elegant
would be to create a new extension so that when there are multiple inputs
to a transaction which all use Schnorr the signature can be a single
combination signature instead of separate signatures for each of them. This
has very little downside and I’m strongly in favor of it being done.

A simpler, less elegant trick which saves more bytes would be to allow
multiple inputs to the same transaction which use the same key to only
result in a single signature. This lowers privacy, because it gives away
the association between utxos before they’re consolidated, but if used
properly would only push back that reveal a little bit. The danger is that
wallets would instead use it improperly and use the same key all the time,
which would always save as many bytes as possible but be a privacy disaster.

A trick which is a just plain bad idea, although it would save even more
bytes, would be not count the bytes of the reveal of a p2sh script to count
if that exact same script has ever been used before. This is clearly a bad
idea, because it directly encourages extremely privacy-averse behavior, and
because it necessitates a data structure of all p2sh scripts which have
ever been done before for validation, which is quite large and costly to
maintain.
-------------------------------------
What do other people think?


If we can't come to an agreement soon, then I'll ask for help
reviewing/submitting patches to Mike's Bitcoin-Xt project that implement a
big increase now that grows over time so we may never have to go through
all this rancor and debate again.

I'll then ask for help lobbying the merchant services and exchanges and
hosted wallet companies and other bitcoind-using-infrastructure companies
(and anybody who agrees with me that we need bigger blocks sooner rather
than later) to run Bitcoin-Xt instead of Bitcoin Core, and state that they
are running it. We'll be able to see uptake on the network by monitoring
client versions.

Perhaps by the time that happens there will be consensus bigger blocks are
needed sooner rather than later; if so, great! The early deployment will
just serve as early testing, and all of the software already deployed will
ready for bigger blocks.

But if there is still no consensus among developers but the "bigger blocks
now" movement is successful, I'll ask for help getting big miners to do the
same, and use the soft-fork block version voting mechanism to (hopefully)
get a majority and then a super-majority willing to produce bigger blocks.
The purpose of that process is to prove to any doubters that they'd better
start supporting bigger blocks or they'll be left behind, and to give them
a chance to upgrade before that happens.


Because if we can't come to consensus here, the ultimate authority for
determining consensus is what code the majority of merchants and exchanges
and miners are running.


-- 
--
Gavin Andresen
-------------------------------------
On Wednesday, October 21, 2015 7:39:45 AM Christian Decker wrote:

Signer malleability is still a notable concern needing consideration. Ideally, 
wallets should be trying to actively CoinJoin, bump fees on, etc any pending 
transactions in the background. These forms of malleability affect nearly as 
many real use cases as third-party malleability.

Luke

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Tell you what, eloquent guy...

Give me 15 minutes in a public open mic session with you and i'll
remove you from your high horse and close your voice in Bitcoin, for
good.

Guaranteed. You're too stupid for me to let you run loose with client
funds and this great innovation.

Anytime, anywhere. I'm ready to dismantle your intellectual bankruptcy
in front of the world.

I'll go for your psychological throat first.

Sincerely,
Venzen Khaosan.



On 10/05/2015 11:56 PM, Mike Hearn via bitcoin-dev wrote:
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBAgAGBQJWFBGjAAoJEGwAhlQc8H1mn2cH/0pTx1C0FK8shPSPaC3xB6sA
DpGTMrLWNai3i9VTwkUw8UvbqeL2QtZDghPdkDcvbmvOMc3UrOMQbc1eQ1eL6i3g
DiUCqUShOIAIvWJXGPTPNBulWBW9VkgK0y3uOprTd5D0VWKpWvDj+DMNqHaAC2Ab
JAfHx0mHlkTfrcBl30eAJWxoqG/ohu5QvTIP64AsK6w53qlbMcB13cES8mS/HJX9
MUtBcCbYRfF3Gu+OeYaEzzzXeuwsqql9qHr2wZYe9rECkSmYgL0DT5+WZiLY8B/x
E3dFtufR7yAHr91/gj9itOKf+unumhduX8LY8ubuIKmuwjdj30MDdNy7fqZ3uGs=
=lftV
-----END PGP SIGNATURE-----

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On 2015-06-19 17:40, Jeff Garzik wrote:

Is there not a dedicated field in a transaction (nSequence) for express 
purpose of indicating when a protocol like this is in use?

As far as I know, transactions which are using those protocols can be 
easily differentiated from those that aren't (which is probably good 
from a payment assurance standpoint and bad from a privacy standpoint).

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCgAGBQJVhFW3AAoJECpf2nDq2eYjkegQAINcnzIPbO/bKqNv14TOonb8
9g/pfvMSQyZjUiu4rB6Iwtn+h1hyLkPc3cdSFV4diQSWeG7Q27ZJzH1T1kYdKp4W
DDH8DwD8PtOu+dgRK9eBsy9h72OncA4JTFhnAXMgfLVBY9eRqXk/DWlzwV/WOn/j
3G5xKKOOeHmKJCaKFwdpZghraLouS72AKSdxCNvleRc4zllV+zqWyHHssNDg7sGH
b/62O3DBZXdlzIEzK8/IeaNMY+UXd984/yQ8KCrHCKjc9uiUjNUCCw4JPo4rB/ZA
Itoc8b6pexRs8h40FdXGYAwvN5xQgcaOL7SsN2nNx/DQWYf+1krBO8Iy4kYw2KGl
8JctcHBOI2gLCxTpB2cWeGPwBQbKJhPsmxTxaTNw5fC6ycAnjoJ2bO1Uz0KfwdnI
2jmwxccB9KauC9zNthxGbvdsHOxE8foZ6AnSDI/qbYQK6MqtSnsa7BUn8vc/y4Uf
bVCsxiywVlHttCJqPh5v16rejCcH2el5Rd5PVCkEagxYFfLA3681ZJKD22UV742l
n8ii7RUJXeps6zjRAc35Ccj5qjhB4SP4qYvKmyEoltYbw1EwXbm93UCsFpuxmQ9g
GbQ/jZXsB1cmHBC+c+3X6SaU6eZdy2jDHsICP7sMx2CrZpcZZO058bqRbmdk8JE6
JI17MYG0ofTLfdCfkgEG
=en1q
-----END PGP SIGNATURE-----



-------------------------------------
Google Docs formatted version:
https://docs.google.com/document/d/1t3kGkAUQ-Yui57P29YhDll5WyJuTiGrUhCW8so-E-iQ/edit?usp=sharing


Meeting Title:
#bitcoin-dev Weekly Development Meeting
Meeting Date:
2015-10-29
Meeting Time:
19:00-20:00 UTC

Participants in Attendance:
dstadulis
morcos
sipa
jgarzik
rusty
warren
jeremyrubin
evoskuil
Luke-Jr
dcousens
gmaxwell
jtimon
mcelrath
btcdrak

IRC Chat Logs:
http://bitcoinstats.com/irc/bitcoin-dev/logs/2015/10/29#l1446145135.0

--------------------------------------------------------------------------------

Topics discussed:

1. Upcoming softfork
1.1 Solely CLTV (morcos, petertodd, dcousens)
1.2 Softfork coordination with other clients
2. Chain Limits Agreement Status
2.1 What should be sufficient consensus for merges?
3. Backporting Policy
4. Leveldb Replacement
4.1 Can be considered when code is abstracted, allows for testing,
alternative implementations exist. Testing encouraged, no future moves
planned.
5. Clang format
5.1 History review:  Proposal a while ago was to clang-format file set <a b
c ...>   Once done, maintain those files' formatting with automation (git
hook checks or whatnot)
5.2 Clang format behavior changes "randomly" from version to version.
6. BIP-68: “Mempool-only sequence number constraint verification”
Implementation PR #6312
6.1 Concern regarding skipping missing inputs
7. BIP-112: Mempool-only CHECKSEQUENCEVERIFY PR #6564


2015-10-29 Meeting Conclusions:

#
Action items
Responsible Parties
ETA/Due Date
1
Morcos to report chain stats


2
Review BIP68 implementation #6312
sipa, rusty

--------------------------------------------------------------------------------

Meetingbot Minutes
Minutes(HTML)
http://www.erisian.com.au/meetbot/bitcoin-dev/2015/bitcoin-dev.2015-10-29-19.02.html
Minutes(text)
http://www.erisian.com.au/meetbot/bitcoin-dev/2015/bitcoin-dev.2015-10-29-19.02.txt
IRC Log:
http://www.erisian.com.au/meetbot/bitcoin-dev/2015/bitcoin-dev.2015-10-29-19.02.log.html
-------------------------------------
On Fri, May 08, 2015 at 06:00:37AM -0400, Jeff Garzik wrote:

You mean anyone-can-spend?

I've got code that does this actually:

https://github.com/petertodd/replace-by-fee-tools/blob/master/spend-brainwallets-to-fees.py

Needs to have a feature where it replaces the txout set with simply
OP_RETURN-to-fees if the inputs don't sign the outputs though.
(SIGHASH_NONE for instance)

-- 
'peter'[:-1]@petertodd.org
00000000000000000ee99382ac6bc043120085973b7b0378811c1acd8e3cdd9c
-------------------------------------
I wonder if that would be a viable way for payment services to pay to
protect against double spending.

If the payment processor was handling 1000 BTC every block and was willing
to pay 0.1% fees, then it could create a transaction with 1BTC in fees.

If an attacker tried to double spend a transaction of 0.1BTC, then even if
he was to spend the entire transaction to fees, the payment processor would
be able to out bid him.

It kind of works like insurance.  The payment processor combines lots of
small double spend threats and protects them with a single transaction.

The processor could keep sending out a larger and large transaction (with
fee) until eventually a block is found.

It requires RBF.  First seen safe would be incompatible, if the double
spender gets their transaction into the system first.

A 1BTC fee transaction in nearly every block would also be a boost for
network security.

It avoids Peter Todd's complaint that mining pools might make secret deals
with payment services.  The transaction would be public and all miners
could include it in their block.

On Thu, Jul 2, 2015 at 5:57 AM, Matt Whitlock <bip@mattwhitlock.name> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Regarding the bit on "getting out in front of the need, to prevent
significant negative impacts to users" I had suggested the following:

On 06/18/2015 03:52 PM, Jeff Garzik wrote:


My thoughts on that:

Possible scope narrowing to one of the following concepts (but please,
someone tell me if this "scope narrowing" is unwise, not timely, or if
there is some other factors that would make it just stupid right now
because other things are in the works or whatever:

~ Jeff Garzik, with respect to his BIP 100 (note Evan Mo, CEO of
Huobi's mining project Digcoin, clarified that the big Chinese mining
pools consider further adjustments to the protocol beyond the
suggested 8 MB block size limit adjustment  such as the Bitcoin core
developer Jeff Garzik's BIP-100 draft  to be feasible)
   ~ Adam Back, with a simplified soft-fork one-way peg
   ~ Gavin Andresen, developing an 8 MB block size limit adjustment in
the context of Core (as an example) with one or more of the above
authors rather than focusing on XT. (This is a big assumption but,
roll with it)

All of this assumes that developer(s) are willing to abandon
intentionally contentious proposals such as the "hard fork to XT w/ 20
MB," remain within the context of Core and be reasonable.

Here I am being aware of the fact that "Pushing a hard fork in the
face of such controversy is a folly, a danger to the network, and that
deserves to be said." - Wladimir J. van der Laan
https://github.com/bitcoin/bitcoin.org/pull/894#issuecomment-112113917




Something else I wanted to point out here in this thread is the
subject of the problem of "developers going off the deep end" which is
what started this thread:

Suppose you have a developer with full commit access who happens to
start threatening to revoking the other developers' commit access on
the repository, or that person doesn't even threaten, one day it just
happens.

What do you have then?  Peter Todd has stated that all one "would
achieve by that sabotage is setting a key-value pair in a centralised
registry."  But is that what we want?

The answer, obviously, is no.

This leads to other questions. What technical mechanisms exist to keep
developers from (in some dubious emotional or psycho state) to just
going off the deep and doing exactly what has been described above, if
they have full commit access?  Is there a process whereby that can't
actually happen unless another developer provides a signature (e.g. a
multisignature type of process)?  What keeps bitcoin safe from "The
Hearn Threat?"

If nothing does, then how would you change that?

And go ahead and tell me if these are dumb questions and I should just
be quiet, but if they are, please do explain why they are such dumb
questions.

- --------

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVg1OFAAoJEGxwq/inSG8COpAIAJrH9Uj9bcKr+UUR7ePV6/Yj
MmNTY2VKAtiQhwHM+Mqk2VvQANs7/uRBdZjzGnw1NRcca/m8Q0yZUHQiP8avCUOE
3MHqGviYjfeJdu1pcf+PO2pAImM5FCFdrfbbiWUt+ZoOKTxZjsLtF4RE+mc13AXJ
dktvy6SFdvQUgEx8pdXEDpmaUSYUr7syFP4sgHZmyMlhvCsXyE/8dC3sZTzEpVnC
xy1dyBmXHPW3W4FfBSblwwWgJWMcIcGJn8OLQKK5pni/iSVL6IMoRI/MLwOJdRr4
lr83g9FR/qxMqAT9UIZtATnePlkkWPU1szvak/tU/49fGioyYOF4b4KPg/bHYSc=
=hBcE
-----END PGP SIGNATURE-----


-------------------------------------
Hi folks,

I think we should agree to disable the ability to subscribe with digests on
bitcoin-dev list.  I think digests were from an earlier era of Internet
history.  Digests are inconvenient for everyone because replies have the
problem of breaking threads.    These days people should be setting mail
filters to separate list mail from other mail.  I think digests are really
not a net-positive for the community so we should just turn off the ability
to do it.

Any objections?

Warren Togami
-------------------------------------


I didn't mention the block size limit; weak blocks are a good idea no matter the limit.

As for miners paying for the work: lots of companies contributed to the Foundation, and will contribute to the DCI. When there are big, stable, profitable companies I think we'll see them task their developers to contribute code.

I think optimizing new block propagation is interesting and important, so I plan on working on it.



-------------------------------------
You keep making moral judgements.  Reality is, if you live in a world with
arsonists, you need to have a building that won't catch on fire, or has
fire extinguishers in place.  Do not depend on arsonists ignoring you
forever as your security model.  Penetration testing to know what
weaknesses exist, what limitations exist, and what can be improved is
essential.  Keeping your head in the sand and hoping people choose to do
the right thing only ends one way.

On Thu, Feb 12, 2015 at 1:52 PM, Justus Ranvier <justusranvier@riseup.net>
wrote:

-------------------------------------

etc) based on what's published in blocks.

If such a vote existed, I would gladly show the pie on BIPxDevs.
However there is no standard way for miners to vote informally BIP they
support.
-------------------------------------
On 07/05/2015 01:50 PM, Eric Lombrozo wrote:

When I read this, I get the impression that you (and possibly many
others) never actually understood the Bitcoin security model in the
first place.

Bitcoin is a digital cash system that prevents double spending without
using a trusted third party.

More specifically, successful double spending in Bitcoin requires an
attacker to pay a proof of work cost that exceeds the cumulative proof
of work paid by all non-attackers since the original spend.

The security model holds for any user who has access to the complete
blockchain, and currently does not hold for all users who do not. An
attacker can double spend without paying the full PoW cost the security
model requires if users do not have a full copy of the blockchain which
which to verify the attacker's blocks.

That's a problem, but it's not an unfixable problem.

The reason an attacker can fool SPV clients into accepting invalid
blocks is because there exists no mechanism via which honest nodes can
prove the invalidity of blocks.

Implement that mechanism, and the security of SPV clients will far more
closely resemble the security of full nodes.


-- 
Justus Ranvier
Open Bitcoin Privacy Project
http://www.openbitcoinprivacyproject.org/
justus@openbitcoinprivacyproject.org
E7AD 8215 8497 3673 6D9E 61C4 2A5F DA70 EAD9 E623
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Shizzle's opinion, it would seem, is highly important.  I'm done here.

Thy Shizzle:
Subject: Re: [Bitcoin-development] Criminal complaints against "network
disruption as a service" startups

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEcBAEBCgAGBQJVD7aKAAoJEGxwq/inSG8C4KsIAIu5atra8Y9R9oejNryjMQkz
UOVORw3y0eD8yaAiJJQzJjmNE6UXC92R3gM3KtQoQchSQ6RhyhZUZkzCY7k2Ug08
8UZnxjgAHCwScGUSgpDu2hcGDtC+Csa1EKOExjCxYCBlVRI+cCJqxIm9d7vGDi4V
R1y57xtKtussJxhZKVjIxothkHtSy5HuaKdKLfI7ikoBAerOVY7bGCxE+drUr4OO
Sgxe94M8z/ecFk3h37ZhuL2P+mNAlCKQkW592628XC0bXN8iT2vW7MnB3BLEBzvb
TeWFYUFjs5v09B6Cw6LQWFGKdFwLGganybeEqoKNfzrihEAa19PFsRWHPStMUCM=
=JnJQ
-----END PGP SIGNATURE-----


-------------------------------------
On Thu, May 7, 2015 at 12:12 AM, Matt Corallo <bitcoin-list@bluematt.me>
wrote:


Miners can always reduce the block size (if they coordinate).  Increasing
the maximum block size doesn't necessarily cause an increase.  A majority
of miners can soft-fork to set the limit lower than the hard limit.

Setting the hard-fork limit higher means that a soft fork can be used to
adjust the limit in the future.

The reference client would accept blocks above the soft limit for wallet
purposes, but not build on them.  Blocks above the hard limit would be
rejected completely.
-------------------------------------
I wish you were just as prudent when you were recommending full RBF to 
mining pools.

On 6/27/15 11:21 AM, Peter Todd wrote:


-- 
Randi Joseph
-------------------------------------
I would like to shortly express my opinion:

- Having BT as an alternative is good idea but it must be secure enough
- Signed BIP70 should be enough. I see only two issues regarding BIP70
(but they apply also to TCP/IP, not just BT): key revocations and MITM
attacks by governments.
- Broadcasting faces is very bad idea IMHO.
- Comparing addresses seems complicated but if hash was displayed as a
unique, picture hard to be mistake or long phrase, it could be more
convenient.
- Maybe storing public key (I do NOT mean bitcoin address!) of
merchant after successful transaction is good compromise?

Another idea: I noticed it's extremely easy to compare two strings if
they are the same size (in terms of millimeters, not number of
characters). If the hash of signing key was printed on a sign near the
POS in specified size (90% of smallest available screen?) and phone
would scale correctly, just putting the phone near the sign would be
enough to instantly spot whether the hashes are same.

Maybe instead of hex/base58 hash encoding use colored barcode. But I'm
not sure if it would improve things.

2015-02-05 23:49 GMT+01:00 Roy Badami <roy@gnomon.org.uk>:


-------------------------------------
On 2015-09-01 18:37, Eric Voskuil wrote:

There's no requirement for there to be multiple interpretations of the
consensus code, this is why libbitcoinconsensus exists.  Why do you
think Bitcoins survival is predicated on reimplementation?



I'm aware that these problems apply to Bitcoin Core.

-------------------------------------
Hi Adam,

I welcomed XT for its declared focus on usability with current means.
I think there is also more room for non-consenus relevant P2P protocol flavors than a single code base can accommodate.
XT is also as Jeff just tweeted a relief valve.

It became important, that Bitcoin is able to evolve even if there are conflicting educated opinions.
If a review process serves decision making, then I’d be glad to participate.

Tamas Blummer


-------------------------------------

Trivial to implement, a headache to *maintain*

But if a new platform is released on an existing blockchain, my wallet
doesn't need to know about the new magic number it claims in order to
handle it correctly.

Say I make a new token layer, BobCoin, which runs on bitcoin and say I
use an HD wallet and always generate new BobCoin token addresses as
m/##'/0'/808'/*'/*/*. If I import that wallet into older HD wallet
software that doesn't know anything about BobCoin, it will still:

- understand what blockchain to query for utxos on the addresses below
that path
- be able to generate valid BobCoin addresses without any updates

I think this is particularly valuable if you're developing against a
platform where updates can't be forced on clients.

To be clear: I am not suggesting this as a general-purpose successor to
BIP44.


Matt Smith | Gem
https://gem.co | GH: @thedoctor


On 6/19/15 5:57 PM, Andreas Petersson wrote:

-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


I've seen two different BIP103's and choose to not write about it
because risk of ambiguity. One of them are proposing a linear growth and
the other one is proposing an exponential growth. The all-linear growth
is an option that will not work well in the future because the growth
will be too slow soon or later. The exponential growth assumes that the
technology will rise in a certain growth, which may be too slow or too
fast in accordance to the technical evolution. None of the BIP103
proposals will actually handle an unexpected future case.

BIP105 has another feature not mentioned in my proposal that is to place
a vote requires a cost as a difficulty increase. I do not think it's a
good option since it will make users refrain from voting to "earn" a
difficulty lowering. The votes are the (yet) only soft way I see to let
the blockchain know if it should allow growing faster or slower. I also
don't see a benefit of having the opportunity to lower the block max
size in comparison to the risks involved with that. Then it proposes a
limit to how much it can increase at all which will need a new hard fork
when we need to increase the limits of the proposal.

I don't see how John Sacco's BIP proposal is similar to this one since
there is no voting mechanism to make the increase dynamic. Also John
proposes that the size will double at each halving instead of each
difficulty retarget. This could, in contrary to increase the fees by
making larger spaces in the blocks, decrease the fees because of that
the fee required to enter the next block will be lowered. Also it
proposes a hard limit at 32 MB which, again, need a new hard fork later.

The formula I've provided isn't actually complicated. The 2^(1/2...)
formula creates a number in the interval 1 to 2. The formula can tell if
the block max size every second year shall double or be the same based
on the last 6 month of votes. Because i believe there should always be
an increase to secure a stable growth of the network, there is also a
linear formula that the growth cannot be lower than. If the 2^(1/2...)
formula gives a lower increase than the linear value for the next
retarget, then the linear value should be used instead.

There will not be a rounding error since the implementation shall floor
the value to a whole byte. The next size should be calculated on that
value. Also, if the block max size is included in the retarget block,
there would be an extra correcting method to uncertain clients. The
formula isn't very different in complexity from the difficulty retarget
formula and will still need the last recalculated value to be computed.

One of the benefits of using an exponential formula is that it could
easily be fit for any arbitrary block period by changing the divisor. I
personally think the two week interval will be smooth enough.

Erik

Den 2015-11-13 kl. 20:37, skrev Luke Dashjr:
Especially, I'd
posted a
summary
does the
number, so
already
as an
be ideal
to it.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQIcBAEBAgAGBQJWR1JfAAoJEJ51csApon2o1zkP/1Ik/VjakUII+2iXvPB+DSJ6
cekIC4A8zlgltSmFyE74IuQBlV/5LumNMCzXoKUaDRuKSedlyh1mUrt8hPFfISfr
yvIeWmXUQhd7s34mTTc9mBvz/TDuxuNYAFe1FYQhNzuV3GaLTysBAXScY5rGIkHf
hdgxG3mPtzaqse1I5e+3jpwlPUYpLn/0A2nmF0iXCoOv1LnTvrlV3thP8Fp/YMt3
iLsiWFQFf1jpA4mDoCC/G5bfYiqvFbtXdOKKZC12Dp3hTZZCzJ21FQ6+o/v4BT7y
MfW9kl3aWf3VSxbkvHppIrX1+HqDwTsn5u9kNcbYn8xBMRpFXFddFnsg/v6ai++L
mev+kIUrXvvDqvRSfQYmHIUKCwo+tzXbHcumydxBp412TOKW5bT1CmCRYMOvY/+C
45VWBj6foUYG/kq3QISm+lptVDQlESlAizHdWNkc9HJpKZG3VkNmmxEEXm3o7J07
LbBQ7bR2MELE6lP2Z3ImTXxZe0ZBdjyjDDV3qsIGK9D7LCK31KE70ZIueE3bePmR
9xWBfzKbm6Y3cQ6+4E8p8US7woVs9LGWXzLdKQyKEoiDx16bF7SOGvSyYcnOPsNu
O7lVpGh8Pezb0ZLEx5UnM5ONm35PzmzAT9Ng2iMEhche3AQS4s/b+wVWpyclQ62e
X4UVSr2O1mbfI9CmCPfI
=qcA8
-----END PGP SIGNATURE-----

-------------------------------------
On Friday, September 04, 2015 9:36:42 PM Andy Chase wrote:

The process loses meaning if it doesn't reflect reality. So only hardforks 
should go through the hardfork process; only softforks through the softfork 
process; etc. Trying to make one-size-fits-all just means de facto accepted 
BIPs wouldn't be recognised as such because nobody cares to meet the higher 
requirements.

Luke

-------------------------------------
To follow up on this, let's say that you want to be able to have up to 1
year relative lock-times. This choice is somewhat arbitrary and what I
would like some input on, but I'll come back to this point.

 * 1 bit is necessary to enable/disable relative lock-time.

 * 1 bit is necessary to indicate whether seconds vs blocks as the unit of
measurement.

 * 1 year of time with 1-second granularity requires 25 bits. However since
blocks occur at approximately 10 minute intervals on average, having a
relative lock-time significantly less than this interval doesn't make much
sense. A granularity of 256 seconds would be greater than the Nyquist
frequency and requires only 17 bits.

 * 1 year of blocks with 1-block granularity requires 16 bits.

So time-based relative lock time requires about 19 bits, and block-based
relative lock-time requires about 18 bits. That leaves 13 or 14 bits for
other uses.

Assuming a maximum of 1-year relative lock-times. But what is an
appropriate maximum to choose? The use cases I have considered have only
had lock times on the order of a few days to a month or so. However I would
feel uncomfortable going less than a year for a hard maximum, and am having
trouble thinking of any use case that would require more than a year of
lock-time. Can anyone else think of a use case that requires >1yr relative
lock-time?

TL;DR

On Sun, Aug 23, 2015 at 7:37 PM, Mark Friedenbach <mark@friedenbach.org>
wrote:

-------------------------------------
You're right, there can be done some optimizations. Workarounds of
workaround. All this adds complexity, which reduces the security.

Marek

On Fri, Jan 23, 2015 at 7:51 PM, Gregory Maxwell <gmaxwell@gmail.com> wrote:

-------------------------------------

Announcing Not-BitcoinXT

https://github.com/xtbit/notbitcoinxt#not-bitcoin-xt




-------------------------------------------------

ONLY AT VFEmail! - Use our Metadata Mitigator to keep your email out of the NSA's hands!
$24.95 ONETIME Lifetime accounts with Privacy Features!  
15GB disk! No bandwidth quotas!
Commercial and Bulk Mail Options!  

-------------------------------------
Martin,

Yes, the second signing could be done by a mobile device that I owned and controlled (I wasn't thinking that initially).  I was thinking that online services are popular because of convenience and there should be a better way to address security (privacy issues not withstanding).

I think these are practical approaches and just doing a sanity check.  Thanks for the vote of confidence.

Brian Erdelyi

Sent from my iPad



-------------------------------------
This has been confirmed as a bug. Thanks again for reporting. I've filed a
fix here (https://github.com/bitcoin/bitcoin/pull/6777), and will be
writing tests to prevent regressions.

On Wed, Oct 7, 2015 at 4:32 PM, James O'Beirne <james.obeirne@gmail.com>
wrote:

-------------------------------------
If you are fine with the SPV security model, you are much better off by
just increasing the Bitcoin block size and using an SPV client, as those do
not care or even see the full block size by only downloading transactions
they care about. Infinite scalability!

The problem with scaling is that ultimately even SPV security relies on
others being able to validate. Both sidechains and larger block sizes make
that harder.

It's simple: either you care about validation, and you must validate
everything, or you don't, and you don't validate anything. Sidechains do
not offer you a useful compromise here, as well as adding huge delays and
conplexity.
On Jun 15, 2015 7:05 PM, "Andrew" <onelineproof@gmail.com> wrote:

-------------------------------------
The message could specify:
{ stib: 0x01,
  TxnCount: (# of entries in the Indexes array)
  Indexes: [{BLK: Block#,Txns:[TxIndex,TxIndex,...]},{BLK:
Block#,Txns:[...]}],
  NewUTXO: (The script that will spend these coins)
}
*stib *is a Script Template Index Bitfield: Must (currently) be the byte
0x01, to indicate the "vanilla" script Chris identified.  If other scripts
appear to fit the bill in the future, they can be assigned to other bits.
*Indexes *is a list of pairs that identify a block by its height and a list
of indexes into the block.  This puts the onus on the transactor to
identify all the inputs instead of requiring the miner to scan for them.

If block heights and transaction indexes are 32-bit integers, this reduces
the per-input size cost by at least 100 bytes, if I did my math right.


On Wed, Nov 25, 2015 at 6:16 AM, Erik via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
Thanks Mark.
Ok next obvious question (apologies for all of these but it seems you are
the authority on Lightning here). Is the Lightning system limited in the
number of hops there can be in the payment channel? I am looking at the
initial Lightning slides presented in February and it looks like the
locktime decrements by 1-day along each hop. So the more hops there are the
longer my bitcoins are potentially locked up for?

On 9 August 2015 at 21:18, Mark Friedenbach <mark@friedenbach.org> wrote:

-------------------------------------
On Sat, Jul 18, 2015 at 6:29 AM, Thomas Voegtlin via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:


100% agreed.


To be clear, the current name of the service is Wallet Name Service,
Netki has tended to be tagged to it as people are associating the
service with us.  We also intend to offer more services than just
this, so its actually not really good for us to think of Netki as the
service name.  I have no issues with a neutral name for the lookup
standard.



You are the second person to raise this.  Clearly this is an item that
requires some discussion before anything is decided for sure.  We had
gone this direction (and I assume Riccardo did as well) to provide a
censor resistant option as well as one that would be low cost for
individuals to be able register their own names.  This also allows for
lookups that never leave the local network.  The trade off there for
mobile wallets was one I feel we failed to properly consider.


<SNIPPING AREAS OF APPARENT AGREEMENT>

I think combining formats to use both the two level lookups and tags
could have value.  Tags could include information like versioning, as
well as whether what is being returned is an address, URL for further
lookup, or other piece of information.



<SNIPPING MORE AGREEMENT>


I concur with this approach.  I think it makes sense for us to stay in
contact and communication with the IETF side with the hope of ending
up with something that is, in the end the same, or at least
compatible.  I also agree that we shouldn't wait on the IETF to move
ahead ourselves, more stay in communication with them so that we don't
end up accidentally going in opposite directions, and also so we can
learn best practices from each other along the way.

As you can see, this has been our approach up until now where we have
gone ahead and built and expanded our "standard" based on our
discussions and integrations with other industry participants.

Thanks for the feedback!

Justin




-- 

Justin W. Newton
Founder/CEO
NetKi, Inc.

justin@netki.com
+1.818.261.4248

-------------------------------------
On Thu, Jun 25, 2015 at 02:43:19PM +0800, Pindar Wong wrote:

Agreed.

IMO any change to the blocksize needs explicit mechanisms to let all
Bitcoin holders have a say in it.


Great! Glad to hear.


Are you thinking this more technical meeting should be before or after
the October event? Perhaps a better question, is what exactly do you see
being discussed at a technical meeting?

-- 
'peter'[:-1]@petertodd.org
0000000000000000007fc13ce02072d9cb2a6d51fae41fefcde7b3b283803d24
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256



On 19 January 2015 12:09:13 GMT-07:00, Jeff Garzik <jgarzik@bitpay.com> wrote:

Protocol buffers isn't any more hashable than XML or json - round trips aren't deterministic with standard protobuf libraries. To make it deterministic you end up creating a new standard.

I have this problem for an asset representation standard for one of my clients, and I've reluctantly had to roll my own format.
-----BEGIN PGP SIGNATURE-----
Version: APG v1.1.1

iQFQBAEBCAA6BQJUvV+KMxxQZXRlciBUb2RkIChsb3cgc2VjdXJpdHkga2V5KSA8
cGV0ZUBwZXRlcnRvZGQub3JnPgAKCRAZnIM7qOfwhRwcCACNkpkkjIh8Zv5I8bOy
BpM2Tc5hVpg4KY6eKRXYLYgxoEnekDXN1/LJ5bfl+xzJTMTdt4f7YF0EjFJSIJ0C
UpR9KbEVShmt7UsoNwwAFxtMQmZe84vANGG11NI/cb95GO2TOlxYtPMFizQrp80s
ULAelID3Pd8yPeadU/yrF+daz9I8UHqOyioL0piWUT+kshuzqQNclHQaPKWoOPbW
XF4w1SAJjb1tHmkHqCY1HRvwlv8fqxXgjtEyjkz/HK70ZzOI+8aR49aigx2njwyL
F8EJ1gO3XkivRidTRKfbSloeq96TRneXXXfmyB6p8jI3O3BRkrk9x465EWMnzYu7
uJqo
=N1G+
-----END PGP SIGNATURE-----



-------------------------------------
On Mon, Jun 22, 2015 at 4:46 PM, Gavin Andresen <gavinandresen@gmail.com>
wrote:


.... I take that back, I'm wrong and Tier is correct: if activation
happened right at midnight 11 Jan 2016 and the next block's timestamp was
before midnight, that next block would just be limited to 1MB in size.

-- 
--
Gavin Andresen
-------------------------------------


The block size limit was put in place as an anti-DoS measure (monster blocks), not "anti-spam". It was never intended to have any economic effect, not on spam and not on any future fee market.


jp


-------------------------------------
Agreed. For this reason, the scaling BIPs which don't allow for easy gaming
such as BIP101, your proposal or Pieter's are preferable for their
predictability and simplicity. Changing the fundamental rules for Bitcoin
is supposed to be hard - why give this power up to a subsection of the
ecosystem in order to make it easier to change or game?

On Tue, Sep 8, 2015 at 9:13 AM, Adam Back <adam@cypherspace.org> wrote:

-------------------------------------
Hello all,

For a long time, I was personally of the opinion that soft forks
constituted a mild security reduction for old full nodes, albeit one
that was preferable to hard forks due to being far less risky, easier,
and less forceful to deploy.

After thinking more about this, I'm not convinced that it is even that anymore.

Let's analyze all failure modes (and feel free to let me know whether
I've missed any specific ones):

1) The risk of an old full node wallet accepting a transaction that is
invalid to the new rules.

The receiver wallet chooses what address/script to accept coins on.
They'll upgrade to the new softfork rules before creating an address
that depends on the softfork's features.

So, not a problem.

2) The risk of an old full node wallet accepting a transaction whose
coins passed through a script that depends on the softforked rules.

It is reasonable that the receiver of a transaction places some trust
in the sender, and on the basis of that, decides to reduce the number
of confirmations before acceptance. In case the transaction indirectly
depends on a low-confirmation transaction using softforked rules, it
may be treated as an anyone-can-spend transaction. Obviously, no trust
can be placed in such a transactions not being reorged out and
replaced with an incompatible one.

However, this problem is common for all anyonecanspend transactions,
which are perfectly legal today in the blockchain. So, if this is a
worry, we can solve it by marking incoming transactions as "uncertain
history" in the wallet if they have an anyonecanspend transaction with
less than 6 confirmations in its history. In fact, the same problem to
a lesser extent exists if coins pass through a 1-of-N multisig or so,
because you're not only trusting the (indirect) senders, but also
their potential cosigners.

3) The risk of an SPV node wallet accepting an unconfirmed transaction
which is invalid to new nodes.

Defrauding an SPV wallet with an invalid unconfirmed transaction
doesn't change with the introduction of new consensus rules, as they
don't validate them anyway.

In the case the client trusts the full node peer(s) it is connected to
to do validation before relay, nodes can either indicate (service bit
or new p2p message) which softforks are accepted (as it only matters
to SPV wallets that wish to accept transactions using new style script
anyway), or wallets can rely on the new rules being non-standard even
to old full nodes (which is typically aimed for in softforks).

4) The risk of an SPV node wallet accepting a confirmed transaction
which is invalid to new nodes

Miners can of course construct an invalid block purely for defrauding
SPV nodes, without intending to get that block accepted by full nodes.
That is expensive (no subsidy/fee income for those blocks) and more
importantly it isn't in any way affected by softforks.

So the only place where this matters is where miners create a block
chain that violates the new rules, and still get it accepted. This
requires a hash rate majority, and sufficiently few economically
important full nodes that forking them off is a viable approach.

It's interesting that even though it requires forking off full nodes
(who will notice, there will be an invalid majority hash rate chain to
them), the attack only allows defrauding SPV nodes. It can't be used
to bypass any of the economic properties of the system (as subsidy and
other resource limits are still enforced by old nodes, and invalid
scripts will either not be accepted by old full nodes wallets, or are
as vulnerable as unrelated anyonecanspends).

Furthermore, it's easily preventable by not using the feature in SPV
wallets until a sufficient amount of economically relevant full nodes
are known to have upgraded, or by just waiting for enough
confirmations.



So, we'd of course prefer to have all full nodes enforce all rules,
but the security reduction is not large. On the other hand, there are
also security advantages that softforks offer:

A) Softforks do not require the pervasive consensus that hardforks
need. Soft forks can be deployed without knowing when all full nodes
will adopt the rule, or even whether they will ever adopt it at all.

B) Keeping up with hard forking changes puts load on full node
operators, who may choose to instead switch to delegating full
validation to third parties, which is worse than just validating the
old rules.

C) Hardfork coordination has a centralizing effect on development. As
hardforks can only be deployed with sufficient node deployment, they
can't just be triggered by miner votes. This requires central
coordination to determine flag times, which is incompatible with
having multiple independent consensus changes being proposed. For
softforks, something like BIP9 supports having multiple independent
softforks in flight, that nodes can individually chose to accept or
not, only requiring coordination to not choose clashing bit numbers.
For hardforks, there is effectively no choice but having every
codebase deployed at a particular point in time to support every
possible hard forks (there can still be an additional hashpower based
trigger conditions for hardforks, but all nodes need to support the
fork at the earliest time it can happen, or risk being forked off).

D) If you are concerned about the security degradation a soft fork
might bring, you can always configure your node to treat a (signalled)
softfork as a hardfork, and stop processing blocks if a sortfork
condition is detected. The other direction is not possible.

-- 
Pieter

-------------------------------------
On Wed, Dec 9, 2015 at 1:58 AM, Jorge Timón <jtimon@jtimon.cc> wrote:

Or better, for forward compatibility (we may want to include more
things apart from nHeight and hashWitnessesRoot in the future):

struct hashRootStruct
{
 uint256 hashMerkleRoot;
 uint256 hashWitnessesRoot;
 uint256 hashextendedHeader;
}

For example, we may want to chose to add an extra nonce there.

-------------------------------------
with pooling protocols to phase out the artificial centralisation.

So how is the level of decentralization measured?   I see many claims on 
this list that such-and-such action will increase or decrease 
centralization and sometimes people talk in absolutes such as something 
being decentralized or centralized.   Some of the arguments seem to make 
claims without providing any kind of analysis or explanation.

Nothing is truly decentralized and decentralization is just an 
approximation of having a collection of centralized systems interact in 
some way.  I would suggest coming up with some sort of metric so these 
discussions can start from a baseline when discussing changes.

Russ



-------------------------------------
On Monday, November 02, 2015 4:27:50 AM jl2012@xbt.hk wrote:

I agree, false alarm. Somehow I had confused the comparison of locktimes this 
morning. :(

Sorry about that,

Luke

-------------------------------------
accordingly to public release[1], They.

1. agreed that blocksize increase is needed.
2. opposed original 20mb, suggest 8mb instead as it is more technically
reasonable.
3. do not want blocksize to change in the "short term future" ( direct
translation. ) and in the document states.
"after discussion we are in agreement that the blocksize should be within
the ball park of 8mb for the short term future."

They have no explicitly rejected or supported the other components of
BIP101. It's my opinion that as long as the change is < 8mb. they'll take
it.

I don't believe in trying to predict the future, on adoption, technology
growth, nor geopolitics. I think it matters very little which BIP we need
up deploying, as long as all the attack vectors are covered, especially for
the dynamically adjustable ones.

One thing is for sure though, not increasing the blocksize is not an option.

we can't predict the future, in the mean time, Hardfork Responsibly™.

[1]
http://7fvhfe.com1.z0.glb.clouddn.com/@/wp-content/uploads/2015/06/%E5%8C%BA%E5%9D%97%E6%89%A9%E5%AE%B9%E8%8D%89%E6%A1%88.jpg

On Fri, Aug 21, 2015 at 7:28 AM, Btc Drak <btcdrak@gmail.com> wrote:




-- 
*Yifu Guo*
*"Life is an everlasting self-improvement."*
-------------------------------------

The idea is to simplify implementation. Existing software can be used
as is to sign and validate PoPs. But I do agree that it would be a
cleaner specification if we would make the PoP invalid as a
transaction. I'm open to changes here. I do like the idea to prepend a
constant string. But that would require changes in transaction signing
and validation code, wouldn't it?


Naming is hard. I think a simpler name that explains what its main
purpose is (prove that you paid for something) is better than a name
that exactly tries to explain what it is. "Proof of transaction
intent" does not help me understand what this is about. But I would
like to see more name suggestions. The name does not prevent people
from using it for other purposes, ie internet over telephone network.

Thank you
/Kalle



-------------------------------------
Thanks again.  The description of bits 16..29 as "can take any value"
suggests to me an improvement for isStandard: if any bits "can take any
value" without affecting the script then they must be off for the script to
pass isStandard.

If I understand it correctly, this requirement will serve as a backup to
future uses of those bits if such uses are deployed as soft forks.

I'm sorry if my suggestion reflects a poor understanding of isStandard, but
I offer it as evidence on whether the mechanism is as well understood as it
should be, since we use soft forks.  If I have misunderstood, feel free to
educate me with a reply.

Thanks!
Notplato



On Oct 10, 2015, at 8:22 AM, G1lius Caesar via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


bits 16..29 are masked off and can take any value.



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
On Sat, Jul 18, 2015 at 01:43:14PM +0200, Mike Hearn via bitcoin-dev wrote:

No, he's talking about the min-relay-fee drop that you wrote:

https://github.com/bitcoin/bitcoin/pull/3305

Based on what I saw in my logs, the double-spends were mainly being done
by exploiting the fact that much of the hashing power has reverted your
10x relay fee drop as it makes wasting bandwidth and mempool RAM easy.
(so much so that crashing nodes with OOM's is fairly cheap)

-- 
'peter'[:-1]@petertodd.org
00000000000000000a56b9b96af356cc8411cea940bb6c25b9cd934f99f9e174
-------------------------------------
On Wed, Jul 29, 2015 at 6:03 PM, Mike Hearn <hearn@vinumeris.com> wrote:
[...]

Mike, my first use of Bitcoin was in 2009.  I wasn't vigorously active
in the Bitcoin community until the beginning of 2011, indeed. But this
is just a couple months after you (E.g. first code available for
BitcoinJ was March 2011-- if you go by forums.bitcoin.org account
times my account was created May 5th 2011 vs yours Dec 14th 2010; less
than five months after yours). I was also working with related systems
long before (E.g. RPOW in 2004). So give me a break, there is no rank
to pull here.

Yet again you've managed to call me a bullshitter and guilty of
"invention" when in fact I'm actually quoting the system's creator
(although without the explicit fallacious argument from authority
style you seem prefer). For someone who seems to base all his
arguments on interpretations of someone's words you sure seem to call
their words lies awfully often:

"Piling every proof-of-work quorum system in the world into one
dataset doesn't scale."
[...]
"Bitcoin users might get increasingly tyrannical about limiting the
size of the chain so it's easy for lots of users and small devices."
----  https://bitcointalk.org/index.php?topic=1790.msg28917#msg28917

If you'll note,, the post was Dec 10th 2010 and, presumably, made with
an improved understanding of the implications of the system then
comments made in 2008 before the system was even operational.

(The same message also mentions that smart contracts can be used to
create trustless trade with off-chain systems;  As well, later in that
thread: "it will be much easier if you can freely use all the space
you need without worrying about paying fees for expensive space in
Bitcoin's chain.")

I haven't bothered arguing from old posts in the past because I find
the practice of argument from authority on this subject abhorrent. It
undermines the unique value of Bitcoin to argue based on a single
personal opinion, to do so is to miss the point of Bitcoin in a deep
and fundamental way. And in my opinion what you're doing is actually
much worse: arguing from distortions of random quotations.  But it's
hard to tolerate the continue revision of history from you in silence.

Moreover, I find those arguments with respect Moore's law especially
unconvincing because while I cannot read the mind of people who are
not a part of this discussion and haven't chosen to comment, I've used
the same argument myself and I know what I was thinking when I used it
(and can establish as much, since I'm more verbose I elaborated on
it):  When someone pointed at Bitcoins _global_ broadcast medium and
loudly said that it cannot work because its absurd; and it's very easy
to point out broad scaling behavior about what Bitcoin could achieve
with complete centralization. Once this has been accepted the argument
is _over_ in Bitcoin's favor: Bitcoin's competition has highly
centralized administration and so once someone has accepted Bitcoin
can (in some way) accommodate the worlds transactions, even if that
comes at the cost of 99% of the decentralization, it's clear that
Bitcoin offers something interesting. (And for example, I elaborated
on this in a Wiki edit in Aug 2011,
https://en.bitcoin.it/w/index.php?title=Scalability&action=historysubmit&diff=14273&oldid=14112
 -- though I shouldn't need to point this out to you, since it was you
who subsequently erased these words from the page.)

[...]

For example, you fought vigorously to get Bitcoin Core off
Bitcoin.org, which would ensure that users were not previously
equipped with a node suitable for operating mining (which then
contributed substantially to the poor usability of solutions like
P2Pool; with 98% of it's install time spent waiting for Bitcoin Core
to sync).

You've (in my view) aggressively advocated increasing the resource
utilization of Bitcoin-- increasing the cost to participate in mining
without delegation, with no consideration (or at least disclosure) of
the ramifications on the system overall:
https://bitcointalk.org/index.php?topic=149668.0

Gavin, for example, has advocated removing mining support from Bitcoin
core on several occasions; and constantly professes ignorance on
anything mining.  His own interests are up to him, but to not be
concerned about a central part of the system for anyone working on
changing it at such a deep level is-- I think-- a bit problematic.

But I didn't intend to lay blame here, if anything I blame myself for
not being more proactive in arguing against things things in the past.

The trend towards mining centralization is a result of various forces,
many of which are modulated by the very things we're discussing here
(or could be modulated by things we haven't discussed).  You're the
principle advocate of increasing the cost of a decentralized ecosystem
around verification and driving the system towards a state where it is
only viable in a more centralized mode.  Bitcoin is an artificial
construction, not a force of nature, and when someone seeks to change
it they ought to take responsibility for what happens--- it's not
acceptable to say "oh well, it's not eh fault of anyone" when the
incentives drive it in a bad direction.

Is that your strategy on the systems resource consumption in general?
Full throttle, no action when it goes off the rails,  when the easily
foreseeable negative outcomes happen it won't be the "fault" of
anyone? If so, I don't think that is acceptable. We need to face the
areas in which the system is failing, now and in the future... and not
just pump for growth at all cost and shrug and say "oh well, we tried"
when the predictable failure happens. It's far from clear to me that
the world will get a second shot at this in the next several decades
if Bitcoin lapses into the same-old, same-old.


The fixation comment was a specific reply to your long list of the
"only reasons" to run a full node, which seemed to be basically said
that the only reason to run one was to act as a server for SPV
clients; as it listed several points on that-- all three of the
numbered points were "serving SPV wallets"-- and buried the rest.  I'm
sorry if I read too much into it, though it's also consistent with
your prior responses that the non-scalability of Bitcoin as a whole is
irrelevant due to SPV.

I don't think there is anything fundamentally bad with SPV, it is what
it is; it's a tool and an important one. But at the moment it is far
more limited than you give it credit for both because it is only
secure under certain assumptions which have been provably violated not
just at risk of violation, and because the more complete vision of it
(e.g. with fraud proofs) has never been implemented.


Now that I've established the "small device" text you're railing on
here actually came from the system's creator prior to your
involvement, can I expect an admission that your own "personal liking"
doesn't have special authority over the system?  But I hope you don't
create an altcoin: I think it's possible to find ways to accommodate
people with very different preferences under one tent, and if we are
to build and support a worldwide system we _must_ find those ways
rather than fragmenting the marketplace.

-------------------------------------
On 8/6/2015 7:53 AM, Pieter Wuille via bitcoin-dev wrote:

Gavin has answered this question in the clearest way possible -- in
tested C++ code, which increases capacity only on a precise schedule for
20 years, then stops increasing.



-------------------------------------
Dear Bitcoin devs,

I am the author of OCaml-bitcoin [1], a library offering an OCaml 
interface
to the official Bitcoin client API.  For those who may be unfamiliar 
with it,
OCaml is one of those functional programming languages with a very rich 
and
expressive type system [2].  Given its emphasis on safety, its 
industrial
users are disproportionally found in the aerospace and financial 
sectors.

Now, OCaml programmers care a lot about types, because experience has
taught them that deep down most programming errors are just type errors.
 From this stems my request: please consider defining more precisely the 
type
information associated with each API call in the JSON-RPC reference [3].

To give you a better idea of what I'm talking about, please take a look 
at
the API offered by OCaml-bitcoin [4], and the associated type 
definitions
[5] (note that these have not been updated for Bitcoin Core 0.10 yet).
I've created the type definitions from information gathered from the 
Bitcoin
wiki and from looking at the Bitcoin Core source-code.  I wouldn't be 
surprised
if it contains errors, because neither the source-code nor the wiki is 
very
precise about the actual types being used.  As an example, consider type
hexspk_t ("hex representation of script public key").  Is this really 
the
same type used in both signrawtransaction and createmultisig?

Improving this situation would pose a minimal burden on bitcoin devs: 
all
that would be required is defining the precise set of types used in the 
RPC
API, and annotating the RPC calls either in the source-code itself or in 
the
API reference documentation.  It would make writing bindings such as 
mine
far easier and less error prone, and it would have the added advantage 
of
better documenting the Bitcoin Core source-code itself.

Also, note that it is not necessary to extend this request to the deep
data structures returned by some API calls.  Consider for instance the
gettransaction function of the OCaml-bitcoin API: it returns the raw 
JSON
object without any attempt to process it.  This is because that's a 
fairly
niche facility, and the bindings would balloon in size if I were to 
process
every single large return object.  Instead, the bindings take the more
pragmatic stance of only processing the parameters and return results 
where
a strong type discipline is imperative.

When I raised this issue on IRC a number of questions were posed.
What follows is my attempt to answer them:

   Q: What does it matter, if JSON only has a tiny set of types?

   A: JSON being the serialisation format is irrelevant.  The client 
bindings
      know that even if a public ECDSA key is serialised as a string, it 
does
      not stop being a public ECDSA key, and should only be used where a 
public
      ECDSA key is expected.

   Q: What does it matter if the types are not even distinguished in the 
C++
      source of Bitcoin Core?

   A: That is unfortunate, because it opens the door to bugs caused by 
type
      errors.  Moreover, even if the C++ source is "stringly-typed" and 
does
      not enforce a strong type discipline, that does not mean that the 
types
      are not there.  Even if a public and private key are both 
represented
      as strings, can you use one where the other is expected?  If not, 
then
      they actually have different types!

   Q: Isn't this a maintenance nightmare, given the changes to Bitcoin 
core?

   A: Actually, the most burdensome part is what motivated this message:
      keeping track of the types used.  If the Bitcoin API reference were
      more precise, keeping the bindings up-to-date would be trivial and
      even mechanical, because the API is now fairly stable.


Thank you very much for your attention, and for all the work you guys 
put
into Bitcoin development.  It is much appreciated and not acknowledged
often enough!

Best regards,
Dario Teixeira

[1] https://github.com/darioteixeira/ocaml-bitcoin
[2] http://ocaml.org/learn/description.html
[3] https://bitcoin.org/en/developer-reference#bitcoin-core-apis
[4] http://ocaml-bitcoin.forge.ocamlcore.org/apidoc/Bitcoin.ENGINE.html
[5] http://ocaml-bitcoin.forge.ocamlcore.org/apidoc/Bitcoin.html



-------------------------------------
I'll leave others to comment on whether we can get consensus on that, 
but your years listed are inconsistent with everything else you've 
written. Should be:

block 400,000 = 2MB (2016)
block 500,000 = 4MB (2018)
block 600,000 = 8MB (2020)

On 17/07/2015 20:06, Chris Wardell via bitcoin-dev wrote:

-------------------------------------
2015-06-15 12:00 GMT+02:00 Pieter Wuille <pieter.wuille@gmail.com>:

Wallets will have the same ability to make PoPs as they have in making
payments, see my motivation and rationale sections. CoinJoin is not
compatible with PoP, Luke-Jr brought that up a week ago:

"This appears to be incompatible with CoinJoin at least. Maybe there's some
clean way to avoid that by using
https://github.com/Blockstream/contracthashtool ?"

I'm not sure if we will be able to support PoP with CoinJoin. Maybe
someone with more insight into CoinJoin have some input?


I don't understand this. The pop includes a nonce randomly generated
by the server. If you're very lucky, 1/(2^48) per try, you can reuse a
pop.


If you pay as you use the service (ie pay for coffee upfront), there's
no need for PoP. Please see the Motivation section. But you are right
that you must have the wallet(s) that paid at hand when you issue a
PoP.


Please elaborate, I don't understand what you mean here.

Regards,
Kalle



-------------------------------------
On Tue, May 12, 2015 at 8:03 PM, Gregory Maxwell <gmaxwell@gmail.com> wrote:


M = 1,000,000
N = number of "starts"

S(0) = hash(seed) mod M
...
S(n) = hash(S(n-1)) mod M

This generates a sequence of start points.  If the start point is less than
the block height, then it counts as a hit.

The node stores the 50MB of data starting at the block at height S(n).

As the blockchain increases in size, new starts will be less than the block
height.  This means some other runs would be deleted.

A weakness is that it is random with regards to block heights.  Tiny blocks
have the same priority as larger blocks.

0) Blocks are local, in 50MB runs
1) Agreed, nodes should download headers-first (or some other compact way
of finding the highest POW chain)
2) M could be fixed, N and the seed are all that is required.  The seed
doesn't have to be that large.  If 1% of the blockchain is stored, then 16
bits should be sufficient so that every block is covered by seeds.
3) N is likely to be less than 2 bytes and the seed can be 2 bytes
4) A 1% cover of 50GB of blockchain would have 10 starts @ 50MB per run.
That is 10 hashes.  They don't even necessarily need to be crypt hashes
5) Isn't this the same as 3?
6) Every block has the same odds of being included.  There inherently needs
to be an update when a node deletes some info due to exceeding its cap.  N
can be dropped one run at a time.
7) When new starts drop below the tip height, N can be decremented and that
one run is deleted.

There would need to be a special rule to ensure the low height blocks are
covered.  Nodes should keep the first 50MB of blocks with some probability
(10%?)
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



On 08/08/2015 01:10 PM, Thomas Zander via bitcoin-dev wrote:

I think the context is not that "Bitcoin's reason for existence is to
avoid trusting anyone but yourself", as you state above. This kind of
dystopia is not, in my view, what the advocates of trustlessness and
security are implying.

There are those instances where we have no option other than to trust
a centralized authority. They are mostly self-appointed and do not
have the interests of the rest of us in mind. The central bank is the
obvious example - they impose their currency as "official" and then
devalue our savings and purchasing power through money supply
inflation - without our permission and in betrayal of the trust
relationship.  Bitcoin allows one to hold money and to conduct money
transactions, transmit value, and so forth without having to trust
*them* - the central bank, or anyone.

In another example, we want to conduct escrow and have to trust the
notary, because of his reputation and framed certificate, but Bitcoin
multisig makes the degree of trust between the escrow parties
irrelevant, so no apparently trustworthy "gentleman" merchant (back
then or now) can socially engineer a transaction and scam us.

I don't know if my reply and its examples is over-simplistic but it
seems you were making a moral appeal that the notion of trustlessness
was destructive - I just wanted to contextualize it to relevant use
cases. It follows, and this is what I understand from Adam's message,
that security and protection of decentralization are paramount
concerns if we want to retain the trustlessness that makes Bitcoin so
useful and powerful.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVxcZSAAoJEGwAhlQc8H1mmAUH/RprBA/tz13/CIJTQ4HHYU5v
kBAyirwhkx5NEvovFGV40PNpIE7OLqfoN2ThpwJSO8fqnPwbcGEr1qvZAaN9A68y
GD1sjw2y+8+hQUtrikMurBFzCX4msBvfYNPHX4J7SBR9qdxC9L7p6HaY5fvdwpQW
DmUMidKuRPpWH5AL9DqB9ZwHtPZ8mVaJGHMw1aI9QV0cTlq49ktbt/246wAjtRkb
Myw2c9hKT2WIwzmWcruokSXJ4yza6DGYKrcyzprIDCJFEj29geoIderyU+0qzRQ7
julB3Ft05xp2F6LheeH40wa7iyeRs6LWRNr2qElutu6Ta7Rvlg5ZCUGN52SGQtY=
=3rcg
-----END PGP SIGNATURE-----

-------------------------------------
Agreed, there is no need to misuse the version field as well. There is more
than enough variability you could roll in the merkle tree including and
excluding transactions, and the scriptSig of the coinbase transaction,
which also influences the merkle root.

I have a fundamental dislike of retroactively changing semantics, and the
version field should be used just for that: a version. I don't even
particularly like flagging support for a fork in the version field, but
since I have no better solution, count me as supporting Sipa's proposal. We
definitely need a more comfortable way of rolling out new features.

Regards,
Chris

On Thu, May 28, 2015 at 3:08 AM Patrick Strateman <
patrick.strateman@gmail.com> wrote:

-------------------------------------
At CryptoCorp we recommend to our customers that they sort
lexicographically by the public key bytes of the leaf public keys.  i.e.
the same as BitPay.

On Wed, 2015-01-14 at 17:37 +0100, Ruben de Vries wrote:

-- 
Miron / devrandom





-------------------------------------
why not allow both serializations and keep serialization format a
parameter, keep everyone happy.

http://twitter.com/gubatron

On Wed, Jan 28, 2015 at 12:14 PM, Mike Hearn <mike@plan99.net> wrote:

-------------------------------------
42 in the whole world, and I'm one of them. Clearly that is a problem, do you even know about AT&T or are you in another country? Cause that statement is utterly ridiculous given the fact there are hundreds of millions of people using AT&T. I was simply sharing my knowledge on this issue since it poses a threat to the health of the bitcoin network, no need for personal attacks. 

None of my accusations were false, there is a firewall in the DVR that is uncontrolled and all ports are blocked via private subnets and no fixed public IP allowed unless you pay. I confirmed every one of these details with AT&T technicians or I wouldn't be saying them.

 

 

 

-----Original Message-----
From: Matt Whitlock <bip@mattwhitlock.name>
To: hurricanewarn1 <hurricanewarn1@aol.com>
Sent: Wed, Sep 2, 2015 5:34 am
Subject: Re: [bitcoin-dev] AT&T has effectively banned Bitcoin nodes via utilizing private subnets.


According to BitNodes, 42 Bitcoin nodes are running on AT&T's
network:

https://getaddr.bitnodes.io/nodes/?q=AT%26T

So I'm thinking
there's nothing wrong with AT&T's default network configuration.

Frankly, the
things you've been writing strongly suggest that you aren't very knowledgeable
about computer networking. Instead of jumping right into making wild accusations
about AT&T, you probably should find someone knowledgeable to verify your
claims.


On Wednesday, 2 September 2015, at 5:20 am, Zach G via bitcoin-dev
wrote:
available and there was zero connections. Bitcoin was literally getting thottled
every second. It would not even allow the connection to get block source. EVERY
port was blocked, making exceptions in the router firewall did nothing. I was
forced to use Blockchain.info which is a major security risk.
am developing a program using Bitcoin Python modules, so I login to my computer
like it's a server and it was flat out rejecting the connection. I could not run
any code until this got fixed, and of course needed the block source to even do
anything. 
emailing the list. Bitcoin Core was crippled and unusable due to the AT&T
settings, and they tried hard to get me to buy monthly subscriptions to get the
answer. This makes it likely that Bitcoin Core is unusable for most AT&T
customers and other ISPs, hence the massive node decline. I'm sure this disrupts
alot of other people besides Bitcoiners too, hence the monthly subscriptions
geared towards people who can't figure out their connection situation.
AT&T literally blocked access to static IP if you don't buy one, so it wasn't a
normal network setup. Unfortunately the same security used to stop hackers and
viruses stops Bitcoin too, so this is probably the settings for almost every
router in the country. Nodes are in fact declining worldwide, down 15% in the
past year alone. Community needs to speak up and also educate before this gets
completely out of control. https://getaddr.bitnodes.io/dashboard/?days=365 6,000
nodes is pathetic as it is and it's constantly declining.
-----Original Message-----
hurricanewarn1 <hurricanewarn1@aol.com>
Subject: Re: [bitcoin-dev] AT&T has effectively banned Bitcoin nodes via
utilizing private subnets.
talking about? Practically every home LAN
desktop computer has the IP address
subnet. This doesn't prevent my Bitcoin Core
connections to other nodes. Moreover, almost all home
the world run on dynamically assigned IP addresses.
cause any problems for connecting outbound to other Bitcoin
that your node can't accept incoming connections unless you
on your router to your computer, but you don't need to be able
incoming connections to participate in the Bitcoin network.
Wednesday, 2 September 2015, at 3:20 am, Zach G via bitcoin-dev wrote:
for
through my
useful tools
sure there was no
was sure these were
throttled every second and
target was rejecting the
subnet settings, and saw that I had a
are private on earth (
Uverse put all their
computer not only hidden
network. That alone is enough to
port, but they made it even crazier by
all the time, so public IP was meaningless
switched over to a public subnet, and right there was
incoming connections. My static IP showed for a minute then
dynamic/hidden again without me even touching anything. The final
was AT&T charges $15-30/month for a public static IP, which is
insane and actually one could argue that violates their own terms of
So the router was still ignoring my public IP settings simply because I
wasn't paying for a public IP, and intentionally changing the settings back.
I
I found
https://www.cryptocoinsnews.com/isps-intentionally-blocking-bitcoin/ It's
based
page. I
https://twitter.com/turtlehurricane/status/638930065980551168 ) and believe
it
later. I
please do
action. The
I probably
answered all my
and trying to sell
out to cryptocoinnews
appears AT&T has not blocked
all ports via the private
unroutable for incoming peers.
and cripples the ability to run a
understandable, since it just about 100%
since they can't even see your computer. What
AT&T technicians did not inform me about this until
myself, despite the fact it is a very obvious cause of
It's probably just ignorance since AT&T has so many complex
it's hard to keep track of, although I have a suspicion that
command chain is withholding information in an attempt to make
$15/month connection service, and once they buy that another
needed to get the static IP.
find info on the internet about private subnets crippling the ability to
Bitcoin. I believe this needs to be explicitly said in instructions for
running a full node, maybe it wasn't a problem in 2009 but now it is a major
issue. On default settings Bitcoin is 100% blocked, and most people do not
have
technicians and
answer until I found
clues about how the
it, since other ISPs like
them about Bitcoin and they
more willing to help me cause

public static IP, the bad news is
we as a community make it well known.
if this information is spread to everyone.
expenditures is simply not the reason people don't
because their ISP has made it just about impossible without
nearly 100% more money per month. If you don't pay the fee AT&T
tell you about the private subnet, at least based on my
<odinn.cyberguerrilla@riseup.net>
<hurricanewarn1@aol.com>; bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
effectively banned Bitcoin nodes by closing port 8333 via a hidden firewall
in
Hash: SHA512
people have already
landline but don't use AT&T's
Uverse)
internet provider - you are
avoiding not only this sort of
potentially getting a better privacy policy
like AT&T's long-term data retention).  You can check
the various local small ISPs to see what their policies
specifically
(above may or may not
decentralized markets)
is for bitcoin of course)
are paying a local ISP for
contracted
old-style DSL has been
essentially providing you AT&T FTTN.
FTTN-BP
privacy policy
subject to AT&T data
(or set
VPN.
stuffs for 6
your stuffs for
Qwest/Century, 1 year.
a year.  This is just
https://www.lawfareblog.com/odni-and-doj-release-last-section-215-collec
tion-order
G via bitcoin-dev:
year, I
strong desire to
reached critical mass since
module. I have literally spent
I thoroughly made sure it was
it's still closed. Strangely
once today but something closed
hours of phone
of what was going on, and
demanded
DVR/cable box, and
make this even more
because it is their
unlikely event
is the driving force behind the
in Bitcoin nodes. Bitcoin is being
themselves, and they won't even tell you that. I
touch with headquarters and threaten to rip it out of the
a
_______________________________________________
list
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
decentralization
social
SIGNATURE-----
iQEcBAEBCgAGBQJV5VDeAAoJEGxwq/inSG8CvkIH/jy4Vo+My3xeBdvFQmxkJWyQ
U5mv2zWEvBYw71Xy1EDzQY1AhEBmatUU1eu2AbOqXdUR4511FxCNzFmTxy6roEiz
EehBkvXNbBCbEzLRisjxuQw34OKM+xfieCqE1mzJok2uSdLMMQLcbWL1/k3/OmS5
9O9z/wMXqU1Jc19MTK+vF1Lz5ilnRn3hEbTaCN3ivYnYFa0DpBH9r0Y07UcoJ6Wr
ui/x0sSSuupAGzOkZ75HQ8yeQXckeAu6TB3/jE8QEqNUmAJkmR8eK4ofXZWFrIjy
mOKeQL4c+jRQnTR8pt+y89g2QIpzFoHaV5T+WvQuC1t8xNOrxLgYFXWgl0dhoYE=

 
-------------------------------------
"I ran some simulations, and if blocks take 20 seconds to propagate, a
network with a miner that has 30% of the hashing power will get 30.3% of
the blocks."

Peter_R's analysis of fee markets in the absence of blocksize limits [1]
shows that the hashrate advantage of a large miner is a side-effect of
coinbase subsidization. As the block rewards get smaller, so will large
miner advantages. An easy way to think about this is as follows:

Currently, the main critique of larger blocksizes is that we'll connected
miners can cut out smaller miners by gratuitously filling up blocks with
self-paying transactions. This only works because block subsidies exist.
The moment block rewards become comparable to block TX fees, this exploit
ceases to be functional.

Basically, large miners will still be forced to move full blocks, but it
will go against their interest to fill them with spam since their main
source of income is the fees themselves. As a result, large miners (unlike
smaller ones) will lose the incentive to mine an un full block this evening
the playing field.

In this context, large blocksizes as proposed by BIP100-101 hope to
stimulate the increase of TX fees by augmenting the network's capacity. The
sooner block rewards become comparable to block fees, the sooner we will
get rid of mine centralization.

Dpinna

[1]
http://www.scribd.com/mobile/doc/273443462/A-Transaction-Fee-Market-Exists-Without-a-Block-Size-Limit
-------------------------------------
Whatever...let's use the current subsidies, the same argument applies, it's
just 20 + 25 = 45 btc per block for miner B vs 27 btc for miner B.
Miner B would still go out of business, bigger blocks still mean more
mining and validation centralization. The question is how far I we willing
to go with this "scaling by sacrificing decentralization", but the answer
can't be "that's to far away in the future to worry about it, right now as
far as we think we can using orphan rate as the only criterion".
On May 31, 2015 4:49 PM, "Gavin Andresen" <gavinandresen@gmail.com> wrote:

-------------------------------------
Looks like I'm the long dissenting voice here? As the originator of the
name CHECKSEQUENCEVERIFY, perhaps I can explain why the name was
appropriately chosen and why the proposed alternatives don't stand up.

First, the names are purposefully chosen to illustrate what they do:

What does CHECKLOCKTIMEVERIFY do? It verifies the range of tx.nLockTime.
What does CHECKSEQUENCEVERIFY do? It verifies the range of txin.nSequence.

Second, the semantics are not limited to relative lock-time / maturity
only. They both leave open ranges with possible, but currently undefined
future consensus-enforced behavior. We don't know what sort of future
behavior these values might trigger, but the associated opcodes are generic
enough to handle them:

CHECKLOCKTIMEVERIFY will pass an nSequence between 1985 and 2009, even
though such constraints have no meaning in Bitcoin.
CHECKSEQUENCEVERIFY is explicitly written to permit a 5-byte push operand,
while checking only 17 of the available 39 bits of both the operand and the
nSequence. Indeed the most recent semantic change of CSV was justified in
part because it relaxes all constraints over the values of these bits
freeing them for other purposes in transaction validation and/or future
extensions of the opcode semantics.

Third, single-byte opcode space is limited. There are less than 10 such
opcodes left. Maybe space won't be so precious in a post-segwitness world,
but I don't want to presume that just yet.


As for the alternatives, they capture only the initial use case of
nSequence. My objection would relax if nSequence were renamed, but I think
that would be too disruptive and unnecessary. In any case, the imagined use
cases for CHECKSEQUENCEVERIFY has to do with sequencing execution pathways
of script, so it's not a stretch in meaning. Previously CHECKMATURITYVERIFY
was a hypothicated opcode that directly checked the minimum age of inputs
of a transaction. The indirect naming of CHECKSEQUENCEVERIFY on the other
hand is due to its indirect behavior. RELATIVELOCKTIMEVERIFY was also a
hypothicated opcode that would check a ficticious nRelativeLockTime field,
which does not exist. Again my objection would go away if we renamed
nSequence, but I actually think the nSequence name is better...

On Tue, Nov 24, 2015 at 2:30 AM, Btc Drak via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Another concept...

It should be possible to use multisig wallets to protect against malware.  For example, a user could generate a wallet with 3 keys and require a transaction that has been signed by 2 of those keys.  One key is placed in cold storage and anther sent to a third-party.

It is now possible to generate and sign transactions on the users computer and send this signed transaction to the third-party for the second signature.  This now permits the use of out of band transaction verification techniques before the third party signs the transaction and sends to the blockchain.

If the third-party is malicious or becomes compromised they would not have the ability to complete transactions as they only have one private key.  If the third-party disappeared, the user could use the key in cold storage to sign transactions and send funds to a new wallet.

Thoughts?

-------------------------------------
On Friday, September 18, 2015 8:24:50 PM Btc Drak via bitcoin-dev wrote:

Not everyone does crazy clock-changing. Using such a time system for 
scheduling seems to inconvenience the wrong position. (although perhaps 
arguably better since most people probably use DST) :p

(Aside, if Google Calendar can't support standard UTC, that sounds like an 
argument against using Google Calendar...)


Tonal time works nice and consistently. :D

Luke

-------------------------------------
Peter Todd wrote:

It would be 'completely predictable' for whoever knew the state and 
policies of a miner's mempool, but from an end user's perspective that 
wouldn't matter much: The end users wouldn't know if their 
transaction(s) would make the cut or not, somewhere in the network, and 
by what time. They (obviously) won't know what miners will find the next 
block(s), they won't know the miners' mempool sizes, potential custom 
eviction policies, etc.

I agree that this can be somewhat remedied by FSSRBF/CPFP, though, 
provided wallets give users a good (semi-automated?) interface for such 
transaction replacements/chains.


-- 
Martin Lie


-------------------------------------
On 07/23/2015 08:42 PM, Slurms MacKenzie via bitcoin-dev wrote:

Yes, quite true. And without the ability to search using filters there
is no private restore from backup short of downloading the full chain,
rendering the idea rather pointless.

This is why privacy remains a significant issue. Privacy is an essential
aspect of fungibility. This is a central problem for Bitcoin. The
correlation of addresses within transactions is of course problematic.
Possibly zero knowledge proof will at some point come to the rescue. But
the correlation of addresses via search works against the benefits of
address non-reuse, and the correlation of addresses to IP addresses
works against the use of private addresses.

Solving the latter two problems can go a long way to reducing the impact
of the former. But currently the only solution is to run a full chain
wallet. This is not a viable solution for many scenarios, and getting
less so.

This is not a problem that can be ignored, nor is it unique to Electrum.
The Bloom filter approach was problematic, but that doesn't preclude the
existence of valid solutions.


Well because of presumed relationship in time these are not actually
separated requests. Which is why even the (performance-unrealistic)
option of a distinct Tor route for each independent address request is
*still* problematic.


Introducing truly-random timing variations into the mixnet solutions can
mitigate timing attacks, but yes, this just makes the already
intolerable performance problem much worse.

e

-------------------------------------
Den 30 jun 2015 02:21 skrev "Tom Harding" <tomh@thinlink.com>:
mined.
situation
profits
majority of
to
to have unintended consequences directly in opposition to your own stated
goal of decentralization.  And yet you persist.
completely unpredictable hodge-podge of relay policies, we should expect
many more participants to bypass the P2P network entirely.

What you are asking for is TSA style reactive security and unverifiable and
fundamentally untrustable security mechanisms, rejecting proactive security
on the grounds that it is inconvenient.

What you ask to see implemented will trivially fall to a sybil attack. It
isn't securable. It is running on the honor system exclusively. It will be
attacked, it will fail, losses will be had, the attackers will walk away
with embarrassingly large sums.

You want verifiable behavior? Incentives to tell the truth? Incentives to
be consistent? Multisignature notaries (Greenaddress.it), payment channel
based hub-and-spokes (LN,  Stroem), etc... Trusting the P2P network is
futile. You need one accountable party that is actually capable of
enforcing the behavior you ask for, one that can build a reputation over
time - the P2P nodes you wish to hold accountable are on the other hand
powerless to stop an actual attack, their reputations are therefore
meaningless and irrelevant. Multisignature notaries aren't, as they can
stop an attack, and they can be sued for breach of contract if they don't -
neither of those applies to P2P nodes.
-------------------------------------
Hi Daniele,

I don't think there is any contention over the idea that miners that control a larger percentage of the hash rate, h / H, have a profitability advantage if you hold all the other variables of the miner's profit equation constant.  I think this is important: it is a centralizing factor similar to other economies of scale.  

However, that is outside the scope of the result that an individual miner's profit per block is always maximized at a finite block size Q* if Shannon Entropy about each transaction is communicated during the block solution announcement.  This result is important because it explains how a minimum fee density exists and it shows how miners cannot create enormous spam blocks for "no cost," for example.  

Best regards,
Peter



-------------------------------------

Unless you're relying upon some hypothetical hyper-inflation of the USD,
how does one accept or justify such fees given the title (and intentions)
of Satoshi's own white paper and corresponding software?

I believe the key words "cash system" must be kept in mind throughout all
of these discussions and developments, or else we risk turning Bitcoin into
something other than cash.

Bitcoin will no longer be a P2P cash system if the fees make transactions
prohibitively expensive for all but the wealthiest of individuals and
corporations.

I understand that a careful balance must be struck between (measurable?)
decentralization and Bitcoin's use as an actual cash system; however, those
who are willing to annihilate the latter to maintain ONLY the former must
at least be honest with everyone that they really don't care if Bitcoin
becomes something entirely different than Satoshi's original invention and
intention.

Call it a necessary transformation or reinvention, and by a new name, if
you will; because, with exorbitant fees, it may no longer be accurate or
appropriate to call it Bitcoin: A Peer-to-peer Electronic CASH System.

Respectfully,
Oliver
On Aug 30, 2015 2:38 AM, "Adam Ritter via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I think it’s pretty clear by now that the assumption that all nodes have pretty similar computational resources leads to very misplaced incentives. Ultimately, cryptocurrencies will allow direct outsourcing of computation, making it possible to distribute computational tasks in an economically sensible way.

Wallets should be assumed to have low computational resources and intermittent Internet connections for the foreseeable future if we ever intend for this to be a practical payment system, methinks.



-------------------------------------
On Wednesday, 14 January 2015, at 3:53 pm, Eric Lombrozo wrote:

I thought pubkeys were represented as raw integers (i.e., they're embedded in Script as a push operation whose payload is the raw bytes of the big-endian representation of the integer). As far as I know, DER encoding is only used for signatures. Am I mistaken?


-------------------------------------
On Sat, May 9, 2015 at 12:00 AM, Damian Gomez <dgomez1092@gmail.com> wrote:
[snip code]

Intriguing; and certainly a change of the normal pace around here.


In these posts I am reminded of and sense some qualitative
similarities with a 2012 proposal by Mr. NASDAQEnema of Bitcointalk
with respect to multigenerational token architectures. In particula,r
your AES ModuleK Hashcodes (especially in light of Winternitz
compression) may constitute an L_2 norm attractor similar to the
motherbase birthpoint metric presented in that prior work.  Rethaw and
I provided a number of points for consideration which may be equally
applicable to your work:
https://bitcointalk.org/index.php?topic=57253.msg682056#msg682056

Your invocation of emotive packets suggests that you may be a
colleague of Mr. Virtuli Beatnik?  While not (yet) recognized as a
star developer himself; his eloquent language and his mastery of skb
crypto-calculus and differential-kernel number-ontologies demonstrated
in his latest publication ( https://archive.org/details/EtherealVerses
) makes me think that he'd be an ideal collaborator for your work in
this area.


-------------------------------------
Where do we stand now on which sequencenumbers variation to use? We really
should make a decision now.

On Fri, Aug 28, 2015 at 12:32 AM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

No, that is not what such a notice means.  The part after the "c" in the 
circle is the legal owner.  If the legal owners are not properly 
identified then the notice is not valid.

---
 From Nolo:

What is a valid copyright notice?

A copyright notice should contain:
•the word "copyright"
•a "c" in a circle (©)
•the date of publication, and
•the name of either the author or the owner of all the copyright rights 
in the published work.

For example, the correct copyright for the fourth edition of The 
Copyright Handbook, by Stephen Fishman (Nolo), is Copyright © 1998 by 
Stephen Fishman.

---
from USPTO:

Use of the notice informs the public that a work is protected by 
copyright, identifies the copyright owner, and shows the year of first 
publication.
---

Russ



-------------------------------------
On Thu, Aug 20, 2015 at 08:45:53PM -0400, Milly Bitcoin via bitcoin-dev wrote:

You know, I've noticed you've spent a tremendous amount of time and
energy on this list promoting these kinds of metrics; obviously you're
somewhat of an expert on this compared to the rest of us.

Why don't you look into spearheading one of these analyses yourself to
show us how it's done?


-- 
'peter'[:-1]@petertodd.org
00000000000000000402fe6fb9ad613c93e12bddfc6ec02a2bd92f002050594d
-------------------------------------
On Wed, Aug 19, 2015 at 8:15 AM, Hector Chu <hectorchu@gmail.com> wrote:

without having any blockchain data available; are you referring to a
different type of validation?

If you're running an SPV node that is listening to full nodes on the
network, you can request an unconfirmed transaction from connected peers
after receiving the inventory message they send - that's how unconfirmed
transactions propagate through the node network. This is not 100% proof
that the transaction is valid for inclusion in the blockchain, but it's a
very good indicator.

- Jameson


-------------------------------------
On Aug 11, 2015 12:11 AM, "Sergio Demian Lerner" <sergio.d.lerner@gmail.com>
wrote:
began using the TheBlueMatt relay network, so deteriorating the ratio 2x
does not put miners in a unknown future, but in an future which is far
better than the state they were a year ago.

It's still worse than doubling the block size, which was your main argument.

pushed even faster, fit in a single network packet, and can do without
inv/getdata round-trips because they basically "pay" for the bandwidth
usage by its own proof of work).
the time it takes currently to propagate an empty block.

So yes, better relay protocols (whether you consider "SPV mining" a form of
that or not) reduce the effect of the block size. That does not give any
benefit for reduced interblock times.

Your argument seems to be "centralization pressure is not bad now, because
it already improved a lot... so we can make it worse again by reducing
interblock time"? I disagree that it is not bad, and shorter blocks have
other downsides which were already mentioned.

avoid. It's a real incentive. It must exists so Bitcoin is incentive
compatible. We can talk for hours and hours and we won't prevent miners
from doing it. I predicted it back in 2013, without even being a miner
myself. It's here to stay. Forever. It's a pity Greg chose that awful name
of "SPV" mining instead some cool name like "Instant" mining that we could
market as Bitcoin latest feature :)

breaks the security assumption of SPV clients...

The SPV security assumption is that no hashrate majority will collude in
order to make my transactions incorrectly look confirmed.

With validation-less mining, even a 0.1% hashrate that is part of a group
with 60% hashrate is enough to make that happen.

Of course they won't intentionally do that. No other miner would agree to
do validation-less mining with them again, making it harder for them to
compete. So it is not permissionless: you get higher profitability by
making an agreement with the largest hashrate. I think that is a much worse
centralization effect than having an optional centralized relay network
available... there could even be multiple such networks.

if I don't know you, I know you wouldn't waste 25 BTC to try to cheat me
for 25 BTC with a probability of 1/100, that's for sure. On average, you
loose 24.75 BTC per cheat attempt.

Per cheat attempt, or per bug.

incentives. SPV mining also must be there to prevent malicious actors from
DoS-ing the relay network. If it's there, then the DoS incentive disappears.

I thought about this too. Since headers-first it would be trivial to do: if
our best header is ahead of our best block, hand out an empty template in
createblocktemplate, and we're done.

Unfortunately, Greg Maxwell pointed out that this (even with a time limit)
amplifies selfish mining, since I can propagate headers before propagating
blocks, in order to make others temporarily work on top of my chain.

and that's good for miners. Last, as I already said, having a lower average
block interval strengthens Bitcoin value proposition, so miners would be
delighted that their bitcoins are more worthy.

Only a small constant factor, but yes.

-- 
Pieter
-------------------------------------
Let me make sure I understand this proposal:

On Fri, May 8, 2015 at 11:36 PM, Gregory Maxwell <gmaxwell@gmail.com> wrote:


I'm going to try to figure out how much transaction fee a transaction would
have to pay to bribe a miner to include it. Greg, please let me know if
I've misinterpreted the proposed algorithm. And everybody, please let me
know if I'm making a bone-headed mistake in how I'm computing anything:

Lets say miners are expressing a desire for 600,000 byte blocks in their
coinbases.

computed_max = 600,000 - 600,000/52 = 588,462 bytes.
  --> this is about 23 average-size (500-byte) transactions less than
600,000.
effective_max = 1,176,923

Lets say I want to maintain status quo at 600,000 bytes; how much penalty
do I have?
((600,000-588,462)/588,462)^2 + 1 = 1.00038

How much will that cost me?
The network is hashing at 310PetaHash/sec right now.
Takes 600 seconds to find a block, so 186,000PH per block
186,000 * 0.00038 = 70 extra PH

If it takes 186,000 PH to find a block, and a block is worth 25.13 BTC
(reward plus fees), that 70 PH costs:
(25.13 BTC/block / 186,000 PH/block) * 70 PH = 0.00945 BTC
or at $240 / BTC:  $2.27

... so average transaction fee will have to be about ten cents ($2.27
spread across 23 average-sized transactions) for miners to decide to stay
at 600K blocks. If they fill up 588,462 bytes and don't have some
ten-cent-fee transactions left, they should express a desire to create a
588,462-byte-block and mine with no penalty.

Is that too much?  Not enough?  Average transaction fees today are about 3
cents per transaction.
I created a spreadsheet playing with the parameters:

https://docs.google.com/spreadsheets/d/1zYZfb44Uns8ai0KnoQ-LixDwdhqO5iTI3ZRcihQXlgk/edit?usp=sharing

"We" could tweak the constants or function to get a transaction fee we
think is reasonable... but we really shouldn't be deciding whether
transaction fees are too high, too low, or just right, and after thinking
about this for a while I think any algorithm that ties difficulty to block
size is just a complicated way of dictating minimum fees.

As for some other dynamic algorithm: OK with me. How do we get consensus on
what the best algorithm is? I'm ok with any "don't grow too quickly, give
some reasonable-percentage-minority of miners the ability to block further
increases."

Also relevant here:
"The curious task of economics is to demonstrate to men how little they
really know about what they imagine they can design." - Friedrich August
von Hayek

-- 
--
Gavin Andresen
-------------------------------------
On Thu, Feb 5, 2015 at 12:33 PM, Wladimir <laanwj@gmail.com> wrote:

Thanks to the extremely quick response (a whopping 9 gitian builders
already!), the executables and tarball for rc4 have been uploaded to
the usual place:

https://bitcoin.org/bin/0.10.0/test/

Wladimir


-------------------------------------
Interesting project, Kristov. Two more ideas for fuzzing bitcoin txs:
- random bit flipping from valid txs
- random tx script generators:
  - from a grammar
  - from a stochastic grammar
  - from a random sequence of opcodes

I've made some really small experiments on fuzzing in the past [1][2], and
I'm interested in helping out.

Best,
Manuel

[1] https://github.com/maraoz/json-fuzzer
[2] https://github.com/maraoz/bitcoin-fuzzer

On Tue, Sep 1, 2015 at 3:03 PM, Wladimir J. van der Laan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
This was discussed in IRC, but (did I miss it?) never made it to the list
outside of being buried in a longer summary.

There is a common complain that bitcoin-dev is too noisy.  The response
plan is to narrow the focus of the list to near term technical changes to
the bitcoin protocol and its implementations (bitcoin core, btcd, ...)

Debates over bitcoin philosophy, broader context, etc. will start seeing
grumpy list admins squawk about "off-topic!"

It is a fair criticism, though, that "take it elsewhere!" needs to have
some place as a suggested destination.  The proposal is to create a second
list, bitcoin-tech-discuss or perhaps just 'bitcoin', with a more general
rubric.  This split has served IRC well and generally manages to keep the
noise down to a productive level.  We want this list to achieve that same
goal; if bitcoin-dev is not productive then it's not useful.
-------------------------------------
Whilst it would be nice if miners in *outside* China can carry on forever
regardless of their internet situation, nobody has any inherent "right" to
mine if they can't do the job - if miners in *outside* China can't get the
trivial amounts of bandwidth required through their firewall *TO THE
MAJORITY OF THE HASHRATE* and end up being outcompeted then OK, too bad,
we'll have to carry on without them.


On Mon, Jun 1, 2015 at 12:13 AM, Mike Hearn <mike@plan99.net> wrote:

-------------------------------------
On Aug 12, 2015 10:11 AM, "Thomas Zander via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
Or,

Don't fear this happening at 1 MB, fear this happening at any size. This
needs to be solved regardless of the block size.
Don't worry, the "doing nothing side" is already taking care of this. I
will give the link for the second time...

https://github.com/bitcoin/bitcoin/pull/6470
-------------------------------------
That would also introduce the anomaly of a script that was once valid
becoming later invalid, when nothing varies other than time.  That is
not super compatible with the current model of reprocessing
transactions in later blocks if the block they were first in gets
reorged.

(Not a huge flexibility loss as you can implement "not after" by
making it the previous holders responsibility to spend a "not before"
back to themselves.)

Adam

On 2 June 2015 at 13:52, Stephen <stephencalebmorse@gmail.com> wrote:


-------------------------------------
Gavin, do you use a debit card or credit card? Then you do fit that use
case. When you buy a coffee at Starbucks, it is your bank that pays
Starbuck's bank. So it is with micropayment hubs.
On Sun, Jun 28, 2015 at 1:12 PM, Mark Friedenbach <mark@friedenbach.org>
wrote:



Very few of my own personal Bitcoin transactions fit that use-case.

In fact, very few of my own personal dollar transactions fit that use-case
(I suppose if I was addicted to Starbucks I'd have one of their payment
cards that I topped up every once in a while, which would map nicely onto a
payment channel). I suppose I could setup a payment channel with the
grocery store I shop at once a week, but that would be inconvenient (I'd
have to pre-fund it) and bad for my privacy.

I can see how payment channels would work between big financial
institutions as a settlement layer, but isn't that exactly the
centralization concern that is making a lot of people worried about
increasing the max block size?

And if there are only a dozen or two popular hubs, that's much worse
centralization-wise compared to a few thousand fully-validating Bitcoin
nodes.

Don't get me wrong, I think the Lightning Network is a fantastic idea and a
great experiment and will likely be used for all sorts of great payment
innovations (micropayments for bandwidth maybe, or maybe paying workers by
the hour instead of at the end of the month). But I don't think it is a
scaling solution for the types of payments the Bitcoin network is handling
today.

-- 
--
Gavin Andresen
-------------------------------------
On Feb 19, 2015, at 6:22 AM, Tamas Blummer <tamas@bitsofproof.com> wrote:

I should have added the project description here, as above is only readable with lighthouse:

Java Language Binding for Core Consensus Library

Bitcoin Core 0.10.0 comes with a library for external services that validates Bitcoin transactions with the code base of the core.

The proposed language binding would unleash innovation of JVM application developer without raising concern of a network fork through incompatible alternate implementations of the protocol.

The language binding would be written with lightweight, immutable, self contained data classes that use only language standard libraries, therefore suitable for any service framework.


Tamas Blummer

-------------------------------------
My current idea:

* There's a scheduled hardcap that goes up over time.

* Miners vote on the blocksize limit within the hardcap, choosing the new
votecap. No particular idea for scheduling change. The 2016 block period
seems a bit long though, in case of sudden peak load.
(I'd suggest rolling vote over X blocks, enacted Y blocks later (with votes
counted from block A to block B = block A+X, the change is enacted at block
C = B+Y = A+X+Y). I'm fine with fixed-period schedules too if they span a
reasonable time, such as IMHO 2 days - we need rapid peak adjustment. No
suggestion on vote result calculation mechanism.)

* Casting votes are free.

* The mean (average) blocksize over the last time period X is calculated
for every block, or at the end of every fixed-length period (depending on
what scheduling is used for votes).

* Creating blocks larger than the mean but below the votecap raises the
difficulty target for the miner (and slightly raises the mean for future
blocks).

* The degree of difficulty raise depends on where between the mean and
votecap that the size of the given block is (and it follows that lots of
votes for large raise reduces per-extra-Kb penalty, allowing for cheaper
peak load adjustment if a large miner majority agrees). The degree of
increase may be either linear or logarithmic, I've got no suggestion
currently on any particular metric.
(Some might think this is an easy way for miners to collude to make large
blocks cheaper. If so, you could commit to only pay fee to miners that
don't vote for a block size above the size you accept, as a
counter-incentive.)

* Question: When the votecap is lowered, should the calculated mean be
forced down to follow (forcing a penalty for making blocks close to the
votecap straight after the change)? If so, how? Or should it be allowed to
fall naturally as new blocks with size below the votecap are created?

This is how miners would pay for actually creating larger blocks, and
leaves us with three methods of keeping the size in check (hardcap, votecap
and softcap). The softcap mechanism is then our third check to use if
deemed necessary (orphaning valid blocks if considered problematically
large). This third option do not need coordination with miners, they just
need to be aware which block size is accepted by the community.

I can't think of any sensible non-miner mechanism of deciding max block
size outside of using a community coordinated softcap, anything else will
not work reliably. Too hard to measure objectively and judge fairly.

The community would thus agree on a hardcap schedule in advance, and have
the option to threaten orphaning blocks via softfork later on if
circumstances would change and the votecap is too large.
-------------------------------------
I sent out an email after 48 hours of dealing with trying to open up my ports for Bitcoin, I was quite frustrated and angry since I had to call like 10 times and I was making zero progress. Most of the AT&T people didn't give me any helpful clues on how to fix the situation. The original email described how there is a firewall in the DVR, and I thought it was blocking the ports. It is true there is a uncontrollable firewall in the DVR, it is false this blocks 8333.

The actual problem is due to AT&T Uverse customers being forced to use a private dynamic IP, the IP is literally hidden from the internet, so it isn't possible to send any requests at it. It will literally ignore pings across all ports. So the solution is to switch to public static IP and make sure you allow incoming traffic. 

It's not so simple though, AT&T will not let you have a public static IP without paying. I've had my router reset 10 times today by AT&T (probably automatically) and it comes back with a private dynamic IP. Then I have to reset it to use public IP and that lasts less than an hour. It literally went from open to closed while typing this email... the IP address went from public to private dynamic. 

https://i.gyazo.com/3c732687fc3d21acb7d62f6d0e23a346.png

This is making using Bitcoin Core almost impossible. I'm at least getting some synch now but maybe a few days of blocks the entire day, cause I can't sit here all day with the computer and keep fixing it.

The proof is in the pudding, there are 37 nodes using AT&T in the ENTIRE world. AT&T is a massive ISP so this is strong evidence that using Bitcoin Core as a full node on AT&T is extremely difficult and actually just about impossible.

https://i.gyazo.com/90beebe056f5fc338165e8d200536c06.png

The other big ISPs have pathetic numbers also due to the same sort've things that AT&T does, but at least Comcast has 400 nodes. AT&T is much harder to use than any other ISP I've dealt with when it comes to Bitcoin Core.

I apologize for sending out the wrong info the first time, although it is still worth noting the DVR firewall is out of your control, which might be a problem if not now then in the future. In any case AT&T has effectively blocked full nodes for Bitcoin Core via the private subnet, and the disability to change it to public without paying $15 more per month, and buying a $15 connection service so they will give you that info (if you dont pay the connection 'specialists' hang up on you). 

It is important to note this is not Bitcoin specific, but effects every program that depends on freely open ports. I don't think AT&T has anything against Bitcoin, it's just their security settings and policies have disabled Bitcoin Core for most customers. Also important to note this isn't a problem specific to AT&T, all the big ISPs are doing similar things. I believe the changes in ISP protocol are the main driving force behind the massive decline in Bitcoin nodes. Another big factor is firewalls, most people can't even remove the firewalls enough to open ports at will. The community needs to educate people on how to use Bitcoin Core when facing these intensifying security measures, or the decline of node numbers will continue.

 

 
-------------------------------------
On Wed, Feb 04, 2015 at 03:23:23PM +0100, Isidor Zeuner wrote:

Have you looked at Armory? IIRC they do this kind of stuff.

-- 
'peter'[:-1]@petertodd.org
0000000000000000165ecbd638ec09226f84c34d3d775d34ca5df4abfa8cb57c
-------------------------------------
Alex,

With all due respect, right now the biggest challenge facing Bitcoin is not technical but political. I would love to see this list go back to technical discussions, but unfortunately, until this political stuff is resolved, even technical discussion is purely philosophical as there’s little chance of actually making good progress on consensus…which in a space where everything depends on consensus pretty much makes everything else moot.


-------------------------------------
Hello Tom, Daniele --

Thank you Tom for pointing out the knapsack problem to all of us.  I will include a note about it when I make the other corrections to the Fee Market paper.

I agree with what Daniele said previously.  The other "non-greedy" solutions to the knapsack problem are most relevant when one is choosing from a smaller number of items where certain items have a size similar to the size of the knapsack itself.  For example, these other solutions would be useful if transactions were often 50 kB, 100 kB, 400 kB in size, and we were trying to find the optimal TX set to fit into a 1 MB block.  

However, since the average transaction size is ~500 bytes, even with a block size limit of 1 MB, we are looking at up to 2000 transactions.  The quantization effects become small.  Rather than 22 triangles as shown in Fig. 3 (http://imgur.com/rWxZddg), there are hundreds or a few thousands, and we can reasonably approximate the discrete set of points as a continuous curve.  Like Daniele pointed out, the greedy algorithm assumed in the paper is asymptotically optimal in such a case.

Best regards,
Peter
-------------------------------------


The issue isn’t really whether it’s 1MB or 2MB or 4MB or 8MB or whatever. First of all, the burden of justifying this change should be on those proposing a hardfork. The default is to not have a hard fork. Second of all, it’s not really about *whether* the block size is increased…but about *when* and *how* it is increased. There’s a good argument to be made that right now it is more important to address issues such as the fact that validation is so expensive (which as others and myself have pointed out has led to a collapse of the security model in the past, requiring manual intervention to temporarily “fix”)…and the fact that we don’t yet have great solutions to dealing with fees, which are a crucial component of the design of the protocol.
-------------------------------------
On Wed, May 6, 2015 at 10:12 PM, Matt Corallo <bitcoin-list@bluematt.me>
wrote: > Recently there has been a flurry of posts by Gavin at >
http://gavinandresen.svbtle.com/ which advocate strongly for increasing >
the maximum block size. However, there hasnt been any discussion on this >
mailing list in several years as far as I can tell.

Thanks Matt; I was actually really confused by this sudden push with
not a word here or on Github--so much so that I responded on Reddit to
people pointing to commits in Gavin's personal repository saying they
were reading too much into it.

So please forgive me for the more than typical disorganization in this
message; I've been caught a bit flatfooted on this and I'm trying to
catch up. I'm juggling a fair amount of sudden pressure in my mailbox,
and trying to navigate complex discussions in about eight different
forums concurrently.

There have been about a kazillion pages of discussion elsewhere
(e.g. public IRC and Bitcointalk; private discussions in the past),
not all of which is well known, and I can't hope to summarize even a
tiny fraction of it in a single message-- but that's no reason to not
start on it.

certainly has a LOT of technical tradeoffs to consider.

There are several orthogonal angles from which block size is a concern
(both increases and non-increases). Most of them have subtle implications
and each are worth its own research paper or six, so it can be difficult
to only touch them slightly without creating a gish gallop that is hard
to respond to.

We're talking about tuning one of the fundamental scarcities of the
Bitcoin Economy and cryptosystem--leaving the comfort of "rule by
math" and venturing into the space of political decisions; elsewhere
you'd expect to see really in-depth neutral analysis of the risks and
tradeoffs, technically and economically.  And make no mistake: there
are real tradeoffs here, though we don't know their exact contours.

Fundamentally this question exposes ideological differences between people
interested in Bitcoin.  Is Bitcoin more of a digital gold or is it more
of a competitor to Square?  Is Bitcoin something that should improve
personal and commercial autonomy from central banks?  From commercial
banks? Or from just the existing status-quo commercial banks?   What are
people's fundamental rights with Bitcoin?  Do participants have a
right to mine? How much control should third parties have over their
transactions?  How much security must be provided? Is there a deadline
for world domination or bust?  Is Bitcoin only for the developed world?
Must it be totally limited by the most impoverished parts of the world?

Bitcoin exists at the intersection of many somewhat overlapping belief
systems--and people of many views can find that Bitcoin meets their
needs even when they don't completely agree politically.  When Bitcoin
is changed fundamentally, via a hard fork, to have different properties,
the change can create winners or losers (if nothing else, then in terms
of the kind of ideology supported by it).

There are non-trivial number of people who hold extremes on any of
these general belief patterns; Even among the core developers there is
not a consensus on Bitcoin's optimal role in society and the commercial
marketplace.

To make it clear how broad the views go, even without getting into
monetary policy... some people even argue that Bitcoin should act
as censor-resistant storage system for outlawed content; -- I think
this view is unsound, not achievable with the technology, and largely
incompatible with Bitcoin's use as a money (because it potentially
creates an externalized legal/harassment liability for node operators);
but these are my personal value judgments; the view is earnestly held
by more than a few; and that's a group that certainly wants the largest
possible blocksizes (though even then that won't be enough).

The subject is complicated even more purely on the technical side
by the fact that Bitcoin has a layered security model which is not
completely defined or understood: Bitcoin is secure if a majority of
hashrate is "honest" (where "honesty" is a technical term which means
"follows the right rules" without fail, even at a loss), but why might
it be honest? That sends us into complex economic and social arguments,
and the security thresholds start becoming worse when we assume some
miners are economically rational instead of "honest".

consistently full or very nearly full. What we see today are

To elaborate, in my view there is a at least a two fold concern on this
particular ("Long term Mining incentives") front:

One is that the long-held argument is that security of the Bitcoin system
in the long term depends on fee income funding autonomous, anonymous,
decentralized miners profitably applying enough hash-power to make
reorganizations infeasible.

For fees to achieve this purpose, there seemingly must be an effective
scarcity of capacity.  The fact that verifying and transmitting
transactions has a cost isn't enough, because all the funds go to pay
that cost and none to the POW "artificial" cost; e.g., if verification
costs 1 then the market price for fees should converge to 1, and POW
cost will converge towards zero because they adapt to whatever is
being applied. Moreover, the transmission and verification costs can
be perfectly amortized by using large centralized pools (and efficient
differential block transmission like the "O(1)" idea) as you can verify
one time instead of N times, so to the extent that verification/bandwidth
is a non-negligible cost to miners at all, it's a strong pressure to
centralize.  You can understand this intuitively: think for example of
carbon credit cap-and-trade: the trade part doesn't work without an
actual cap; if everyone was born with a 1000 petaton carbon balance,
the market price for credits would be zero and the program couldn't hope
to share behavior. In the case of mining, we're trying to optimize the
social good of POW security. (But the analogy applies in other ways too:
increases to the chain side are largely an externality; miners enjoy the
benefits, everyone else takes the costs--either in reduced security or
higher node operating else.)

This area has been subject to a small amount of academic research
(e.g. http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2400519). But
there is still much that is unclear.

The second is that when subsidy has fallen well below fees, the incentive
to move the blockchain forward goes away.  An optimal rational miner
would be best off forking off the current best block in order to capture
its fees, rather than moving the blockchain forward, until they hit
the maximum. That's where the "backlog" comment comes from, since when
there is a sufficient backlog it's better to go forward.  I'm not aware
of specific research into this subquestion; it's somewhat fuzzy because
of uncertainty about the security model. If we try to say that Bitcoin
should work even in the face of most miners being profit-maximizing
instead of altruistically-honest, we must assume the chain will not
more forward so long as a block isn't full.  In reality there is more
altruism than zero; there are public pressures; there is laziness, etc.

One potential argument is that maybe miners would be _regulated_ to
behave correctly. But this would require undermining the openness of the
system--where anyone can mine anonymously--in order to enforce behavior,
and that same enforcement mechanism would leave a political level to
impose additional rules that violate the extra properties of the system.

So far the mining ecosystem has become incredibly centralized over time.
I believe I am the only remaining committer who mines, and only a few
of the regular contributors to Bitcoin Core do. Many participants
have never mined or only did back in 2010/2011... we've basically
ignored the mining ecosystem, and this has had devastating effects,
causing a latent undermining of the security model: hacking a dozen or
so computers--operated under totally unknown and probably not strong
security policies--could compromise the network at least at the tip...
Rightfully we should be regarding this an an emergency, and probably
should have been have since 2011.  This doesn't bode well for our ability
to respond if a larger blocksize goes poorly. In kicking the can with
the trivial change to just bump the size, are we making an implicit
decision to go down a path that has a conclusion we don't want?

(There are also shorter term mining incentives concerns; which Peter
Todd has written more about, that I'll omit for now)


I made a few relevant points back in 2011
(https://en.bitcoin.it/w/index.php?title=Scalability&action=historysubmit&diff=14273&oldid=14112)
after Dan Kaminsky argued that Bitcoin's decentralization was pretext:
that it was patently centralized since scaling directly in the network
would undermine decentralization, that the Bitcoin network necessarily
makes particular tradeoffs which prevent it from concurrently being all
things to all people.  But tools like the Lightning network proposal could
well allow us to hit a greater spectrum of demands at once--including
secure zero-confirmation (something that larger blocksizes reduce if
anything), which is important for many applications.  With the right
technology I believe we can have our cake and eat it too, but there needs
to be a reason to build it; the security and decentralization level of
Bitcoin imposes a _hard_ upper limit on anything that can be based on it.

Another key point here is that the small bumps in blocksize which
wouldn't clearly knock the system into a largely centralized mode--small
constants--are small enough that they don't quantitatively change the
operation of the system; they don't open up new applications that aren't
possible today. Deathandtaxes on the forum argued that Bitcoin needs
a several hundred megabyte blocksize to directly meet the worldwide
transaction needs _without retail_... Why without retail? Retail needs
near instant soft security, which cannot be achieved directly with a
global decentralized blockchain.

I don't think 1MB is magic; it always exists relative to widely-deployed
technology, sociology, and economics. But these factors aren't a simple
function; the procedure I'd prefer would be something like this: if there
is a standing backlog, we-the-community of users look to indicators to
gauge if the network is losing decentralization and then double the
hard limit with proper controls to allow smooth adjustment without
fees going to zero (see the past proposals for automatic block size
controls that let miners increase up to a hard maximum over the median
if they mine at quadratically harder difficulty), and we don't increase
if it appears it would be at a substantial increase in centralization
risk. Hardfork changes should only be made if they're almost completely
uncontroversial--where virtually everyone can look at the available data
and say "yea, that isn't undermining my property rights or future use
of Bitcoin; it's no big deal".  Unfortunately, every indicator I can
think of except fee totals has been going in the wrong direction almost
monotonically along with the blockchain size increase since 2012 when
we started hitting full blocks and responded by increasing the default
soft target.  This is frustrating; from a clean slate analysis of network
health I think my conclusion would be to _decrease_ the limit below the
current 300k/txn/day level.

This is obviously not acceptable, so instead many people--myself
included--have been working feverishly hard behind the scenes on Bitcoin
Core to increase the scalability.  This work isn't small-potatoes
boring software engineering stuff; I mean even my personal contributions
include things like inventing a wholly new generic algebraic optimization
applicable to all EC signature schemes that increases performance by 4%,
and that is before getting into the R&D stuff that hasn't really borne
fruit yet, like fraud proofs.  Today Bitcoin Core is easily >100 times
faster to synchronize and relay than when I first got involved on the
same hardware, but these improvements have been swallowed by the growth.
The ironic thing is that our frantic efforts to keep ahead and not
lose decentralization have both not been enough (by the best measures,
full node usage is the lowest its been since 2011 even though the user
base is huge now) and yet also so much that people could seriously talk
about increasing the block size to something gigantic like 20MB. This
sounds less reasonable when you realize that even at 1MB we'd likely
have a smoking hole in the ground if not for existing enormous efforts
to make scaling not come at a loss of decentralization.


I'm curious as to what discussions people have seen; e.g., are people
even here aware of these concerns? Are you aware of things like the
hashcash mediated dynamic blocksize limiting?  About proposals like
lightning network (instant transactions and massive scale, in exchange
for some short term DOS risk if a counterparty opts out)?   Do people
(other than Mike Hearn; I guess) think a future where everyone depends
on a small number of "Google scale" node operations for the system is
actually okay? (I think not, and if so we're never going to agree--but
it can be helpful to understand when a disagreement is ideological).


-------------------------------------
OP_0 gives a zero length byte array because OP_0 == 0x00 which is equivalent to pushdata with zero length.

OP_EQUAL compares byte strings as-is. So it will push "false" because empty string is not the same as a single-byte string with 0x00 byte in it. Value "false" in turn is encoded as empty string, just like result of OP_0.


-------------------------------------
Pieter Wuille 於 2015-08-01 16:45 寫到:


Since I'm using "30 days after 75% miner support", the actual deployment 
period will be longer than 30 days. Anyway, if all major exchanges and 
merchants agree to upgrade, people are forced to upgrade immediately or 
they will follow a worthless chain.



Since the block reward is miners' major income source, no rational miner 
would create mega blocks unless the fee could cover the extra orphaning 
risk. Blocks were not constantly full until recent months, and many 
miners are still keeping the 750kB soft limit. This strongly suggests 
that we won't have 4MB blocks now even Satoshi set a 8MB limit.

I don't have the data now but I believe the Satoshi Dice model failed 
not primarily due to the 1MB cap, but the raise in BTC/USD rate. Since 
minting reward is a fixed value in BTC, the tx fee must also be valued 
in BTC as it is primarily for compensating the extra orphaning risk. As 
the BTC/USD rate increases, the tx fee measured in USD would also 
increase, making micro-payment (measured in USD) unsustainable.

We might have less full nodes, but it was Satoshi's original plan: "At 
first, most users would run network nodes, but as the network grows 
beyond a certain point, it would be left more and more to specialists 
with server farms of specialized hardware. A server farm would only need 
to have one node on the network and the rest of the LAN connects with 
that one node." Theoretically, we only require one honest full node to 
prove wrongdoing on the blockchain and tell every SPV nodes to blacklist 
the invalid chain.

I think SPV mining exists long before the 1MB block became full, and I 
don't think we could stop this trend by artificially suppressing the 
block size. Miners should just do it properly, e.g. stop mining until 
the grandparent block is verified, which would make sure an invalid fork 
won't grow beyond 2 blocks.





If we could have a longer initial ramp up period, we may adopt a slower 
long term parameter. I think we should at least restore the original 
32MB limit in a reasonable time frame, say 6-8 years, instead of 20 
years in your proposal. If you believe Bitcoin should become a global 
settlement network, 32MB would be the very minimum as that is only 75% 
of current SWIFT traffic.

-------------------------------------
Since you missed it, here is the suggestion again:
http://gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf


On Sun, Jun 14, 2015 at 6:06 AM, Mats Henricson <mats@henricson.se> wrote:




-- 
Jeff Garzik
Bitcoin core developer and open source evangelist
BitPay, Inc.      https://bitpay.com/
-------------------------------------
The only reason why Bitcoin has grown the way it has, and in fact the only
reason why we're all even here on this mailing list talking about this, is
because Bitcoin is growing, since it's "better money than other money". One
of the key characteristics toward that is Bitcoin being inexpensive to
transact. If that characteristic is no longer true, then Bitcoin isn't
going to grow, and in fact Bitcoin itself will be replaced by better money
that is less expensive to transfer.

So the importance of this issue cannot be overstated -- it's compete or die
for Bitcoin -- because people want to transact with global consensus at
high volume, and because technology exists to service that want, then it's
going to be met. This is basic rules of demand and supply. I don't
necessarily disagree with your position on only wanting to support
uncontroversial commits, but I think it's important to get consensus on the
criticality of the block size issue: do you agree, disagree, or not take a
side, and why?


On Tue, Aug 11, 2015 at 2:51 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

-------------------------------------
I think it would be helpful if we could all *chill* and focus on the solid
engineering necessary to make Bitcoin succeed.

p.


On Mon, Jun 1, 2015 at 7:02 PM, Chun Wang <1240902@gmail.com> wrote:

-------------------------------------
.,
,
/.
, /,

,.
   / ,
..
,,,  . // .,      .

_. ...  ..   ._.

,    _


,



,
,
  , , ...     _  _.

,.

.  ,.,    _.
.,    ,  ..
,

,,

._

.  .

_
.
,
,     ,    ,   /..,,

/ ,

.     .

_
.,. _.. ,
,

.. _
   ..

,.,, _
, _
,
///
. ,

   / . ,.
  ,
,.,
. ,
, .,   ,. ._ ,  ,,,//

,        ,
.

,

,
  . . ,

, //  .
,  ,
/

      _,.

, . ,, .

..
  /,/ .
.


  .   .,,_//
,,
.,  .

.  /_. ,
/
.
  /
.._
.
,, / .
   . _ ,
,  ,
/     ,    _ .,
, ,,, ..  ,
  ,

  /.,.
  /. /
. ,/  ,

. .   /,
/,
._
   ,/.
_
.,
,//
, .,,, , ,    , ,
,

,.   ,.,.  .

,  .    ,.  .,   ,
/   _
.
/
  ,.,. ,
,._


,,

, _ _ ,

,
. ,,   ,  _


_..,

  ,
// ,
__ /
!;"$'''. b
    __

On Sat, Dec 12, 2015, 3:01 PM Jorge Timón <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On 2015-09-01 10:16, Chris D'Costa via bitcoin-dev wrote:

You are describing the essence of sidechains.  You might want to check
out Elements Alpha, which has some outrageous experimental changes to
transaction structure.  It's a technical Bitcoin sandbox which doesn't
require launching yet another altcoin.

-------------------------------------
On Tue, Aug 18, 2015 at 11:54 AM, jl2012 via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Apart from classifying all potential consensus rule changes and
recommend a deployment path for each case, deploying an
uncontroversial hardfork is one of the main goals of bip99:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009837.html


The uncontroversial hardfork doesn't need to change the maximum block
size: there's plenty of hardfork proposals out there, some of them
very well tested (like the proposed hardfork in bip99).


I disagree with this. I think it should be schedule at least a year
after it is deployed in the newest versions.
Maybe there's something special about June 2016 that I'm missing.

-------------------------------------
On Mon, Sep 28, 2015 at 10:16 PM, Dave Scotese via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

As an aside, this list loses utility if people insist on taking
tangential questions to the list in the middle of threads. It's
preferable to either split the thread or take the message off list.

The naming arose from a series of historical naming-by-comparisons:

The bitcoin network has self-arising forks in state when miners
concurrently create blocks. These are natural, unavoidable, and
self-resolving.

If a nodes enforce different and incompatible rules-- for example,
some decide to require that the subsidy stay at 25 BTC forever, then a
fork may come into existence which is not self resolvable.

Thus the term hardfork arose to talk about rule changes which were
incompatible with the existing network, would require all users to
upgrade, would exclude all non-consenting users from the resulting
system, and which have the power to arbitrarily rewrite rules.  This
is in contrast to "forks" which are boring, natural, and happen every
day.

Its often possible to make critical fixes and powerful improvements to
the Bitcoin consensus rules by using the unavoidable power of miners
to filter transactions according to their own rules.  New features and
fixes can be carved out of existing "do anything" space in the
protocol: like carving a statue out of a block of marble. Doing so
reduces the incidence of flag days which are costly to coordinate and
actively risky for users and avoids forcing constant software churn,
which is bad for decentralization. Such changes are a strict narrowing
of permissible actions. And as such, so long as they have a
super-majority hashpower behind them any network forking that happens
to result from them is automatically self-resolving.

So by contrast with hardfork the term softfork came into use to
describe these _compatible_ protocol rule changes.

There is explicit support for compatible rule changes the bitcoin
protocol in the form of no-op opcodes and free form, non-enforced,
version fields (for example). Every fix or enhancement you've heard
about to Bitcoin's consensus rules (going back to the system's
original author) was performed via some form of this mechanism.

In the modern form, the behavior to be soft-forked out is first made
non-standard (if it wasn't already-- they almost always are) meaning
that participants will not relay, mine, or display unconfirmed txn in
their wallets transactions which violate the new rule.  But if a
violation shows up in a block, the block is still accepted.  After
that the blockchain itself is used to coordinate a vast super-majority
of hashpower (recently 95% has been used) agreeing to enforce the new
rule which results in confidence confidence of low disruption on
account of the enforcement. Then when the threshold is reached, they
enforce (automatically).  Old software continues to enforce all the
old rules they always enforced, the only difference in behavior
relates to non-standard transactions and contests between otherwise
valid blocks.  Even unupgraded participants can tell that the network
is doing something new on account of the block version changing (and,
for example, Bitcoin Core warns users about this).

The primary disadvantage of this approach is that it only allowed you
to carve functionality of of "do anything" space, which is quite
natural for some features (especially since the Bitcoin protocol
includes tons of do anything space)--- e.g. height in coinbase, DER
strictness, transactions that have integer overflow creating a
kazillion coins-- but less natural for others.

Of course, it's always possible for the majority of hashpower to have
hidden transaction exclusion rules that _no one_ but them knows about
and this cannot be prevented, but at least the mechanism proscribed in
modern soft-forks is transparent (the network tells you that its doing
something you don't understand).

-------------------------------------
Although the chance is very slim, it is possible to have multiple 
hardforks sharing the same flag block. For example, different proposals 
may decide the flag time based on voting result and 2 proposals may have 
the same flag time just by chance. The coinbase message is to preclude 
any potential ambiguity. It also provides additional info to warning 
system of non-upgrading nodes.

If we are pretty sure that there won't be other hardfork proposal at the 
same time, the coinbase message may not be necessary. With some prior 
collaboration, this may also be avoided (e.g. no sharing flag block 
allowed as consensus rules of the hardforks)

The "version 0" idea is not compatible with the version bits voting 
system, so I have this hardfork bit BIP after thinking more carefully. 
Otherwise they are technically similar.

Michael Ruddy 於 2015-08-01 09:05 寫到:


-------------------------------------
We're not modifying BIP 70, it's now immutable and can only be extended.

There's really not much point in having a dedicated chain ID for regtest
mode. You shouldn't be finding BIP70 requests for regtest outside of your
own developer machine, where the id doesn't matter.
-------------------------------------
RE: fixing sigop counting, and building in UTXO cost: great idea! One of
the problems with this debate is it is easy for great ideas get lost in all
the noise.

RE: a hard upper limit, with a dynamic limit under it:

I like that idea. Can we drill down on the hard upper limit?

There are lots of people who want a very high upper limit, right now (all
the big Bitcoin companies, and anybody who thinks as-rapid-as-possible
growth now is the best path to long-term success). This is the "it is OK if
you have to run full nodes in a data center" camp.

There are also lots of people who want an upper limit low enough that they
can continue to run Bitcoin on the hardware and Internet connection that
they have (or are concerned about centralization, so want to make sure
OTHER people can continue to run....).

Is there an upper limit "we" can choose to make both sets of people mostly
happy? I've proposed "must be inexpensive enough that a 'hobbyist' can
afford to run a full node" ...

Is the limit chosen once, now, via hard-fork, or should we expect multiple
hard-forks to change it "when necessary" ?

The economics change every time the block reward halves, which make me
think that might be a good time to adjust the hard upper limit. If we have
a hard upper limit and a lower dynamic limit, perhaps adjusting the hard
upper limit (up or down) to account for the block reward halving, based on
the dynamic limit....



RE: the lower dynamic limit algorithm:  I REALLY like that idea.

-- 
--
Gavin Andresen
-------------------------------------
On Thu, Feb 12, 2015 at 3:15 PM, Mike Hearn <mike@plan99.net> wrote:


I think that is a misdirection on your part. The point of replace-by-fee is
to make 0-confirms reliably unreliable. Currently people can "get away"
with 0-confirms but it's only because most people arent actively double
spending, and when they do it is for higher value targets. Double spend
attacks *are* happening a lot more frequently than is being admitted here,
according to Peter from work with various clients.

Like single address reuse, people have gotten used to something which is
bad. Generally accepting 0-conf is also a bad idea(tm) and instant
confirmation solutions should be sought elsewhere. There are already
interesting solutions and concepts: greenaddress for example, and
CHECKLOCKTIMEVERIFY micropayment channels for example. Rather than
supporting and promoting risky 0-confirms, we need to spend time on better
alternative solutions that will work for everyone and not during the
honeymoon phase where attackers are fewer.
-------------------------------------
I'm also strongly in favor of moving forward with this plan.

A couple of points:
1) There has been too much confusion in looking at segwit as an alternative
way to increase the block size and I think that is incorrect.  It should
not be drawn into the block size debate as it brings many needed
improvements and tools we'd want even if no one were worried about block
size now.
2) The full capacity increase plan Greg lays out makes it clear that we can
accomplish a tremendous amount without a contentious hard fork at this
point.
3) Let's stop arguing endlessly and actually do work that will benefit
everyone.




On Sun, Dec 20, 2015 at 11:33 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Mon, Dec 21, 2015 at 5:14 AM, jl2012 via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


"Prunable in transmission" means that you have to include it when not
sending the witnesses?

That is a name collision with UTXO set prunable.  My initial thought when
reading that was "but scriptSigs are inherently prunable, it is
scriptPubKeys that have to be held in the UTXO database" until I saw the
"in transmission" clarification.
-------------------------------------

But is this useful without having decided on a way to signal which blocks pruned nodes do have?

It looks like the part between paranthesis is speculation and should be left to a future BIP. 

Wladimir

-------------------------------------
When blocks are found under or over the 10 minute threshold, hashing
difficulty is raised or reduced dinamically to keep a balance. This
intelligent measure has avoided us having discussions and kept a balance.

The same way you can't assume how much hashpower there will be to find the
next blocks, why can't we have a
function that adapts to the transactional volume on the blockchain, one
which allows us to grow/shrink an acceptable maximum block size. We're not
putting caps on processing, why should we put a date based cap on
transactional volume per block? You can't predict the future, but you can
look at what's happened recently to correct these limits.

Such function/filter should be able to recognize real sustained growth in
transactional volume and let us adjust the maximum accepted blocksize to
allow for the organic growth that will come due to real activity from
things like distributed market-places, decentralized bitcoin based services
(and all the things the community dreams about and might be building
already), truly decentralized technological breakthroughs that geniunely
need to use the blockchain. <Going the off-chain way only leads to
centralization and personal/corporate agendas, which to me goes against the
Bitcoin ethos>

It should be able to adapt fast enough so that we don't have episodes where
people need to wait 4 hours to days for transactions to get on the
blockchain and be confirmed. I believe proposals that include "every
100,000 blocks" are out of touch with reality, the blocksize needs to adapt
the same way blockdifficulty already adapts to growth or lack of hashing
power.

I'm not a statistician/mathematician, but I'm sure if we propose the
parameters that need to be considered for a realistic blocksize that
reflects the needs of the Bitcoin network users, there's plenty of
crypto/statistician/mathematician brain power to propose such filtering
function here.

Things that could be considered:
- median number of transactions per block (between 6 to 12 hours, you
should be able to adjust to a real shopping sprint for instance, or huge
pop band/artist decides to sell concert tickets on Bitcoin)
- median fees offered per transaction (can we detect spammers)
- median blocksizes
- median size per transaction
- number of new addresses signing off transactions, number of addresses
we've already seen in the blockchain before (are these spammers creating
lots of new addresses to move around the same outputs, is there an
efficient way to detect the likelyhood of a transaction being spam? Bayes?
No clue, no mathematician)
- median velocity between which an address receives an input and sends it
to another one?
- more things I've no knowledge of since I'm not familiar with the details,
but could immediatly come to mind to the experts.

Mining Centralization is already happening due to its competitive nature,
we don't complain or try to force hashing limits, we shouldn't do the same
for storage. There will be no shortage of blockchain mirrors, and those
interested in running full nodes, will surely find a way to do so.

Angel

http://twitter.com/gubatron

On Fri, Jul 17, 2015 at 4:29 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
The hashpower is a function of the block reward (subsidy + fees): it's
economically irrational to have costs greater than the reward (better just
turn off your miners) and in a perfect competition (a theoretical model)
profits tend to zero. That is, the costs tend to equal revenue (block
reward).
On Dec 26, 2015 6:38 PM, "Eric Lombrozo" <elombrozo@gmail.com> wrote:

-------------------------------------
On Wednesday, May 27, 2015 1:48:05 AM Pieter Wuille wrote:

I suggest adding a section describing how this interacts with and changes GBT.

Currently, the client tells the server what the highest block version it 
supports is, and the server indicates a block version to use in its template, 
as well as optional instructions for the client to forcefully use this version 
despite its own maximum version number. Making the version a bitfield 
contradicts the increment-only assumption of this design, and since GBT 
clients are not aware of overall network consensus state, reused bits can 
easily become confused. I suggest, therefore, that GBT clients should indicate 
(instead of a maximum supported version number) a list of softforks by 
identifier keyword, and the GBT server respond with a template indicating:
- An object of softfork keywords to bit values, that the server will accept.
- The version number, as presently conveyed, indicating the preferred softfork 
flags.

Does this sound reasonable, and/or am I missing anything else?

Luke


-------------------------------------
I only got into Bitcoin in 2011, after the block size limit was already in place. After going through some more of the early history of Bitcoin to better understand the origins of this, things are starting to come into better perspective.

Initially there was no block size limit - it was thought that the fee market would naturally develop and would impose economic constraints on growth. But this hypothesis failed after a sudden influx of new uses. It was still too easy to attack the network. This idea had to wait until the network was more mature to handle things.

Enter a “temporary” anti-spam measure - a one megabyte block size limit. Let’s test this out, then increase it once we see how things work. So far so good…

Except…well:

1) We never really got to test things out…a fee market never really got created, we never got to see how fees would really work in practice.

2) Turns out the vast majority of validation nodes have little if anything to do with mining - validators do not get compensated…validation cost is externalized to the entire network.

3) Miners don’t even properly validate blocks. And the bigger the blocks get, the greater the propensity to skip this step. Oops!

4) A satisfactory mechanism for thin clients to be able to securely obtain reasonably secure, short proofs for their transactions never materialized.
-------------------------------------
Can anyone opposed to this proposal articulate in plain english the worst
case scenario(s) if it goes ahead?

Some people in the conversation appear to be uncomfortable, perturbed,
defensive etc about the proposal . But I am not seeing specifics on why it
is not a feasible plan.

From:  Mike Hearn <mike@plan99.net>
Date:  Friday, 8 May, 2015 2:06 am
To:  Btc Drak <btcdrak@gmail.com>
Cc:  Bitcoin Dev <bitcoin-development@lists.sourceforge.net>
Subject:  Re: [Bitcoin-development] Block Size Increase


I have explained why I believe there is some urgency, whereby "some urgency"
I mean, assuming it takes months to implement, merge, test, release and for
people to upgrade.

But if it makes you happy, imagine that this discussion happens all over
again next year and I ask the same question.

----------------------------------------------------------------------------
-- One dashboard for servers and applications across Physical-Virtual-Cloud
Widest out-of-the-box monitoring support with 50+ applications Performance
metrics, stats and reports that give you Actionable Insights Deep dive
visibility with transaction tracing using APM Insight.
http://ad.doubleclick.net/ddm/clk/290420510;117567292;y_____________________
__________________________ Bitcoin-development mailing list
Bitcoin-development@lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development

-------------------------------------
Thanks for giving serious consideration to my post.

With regard to your question "if a transaction spends a "coin" that
ends in "1" and creates a new coin that ends in "1", which partition
should process the transaction?", I would answer that only one
partition is involved. In other words, there are N independent block
chains that never cross paths.

With regard to your question "what is the prior data needed to
validate that kind of TXs?" I do not understand what this means. If
you can dumb it down a bit that would be good because there could be
some interesting concern in this question.

Since partitions are completely segregated, there is no need for a
node to work on multiple partitions simultaneously. For attacks to be
defeated a node needs to be able to work on multiple partitions in
turn, not at the same time. The reason is because if the computing
power of the good-faith nodes is unbalanced this gives attackers an
unfair advantage.

On 12/9/15, Loi Luu via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#1f498d">
    BIP has now been renamed:<br>
<a class="moz-txt-link-freetext" href="https://github.com/bitcoincoltd/bips/blob/master/bip-00xx-dynamic-rate-lookup.mediawiki">https://github.com/bitcoincoltd/bips/blob/master/bip-00xx-dynamic-rate-lookup.mediawiki</a><br>
    <br>
    <div class="moz-signature">David Barnes<br>
      <br>
    </div>
    <div class="moz-cite-prefix">On 7/17/2015 7:24 PM, Matt Whitlock
      wrote:<br>
    </div>
    <blockquote cite="mid:15824042.UJUZ1yd9UW@crushinator" type="cite">
      <pre wrap="">You should rename your file to something like "bip-draft-dynamic-rate-lookup". Using an arbitrary BIP number will cause confusion when that BIP number is actually assigned later.

</pre>
    </blockquote>
  </body>
</html>

-------------------------------------
On 21/02/15 14:20, 木ノ下じょな wrote:

Your proposal is missing Abstract and Motivation sections. Abstract
tells us WHAT are trying to achieve, Motivation tells WHY. It's not
worth to dig into technical details of your implementation until these
two questions are answered.

-- 
Best Regards / S pozdravom,

Pavol Rusnak <stick@gk2.sk>


-------------------------------------
First-seen-safe Replace-by-Fee is now available as a patch against
v0.10.2:

    https://github.com/petertodd/bitcoin/tree/first-seen-safe-rbf-v0.10.2

I've also had a pull-req against git HEAD open for a few weeks now:

    https://github.com/bitcoin/bitcoin/pull/6176#issuecomment-104877829

I've got some hashing power interested in running this patch in the near
future, so I'm offering a bounty of up to 1 BTC to anyone who can find a
way to attack miners running this patch. Specifically, I'm concerned
about things that would lead to significant losses for those miners. A
total crash would be considered very serious - 1 BTC - while excess
bandwidth usage would be considered minor - more like 0.1 BTC. (remember
that this would have to be bandwidth significantly in excess of existing
attacks)

For reference, here's an example of a crash exploit found by Suhas
Daftuar: https://github.com/bitcoin/bitcoin/pull/6176#issuecomment-104877829

If two people report the same or overlapping issues, first person will
get priority. Adding a new test that demos your exploit to the unit
tests will be looked upon favorably. That said, in general I'm not going
to make any hard promises with regards to payouts and will be using my
best judgement. I've got a bit over 2BTC budgetted for this, which is
coming out of my own pockets - I'm not rich! All applicants are however
welcome to troll me on reddit if you think I'm being unfair.


Suhas: speaking of, feel free to email me a Bitcoin address! :)

-- 
'peter'[:-1]@petertodd.org
000000000000000006dd456cf5ff8bbb56cf88e9314711d55b75c8d23cccddd5
-------------------------------------
The nice thing about 1 MB is that you can store ALL bitcoin transactions
relevant to your lifetime (~100 years) on one 5 TB hard drive
(1*6*24*365*100=5256000). Any regular person can run a full node and store
this 5 TB hard drive easily at their home. With 10 MB blocks you need a 50
TB drive just for your bitcoin transactions! This is not doable for most
regular people due to space and monetary constraints. Being able to review
all transactions relevant to your lifetime is one of the key important
properties of Bitcoin. How else can people audit the financial transactions
of companies and governments that are using the Bitcoin blockchain? How
else can we achieve this level of transparency that is essential to keeping
corrupt governments/companies in check? How else can we keep track of our
own personal transactions without relying on others to keep track of them
for us? As time passes, storage technology may increase, but so may human
life expectancy. So yes, in this sense, 1 MB just may be the magic number.

Assuming that we have a perfectly functional off-chain transaction system,
what do we actually gain by going from 1 MB to 1000 MB (my approximate
limit for regular users having enough processing power)? If there is no
clear and substantial gain, then it is foolish to venture into this
territory, i.e. KEEP IT AT 1 MB! For example Angel said he wants to see
computers transacting with computers at super speeds. Why do you need to do
this on the main chain? You will lose all the transparency of the current
system, an essential feature.


On Fri, May 8, 2015 at 10:36 PM, Angel Leon <gubatron@gmail.com> wrote:



-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
-------------------------------------
I have been toying with an idea and figured I'd run it by everyone here 
before investing further time in it.  The goal here is to make it 
sustainable, and perhaps profitable, to run full nodes on the Bitcoin 
Network in the long term.

- Nodes can participate in a market wherein they are paid by nodes, wallets, 
and other services to supply Bitcoin Network data.  Payment should be based 
on the cost imposed on the Node to do the work and send the data, but can be 
set in any way the node operator desires.  It's a free market.
- Nodes that are mostly leeching data from the Bitcoin Network, such as 
those that do not receive inbound connections to port 8333, will send 
payments to the nodes they connect to, but will likely receive no payments 
from other nodes, wallets, and other services.
- Nodes that are providing balanced full service to the Bitcoin Network will 
tend to have a balance of payments coming in and going out with regards to 
other balanced full service nodes, leaving them revenue neutral there.  But 
they will receive payments from leech nodes, wallets, and other services.

The net effect here is that the cost to run nodes will be shared by those 
who are using the Bitcoin network but not contributing by running a full 
node.  A market will develop for fees to connect to the Bitcoin Network 
which should help cover the cost of running the Network.  It's still 
possible to continue offering access to your node for free as there is 
nothing forcing you to charge a fee.  But this isn't very sustainable 
long-run.  Market efficiencies should eventually mean nodes take in only 
what is required to keep the Network operational.

Raystonn



-------------------------------------
I'm rather perplexed about this proposal. What exactly is wrong with
the existing BIPs process? I mean, it seems to me anyone can publish a
BIP pretty easily in the BIPs repository. There doesnt seems to be any
real barrier to entry whatsoever. I know there have been all manner of
aspersions, but having just written two BIPs there was no friction at
all.

Whether the ecosystem adopts a BIP is another question of course, but
that's out of scope of the BIPs project anyhow. Take BIP101
controversial as it gets, but it's there. Whether Bitcoin implementers
implement it is another kettle of fish and a matter for each project
to decide. It's absolutely NOT the realm of the BIPs project itself.
Bitcoin Core does not make any consensus critical changes with a BIP.
Where one seeks to establish certain standards, say for privacy, a BIP
would be appropriate so the ecosystem can harmonise methodology across
the board.

The status of a BIP is not really determined by anyone, it's by
adoption - that's where consensus happens. There's a little legroom
around this but I'm not entirely sure what you are trying to solve.
Yes the process is loose, but is it broken? There have been a flood of
BIPs added recently with zero bureaucracy or friction.

BIP0001 is the BIP that defines the BIP process. Interestingly enough
the only BIP that might be controversial is in fact a BIP to change
the way BIPs are handled!

So I'd really prefer to start this conversation with a breakdown of
what you think is broken first before tackling what may or may not
need fixing. I would be very cautious bringing "administrative"
burdens to the process or evicting common sense from the proceedings.
Much of the debates around consensus building seem to negate the
importance of common sense and the simple fact that "it's obvious when
you see it".

I'm sure there can be improvements, but for me personally, I need to
see what is broken before I can make any judgement on a potential way
forward, and if it's not broken, we should leave it alone.


On Fri, Sep 4, 2015 at 5:40 AM, Andy Chase via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
So let's play this out a little.. Let's call it "Solomon's spend[1]"

Exchange gets hacked, bitcoins move.

The exchange has a contract with an insurance company and miners for 
'scorched earth' theft response that creates a double-spend of the 
original transaction.

So now there's a 10,000 bitcoin incentive for miners to roll back the
chain and start (re)mining the block where the theft occurred.

The exchange gets an insurance payout, some miner wins the lottery, and
the thief gets nothing. Seems like a good deal, what am I missing?

[1] http://en.wikipedia.org/wiki/Judgment_of_Solomon

On Sun, Feb 22, 2015 at 04:06:13AM -0800, Eric Lombrozo wrote:




-- 
----------------------------------------------------------------------------
Troy Benjegerdes                 'da hozer'                  hozer@hozed.org
7 elements      earth::water::air::fire::mind::spirit::soul        grid.coop

      Never pick a fight with someone who buys ink by the barrel,
         nor try buy a hacker who makes money by the megahash



-------------------------------------
His account on that website was also compromised.

2015-08-17 21:02 GMT+02:00 Anon Moto via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
Surely you have some sort of empirical measurement demonstrating the
validity of that statement? That is to say you've established some
technical criteria by which to determine how much centralization pressure
is too much, and shown that Pieter's proposal undercuts expected progress
in that area?

On Fri, Aug 7, 2015 at 12:07 PM, Ryan Butler <rryananizer@gmail.com> wrote:

-------------------------------------
I think we need a second mailing list: bitcoin-process for people to
learn about bitcoin process.

And someone to write a FAQ on it's sign up page so people interested
could at least discuss from a starting point of understanding how and
why it works the way it does!

Adam

-------------------------------------

"Scorched earth" makes no sense by itself. However, it can be a part of a
bigger picture. Imagine an insurance service which will make sure that
merchants are compensated for every scorched-earth or double-spend
transaction, as long they pay 0.1% premium from their revenue.

Merchants won't really care how it works as long as it does. All they know
is that they need to use a particular open-source wallet, and they will
receive a payment no matter what.
You won't need a TTP to process each payment.
-------------------------------------
On 6/26/2015 7:09 AM, Pieter Wuille wrote:

"Reasonably achievable" is a guideline that would keep bitcoin out of
trouble caused by either too little, or too much, declared capacity. 
This matches Gavin's thinking, though you may differ on the numbers.



Unless it is reasonably achievable.  Leave the rest to the free market.




-------------------------------------
What is required to spend bitcoin is that input be provided to the UTXO
script that causes it to return true.  What Chris is proposing breaks the
programmatic nature of the requirement, replacing it with a requirement
that the secret be known.  Granted, the secret is the only requirement in
most cases, but there is no built-in assumption that the script always
requires only that secret.

This idea could be applied by having the wildcard signature apply to all
UTXOs that are of a standard form and paid to a particular address, and be
a signature of some kind of message to that effect.  I imagine the cost of
re-scanning the UTXO set to find them all would justify a special extra
mining fee for any transaction that used this opcode.

Please be blunt about any of my own misunderstandings that this email makes
clear.

On Tue, Nov 24, 2015 at 1:51 PM, Bryan Bishop via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
On 9/2/2015 9:05 PM, Jeff Garzik via bitcoin-dev wrote:

Another market dependency is even more direct.

Blocksize that can be bought with either difficulty or bitcoin has 
incentives whose strength (though not direction) is subject to the 
exchange rate.  Hence those incentives are subject to the whims of fiat 
holders, who can push the exchange rate around.


-------------------------------------
Hi there,

traditionally, the Bitcoin client strives to hide which output
addresses are change addresses going back to the payer. However,
especially with today's dynamically calculated miner fees, this
may often be ineffective:

A user sending a payment using the Bitcoin client will usually enter
the payment amount only up to the number of digits which are
considered to be significant enough. So, the least significant digits
will often be zero for the payment. With dynamically calculated miner
fees, this will often not be the case for the change amount, making it
easy for an observer to classify the output addresses.

A possible approach to handle this issue would be to add a randomized
offset amount to the payment amount. This offset amount can be small
in comparison to the payment amount.

Any thoughts?

Best regards,

Isidor


-------------------------------------
On 02/10/2015 02:41 AM, Natanael wrote:

Hi Natanael,

BIP70 exists for seller non-repudiation (i.e. a cryptographically signed
receipt for payment) and establishing strong seller identity in a
face-to-face or other non-web scenario (since TLS doesn't help).
Anything else is incidental.


There's quite a bit that can be done with wallets and web sites, but
personally I'd freak out if my wallet prompted me because I visited a
web site.

e

-------------------------------------
This is off-topic here.
Consensus critical changes (those aren't changes that Bitcoin Core
developers can make unilaterally against the will of alternative
implementations or users) and Bitcoin Core development are independent
from bitcointalk.org and /r/bitcoin.
And as you say, people can create competing without moderation or
"censorship" (ie /r/bitcoin_uncensored or bitcontalkuncensred.org
/r/bitcoin_moderated_by_someone_else ).


On Sat, Aug 22, 2015 at 3:39 PM, David Vorick via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
size", I guess the community should say "then we will just ignore your

Oh good! We can just kick anyone out of the consensus process if we think
they make no sense.

I guess that means me and Gavin can remove everyone else from the developer
consensus, because we think trying to stop Bitcoin growing makes no sense.

Do you see the problem with this whole notion? It cannot possibly work.
Whenever you try and make the idea of developer consensus work, what you
end up with is "I believe in consensus as long as it goes my way". Which is
worthless.



Because he is formally the maintainer.

Maybe you dislike that idea. It's so .... centralised. So let's say Gavin
commits his patch, because his authority is equal to all other committers.
Someone else rolls it back. Gavin sets up a cron job to keep committing the
patch. Game over.

You cannot have committers fighting over what goes in and what doesn't.
That's madness. There must be a single decision maker for any given
codebase.



No. I'll write an article like the others, it's better than email for more
complicated discourse.

As others have said, if the answer is "forever, adoption is always the most

This appears to be another one of those fundamental areas of disagreement.
I believe there is no chance of Bitcoin ending up like Visa, even if it is
wildly successful. I did the calculations years ago that show that won't
happen:

    https://en.bitcoin.it/wiki/Scalability

Decentralisation is a spectrum and Bitcoin will move around on that
spectrum over time. But claiming we have to pick between 1mb blocks and
"Bitcoin = VISA" is silly.



Peter:   your hypocrisy really is bottomless, isn't it? You constantly
claim to be a Righteous Defender of Privacy, but don't even hesitate before
publishing hacked private emails when it suits you.

Satoshi's hacker had no illusions about your horrible personality, which is
why he forwarded that email to you specifically. He knew you'd use it. You
should reflect on that fact. It says nothing good about you at all.
-------------------------------------


On 4/16/2015 8:34 PM, Mark Friedenbach wrote:

Anyone can alter the txid - more details needed. The number of altered
txids in practice is not so high in order to make us believe anyone can
do it easily. It is obvious that all current bitcoin transactions are
malleable, but not by anyone and not that easy. At least I like to think so.

(tx1) and broadcast it, you can alter its txid at your will, without any
mining power and/or access to my private keys so I would end up not
recognizing my own transaction and probably my change too (if my systems
rely hardly on txid)?



-------------------------------------







Researchers should also keep in mind that some of developers are 
immature and have limited knowledge or experience beyond their Bitcoin 
expertise ("Idiot-savants").  Others want to be in "charge" of 
drama-laced posts on reddit and they get upset if others do the same 
things.

In any case these rants and attacks by Todd and Garzik should be posted 
on their personal blogs or reddit instead of this list.

Russ








-------------------------------------
Thank you Eric for saying what needs to be said.

Starting a fork war is just not constructive and there are multiple
proposals being evaluated here.

I think that one thing that is not being so much focussed on is
Bitcoin-XT is both a hard-fork and a soft-fork.  It's a hard-fork on
Bitcoin full-nodes, but it is also a soft-fork attack on Bitcoin core
SPV nodes that did not opt-in.  It exposes those SPV nodes to loss in
the likely event that Bitcoin-XT results in a network-split.

The recent proposal here to run noXT (patch to falsely claim to mine
on XT while actually rejecting it's blocks) could add enough
uncertainty about the activation that Bitcoin-XT would probably have
to be aborted.

Adam

On 17 August 2015 at 15:03, Eric Lombrozo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------


On 05/07/15 19:34, Mike Hearn wrote:

No, I'm very concerned about both.


I agree, thats why this mailing list was created in the first place
(well, also because bitcointalk is too full of spam, but close enought :))


Its true, just like its true the general public can opt to run any
version of software they want. That said, the greater software
development community has to update /all/ the software across the entire
ecosystem, and thus provide what amounts to a strong recommendation of
which course to take. Additionally, though there are issues (eg if there
was a push to remove the total coin limit) which are purely political,
and thus which should be up to the greater public to decide, the
blocksize increase is not that. It is intricately tied to Bitcoin's
delicate incentive structure, which many of the development community
are far more farmiliar with than the general Bitcoin public. If there
were a listserv that was comprised primarily of people on
#bitcoin-wizards, I might have suggested a discussion there, first, but
there isnt (as far as I know?).


Yes....I'm gonna split the topic since this is already far off course
for that :).


Let me answer that in a new thread :).


-------------------------------------
On Tue, Jun 30, 2015 at 03:12:52PM +0200, Adam Back wrote:

Well, as you know I have good reason to believe those contracts are
being actively worked on right now. I've been talking about this issue
for something like two years now, and rather than seeing a shift away
from use of zeroconf, we're seeing new services adopting it, always
large, centralized, startups in the payment space. Meanwhile the story
for decentralized wallets is if anything even worse, and most such
wallets don't even have code to detect double-spends at all.

From the point of view of large companies like Coinbase, getting hashing
power contracts and sybil attacking the network is relatively easy. Why
would they invest in genuine improvements when they can take the easy
way out? Especially when the easy way is something their smaller
competitors simply have no access too? Working on those contracts now
only makes sense, especially as the reliability of the P2P network in
providing zeroconf guarantees continues to decline as transaction volume
increases, and uniformity of nodes decreases.

By acting sooner rather than later in adopting full-RBF I think we have
a shot at changing the direction of the industry; if we wait I think we
stand a real chance of that dangerous infrastructure being put into
place. Equally, when you ask who is benefiting from the status quo, it
isn't decentralized wallets, but a small number of centralized startups
who have an advantage that the former can't match.


You know, if the status quo didn't have the downsides I mention above,
I'd probably agree with you on that point. But the risks outweigh it
IMO.


Note how relaying proof of double-spent status is only useful if you can
do something about it; the only method available without a scripting
language soft-fork is the scorched earth concept, which ironically
relies on full-RBF.


I'd suggest using nSequence for that purpose by defining non-maxint
nSequence as allowing RBF. (as well as non-maxint - 1 for nLockTime
usage to discourage fee sniping) Mark Friedenbach's sequence number BIP
is going to make use of transaction replacement anyway after all, so
doing that would be forward-compatible with it.

-- 
'peter'[:-1]@petertodd.org
0000000000000000129dec64f63611dc737b87331bb165740fb5552a92833a12
-------------------------------------
"My opinion is the most sustainable solution would be to identify a
team of admins and use something like Digital Ocean's new team accounts
feature and have someone like SolidX contribute funds for the servers
and a few hours a week from one of their sysadmins to the team."

This is a perfectly fine option. Alternatively, if the paid mailing list
option is preferred, I'd suggest Intermedia:
-------------------------------------
Hello,

I am seeking some expert feedback on an idea for scaling Bitcoin. As a
brief introduction: I work in the payment industry and I have twenty years'
experience in development. I have some experience with process groups and
ordering protocols too. I think I understand Satoshi's paper but I admit I
have not read the source code.

The idea is to run more than one simultaneous chain, each chain defeating
double spending on only part of the coin. The coin would be partitioned by
radix (or modulus, not sure what to call it.) For example in order to
multiply throughput by a factor of ten you could run ten parallel chains,
one would work on coin that ends in "0", one on coin that ends in "1", and
so on up to "9".

The number of chains could increase automatically over time based on the
moving average of transaction volume.

Blocks would have to contain the number of the partition they belong to,
and miners would have to round-robin through partitions so that an attacker
would not have an unfair advantage working on just one partition.

I don't think there is much impact to miners, but clients would have to
send more than one message in order to spend money. Client messages will
need to enumerate coin using some sort of compression, to save space. This
seems okay to me since often in computing client software does have to
break things up in equal parts (e.g. memory pages, file system blocks,) and
the client software could hide the details.

Best wishes for continued success to the project.

Regards,
Akiva

P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
-------------------------------------
On 2015-03-12 04:51 AM, Neill Miller wrote:

A good way to go about this from a UX point of view is warn the user
that their "phrase is non-standard", but allow them to insist.

 bytes thus allowing for complete and universal seed derivation without any reliance on word list. The word list is merely to generate a mnemonic, after that it has no role in seed generation so you can change it at anytime and it will never effect future mnemonics.

-- 
devrandom / Miron


-------------------------------------
Hi All,
I am currently running some tests on the peering system in Bitcoind for a
research paper. We hope to develop improvements which we can share with the
community. A wide diversity of real peers.dat files would be very helpful.
If you are willing, please email me your peers.dat.
Thanks,
-------------------------------------
On 7 August 2015 at 09:52, Wes Green via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:​


The equilibrium for that game is just keeping the same block size, isn't
it? Assume there's a huge backlog of fee-paying transactions, such that you
can trivially fill 1MB, and easily fill 1.25MB for the forseeable future;
fee per MB is roughly stable at "f". Then at any point in time, a miner has
the choice between receiving 25 + f btc, or increasing the blocksize to 1+p
MB and earning (25+f+pf) * (1-p) = f-ppf = 25(1-p) + f(1-pp) < 25+f. Even
if you think bigger blocks are good long term, wouldn't you reason that
other people will think so too, so why pay the price for it yourself,
instead of waiting for someone else to pay it and just reaping the benefit?


An idea I've been pondering is having the block size adjustable in
proportion to fees being paid. Something like "a block is invalid if
a(B-c)*B > F" where B is the block's size, F is the total fees for the
block, and a and c are scaling parameters -- either hardcoded in bitcoin,
or dynamically adjusted by miner votes. ATM, bitcoin's behavour is
effectively the same as setting c=1MB, a>=21M BTC/byte.

Having a more reasonable value for a would make it much easier to produce a
fee market for bitcoin transactions -- if the blocksize is currently around
some specific "B", then the average cost per byte of a transaction is just
"a(B-c)". If you pay more than that, then a miner will include your txn
sooner, increasing the blocksize beyond average if necessary; if you pay
less, you may have to wait for a lull in transactions so that the blocksize
ends up being smaller than average and miners can afford to include your
transaction (or someone might send an unnecessarily high fee paying txn
through, and yours might get swept along with it).

To provide some real numbers, you need to make some assumptions on fee
levels. If you're happy with:

 - 1 MB blocks are fine, even if no one's paying any fees
 - if people are paying 0.1 mBTC / kB (=0.1 BTC/MB) in fees on average then
8MB is okay

then a(1-c)=0, so c=1MB, and a(8-1)=0.1, so a=0.0143 and the scaling works
out like:

 - 1MB blocks: free transactions, no fees
 - 2MB blocks: 0.0143 mBTC/kB, 0.02 btc in fees/block
 - 4MB blocks: 0.043 mBTC/kB, 0.17 btc in fees/block
 - 8MB blocks: 0.1 mBTC/kB, 0.8 btc in fees/block
 - 20MB blocks: 0.27 mBTC/kB, 5.4 btc in fees/block
 - 40MB blocks: 0.56 mBTC/kB, 22 btc in fees/block

In the short term, miners can just maximise fees for a block -- ie, add the
highest fee/byte txns in order until adding the next one would invalidate
the block.

Over the long term, you'd still want to be able to adjust a and c -- as the
price of bitcoin in USD/real terms goes up, a should decrease
proportionally; as hardware/software improve, a should decrease and/or c
should increase.  Essentially miners would want to choose a,c such that the
market for block space clears at a price of some $x/byte, where $x is
determined by their costs -- ie, hardware/software constraints. If they set
a too high, or c too low, then they'll be unable to accept some
transactions offering $x/byte, and thus lose out. If they set a too low or
c too high, they'll be mining bigger blocks for less reward, and lose out
that way too. At the moment, I think it's a bit of both problems -- c is
too low (meaning some transactions get dropped), but a is too high (meaning
fees are too low to pay for the effort of bigger blocks).

Note that, as described, miners could try cheating this plan by making a
high fee transaction to themselves but not publishing it -- they'll collect
the fee anyway, and now they can mine arbitrarily large blocks. You could
mitigate this by having a(B-c) set the /minimum/ fee/byte of every
transaction in a block, or alternatively by enforcing each miner pay a
significant %ge of collected fees to the miner of the next block(s).

​Cheers,
aj​

-- 
Anthony Towns <aj@erisian.com.au>
-------------------------------------
I am more fazed by PR 5288 and PR 5925 not getting merged in, than by this
thread. So, casting my ballot in favor of the block size increase. Clearly,
we're still rehearsing proper discourse, and that ain't gonna get fixed
here and now.

On Thu, May 7, 2015 at 9:29 PM, Matt Corallo <bitcoin-list@bluematt.me>
wrote:

-------------------------------------
On Friday, September 04, 2015 12:30:50 AM Andy Chase via bitcoin-dev wrote:

Sigh. There is *no governance at all*. Any such a BIP like this needs to 
document the natural forces involved in real-world acceptance, not try to lay 
down "rules" that people are expected to follow.

For hardforks, that means economic consensus. For softforks, miner majority. 
For basically anything else, real-world implementation and use (by any 
significant quantity of people).

Luke

-------------------------------------
Hello all,

I've seen ideas around hard fork proposals that involve a block version
vote (a la BIP34, BIP66, or my more recent versionbits BIP draft). I
believe this is a bad idea, independent of what the hard fork itself is.

Ultimately, the purpose of a hard fork is asking the whole community to
change their full nodes to new code. The purpose of the trigger mechanism
is to establish when that has happened.

Using a 95% threshold, implies the fork can happen when at least 5% of
miners have not upgraded, which implies some full nodes have not (as miners
are nodes), and in addition, means the old chain can keep growing too,
confusing old non-miner nodes as well.

Ideally, the fork should be scheduled when one is certain nodes will have
upgraded, and the risk for a fork will be gone. If everyone has upgraded,
no vote is necessary, and if nodes have not, it remains risky to fork them
off.

I understand that, in order to keep humans in the loop, you want an
observable trigger mechanism, and a hashrate vote is an easy way to do
this. But at least, use a minimum timestamp you believe to be reasonable
for upgrade, and a 100% threshold afterwards. Anything else guarantees that
your forking change happens *knowingly* before the risk is gone.

You may argue that miners would be asked to - and have it in their best
interest - to not actually make blocks that violate the changed rule before
they are reasonably sure that everyone has upgraded. That is possible, but
it does not gain you anything over just using a 100% threshold, as how
would they be reasonably sure everyone has upgraded, while blocks creater
by non-upgraded miners are still being created?

TL;DR: use a timestamp switchover for a hard fork, or add a block voting
threshold as a means to keep humans in the loop, but if you do, use 100% as
threshold.

-- 
Pieter
-------------------------------------
Hi everyone,

I'm a long-time lurker of this mailing list but it's the first time I post
here, so first of all I'd like to thank all of the usual contributors for
the great insights and technical discussions that can be found here. As
this is such a momentous point in the history of Bitcoin, I'd just like to
throw in my opinion too.

First, I agree with Oliver Egginger's message that it's much more elegant
to keep the numbers as powers of 2 rather than introducing somewhat
arbitrary numbers like 20. This also makes it easier to count the level of
support for what would be a clear spectrum of discrete levels (1, 2, 4, ...
32, 64, ..., infinite). If a temporary peace accord can be reached with a
value like 8 or 16, this will buy us some time for both the user base to
continue growing without hitting the limit and for newer technologies like
the lightning network to be developed and tested. We will also see whether
the relatively small increase causes any unexpected harm or whether (as I
expect) everything continues to run smoothly.

Personally, I'd like to see Bitcoin grow and become what I think most
Bitcoin users like myself expect from it: that it should be a payment
network directly accessible to people all over the world. In my opinion, it
is the proposition of Bitcoin as a form of electronic money that
additionally makes it a good store of value. I don't believe in the idea
that it can exist as just some sort of digital gold for a geeky financial
elite. And I haven't been persuaded by those who claim the scarcity of
block space is an economic fundamental of Bitcoin either. It seems to me
there's a lot of batty economic ideas being bandied about regarding the
supposed long-term value of the cap without much justification. In this
sense, my sympathies are with those who want to remove the maximum block
size cap. This was after all the original idea, so it's not fair for the
1MB camp to claim that they're the ones preserving the essences of Bitcoin.

But, anyway, I also think that a consensus at this point would be much
better than a head-on confrontation between two incompatible pieces of
software competing to gain the favour of a majority of exchanges and
merchants. With this in mind, can't we accept the consensus that raising
the hard-coded limit to a value like 8MB buys us a bit of time and should
be at least palatable to everyone? This may not be what the staunch
supporters of the 1MB limit want, but it's also not what I and others would
want, so we're talking about finding some common ground here, and not about
one side getting their way to the detriment or humiliation of the other.

The problem with a compromise based on a one-off maximum-size increase, of
course, is that we're just kicking the can down the road and the discussion
will continue. It's not a solution I like, but how can we get people like
say Greg Maxwell or Pieter Wuille to accept something more drastic? If they
find a new maximum-size cap acceptable, then it could be a reasonable
compromise. A new cap will let us test the situation and see how the
Bitcoin environment reacts. The next time the discussion crops up (probably
very soon, I know...), we may all have a better understanding of the
implications.

Ángel José Riesgo
-------------------------------------
Opened a repo containing the full text of BIP 100 discussion document, in
markdown format.

The BIP 100 formal spec will be checked in here as well, before submitting
to upstream bips.git repo.
-------------------------------------

I do want something better, but not for the focus you have.

Not because what you produce was not high quality, but because quality is achieved at a very
high cost and is hard to uphold over generations of developer. You focus on a single use case
while there are many out there for distributed ledgers.

I think in an infrastructure for enterprise applications, building consensus on the ledger is a
cornerstone there, but is only a piece of the solution. I built several commercially successful
deployments where I delegated the consensus building to a border router, a Bitcoin Core,
then interfaced that trusted peer with my  implementation that accepted Core’s decisions
in an SPV manner. One might think of this setup as wasteful and unsuitable for “small devices”
therefore an example of centralization people here try to avoid.

Enterprises have sufficient resources. Solving the business problem is valuable to them even at
magnitudes higher cost than a hobbyist would bear.

For mainstream adoption you need to get enterprises on board too, and  that is what I care of.
Enterprises want code that is not only high quality, but is easy to maintain with a development
team with high attrition. One has to take whatever help is offered for that, and one is modern
languages and runtimes.

Bits of Proof’s own implementation of the scripts was not practically relevant in my commercially
successful deployments, because of the use of a border router, but it helped development,
enabling easier debug and precise error feedback esp. end even after Core had a reject message.

I integrated libconsensus only for the hope that is significantly fastens application side tx verification,
 which it has turned out it does not, until secp265k1 is integrated.

I would likely use an other extended libconsensus too, but do not think there was a dependency on
that for enterprise development.

It would help there more to have a slim protocol server, no wallet, no rpc, no qt but a high
performance remoting API.



Storage and validation is non-trivially interconnected, but I now the separation can be done,
since I did it.

Excuse me, but function pointers is a pattern I used in the 80’s. I know that they are behind
the curtain of modern abstractions with similar use, I still prefer not to see them again.

Tamas Blummer

-------------------------------------

Pieter, what's actually happening is that the bitcoin-core release has
become a Schelling point in the consensus game:

https://en.wikipedia.org/wiki/Schelling_point

Due to the strong incentives for consensus, everyone is looking for an
obvious reference point that they think everyone else will also pick, even
though the point itself isn't critical, only that everyone agree on
whatever point is picked. Like it or not, the bitcoin-core release, and by
extension it's committers have a great degree of influence over what the
community as a whole decides to do. If core screws things up badly enough,
yes, the community will settle on some other focal point for consensus, but
the cost and risk of doing so is high, so there is indeed unavoidable moral
hazard for whoever has control over any such focus point.

Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com>

On Wed, Dec 16, 2015 at 10:34 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
It might be as well to keep the archive but disable new posts as
otherwise we create bit-rot for people who linked to posts on
sourceforge.

The list is also archived on mail-archive though.
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/

Adam

On 14 June 2015 at 22:55, Andy Schroder <info@andyschroder.com> wrote:


-------------------------------------
Right you are!
I saw Thomas's email about Electrum 2.0 not supporting BIP39.
It seems he had the idea that the wordlist was a strict requirement yet it is not, it is unfortunate that Electrum did not go the route of BIP39. The wordlist is irrelevant and merely used to help build mnemonics.
Also as I've shown, you can work a version into it, I was going to actually propose it to the BIP39 authors but didn't think it was an issue.
I think BIP39 is fantastic.
I think Electrum 2.0 (And everyone) should use BIP39  On 2015-03-11 06:21 PM, Thy Shizzle wrote:

Unfortunately there's more incompatibility than just the date issue:

* seed: some follow BIP39, and some roll their own
* HD structure: some follow BIP44, some BIP32 derivation, and some roll
their own

So actually very few wallets are seed-compatible, even ignoring the date
question.


That's a reasonable solution.


-- 
devrandom / Miron
-------------------------------------
On 8/20/2015 3:23 AM, Milly Bitcoin via bitcoin-dev wrote:


Pieter built a nice simulation tool and posted some results.

I tweaked the parameters and ran the tool in a way that tested ONLY for 
hashrate centralization effects, and did not conflate these with network 
partitioning effects.

I found that small miners were not at all disadvantaged by large blocks.

The only person who commented on this result agreed with me.  He also 
complimented Pieter's insight (which is entirely appropriate since 
Pieter did the hard work of creating the tool).

http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/008820.html



-------------------------------------


Select your preferable compression library and google for it with +CVE.

E.g. in zlib:

http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html

…allows remote attackers to cause a denial of service (crash) via a crafted compressed stream…
…allows remote attackers to cause a denial of service (application crash)…
etc.

Do you want to expose such lib to the potential attacker?
--  
Pavel Janík





-------------------------------------
On 2015-06-16 07:55, Aaron Voisine wrote:

How will the SPV wallet users pay for this service? With their money, or 
with their privacy?


-------------------------------------
Hello,

In response to public and private comments and feedback, we have updated
this working draft.

https://drive.google.com/file/d/0BwEbhrQ4ELzBOUVtOHJQdlhvUmc/view?usp=sharing

Update highlights:

1. Specific clarifications on replacing the Coinbase subsidy and
supplementing *and* not replacing transaction fees.

2. Clarification on block chain overhead. The value of data mining is on a
bell curve, so year six data will be removed every year.

3. Added references to an ability to create global, national and regional
Bitcoin Price Indices for popular baskets of goods transacted with Bitcoin.

4. Added references for an ability to use structured block chain data for
Bitcoin capacity and fork planning.

5. Removed references to price speculation.

6. Added preferences for deployment dates of January 2017 or January 2018.

7. Moving towards BIP format after discussion and evaluation period.
Technical content will increase in due course and discussion content will
be removed.

Further views and feedback welcome.

Regards,

Ahmed


On Mon, Aug 17, 2015 at 5:23 PM, Ahmed Zsales <ahmedzsales18@gmail.com>
wrote:

-------------------------------------
Other than the source code, the best documentation I've come across is a few
lines on IRC explaining the high-level design of the protocol:
https://botbot.me/freenode/bitcoin-wizards/2015-07-10/?msg=44146764&page=2

On Thu, Aug 6, 2015 at 10:18 AM Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

On 02/05/2015 04:04 PM, MⒶrtin HⒶboⓋštiak wrote:

I was analyzing the model as you described it to me. A formal analysis
of the security model of a particular implementation, based on inference
from source code, is a bit beyond what I signed up for. But I'm
perfectly willing to comment on your description of the model if you are
willing to indulge me.



How do they compare words if they haven't yet established a secure channel?


So the assumption is that there exists a secure (as in proximity-based)
communication channel?

e


-------------------------------------
Spammers out there are being very disrepectful of my fullnode resources 
these days!  I'm making some changes. In case others are interested, 
here's a description:

There is now a maximum size for the memory pool.  Space is allocated 
with a pretty simple rule.  For each tx, I calculate MY COST of 
continuing to hold it in the mempool.  I measure the cost to me by 
"expected byte stay":

expectedByteStay = sizeBytes * expectedBlocksToConfirm(feeRate)


Rule 1: When there's not enough space for a new tx, I try to make space 
by evicting txes with expectedByteStay higher than tx.

I'm NOT worrying about
  - Fees
    EXCEPT via their effect on confirmation time

  - Coin age
    You already made money on your old coins.  Pay up.

  - CPFP
    Child's expectedBlocksToConfirm is max'ed with its
    parent, then parent expectedByteStay is ADDED to child's

  - Replacement
    You'll get another chance in 2 hours (see below).


Rule 2: A transaction and its dependents are evicted on its 2-hour 
anniversary, whether space is required or not


The latest expectedBlocksToConfirm(feeRate) table is applied to the 
entire mempool periodically.

What do you think?  I'll let you know how it works out.  I'm putting a 
lot of faith in the new fee estimation (particularly its size 
independence).  Another possibility is clog-ups by transactions that 
look like they'll confirm next block, but don't because of factors other 
than fees (other people's blacklists?)


-------------------------------------
As for "the ecosystem waiting around for laggards", yes, it is absolutely the ecosystems y responsibility to not take actions that will result in people losing money without providing them far more than enough opportunity to fix it. One of the absolute most important features of Bitcoin is that, if you're running a full node, you are provided reasonable security against accepting invalid transactions.

On December 16, 2015 1:51:47 PM PST, Jameson Lopp <jameson.lopp@gmail.com> wrote:
-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

In any case this is basically the purpose of version tracking software
such as Git or CVS or any other. It would not be hard to figure out who
had done what. I see you're splitting hairs over nothing as usual,
though, Russ, so I'll leave you to it.

phm

Milly Bitcoin via bitcoin-dev wrote:
individual works as part of a compilation as opposed to claiming a
copyright on the compilation itself (which is what the current notice
is). > > Russ > > > On 10/6/2015 1:08 AM, Milly Bitcoin wrote: >>> The
copyright notice refers to the fact that each contributor owns >>>
copyright >>> to his own contributions. There is no legal group that
owns copyright >>> to the >>> entirety of the code. >>> >> >> No, that
is not what such a notice means.  The part after the "c" in the >>
circle is the legal owner.  If the legal owners are not properly >>
identified then the notice is not valid. >> >> --- >>  From Nolo: >> >>
What is a valid copyright notice? >> >> A copyright notice should
contain: >> •the word "copyright" >> •a "c" in a circle (©) >> •the date
of publication, and >> •the name of either the author or the owner of
all the copyright rights >> in the published work. >> >> For example,
the correct copyright for the fourth edition of The >> Copyright
Handbook, by Stephen Fishman (Nolo), is Copyright © 1998 by >> Stephen
Fishman. >> >> --- >> from USPTO: >> >> Use of the notice informs the
public that a work is protected by >> copyright, identifies the
copyright owner, and shows the year of first >> publication. >> --- >>
bitcoin-dev mailing list > bitcoin-dev@lists.linuxfoundation.org >
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJWFEewAAoJEIUV926tz9E87a4P/21Znk1tx/ZCA4eykig75S9I
d0xyLRREVL/SBOBzE8kJh1YmiHAG1ziHzsgpobNn/N2VuKanCKSXuR3niV5WRtYw
+Oa4uVZUNtAtXKSrFSiGpwJoZN5JnUADcx4sK7En3Z5gFEYSAPrMwvm+M0upl9rd
5b2/VQ/dm3fDUnntnz4DfhV3otEbcLo6imaaV6RDIPru61Fc20blHJhQPEX0laex
N1rUiUvsyUL9H66fGFYmm/6YJcO26k3gPNmmODJdApv7uTVfHBj3c2r4xKSIDVES
WxdL+DdyzJQU6Ng95793QTx29Wn8pV1FlMkC9TQ3biQ1ivoAAKbvxzI27swbPx7d
WGu/ATDj3UN2RBY3hzTTpMIVK5kITVo+QGtA8cg+KcLjVPaasYdb13zy/pE6PO8J
4AWd/nYP/bQlkrebeFylY7vQi6TNCDtpfkJE2r8H+3RovqigN+pLLVhuEVlOBtM1
7N5gAvJWqKtUIgzNKte+eS/yaOFBhHp+veC+QfNMDechC4OGM7IDVdf9oVy9DSCX
68XThI62AT+uhKjvs8ZG3L88AUiiYK6RC4YCUZVoydbQHovmvQRL3Wb36n+krcGH
iV3n3lfk7+D9IrX6ieRwmHpa9a7VAIekqUuCSBdsXBCUM5zNz48bRNJSofjWLtSm
5p0mp5jQxte8loevZztf
=psJx
-----END PGP SIGNATURE-----


-------------------------------------
I'm supporting this proposal and since I'm already using the Encompass
wallet software I would like to highlight that this use case is not only
practical but has already a working reference implementation.

The only donwside I see is that it means we get yet another HD wallet
definition.

Is there anything else what would speak against assigning a BIP number
to this proposal? This would allow kefkius and his team to use the
standard in Encompass and share it with other software packages which
might be interested in using deterministic cross-currency wallets.

On 04/09/2015 10:16 PM, Kefkius wrote:



-------------------------------------
I am in a timezone that uses DST (currently PDT), but I would like us to
use a timezone that does NOT use DST.  It will be nice to have something
that reflects the seasonal patterns like my own body does.  I hate the time
change in both ways.

On Fri, Sep 18, 2015 at 2:50 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
The miners with invalid blocks were punished with a loss of bitcoin
income...


On Sat, Jul 11, 2015 at 4:05 AM, Nathan Wilcox <nathan@leastauthority.com>
wrote:

-------------------------------------
I guess the most basic question is how do you define a coin here?

Thanks,
Loi Luu
On 10 Dec 2015 2:26 a.m., "Akiva Lichtner" <akiva.lichtner@gmail.com> wrote:

-------------------------------------
On Wed, Mar 25, 2015 at 6:44 PM, Tom Harding <tomh@thinlink.com> wrote:

Not at all.


The sender is always able to intentionally hide their payment under a
rock-- There is no encoding that can prevent that.

The defense against that is to not accept payments not made according
to the payees specification.


To reject reused scriptPubKeys you must remember past scriptPubkeys in
order to test against them.

For illustration purposes imagine a bitcoin system where there is only
a single base unit available for trade.

Verification of that chain requires O(1) storage (the identity of the
current chain tip, and the identity of the spendable coin.).
Verification with duplicate elimination requires O(N) storage (with N
being the length of the history), since you need to track all the
duplicates to reject.

(The same is true for actual Bitcoin as well, though the constant
factors make the difference somewhat less stark.)


-------------------------------------
On Thu, May 7, 2015 at 1:26 PM, Matt Corallo <bitcoin-list@bluematt.me>
wrote:



I think the strongest thing I've ever said is:

"There is consensus that the max block size much change sooner or later.
There is not yet consensus on exactly how or when. I will be pushing to
change it this year."

This is what "I will be pushing to change it this year" looks like.

-- 
--
Gavin Andresen
-------------------------------------
On Sat, Jul 11, 2015 at 1:09 PM, Nathan Wilcox <nathan@leastauthority.com>
wrote:


The benefit drops off pretty quickly as the timeout increases (and
eventually goes negative).

You could look at it that headers having 4 states.

1) Valid
2) Probably Valid
3) Probably Invalid
4) Invalid

SPV mining puts newly received headers into the "probably valid" category.

It builds empty (coinbase only) blocks on top of probably valid headers and
build empty blocks on the header.

Once it receives the full block, it can change the state to Valid.  At that
point, it can build full blocks on top of the header.

As time passes without the full block being received/validated, it becomes
less and less likely that the block is actually valid.

The timeout is to recognize that fact.  Making the timeout 24 hours is not
likely to give the miner much benefit over making it 1-2 minutes.

Setting the timeout to low means that the miner sometimes switches away
from a header that turns out to be valid.

Setting it to high means that the miner ends up staying to long on a header
that turns out to be invalid.

At some point, the second effect is stronger than the first effect.  The
timeout needs to be high enough so switching away from valid headers is
rare but low enough so that it doesn't stay on invalid headers for ages.

If 99% of full blocks are received (and validated) within 30 seconds of the
header arriving, then it is reasonable for the miner to assume that if the
full block hasn't arrived within 60 seconds of the header arriving, then
the header is for an invalid block.

Yes. If it's rare enough, then skipping transaction validation saves more

SPV miners don't actually produce invalid blocks though (other than
building on invalid blocks).  The full blocks they produce are still fully
checked blocks.



SPV mining is mainly to protect against latency.  The reason that matters
is that latency means that hashers end up building on blocks even though a
new block has been found.

You can look at it as wasting hashing power due to latency.

In the world where minting fees are very low, there is no point in SPV
mining.

I assume at the point, the memory pool/queue is a few blocks deep.  This
means that the pool can create a full block without having to wait for new
transactions to be sent in.

It still needs to wait for the new full block before it knows which
transactions to remove from its memory pool.

Pools have to pay their hashers for hashing power.  When minting fees are
tiny, pools only get income only from tx fees.

There is no point in creating empty blocks, since they don't pay anything.

Between when the block is found and the pool has a new block ready to mine,
there is no incentive for the pool to give out new work.  The stratum
protocol could be modified so pools can say.  (It might already support
this)

<Send work to hashers>

-- block is found by some other pool --

<Cancel work for miners>

-- pool builds new block --

<Send new work to hashers>

The cancel command says that the pool will not accept any of the work that
has been made obsolete.

This gives a window of 20-30 seconds after each block where the pool has
invalidated the old work, but does not send new work.  Hashers' hardware
would be stalled.

On the other hand, the pool that found the block could create a new block
straight away.  There is an incentive for hashers to have a connection to
multiple pools.

Pools might go pure pay per share.  The protocol would say how much they
are willing to pay for a share and the local mining proxy would pick the
most profitable pool.  This eliminates pools having lots of ways of saying
how they charge fees, you just connect to lots of pools and go with the one
that pays the most.

More interestingly, the average fee per byte for transactions in the memory
pool is likely to increase as time passes since the last block.

When two blocks are found very fast one after another, the second block is
likely to have lower average fees.  This is because the first block would
have included most of the high value transactions and there wasn't enough
time for new ones to arrive before the second block was found.

Hashers would end up getting variable payouts based on how long since the
last block was received.  If some miners increase/decrease their output
based on the fees the pools offer, then as time passes since the last
block, the hashing rate would increase.  This reduces the variation in
block to block times.

For example, if there was 1.5MB of transactions in the memory pool and they
all paid the same fee per byte, then the block would be a full block at
that rate.  However, there would only be 0.5MB of transactions left.  This
means that the next block would be half full and only be able to pay out
50% of the fee, but the difficulty would be the same.  The pay per hash
would be 50% lower.  Once 0.5MB of new transactions arrive, the fee would
be back up to the same as the previous block.

If there are major SHA256 altcoins (or side chains), then miners might end
up switching between coins.  The longer a coin went without a new block
being found, the more tx fees available in the memory pool, so the more
hashing power would decide to switch to that coin.

There could be a situation where adding a few more transactions to the
memory pool of a coin would cause a 100X increasing in hashing until the
block was found and then it stalling again as the hashing power switches
away.  This is similar to alt coins getting blasted by coin switching pools
and then dropping to almost no power.
-------------------------------------
On Sun, Nov 1, 2015 at 6:46 PM, Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


I like those guidelines, although I'm sure there may be lots of arguing
over what fits under "protects the integrity of the network" or what
constitutes "reasonable notice" (publish a BIP at least 30 days before
rolling out a change? 60 days? a year?)

-- 
--
Gavin Andresen
-------------------------------------
It's worth noting that even massive companies with $30M USD of funding don't run a single Bitcoin Core node, which is somewhat against the general concept people present of companies having an incentive to run their own to protect their own wallet. 



-------------------------------------
Libconsensus will create an in-process alternative to the border router setup I currently advocate in a production environment.
It is not sufficient yet, since only checking scripts, but is the move I was long waiting for. 

I  launched a Lighthouse project to add Java Language Binding to lib consensus. Let's turn the debate to a constructive vote.

See on https://www.reddit.com/r/LighthouseProjects

Tamas Blummer

-------------------------------------
On Thu, Oct 29, 2015 at 6:57 AM, telemaco via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

The word "database" is likely confusing people here.  This is not a
database in an ordinary sense.

The bitcoin core consensus engine requires a highly optimized ultra
compact data structure to perform the lookups for coin existence. The
data stored is highly compressed and very specialized, it would not be
useful to other applications.  Right now, on boring laptop hardware,
during network synchronization updates to this database run at over
10,000 records per second, while the system is also busy doing the
other validation chores of a node. This is backended by a high
performance transactional key value store.  The need for performance
here is essential to even keeping up with the network, it's not about
enabling any kind of fancy querying (bitcoin core does not offer fancy
querying), it's about the base load that every node must handle to
usably sync up and keep up with the Bitcoin network.

The backend can be swapped out for something else that provides the
same properties, but doing so does not give you any of the
inspection/analytics that you're looking for.  Systems that do that
exist, and they require databases taking hundreds of gigabytes of
storage and take days to weeks to import the network data.  They're
great for what they're for, but they're not suitable for consensus use
in the system for space efficiency, performance, and consensus
consistency reasons.

-------------------------------------
Some changes:

Votes need to be 100%, not 50.01%. That way small miners have a fair
chance. A 50.01% vote means large miners call the shots.

Users (people who make transactions) need to vote. A vote by a miner
shouldn't count without user votes. Fee incentives should attract
legitimate votes from miners. A cheating miner will be defeated by another
miner who includes those votes, and take the fees.

This lets wallet providers and exchanges cast votes (few wallets will
implement prompts and will just auto vote, so if you don't agree, switch
wallets. Vote with your wallet).

~Vince
On Jun 3, 2015 12:34 PM, "Stephen Morse" <stephencalebmorse@gmail.com>
wrote:

-------------------------------------
Gavin has been very clear about the fact that he's on vacation. I'm not sure what you want Mike to say. It's obvious the Bitcoin Core developer pitchforks are out for him so there isn't really anything he can possibly say which will be constructively received on this highly adversarial and increasingly ridiculous charade of a mailing list. I feel as though they've made their case abundantly clear to anyone paying attention.

The community will weigh the independent merit of the two points of view and that community is not as naive and uninformed as everyone on this list likes to portray them to be. Your concern for companies' welfare is appreciated but I'm confident they can manage their own independent assessments of this matter as well as seek out enough varied expert opinions such that they can make an informed decision.

19.08.2015, 19:53, "Adam Back via bitcoin-dev" <bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
I just sent the following email to F2Pool:


I was disappointed to see Peter Todd claiming that you have (or will?) run
his replace-by-fee patch.

I strongly encourage you to wait until most wallet software supports
replace-by-fee before doing that, because until that happens replace-by-fee
just makes it easier to steal from bitcoin-accepting merchants.

I will tell you the same thing about 8MB blocks: until most merchants
support bigger blocks I will strongly encourage you keep creating
less-than-1MB blocks. If we want Bitcoin to succeed more quickly, we should
all be thinking about what is good for the whole system: users, merchants,
exchanges and miners.

As always, if you have questions or concerns feel free to email me.


-- 
--
Gavin Andresen
-------------------------------------
Use of the bitcoin symbol in text is inconvenient, because the bitcoin
symbol isn't in the Unicode standard. To fix this, I've written a proposal
to have the common B-with-vertical-bars bitcoin symbol added to Unicode.
I've successfully proposed a new character for Unicode before, so I'm
familiar with the process and think this has a good chance of succeeding.
The proposal is at http://righto.com/bitcoin-unicode.pdf

I received a suggestion to run this proposal by the bitcoin-dev group, so I
hope this email is appropriate here. Endorsement by Bitcoin developers will
help the Unicode Committee realize the importance of adding this symbol, so
please let me know if you support this proposal.

Thanks,
Ken
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I will jump in just because I feel like it because the questions are
fun and so on.  (Of course I am not Gregory)

On 07/29/2015 02:28 PM, Raystonn . via bitcoin-dev wrote:

Note that I am not Gregory, so with that caveat...


No, it will have multiple and diverse purposes into which it can be
used for and can evolve, it would not be sufficient to state that it
has "a future" merely as a high-value settlement network.


If you have a proposal on this, please submit it in the formal way as
a BIP draft.   Enough time has been burnt on the subject, imho.


It is the market.  What will happen will happen.  If bitcoin
development pushes fees upward as an overall trend and the overall
cost to transact continues to increase, billions of people around the
world will as a result be forced out from most use cases of bitcoin
and the "bleeding out" will occur naturally to alts (to the extent
that persons already possessed bitcoin first and need to transact).
As stated above, liquidity moves to location of least friction.
Bitcoin bagholders can whine all they want, but value will distribute
into the alts gradually.


"allow" is not a relevant term here, as it is not up to anyone what
people are going to do with their crypto of any kind.  Unless, of
course, you are fool enough to be using Coinbase and Bitpay or
something like that.  They own "your" coin, and they will decide, or
allow, what you do with it or whether you can even access it.
As has been stated before here, I hope you are not using such services.
On the other hand, the following are very interesting:
https://gear.mycelium.com/ - a Payment processor
http://openbazaar.org a decentralized Market
https://bitsquare.io/ a decentralized Exchange
https://electrum.org/ a light wallet that you manage

 then:

Irrelevant.  Better question is, How much should one give? The more
you can give, the better off you will be.


Too much attention is paid to the miners.  Miners should not be
butthurt when people say that we should not put them up on a pedestal.
 Think ahead, to when there will no longer be bitcoin mining such as
there is today.


People will continue to buy and sell.  Some major changes are in
store, however.  If you would like, see my reflections on what the
months ahead will hold, here:
http://www.twitlonger.com/show/n_1sn3lqs


- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVufH2AAoJEGxwq/inSG8C1mgH/3poEpk8pDDgZ7YQlGmAZjiO
MDBempLkfm1BFFoNAzjMn9mwtmL9wDfpn/sd/YbuIriJjQR2WSl6zy/sLx/uIYxd
qRuSRwOzN6wN7NfAuG7Lt3NtawOjAgl87n5YhRVB/d/MAK5HAvx3L9ME1Px//qsF
Czg5r0XG4ZiQnT8J30caMtooSVU9toradAmMleVbMVOi9KViyuW2IvXz5mM1jYHh
h+CB+CVHlhuKubXWpnnxYtOLLRQM5QSyfQiMPimVG0QPSOC5UkXJNo5gK6YMtBkT
0FevJyoMF+0LVTTPVGms+jolxu2PX3RW59nhNKEAuxOWfeHdMFFGtPP04XbpqSo=
=R3aj
-----END PGP SIGNATURE-----

-------------------------------------
On Tuesday 28. July 2015 19.40.21 Eric Lombrozo via bitcoin-dev wrote:

This skips over the question why you need a fees market. There really is no 
reason that for the next 10 to 20 years there is a need for a fees market to 
incentive miners to mine.  Planning that far ahead is doomed to failure.

-- 
Thomas Zander

-------------------------------------
Just to clarify, SPV is fundamentally busted as it currently exists. I’m talking about potential optimizations for future protocols.

- Eric Lombrozo


-------------------------------------
Forgive me if I have missed the exact use-case, but this seems overly
complex. Surely fill-or-kill refers to getting a transaction confirmed
within a few confirms or to drop the tx from the mempool so it wont be
considered for inclusion anymore. As such, you could just repurpose a small
range of nLocktime such that a TX will be accepted into mempool for a
specific period before expiring.

On Thu, Sep 17, 2015 at 7:41 PM, jl2012 via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On 2015-06-16 12:55 AM, Aaron Voisine wrote:

I would also guess that the cost to provide service to SPV wallets is
less than $0.01/mo per wallet and in any case less than long-term
transaction fees.  This can either be taken up by the wallet author or
transparently through a payment channel by the user.


-- 
devrandom / Miron


-------------------------------------
On Sun, Feb 22, 2015 at 08:36:01AM -0800, Tom Harding wrote:

No, OTOH if they don't then the situation is no difference from what we
have now, and replace-by-fee does no harm. Meanwhile, relaying of bare
double-spend signatures can be implemented in the future, as I suggested
last year for your/Andresen's double-spend relaying patch.

Did you notice the even more obvious way to defeat ANYONECANPAY scorched
earth with that patch?


So? RBF nodes will.


I suspect many won't, because few people need to rely on unconfirmed
transactions anyway.


If you're going to consider replacement, conflict processing will
definitely be more expensive. :)

An actual DoS attacker would do their DoS attack in a way where conflict
processing has nothing to do with it, so this change does no actual
harm.


What exact git commit were you looking at? I did have an early one that
did have a bug along those lines, now fixed.

The current version ensures every replacement pays at least as much
additional fees as would normally cost to broadcast that much data on
the network, and additionally requires the fees/KB to always increase;
under all circumstances it should be no more of a DoS threat than
low-fee transactions are otherwise. I'd like to know if there is a flaw
in that code however!

-- 
'peter'[:-1]@petertodd.org
000000000000000017c2f346f81e93956c538531682f5af3a95f9c94cb7a84e8
-------------------------------------
FYI, looks like someone was trying to boost up a transaction tonight
and managed to get it pushed through.

parent: 4cc3e2b6407ae8cdc1fd62cb3235f9c92654277684da8970db19a0169e44c68c
child: 161b302d1af8b6eacf1140726b26c67fa72ecf4f7f7e6cd8d83ef492b8b490ea
gchld: 4f6821b50c046ae40d488aa18d88d41c9d0686daedf835b68b8c5086b73939fd
ggch: 0c159d19f6452f12512a4ec16868a3af00fe381a0913f4fc69b3fc14c4588aa9
gggch: 4e4f96c5ba416961be347ffc496e8ce12046191ab7fb252e88966ce365d2bc5f

though it's position in the block doesn't seem to be a priority / fee cut line.

-------------------------------------
On Thu, Jul 30, 2015 at 11:24 AM, Bryan Bishop via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



This is a meme that keeps coming up that I think just isn't true.

What other decentralized systems can we look at as role models?

How decentralized are they?

And why did they succeed when "more efficient" centralized systems did not?


The Internet is the most successful decentralized system to date; what
lessons should we learn?

How decentralized is the technology of the Internet (put aside governance
and the issues of who-assigns-blocks-of-IPs-and-registers-domain-names)?
How many root DNS servers?  How many BGP routers along the backbone would
need to be compromised to disrupt traffic? Why don't we see more
disruptions, or why are people willing to tolerate the disruptions that DO
happen?

And how did the Internet out-compete more efficient centralized systems
from the big telecom companies?  (I remember some of the arguments that
unreliable, inefficient packet-switching would never replace dedicated
circuits that couldn't get congested and didn't have inefficient timeouts
and retransmissions)


What other successful or unsuccessful decentralized systems should we be
looking at?


I'm old-- I graduated from college in 1988, so I've worked in tech through
the entire rise of the Internet. The lessons I believe we should take away
is that a system doesn't have to be perfect to be successful, and we
shouldn't underestimate people's ability to innovate around what might seem
to be insurmountable problems, IF people are given the ability to innovate.

Yes, people will innovate within a 1MB (or 1MB-scaling-to-2MB by 2021) max
block size, and yes, smaller blocks have utility. But I think we'll get a
lot more innovation and utility without such small, artificial limits.

-- 
--
Gavin Andresen
-------------------------------------
I'm not sure if anyone has suggested this in the past, but a novel approach
would be to simply let anyone open a pull request and use the PR # as the
BIP #. This would avoid conflicts, and avoid the chore of having someone
manually assign them.

Downside would be that some numbers will never get used (for example if PRs
are opened to update existing BIPs), but this doesn't seem to be a huge
problem since already many numbers are going unused.

This process can still be independent from approving/merging the BIP into
master, if it meets quality standards.
On Thu, Dec 31, 2015 at 3:14 PM Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Mon, Jun 1, 2015 at 3:59 PM, Gavin Andresen wrote:

It would feel better for me if you would keep the power of two:

2^0 = 1MB
2^1 = 2MB
2^2 = 4MB
2^3 = 8MB
.
.
.

But that's only personal. Maybe other people feeling the same.

- oliver


-------------------------------------
Glad you like it, I was afraid that I missed something obvious :-)

The points the two of you raised are valid and I will address them as soon
as possible. I certainly will implement this proposal so that it becomes
more concrete, but my C++ is a bit rusty and it'll take some time, so I
wanted to gauge interest first.

minimum, there needs to be a legacy txid to normalized txid map in the
database.
could require a SPV proof of the spending transaction to be included with
legacy transactions.  This would allow clients to verify that the
normalized txid matched the legacy id.
| index}.  This allows a legacy transaction to be upgraded.  OutPoints
which use a normalized txid don't need the SPV proof.

It does and I should have mentioned it in the draft, according to my
calculations a mapping legacy ID -> normalized ID is about 256 MB in size,
or at least it was at height 330'000, things might have changed a bit and
I'll recompute that. I omitted the deprecation of legacy IDs on purpose
since we don't know whether we will migrate completely or leave keep both
options viable.

which opcodes does this affect, and how, exactly, does it affect them? Is
the merkle root in the block header computed using normalized transaction
ids or normalized ids?

I think both IDs can be used in the merkle tree, since we lookup an ID in
both indices we can use both to address them and we will find them either
way.

As for the opcodes I'll have to check, but I currently don't see how they
could be affected. The OP_*SIG* codes calculate their own (more
complicated) stripped transaction before hashing and checking the
signature. The input of the stripped transaction simply contains whatever
hash was used to reference the output, so we do not replace IDs during the
operation. The stripped format used by OP_*SIG* operations does not have to
adhere to the hashes used to reference a transaction in the input.

proposal before getting a BIP number. At least, I find that actually
writing the code often turns up issues I hadn't considered when thinking
about the problem at a high level. And I STRONGLY believe BIPs should be
descriptive ("here is how this thing works") not proscriptive ("here's how
I think we should all do it").

We can certainly split the proposal should it get too large, for now it
seems manageable, since opcodes are not affected. Bloom-filtering is
resolved by adding the normalized transaction IDs and checking for both IDs
in the filter. Since you mention bundling the change with other changes
that require a hard-fork it might be a good idea to build a separate
proposal for a generic hard-fork rollout mechanism.

If there are no obvious roadblocks and the change seems generally a good
thing I will implement it in Bitcoin Core :-)

Regards,
Chris

On Wed, May 13, 2015 at 3:44 PM Gavin Andresen <gavinandresen@gmail.com>
wrote:

-------------------------------------
I honestly don't understand your position, but I get the sense that you are
suggesting Satoshi wouldn't be welcome to return if he wanted to be active
in development again?

Warren
On Aug 17, 2015 1:38 PM, "Oliver Egginger" <bitcoin@olivere.de> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Grosses me out that you have enforced KYC as part of what you are
doing for anyone who would decide to get involved:

https://wiki.lykkex.com/?id=start#lykke_citizens

Good luck with that, I'm sure not going to be a part of it, and I
recommend that no-one else does either.

- - O

Richard Olsen via bitcoin-dev:

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEcBAEBCgAGBQJWDLjaAAoJEGxwq/inSG8CkQAH/i6603ivtZXjNw5ZlH1W2p7z
c88sb5CcTuTUi+zEx6Q0MRUFfdYcrcBrGsua3CKU9226rpL4acD2Bby5kUPZ1h2/
Rl5EiZa11oeqZaZaO5ZmXZ33BOaO2gxqqYEF1zBOzDgky6cqRrj8t4VAj5CKsxsP
ktM98UqVXdcuOfBP7y/xqX1Yw9e55PpwUCtaazLo8UkPLMrtdzrbKVZBtjqGxMnG
ZxmYku8g6xdmZAMz9xn9oVGtuMHrEjhIVycz3FMHBjoZNLE9yK4YeWyEvLI4YPFt
KBR7HvGDava3dzMM5ugw3hgFShfegjrIunWQ/vC9RCjBMLVGVX5RgEblgQe29eY=
=41DC
-----END PGP SIGNATURE-----

-------------------------------------
You're aware that my entire stack was built around this model and I've even built a fully fledged desktop GUI, multisig account manager, and servers supporting pull and event subscription atop it, right?

On September 17, 2015 5:07:20 PM PDT, "Wladimir J. van der Laan via bitcoin-dev" <bitcoin-dev@lists.linuxfoundation.org> wrote:

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------------------------------

Bear in mind, the spec defines "identity" to mean:

     *Identity is a particular extended public/private key pair. *

So that's not quite what is meant normally by identity. It's not a
government / real name identity or an email address or phone number kind of
identity.
-------------------------------------
On Thu, Aug 20, 2015 at 9:14 AM, Tamas Blummer <tamas@bitsofproof.com> wrote:

It wasn't just me: I didn't had the idea of creating a libconsensus
with a C API (thank Matt Corallo for that), I didn't removed all the
undesired dependencies or prepared the building part (thank Cory
Fields) and also thank at least Wladimir and Pieter who also
contributed in some ways I don't remember.
And of course also thank all the reviewers that made the PR merges possible.

I'm really happy to hear that libconsensus is being used, thank you
for your effort there too.


I don't understand what you mean by "quality" in this context. One of
the goals is to have as little dependencies as possible (so "more
modern tool sets" may not be suitable for libconsensus). libsecp256k1
will keep on being a dependency (highly optimized C code) and that's
about it.
Ideally I would like to slowly move libconsensus from C++ to C too,
but it seems other people would prefer to move to C++11 instead.


Yes, they are simpler and thus there's less risks of consensus fork
bugs, but it still exists.
It is true that the consensus code is currently spread all around
(specially in main.cpp), but completing libconsensus would solve that.
Lastly, since for consensus rules "the code is the specification", it
is unfortunate that the specification is coupled with a concrete
implementation (Bitcoin Core) and we should fix that.


But the goal is not reimplementing the consensus rules but rather
extract them from Bitcoin Core so that nobody needs to re-implement
them again.
It is not only exposing it but also separating it from Bitcoin Core so
that they can be changed without having to also change/take into
account non-consensus Bitcoin Core specific things.
A single PR would certainly be unacceptable, I was making many little
more acceptable ones (some of them already merged):

* [1/9] Consensus
** MERGED or DELETED
*** MERGED Consensus: Decouple pow from chainparams #5812 [consensuspow]
*** MERGED MOVEONLY: Move constants and globals to consensus.h #5696
[consensus_policy0]
*** DELETED Refactor: Create CCoinsViewEfficient interface for
CCoinsViewCache #5747 [coins]
*** MERGED Chainparams: Refactor: Decouple IsSuperMajority from
Params() #5968 [params_consensus]
*** MERGED Remove redundant getter
CChainParams::SubsidyHalvingInterval() #5996 [params_subsidy]
*** MERGED Separate CValidationState from main #5669 [consensus]
*** DELETED Consensus: Refactor: Separate CheckFinalTx from
main::IsFinalTx #6063 [consensus_finaltx]
*** MERGED Consensus: Decouple ContextualCheckBlockHeader from
checkpoints #5975 [consensus_checkpoints]
*** MERGED Separate Consensus::CheckTxInputs and GetSpendHeight in
CheckInputs #6061 [consensus_inputs]
*** MERGED Bugfix: Don't check the genesis block header before
accepting it #6299 [5975-quick-fix]
** REBASE Chainparams: Explicit Consensus::Params arg in consensus
functions #6024 [params_consensus2]
** REBASE Optimizations: Consensus: In AcceptToMemoryPool,
ConnectBlock, and CreateNewBlock #6445 [consensus-txinputs-0.12.99]
** REBASE MOVEONLY: Move most of consensus functions (pre-block) #6051
[consensus_moveonly]
** REBASE Consensus: Refactor: Turn CBlockIndex::GetMedianTimePast
into independent function #6009 [consensus_mediantime]
** DEPENDENT Consensus: Refactor: Consensus version of
CheckBlockHeader() #6035 [consensus_checkblockheader]
** DEPENDENT Consensus: Consensus version of pow functions [consensus_pow2]
** DEPENDENT API: Expose bitcoinconsensus_verify_header() in
libconsensus #5995 [consensus_header]
** DEPENDENT API: Expose bitcoinconsensus_verify_block() in
libconsensus #5946 [consensus_tip]

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

A decentralized, distributed application should offer its users
decentralized, distributed method of weighing in on the direction of
how it evolves as well as having an open development model.  The
reference to Facebook and Myspace is completely inapplicable here
because the tyranny of such spaces isn't what we have with bitcoin
(fortunately), nor would we want to try to replicate it, ever, in any
way, shape, or form.

Yes, it does bother (some) people to see the consensus based system
because of the difficulties that can be associated with implementing
it.  But that's the way it is.  If you don't like consensus based
systems (or decentralized, distributed systems) this is probably the
wrong space for you.

On 06/13/2015 04:57 PM, Raystonn wrote:
/msg02323.html
- -- 'peter'[:-1]@petertodd.org <http://petertodd.org>
- --------
_______________________________________________
- --------

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVfQL0AAoJEGxwq/inSG8CRqMH/0l9tHGA8figVGnIBoMgdpVi
uwMGTQTjLUf12/NFS27vT+OLMWqZRvVXvlxDF25N7la+QImhh67LqmQy8fkwGg5T
kJ6MkkFLgy05aqE/X3ywJUifOKmS3Y/RDDUJhrFjjHrsMGoF4ATtVwTpUBLik+kX
G3XRNlInmyB55UEcpyfBg9kfLz8xiy6sBPeaeGnFLCNWTs5TgJ6DTFqhBAAmE8Hw
k0tN6mW3wYS610FFkS2E3+W8O8KGs4oqAYLX/ZQOhX9oKjBvWWI4ppRpSDyBNcxd
A6VAKyU8HCuDHAEwba6gdlUa+yf4qxuZV1KCNENbvtN1CTsJ6oh0OxnEO6dtogo=
=KZmG
-----END PGP SIGNATURE-----


-------------------------------------
On Fri, Jun 12, 2015 at 1:04 PM, Eric Lombrozo <elombrozo@gmail.com> wrote:



Then they shouldn't care about the block size limit, since an increase in
block size (and thus in the number of txs they get fees from) will only
increase their revenue "negligibly".
-------------------------------------
On Mon, Jun 22, 2015 at 4:27 PM, Kalle Rosenbaum <kalle@rosenbaum.se> wrote:


Thanks, I'll fix.



Excellent point. That could only happen if activation happened on 11 Jan
2016; instead of complicating the code and spec with another condition, I
think it would be better to specify that the activation date is the later
of the miner supermajority and 11 Jan, with the first big block two weeks
later.


-- 
--
Gavin Andresen
-------------------------------------
0, 1, 3, 4, 5, 6 can be solved by looking at chunks chronologically. Ie,
give the signed (by sender) hash of the first and last block in your range.
This is less data dense than the idea above, but it might work better.

That said, this is likely a less secure way to do it. To improve upon that,
a node could request a block of random height within that range and verify
it, but that violates point 2. And the scheme in itself definitely violates
point 7.
On May 12, 2015 3:07 PM, "Gregory Maxwell" <gmaxwell@gmail.com> wrote:

-------------------------------------
Hi,

If you want to write such baseless acusations and inflamatory phrases, please do it somewhere else. We should have the highest respect for what these people are doing, and we should try to do something constructive, not waste time with anger and disrespect.

Nobody should be forced to do anything. People like you will not force me or anyone else to run code for your controversial hard fork just because you think the future will be bright with huge blocks. Just like the developers will not force you to continue running code implementing the current consensus rules.

The developers are not telling you what to do, they are trying to do what they consider is best for the ecosystem given their technical abilities.

Valiz

--------------------------------------------
În data de L, 17.8.15, NxtChg via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> a scris:

 Subiect: Re: [bitcoin-dev] Annoucing Not-BitcoinXT
 Către: jyellen@toothandmail.com, bitcoin-dev@lists.linuxfoundation.org
 Data: Luni, 17 August 2015, 13:09
 
 
 
 used to protect the status quo until real technical
 consensus is formed about the blocksize."
 
 consensus..."
 
 You mean
 the bunch of self-proclaimed Bitcoin wizards, who decided
 they have the right to tell everybody what to do, and who
 never got to grow up and are now angry at the world for not
 listening to them anymore? That "technical
 consensus"?
 
 "Bitcoin is decentralized, but you are
 only allowed to do what we tell you to do. It's our pet
 project, we wrote code for it!"
 
 That's what it all boils down to, all these
 dirty games of calling XT an alt-coin and censoring its
 posts, pretending to be Satoshi, sabotaging XT switch, etc.:
 "How dare they not listen to Us The Smartest
 anymore?!!!"
 
 Pathetic.
 The history will roll over you in a blink. The harder you
 try, the quicker it will go.
 
 _______________________________________________
 bitcoin-dev mailing list
 bitcoin-dev@lists.linuxfoundation.org
 https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
 

-------------------------------------
This is essentially the "nuclear option".  You are destroying the current
chain (converting it to a chain of coinbases) and using the same POW to
start the new chain.  You are also giving everyone credit in the new chain
equal to their credit in the old chain.

It would be better if the current chain wasn't destroyed.

This could be achieved by adding the hash of an extended block into the
coinbase but not requiring the coinbase to be the only transaction.

The new block is the legacy block plus the associated extended block.

Users would be allowed to move money to the extended block by spending it
to a specific output template.

<public key hash> OP_1 OP_TO_EXTENDED OP_TRUE

OP_1 is the extended block index and initially, only one level is available.

This would work like P2SH.  Users could spend the money on the extended
block chain exactly as they could on the main chain.

Money can be brought back the same way.

<public key hash> <txid1> <txid2> ... <txid-n> <N> OP_0 OP_UNLOCK OP_TRUE

The txids are for transactions that have been locked in root chain.  The
transaction is only valid if they are all fully funded.  The fee for the
transaction would be fee - (cost to fund unlocked txids).  A negative fee
tx would be invalid.

This has the advantage that it keeps the main chain operating.  People can
still send money with their un-upgraded clients.  There is also an
incentive to move funds to the extended block(s).  The new extended blocks
are more complex, but potentially have lower fees.  Nobody is forced to
change.  If the large blocks aren't needed, nobody will both to use them.

The rule could be

Now:
0) 1 MB

After change over
0) 1 MB
1) 2 MB

After 2 years
0) 1 MB
1) 2 MB
2) 4MB

After 4 years
0) 1 MB
1) 2 MB
2) 4MB
3) 8MB
-------------------------------------
On Wed, Jul 29, 2015 at 10:38 PM, Eric Voskuil <eric@voskuil.org> wrote:

Ok, let's assume we want to expose verifyHeader first (which I think
will be easier).


In https://github.com/bitcoin/bitcoin/pull/5995 I had one (probably
stupid) proposal.
But it had so many preparations commits that I had to close it.
In the last commit
https://github.com/jtimon/bitcoin/commit/00b9b227afc8669a877984561329dde75d3d8942
you can see that I'm adding a new function in
script/bitcoinconsensus.cpp with the following declaration:

int bitcoinconsensus_verify_header(const unsigned char* blockHeader,
unsigned int blockHeaderLen,
 const Consensus::Params& params, int64_t nTime, CBlockIndexBase*
pindexPrev, PrevIndexGetter indexGetter,
 bitcoinconsensus_error* err)

The ugly parts that you may not like are the CBlockIndexBase struct
(or maybe it's not so unreasonable) and the function pointer
PrevIndexGetter.
To see their "ugliness" you can look at:

https://github.com/jtimon/bitcoin/commit/4528ec69617f1b6d6c8f0d73dc4091cded7c216c

The PrevIndexGetter function pointer that Bitcoin Core would use
internally would be:

const CBlockIndexBase* GetPrevIndex(const CBlockIndexBase* pindex)
{
    return ((CBlockIndex*)pindex)->pprev;
}

with an ugly casting. But, well, I guess that's only ugly for Bitcoin
Core, not necessarily for other libconsensus users, which can define
their own function pointer, provided that it's of the form:

typedef const CBlockIndexBase* (*PrevIndexGetter)(const CBlockIndexBase*);

The struct that I think needs more refinement (and I just used what I
considered easier to implement at the time) is the CBlockIndexBase
struct itself:

+struct CBlockIndexBase
+{
+ //! pointer to the hash of the block, if any. Memory is owned by
this CBlockIndexBase
+ const uint256* phashBlock;
+ //! block header
+ int32_t nVersion;
+ uint256 hashMerkleRoot;
+ uint32_t nTime;
+ uint32_t nBits;
+ uint32_t nNonce;
+ //! height of the entry in the chain. The genesis block has height 0
+ int nHeight;
+};

I don't like phashBlock being a pointer instead of just a ref or even an object
Should that struct have a CBlockIndexBase* pprev; field (moving it
down from CBlockIndex)?
That's the kind of question where your feedback seems very important
from other-implementations developers (because you won't necessarily
take into account the difficulty of the refactors required in Bitcoin
Core to expose the right interface, and "libconsensus shouldn't care"
either, all we want is the best interface).


Agreed, and I would say all of the checkpoint check separation has
been done already.
What I mean by step functions is...look at verfyHeader internals, for example:

https://github.com/jtimon/bitcoin/commit/11ede96f59f611ede596a1335e896b1fef4fb5b2

It internally calls Consensus::CheckBlockHeader (quite cheap with no
context required) and Consensus::ContextualCheckBlockHeader (not so
cheap).
Bitcoin Core never calls (yet) the full verifyHeader at once. It does
the cheap tests first and the expensive later. For example,

call CheckBlockHeader, then CheckBlock (also cheap), then
ContextualCheckBlockHeader and then ContextualCheckBlock.

The question is, will other implementations want access to these
not-full-but-cheap tests?
In other words, apart from exposing VerifyHeader that fully validates
all consensus rules for a header, do we also want to expose
CheckBlockHeader and ContextualCheckBlockHeader to give more
flexibility to libconsensus' users?

I think, yes, other implementations will want this for the same DoS
reasons that Bitcoin Core currently wants them. But it would be nice
to know what a second person thinks about this.


In fact, one thing does: never changing the code again (but the cure
would be worse than the illness).
Agreed, any software changes in the consensus code can cause consensus
forks (and that's why you don't want to touch libconsensus that much
once it's separated).



Well, the "one true library" will be much better than the current "one
true full node".
The "one true library" would be the specification of the consensus
rules, but that doesn't mean you can't fork and modify it however you
want.


I get this point, even if the current satoshi client contains the
consensus rules specification (and many other things, obviously), that
doesn't mean is somehow protected from forking with itself if the
consensus code is changed in the wrong way accidentally. But the more
separated libconsensus and Bitcoin Core (satoshi client) are, the less
likely that changes in Bitcoin Core that weren't supposed to change
consensus rules actually do it by accident (like last time with the
migration out of bdb).


I think alternative implementations using a full libconsensus can
increase their adoption a lot, since they become just as vulnerable to
consensus forks as Bitcoin Core (instead of more vulnerable like now).


You mean libbitcoin's code is better organized than Bitcoin Core's?
I don't doubt it. Maybe we can create a full-libbitcoin-libconsensus
first and work on the API there.


Oh, I see, you don't like that libsecp256k1 is currently a subtree of
Bitcoin Core either for the same reasons, right?
To not need to know when the changes in libconsensus are applied in
Bitcoin Core.
Mhmm, once libconsensus is complete, why would you care about it?
You just care about the libconsensus version (which doesn't have to
coincide with Bitcoin Core versions anymore).


For the sake of clarity, please say "use the library's API". It's
going to use the library one way or another.


To be clear, I don't oppose to "dogfooding", it's just clear to me
that it will take even longer.
So what I don't understand is "once libbitcoin is complete and ready
for us to use, we will keep using our reimplementation of consensus
until Bitcoin Core uses the API as well. If Bitcoin core doesn't use
the API, we prefer not to use the library at all and keep having the
same consensus risk. We will do what we think it's worse for us until
Bitcoin Core uses the library through the API".


And we will hopefully migrate the current libconsensus from openSSL to
libsecp256k1 soon. So we will be able to enjoy libsecp256k1's
performance improvements without risking consensus. One problem less.


This was just a joke because you said something similar earlier.
Don't take it seriously.

-------------------------------------

If all the mining nodes are in one data center, and if all the nodes are programmed to build blocks in essentially the same way, then I would agree that the orphan cost would be negligible!  I will add this as an example of a network configuration where the results of my paper would be less relevant.  

Peter  


On 2015-08-29, at 7:35 PM, Matt Corallo <lf-lists@mattcorallo.com> wrote:


-------------------------------------
On Wed, Sep 30, 2015 at 5:11 PM, Mike Hearn <hearn@vinumeris.com> wrote:

Yes, your article contained numerous factual and logical inaccuracies
which I corrected (many of which you had been previously corrected on
as well by others). (For example:
https://www.reddit.com/r/Bitcoin/comments/3griiv/on_consensus_and_forks_by_mike_hearn/cu0yv0r
)

I would have hoped that after so many corrections you would have
updated your beliefs.


Yes, because what 101 does is not a hard-fork from the perspective of
BitcoinJ clients. Please do not conflate BitcoinJ with all of SPV; a
SPV client could validate the information received more extensively or
respond to alerts in reject rule violating blocks--  BitcoinJ does
not, but this is BitcoinJ's design decision to lack security in this
respect and not something inherent to SPV).

Directly fixing the time-warp attack, for example, would be a hard
fork from the perspective of BitcoinJ clients.  Recovering the fixed 0
bits in the header for use as extra-nonce would be a hard fork from
the perspective of BitcoinJ clients. Changing the transaction format
to include an explicit nonce for ECDH (e.g. stealth addresses) would
be a hard fork from the perspective of BitcoinJ clients. Increasing
the precision of Bitcoin by 1000 would be a hard fork from the
perspective of BitcoinJ. As would adjusting the hashtree to commit to
fees, including fees under OP_CHECKSIGs hash, or switching to the
segregated witness commitment structure from elements alpha that
allows syncing the chain without fetching signatures... all that would
be hardforks from the perspective of BitcoinJ.

Because of an cheaply avoidable the lack of validation in BitcoinJ no
increase of the blocksize is a hard-fork from its perspective. Nor
would increasing the subsidy to miners, or allowing third parties to
confiscate coins. But other SPV clients could, if they wanted to,
reject blocks the violated most of these criteria.

The argument you are presenting against BIP65 is that it is bad
because it is silently accepted. But this applies no less to 101 for
SPV clients, and in 101's case it's a failure to enforce pre-existing
rules which the users might care a great deal about. Worse, counting
on this kind of behavior can build a dependence on weak security forms
of SPV and inhibits the use of full security SPV.

In truth, both of BIP101 and BIP65 are detectable even by the most
simplistic and pre-change clients due to the voluntarily use of block
version signaling. Any participant in the network is free to take
whatever action they choose to take in response to such an event.
Bitcoin Core's behavior is to issue alerts to the user when unexpected
block versions show up on the network.  Users and implementer are free
to turn changes like BIP65 into hardforks from the perspective of
their own system, necessitating manual intervention, by simply forcing
the block version to be a particular value (or shutting down when
there are many blocks of a new version; until manually authorized to
continue).


For many changes, including CLTV the actual soft fork change is by far
the most natural way of implementing the change itself. One simply
takes an existing non-standard placeholder op code sequence and
assigns it the new VERIFY style meaning. It is clean, tidy, and the
result is nearly as if the system has had it all along. The only
complexity is around the activation and can be dropped in future code.

Beyond that, the primary upside is no forced industry wide "flag day"
where everyone is _forced_ to modify their software arises, taking
considerable cost.  People who care about the new rule can use it,
people who don't don't. All the rules that you care about enforcing
remain in force-- you still prevent inflation, you still will not
tolerate the theft of your own coins (or those of most other people),
etc.. No one is necessarily caught by surprise since the block
versions communicate that something is happening, allowing network
participants to choose to act (or not).

For example, for years you stonewalled P2SH and multi-signature.  You
didn't care about it. You didn't think it was valuable. You didn't add
it to your software, even after it was well specified and deployed in
production. Could it have been done as a hard-fork?  Likely not: you
would have prevented it. But as a soft-fork you were free to ignore it
with no ill-effect for a long time existing for those who cared about
it, and not for you, until widespread use resulted in demand enough to
justify accepting a patch that permitted sending to it.

What if we'd needed a hard fork to enable CoinJoin or other privacy
features?  I think would have blocked that too.

The relative ease of handling soft-forks which you are indifferent to
means that there is little reason to object to a compatible change
that gives other people flexibility they care about greatly but which
you are indifferent to; and it forces people who would oppose a
functionality because they don't want others to have some piece of
freedom to try to frame justifications in language other than "I don't
think it's worth the cost" since they have the nearly free option of
ignoring the change-- they're forced to actually argue against other
people having that freedom.

Soft-forks also allows us to deploy fixes to the Bitcoin protocol
which are more important-to-have but not urgently critical (like
height in coinbase),  or sometimes to deploy fixes to critical
vulnerabilities without first handing everyone excruciatingly detailed
instructions on exploiting them, simply by closing off an pattern of
protocol which is obviously bad and risky.

The primary cost of a soft-fork for non-participants is simply some
risk of increased network instability around the change-- but short
lived forks happen every day, and longer lived ones happen from time
to time. Larger amounts of instability occur from time to time due to
network partitioning, misconfiguration, and software bugs-- and client
software must be prepared to cope with it; this is a fact of bitcoin
and decenteralized systems in general. Upgraded, change enforcing,
client software is not exposed to the this instability, and
non-upgraded software could choose to mitigate any exposure by
monitoring the block versions. This is a far better situation that the
natural instability that will happen from time to time in a
decentralized system.

By contrast, the programmed activation point of BIP101 at 75% almost
guarantees activation among considerable controversy, promising
network instability which BitcoinJ clients would experience upgraded
or not, even if the larger block side was ultimately the losing side
in the switch. I find it more than a little strange that you think the
instability of a 75%-version-hashpower cut is acceptable but the a
95%-version-hashpower compatible change is not.

Finally, there is the demonstrated track record: They work; they
deliver new features to people. Our experience in the half dozen or
more soft-forks in the system so far is that in practice do not cause
significant problems, including financial losses for SPV wallet users.
Even with that complete success there has been room for improvement,
which is why the process has evolved over time to feature things like
preemptive non-standardness, high switchover thresholds, etc.  and
these will continue to evolve over time.

I hope that you can put aside your effort to force a blocksize
increase on others for a moment and add functionality, of the kind the
Bitcoin Core has had for years, to BitcoinJ to improve the experience
with soft-forks if you think it isn't good enough as is...

-------------------------------------
The problem with this approach is that you need 100% exact behaviour for
every node on the network in their decision to reject a particular block.
So we need a 100% mempool synchronization across all nodes - otherwise just
an attempted double spend could result in a fork in the network because
some nodes saw it and some didn't. And actually, if we had 100% mempool
synchronization, we wouldn't need a blockchain in the first place, because
we could just use "first to enter mempool" as validity criterion.

On Wed, Jul 1, 2015 at 1:41 AM, Peter Grigor <peter@grigor.ws> wrote:

-------------------------------------
On Tue, Jul 28, 2015 at 10:43 AM, Wladimir J. van der Laan
<laanwj@gmail.com> wrote:

As explained to Eric, it's not that I don't want Bitcoin Core to use
future-libconsensu through the API instead of a subtree: it's just
that that's more long-term and more work. And I don't see why other
implementations should really care about it.


Well, pure movements will not be enough, parameters will have to
change, incompatible dependencies have to be removed (ie util.h which
contains globals), etc.
But yes, I think we can do it with only low-risk and easy-to-review commits.


And still, this doesn't require Bitcoin Core to use the API, a subtree
is enough at first.
This "easy step" doesn't guarantee that Bitcoin Core is using
future-libconsensus' API.


I really think these code separations help with this (ie there are
many more people in the world with enough knowledge to review the qt
or even policy parts than there's people able to review consensus
changes).


I know and I said so.

-------------------------------------
On Sep 30, 2015 9:56 PM, "Mike Hearn via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
don't. You get constant mini divergences until everyone has upgraded, as
opposed to a single divergence with a hard fork (until everyone has
upgraded). The quantity of invalid blocks mined, on the other hand, is
identical in both types.

Exactly, all those "mini divergences" eventually disappear (because we're
assuming the hashrate majority has upgraded and non-upgraded miners accept
upgraded blocks as valid), even if the hashrate minority never upgrades.
On the other hand, the "single divergence" in the hardfork keeps growing
forever (unless all miners evetually upgrade.
With softforks, we maintain eventual consistency, with hardforks we don't.
-------------------------------------
On Thu, Aug 20, 2015 at 1:44 AM, NxtChg via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Certainly I have talked to much this month, my apologies.
I believe most of my posts (if not all) were on-topic but I could
still had repeated myself much less.
I've been trying to concentrate my usual points in documents or
threads that I can link to so that my comments can be shorter.
But, yes, most of my posts have been related to general consensus
topics and not specific to Bitcoin Core development (that's part of
why I think the bitcoin-consensus and bitcoin-dev lists would be a
good separation).
In any case, my apologies for this unplanned record.

-------------------------------------


I hate to break it to you, but you broadcast a photo of your face every
time you walk outside ;)

Bluetooth MAC addresses are random, they aren't useful identifiers. If
someone can see you, a face is a far more uniquely identifying thing than a
MAC.

"Payment spam" might be a problem. I can imagine a wallet requiring that
such requests are signed and then spammers can be blacklisted in the usual
fashion so they can't push things to your phone anymore. Anyway, a hurdle
that can be jumped if/when it becomes an issue.
-------------------------------------
This is a message that I wrote and had hoped that all the core devs would
sign on to, but I failed to finish organizing it.  So I'll just say it from
myself.

There has been a valuable discussion over the last several months regarding
a hard fork with respect to block size.  However the sheer volume of email
and proportion of discussion that is more philosophical than technical has
rendered this list almost unusable for its primary purpose of technical
discussion related to Bitcoin development.  Many of us share the blame for
letting the discourse run off topic to such a degree, and we hope that an
appeal for individual self restraint will allow this list to return to a
higher signal-to-noise ratio.
-Please consider the degree to which any email you send is related to
technical development before sending it.
-Please consider how many emails you are sending to this list regarding the
same topic.
This list is not appropriate for an endless back and forth debate on the
philosophical underpinnings of Bitcoin.  Although such a debate may be
worthwhile it should be taken to another forum for discussion.  Every email
you send is received by hundreds of developers who value their time as much
as you value yours.  If your intended audience isn't really the majority of
them, perhaps private communication would be more appropriate.

Thanks,
Alex
-------------------------------------
I shouldn't have said unlimited, i should have said a greater blocksize
limit such as 8mb.

Anyways, why is that the assumption?  If a miner can do so, and do so
profitably, isn't that just competition?  Isn't that what we want?  If a
miner can mine low transaction fees at a profit then don't they deserve to
have their spot?  Surely if they do so unprofitably they quickly find
themselves out of business?  Besides, if a miner mines low fee transactions
by breaking rank, how does this affect another miner EXCEPT for the
additional blocksize load.  I would maintain this is just competition
amongst miners gentlemen.  And it's a good thing.

Right now things are distorted because most income comes from the coinbase,
but as transaction fees start to constitute the majority of income this
idea seems to have more importance.
On Jul 29, 2015 11:00 PM, "Adam Back" <adam@cypherspace.org> wrote:

-------------------------------------
It will help to assume that there is at least one group of evil people who
are investing in Bitcon's demise.  Not because there are, but because there
might be.  So let's assume they are making a set of a billion transactions,
or a trillion, and maintaining currently-being-legitimately-used hashing
power.  When block size is large enough to frustrate other miners, this
hash power (or some piece of it) will be experimentally shifted to solving
a block containing an internally consistent subset of the prepared
trasnsactions to fill it - experimentally at first, but on the active
Bitcoin network.  One seemingly random, bloated, useless (except for the
universal timestamp) block will be created and the evil group will measure
the effect on the mining community - client takedowns, market exits, and
whatever else interests them.  Then they lie in wait, perhaps let out one
more to do another experiment, but with the goal of eventually catching us
unawares and doing as much damage to morale as possible.

Good concrete descriptions of the threats against which we want to guard
will be very helpful.  Maybe there are already unit tests for such things
or requests for miners' reactions to them (as opposed to just the
software's behavior).  My description might be a bit too long and perhaps
not a very good example, but do we have a place where such examples can be
constructed?

While we will do our best to guard against such nightmares, it's also
helpful to imagine what we will do if and when one of them ever actually
occurs.  Yes, I'm paranoid; because those who like to control everything
are losing it.

Dave

On Sun, Aug 2, 2015 at 3:38 AM, Venzen Khaosan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
In that case it didn't remedy the problem of constantly full blocks, but
got a ton of people on board the bitcoin train in the meanwhile. And with
them more interest, investments, research and ideas.

In the case we increase the limit and no government or banking group
immediately uses up all the space in blocks, it of course would prevent
constantly full blocks for the time being. Bitcoin keeps working reliably,
is fast and cheap -- more users on board the bitcoin train. And as long as
we do not immediately increase it to 1.6GB (something no one is
suggesting), we could still maintain a good level of decentralization,
maybe even increase it together with an increase in users.

Eventually we'll have to deal with the fading subsidy, but there's no
reason to deal with it at this point in time. What we need today is more
users and artifically limiting the blocksize doesn't help with that. At the
very least we should try to incorporate technological growth into bitcoin.
Keep the 2010 1MB limit, but account for "inflation". If you're arguing
that noone can predict the future development of bandwidth, cpu and storage
I'd say it doesn't matter if we are too optimistic, because when we start
seeing that a future doubling of the limit will cause major issues, it will
be easy to find consensus and change the rule accordingly.


2015-08-14 4:26 GMT+02:00 Venzen Khaosan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------

On Mon, 26 Jan 2015, Gregory Maxwell wrote:


Progress information for the list: there is now a pull request
implementing the strict DER verification behavior, as well as the
deployment specified in BIP66 for Bitcoin Core. It needs
your review and testing:

https://github.com/bitcoin/bitcoin/pull/5713

Wladimir


-------------------------------------
<html><head></head><body><div style="font-family: Verdana;font-size: 12.0px;"><div>
<div>Why not help on a project that actually seems to offer great scalability like the lightning network? There have been great progress there.</div>

<div>&nbsp;</div>

<div>Seems like you did your calculations some time ago to prove that your increase is reasonable, yet when others come with different numbers that don&#39;t support your position you say it doesn&#39;t matter.</div>

<div>&nbsp;
<div name="quote" style="margin:10px 5px 5px 10px; padding: 10px 0 10px 10px; border-left:2px solid #C3D9E5; word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">
<div style="margin:0 0 10px 0;"><b>Sent:</b>&nbsp;Thursday, July 23, 2015 at 4:28 PM<br/>
<b>From:</b>&nbsp;&quot;Gavin Andresen via bitcoin-dev&quot; &lt;bitcoin-dev@lists.linuxfoundation.org&gt;<br/>
<b>To:</b>&nbsp;&quot;Tom Harding&quot; &lt;tomh@thinlink.com&gt;<br/>
<b>Cc:</b>&nbsp;bitcoin-dev@lists.linuxfoundation.org<br/>
<b>Subject:</b>&nbsp;Re: [bitcoin-dev] Bitcoin Core and hard forks</div>

<div name="quoted-content">
<div>
<div class="gmail_extra">
<div class="gmail_quote">On Thu, Jul 23, 2015 at 12:17 PM, Tom Harding via bitcoin-dev <span>&lt;<a href="bitcoin-dev@lists.linuxfoundation.org" target="_parent">bitcoin-dev@lists.linuxfoundation.org</a>&gt;</span> wrote:

<blockquote class="gmail_quote" style="margin: 0 0 0 0.8ex;border-left: 1.0px rgb(204,204,204) solid;padding-left: 1.0ex;"><span>On 7/23/2015 5:17 AM, Jorge Tim&oacute;n via bitcoin-dev wrote:</span><br/>
<span>&gt; they will simply advance the front and start another battle, because<br/>
&gt; their true hidden faction is the &quot;not ever side&quot;. Please, Jeff, Gavin,<br/>
&gt; Mike, show me that I&#39;m wrong on this point. Please, answer my question</span><br/>
<span>&gt; this time. If &quot;not now&quot;, then when?</span><br/>
<br/>
Bitcoin has all the hash power.&nbsp; The merkle root has effectively<br/>
infinite capacity.&nbsp; We should be asking HOW to scale the supporting<br/>
information propagation system appropriately, not WHEN to limit the<br/>
capacity of the primary time-stamping machine.<br/>
<br/>
We haven&#39;t tried yet.&nbsp; I can&#39;t answer for the people you asked, but<br/>
personally I haven&#39;t thought much about when we should declare failure.</blockquote>

<div>&nbsp;</div>
</div>

<div>Yes! Lets plan for success!</div>

<div>&nbsp;</div>

<div>I&#39;d really like to move from &quot;IMPOSSIBLE because... &nbsp;(electrum hasn&#39;t been optimized</div>

<div>(by the way: you should run on SSDs, LevelDB isn&#39;t designed for spinning disks),</div>

<div>what if the network is attacked? &nbsp;(attacked HOW???), current p2p network is using</div>

<div>the simplest, stupidest possible block propagation algorithm...)&quot;</div>

<div>&nbsp;</div>

<div>... to &quot;lets work together and work through the problems and scale it up.&quot;</div>

<div>&nbsp;</div>

<div>I&#39;m frankly tired of all the negativity here; so tired of it I&#39;ve decided to mostly ignore</div>

<div>all the debate for a while, not respond to misinformation I see being spread</div>

<div>(like &quot;miners have some incentive to create slow-to-propagate blocks&quot;),</div>

<div>work with people like Tom and Mike who have a &#39;lets get it done&#39; attitude, and</div>

<div>focus on what it will take to scale up.</div>

<div>&nbsp;</div>
--

<div class="gmail_signature">--<br/>
Gavin Andresen</div>

<div class="gmail_signature">&nbsp;</div>
</div>
</div>
_______________________________________________ bitcoin-dev mailing list bitcoin-dev@lists.linuxfoundation.org <a href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev" target="_blank">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a></div>
</div>
</div>
</div></div></body></html>

-------------------------------------
Wouldnt the experience for SPV nodes be chaotic?  If the full nodes
are 50:50 XT and bitcoin core, then SPV clients would connect at
random and because XT and core will diverge immediately after
activation.

Adam


On 19 August 2015 at 15:28, Jorge Timón
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Aug 7, 2015 11:19 PM, "Sergio Demian Lerner via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
with any of them: "SPV" mining and the relay network has shown that block
propagation is not an issue for such as small change. Mining centralization
won't radically change for a 2x adjustment.

I don't understand this. All problems that result from propagation delay
are literally doubled by doing so. Centralization pressure results from the
ratio between propagation time and interblock time. Efficient propagation
algorithms like the relay network make this presumably grow sublinear with
larger blocks, but changing the interblock time affects it exactly
proportionally.

All problems that result from propagation delay are literally doubled by
doing this. Doubling the block size has a smaller effect. You may argue
that these centralization effects are small, but reducing the interblock
time has a stronger effect on them than the block size.

Also, you seem to consider SPV mining a good thing? It requires trust
between miners that know eachother, and fundamentally breaks the security
assumption of SPV clients... and if the propagation/interblock ratio was
lower, SPV mining would have less effect. I'd say it is exactly a result of
the centralization pressure we're trying to avoid.

-- 
Pieter
-------------------------------------


Regarding rhetoric, fair enough, Gavin - I’m human and I could be wrong. It is my educated best guess, a conclusion I’ve drawn given my understanding of computer science, economics, and what’s been happening in this space.
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----

Hash: SHA1


https://github.com/justusranvier/rfc/blob/payment_code/bips/bip-pc01.mediawiki


This link contains an RFC for a new type of Bitcoin address called a
"payment code"


Payment codes are SPV-friendly alternatives to DarkWallet-style stealth
addresses which provide useful features such as positively identifying
senders to recipients and automatically providing for transaction refunds.


Payment codes can be publicly advertised and associated with a real-life
identity without causing a loss of financial privacy.


Compared to stealth addresses, payment codes require less blockchain data
storage.


Payment codes require 65 bytes of OP_RETURN data per sender-recipient pair,
while stealth addresses require 40 bytes per transaction.


-----BEGIN PGP SIGNATURE-----

Version: GnuPG v1


iQIcBAEBAgAGBQJVOqCRAAoJECpf2nDq2eYjluEP/RVJk+miDIihY4ilIvUbKvMd

JLLqHr7Q1dlZyMIG/UqVWdoP5hzg/16B+q2iAB9jXozPnrDp0mggBh6rIGroevAa

Kqfrs+Rrog1w9auhd67LWORDqav6YIrjTJIxdLxe11IEiq5rWbHPNUEDMzdEmHbz

QfTH7KWAP2BasO5ETXcfu6BcccrXZ3XOKLON2h3NGD/cEDizY+uT2k3QN54z+KxG

NB9scKbzVvsJwkyBrgbV+As9H3k6PnFsojYgAaE9gkp7D2+ahjzUiOH5rv6TbbYR

o2X5MOiTY2/YZEqZPG7IR03ZAgeLVCvXXysjPOfzUKbmTF4w849sm8BuhixzDXHo

2V/HHKoGclIohcODBCWi0tVQXshZt4QkCNJBW5o3nL6Nn2YOp6hmw8YKAHnw3E7h

/wIgk5f+NOLl/iIxoAxAdavEj5P6N4ic+OB6MAjnhEilWfBvCIpqWLGNvrtOhEa9

EnPHcgb4ILBu4OionJhsNpJ/O95C0OEypMm25MIS+rQcV4Uxe5IOS2OuT/GreLET

n/7Y0mJbqYbLBjVsfS+DNjvsgyJl5AxhcMrdVyXJjSYVcCoRhcoX5Ceidd+YkbHI

OMs5f63tM1Rgi/WY4Ct80SD5EbULZuu8j1KJ9HPGuMt081JSBH+L5isiKuazPeO+

SGApMBd4Q89fKzL2djae

=Dypr

-----END PGP SIGNATURE-----
-------------------------------------
Regarding the re-hashing the transaction data once per input being a
bottleneck, I was mistakenly only thinking about this from the point of
view of the signer. Full nodes have to check all transactions' inputs,
which is much more costly, as the link Gavin posted shows.

On Thu, Apr 9, 2015 at 10:45 AM, Mike Hearn <mike@plan99.net> wrote:

I can think of a few convoluted use cases, but not any good ones. People
have definitely looked for this feature before, though, just look at this
Bitcoin SE post
<http://bitcoin.stackexchange.com/questions/1495/is-there-a-way-to-automatically-send-bitcoins-from-one-wallet-to-another>.
I think there are better ways to handle key management than
auto-forwarding, though. Anyone looking for this feature probably just
wasn't aware that there are better solutions.

On Thu, Apr 9, 2015 at 10:45 AM, Mike Hearn <mike@plan99.net> wrote:

In the bitcointalk article referenced, Sergio actually gave us the answer:

(Inputs-with-script-cleared) || <previn-index> )
"Hash(Inputs-with-script-cleared)" can be cached and reused.

Basically, just re-order the way stuff is serialized. Put the stuff that is
nearly always signed at the beginning, and vice versa. I'll see if I can
update the proposal to make this optimization possible. What I suspect,
though, is that with all the new controls, blocks with ordinary
transactions will verify faster, but an attacker could still create a very
CPU intensive block by signing inputs with a wide variety of nHashTypes and
then signing the last one with the equivalent of SIGHASH_ALL. I don't think
that's a big limitation, though, the attack is already somewhat possible,
and would be very hard to do, and doesn't really gain the attacker anything
(other than infamy).

On Thu, Apr 9, 2015 at 1:28 PM, Peter Todd <pete@petertodd.org> wrote:


I've never been able to really see a good use case for OP_CODESEPARATOR,
and I'm not sure I completely have my head wrapped around what you're
proposing. From this
<http://bitcoin.stackexchange.com/questions/34013/what-is-op-codeseparator-used-for>
 and this
<https://bitcointalk.org/index.php?topic=52949.msg631255#msg631255>,
though, it seems like OP_CODESEPARATOR cannot really be made useful unless
you already have a way to sign without hashing the TXIDs referenced by your
input, in which case you need to modify the nHashType.

Best,
Stephen
-------------------------------------
Here are the latest results on compression ratios for the first 295,000
blocks, compressionlevel=6.  I think there are more than enough
datapoints for statistical significance. 

Results are very much similar to the previous test.   I'll work on
getting a comparison between how much time savings/loss in time there is
when syncing the blockchains: compressed vs uncompressed.  Still, I
think it's clear that serving up compressed blocks, at least historical
blocks, will be of benefit for those that have bandwidth caps on their
internet connections.

The proposal, so far is fairly simple:
1) compress blocks with some compression library: currently zlib but I
can investigate other possiblities
2) As a fall back we need to advertise compression as a service.  That
way we can turn off compression AND decompression completely if needed.
3) Do the compression at the datastream level in the code.  CDataStream
is the obvious place.


Test Results:

range = block size range
ubytes = average size of uncompressed blocks
cbytes = average size of compressed blocks
ctime = average time to compress
dtime = average time to decompress
cmp_ratio% = compression ratio
datapoints = number of datapoints taken

range       ubytes    cbytes    ctime    dtime    cmp_ratio%    datapoints
0-250b      215            189    0.001    0.000    12.40             91280
250-500b    438            404    0.001    0.000    7.85             13217
500-1KB     761            701    0.001    0.000    7.86               11434
1KB-10KB    4149    3547    0.001    0.000      14.51             52180
10KB-100KB  41934    32604    0.005    0.001    22.25         82890
100KB-200KB 146303    108080    0.016    0.001    26.13    29886
200KB-300KB 243299    179281    0.025    0.002    26.31    25066
300KB-400KB 344636    266177    0.036    0.003    22.77    4956
400KB-500KB 463201    356862    0.046    0.004    22.96    3167
500KB-600KB 545123    429854    0.056    0.005    21.15    366
600KB-700KB 647736    510931    0.065    0.006    21.12    254
700KB-800KB 746540    587287    0.073    0.008    21.33    294
800KB-900KB 868121    682650    0.087    0.008    21.36    199
900KB-1MB   945747    726307    0.091    0.010    23.20    304

On 10/11/2015 8:46 AM, Jeff Garzik via bitcoin-dev wrote:

-------------------------------------

@gmaxwell Bip Editor, and the Bitcoin Dev Community,

After several weeks of experimenting and testing with various
compression libraries I think there is enough evidence to show that
compressing blocks and transactions is not only beneficial in reducing
network bandwidth but is also provides a small performance boost when
there is latency on the network.

The following is a BIP Draft document for your review. 
(The alignment of the columns in the tables doesn't come out looking
right in this email but if you cut and paste into a text document they
are just fine)


<pre>
  BIP: ?
  Title: Datastream compression of Blocks and Tx's
  Author: Peter Tschipper <peter.tschipper@gmail.com>
  Status: Draft
  Type: Standards Track
  Created: 2015-11-30
</pre>

==Abstract==

To compress blocks and transactions, and to concatenate them together
when possible, before sending.

==Motivation==

Bandwidth is an issue for users that run nodes in regions where
bandwidth is expensive and subject to caps, in addition network latency
in some regions can also be quite high. By compressing data we can
reduce daily bandwidth used in a significant way while at the same time
speed up the transmission of data throughout the network. This should
encourage users to keep their nodes running longer and allow for more
peer connections with less need for bandwidth throttling and in
addition, may also encourage users in areas of marginal internet
connectivity to run nodes where in the past they would not have been
able to.

==Specification==

Advertise compression using a service bit.  Both peers must have
compression turned on in order for data to be compressed, sent, and
decompressed.

Blocks will be sent compressed.

Transactions will be sent compressed with the exception of those less
than 500 bytes.

Blocks will be concatenated when possible.

Transactions will be concatenated when possible or when a
MSG_FILTERED_BLOCK is requested.

Compression levels to be specified in "bitcoin.conf".

Compression and decompression can be completely turned off.

Although unlikely, if compression should fail then data will be sent
uncompressed.

The code for compressing and decompressing will be located in class
CDataStream.

Compression library LZO1x will be used.

==Rationale==

By using a service bit, compression and decompression can be turned
on/off completely at both ends with a simple configuration setting. It
is important to be able to easily turn off compression/decompression as
a fall back mechanism.  Using a service bit also makes the code fully
compatible with any node that does not currently support compression. A
node that do not present the correct service bit will simply receive
data in standard uncompressed format.

All blocks will be compressed. Even small blocks have been found to
benefit from compression.
 
Multiple block requests that are in queue will be concatenated together
when possible to increase compressibility of smaller blocks.
Concatenation will happen only if there are multiple block requests from
the same remote peer.  For example, if peer1 is requesting two blocks
and they are both in queue then those two blocks will be concatenated.
However, if peer1 is requesting 1 block and peer2 also one block, and
they are both in queue, then each peer is sent only its block and no
concatenation will occur. Up to 16 blocks (the max blocks in flight) can
be concatenated but not exceeding the MAX_PROTOCOL_MESSAGE_LENGTH.
Concatenated blocks compress better and further reduce bandwidth.

Transactions below 500 bytes do not compress well and will be sent
uncompressed unless they can be concatenated (see Table 3).

Multiple transaction requests that are in queue will be concatenated
when possible.  This further reduces bandwidth needs and speeds the
transfer of large requests for many transactions, such as with
MSG_FILTERED_BLOCK requests, or when the system gets busy and is flooded
with transactions.  Concatenation happens in the same way as for blocks,
described above.

By allowing for differing compression levels which can be specified in
the bitcoin.conf file, a node operator can tailor their compression to a
level suitable for their system.

Although unlikely, if compression fails for any reason then blocks and
transactions will be sent uncompressed.  Therefore, even with
compression turned on, a node will be able to handle both compressed and
uncompressed data from another peer.

By Abstracting the compression/decompression code into class
"CDataStream", compression can be easily applied to any datastream.

The compression library LZO1x-1 does not compress to the extent that
Zlib does but it is clearly the better performer (particularly as file
sizes get larger), while at the same time providing very good
compression (see Tables 1 and 2).  Furthermore, LZO1x-999 can provide
and almost Zlib like compression for those who wish to have more
compression, although at a cost.

==Test Results==

With the LZO library, current test results show up to a 20% compression
using LZO1x-1 and up to 27% when using LZO1x-999.  In addition there is
a marked performance improvement when there is latency on the network.
improvement in performance when comparing LZO1x-1 compressed blocks with
uncompressed blocks (see Table 5).

The following table shows the percentage that blocks were compressed,
using two different Zlib and LZO1x compression level settings.

TABLE 1:
range = data size range
range           Zlib-1  Zlib-6  LZO1x-1 LZO1x-999
-----------     ------  ------  ------- --------
0-250           12.44   12.86   10.79   14.34
250-500         19.33   12.97   10.34   11.11   
600-700         16.72   n/a     12.91   17.25
700-800         6.37    7.65    4.83    8.07
900-1KB         6.54    6.95    5.64    7.9
1KB-10KB        25.08   25.65   21.21   22.65
10KB-100KB      19.77   21.57   4.37    19.02
100KB-200KB     21.49   23.56   15.37   21.55
200KB-300KB     23.66   24.18   16.91   22.76
300KB-400KB     23.4    23.7    16.5    21.38
400KB-500KB     24.6    24.85   17.56   22.43
500KB-600KB     25.51   26.55   18.51   23.4
600KB-700KB     27.25   28.41   19.91   25.46
700KB-800KB     27.58   29.18   20.26   27.17
800KB-900KB     27      29.11   20      27.4
900KB-1MB       28.19   29.38   21.15   26.43
1MB -2MB        27.41   29.46   21.33   27.73

The following table shows the time in seconds that a block of data takes
to compress using different compression levels.  One can clearly see
that LZO1x-1 is the fastest and is not as affected when data sizes get
larger.

TABLE 2:
range = data size range
range           Zlib-1  Zlib-6  LZO1x-1 LZO1x-999
-----------     ------  ------  ------- ---------
0-250           0.001   0       0       0
250-500         0       0       0       0.001
500-1KB         0       0       0       0.001
1KB-10KB        0.001   0.001   0       0.002
10KB-100KB      0.004   0.006   0.001   0.017
100KB-200KB     0.012   0.017   0.002   0.054
200KB-300KB     0.018   0.024   0.003   0.087
300KB-400KB     0.022   0.03    0.003   0.121
400KB-500KB     0.027   0.037   0.004   0.151
500KB-600KB     0.031   0.044   0.004   0.184
600KB-700KB     0.035   0.051   0.006   0.211
700KB-800KB     0.039   0.057   0.006   0.243
800KB-900KB     0.045   0.064   0.006   0.27
900KB-1MB       0.049   0.072   0.006   0.307

TABLE 3:
Compression of Transactions (without concatenation)
range = block size range
ubytes = average size of uncompressed transactions
cbytes = average size of compressed transactions
cmp% = the percentage amount that the transaction was compressed
datapoints = number of datapoints taken

range       ubytes    cbytes    cmp%    datapoints
----------  ------    ------    ------  ----------    
0-250       220       227       -3.16   23780
250-500     356       354       0.68    20882
500-600     534       505       5.29    2772
600-700     653       608       6.95    1853
700-800     757       649       14.22   578
800-900     822       758       7.77    661
900-1KB     954       862       9.69    906
1KB-10KB    2698      2222      17.64   3370
10KB-100KB  15463     12092     21.80   15429

The above table shows that transactions don't compress well below 500
bytes but do very well beyond 1KB where there are a great deal of those
large spam type transactions.   However, most transactions happen to be
in the < 500 byte range.  So the next step was to appy concatenation for
those smaller transactions.  Doing that yielded some very good
compression results.  Some examples as follows:

The best one that was seen was when 175 transactions were concatenated
before being compressed.  That yielded a 20% compression ratio, but that
doesn't take into account the savings from the unneeded 174 message
headers (24 bytes each) as well as 174 TCP ACKs of 52 bytes each which
yields and additional 76*174 = 13224 byte savings, making for an overall
bandwidth savings of 32%:

     2015-11-18 01:09:09.002061 compressed data from 79890 to 67426
txcount:175

However, that was an extreme example.  Most transaction aggregates were
in the 2 to 10 transaction range.  Such as the following:

     2015-11-17 21:08:28.469313 compressed data from 3199 to 2876 txcount:10

But even here the savings of 10% was far better than the "nothing" we
would get without concatenation, but add to that the 76 byte * 9
transaction savings and we have a total 20% savings in bandwidth for
transactions that otherwise would not be compressible.  Therefore the
concatenation of small transactions can also save bandwidth and speed up
the transmission of those transactions through the network while keeping
network and message queue chatter to a minimum.

==Choice of Compression library==

LZO was chosen over Zlib.  LZO is the fastest most scalable option when
used at the lowest compression setting which will be a performance boost
for users that prefer performance over bandwidth savings. And at the
higher end, LZO provides good compression (although at a higher cost)
which approaches that of Zlib.

Other compression libraries investigated were Snappy, LZOf, fastZlib and
LZ4 however none of these were found to be suitable, either because they
were not portable, lacked the flexibility to set compression levels or
did not provide a useful compression ratio.

The following two tables show results in seconds for syncing the first
200,000 blocks. Tests were run on a high-speed wireless LAN with very
little latency, and also run with a 60ms latency which was induced with
"Netbalancer".
               
TABLE 4:
Results shown in seconds on highspeed wireless LAN (no induced latency)
Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999
---------------  -----  ------  ------  -------  ---------
10000            255    232     233     231      257      
20000            464    414     420     407      453      
30000            677    594     611     585      650      
40000            887    787     795     760      849     
50000            1099   961     977     933      1048   
60000            1310   1145    1167    1110     1259  
70000            1512   1330    1362    1291     1470  
80000            1714   1519    1552    1469     1679   
90000            1917   1707    1747    1650     1882  
100000           2122   1905    1950    1843     2111    
110000           2333   2107    2151    2038     2329  
120000           2560   2333    2376    2256     2580   
130000           2835   2656    2679    2558     2921 
140000           3274   3259    3161    3051     3466   
150000           3662   3793    3547    3440     3919   
160000           4040   4172    3937    3767     4416   
170000           4425   4625    4379    4215     4958   
180000           4860   5149    4895    4781     5560    
190000           5855   6160    5898    5805     6557    
200000           7004   7234    7051    6983     7770   

TABLE 5:
Results shown in seconds with 60ms of induced latency
Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999
---------------  -----  ------  ------  -------  ---------
10000            219    299     296     294      291
20000            432    568     565     558      548
30000            652    835     836     819      811
40000            866    1106    1107    1081     1071
50000            1082   1372    1381    1341     1333
60000            1309   1644    1654    1605     1600
70000            1535   1917    1936    1873     1875
80000            1762   2191    2210    2141     2141
90000            1992   2463    2486    2411     2411
100000           2257   2748    2780    2694     2697
110000           2627   3034    3076    2970     2983
120000           3226   3416    3397    3266     3302
130000           4010   3983    3773    3625     3703
140000           4914   4503    4292    4127     4287
150000           5806   4928    4719    4529     4821
160000           6674   5249    5164    4840     5314
170000           7563   5603    5669    5289     6002
180000           8477   6054    6268    5858     6638
190000           9843   7085    7278    6868     7679
200000           11338  8215    8433    8044     8795

==Backward compatibility==

Being unable to present the correct service bit, older clients will
continue to receive standard uncompressed data and will be fully
compatible with this change.

==Fallback==

It is important to be able to entirely and easily turn off compression
and decompression as a fall back mechanism. This can be done with a
simple bitcoin.conf setting of "compressionlevel=0". Only one of the two
connected peers need to set compressionlevel=0 in order to turn off
compression and decompression completely.

==Deployment==

This enhancement does not require a hard or soft fork.

==Service Bit==

During the testing of this implementation, service bit 28 was used,
however this enhancement will require a permanently assigned service bit.

==Implementation==

This implementation depends on the LZO compression library: lzo-2.09

     https://github.com/ptschip/bitcoin/tree/compress

==Copyright==

This document is placed in the public domain.



-------------------------------------
When implemented, the block size limit was put in place to prevent the 
potential for a massive block to be used as an attack to benefit the miner 
of that block.  The theory goes that such a massive block would enrich its 
miner by delaying other miners who are now busy downloading and validating 
that huge block.  The original miner of that large-block would be free to 
continue hashing the next block, giving it an advantage.

Unfortunately, this block size limit opened a different attack.  Prior to 
the limit, any attempt to spam the network by anyone other than someone 
mining their own transactions would have been economically unfeasible.  As 
every transaction would have a fee, there would have been a real cost for 
every minute of spam.  The end result would have been a transfer of wealth 
from spammer to Bitcoin miners, which would have harmed the spammers and 
encouraged further mining of Bitcoin, a very antifragile outcome.

But now we have the block size limit.  Things are very different with this 
feature in place.  The beginning of a spam attack on the Bitcoin network 
will incur transaction fees, just like before.  But if spam continues at a 
rate exceeding the block size limit long enough for transactions to be 
dropped from mempools, the vast majority of spam transaction fees will never 
have to be paid.  In fact, as real users gain in desperation and pay higher 
fees to get their transactions through in a timely manner, the spammers will 
adjust their fees to minimize the cost of the attack and maximize 
effectiveness.  Using this method, they keep their fees at a point that 
causes most of the spam transactions to be dropped without confirmation 
(free spam), while forcing a floor for transaction fees.  Thus, while spam 
could be used by attackers to disable the network entirely, by paying 
high-enough fees to actually fill the blocks with spam, it can also be used 
by a single entity to force a transaction fee floor.  Real users will be 
forced to pay a transaction fee higher than the majority of the spam to get 
their transactions confirmed.  So this is an effective means for a minority 
of miners to force higher fees through spam attacks, even in the face of 
benevolent miners who would not support a higher fee floor by policy. 
Miners would simply have no way to fix this, as they can only put in the 
transactions that will fit under the block size limit.

In the face of such a spam attack, Bitcoin's credibility and usability would 
be severely undermined.  The block size limit enables this attack, and I now 
argue for its removal.  But we can't just remove it and ignore the problem 
that it was intended to address.  We need a new fix for the large-block 
problem described in the first paragraph that does not suffer from the 
dropped-transaction spam-attack problem that is enabled by the block size 
limit today.  My proposal is likely to be controversial, and I'm very much 
open to hearing other better proposals.

Large blocks created by a miner as a means to spam other miners out of 
competition is a problem because miners do not pay fees for their own 
transactions when they mine them.  They collect the fees they pay.  This 
breaks the economic barrier keeping people from spamming the network, as the 
spamming is essentially free.  The proposed fix is to add a new rule on how 
fees are handled.  Some amount of every fee should be considered as burned 
and can never be spent.  I will propose 50% of the fee here, but there may 
be better numbers that can be discovered prior to putting this into place. 
If we'd like miners to continue to collect the same fees after this change, 
we can suggest the default fee per transaction to be doubled.  Half of every 
fee would be burned and disappear forever, effectively distributing the 
value of those bitcoins across the entire money supply.  The other half 
would be collected by the miner of the block as is done today.  This 
solution would mean large blocks would cost a significant number of bitcoin 
to create, even when all of the transactions are created by the miner of 
that block.  For this to work, we'd need to ensure a minimum fee is paid for 
most of the transactions in every block, and the new transaction fee rule is 
in place.  Then the block size limit can be removed.

Raystonn



-------------------------------------
I feel your pain. I've had the same thing happen to me in the past. And I
agree it's more likely to occur with my proposed scheme but I think with HD
wallets there will still be UTXOs left unspent after most transactions
since, for privacy sake it's looking for the smallest set of addresses that
can be linked.
On May 9, 2015 9:11 PM, "Matt Whitlock" <bip@mattwhitlock.name> wrote:

-------------------------------------
Sigh. The wallet words system is turning into kind of a mess.

I thought the word list is in fact not a fixed part of the spec, because
the entropy is a hash of the words. But perhaps I'm misunderstanding
something.

The main problem regular SPV wallets have with BIP39 is that there is no
birth time included in the data. Therefore we must ask users to write down
a timestamp as well, so we know where to start rescanning the chain. It
sounds like the Electrum version doesn't fix this, so now we have at least
FIVE incompatible results from a 12 word list:

   - Electrum v2 with a version number but no date
   - myTREZOR with no version and no date and BIP44 key derivation. Some
   seeds I believe are now being generated with 24 words instead of 12.
   - MultiBit HD with no version and a date in a custom form that creates
   non-date-like codes you are expected to write down. I think BIP32 and BIP44
   are both supported (sorta).
   - GreenAddress with no version, no date and BIP32
   - Other bitcoinj based wallets, with no version and a date written down
   in normal human form, BIP32 only.

I really hope we can recover from this somehow because otherwise all
wallets will have to provide the user with a complicated matrix of
possibilities and software combinations, and in practice many won't bother
so these word combinations will actually end up being wallet specific for
no particularly good reason, just very minor details like the presence or
absence of single fields.

It feels like we somehow fell flat on our faces just before the finishing
line. This is deeply unfortunate. Compatibility and UX consistency is
important!

Currently, I don't have any bright ideas for how to get everyone back onto
the same page with a fully compatible system that is acceptable to all. If
anyone else has suggestions, I'm all ears.
-------------------------------------
It would be possible to run a simplified version of the bits proposal,
until BIP 66 locks.

It's obviously not worth it at this point though, though it could be 1-2
weeks more.

Version 2 means neither option
Version 3 means BIP 66 only
Version 4 means CLTV only
Version 5 means both

If (Version 3 + version 5) > 95%, reject 2 & 4
If (Version 4 + version 5) > 95%, reject 2 & 3

For 2 options at the same time, this isn't much extra overhead.


On Fri, Jun 26, 2015 at 12:52 AM, Eric Lombrozo <elombrozo@gmail.com> wrote:

-------------------------------------
On Sun, Nov 8, 2015 at 8:54 AM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



Gavin, could you please provide some clarity around the definition and
meaning of "key-holder [decentralization]"? Is this about the absolute
number of key-holders? or rather about the number of transactions (per unit
time?) that key-holders make? Both/other?

Anyone can generate a private key, and anyone can sign a transaction
spending to a new commitment. Child-pays-for-parent could be used when
transaction fees are too high. Perhaps more interesting would be something
like lightning network payment channels, where only the commitment
transaction needs to be in the blockchain history; does that count as
key-holder decentralization at all?

Also, consider the following scenario. Suppose there's a bunch of
merge-mined sidechains that are mainnet BTC-pegged, and these sidechains
are accessible by the lightning network protocol (multi-chain payments).
Suppose also that on the different sidechains there are different
transaction fee trends because of various technical differences underlying
consensus or a different blockchain implementation (who knows). When
someone routes payments to one of those different sidechains, because UTXOs
could be cheaper over there due to different fee pressures, ... would that
count as key-holder decentralization? Some of this scenario is described
here, although not in more detail:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-September/010909.html

Previously there has been the suggestion to use BTC-pegged merge-mined
chains to handle excess transaction demand:
http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/
https://github.com/vbuterin/scalability_paper/blob/master/scalability.pdf
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-March/004797.html

I notice that in the Poon file there is a concern regarding "only 10 key
holders", but how does that scenario really work? I think the actual
scenario they mean to describe is "there's always a transaction backlog
where the fees are so high that lower fee transactions can never get
confirmations". So, more specifically, the scenario would have to be
"lightning network exists and is working, and no lightning node can ever
route enough different payments to commit to the blockchain under any
circumstance". How would that be possible? Wouldn't most participants
prefer the relatively instantaneous transactions of lightning, even if they
can afford extremely high fees? Seems like the settlements have all
necessary reason to actually happen, don't know what your concern is,
please send help.

I don't mean to put words in anyone's mouth, everything above is mostly
asking for clarification around definitions. Some of these questions are
repeats from:
http://gnusha.org/bitcoin-wizards/2015-11-08.log

Thank you.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
I believe in the end it's the usability of bitcoin that matters.

For instance, a goal could be that no user on the network should wait more
than an hour to get 3 confirmations on the blockchain so that they can
actually have useful Bitcoins.

We can debate all we want about lots of technical aspects, but if you can't
send money what's the point?

My humble proposal tried to take that into consideration, but I like way
way more what you propose with NG.

http://twitter.com/gubatron

On Fri, Nov 13, 2015 at 11:37 AM, Emin Gün Sirer <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thu, May 7, 2015 at 5:11 PM, Mike Hearn <mike@plan99.net> wrote:

Consensus is arrived when the people who are most active at the time
(active in contributing to discussions, code review, giving opinions etc.)
agreed to ACK. There are a regular staple of active contributors. Bitcoin
development is clearly a meritocracy. The more people participate and
contribute the more weight their opinions hold.


I am not sure that is fair, your PR was reverted because someone found a
huge exploit in your PR enough to invalidate all your arguments used to get
it merged in the first place.



I don't think it's as simple as that. Objections for the sake of
objections, or unsound technical objections are going to be seen for what
they are. This is a project with of some of the brightest people in the
world in this field. Sure people can be disruptive but their reputation
stand the test of time.

The consensus system might not be perfect, but it almost feels like you
want to declare a state of emergency and suspend all the normal review
process for this proposed hard fork.
-------------------------------------


I fully agree that core developers are not the only people who should have a say in this. But again, we’re not talking about merely forking some open source project - we’re talking about forking a ledger representing real assets that real people are holding…and I think it’s fair to say that the risk of permanent ledger forks far outweighs whatever benefits any change in the protocol might bring. And this would be true even if there were unanimous agreement that the change is good (which there clearly IS NOT in this case) but the deployment mechanism could still break things.

If anything we should attempt a hard fork with a less contentious change first, just to test deployability.


Again, let’s figure out a hard fork mechanism and test it with a far less contentious change first


For the record, I do not work for Blockstream. Neither do a bunch of other people who have published a number of concerns. Very few of the concerns I’ve seen from the technical community seem to be motivated primarily by profit motives.

It should also be pointed out that *not* making drastic changes is the default consensus policy…and the burden of justifying a change falls on those who want to make the change. Again, the risk of permanent ledger forks far outweighs whatever benefits protocol changes might bring.


Miners are NOT in direct competition with the lightning network and sidechains - these claims are patently false. I recommend you take a look at these ideas and understand them a little better before trying to make any such claims. Again, I do not work for Blockstream…and my agenda in this post is not to promote either of these ideas…but with all due respect, I do not think you properly understand them at all.


I don’t think the concern here is so much that some people want to increase block size. It’s the *way* in which this change is being pushed that is deeply problematic.


-------------------------------------
Peter Todd, as discussed on IRC, I'm not opposed to median time, which
has many of the properties nheight has, I'm just opposed to just using
nTime which is what all "hardfork proposals" I've seen so far
(including this one) do.

On Wed, Jul 22, 2015 at 12:05 AM, Ross Nicoll <jrn@jrn.me.uk> wrote:

No, the height is in the current block after bip34, no context required.
In any case, you already have the nHeight in most functions that would
require it (for example, main::ConnectBlock).
The median time actually needs a context (the last 10 headers), but
it's not hard to calculate and pass around either.
But simply using nTime is not a good idea. Leaving aside time zones,
einstein and all that it introduces edge cases and weird incentives
for no good reason.
If the goal is to make it "human-schedule-friendly", median time
should be good enough.
If we're going to make miners 95% confirm after the date/height, I
still prefer the height, but as said median time seems a reasonable
compromise.

Can we move the "height/medianTime/nTime" and "is it good to confirm
that the change is uncontroversial to miners by requiring 95% to
activate the consensus change, like we do with uncontroversial
softforks?" discussions to the thread with my bip draft (
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/008936.html
) on precisely this subject?
I have requested a bip number.
Let's please have an uncontroversial hardfork to set a precedent.
Hopefully that way we may decouple some parts of the blocksize
discussion.

-------------------------------------
Wouldn't block withhold be fixed by not letting miners in pools know which
block candidates are valid before the pool knows? (Note: I haven't read any
other proposals for how to fix it, this may already be known)

As an example, by having the pool use the unique per-miner nonces sent to
each miner for effective division of labor as a kind of seed / commitment
value, where one in X block candidates will be valid, where X is the
current ratio between partial PoW blocks sent as mining proofs and the full
difficulty?

The computational work of the pool remains low (checking this isn't harder
than the partial PoW validation already performed), they pool simply looks
at which commitment value from the pool that the miner used, looks up the
correct committed value and hashes that together with the partial PoW. If
it hits the target, the block is valid.
-------------------------------------
Mike wrote:

To be clear in case you are missing part of the mechanism.: it is
forward and backwards compatible meaning a 1MB address can receive
payments from an 8MB address (at reduced security if it has software
that doesnt understand it) and a 1MB address can pay an 8MB address by
paying to an OP_TRUE that has meaning to the extension block nodes.

A 1MB client wont even understand the difference between a 1MB and 8MB
out payment.  An 8MB client will understand and pay 1MB addresses in a
different way (moving the coin back to the 1MB chain).

So its opt-in and incrementally deployable.  Exchanges could encourage
their users to use wallets that support 8MB blocks, eg by charging a
fee for 1MB transactions.  If 1MB blocks experience significant fee
pressure, this will be persuasive.  Or they could chose not to and eat
the cost.  This is all normal market adoption of a new cheaper
technical option (in this case with a tradeoff of reduced
security/more centralisation for those opting in to it).


Extension blocks & lightning are unrelated things.

While I understand the need for being practical, there is IMO, amongst
engineering maxims something as far as being too pragmatic,
dangerously pragmatic even.  We cant do stuff in bitcoin that has bad
carry costs, nor throw out the baby with the bathwater.

The situation is just that we are facing a security vs volume tradeoff
and different people will have different requirements and comfort
zones.  If I am not misremembering, I think you've sided typically
with the huge block, big data center only end of the spectrum.  What I
am proposing empowers you to do experiments in that direction without
getting into a requirements conflict with people who value more
strongly the bitcoin properties arising from it being robustly
decentralised.

I am not sure personally where the blocksize discussion comes out - if
it stays as is for a year, in a wait and see, reduce spam, see
fee-pressure take effect as it has before, work on improving improve
decentralisation metrics, relay latency, and do a blocksize increment
to kick the can if-and-when it becomes necessary and in the mean-time
try to do something more long-term ambitious about scale rather than
volume.  Bitcoin without scale improvements probably wont get the
volume people would like.  So scale is more important than volume; and
security (decentralisation) is important too.  To the extreme analogy
we could fix scale tomorrow by throwing up a single high perf
database, but then we'd break the security properties arising from
decentralisation.  We should improve both within an approximately safe
envelope IMO.

Adam


-------------------------------------
Who is funding this?

Why not fund Core development?
On 30 Sep 2015 7:37 am, "Richard Olsen via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

Say you generate a child key using the path m/6'/4'/7'/99'/0/196, which 
is what your proposed path structure would be, and it results in the 
address 1DpY7PtPVURvjrGsdAjbZAZ7cL9GD8tc5w.

When the wallet notices a transaction in the blockchain that has 
1DpY7PtPVURvjrGsdAjbZAZ7cL9GD8tc5w as an output, it's going to have to 
lookup the address within its database to get the values 6/4/7/99/0/196, 
as there's no way to retrieve them from just the address.  So 
technically, you might as well just use m/account'/change/index if using 
hardened child keys, or m/change/index if not, as recommended, because 
the wallet will still function the exact same way.

Matt



On 06/20/2015 06:31 AM, Matt Smith wrote:



-------------------------------------
Great data points, but isn't this an argument for improving Electrum Server's database performance, not for holding Bitcoin back?

(Nice alias, by the way. Whimmy wham wham wozzle!)


On Thursday, 23 July 2015, at 5:56 pm, Slurms MacKenzie via bitcoin-dev wrote:

-------------------------------------
Summary
-------

Opt-In Full-RBF allows senders to opt-into full-RBF semantics for their
transactions in a way that allows receivers to detect if the sender has
done so. Existing "first-seen" mempool semantics are left unchanged for
transactions that do not opt-in.

At last week's IRC meeting(1) we decided to merge the opt-in Full-RBF
pull-req(2), pending code review and this post, so this feature will
likely make it into Bitcoin Core v0.12.0


Specification
-------------

A transaction is considered to have opted into full-RBF semantics if
nSequence < 0xFFFFFFFF-1 on at least one input. Nodes that respect the
opt-in will allow such opt-in transactions (and their descendents) to be
replaced in the mempool if they meet the economic replacement criteria.
Transactions in blocks are of course unaffected.

To detect if a transaction may be replaced check if it or any
unconfirmed ancestors have set nSequence < 0xFFFFFFFF-1 on any inputs.


Rational
--------

nSequence is used for opting in as it is the only "free-form" field
available for that purpose. Opt-in per output was proposed as well by
Luke-Jr, however the CTxOut data structure simply doesn't contain any
extra fields to use for that purpose. nSequence-based opt-in is also
compatible with the consensus-enforced transaction replacement semantics
in BIP68.

Allowing replacement if any input opts in vs. all inputs opting in is
chosen to ensure that transactions authored by multiple parties aren't
held up by the actions of a single party. Additionally, in the
multi-party scenario the value of any zeroconf guarantees are especially
dubious.

Replacement is allowed even if unconfirmed children did not opt-in to
ensure receivers can't maliciously prevent a replacement by spending the
funds. Additionally, any reasonable attempt at determining if a
transaction can be double-spent has to look at all unconfirmed parents
anyway.

Feedback from wallet authors indicates that first-seen-safe RBF isn't
very useful in practice due to the limitations inherent in FSS rules;
opt-in full-RBF doesn't preclude FSS-RBF from also being implemented.


Compatibility
-------------

Opt-in RBF transactions are currently mined by 100% of the hashing
power. Bitcoin Core has been producing transactions with non-maxint
nSequence since v0.11.0 to discourage fee sniping(3), and currently no
wallets are known that display such transactions yet do not display
opt-in RBF transactions.


Demonstrations
--------------

https://github.com/petertodd/replace-by-fee-tools#incremental-send-many


1) http://lists.linuxfoundation.org/pipermail/bitcoin-discuss/2015-November/000010.html

2) https://github.com/bitcoin/bitcoin/pull/6871

3) https://github.com/bitcoin/bitcoin/pull/2340

-- 
'peter'[:-1]@petertodd.org
00000000000000000f30567c63f8f4f079a8ecc2ab3d380bc7dc370e792b0a3a
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256



On 6 August 2015 10:21:54 GMT-04:00, Gavin Andresen via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

Incidentally, why is that competition good? What specific design goal is that competition achieving?

-----BEGIN PGP SIGNATURE-----

iQE9BAEBCAAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJVw9fn
AAoJEMCF8hzn9Lnc47AH/idUy2rGlUCBTTU/jDpNjMy5VGYYRawx50lrnGBufvIJ
8ZbFleI+gbnFCaJiaPF9ZN0mTjFWv7YcFzlwoPam11UfhEYI2Cl1aGha+R7g/18t
+1256i4Ykg0uEqrX9ITpYyzoBsVMaqsaOGBbJbUUtHoD1V1GCYBYi5JAl1msGjH/
2o+/Gh7gBB1Ll6SPtgeM1cCudRXA7PJr3WTjkLy8oGKY7lmVsPUfQ7h3OBJMTwa5
B+i1KTpSWdWyciWk0a3z7cxNfaajd7Pj3jZYoeCzKJdZja7lnB7FzUnaPE3y0wse
Bby6w48R4VeYsVhM+GolvRDVVSQN9XNfRSiRjuMW4Eg=
=wzhz
-----END PGP SIGNATURE-----


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 05/07/2015 03:49 AM, Peter Todd wrote:

..for some very strange definitions of "good".

That paper may present valid game theory, yet game theory has a
well-known limitation when it comes to predicting real world behavior
in that the predictions are only as good as the simplified model those
predictions are based on is accurate.

At the very least, we should wait to draw any conclusions from that
paper until it has been sanity checked by a praxeological review.
-----BEGIN PGP SIGNATURE-----

iQIcBAEBAgAGBQJVStYTAAoJECpf2nDq2eYjqzAQAJkLwVq3cJxaP5MirS6j+SkN
NuRIQS8EzJkvojZvHCSRz3xPZpl9Cx2T6/hsLjIfzvMuDHKsaOkkLlL0q95ekv4T
acfami64326DFAxiO0ptspPjCRipagmjSEwZGZwC/QZtTdnt+N9LsH0SFDP6hxbY
Kf11LRd11Ap4v/VnBg/zb4daZnVm0k0nfZxK4rG1zN14r5JEu6eiodUBZc6e4qih
LmopoddIwJS4MY1GoR2kCehAbJseZZyQQmHFEX1Vhc74ETGXWApfgF0tpo6ZMutd
OT0WGhCpj4yG1u5bRaiNnsOy9WcBTKzDOLZUVVh/GhUGHWUulZu8ujYrX7Q6GR5S
VPvOOL6Ts/RGEAE1UWKzHfPjrLZAHKgLAzBjm6o1ZXdBcnV+FsThNvd7fxHvaJsO
pWGSu8qDmN/wH657Tphbthb4T/awnuf4rO6oBP+OGu+ydPIlIlt6rM2E4Bq366yy
CJbzSR3x/P7fRmT2bbSg4rxTDyLFJpNIWOcNaMRBeO69OdNZxlranvFvEl/6FfqK
GO/LPQiYCe/+yhXgUJzzlYpayPiPFWCg0FxwQ+xl1josTsrfPE4BUivkZvIlqOIY
LX1fDHt/IIUNp8OUkY2eERxeB//dlY55nP7VGUEJLNnBkXuoBd70lMtGXxtgvw2M
Wy5VER9CiEOUMMwzWi3Q
=8mit
-----END PGP SIGNATURE-----
-------------------------------------
On Thu, Oct 1, 2015 at 10:05 AM, Marcel Jamin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


We do: a.b.c, the next major version is, 0.12.0, and maintenance releases
are 0.12.1 etc. Release candidates are 0.12.0-rc1 for example.
-------------------------------------
Here are some proposals regarding the minimum block size questions, as well
as other related scalability issues.

Dynamic block size limit controller (maaku)
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07599.html
https://www.reddit.com/r/Bitcoin/comments/35c47x/a_proposal_to_expand_the_block_size/

Re: dynamic block size limit controller (gmaxwell)
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07620.html

Various other gmaxwell-relayed ideas
http://www.reddit.com/r/Bitcoin/comments/37pv74/gavin_andresen_moves_ahead_with_push_for_bigger/crp2735

Increasing the max block size using a soft-fork (Tier Nolan)
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07927.html

Elastic block cap with rollover penalties (Meni Rosenfield)
https://bitcointalk.org/index.php?topic=1078521
worked example
https://bitcointalk.org/index.php?topic=1078521.msg11557115#msg11557115
section 6.2.3 of https://cryptonote.org/whitepaper.pdf
rollover transaction fees https://bitcointalk.org/index.php?topic=80387.0

Variable mining effort (gmaxwell)
http://sourceforge.net/p/bitcoin/mailman/message/34100485/

BIP100 Soft-fork limit of 2 MB (jgarzik)
http://gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf

Transaction fee targeting
https://bitcointalk.org/index.php?topic=176684.msg9416723#msg9416723

Difficulty target scaling
https://www.reddit.com/r/Bitcoin/comments/38937n/idea_make_the_difficulty_target_scale_with_block/

Annual 50% max block size increase
https://www.reddit.com/r/Bitcoin/comments/351dft/what_about_gavins_2014_proposal_of_having_block/

Various algorithmic adjustment proposals
https://bitcointalk.org/index.php?topic=1865.0
https://www.reddit.com/r/Bitcoin/comments/1owbpn/is_there_a_consensus_on_the_blocksize_limit_issue/ccwd7xh
https://www.reddit.com/r/Bitcoin/comments/35azxk/screw_the_hard_limit_lets_change_the_block_size/
https://www.reddit.com/r/Bitcoin/comments/359y0i/quick_question_about_the_block_size_limit_issue/
http://www.reddit.com/r/Bitcoin/comments/385xqj/what_if_block_size_limits_were_set_to_increase/
http://www.age-of-bitcoin.com/dynamic-block-size-cap-scaling/
(against)
http://garzikrants.blogspot.com/2013/02/bitcoin-block-size-thoughts.html

Average over last 144 blocks
http://www.reddit.com/r/Bitcoin/comments/38fmra/max_block_size_2_average_size_of_last_144_blocks/

Extension blocks (Adam Back) (why would he burn this idea for something so
trivial?)
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07937.html
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg08005.html
http://www.reddit.com/r/Bitcoin/comments/39kqzs/how_about_a_softfork_optin_blocksize_increase/
http://www.reddit.com/r/Bitcoin/comments/39hgzc/blockstream_cofounder_president_adam_back_phd_on/cs3tgss

Voting by paying to an address (note: vote censorship makes this
impractical, among other reasons)
http://www.reddit.com/r/Bitcoin/comments/3863vw/a_brandnew_idea_for_resolving_the_blocksize_debate/
http://www.reddit.com/r/Bitcoin/comments/1g0ywj/proposal_we_should_vote_on_the_blocksize_limit/
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg02325.html

Vote by paying fees
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg08164.html
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg02323.html

Double the max block size at each block reward halving
https://www.reddit.com/r/Bitcoin/comments/359jdc/just_double_the_max_blocksize_on_every_block/

Reducing the block rate instead of increasing the maximum block size
(Sergio Lerner)
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07663.html
https://www.reddit.com/r/Bitcoin/comments/35kpgk/sergio_lerner_on_bitcoindevelopment_reducing_the/

Decrease block interval
https://www.reddit.com/r/Bitcoin/comments/2vefmp/please_eli5_besides_increasing_the_block_size_why/
https://www.reddit.com/r/Bitcoin/comments/35hpkt/please_remind_me_once_again_why_we_cant_decrease/

Increase default soft block size limit in Bitcoin Core
http://www.reddit.com/r/Bitcoin/comments/38i6qr/why_not_increase_the_default_block_size_limit/
https://github.com/bitcoin/bitcoin/pull/6231

Consider the size of the utxo set when determining max block size (note
that utxo depth cannot have consensus)
https://bitcointalk.org/index.php?topic=153401.20

Reduce and decrease the max block size
https://www.reddit.com/r/Bitcoin/comments/381ygv/who_is_in_favour_of_reducing_the_blocksize_limit/
https://www.reddit.com/r/Bitcoin/comments/2vedt4/better_we_make_block_size_50kb_and_test/

Change the value of MAX_BLOCK_SIZE in Bitcoin Core
https://bitcointalk.org/index.php?topic=140233.0

Problems with floating block size limits (petertodd)
https://bitcointalk.org/index.php?topic=144895.0

Develop other ways to support high transaction volumes (gavinandresen)
https://bitcointalk.org/index.php?topic=96097.msg1059475#msg1059475

Simplified payment verification
https://bitcoin.org/bitcoin.pdf

Lightning network
https://lightning.network/lightning-network-paper-DRAFT-0.5.pdf

GHOST
https://eprint.iacr.org/2013/881.pdf

Payment channels
https://bitcoinj.github.io/working-with-micropayments

Tree chains
http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg04388.html
https://github.com/petertodd/tree-chains-paper

fedpeg + SPV
https://github.com/ElementsProject/elements/blob/7848ae0eed5506fb32872b6d74a12fd781aa3024/contrib/fedpeg/rotating_consensus.py

Known missing:
- old bitcoin-development proposals
- old bitcointalk proposals
- anything unique from IRC

On a related note, the other day I found that reading all of the -wizards
logs regarding sidechains only takes 2 hours. So... that's something. YMMV.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
On 05/25/2015 11:05 PM, Peter Todd wrote:

It should work, I'm testing this regularly. Can you report an issue
through the app and attach your log when this happens again?




-------------------------------------
I don't see why existing software could create a 40-byte OP_RETURN but not
larger? The limitation comes from a relay policy in full nodes, not a
limitation is wallet software... and PoPs are not relayed on the network.

Regarding sharing, I think you're talking about a different use case. Say
you want to pay for 1-week valid entrance to some venue. I thought the
purpose of the PoP was to be sure that only the person who paid for it, and
not anyone else can use it during that week.

My argument against that is that the original payer can also hand the
private keys in his wallet to someone else, who would then become able to
create PoPs for the service. He does not lose anything by this, assuming
the address is not reused.

So, using a token does not change anything, except it can be provided to
the payer - instead of relying on creating an implicit identity based on
who seems to have held particular private keys in the past.
On Jun 16, 2015 9:41 PM, "Kalle Rosenbaum" <kalle@rosenbaum.se> wrote:

-------------------------------------
Assigning 5% of block space based on bitcoin-days destroyed (BDD) and the other 95% based on fees seems like a rather awkward approach to me. For one thing, it means two code paths in pretty much every procedure dealing with a constrained resource (e.g. mempool, CNB). This makes code harder two write, harder to maintain, and slower to execute. As a result, some people have proposed eliminating BDD priority altogether. I have another idea.

We can create and maintain a conversion rate between BDD and fees to create a composite priority metric. Then we just do compPrio = BDD * conversionRate + txFee.

How do we calculate conversionRate? We want the following equation to be true:

sum(fees) = sum(BDD) * conversionRate * BDDweight

So we sum up the mempool fees, and we sum up the mempool BDD. We get a policy statement from the command line for a relative weight of BDD vs fees (default 0.05), and then conversionRate = (summedFees / summedBDD) * BDDWeight.

As an optimization, rather than scanning over the whole mempool to calculate this, we can just store the sum and add or subtract from it each time a tx enters or leaves the mempool. In order to minimize drift (the BDD for a transaction changes over time), we recalculate the whole thing each time a new block is found.
-------------------------------------
On Monday, August 10, 2015 6:32:10 PM Ross Nicoll wrote:

No, accidental. I'll re-CC it on this email.


Regtest isn't really a network at all, just a testing mode of Bitcoin Core...


Is that a bad thing?


Sorry, I meant to stress that BIPs are for *Bitcoin* improvements 
specifically. Things which only improve altcoins, while a perfectly fine thing 
to standardise, are outside the scope of what belongs in a BIP.

Perhaps, however, this could be made to kill 2 birds with one stone, by 
ensuring it addresses the need for payments made of bitcoins on a sidechain?
For this, a merchant who wants a sidechain payment would presumably be able to 
provide a script from the main chain already, but an extension allowing 
payment directly on the sidechain (at the customer's choice) avoids the need 
to round-trip it...

Luke

-------------------------------------
Den 1 feb 2015 00:37 skrev "Natanael" <natanael.l@gmail.com>:

tries. log(50M)/log(2) = 25.6 bits of entropy.

Oops. Used the wrong number in the entropy calculation. Add one bit, the
division by 2 wasn't supposed to be used in the entropy calculation.
Doesn't change the equation much, though.
-------------------------------------
Greetings mailing list.

Not sure that this content is 100% appropriate here, but Peter Todd
invited me to post this for archival purposes.  The original thread
has been removed from the search results, but is still up here:
http://www.reddit.com/r/Bitcoin/comments/2z9k5p/factom_announces_launch_date_for_software_token/


I have added more thoughts too.



-----BEGIN REDDIT MESSAGE-----


That is only part of the story.  Factom is attempting to make a
publishing platform which is simultaneously censorship and spam
resistant.  This is what makes Bitcoin magical, and what Factom is
trying to accomplish.  Last Summer, I went down the road that you are
going down and kept coming up with a system that was susceptible to
either one or the other.  I gave the entities you described the
glorious name **Compaction Service Providers** (CSP) and even wrote
about it [here](https://github.com/FactomProject/FactomDocs/commit/2791c51917e3ecc65dc52bfc434ca6dec0b3a1fd)
back when we were Notarychains.  With free entry of CSPs, censorship
would be limited, but the entire system would get spammed quickly, and
there would not be a good way to accurately locate the data you
needed.  Without free entry, once a specific CSP (or proofchain
packager) was selected by a network, the CSP could selectively censor
within that network.  Lock in effects would be strong, so switching
the entire network over to a new CSP would be expensive.

The CSPs (and Proofchain packagers) could "exclude, delay, or reorder
the customer's timestamped entries".  This is fine as long as the CSP
doesn't have an incentive to do these things.

You claim that proofchains packagers will be the very business that
issues a stock.  Since stockholders are trusting the company to return
dividends in the first place, the trust can be expanded to managing
all the stock trades too.  In my mind, the company who issues the
stock still may game the system they control for their personal
benefit.  What is needed is a scalable disimpassioned 3rd party.
Something of the scale where if the company president calls up and
says "Delay these disfavored parties" that the packagers tell him his
company isn't big enough to push them around.

I think **Factom sits in a sweet spot between** your proposed
**centralized** solution **and** Bitcoin's anonymous membership
authority set (**Proof of Work**).  The Federated servers must
cooperate to move Factom forward, but like Bitcoin, require a majority
to effectively censor a transaction.  It is a whole lot easier to
censor with Proofchains.




Yes, to this point, Factom being forked is way worse than seizing up.
The Federated servers are constantly watching their peers and keeping
them honest.  Since we have a defined majority instead of an anonymous
membership set, if one Federated server goes rouge, the honest
majority will all place the correct anchor.  You will see 1 anchor
where someone is maybe trying to defraud you, and 31 anchors that have
the correct data.



I had considered merge mining, but your [arguments against
it](https://letstalkbitcoin.com/ltb104-tree-chains-with-peter-todd/)
in reference to sidechains is compelling.  Without a majority of
miners, then the system is vulnerable to consensus attack.  We gain
the non-reversability by placing anchors in bitcoin without needing to
recruit mining pools.

We could have gone to proof of stake, but then someone who funded it
early on would have a disproportionate say in how the system was run.
Since we have the two step payment process, we can leverage that to
determine who is actively participating in the system, and let them
determine who sets policy.



We are making the system so that it is visible if someone is trying to
do this, and the other members fight against it.


But what you are proposing is a single trusted third party.



I disagree.  Bitcoin is optimized for proving a negative over the
domain of Bitcoin value transactions.  Lets take a closed system like
Counterparty's current implementation.  To prove the negative (that an
asset has not been sent to someone else first) you need to parse the
entire Bitcoin blockchain looking for Counterparty transactions.  One
of two things will happen soon.  The 1MB limit will be raised, or not.

* Raised blocksize.  In order to see if your Counterparty asset was
doublespent, you will need to parse through many terabytes of Bitcoin
transactions to find the few MiB of Counterparty transactions.  You
would also need to wade through all the other embedded protocols like
Omni, ProofOfExistence.com, and all the others in your search for
Counterparty transactions.  Factom is setup so that interpreted
protocols like Counterparty do not need to wade through all other
protocol's data.

* Block limit stays.  Each Bitcoin transaction becomes expensive.
Each transaction might rise to $5, $10, $15, who knows.  Distributions
to asset holders would cost hundreds or thousands per dividend.




You are making economic statements with technical arguments to back
them up.  I think the economics and technicals are not as tightly
bound as you imply.


TLDR:
Factom is trying to be a censorship and spam resistant disimpassioned
3rd party, like Bitcoin.

-----END REDDIT MESSAGE-----






I like to think in audited vs interpreted protocols.  Think Bitcoin vs
Counterparty.  Bitcoin won't let an invalid transaction into the
system.  Counterparty filters out invalid transactions after the fact.

Proofchains are good for audited protocols where there is a
predetermined auditor.  There is a gatekeeper who only adds in valid
transactions.

Factom is good for interpreted protocols.  A user's software will
filter out transactions which do not pass a ruleset that they agreed
to.

Both are immutable and serve as proof of publication (POP).  Sure the
POP in Factom is more complicated, but the publishing powers are
shared.

On the bitcoin wizards
[IRC](https://download.wpsoftware.net/bitcoin/wizards/2015-03-16.html),
phantomcircuit seems to have gotten close, the conversation resolved
with Alice burning her house down.

There are applications where proofchains will work just fine.  If you
are securing your own blockchian for your own data, proofchains will
work.  You are not worried about censoring yourself.

If two rivalrous institutions are sharing a blockchain, then giving
one of them exclusive power of making the blockchain is undesirable
for the non-authoritative institution.  No need to discuss that
arrangement anymore.

With threshold multisig, now multiple institutions would need to
cooperate amongst each other to create a communal blockchain. In this
example, a majority of keyholders can directly censor the minority.
The minority might have recourse like in Szabo's property club blog
post to fork the chain and start an alternate system, but if the
minority is too small, then the network will not jump to the fairer
fork.

OK, lets move authority to an industry group.  For something like
property records, it is shown to work in a centralized model.  Making
that model immutable with proofchains will certainly work.  Property
records are highly gated as of now at the county seat.  Transitioning
the county property database to a proofchains based POP will work.
They are audited records, and the auditor is predetermined.  They
already have censorship powers, and would in Factom too.  The only
difference would be that in proofchains an invalid record would not
exist, and in Factom, an invalid record would exist, but not be signed
by the county.

As the individual players in a system become more numerous and less
powerful, it becomes harder to have a disimpassioned industry group.
This is similar to politics where we see dispersed costs and
concentrated benefits.

Lets jump to the end and try to imagine how Counterparty would run on
proofchains.  Who would be the one to package the transactions?  The
counterpart devs can censor now, by updating the software to blacklist
certain addresses.  They are already the predetermined auditor.  The
Counterparty Foundation could package the transactions in a
proofchain.  The difference to me lies in how easy it is to censor.
It feels harder to censor by baking specific blacklists into the
software than keeping a blacklisted party from ever publishing at all.
One is very visible and the latter maybe not as much.  (Something like
proofchains is how I initially imagined Mastercoin and Counterparty
would work, since it seems silly to have every transaction be a BTC
transaction too.  I underestimated their desire for censorship
resistance.)


In the end it comes down to the data being published, and how/when it
is audited.  Proofchains prefilters data and couples the auditor with
the packager.  Factom allows the users to choose how they audit data
independent of the packager.  How much power do you want to invest in
one entity?  Factom allows splitting of those powers.

-Brian Deery
-------------------------------------
On Thu, Dec 10, 2015 at 6:47 AM, jl2012--- via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

I should have also commented on this: the block can indicate how many
sum criteria there are; and then additional ones could be soft-forked
in. Haven't tried implementing it yet, but there you go. :)

-------------------------------------
On 12.08.15 11.45, Jorge Timón via bitcoin-dev wrote:

I don't think rising fees is the issue.

Imagine that the government is worried because air lines are selling
tickets cheaply and may run themselves out of business. So their
solution is passing a new law that says only one commercial air plane is
allowed to be in the air at any given time.

This should help a ticket market to develop and prevent air lines from
giving away almost free tickets. In this way the government can protect
the air lines from themselves.

I would not classify all issues that would come out of this as
"potential indirect consequences of rising ticket prices."

It would just make air travel unusable.

That's the problem we may face in the short term.

It would be unwise to go all-in on a solution that doesn't exist yet,
which may or may not arrive in time, and may or may not do the job that
is needed. We need to use the solution we already have so that we can
get by in the short term.

I don't think mining pools will immediately make blocks as big as
possible if the hard limit is raised. Remember that mining pools had to
be coaxed into increasing their block size. Mining pools were making
small blocks to reduce the rate of orphaned blocks. Block propagation is
faster today, but this issue still exists. You need a lot of transaction
fees to make up for the danger of losing 25 BTC. Many pools don't even
pay out transaction fee income to their miners.

-- 
Regards,
Geir H. Hansen, Bitminter mining pool


-------------------------------------
I like the bitcoin days destroyed idea.

I like lots of the ideas that have been presented here, on the bitcointalk
forums, etc etc etc.

It is easy to make a proposal, it is hard to wade through all of the
proposals. I'm going to balance that equation by completely ignoring any
proposal that isn't accompanied by code that implements the proposal (with
appropriate tests).

However, I'm not the bottleneck-- you need to get the attention of the
other committers and convince THEM:

a) something should be done "now-ish"
b) your idea is good

We are stuck on (a) right now, I think.


On Fri, May 8, 2015 at 8:32 AM, Joel Joonatan Kaartinen <
joel.kaartinen@gmail.com> wrote:



-- 
--
Gavin Andresen
-------------------------------------

There is another concern regarding "flexcap" that was not discussed.

A change to difficulty in response to anything BUT observed block
production rate unavoidably changes the money supply schedule, unless
you also change the reward, and in that case you've still changed the
timing even if not the average rate.


On 8/14/2015 8:14 AM, Jakob Rnnbck via bitcoin-dev wrote:


-------------------------------------
Indeed, for an occasional reader perspective, nightmarish. List signature
also a bit annoying but it's possible to live with it.

On Tue, Sep 1, 2015 at 8:27 AM, Warren Togami Jr. via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
miner who finds the block a bonus.
...
of the game theory math?

Yes, Section 8.D. in Ittay's paper discusses this countermeasure, as well
as a few others:
    https://www.cs.cornell.edu/~ie53/publications/btcPoolsSP15.pdf

- egs
-------------------------------------
Interesting:
http://kke5edzhy54hiee5.onion/(requires Tor Browser)
or without Tor Browser:http://kke5edzhy54hiee5.onion.to/
-------------------------------------
Hi Gavin,

On Sun, Aug 09, 2015 at 06:44:08PM -0400, Gavin Andresen via bitcoin-dev wrote:

I'm writing a (hopefully more accessible) summary on Lightning
currently. It might not go into too much detail with infrastructure, but
is a bit more UX focused.


It's a bit of a tangent, but I see it as necessary that all Lightning
services/wallets support on-chain payments for a multitude of reasons,
including usability and long-term security/fungibility. For that reason,
the UX flow for payment after channels are established should not be
significantly different than Payment Protocol based payment flows (with
the only exception being a possible additional fee dialog box/alert when
the fees will be higher than expected/on-chain).

-- 
Joseph Poon

-------------------------------------
Although I'd say your writing style is a bit unfriendly and hard to 
identify/unclear at the specifics of some of your complaints, I do agree 
with many of your points.

I really have no idea what format the .txt.gz files are in that are on 
the current archive. If you unzip them, they are not text tiles.

gmane was able to change the subscription of the previous list to the 
current list. So, anyone who had a newsreader setup, it should be 
working again. There is some gap in their archive at the beginning of 
the archive, as well as last week while the list was transitioned. They 
requested a link to a single mbox file archive that they can use to 
restore the list archive in it's entirety. Anyone know how we can get 
this? The reference to another mailing list below 
(https://cpunks.org/pipermail/cypherpunks/) has such a link on its 
archive page. It looks like the same interface so maybe that feature can 
just be turned on? I'm not sure if it gzips the mbox file on the fly 
whenever an http request is made or if it has some schedule and may be 
missing a few of the most recent messages?


Also, regarding the footer, if it can't be changed on a per user basis, 
I'd vote to get rid of it. Anyone with a reasonably powered brain can go 
to lists.linuxfoundation.org and find a way to get to the bitcoin-dev 
mailing list page. The information provided in the footer is also 
included in the email message headers as well. It's also included in the 
welcome e-mail when someone subscribes to the list.




Andy Schroder

On 06/27/2015 06:21 PM, grarpamp wrote:


-------------------------------------
I should add that in the interest of peace and goodwill, I extend an offer to both Mike and Gavin to make their grievances heard…but only on the condition that we make a good effort to avoid misrepresentation and misreading of the other side’s intentions.


-------------------------------------
Jorge,

separating script engine into libconsensus was very helpful, since wrapped the piece of consensus
that would least likely to be captured exactly with an implementation from scratch. Thank you for your
effort there.  Bits of Proof now uses its own or alternatively libconsensus for full validation.

I am sceptical however that a “full” consensus lib extracted from satoshi’s code is worth trying.
Not because it was impossible, but because the result would not be higher quality, if measured on agreement
with satoshi, than other re-implementations. It would actually be lower quality because of the antique tool set.

The rules outside script engine are simpler, therefore much easier to capture exactly. They are however
scattered around in the spaghetti and are often just a single if statement, also repeated elsewhere.

You would either have to very extensively refactor the code, that unlikely goes through as a PR, or
do what me and others did. Read satoshi code and rewrite the same. You have
a slight advantage of copy-paste small fragments, but I doubt the consensus relevant advantage of that.

Tamas Blummer


-------------------------------------
Yup I've been looking over twister thoroughly this week, it's very close to what I'm thinking and I'm sure the developer of that could easily do the project I'm doing. However he decided to make an alt coin and didn't use the real blockchain, which makes his network weak. It's a waste to not take advantage of the immense bitcoin computing power for this. I'm only planning on storing hashes in the blockchain though to prevent data bloat, and the parallel program which stores all the data will be a lot like twister. Additionally I'm going to take it beyond a messaging system, will be able to literally upload anything and everything to this system. Theres no limit to what this de-centralized system can do if designed correctly. Crypto graffiti has given a taste of that by uploading jpeg and png to the blockchain. 

Will be working on this 24/7 until the first release, will definitely tell you and others when the code is ready to be looked over. Fortunately I am employed by bitcoin so I can focus on this 

-----Original Message-----
From: Michael Wozniak <mwozniak@itbit.com>
To: hurricanewarn1 <hurricanewarn1@aol.com>
Sent: Mon, Aug 31, 2015 09:15 AM
Subject: Re: [bitcoin-dev] Censorship



<div id="AOLMsgPart_2_3e2f989c-2ac0-4675-a613-58f047c46a67">
<div class="aolReplacedBody">
 <div dir="ltr">
Have you seen this: 
  <a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="http://twister.net.co/">http://twister.net.co/</a>
  <div>
   

  
  

It sounds very similar to what you're trying to do.  Also, I'd be willing to help out in the form of running some alpha/beta software as part of the network once you get something working.  Depending what languages you're writing in, I might help with code.
  
 </div>
 <div class="aolmail_gmail_extra">
  

  <div class="aolmail_gmail_quote">
On Mon, Aug 31, 2015 at 8:06 AM, Zach G via bitcoin-dev 
   <span dir="ltr"><<a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:bitcoin-dev@lists.linuxfoundation.org">bitcoin-dev@lists.linuxfoundation.org</a>></span> wrote:
   

   <blockquote class="aolmail_gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
    <font color="black" size="2" face="arial"><font face="arial" color="black" size="2"><font size="2">The de-centralized forum directly integrates the Bitcoin blockchain with every post. I can see how you misunderstood since I wasn't specific in my first email. I'm surprised no one has done this yet, people have done things very similar but never took the leap to integrate the actual Bitcoin blockchain. Basically <a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="http://cryptograffiti.info/">http://cryptograffiti.info/</a> but with a parallel program that stores most of the data in a hash reference table, and shares that data via a torrent-esque system.
 
  The Bitcoin of thoughts.</font></font> 
     
 
      
 
      
     
 
      
 
      
     <div style="font-family:arial,helvetica;font-size:10pt;color:black">
      <span>-----Original Message-----
 From: Natanael <<a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:natanael.l@gmail.com">natanael.l@gmail.com</a>>
 To: NxtChg <<a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:nxtchg@hush.com">nxtchg@hush.com</a>>
 Cc: bitcoin-dev <<a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:bitcoin-dev@lists.linuxfoundation.org">bitcoin-dev@lists.linuxfoundation.org</a>>; hurricanewarn1 <<a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:hurricanewarn1@aol.com">hurricanewarn1@aol.com</a>>
 Sent: Mon, Aug 31, 2015 7:46 am
 Subject: Re: [bitcoin-dev] Censorship
 
 </span>
      

       <div class="aolmail_h5">
        <div> 
         <div> 
          <div dir="ltr">
One last comment here on this topic;
           
          <div dir="ltr">
For anybody who wants to discuss decentralized communication mechanisms in general, they can come to 
           <a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="http://www.reddit.com/r/p2pcomms">www.reddit.com/r/p2pcomms</a> (up until these decentralized forums have become stable and common).
          </div> 
          <div dir="ltr">
I've seen quite a few more of these projects lately, I want to make a list of them and would definitely like to contribute to making them not just usable, but good enough to gain popularity on their own. 
          </div> 
          <div dir="ltr">
(And in case you wonder about my approach to moderation: let every user pick which moderators / filters / servers he trusts, and let them share their subscription preferences in place of sharing links to centralized forums.) 
          </div> 
          <div dir="ltr">
- Sent from my phone
          </div> 
          <div class="aolmail_gmail_quote">
 Den 31 aug 2015 10:45 skrev "NxtChg via bitcoin-dev" < 
           <a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:bitcoin-dev@lists.linuxfoundation.org">bitcoin-dev@lists.linuxfoundation.org</a>>: 
           
 
           <blockquote class="aolmail_gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"> 
            
            
 
            
 Zander is working on the same thing: 
            <a rel="noreferrer" target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="https://www.reddit.com/r/AetheralResearch/">https://www.reddit.com/r/AetheralResearch/</a> 
            
 
            
 But it's actually quite difficult to make it truly censorship-resistant: both in solving the theymos factor and spam/abuse/overloading as an attack. 
            
 
            
 
            
            
            
 
            
 We have /r/bitcoinxt and so far it has been great. But we also need a regular forum. 
            
 
            
 Roger Ver controls 
            <a rel="noreferrer" target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="http://bitcoin.com">bitcoin.com</a>, as I understand? 
            <a rel="noreferrer" target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="https://bitcoin.com/forum/">https://bitcoin.com/forum/</a> would be nice. 
            
 
            
 And it must be a real community, not "say whatever you want because free speech". We've seen how that turned out to be. 
            
 
            
 Something like 
            <a rel="noreferrer" target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="http://battle.net">battle.net</a> or Steam forums: heavily moderated, not for opinions, but for spam/noise/insults. 
            
 
            
 Again, this needs leadership. Anyone can install a forum software, what is needed is an "official seal of approval" and regular presence of top XT people there. 
            
 
            
 And a will to setup proper moderation. Then people will move. 
            
 
            
 _______________________________________________ 
            
 bitcoin-dev mailing list 
            
 
            <a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:bitcoin-dev@lists.linuxfoundation.org">bitcoin-dev@lists.linuxfoundation.org</a> 
            
 
            <a rel="noreferrer" target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a> 
            
 
           </blockquote> 
          </div> 
         </div> 
        </div> 
       </div>
      </div>
     </div> </font>
    
_______________________________________________
    
 bitcoin-dev mailing list
    
 
    <a target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="mailto:bitcoin-dev@lists.linuxfoundation.org">bitcoin-dev@lists.linuxfoundation.org</a>
    
 
    <a rel="noreferrer" target="_blank" removedlink__58ec13b3-aba4-4e37-b10b-87cbd851ae2f__href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a>
    
 
    

   </blockquote>
  </div>
  

 </div> 
</div>
</div>
</div>
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Sorry, but I think you need to re-read my first message. What you've written below has nothing to do with what I actually said re: how you're BIP102 and associated pull-req doesn't measure miner consensus.


On 22 July 2015 13:43:19 GMT-04:00, Jeff Garzik <jgarzik@gmail.com> wrote:
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCAAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJVsBl2
AAoJEMCF8hzn9Lnc47AIAICM9pA+Jc6rkJ14U0vYqzhwTHmxuaNTXodmI1z88OKM
zCCJQHNw/Xhy339/ZGFeUuVS/Csw275dtzZutLoZamnGnQLh9LllxYFzN8eGJkCL
Ecfo0JcyhduwUihgDfzgE++z5/Q0z5sIo+pZBNipqXW1+N0P/GAvYlHqeb9r0uXG
ccJghZUTwqzm6aySfvXVveTmp0AtjVko1jP1sTxF2pI/RIqBdMY4wEsZvmEhX7Tk
g2iRiPWiEIYR1qETm6e5aQ/tj8W73932s15ozIM35nD5QId5qotQHTVttLAruQvl
2Z35F79TIYDvYtnnRNWIsOyiwreH/y5c0kSUIgrjASA=
=+jTv
-----END PGP SIGNATURE-----


-------------------------------------

No, I think it's less efficient (for the client).

Quick sums:  blocks with 1500 transactions in them are common today. But
Bitcoin is growing. Let's imagine a system 10x larger than today. Doesn't
seem implausible to reach that in the next 5-10 years, so 15,000
transactions. Each transaction has multiple elements we might want to match
(addresses, keys, etc).

Let's say the average tx contains 5 unique keys/elements. That's the base
case of {1 input, 2 outputs} without address reuse, plus fudged up a bit
for multi-sends then down a bit again for address reuse.

15,000*5=75,000 unique elements per block. With an FP rate of 0.1% we get:

http://hur.st/bloomfilter?n=75000&p=0.001

131.63KB per block extra overhead.

144 blocks in a day, so that's 18mb of data per day's worth of sync to pull
down over the network. If you don't start your wallet for a week that's 126
megabytes of data just to get started.

Affordable, yes (in the west). Fast enough to be competitive? Doubtful. I
don't believe that even in five years mobiles will be pulling down and
processing that much data within a few seconds, not even in developed
countries.

But like I said, I don't see why it matters. Anyone who is watching the
wire close to you learns which transactions are yours, still, so it doesn't
stop intelligence agencies. Anyone who is running a node learns which
transactions in the requested block were yours and thus can follow the tx
chain to learn which other transactions might be yours too, no different to
today. If you connect to a single node and say "give me the transactions
sending money to key A in block N", it doesn't matter if you then don't
request block N+6 from the same peer - they know you will request it
eventually anyway, just by virtue of the fact that one of the transactions
they gave you was spent in that block.
-------------------------------------
My understanding, which is very likely wrong in one way or another, is
transaction size and block size are two slightly different things but
perhaps it's so negligible that block size is a fine stand-in for total
transaction throughput.

Potentially Doubling the block size everyday is frankly imprudent. The
logarithmic increases in difficulty, which were often closer to 10% or 20%
every 2016 blocks was and is plenty fast, potentially changing blocksize by
twice daily is the mentality I would expect from a startup with the move
fast break things motto.

Infrastructure takes time, not everyone wants to run a node on a virtual
amazon instance, provisioning additional hard drive and bandwidth can't
happen overnight and trying to plan when block size from one week to the
next is a total mystery would be extremely difficult.

Anyone who has spent time examining the mining difficulty increases and
trajectory knows future planning is very very hard, allowing block size to
double daily would make it impossible.

Perhaps a middle way would be 300%  increase every 2016 blocks, that will
scale to 20mbs within a  month or two

The problem is logarithmic increases seem slow until they seem fast. If the
network begins to grow and block size hits 20, then the next day 40, 80...
Small nodes could get swamped within a week or less.

As for your point about Christmas, Bitcoin is a global network, Christmas,
while widely celebrated, isn't the only holiday, and planning around
American buying habits seems short sighted and no different from developers
trying to choose what the right fee pressure is.

On May 28, 2015 1:22 PM, "Gavin Andresen" <gavinandresen@gmail.com> wrote:
wrote:
questions though.
scale by transactions confirmed instead? Anyone can write and relay a
transaction, and those are what we want to scale for, why not measure it
directly?
block...
a reasonable time period for planning on changes. Two weeks is plenty fast,
especially at a 50% rate increase, in a few months the block size could be
dramatically larger.
networks around Christmas?
block size will be dipping up and down. Also if something breaks trying to
fix it in a day seems problematic. The hard fork database size difference
error comes to mind. Finally daily 50% increases could quickly crowd out
smaller nodes if changes happen too quickly to adapt for.
there are fee-paying transactions around to pay them. What scenario are you
imagining where transaction volume increases by 50% a day for a sustained
period of time?
-------------------------------------
Hello,

I haven't heard anything about Peter Todd's tree chain idea 
(https://letstalkbitcoin.com/ltb104-tree-chains-with-peter-todd/) in a 
while. Is there some reason it has dropped from discussion? Was there 
some major flaw discovered? It seemed like it would be an additional 
technology actively discussed as an alternative to raising the block size.




-- 
Andy Schroder


-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

FWIW, the 6 confirmations figure came from a modest estimate of a miner
with 10% of the hash rate, such that there is < 0.1% probability of the
transaction being undone.

I wonder at times if this figure should fluctuate with the hashrate of
the largest player. Presently, AntMiner has 20% of the hashrate,
requiring 11 blocks to give you the same certainty. And previously when
GHash.io had 45%, the number of blocks to wait would be 340 - over two days!

With this in mind, I would be wary about publishing these numbers as
they are prone to change.

On 25/07/15 03:18, gb via bitcoin-dev wrote:

- -- 
My PGP key can be found here <https://thomaskerin.io/me.pub.asc>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCgAGBQJVs6WBAAoJEAiDZR291eTlmA4P/1uARaRISbq6ZN9gSy+Tsq+N
aooU/irB06IdTnOrxqW/iAS2M2SxqTq5/M6PVMK6LAefRuAAYE6KeDQb5/n2QWIM
vBgVeDPBVkq+KHOJlaswO962kl/Mr9TC9xb9hbfB9IdQACLbSwfyQ+cYNY3RRnvl
Jkgj7boVjA4o/lE/BxTshPTriQNtVl9c6OxOfXsZotpTphSlMGIUrEHR/t2rjbcV
yPeTwHFIAaqcCMivYvfsk24JD9DiygwGVvjqwQPsNF8H9y6xor+QAc23aaD8WPi1
1J91bfRxJWghxyGmsPx1G/EVi/0retE1tZdkyqlahThdSACZtUfA997V0KT/DrdH
svHjNclViHExWGL7cUd2s9AMjIz1vr0tUGxvU7KsZT2kEXP0qp96mjIfo0TkZbUb
xsYMKujE8ZRpn8+CakfeT7RMtAhGIRvtPDQDm+Qv84A6JOufrgF4C0B00/9kERIe
g5hH2YG2R4YD40G4wtxEGpk/2jcdWc37CJ+T17+7m8MgPFNmX8V5YsAFwfPe6iAt
1QON3crilFRYCawYcOypbjh4hb5O5Usvg0msUrvzaRJ7Gj6K/SmFdG4hOepCHbPc
g2Bu15ifdmaCa8ssZHK+HJmhbGTMkDqdBnv2lziR8TXIC/se2+y7Iasz4eVkfG1/
RkDgokFOv7YA5aqp5ZHn
=VgWS
-----END PGP SIGNATURE-----

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

3rd party / web wallets are no longer viable except as means to burn
customers and divulge (or be forced to divulge) their data to
governments and corporations.

Rather than restate what I have already posted on this matter I'll
leave it there. It's time also for those who are managing bitcoin.org
to reconsider what's posted there (the criteria for what's posted there

 - at present the "web wallet" section should be excluded, that is to
say, Removed! from bitcoin.org

 with the possible exception of CoinKite to remain, which has a
reasonable argument for having made such privacy advances as to merit
usage by people (and to remain at bitcoin.org)

Additionally, I see no point in recommending any of the other wallets
except Electrum, Mycelium, Core, and in the hardware side, the ones
that appear (Trezor and HW1).

Furthermore, I believe those of you who are working for Coinbase
customer operations or Bitpay (I will not name names, you know who you
are) should resign from your employment.  I will bring this point up
regularly.  You can easily find employment elsewhere, your skills are
in high demand.

- -O

Alon Muroch:
New Year. New Location. New Benefits. New Data Center in Ashburn, VA.

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEcBAEBCgAGBQJUwAGjAAoJEGxwq/inSG8CJoAIAMDR0h40IhFQNa8BW4AFeKUR
7tg84e752c7wY153GY/P7MOFL6w3E9h4tXzxdohTMMfF5Q6Ip6HaaifYmMpegFSS
WEHK0a3C2F+4sQMmMBtWbfyPsG5sJYtldY5hboSbh/6vXJJLXLSd+Sz3WHYx1Qjs
qn6sw5CA2Q0fborTxcsNZixUXD/OF5tTjDozp+KfnZ0imvBoKfhfJFlaNUXNon7U
zdPfahOrRIM5o70pjo6VwoutKRXr49JIoi47r9Uc3ujckUbLA5CVBApj4FApayb5
sXk8Ks+p6IvBr6Q0ycxXOKmPwbSALC5pLa7Ncb1MFFBGzxKFsMjoRwOLTXHlLUE=
=WgO4
-----END PGP SIGNATURE-----


-------------------------------------
BIP65 [1] says this:

I'd interpret "can't prove that it is impossible to spend" = cannot be used 
for freezing funds.

Then later, at "Motivation", it says:

This clearly says that funds can be frozen.
Can the BIP65-thing be used to freeze funds or can it not be?

Notice: I am by no means someone who is able to read Bitcoin script. I'm 
rather an end user. So maybe I'm misinterpreting the document?
I'm nevertheless trying to provide a "neutral" review from an outsider who's 
trying to understand whats new in 0.11.2.
You may want to discard my opinion if you think that BIP65 is aimed at an 
audience with more experience.

Greetings and thanks for your work!

[1] 
https://github.com/bitcoin/bips/blob/d0cab0379aa50cdf4a9d1ab9e29c3366034ad77f/bip-0065.mediawiki
-------------------------------------
Or put another way, lowering the block size limit (or cancelling an
increase) is a soft fork.  Like all soft forks, a majority of the hash
power can force the soft fork to take place.

On Wed, Jun 24, 2015 at 10:05:42AM -0700, Mark Friedenbach wrote:



-------------------------------------
Levin, it is a complicated issue for which there isn't an easy answer. Part
of the issue is that "block size" doesn't actually measure resource usage
very reliably. It is possible to support a much higher volume of typical
usage transactions than transactions specifically constructed to cause DoS
issues. But if "block size" is the knob being tweaked, then you must design
for the DoS worst case, not the average/expected use case.

Additionally, there is an issue of time horizons and what presumed
improvements are made to the client. Bitcoin Core today can barely handle
1MB blocks, but that's an engineering limitation. So are we assuming fixes
that aren't actually deployed yet? Should we raise the block size before
that work is tested and its performance characteristics validated?

It's a complicated issue without easy answers, and that's why you're not
seeing straightforward statements of "2MB", "8MB", or "20MB" from most of
the developers.

But that's not to say that people aren't doing anything. There is a
workshop being organized for September 12-13th that will cover much of
these "it's complicated" issues. There will be a follow-on workshop in the
Nov/Dec timeframe in which specific proposals will be discussed. I
encourage you to participate:

http://scalingbitcoin.org/

On Sun, Aug 16, 2015 at 9:41 AM, Levin Keller via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 04/27/2015 02:53 PM, Brian Deery wrote:


I've updated the proposal to provide for alternate methods of
notification that can be used *in addition to* notification transactions.

This frees the sender from constantly monitoring an address known to
be associated with their identity, although they should check it
periodically since not all senders will be capable of using every type
of alternate notification method.

I defined a Bitmessage notification method as an example; more can be
added if required.

https://github.com/justusranvier/rfc/blob/payment_code/bips/bip-pc01.mediawiki
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBAgAGBQJVQWzPAAoJECpf2nDq2eYj8uAP/RsP050K9z8oDGy0KO+zjFwM
iNzlsQaPY8kmR2k2oLXJp1n4QZIQAZly+vxjBZnOWXwAhrBcvnhNBvqdigwZYg3B
oGyvGvArzkve86Ke1WF1hZEAvml3cmQ5jxYKMlwhzRPcHq2kwznw+5jRuTbf1JbE
PxY5pOfnZ9ADVF5FkLR2KwBNvGA83Cle01hKd0eB6Omlb6azKDBXUqfzPPpB4lmp
A8D0P3zkayzBYIiybUExfPJHUthd5wXL/TLwFkysPV7SnJE64C6Q2StD4wUtNQRS
LDOw37RwhMx2Oz2YH3Ywi6w5tqYQP4LEWBuFb+LOqqphpV/FpFDl/Uznos00pz61
V9x2wrfg+MQnYk5/VIjPQxqvRsiu8yg4c2x0v+KIIMsuXKEPRFxaSS3DoaWUa1Jy
+WDobHndIDw1TDqctP8LIiZ7zbWNYKJ4HgUaacHvTiA7TrJXi1KHo2b1pmnTbKhv
fdHbjzd8UiMED6qeyrz1gGhjC2uSTwjZAmbBkCJccOGsrILoV1fifUW+de8qsBMH
6w6RiDwfeY2fHJHBS6O2Nhfk3OE5JaCWvy727ZXEq8lQ6dW0GOZvXg7INaktLagC
tqvg85J5eCm7X8xQ33vgx8q2xt+IriMI2UnTILtb2H8XSiMRQSP7XfWDMfVGu4pb
xbGZKbNS4WS+2FFKM4DK
=6l99
-----END PGP SIGNATURE-----
-------------------------------------

Furthermore, Bitcoin is significantly more than a "software project": it sits at a unique intersection of computer science, economics, physics, law and more.  While I agree that minor bug-fixes and code-maintenance-type issues should be dealt with quietly by developers, decisions regarding Bitcoin’s governance and its evolution should be shaped by an interdisciplinary group of stakeholders from across the community.  The hard- vs soft-fork debate is not just a code maintenance issue.  

Once again, let’s use the current gridlock in Core to rally the growth of new forkwise-compatible implementations of the protocol.  Gavin and Mike’s initiative with BIP101 and Bitcoin XT should be encouraged as one possible model for coming to consensus on hard-forking changes.  

Best regards,
Peter




-------------------------------------
Some miners have voluntarily deployed CPFP.

I had a thread about it earlier this month where some other ideas got
tossed around.
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009304.html

The specific pull request to get it into the reference implementation
is still open based on a merge conflict
https://github.com/bitcoin/bitcoin/pull/1647

Peter Todd has another thread on RBF which you mentioned earlier
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-April/005620.html

There is also a beautiful example of CPFP in action from Eligius in
this transaction:
https://blockchain.info/tx/4cc3e2b6407ae8cdc1fd62cb3235f9c92654277684da8970db19a0169e44c68c

Follow the spent outputs and you will see the person is trying to
incentive the transaction by upping the fees of dependent
transactions.  It set in the mempool until Eligius won a block, then
it made it into the chain.  CPFP still works, but only in an Eligius
block (that I can see).  So it's better than nothing.

-------------------------------------
joe2015--- via bitcoin-dev [bitcoin-dev@lists.linuxfoundation.org] wrote:

This is a very complex way to enter zombie mode.

A simpler way is to track valid PoW chains by examining only the header, that
are rejected for other reasons.

Once a chain is seen to be 6 or more blocks ahead of my chain tip, we should
enter "zombie mode" and refuse to mine or relay, and alert the operator, because
we don't know what we're doing and we're out of date.  This way doesn't require
any modifications to block structure at all.

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


-------------------------------------
Hello all,

I was very surprised to learn that Blockstream will implement Sidechains
for exchanges [1], [2] and has been working on this privately. Can somebody
explain this “announcement”? Just a few comments on this “proposal”.

“This new construction establishes a security profile inherently superior
to existing methods of rapid transfer and settlement, and is directly
applicable to other problems within existing financial institutions.”

First of all, what does Bitcoin have to do with existing financial
institutions? Secondly, what in do you mean by “rapid transfer” and
"settlement"? Bitcoin is anonymous, digital cash. There is no such thing as
settlement, there is only the transfer of digital cash and that's it
(settlement is a bad word for this kind of transfer of property). If you
make up new terms define them accurately and don't play the
crypto-buzzword-bingo game.

“This, in addition to increasing the security of funds normally subject to
explicit counterparty risk, fosters conditions that increase market
liquidity and reduce capital requirements for on-blockchain business
models.”

Again – what does Bitcoin have to do with “market liquidity” and “capital
requirements”?

“Blockstream's innovative solutions are definitely a game changer for the
Bitcoin industry.”

Does Blockstream have commercial products now?

"These initial launch partners include Bitfinex, BTCC, Kraken, Unocoin, and
Xapo, and discussions are underway with another dozen major institutional
traders and licensed exchanges. "

??? so many questions and no answers.

Regards,
Benjamin
-------------------------------------
On Sun, Jan 04, 2015 at 05:44:59PM +0000, Gregory Maxwell wrote:

Yup.

I have a pull-req open to fix this:

https://github.com/bitcoin/bitcoin/pull/5521

-- 
'peter'[:-1]@petertodd.org
00000000000000000237ec84e4b02efbdf3bcbf62308c873da802caedd12432f
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 03/13/2015 05:24 PM, Mike Hearn wrote:

If a peer claims to provide network services, and does not do so while
consuming another node's resources, that might be considered exceeding
authorized access.

bitcoind should probably have more fine-grained control over how it
allocates connection resources between peers vs clients.
-----BEGIN PGP SIGNATURE-----

iQIcBAEBAgAGBQJVA2bPAAoJECpf2nDq2eYjm/UP/0MZmdEBameT6tnLnebkru5d
UeHsX6Qikv3qF+i936SkoDylg08PJNWlpApuXC5t52x262V763y9tGV8qqh3vTSf
LeLeKY1M4mYCjHjegpz3JXzzF9i9OqgWl+0OxGOHDHyp8COfzKzC9FEUP3XBqitb
swyeS2t0LkzJnXYV8z8pDOxn4pZN0cUaKPvBIRKEUs4PgA4JVpRTM5Rvzi7oOItz
GHknxH++ja7kfFpgRSJMh3gHu4xhRiHfzGPaszrrrznpubNr42+4ouBy+QDr2XYr
1AtklROYLySeUtd0yNxeWdeaLIBSTiiDisNkD62MOTr0Zmdnc6M7IefSCqLN4fD9
wPu5a5h4HI/N/4/+kUhGmW+g5vagKMkCVlUIsG7gpGQJk4HyLElAdmgDToPJTrvr
htrd7k5HjjZu8oAt/vYcx15myuQ7VXc7v193g7m3kRRx4rnZ5XCu5BJd92uaOW1e
9ARhN7hfNQbfVkZw0f+qfG0fzMSAk3aHxpao7topwKARQfYJ++Mry5qAzFfxWred
IHXHbd4JqafsUJxTqDvm7oVP+l+XqlFkZTGi5u6NjPSeJL0IMFI5NqOepqAqwi0P
n9tePxN19+TmK2TSGtuzWBNZXcbwujSmvzRnDouxpcTyhRXc5YBbetI4/s0xcAyK
sQ2dm0SKF4S8MclylelW
=IpAp
-----END PGP SIGNATURE-----
-------------------------------------
On Thu, Jul 30, 2015 at 6:20 PM, Gavin Andresen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

When considering "too conservative" options, let's not forget that,
say, scheduling 2MB for 2020 doesn't preclude us from deciding that
was too conservative and schedule, say 4MB for 2018 later. The first
hardfork would be "useless", but it would set a "minimum increase"
that would have been useful if the second one never happened.


To be clear, for this concrete case block.nTime would just work just
fine. I just want us to decide on one of the options and uniformly
recommend that options for all cases in BIP99 [just renamed the file,
https://github.com/jtimon/bips/blob/bip-forks/bip-0099.org ].
But, yes, please, let's discuss this in the other thread.

-------------------------------------
Forgive me if this idea has been suggested before, but I made this
suggestion on reddit and I got some feedback recommending I also bring it
to this list -- so here goes.

I wonder if there isn't perhaps a simpler way of dealing with UTXO growth.
What if, rather than deal with the issue at the protocol level, we deal
with it at the source of the problem -- the wallets. Right now, the typical
wallet selects only the minimum number of unspent outputs when building a
transaction. The goal is to keep the transaction size to a minimum so that
the fee stays low. Consequently, lots of unspent outputs just don't get
used, and are left lying around until some point in the future.

What if we started designing wallets to consolidate unspent outputs? When
selecting unspent outputs for a transaction, rather than choosing just the
minimum number from a particular address, why not select them ALL? Take all
of the UTXOs from a particular address or wallet, send however much needs
to be spent to the payee, and send the rest back to the same address or a
change address as a single output? Through this method, we should wind up
shrinking the UTXO database over time rather than growing it with each
transaction. Obviously, as Bitcoin gains wider adoption, the UTXO database
will grow, simply because there are 7 billion people in the world, and
eventually a good percentage of them will have one or more wallets with
spendable bitcoin. But this idea could limit the growth at least.

The vast majority of users are running one of a handful of different wallet
apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;
Blockchain.info; and maybe a few others. The developers of all these
wallets have a vested interest in the continued usefulness of Bitcoin, and
so should not be opposed to changing their UTXO selection algorithms to one
that reduces the UTXO database instead of growing it.

be larger, the fee could stay low. Miners actually benefit from them in
that it reduces the amount of storage they need to dedicate to holding the
UTXO. So miners are incentivized to mine these types of transactions with a
higher priority despite a low fee.

Relays could also get in on the action and enforce this type of behavior by
refusing to relay or deprioritizing the relay of transactions that don't
use all of the available UTXOs from the addresses used as inputs. Relays
are not only the ones who benefit the most from a reduction of the UTXO
database, they're also in the best position to promote good behavior.

--
*James G. Phillips IV*
<https://plus.google.com/u/0/113107039501292625391/posts>

*"Don't bunt. Aim out of the ball park. Aim for the company of immortals."
-- David Ogilvy*

 *This message was created with 100% recycled electrons. Please think twice
before printing.*
-------------------------------------
Please see the following draft BIP which should decrease the amount of
bytes needed per transaction. This is very much a draft BIP, as the design
space for this type of improvement is large.

This BIP can be rolled out by a soft fork.

Improvements are around 12% for  standard "one in two out" txn, and even
more with more inputs hashes.

https://gist.github.com/JeremyRubin/e175662d2b8bf814a688
-------------------------------------
On 3 August 2015 at 08:53, Adam Back <adam@cypherspace.org> wrote:


I will assert that the block size is political because it affects nearly
all users to some degree and not all those users are technically inclined
or care to keep decentralisation in the current configuration as you do.
This debate has forgotten the current and future users of Bitcoin. Most of
them think the hit to node count in the short term preferable to making it
expensive and competitive to transact.

We all need a little faith that the system will reorganise and readjust
after the move to big blocks in a way that still has a reasonable degree of
decentralisation and trustlessness. The incentives of Bitcoin remain, so
everyone's decentralised decision throughout the system, from miners,
merchants and users, will continue to act according to those incentives.
-------------------------------------
Fill or kill us normally used for trades and I think it can be confusing.
Previous times this has been discussed it has been discussed under
nExpiryTime or op_height (which enables expiration), for example, in the
freimarkets white paper.

As Mark points out this can be made safe by requiring that all the outputs
of a transaction that can expire have op_maturity/csv/rcltv of 100. That
makes them as reorg-safe as coinbase transactions. Unfortunately this
doesn't play very well with p2sh...
On Sep 17, 2015 3:08 PM, "Mark Friedenbach via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I agree that nHeight is the simplest option and is my preference.
Another option is to use the median time from the previous block (thus you
know whether or not the next block should start the miner confirmation or
not). In fact, if we're going to use bip9  for 95% miner upgrade
confirmation, it would be nice to always pick a difficulty retarget block
(ie block.nHeight % DifficultyAdjustmentInterval == 0).
Actually I would always have an initial height in bip9, for softforks too.
I would also use the sign bit as the "hardfork bit" that gets activated for
the next diff interval after 95% is reached and a hardfork becomes active
(that way even SPV nodes will notice when a softfork  or hardfork happens
and also be able to tell which one is it).
I should update bip99 with all this. And if the 2 mb bump is
uncontroversial, maybe I can add that to the timewarp fix and th recovery
of the other 2 bits in block.nVersion (given that bip102 doesn't seem to
follow bip99's recommendations and doesn't want to give 6 full months as
the pre activation grace period).
On Dec 18, 2015 8:17 PM, "Chun Wang via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

I still see footers being added to this list by SourceForge?



I've asked Jeff to not use his @bitpay.com account for now.

The only real fix is to use a mailing list operator that is designed to
operate correctly with DKIM/DMARC, either by not modifying messages in
transit, or by re-sending (and ideally re-signing) under their own identity.

Though I'm sure this won't be an issue for the Linux Foundation, the latter
approach is dangerous because it means the list operator takes full
responsibility for any spamming that occurs from that domain. If the mail
server is ever hacked or spammers start posting to the lists themselves,
all that spam will be seen as originating from the listserv itself and the
reputation will be degraded. It can end with everyone's mail going to the
spam folder.
-------------------------------------
I want to put a big thank-you out to Pindar, Warren, and others in the
organizing committee who I know must have put in a lot of hours to make
this happen. I will be attending, and I hope to see many of you there too.
It is my sincere hope that the academic structure of a workshop will help
break down some of the communication walls that have arisen in this debate,
and help us all work towards finding a compromise towards scaling bitcoin,
something we all want to see happen.

On Tue, Aug 11, 2015 at 1:45 AM, Pindar Wong via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Here’s a BIP. I wrote the BIP mostly to stir the pot on ideas of governance, but I’m moderately serious about it. This is set in Markdown for readability, but here’s the BIP-0001 Medawiki version: https://gist.github.com/andychase/dddb83c294295879308b <https://gist.github.com/andychase/dddb83c294295879308b>


  Title: BIP Acceptance Process
  Author: Andy Chase
  Status: Draft
  Type: Process
  Created: 2015-08-31

Abstract
========

The current process for accepting a BIP is not clearly defined. While
BIP-0001 defines the process for writing and submitting a Bitcoin
improvement proposal to the community it does not specify the precise
method for which BIPs are considered accepted or rejected.

This proposal sets up a method for determining BIP acceptance.

This BIP has two parts:

-   It sets up a process which a BIP goes through for comments
  and acceptance.
  -   The Process is:
      -   BIP Draft
      -   Submitted for comments (2 weeks)
      -   Waiting on opinion (two weeks)
      -   Accepted or Deferred
-   It sets up committees for reviewing comments and indicating
  acceptance under precise conditions.
  -   Committees are authorized groups that represent client authors,
      miners, merchants, and users (each as a segment). Each one must
      represent at least 1% stake in the Bitcoin ecosystem.

BIP acceptance is defined as at least 70% of the represented percentage
stake in 3 out of the 4 Bitcoin segments.

Copyright
=========

This document is placed into the public domain.

Motivation
==========

BIPs represent important improvements to Bitcoin infrastructure, and in
order to foster continued innovation, the BIP process must have clearly
defined stages and acceptance acknowledgement.

Rationale
=========

A committee system is used to organize the essential concerns of each
segment of the Bitcoin ecosystem. Although each segment may have many
different viewpoints on each BIP, in order to seek a decisive yes/no on
a BIP, a representational authoritative structure is sought. This
structure should be fluid, allowing people to move away from committees
that do not reflect their views and should be re-validated on each BIP
evaluation.

Weaknesses
==========

Each committee submits a declaration including their claim to represent
a certain percentage of the Bitcoin ecosystem in some way. Though
guidelines are given, it's up to each committee to prove their stake,
and it's up to the reader of the opinions to decide if a BIP was truly
accepted or rejected.

The author doesn't believe this is a problem because a BIP cannot be
forced on client authors, miners, merchants, or users. Ultimately this
BIP is a tool for determining whether a BIP is overwhelmingly accepted.
If one committee's validity claim becomes the factor that decides
whether the BIP will succeed or fail, this process simply didn't return
a clear answer and the BIP should be considered deferred.

Process
=======

-   **Submit for Comments.** The first BIP champion named in the
  proposal can call a "submit for comments" at any time by posting to
  the [Dev Mailing
  List](https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>)
  mailling with the BIP number and a statement that the champion
  intends to immediately submit the BIP for comments.
  -   The BIP must have been assigned BIP-number (i.e. been approved
      by the BIP editor) to be submitted for comments.
-   **Comments.**
  -   After a BIP has been submitted for comments, a two-week waiting
      period begins in which the community should transition from
      making suggestions about a proposal to publishing their opinions
      or concerns on the proposal.
-   **Reported Opinions.**
  -   After the waiting period has past, committees must submit a
      summary of the comments which they have received from their
      represented communities.
  -   The deadline for this opinion is four weeks after the BIP was
      submitted for comments.
  -   Committees cannot reverse their decision after the deadline, but
      at their request may flag their decision as "likely to change if
      another submit for comments is called". Committees can change
      their decision if a resubmit is called.
  -   Opinions must include:
      -   One of the following statements: "Intend to accept", "Intent
          to implement", "Decline to accept", "Intend to accept, but
          decline to implement".
      -   If rejected, the opinion must cite clear and specific
          reasons for rejecting including a checklist for what must
          happen or be change for their committee to accept
          the proposal.
      -   If accepted, the committee must list why they accepted the
          proposal and also include concerns they have or what about
          the BIP that, if things changed, would cause the committee
          to likely reverse their decision if another submit for
          comments was called.
-   **Accepted.**
  -   If at least 70% of the represented percentage stake in 3 out of
      4 segments accept a proposal, a BIP is considered accepted.
      -   If a committee fails to submit an opinion, consider the
          opinion "Decline to accept".
  -   The BIP cannot be substantially changed at this point, but can
      be replaced. Minor changes or clarifications are allowed but
      must be recorded in the document.
-   **Deferred.**
  -   The acceptance test above is not met, a BIP is sent back
      into suggestions.
  -   BIP can be modified and re-submitted for a comments no sooner
      than two months after the date of the previous submit for
      comments is called.
  -   A BIP is marked rejected after two failed submission attempts. A
      rejected BIP can still be modified and re-submitted.

Committees
==========

**BIP Committees.**

-   BIP Committees are representational structures that represent
  critical segments of the Bitcoin ecosystem.
-   Each committee must prove and maintain a clear claim that they
  represent at least 1% of the Bitcoin ecosystem in some form.
  -   If an organization or community does not meet that requirement,
      it should conglomerate itself with other communities and
      organizations so that it does.
-   The segments that committees can be based around are:
  -   Bitcoin software
  -   Merchants/services/payment processors
  -   Mining operators
  -   User communities
-   A person may be represented by any number of segments, but a
  committee cannot re-use the same resource as another committee in
  the same segment.

-   **Committee Declarations.** At any point, a Committee Declaration
  can be posted.
-   This Declaration contain details about:
  -   The segment the Committee is representing
  -   Who the committee claim to represent and it's compositional
      makeup (if made up of multiple miner orgs, user orgs, companies,
      clients, etc).
  -   Proof of claim and minimum 1% stake via:
      -   Software: proof of ownership and user base (Min 1% of
          Bitcoin userbase)
      -   Merchant: proof of economic activity (Min 1% of Bitcoin
          economic activity)
      -   Mining: proof of work (Min 1% of Hashpower)
      -   For a user organization, auditable signatures qualifies for
          a valid committee (Min 1% of Bitcoin userbase)
  -   Who is running the committee, their names and roles
  -   How represented members can submit comments to the committee
  -   A code of conduct and code of ethics which the committee
      promises to abide by
-   A committee declaration is accepted if:
  -   The declaration includes all of the required elements
  -   The stake is considered valid
-   Committee validation is considered when considering the results of
  opinions submitted by committee on a BIP. A committee must have met
  the required stake percentage before a BIP is submitted for
  comments, and have maintained that stake until a valid opinion
  is submitted.
  -   Committees can dissolve at any time or submit a declaration at
      any time
  -   Declaration must have been submitted no later than the third day
      following a BIP's request for comments to be eligible for
      inclusion in a BIP

BIP Process Management Role
===========================

BIPs, Opinions, and Committee Declaration must be public at all times.

A BIP Process Manager should be chosen who is in charge of:

-   Declaring where and how BIPs, Opinions, and Committee Declaration
  should be posted and updated officially.
-   Maintaining the security and authenticity of BIPs, Opinions, and
  Committee Declarations
-   Publishing advisory documents about what kinds of proof of stakes
  are valid and what kinds should be rejected.
-   Naming a series of successors for the roles of the BIP Process
  Manager and BIP Editor (BIP-001) as needed.

Conditions for activation
=========================

In order for this process BIP to become active, it must succeed by its
own rules. At least a 4% sample of the Bitcoin community must be
represented, with at least one committee in each segment included. Once
at least one committee has submitted a declaration, a request for
comments will be called and the process should be completed from there.
-------------------------------------


Le 17/07/2015 03:01, Justin Newton via bitcoin-dev a crit :

Hi Justin,

Your lookup solution is indeed more efficient than OpenAlias, and more
robust to DoS. However, that is not because you use a two-tier lookup.

Indeed, instead of having the following records:
_wallet.sample = "btc ltc"
_btc._wallet.sample = "mybitcoinadress"

you could simply have:
_wallet.sample = "btc ltc"
_btc.sample = "mybitcoinaddress"

In practice, a wallet supporting only Bitcoin will skip the currencies
lookup in both cases, and go directly for the _btc record.

One benefit of having an intermediate "_wallet" level is to allow zone
delegation. Is that the reason for that choice?

Thomas

-------------------------------------
On Wed, May 27, 2015 at 7:47 AM, Peter Todd <pete@petertodd.org> wrote:

You've misunderstood it, I think-- Functionally nlocktime but relative
to each txin's height.

But the construction gives the sequence numbers a rational meaning,
they count down the earliest position a transaction can be included.
(e.g. the highest possible sequence number can be included any time
the inputs are included) the next lower sequence number can only be
included one block later than the input its assigned to is included,
the next lower one block beyond that. All consensus enforced.   A
miner could opt to not include the higher sequence number (which is
the only one of the set which it _can_ include) it the hopes of
collecting more fees later on the next block, similar to how someone
could ignore an eligible locked transaction in the hopes that a future
double spend will be more profitable (and that it'll enjoy that
profit) but in both cases it must take nothing at all this block, and
risk being cut off by someone else (and, of course, nothing requires
users use sequence numbers only one apart...).

It makes sequence numbers work exactly like you'd expect-- within the
bounds of whats possible in a decentralized system.  At the same time,
all it is ... is relative nlocktime.


-------------------------------------
I'm not intending to completely dismiss your concerns but as a data point: I read this list daily and it usually takes 15 minutes or less while I drink a cup of coffee.

My concern is that this is one of the (maybe *the*) last uncensored persisted forums related to technical bitcoin discussion with wide viewership. It is increasingly difficult for an average somewhat technical person to attempt to hold the bitcoin development community accountable for its actions or have their voice heard. It would really be a shame if messages like this were relegated to a black hole where things like the exchange rate and various scammy spam goes to fester.


Who will draw this line? It's unclear to me who the list admins are.

-------------------------------------
Hello all,

The number of incidents involving malware targeting bitcoin users continues to rise.  One category of virus I find particularly nasty is when the bitcoin address you are trying to send money to is modified before the transaction is signed and recorded in the block chain.  This behaviour allows the malware to evade two-factor authentication by becoming active only when the bitcoin address is entered.  This is very similar to how man-in-the-browser malware attack online banking websites.

Out of band transaction verification/signing is one method used with online banking to help protect against this.  This can be done in a variety of ways with SMS, voice, mobile app or even security tokens.  This video demonstrates how HSBC uses a security token to verify transactions online.  https://www.youtube.com/watch?v=Sh2Iha88agE <https://www.youtube.com/watch?v=Sh2Iha88agE>.

Many Bitcoin wallets and services already use Open Authentication (OATH) based one-time passwords (OTP).  Is there any interest (or existing work) in in the Bitcoin community adopting the OATH Challenge-Response Algorithm (OCRA) for verifying transactions?

I know there are other forms of malware, however, I want to get thoughts on this approach as it would involve the use of a decimal representation of the bitcoin address (depending on particular application).  In the HSBC example (see YouTube video above), this was the last 8 digits of the recipient’s account number.  Would it make sense to convert a bitcoin address to decimal and then truncate to 8 digits for this purpose?  I understand that truncating the number in some way only increases the likelihood for collisions… however, would this still be practical or could the malware generate a rogue bitcoin address that would produce the same 8 digits of the legitimate bitcoin address?

Brian Erdelyi
-------------------------------------
I think I can solve the debate and give everyone what they want.

Some people want BIP65, others do not.

We can roll out 65 in a clever way, such that Greg/PeterT can get it, but
Mike and Peter R don't need to have it (both versions can run alongside
each other). Even better, people can switch back and forth between versions
as much as they like.

How might this work? Well, paradoxically, we could do this by *imposing
additional constraints* on transaction validation, such that transactions
made a very specific certain way will always look valid to non-CLTVers, but
for CLTVers they will not be valid unless the CLTV rules are followed. The
obvious concern is that non-CLTV people might receive invalid payments.
However, their software is already set up to request payments in a non-CLTV
way, so, luckily, this is actually not a problem at all! SPV clients can
elect to only connect to nodes which are non-CLTV.

Problem solved!

I am happy to have solved this problem for you all, and ended this discord
harmoniously. If we all put our heads together, these words of founding
father Aretha Franklin will ring true: "there's nothing we can't overcome".


On Tue, Oct 6, 2015 at 3:29 AM, Marcel Jamin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

On Feb 15, 2015, at 6:02 PM, Peter Todd <pete@petertodd.org> wrote:

I thought I was clear, that I am using Bitcoin Core as border router talking to its P2P interface.

The reimplementation of consensus code helped me to deeply understand the protocol, aids debugging
and now comes handy to create a side chain.



Acquire some before you claim its useless.

Tamas Blummer
-------------------------------------
Jan, this is great stuff! Thanks for sharing your experiences.

I think the 4k payments requests issue must be solvable somehow. I had
no trouble transmitting that amount via NFC, although yes a bit of delay
was noticable.

About payment_url: Protobuf allows changing optional to repeated and yes
it's backwards compatible. Which is why I'm personally against parsing
two fields rather than just one.


No it isn't. It's implemented in bitcoinj though.




-------------------------------------
Hi Jean-Paul,

that's a very interesting point of view and I have never thought about
it this way, since I have a totally different background.

How would you go on about defining a min spec? Is this done by testing
the software on different hardware configurations or are you looking
at the requirements a priori?

Best regards
Henning


On Wed, Jul 01, 2015 at 09:04:19PM -0700, Jean-Paul Kogelman wrote:





-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp

-------------------------------------
On Thursday, October 22, 2015 2:55:14 PM Justus Ranvier wrote:

No, they just need to improve their software, and only to support receiving 
with payment codes (not sending to them). BIPs should in general not be 
designed around current software, especially in this case where there is no 
benefit to doing so (since it requires software upgrades anyway).

Luke

-------------------------------------

Which part of "in the next few years" was unclear?

This seems to be a persistent problem in the block size debates: the
assumption that there are only two numbers, zero and infinity.

BIP101 tops out at 8 gigabyte blocks, which would represent extremely high
transaction rates compared to today. *If* Bitcoin ever became so popular,
it would be a long way in the future, and many things could have happened:

   1. Bitcoin may have become as irrelevant as the Commodore 64 is.
   2. We may have invented upgrades that make Bitcoin 100x more efficient
   than today.
   3. Hardware may have improved so much that it no longer matters.
   4. The world may have been devastated by nuclear war and nobody gives a
   shit about internet currencies anymore, because there is no internet.

It's silly to ignore the time dimension in these decisions. Bitcoin will
not last forever: even if it becomes very successful it will one day it
will be replaced by something better, so it does not have to handle
infinite usage.

But hey, as you bring it up, I'd have been happy with no upper limit at
all. There's nothing magic about 8 gigabytes. I go along with BIP 101
because it is still the only proposal that is both reasonable and
implemented, and I'm willing to compromise.
-------------------------------------
Hi Jorge,

I'm glad we seem to be reaching agreement that hard forks aren't so bad
really and can even have advantages. It seems the remaining area of
disagreement is this rollout specifically.

Indeed it will, but the point of fully verifying is to *not* converge with
the miner majority, if something goes wrong and they aren't following the
same rules as you. Defining "work" as "converge with miner majority" is
fine for SPV wallets and a correct or at least reasonable definition. But
not for fully verifying nodes, where non-convergence is an explicit design
goal! That's the only thing that stops miners awarding themselves infinite
free money!

No, I'm focused on the block size issue right now. I don't think there's
much point in improving the block chain protocol if most users are going to
be unable to use it. But the modification is simple, right? You just
replace this bit:

  CHECKLOCKTIMEVERIFY redefines the existing NOP2 opcode

with this

  CHECKLOCKTIMEVERIFY defines a new opcode (0xc0)

and that's it. The section *upgrade and testing plan* only says TBD so that
part doesn't even need to change at all, as it's not written yet.
-------------------------------------


I am so sorry. I thought this was XT list :)

Apologies to everyone.


-------------------------------------
For what it's worth, there was consideration of replacing protocol
buffers when modifying BIP70 to function with the altcoin I work on
(changes were required anyway in eliminate any risk that payment
requests could not be accidentally applied to the wrong blockchain). The
eventual conclusion was that while we might have used JSON or XML if we
were starting from scratch, there's no choice that's clearly better.
While deployed infrastructure for payment protocol is still quite
limited, it seems that the cost to replace at this point is higher than not.

If there's ever a major reworking of the standard, for example to handle
recurring payments, it's probably worth thinking about then, but
protocol buffers result in a compact data format which is supported by
most major languages (and size is a concern if dealing with Bluetooth or
NFC), and has no major drawbacks I am aware of.

Ross

On 19/01/2015 20:40, Mike Hearn wrote:

-------------------------------------
At the recent Scaling Bitcoin conference in Hong Kong we had a chatham
house rules workshop session attending by representitives of a super
majority of the Bitcoin hashing power.

One of the issues raised by the pools present was block withholding
attacks, which they said are a real issue for them. In particular, pools
are receiving legitimate threats by bad actors threatening to use block
withholding attacks against them. Pools offering their services to the
general public without anti-privacy Know-Your-Customer have little
defense against such attacks, which in turn is a threat to the
decentralization of hashing power: without pools only fairly large
hashing power installations are profitable as variance is a very real
business expense. P2Pool is often brought up as a replacement for pools,
but it itself is still relatively vulnerable to block withholding, and
in any case has many other vulnerabilities and technical issues that has
prevented widespread adoption of P2Pool.

Fixing block withholding is relatively simple, but (so far) requires a
SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
do this hard-fork in conjunction with any blocksize increase, which will
have the desirable side effect of clearly show consent by the entire
ecosystem, SPV clients included.


Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block
witholding attacks are a good thing, as in their model they can be used
by small pools against larger pools, disincentivising large pools.
However this argument is academic and not applicable to the real world,
as a much simpler defense against block withholding attacks is to use
anti-privacy KYC and the legal system combined with the variety of
withholding detection mechanisms only practical for large pools.
Equally, large hashing power installations - a dangerous thing for
decentralization - have no block withholding attack vulnerabilities.

1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------------------------------
To be quite frank, I'm a little disappointed we've fallen back on arguing over numbers pulled out of a hat rather than discussing far more fundamental issues such as the dev process generally, consensus building, and our basic understanding of what Bitcoin really is, its strengths and weaknesses, where it shows most promise, and communicating a more unified vision to the industry and the public.

On September 18, 2015 10:10:08 AM PDT, Dave Scotese via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------------------------------
This is quite off topic, so please consider another channel. As long as you
have my attention, I previously started a cryptographically secure RPS game
as a proof of concept.

https://github.com/thofmann/rock-paper-scissors-protocol-secure/blob/master/readme.md

Unfortunately, I do not have the time to develop it at the moment, but you
can message me off the channel if you would like to discuss things further.

Thank you,
Trevin Hofmann
On Sep 29, 2015 12:47 AM, "Neil Haran via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Although I agree that how safe a pre-hardfork upgrade period is depends on
the complexity of the changes (we should assume everyone may need time to
reimplementat it themselves in their own implementations, not just upgrade
bitcoin core) and bip102 is indeed a very simple hardfork, I think less
than 6 months for a hardfork is starting to push it too much.
For a more complex hardfork (say, a SW hardfork or a collection of many
little fixes) I believe 1 year or more would make more sense.

BIP99 recommends a time threshold (height or median time) + 95% miner
upgrade confirmation with BIP9 (version bits).
So how about the following plan?

1) Deploy BIP102 when its ready + 6 median time months + 95% miner upgrade
confirmation

2) Deploy SW when it's ready + 95% miner upgrade confirmation via bip9.

Note that both "when it's ready" depend on something we are not paying a
lot of attention to: bip9's implementation (just like bip113, bip68-112,
bip99, the coinbase-commitments-cleanup post-SW uncontroversial hardfork,
etc).

Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already
equivalent to the 2-4-8 "compromise" proposal (which by the way I never
liked, because I don't think anybody should be in a position to
"compromise" anything and because I don't see how "let's avoid an
unavoidable economic change for a little bit longer" arguments can
reasoably claim that "we need to kick the can down the road exactly 3 more
times" or whatever is the reasoning behind it).
-------------------------------------
I think I have explained my motivation but let me try to make it 
clearer.

For example, BIP62 says "scriptPubKey evaluation will be required to 
result in a single non-zero value". If we had BIP62 before BIP16, P2SH 
could not be done in the current form because BIP16 leaves more than one 
element on the stake for non-upgrading nodes. BIP17 also violates BIP62.

BIP68 is "only enforced if the most significant bit of the sequence 
number field is set.", so it is optional, anyway. All I do is to move 
the flag from sequence number to version number.

The blocksize debate shows how a permanent softfork may cause trouble 
later. We need to be very careful when doing further softforks, making 
sure we will have enough flexibility for further development.

Mark Friedenbach 於 2015-08-08 14:56 寫到:


-------------------------------------
Micha I think you are correct, I dont think extension blocks (or
sidechains for that matter) can allow soft-fork increase of the total
Bitcoins in the system, because the main chain still enforces the 21m
coin cap.  A given extension block could go fractional, but if there
was a run to get out, the last users out will lose, or they'll all
take a hair-cut etc.  So presumably users would decline to use an
extension block with fractional bitcoin.

I mean you could view it like say an exchange (mtgox?) that somehow
accidentally or intentionally creates fictional Bitcoin IOUs in it's
system, eg in some kind of pyramid scheme - that doesnt create more
Bitcoins, it just means people who think they have IOUs for real
Bitcoins, are fractional or fake.  With an extension block or
sidechain furthermore it is transparent so they will know they are
fractional.

Relatedly it seems possible to implement a sidechain with advertised
demurrage, with an exit or entrance fee to discourage holding outside
of the chain to avoid demurrage.  There are apparently economic
arguments for why people might opt in to that (higher velocity
economic activity, gresham's law, merchants offering discounts for
buying with demurrage Bitcoins, maybe lower per transaction fees
because say miners can mine the demurrage).  However that is a
different topic, again not changing the number of coins in
circulation.

Adam


On 7 October 2015 at 08:13, Micha Bailey via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

As the outcome of a block size BIP would be a code change to Bitcoin Core,
I cannot make improvements, only ask for them. Which is what I'm doing.

I agree that BIP 1 is not clear enough. Gavin is writing a BIP to accompany
his patch, because BIPs are best when they describe working code, and BIP 1
*is* at least clear about that. Otherwise it can turn out during
implementation that something was different to what was anticipated. I'm
sure you agree with this.

So a BIP is coming. However, BIP 1 also says this:

Vetting an idea publicly before going as far as writing a BIP is meant to


and

BIP authors are responsible for collecting community feedback on a BIP


OK. Gavin has been vetting the idea publicly and collecting community
feedback. Note that the entire Bitcoin community is not on this list, so he
published a series of blog posts to get wider feedback, and then was
criticised for not doing it all here instead.

But anyway - so far, so good.  The procedure is being followed.

What happens once a BIP is written? The process says:

For a BIP to be accepted it must meet certain minimum criteria. It must be





This is where the problem starts.

The BIP process you refer to *does not state how acceptance will happen*.
It merely sets out a few minimum requirements like making some sort of
sense, having code. It's also full of extremely vague descriptions like
"must represent a net improvement". Improvement according to who? That's
left unexplained.

And then it says what happens once a BIP is accepted.

The middle bit is missing. When there is disagreement over a consensus BIP,
how are decisions made?
-------------------------------------
Before miners get angry, consider that whatever the community does will
attempt to preserve the efforts you have made to make Bitcoin a success.
Paragraph five, below, includes a provision to protect you, so please don't
write me off.

The competition is essential to protecting the data in the blockchain.  I
worry that any time (eg all time so far) the rules of that competition
remain static, a group of people willing to develop and optimize hardware
that performs the work will form, and their products will find a following
- the miners.  These miners and hardware producers will advance the
technology until the cost of competing in the space is so high that mining
bitcoin is, for all practical purposes, centralized.

I propose that the proof of work algorithm be scheduled to change
periodically.  The current definition is pretty simple and there's no
reason not to continue using simple definitions.  We could develop a list
of hash algorithms which could be indexed so that the first few bits of the
difficulty-change block's hash (or even *every* block's hash) could be used
to select one to be used for the next block.  The principle is to
discourage specialization of hardware designed for what we must admit is an
arbitrary computing exercise, intended only to enable competition.

Of course chip designers could start working on hardware that can handle
all the algorithms defined, but the protocol can also warn them that from
time to time, the community will alter the content of the list of hash
algorithms, specifically to ensure that *general purpose *computing
machines (ie, what the average tech aficionado will have) is the best
device for mining bitcoin.

If such a variable PoW were to be used, I recommend that most of the
elements in the initial list of algorithms be the current PoW algorithm so
that most blocks can be solved by the existing mining community, and only
one every now and then will be available to everyone else.  Over time, that
ratio would fall, giving the miners time to convert their expertise into
more productive activities.

I refer you to the stories at the beginning of each chapter of Douglas
Hofstadter's
Gödel, Escher, Bach: an Eternal Golden Braid, a few of which describe a
competition between a record-player-making tortoise and Achilles, who works
on making records that break the record players.  It offers some through
provoking musings that can easily be related to this thing Satoshi made.

notplato
-------------------------------------
Why do it as an OP_RETURN output? It could be a simple token in the coinbase input script, similar to how support for P2SH was signaled among miners. And why should there be an explicit token for voting for the status quo? Simply omitting any indication should be an implicit vote for the status quo. A miner would only need to insert an indicator into their block if they wished for a larger block.

That said, proposals of this type have been discussed before, and the objection is always that miners would want larger blocks than the rest of the network could bear. Unless you want Bitcoin to become centralized in the hands of a few large mining pools, you shouldn't hand control over the block size limits to the miners.


On Sunday, 31 May 2015, at 3:04 pm, Stephen Morse wrote:


-------------------------------------
On 24 June 2015 1:49:51 PM AEST, Jeff Garzik <jgarzik@gmail.com> wrote:

Of course they can. What, then, is the need for BIP100's hard-limit voting mechanism?

You only need consensus rules to enforce block size limits if you're enforcing them _against_ miners. Which may be a perfectly valid thing to do (if your threat model includes, for example, the possibility that large miners deliberately create large blocks to gain an advantage over small miners.) But BIP100 doesn't address that anyway. 

Wouldn't it be safer for consensus to get behind Gavin's simpler 8MB->8GB hard-limit growth curve*, and then encourage miners to enforce a soft limit below that, agreed through a voting mechanism? The later can be implemented at any time without consensus changes -- nobody can prevent miners from coordinating the max block size they'll build on anyway.

* but with a safer "supermajority" than 75% please :)
-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.

-------------------------------------
Nothing to apologize for. And yes, that's the correct one.
On Jun 6, 2015 2:39 AM, "Pindar Wong" <pindar.wong@gmail.com> wrote:

-------------------------------------
On 22/01/15 10:36, U.Mutlu wrote:

Bitcoin Core still does not support Hierarchical Deterministic aka BIP32
Wallets. Without them it is possible to backup just one private key per
backup, which is rather useless and in fact dangerous.


I would suggest looking at different software wallets that do support
such functionality such as Electrum or Multibit.

-- 
Best Regards / S pozdravom,

Pavol Rusnak <stick@gk2.sk>


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256



On 23 January 2015 08:35:23 GMT-08:00, slush <slush@centrum.cz> wrote:

That's what P2SH is for; the senders will just be sending to a P2SH address.


Hard-forks aren't hard for directly political issues, they're politically hard because they're risky by requiring everyone yo upgrade at once. In the case of signature validation, that touches a *lot* of third party code that people rely on to avoid being defrauded.

FWIW I've actually got a half-finished writeup for how to use OP_CODESEPARATOR and a CHECKSIG2 soft-fork to have signatures sign fees and so forth.
-----BEGIN PGP SIGNATURE-----
Version: APG v1.1.1

iQFQBAEBCAA6BQJUwonGMxxQZXRlciBUb2RkIChsb3cgc2VjdXJpdHkga2V5KSA8
cGV0ZUBwZXRlcnRvZGQub3JnPgAKCRAZnIM7qOfwhbwkCADP7AcJ6a6V/y7MHt2x
ZiCXYsfHq5j03kbSWXGi1Q/9RqWGVha1fhWPp62yhDxbWOfh5QKauCbrt2g1AqT3
xbnh+2XE1rApBQIiJ6u0wZmpCi+4EhH2M9R8UYu9oIMzBe4K2jhzUbzcOR9Qplyq
9j6yevNrvtNHZb2OTiaKelxnuZUEiAsONHPOvR8Fkflwbd/w279OeilRjHYt3A/J
U22KOwjNrpa7/QE/HeC0QINqr3S132Yg4iYFwPviBwGq/WXQuLHIzGtgKOzrIC1T
h6kpWO9CjSxVbjMrf68IrSHRv92K8y1LiHFRZvzp3ulzcGBo2btazmrp/fUDLCr0
6uFg
=uDeM
-----END PGP SIGNATURE-----



-------------------------------------
On Tuesday, November 03, 2015 8:37:44 PM Christian Decker wrote:

Changing the network protocol is trivial in comparison to making a permanent 
increase in UTXO set costs.


The problem isn't changing inputs/outputs, but that such changes invalidate 
later spends. In particular, note that wallets *should ideally* be actively 
trying to make transfers using multiple malleated versions of the same 
payment.

So the way to make an anti-malleable wallet, would be to strictly enforce the 
no-address-reuse rule on payments received (note this has no effect on 
other/current wallets) and rely only on the hash of that scriptPubKey+value 
for the input in subsequent transactions. This way, no matter what inputs or 
other outputs the transaction paying the address/invoice uses, the subsequent 
transaction ignores them and remains valid. (I am not suggesting this as a 
mandatory change that all wallets must adopt to receive the current semi-
malleability protection you propose - only that it be *possible* for wallets 
to upgrade to or offer in the future.)

Luke

-------------------------------------
Working on a proposal for extensions based on bip44 conventions.

Most interesting is: Fully Deterministic GPG keys.

https://github.com/taelfrinn/bip44extention

Would welcome any comments or interest
-------------------------------------
On the -dev IRC I asked the same question and people seem don't like it. 
I would like to further elaborate this topic and would like to consult 
merchants, exchanges, wallet devs, and users for their preference

Background:

People will be able to use segregated witness in 2 forms. They either 
put the witness program directly as the scriptPubKey, or hide the 
witness program in a P2SH address. They are referred as "native SW" and 
"SW in P2SH" respectively

Examples could be found in the draft BIP: 
https://github.com/jl2012/bips/blob/segwit/bip-segwit.mediawiki

As a tx malleability fix, native SW and SW in P2SH are equally good.

The SW in P2SH is better in terms of:
1. It allows payment from any Bitcoin reference client since version 
0.6.0.
2. Slightly better privacy by obscuration since people won't know 
whether it is a traditional P2SH or a SW tx before it is spent. I don't 
consider this is important since the type of tx will be revealed 
eventually, and is irrelevant when native SW is more popular

The SW in P2SH is worse in terms of:
1. It requires an additional push in scriptSig, which is not prunable in 
transmission, and is counted as part of the core block size
2. It requires an additional HASH160 operation than native SW
3. It provides 160bit security, while native SW provides 256bit
4. Since it is less efficient, the tx fee is likely to be higher than 
native SW (but still lower than non-SW tx)
---------------------------

The question: should we have a new payment address format for native SW?

The native SW address in my mind is basically same as existing P2PKH and 
P2SH addresses:

BASE58(address_version|witness_program|checksum) , where checksum is the 
first 4 bytes of dSHA256(address_version|witness_program)

Why not a better checksum algorithm? Reusing the existing algorithm make 
the implementation much easier and safe.

Pros for native SW address:
1. Many people and services are still using BASE58 address
2. Promote the use of native SW which allows lower fee, instead of the 
less efficient SW in P2SH
3. Not all wallets and services support payment protocol (BIP70)
4. Easy for wallets to implement
5. Even if a wallet wants to only implement SW in P2SH, they need a new 
wallet format anyway. So there is not much exta cost to introduce a new 
address format.
6. Since SW is very flexible, this is very likely to be the last address 
format to define.

Cons for native SW address:
1. Addresses are bad and should not be used anymore (some arguments 
could be found in BIP13)
2. Payment protocol is better
3. With SW in P2SH, it is not necessary to have a new address format
4. Depends on the length of the witness program, the address length 
could be a double of the existing address
5. Old wallets won't be able to pay to a new address (but no money could 
be lost this way)

------------------------------

So I'd like to suggest 2 proposals:

Proposal 1:

To define a native SW address format, while people can still use payment 
protocol or SW in P2SH if the want

Proposal 2:

No new address format is defined. If people want to pay as lowest fee as 
possible, they must use payment protocol. Otherwise, they may use SW in 
P2SH

Since this topic is more relevant to user experience, in addition to 
core devs, I would also like to consult merchants, exchanges, wallet 
devs, and users for their preferences.

-------------------------------------
Hey!

On Sun, Feb 22, 2015 at 05:37:16PM -0500, Andy Schroder wrote:

It doesn't need to be a recommendation I think, but maybe it would be
good to mention that a wallet may do that, if it wants.


Aw, interesting. Sometimes transfers seem to start and then not complete
in some way and occasionally the NFC dongle is then totally 'stuck' in some
way afterwards, that even after restarting the Python script or
reloading the driver nothing works anymore. I have to actually unplug
the dongle and plug it in again. Obviously not exactly production ready.
I had the same problems with the command line tools based on libnfc, so
it might be a problem lower down the stack. I'm not sure I have the
expertise to troubleshoot that.


Good point, I'm currently simply removing the signature, so that I can
modify the payment request. I haven't spoken with BitPay yet, but I hope
that they will extend their API at some point to set additional
payment_urls or provide a Bluetooth MAC and then I can do it properly
with signed requests.


You are right, I forgot to actually disable wifi and cellular data when
recording the video. But as you know it would work the same way offline.


There is a specific NFC intent that you have to list in your Android
manifest, but you are right that if you already support BIP21 URIs then
it is often fairly easy and quick to also support them via NFC.

Whereas the mime type approach means that you necessarily need to be
able to actually understand BIP70, which a lot of wallet don't yet. But
personally that wouldn't hold me back using the mime type if I feel it's
the better experience. Those wallets simply have to fall back on
scanning the QR code in the meantime and then get up to speed on their
NFC and BIP70 support.

I'm still concerned that the fact, that Bluetooth is often disabled, is a
problem for the UX. And it's not just a one-time thing as with NFC,
which is - in my experience - also often disabled, but then people turn
it on and leave it on. But with Bluetooth the Android system is geared
much more towards turning it off after use and people have this general
idea of 'it uses energy, so I should disable it' and sometimes also
'Bluetooth is insecure and if I leave it on I will get hacked'. So
chances are, Bluetooth will be off most of the time, which means
everytime you pay the dialog 'Turn on Bluetooth?' will pop up, which
isn't exactly streamlined.

So the advantage of transmitting the whole BIP70 payment request via NFC
I see is, that you don't need Bluetooth to get the payment request and
for sending the transaction back the wallet can then make an intelligent
decision and first try via HTTP and only after that fails, say something
like: "You are currently offline, turn on and transmit via Bluetooth
instead?". Much less confusing to the user, in my opinion.

Another idea could be to request the permission BLUETOOTH_ADMIN which,
as far as I know, allows you to programmatically turn on Bluetooth
without user interaction. The wallet could then have a setting somewhere
that says 'automatically turn on Bluetooth during payments' which would
enable and then disable (if it was off before) Bluetooth during the
payment process. That should also be a decent compromise, at the cost of
another permission.


I'm fine with doing changes here - I don't think there is all that much
stuff out there yet which would break from it. At the moment I'm also
modifying BitPay's memo field to contain 'ack', as Andreas' wallet
otherwise reports a failure if I transmit the original via Bluetooth. :-)
But I was assuming that was temporary anyway (?).

Jan


-------------------------------------
On Fri, Jun 19, 2015 at 07:30:17AM -0700, Adrian Macneil wrote:

Unless you're sybil attacking the network and miners, consuming valuable
resources and creating systemic risks of failure like we saw with
Chainalysis, I don't see how you're getting "very small" double-spend
probabilities.

You realise how the fact that F2Pool is using full-RBF right now does
strongly suggest that the chances of a double-spend are not only low,
but more importantly, vary greatly? Any small change in relaying policy
or even network conditions creates opportunities to double-spend.


You know, you're creating an interesting bit of game theory here: if I'm
a miner who doesn't already have a mining contract, why not implement
full-RBF to force Coinbase to offer me one? One reason might be because
other miners with such a contract - a majority - are going to be asked
by Coinbase to reorg you out of the blockchain, but then we have a
situation where a single entity has control of the blockchain.

For the good of Bitcoin, and your own company, you'd do well to firmly
state that under no condition will Coinbase ever enter into mining
contracts.

-- 
'peter'[:-1]@petertodd.org
00000000000000000fe727215265d9ddacb2930ad2d45920b71920b7aed687f1
-------------------------------------
Den 27/06/2015 08.21 skrev "Gregory Maxwell" <gmaxwell@gmail.com>:


Does anybody know when/if 0.10.2 will be available on the Ubuntu PPA?

I could of course just install manually, but I like the convenience of a
PPA.
-------------------------------------
I am interested in finding or writing a fuzzer for push tx APIs. I did not
find one after a brief search. Has anyone found otherwise, or is she in the
process of writing one?

If not, what features would people recommend for a new push tx fuzzer?

Endpoints I would like to test include:

https://live.blockcypher.com/btc-testnet/pushtx/

https://insight.bitpay.com/tx/send

https://blockchain.info/pushtx

https://coinb.in/#broadcast

https://btc.blockr.io/tx/push

https://chain.localbitcoins.com/tx/send


The fuzzer should be able to send random data, invalid characters, etc. but
also fuzz particular aspects of the transaction format such as malformed
P2SH and P2PKH transactions, fields such as lock time, size, # inputs,
version number, vin size, etc. It should also be able to fuzz a variety of
valid and invalid script formats using odd op codes, changing the order of
op codes, etc.


If anyone has recommendations about how such a fuzzer should be structured,
please let me know.


Finally, if you are interested in collaborating, please contact me via
private message.


Thanks!

Kristov
-------------------------------------
On 9/29/2015 7:05 PM, Rusty Russell wrote:

At least you changed the BIP to make it possible to see a fall off in 
support, even though nothing is done about it.


-------------------------------------
It sounds like you are seeking transaction expiration from the mempool, not
CPFP.



On Sat, Jul 11, 2015 at 4:30 PM, Dan Bryant <dkbryant@gmail.com> wrote:

-------------------------------------
On Aug 10, 2015 7:03 PM, "odinn via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
Note that I've

No offence, but I think that anyone who claims a block size limit change
can be done as a soft fork has some basic reading to do first.

Also, please keep this thread about Lightning.

-- 
Pieter
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



What holds you back from using m/i'/45' where i' is your multisig
"account" number?

Within your BIP45 wallet (lets assume Copay), you would not provide
the xpubkey at m/45', instead you would provide your xpubkey at m/i'/45'
.

It's probably no longer pure BIP45.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQIcBAEBAgAGBQJWEm34AAoJECnUvLZBb1Ps+TgP/RmIxp31GOF9JDYVvSpaWfjf
NhB7o5/AiQ5rNv2BL5Wdoebb3EHzPTjzf/gGq3vSHQkm+c5rnDO8kTtwlNog7HN3
djtlbdhftlqcWdgBYNRstMj52NlfQKz4exvaPTsFlxUS02VzafPhssde9H7KdBHw
HoePJziOku6ibHW3u2CdEtGHYSRZ0w7xUmGYUjYc9LRa86Gl5aHFOzTVmboQgNW+
MACb30gSjMGP6i4iHsuwYMpc7DJ1SyooqAL64Z+YN/ZJayHatJIxVx8GGPC2HW5g
nj1LuJacK5VA2VtqtnRhVP0IEm4rKSgSzJz/534lB8/6RFejR0VnjzLZcMsa2TcQ
7yWf6xAN3ijMWi8mxG2RL/2xZiH3txpdMLme3e5YyZ8sgDw04zJI8lKRkg8kN8Wo
G8JQ/6hAzK9OhceoCnazE1ODXRiKa/jXvtfdiuzAb7Q3zFbnXNT2b7dshU0eY6iW
GWBmOInBROfoAqt43gQIMC0gpBsQc9JlVN5s6CgPxhEAMsZI/akKm4qufGbvb767
USgzh6UPE+secLkgjZC/lVKmk7rD+7poJjZB5XzaGdKAxzpFTqw/gLXhfG17TVPA
ZwA2UFhnHVitP839RenrCxWORng2F8gDO/ElWTWtwCq7+c2UxEOxSiKfTOahEoFw
LmOUFxn5iMSbdBEbkm54
=cbgc
-----END PGP SIGNATURE-----

-------------------------------------
On Fri, May 8, 2015 at 2:59 PM, Alan Reiner <etotheipi@gmail.com> wrote:



These are good points and got me thinking (but I think you're wrong). If we
really want each of the 10 billion people soon using bitcoin once per
month, that will require 500MB blocks. That's about 2 TB per month. And if
you relay it to 4 peers, it's 10 TB per month. Which I suppose is doable
for a home desktop, so you can just run a pruned full node with all
transactions from the past month. But how do you sync all those
transactions if you've never done this before or it's been a while since
you did? I think it currently takes at least 3 hours to fully sync 30 GB of
transactions. So 2 TB will take 8 days, then you take a bit more time to
sync the days that passed while you were syncing. So that's doable, but at
a certain point, like 10 TB per month (still only 5 transactions per month
per person), you will need 41 days to sync that month, so you will never
catch up. So I think in order to keep the very important property of anyone
being able to start clean and verify the thing, then we need to think of
bitcoin as a system that does transactions for a large number of users at
once in one transaction, and not a system where each person will make a
~monthly transaction on. We need to therefore rely on sidechains,
treechains, lightning channels, etc...

I'm not a bitcoin wizard and this is just my second post on this mailing
list, so I may be missing something. So please someone, correct me if I'm
wrong.



-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
-------------------------------------
On Mon, Oct 5, 2015 at 2:29 PM, Clément Elbaz <clem.ds@gmail.com> wrote:

Why would you care about payments to other people?
The scriptPubKey's that you give to your payers certainly have meaning to you.


What is it important that you are able to calculate balances of
wallets that aren't yours?


Why would anyone "pay you" to a scriptPubKey you don't understand?

I can "pay" the bill of my internet services by burying cash in a park
nearby my house for my provider to pick up later.
But if I don't tell my provider, it will never know. If I inform it, I
will get an answer: "no, sorry, we won't accept this new 'form of
payment' of yours as payment".

-------------------------------------
Hello,

I have been reading an argument saying that paying higher fees would scare Bitcoin users and they would stop using it, preferring bank transfers or other payment methods. This does not make sense for me. If some users leave, then demand for bitcoin transactions goes down and so do the fees. The others remain.

Fee market means that an equilibrium is found between the demand for bitcoin transactions and the available supply (given by the block size). The fee is the price that finds this equilibrium.

If a fee market starts to exist, the first ones to leave are the spammers, probably followed by the gamblers and perhaps people transacting very low amounts. The people that actually need Bitcoin would remain.

Please allow this fee market to form...

In the absence of a functioning fee market, I will refuse to run Bitcoin code that increases the block size and will do my best to tell everyone I know not to upgrade towards running such code. If Bitcoin succombs to the free stuff army, I will sell all the coins and leave. Nothing is for free.

I apologize for any exagerations, but I just felt strongly towards expressing my opinion here. I'm only a local Bitcoin trader, computer engineer, with a reasonable understanding of free markets. And I'm running only one full node.

Kind regards,
Valentin

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 01/20/2015 12:48 PM, Tamas Blummer wrote:

Depending on the threat model, the incentive to force confiscation
might also be lower.

- -- 
Justus Ranvier                   | Monetas <http://monetas.net/>
<mailto:justus@monetas.net>      | Public key ID : C3F7BB2638450DB5
                                 | BM-2cTepVtZ6AyJAs2Y8LpcvZB8KbdaWLwKqc
-----BEGIN PGP SIGNATURE-----

iQIcBAEBAgAGBQJUvq0CAAoJECpf2nDq2eYjr9kP/RWEg8Az43T+7qMFnrk37+y/
0pyEQ/zisao1d0LouxyGFu704U8Qayk96hUu+2GAQpS8hHVA0CmDW8E1hqKG2nGl
MTTQYp7932NY2NysIvNaQDhVErZZFqMpPYCnsSrnwUrygh+QjWAI8nvrrcgprG5/
zybzs5IJjFQ7QwYJ92D01shkqQJLYYspp2ME3z97AwPCBanN8eG4Iji/V8/aJqcZ
ZqF7yUjAySVUOUzR+Vju1C7N1i9MHzIG9vZA/jkaCiqZ8bvyQTm9LwSK3quoxGAB
lTplIwKjWsEvs0nm0RyurcPIWq1ppfPiWCaMCNDA5Byz3mJbSrRW5ErFgBtpYkgw
CF+WqoWU8fajQjqd8xcsKJmVyQqk4dUWXJQLGnd6pC3DCZGOPhr+6674vgmEQG5A
bXoBAtJfAJkxkDGEsngs4EBGc08iy+t6tJUh7+wI/La8xulM5BgJkQRTnL4Hn6KS
pcgYV9JP1BWMB4fkdL81mKnG98BJ98pj019C0nuPYQtSA0rUsWG9d3NYDPe87I+K
7UJ6NlNxTLxnS7nhr8Wk9UdqkFMsCQxF/RFR6I9vCQ/FMSD+i1786I72kkyf4cWJ
4ZssTX3yo6pN/faU2cBk84PQlA2ziARXqO+jzbxVR7AFpT2BESUtBdirh1CPEMfR
piBBTr6I86R2bpZYv046
=pJvU
-----END PGP SIGNATURE-----
-------------------------------------
This post by Gavin got rejected by the moderators.

Without a doubt this moderation policy is already a disaster. I'm fully
expecting this message to get rejected too, but so you can see it Rusty: so
far in the reject bin there are messages from:

   - Well known uber-troll Gavin Andresen
   - And his partner in crime, Sergio Damian Lerner
   - Someone discussing a bug
   - Someone who wants to discuss CLTV
   - Someone pointing out that censorship of technical discussion is rarely
   a good idea
   - Someone pointing out that censorship of people complaining about
   censorship is also taking place.

WTF?
-------------------------------------
On Thu, Sep 24, 2015 at 7:02 PM, Suhas Daftuar via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Is there actually a requirement for the new message?  New nodes could just
unilaterally switch to sending headers and current nodes would be
compatible.

It looks like the only DOS misbehaving penalty is if the header is invalid
or if the headers don't form a chain.
-------------------------------------
On Sun, Dec 20, 2015 at 6:24 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



Yes, this is almost what -has- to happen in the long term.

Ideally we should start having wallets generate those proofs now, and then
introduce the max-age as a second step as a planned hard fork a couple
years down the line.

However,
1) There is also the open question of "grandfathered" UTXOs - for those
wallets generated in 2009, buried in a landfill and then dug out 10 years
ago

2) This reverses the useful minimization attribute of HD wallets - "just
backup the seed"
-------------------------------------
The `n` is the curve order, as shown here:

https://en.bitcoin.it/wiki/Secp256k1

This step is necessary to keep you on the curve. The
secp256k1_ec_privkey_tweak_add function from libsecp256k1 handles this
automatically, but if you use OpenSSL or some non-EC math library, you
probably have to do it yourself.

-William

On Fri, Jun 12, 2015 at 11:22 AM, James Poole <james@microtrx.com> wrote:


-------------------------------------
Luke,

- Definitely agree with most of your suggestions on the practical side;
several clarification could be made.
- The power to decrease the hard limit appears riskier long term in my
analysis.  This is mitigated somewhat by the ease at which miners may
locally or collectively lower the block size at any time, without a vote.


On Wed, Sep 2, 2015 at 8:17 PM, Luke Dashjr <luke@dashjr.org> wrote:

-------------------------------------
On Wed, Dec 30, 2015 at 05:42:47PM +0100, Marco Pontello via bitcoin-dev wrote:

You ever noticed how actually getting a BIP # assigned is the *last*
thing the better known Bitcoin Core devs do? For instance, look at the
segregated witness draft BIPs.

I think we have problem with peoples' understanding of the Bitcoin
consensus protocol development process being backwards: first write your
protocol specification - the code - and then write the human readable
reference explaining it - the BIP.

Equally, without people actually using that protocol, who cares about
the BIP?


Personally if I were assigning BIP numbers I'd be inclined to say "fuck
it" and only assign BIP numbers to BIPs after they've had significant
adoption... It'd might just cause a lot less headache than the current
system.

-- 
'peter'[:-1]@petertodd.org
000000000000000006808135a221edd19be6b5b966c4621c41004d3d719d18b7
-------------------------------------
On 19/09/15 15:11, Rune K. Svendsen wrote:

This isn't a particularly good definition.

"An honest miner is a miner that supports the network by building on top
of the best valid chain."

What is the "best valid chain"? The one with the most proof of work? The
one that meets some other definition of "best"?

"A malicious miner is one who wants to disrupt the Bitcoin network, not
support it"

This is a tautology, the equivalent of saying "a malicious miner is a
miner that is malicious" A true, but entirely useless, statement.

"for example by executing a 51% attack which mines empty blocks on top
of the best chain."

Again, you're begging the question with the word "attack", because
that's what you're supposed to demonstrate.

Apparently the difference between honest mining and malicious mining is
empty blocks? You've said in both cases the miners are extending the
"best valid chain". Is extending the best valid chain with an empty
block always a malicious act?

What's the significance of 51% in this definition? Is the same empty
block which extended the best valid chain honest if the miner who
produced it has 49% of the network hashing power and malicious if they
add a few more ASIC units?

-- 
Justus Ranvier
Open Bitcoin Privacy Project
http://www.openbitcoinprivacyproject.org/
justus@openbitcoinprivacyproject.org
E7AD 8215 8497 3673 6D9E 61C4 2A5F DA70 EAD9 E623
-------------------------------------
In terms of usage I think you'd more imagine a wallet that basically
parks Bitcoins onto channels at all times, so long as they are
routable there is no loss, and the scalability achieved thereby is
strongly advantageous, and there is even the potential for users to
earn fees by having their wallets participate in channel rebalancing
(where hubs pay users to rebalance channels - end up with the same net
position but move funds from one user-owned channel to another.)
Exchange deposit, withdrawal, payments, even in-exchange trades can
usefully happen in lightning for faster, cheaper more scalable
transactions.

Adam

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Not that issue - that's both easily avoidable, and has nothing to do with the replace-by-fee patch. I'm talking about something in the specific patch - good test to see if you've fully reviewed it.


On 22 February 2015 14:25:24 GMT-05:00, Tom Harding <tomh@thinlink.com> wrote:
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCAAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJU6k8q
AAoJEMCF8hzn9LncssUH/0acS1lhG8igRWBusnpDD+on+ryXNlTDKZGExzUKy7Wq
7SzYfMX8LAf/0Wbzs6wtyGzVjQOGmcM0XTAFN+Rp2rP3ZuSzAqO41Re+aUkiB67y
4PD8R05DmDgbc257HwIQM1aa+NPzzW5p8C+HnyZKpUqMNUAZOUVks22oRGywUXQY
WrNKiSFQMxW0l1thjX63/x3iXjV92gxyd9qWK8uPAokwNEdULPU5S1mlZbji+MaJ
cfR6WB02JR/GHPDK1rwmM8vAwQY82CMOJK3HB+1Dx88NvN5Ucn+ppVFtNETHA5g8
e7UcFeXXeMRF2AMwc9lFEmYsXmSAMJrTFeO981KoOHs=
=fESj
-----END PGP SIGNATURE-----



-------------------------------------
Hector,
 I can only say 2 things in the brief time I have now:

1. There is a solution that I proposed for proving you own a copy of the
block-chain. It's using aymmetric-time functions:

https://bitslog.wordpress.com/2014/11/03/proof-of-local-blockchain-storage/

2. I'm finishing a paper on a transaction system (DAGCoin) which relies on
proof-of-work per transactions, and no per blocks. Actually it has no
blocks.

This has been explored in the past, but I came up with some basic ideas
that make this work a few months ago. I will forward a draft to you while
at the same time I try to analyze your proposal. Or I may publish the draft
altogether.

Best regards,
 Sergio






On Fri, Aug 21, 2015 at 5:50 PM, Tamas Blummer via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hi Warren,

If you set dmarc_moderation_action to "Munge from", the list will detect
when someone posts from a domain that publishes a request for strict
signature checking for all mails originating from it (in DNS) and rewrite
the envelope-from to the list's address.  Reply-to will be added and set to
the original sender.

I think that this is probably a better way to workaround the issue (rather
than playing with getting the list to not break the signature) until these
things mature further.

Thoughts?

--adam




On Fri, Jun 19, 2015 at 6:38 AM, Warren Togami Jr. <wtogami@gmail.com>
wrote:

-------------------------------------
Yes, on it.



Prabhat Kumar Singh


On Thu, Aug 27, 2015 at 5:44 PM, <jl2012@xbt.hk> wrote:

-------------------------------------
I think that the argument for Peter's optimal block construction algorithm
(leading to the definition of the Fee Demand Curve) is that in the limit of
a mempool with a very large number of transactions, you should be able to
assume that for any given transaction [image: i] of size [image: s_i] and
fee density [image: \phi_i], many transactions [image: j] will exist such
that [image: s_j<s_i] and [image: \phi_j=\phi_i]. This means that, in
solving the knapsack problem, one has the choice of including what
effectively amounts to fractions of transactions. This in turn results in
an unbounded knapsack problem for which Peter's greedy algorithm is an
asymptotically optimal solution.

Obviously this breaks down in small mempool scenarios where the
discreteness of transactions can not be washed away (such as the current
state of the network).

I hope this clarifies your point.

Daniele

---------- Forwarded message ----------
From: Daniele Pinna <daniele.pinna@gmail.com>
Date: Sun, Aug 30, 2015 at 1:11 PM
Subject: Another issue with the Fee Demand curve
To: Peter R <peter_r@gmx.com>


This was pointed out to me by Tom Harding.

Choosing what the optimal way of choosing transactions to be included in a
block is a knapsack problem (an NP-complete problem!):

https://en.wikipedia.org/wiki/Knapsack_problem

For which your suggested solution, which is by no means exact, is known as
the "Greedy Approximation Algorithm". This algorithm in turn works best
only when a very large quantity of high density transactions is available
in the mempool.

This wouldn't invalidate your proof but it may significantly alter the
optimal block size value.

Just thought I would throw this your way.

Daniele

---------- Forwarded message ----------
From: Tom Harding <tomh@thinlink.com>
Date: Sat, Aug 29, 2015 at 10:21 PM
Subject: Re: [bitcoin-dev] Unlimited Max Blocksize (reprise)
To: Daniele Pinna <daniele.pinna@gmail.com>


Daniele --

Thanks!  I printed it and read the whole thing.  It's really a great step
forward building on the Peter R paper. The conclusions are enticing.  I'm
looking forward to understanding it in more detail and participating in the
discussion.

I find your work to be rational and scholarly without being obscure,
pedantic, or getting caught up in unimportant details.  It has a long-term
focus.


"The optimal strategy, ﬁrst analyzed in [3]"  I don't recall whether Peter
R considered constrained block space or not, but if it is considered,
simple fee-density prioritization is not necessarily optimal (it is a
knapsack problem).




On 8/29/2015 10:16 AM, Daniele Pinna wrote:

Here you go Tom!

https://www.scribd.com/doc/276849939/On-the-Nature-of-Miner-Advantages-in-Uncapped-Block-Size-Fee-Markets

Daniele Pinna, Ph.D

On Thu, Aug 27, 2015 at 12:59 AM, Tom Harding <tomh@thinlink.com> wrote:

-------------------------------------
On Thu, Jul 30, 2015 at 3:14 PM, Tom Harding via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

The blocksize limit (your "production quota") is necessary for
decentralization, not for having a functioning fee market.
But I have the hope that hitting the limit would help with getting rid
of all that special-case or at least would encourage miners to
implement their own policies.
If we can agree that hitting the limit will JUST cause higher fees and
not bitcoin to fail, puppies to die or the sky to turn purple I think
that's a great step forward in this debate.
(as said, I think it can even have positive consequences; for example,
higher fees may be just what is needed for more scalable solutions
[like payment channels] to be adopted by bitcoin companies). Hitting
the limit may produce a more healthy market, but it is true that a
market for fast confirmations already exists.

Unless we want to completely get rid of the blocksize limit (which I
would consider another debate entirely), we will eventually hit the
limit anyway. Why not now so that we can make sure the software is
completely compatible with having a limit?
Why we can hit the limit eventually but not now?

(As said, unless you think the limit should be completely removed forever).

-------------------------------------
For now, lets leave the discussion to JUST the block size increase. If
it helps - everyone should assume that their pet feature is included in
a hard fork or, if you prefer, that no other features are included in a
hard fork.

On 05/06/15 23:11, Matt Whitlock wrote:


-------------------------------------
2015-06-06 18:10 GMT+02:00 Tom Harding <tomh@thinlink.com>:

with the nLocktime solution, the copied outputs are not needed.


I did remore the constant (a "PoP" literal ascii encoded string)
because it didn't add much. The recipient will expect a pop, so it
will simply treat it as one. I did add a 2 byte version field to make
it extendable.


Noted :-)

Thank you
/Kalle


-------------------------------------
On 9/29/2015 7:02 PM, Jonathan Toomim (Toomim Bros) wrote:

If that were true then probably 20-30% of the posting here would be 
off-topic.  lol.

Russ



-------------------------------------
On Fri, Jun 19, 2015 at 09:37:49PM +0800, Chun Wang wrote:

No worries, let me know if you have any issues. You have my phone
number.

While my own preference - and a number of other devs - is full-RBF,
either one is a good step forward for Bitcoin.

-- 
'peter'[:-1]@petertodd.org
000000000000000003188926be14e5fbe2f8f9c63c9fb8e2ba4b14ab04f1c9ab
-------------------------------------
On Fri, Oct 16, 2015 at 3:26 AM, Rusty Russell via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Once more functions (specially consensus-critical functions) take
nTime explicitly as parameter instead of relying on the
library-unfriendly GetAdjustedTime(), then SetMockTime() will be less
necessary for testing. For example, see
https://github.com/jtimon/bitcoin/commit/88a35548518a27c7d24efe064e1bf4e5b3029578#diff-524ba4b43aa70d393ef51ab42a6d25f2L52

-------------------------------------
Incidentally, even once we have the "Internet of Things" brought on by 21,
Inc. or whoever beats them to it, I would expect the average home to have
only a single full node "hub" receiving the blockchain and broadcasting
transactions created by all the minor SPV connected devices running within
the house. The in-home full node would be peered with high bandwidth
full-node relays running at the ISP or in the cloud. There are more than
enough ISPs and cloud compute providers in the world such that there should
be no concern at all about centralization of relays. Full nodes could some
day become as ubiquitous on the Internet as authoritative DNS servers. And
just like DNS servers, if you don't trust the nodes your ISP creates or
it's too slow or censors transactions, there's nothing preventing you from
peering with nodes hosted by the Googles or OpenDNSs out there, or running
your own if you're really paranoid and have a few extra bucks for a VPS.

--
*James G. Phillips IV*
<https://plus.google.com/u/0/113107039501292625391/posts>
<http://www.linkedin.com/in/ergophobe>

*"Don't bunt. Aim out of the ball park. Aim for the company of immortals."
-- David Ogilvy*

 *This message was created with 100% recycled electrons. Please think twice
before printing.*

On Mon, May 25, 2015 at 10:23 PM, Jim Phillips <jim@ergophobia.org> wrote:

-------------------------------------
On Mon, Aug 10, 2015 at 08:14:08PM +0100, Hector Chu wrote:

Sure, that's a way to increase your net worth in real terms, but it only
works if your interest rate on your fiat account is greater than the
price rise in bitcoin over the same term. If you pull out a BTC today at
$300, put it in a bank account earning 3% interest for a year and then
buy $309 worth of bitcoin when the price has risen to $400 per BTC,
and only get 0.7725 of a bitcoin, that's not a winning proposition.
I'd call that earning a -22.75% rate (in bitcoin terms), while a 0%
rate would just be ending up with as many bitcoin after a year as you
started with. Note that in USD (and real) terms, in this scenario 77%
of a bitcoin is actually worth more after a year than 1 bitcoin is now.

You might get a positive rate of return on bitcoin invested today by
running an exchange or a gambling service of some sort; but I think
mostly, people are just sitting on their coins hoping they appreciate. If
so, (in my terminology at least) they're earning 0%, denominated in
bitcoin, and have a time-value of bitcoin of zero.

Cheers,
aj


-------------------------------------
Yes, I believe consensus rule changes don't need to be couple with
major releases, there's no problem that I can see in them being minor
releases if they're not ready on time for a major release.

On Wed, Sep 30, 2015 at 7:57 PM, Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Well, let's agree to disagree on these two things:

- I define "working" for a full node as verifying everything; if a node
starts skipping bits then I'd say it's not really "working" according to
its original design goals

- Saying the pre-fork behaviour is defined and deterministic is true, but
only in the sense that reading an uninitialised variable in C is defined
and deterministic. It reads whatever happens to be at that stack position:
easily defined. For many programs, that may be the same value each time:
deterministic. Nonetheless, it's considered undefined behaviour by the C
specification and programmers that rely on it can easily create security
holes.

In the same way, I'd consider a node running a script with a NOP and
reaching the opposite conclusion from other nodes to be a case of undefined
behaviour leading to a non-fully-working node.

But these are arguments about the semantics of words. I think we both know
what each other is getting at.

On Mon, Oct 5, 2015 at 1:23 PM, Jeff Garzik <jgarzik@gmail.com> wrote:

-------------------------------------
On 7/23/2015 10:57 PM, Dave Scotese via bitcoin-dev wrote:

Just a point about terminology:

Roadmap - A plan of proposed changed used to meet some sort of goal.  If 
the goal is increased scaling then you list a series of changes that 
need to be done to achieve that goal.

Baseline - That is the "If We Do Nothing" Analysis.  Each proposed 
change will generally have one or more alternatives which are compared 
to the "baseline."

those would be 2 different documents.

Russ






-------------------------------------
Oh, no, sorry, it also covers bip62.

On Fri, Apr 24, 2015 at 10:55 AM, Jorge Timón <jtimon@jtimon.cc> wrote:


-------------------------------------
This topic is straying from Bitcoin development into general Bitcoin
governance, policy, or other meta-issues.

We have now the new bitcoin-discuss mailing list now, specifically for
these more free-flowing topics:

https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-discuss

Please take further discussion of this thread to that forum.

Thank you,

The bitcoin-dev moderation team


On Sat, Nov 14, 2015 at 1:45 PM, Peter R via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Isn't this all backward? The "authority" component of the URL should identify the chain, and the "path" component should identify the particular block, tx, or address in that chain.

So instead of:

blockchain://tx/ca26cedeb9cbc94e030891578e0d2b688a28902114f6ad2f24ecd3918f76c17f?chain=000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f

It should be:

blockchain://000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f/tx/ca26cedeb9cbc94e030891578e0d2b688a28902114f6ad2f24ecd3918f76c17f

And I would agree with allowing well-known chains to register a name, to be used as an alternative to the literal, hash syntax:

blockchain://bitcoin/tx/ca26cedeb9cbc94e030891578e0d2b688a28902114f6ad2f24ecd3918f76c17f


On Tuesday, 1 September 2015, at 4:49 pm, Marco Pontello wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256



On 23 July 2015 10:19:59 GMT-04:00, slurms--- via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

Note how due to bandwidth being generally asymetric your findings are probably optimistic - you've measured download capacity. On top of that upload is further reduced by the fact that multiple peers at once need to be sent blocks for reliability.

Secondly you're measuring a network that isn't under attack - we need significant additional margin to resist attack as performance is consensus-critical.

-----BEGIN PGP SIGNATURE-----

iQE9BAEBCAAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJVsRCj
AAoJEMCF8hzn9Lnc47AIAIQbznavjd2Rbqxeq5a3GLqeYoI4BZIQYqfWky+6OQtq
yGRKaqPtGuES5y9L0k7efivT385mOl87PWnWMy61xxZ9FJgoS+YHkEx8K4tfgfA2
yLOKzeFSar2ROCcjHYyPWa2XXjRbNmiLzfNuQyIBArg/Ch9//iXUUM+GG0mChF5k
nUxLstXgXDNh5H8xkHeLi4lEbt9HFiwcZnT1Tzeo2dvVTujrtyNb/zEhNZScMXDc
UOlT8rBLxzHlytKdXt1GNKIq0feTRJNbreBh7/EB4nYTT54CItaaVXul0LdHd5/2
kgKtdbUdeyaRUKrKcvxiuIwclyoOuRQp0DZThsB262o=
=tBUM
-----END PGP SIGNATURE-----


-------------------------------------


I guess we sort of disagree here, perhaps my word “strength” was not the right word. Yes, running 6000 vs 7000 nodes makes no difference for the network strength, but (a) running 50 nodes vs 5000 does make a difference. I would love to see how the number of nodes drop if companies like blockcypher turn off their servers. Obviously it would not go 50. (b) running different clients (if blockcypher runs non-reference-bitcoinD client) makes the network less open wide-spread bugs


I feel we are really derailing the original topic btw  :-)






-------------------------------------
They do so by not building on larger blocks
On Jun 23, 2015 9:31 PM, "Raystonn" <raystonn@hotmail.com> wrote:

-------------------------------------
Added back the list, I didn't mean to reply privately:

Fair enough, I'll try to find time in the next month or three to write up
four plausible future scenarios for how mining incentives might work:

1) Fee-supported with very large blocks containing lots of tiny-fee
transactions
2) Proof-of-idle supported (I wish Tadge Dryja would publish his
proof-of-idle idea....)
3) Fees purely as transaction-spam-prevention measure, chain security via
alternative consensus algorithm (in this scenario there is very little
mining).
4) Fee supported with small blocks containing high-fee transactions moving
coins to/from sidechains.

Would that be helpful, or do you have some reason for thinking that we
should pick just one and focus all of our efforts on making that one
scenario happen?

I always think it is better, when possible, not to "bet on one horse."


On Tue, May 12, 2015 at 10:39 AM, Thomas Voegtlin <thomasv@electrum.org>
wrote:




-- 
--
Gavin Andresen
-------------------------------------
On 07/03/15 16:53, Mem Wallet wrote:

Hi!

As an author of BIP44 I don't think that you should use BIP44 for this
and a new BIP number should be allocated. To me it does not make much
sense to create GPG key hierarchy per Bitcoin account, but rather create
a GPG key hierarchy per device/master seed.

I am currently in process of implementing a SignIdentity message for
TREZOR, which will be used for HTTPS/SSH/etc. logins.

See PoC here:
https://github.com/trezor/trezor-emu/commit/9f612c286cc7b8268ebaec4a36757e1c19548717

The idea is to derive the BIP32 path from HTTPS/SSH URI (by hashing it
and use m/46'/a'/b'/c'/d' where a,b,c,d are first 4*32 bits of the hash)
and use that to derive the private key. This scheme might work for GPG
keys (just use gpg://user@host.com for the URI) as well.

-- 
Best Regards / S pozdravom,

Pavol Rusnak <stick@gk2.sk>


-------------------------------------
I suppose whether the wallet devs want to implement the soft fork or not is
irrelevant. They only need to indicate if they are ready i.e. they've
tested the new soft fork, hard fork or feature and validated that it
doesn't break their nodes or wallet software.
On Dec 9, 2015 6:40 AM, "Chris Priest" <cp368202@ohiou.edu> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I fully agree and support this idea.

Some recent discussion on social media which touches on this very
subject of bitcoin and sourceforge.... (I include nmap and gittorrent
as well because those seem relevant, imho)

https://twitter.com/jgarzik/status/607750046021357568

https://twitter.com/nmap/status/608418994236891137

https://twitter.com/ktorn/status/607818378531631106

https://twitter.com/ktorn/status/607822900331020288

On 06/14/2015 03:12 AM, Warren Togami Jr. wrote:
dgy_ads_and_installer
43127.DBnVxmfOIh%401337h4x0r/#msg34192607
- --------

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVffgcAAoJEGxwq/inSG8COOsH/jC5TAjec1ridg9Ww/1+SW26
QvTaZ79PrK4+/5rvt3qXtCicOidGLTGpk/ixrgVN64nOiquaQm8JM/BrOrtZbYN0
/lXjhR6N8AEKYYvtjCQdD/JjNZ8Z0QvRZ4+XKUblBagm4BkRt4OtaVkctechscbM
WiMh+SfUPPlGiuucotiBFliF4TprFTCw0w/+WY521yKE5qgTPc6ZKBHI5TzYROoF
aAz7i6GlAZR0qlbV91IzakszZWF/Im6KHG30CYbU4eTb6Ic9tVHogC2EuW2zePd3
NxRXE4M0FunnVX61Eg3Bglm73h6SuzsL9x79Ckp0UXpZ8sJ7+mYCDKTZSUEWeJs=
=Xje2
-----END PGP SIGNATURE-----


-------------------------------------
Well, with utxo commitments at some point maybe is enough to validate the
full headers history but only the last 5 years of ttansaction history
(assuming utxo commitments are buried 5 years worth of blocks in the past).
This scales much better than validating the full history and if we get a 5
year reorg something is going really wrong anyway...
Maybe after validating the last 5 years you also want to validate the rest
of the history backards to get the "fully-full node" security.
Of course 5 years it's just an arbitrary number: 2 or maybe even 1 would
probably be secure enough for most people. I've referred to this idea as
"hard checkpoints" or "moving the genesis block forward" in the past.
On Sep 18, 2015 4:18 PM, "Rune Kjær Svendsen" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Mike,

You can not consider the outcome resulting by replace-by-fee fraudulent, as it could be the world as observed by some.

Some others might have seen the replaced transaction, but that only indicates for sure that the signer is fraudulent.

What should a node do that really cares of good reputation? Ignore both to be on the safe side?

Tamas Blummer

-------------------------------------
On 2015-05-11 10:34, Peter Todd wrote:

Same way ghash.io was banned from the network when used Finney attacks
against BetCoin Dice.

As Andreas Antonopoulos says, if any of the miners do anything bad, we
just ban them from mining. Any sort of attack like this only lasts 10
minutes as a result. Stop worrying so much.

https://youtu.be/ncPyMUfNyVM?t=20s




-------------------------------------
Hello,

Although Electrum clients connect to several servers in order to fetch
block headers, they typically request address balances and address
histories from a single server. This means that the chosen server knows
that a given set of addresses belong to the same wallet. That is true
even if Electrum is used over TOR.

There have been various proposals to improve on that, but none of them
really convinced me so far. One recurrent proposal has been to create
subsets of wallet addresses, and to send them to separate servers. In my
opinion, this does not really improve anonymity, because it requires
trusting more servers.

Here is an idea, inspired by TOR, on which I would like to have some
feedback: We create an anonymous routing layer between Electrum servers
and clients.

* Each server S publishes a RSA public key, KS
* Each client receives a list of available servers and their pubkeys
* For each wallet address, addr_i, a client chooses a server S_i, and a
RSA keypair (K_addr_i, k_addr_i)
* The client creates a list of encrypted requests. Each request contains
addr_i and K_addr_i, and is encrypted with the pubkey KS_i of S_i
* The client chooses a main server M, and sends the list of encrypted
requests to M
* M dispatches the client's requests to the corresponding servers S_i
(without the client's IP address.)
* Each server decrypts the requests it receives, performs the request,
and encrypts the result with K_addr_i
* M receives encrypted responses, and forwards them to the client.
* The client decrypts the encrypted response with k_addr_i

What do you think? What are the costs and benefits of such an approach?

(Note: this will not work if all servers, or a large fraction of them,
are controlled by the same entity that controls M)


Thomas

-------------------------------------
On Sat, Jun 27, 2015 at 3:19 AM, Venzen Khaosan <venzen@mail.bihthai.net>
wrote:


You're not taking speculative cycles into account. For most of 2013 the
price was in the $100 range. Adoption as a store-of-value is what
determines the price over the long term, as with any monetary commodity.
-------------------------------------

I don't believe any posting by Mr. Hearn warrants any actions by some 
undefined community.  Since I disagree with TaoEffect.com (aka "PRIVATE 
REGISTRANT") he has no consensus.  lol


Russ



-------------------------------------
I would support a dynamic block size increase as outlined. I have a few
questions though.

Is scaling by average block size the best and easiest method, why not scale
by transactions confirmed instead? Anyone can write and relay a
transaction, and those are what we want to scale for, why not measure it
directly?

I would prefer changes every 2016 blocks, it is a well known change and a
reasonable time period for planning on changes. Two weeks is plenty fast,
especially at a 50% rate increase, in a few months the block size could be
dramatically larger.

Daily change to size seems confusing especially considering that max block
size will be dipping up and down. Also if something breaks trying to fix it
in a day seems problematic. The hard fork database size difference error
comes to mind. Finally daily 50% increases could quickly crowd out smaller
nodes if changes happen too quickly to adapt for.





CABsx9T3-zxCAagAS0megd06xvG5n-3tUL9NUK9TT3vt7XNL9Tg@mail.gmail.com>
wrote:
and
1.5.
size
target
size-independent
timing
again.
------------------------------------------------------------------------------
-------------------------------------
On Mon, Sep 28, 2015 at 12:48:57PM +0200, Mike Hearn wrote:

Hmm? You didn't quote any of my email, so I'll remind you what I did say
we had consensus about:

    2) We have consensus on the semantics of the CLTV opcode

and

    3) We have consensus that Bitcoin should adopt CLTV

    The broad peer review and discussion that got #6124 merged is a clear
    sign that we expect CLTV to be eventually adopted.  __The question isn't
    if CLTV should be added to the Bitcoin protocol, but rather when.__

(emphasis mine)

Both those statements of consensus are *not* about how CLTV is to be
deployed. I did discuss deployment later:

    6) We have the __necessary consensus__ to deploy CLTV via IsSuperMajority()

    The various "nVersion bits" proposals - which I am a co-author of - have
    the primary advantage of being able to cleanly deal with the case where
    a soft-fork fails to get adopted. However, we do have broad consensus,
    including across all sides of the blocksize debate, that CLTV should be
    adopted. __The risk of CLTV failing to get miner adoption, and thus
    blocking other soft-forks, is very low.__

I probably could have worded this section a bit more clearly; when I say
"necessary consensus" I'm referring to the consensus required for a
soft-fork deployment. At minimum a simple majority of hashing power -
your approval isn't required.

For a safe soft-fork, we'd like a super majority of miners to be on
board. For a IsSuperMajority() soft-fork - as opposed to nVersion bits -
we also need the probability of the soft-fork being rejected to be very
low. To achieve that, having consensus that CLTV is a good idea is the
best situation to be in. But that's not to say that a few dissenting
voices should be seen as a blocker to progress - rather is just makes
the deployment a bit more risky, being a sign that the consensus may
change in the future, with the soft-fork being later rejected. For
example strong objections by a respected Bitcoin developer who has made
significant contributions to the consensus codebase and protocol
development would be a strong sign that a IsSuperMajority() soft-fork
might fail, and deployment via nVersion bits is probably a better
approach. Fortunately we're not in that situation.

Hard-forks are a very different situation, with significantly more need
for very broad consensus, but that's been well discussed elsewhere.


I have three questions to you:

1) Do you agree that CLTV should be added to the Bitcoin protocol?

Ignoring the question how exactly it is added, hard-fork or soft-fork.


2) Will you add a IsSuperMajority() CLTV soft-fork to Bitcoin XT if it
   is added to Bitcoin Core?

If you refuse to do this the risk of the soft-fork is increased a bit,
although miner support for XT has remained extremely low, and the 95%
switch-over threshold has a significant margin for error. (there's a 75%
threshold to consider as well, however as XT has adopted my pull-req
#5000 - Discourage NOPs reserved for soft-fork upgrades - those miners
will only produce valid blocks under CLTV rules)


3) Will you add soft-fork detection to bitcoinj, to allow SPV clients to
   detect advertised soft-forks and correctly handle them?

Notably, if you do this your objections against soft-forks will be met,
as the behavior of a SPV client with soft-fork detection during a
soft-fork will be identical to that client during a hard-fork. In
particular, the SPV client will correct reject invalid blocks, and
continue to follow only the longest valid chain. (modulo unadvertised
forks of course, an inherently unavoidable problem with the SPV security
model) Secondly, that code should also detect forks it doesn't know
about - as is done in Bitcoin Core already - and warn the user.

-- 
'peter'[:-1]@petertodd.org
00000000000000000d74f5def1087f3ec1571cb468e471e71f96063253988c78
-------------------------------------
On Fri, Jun 12, 2015 at 10:37 AM, Tier Nolan <tier.nolan@gmail.com> wrote:


Note that we've been above the 75% threshold since june 7th (before Peter's
main was sent).

-- 
Pieter
-------------------------------------
On 2015-09-01, at 12:56 AM, Peter Todd via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote

I don't believe anyone is arguing otherwise.  Miners with a larger fraction of the network hash rate, h/H, have a theoretical advantage, all other variables in the miner's profitability equation held constant.  

Dpinna originally claimed (unless I'm mistaken) that his paper showed that this advantage decreased as the block reward diminished or as the total fees increased.  This didn't seem unreasonable to me, although I never checked the math.  

Best regards,
Peter


 

-------------------------------------
I think we'll just have to agree to disagree on this one. I've implemented
BIP70 a couple of times now and didn't find it to be difficult. I know you
had odd problems with the C# protobuf implementation you were using but
library bugs can happen for any kind of programming.

I forgot to mention the other reason it's done this way. One of the driving
goals of BIP70 was to support the TREZOR and similar devices. For hardware
wallets, it's critical to keep the amount of code they need to run as small
as possible. Any bugs in the code there can cause security holes and lead
to the device being hacked.

Doing it the way you suggest would mean the secure code would have to
contain complex and bug-prone text parsing logic as well as a full blown
HTTP and SSL stack, that requires not only X.509 handling but also lots of
other stuff on top. It'd increase cost, complexity and decrease security
quite a bit.

Whilst I appreciate if your platform provides a scripting-like API and
nothing low level it might seem easier to use JSON+HTTPS, that isn't the
case for one of the primary design targets.



On Wed, Jan 28, 2015 at 6:04 PM, Nicolas Dorier <nicolas.dorier@gmail.com>
wrote:

-------------------------------------
On Thu, Mar 26, 2015 at 9:26 PM, Tom Harding <tomh@thinlink.com> wrote:

Great, that can be accomplished by simply encoding an expiration into
the address people are using and specifying that clients enforce it.


-------------------------------------
On Wed, Jul 22, 2015 at 10:32 PM, Sriram Karra <karra.etc@gmail.com> wrote:


Peter, sorry,  scratch that.
-------------------------------------
On Fri, Aug 7, 2015 at 1:33 PM, Jorge Timón <jtimon@jtimon.cc> wrote:

It is frustrating to answer questions that we answered months ago,
especially when I linked to these in response to your recent "increase
advocates say that not increasing the max block size will KILL BITCOIN"
false claim:
  http://gavinandresen.ninja/why-increasing-the-max-block-size-is-urgent
  https://medium.com/@octskyward/crash-landing-f5cc19908e32

Executive summary: when networks get over-saturated, they become
unreliable.  Unreliable is bad.

Unreliable and expensive is extra bad, and that's where we're headed
without an increase to the max block size.

RE: the recent thread about "better deal with that type of thing now rather
than later" :  exactly the same argument can be made about changes needed
to support a larger block size-- "better to do that now than to do that
later."  I don't think either of those arguments are very convincing.


-- 
--
Gavin Andresen
-------------------------------------
I'd like to offer that the best practice for the shared wallet use case
should be multi-device multi-sig.  The mobile has a key, the desktop has
a key and a third-party security oracle has a third key.  The oracle
would have different security thresholds for countersigning the mobile.

This way you can have the same overall wallet on all devices, but
different security profiles on different keys.

That said, I do agree that mnemonic phrases should be portable, and find
it unfortunate that the ecosystem is failing to standardize on phrase
handling.

On 2015-03-11 04:22 PM, Mike Hearn wrote:

-- 
devrandom / Miron


-------------------------------------
On Aug 11, 2015 9:37 PM, "Michael Naber" <mickeybob@gmail.com> wrote:

question at hand is whether we should constrain that limit below what
technology is capable of delivering. I'm arguing that not only we should
not, but that we could not even if we wanted to, since competition will
deliver capacity for global consensus whether it's in Bitcoin or in some
other product / fork.

You didn't answer the 2 questions...
Anyway, if we don't care about centralization at all, we can just remove
the limit: that's what "technology can provide".
Maybe in that case it is developers who move to a decentralized
competitor...

network -- which is a network where all the participating nodes are aware
of and agree upon every transaction. Constraining Bitcoin capacity below
the limits of technology will only push users seeking to participate in a
global consensus network to other solutions which have adequate capacity,
such as BitcoinXT or others. Note that lightning / hub and spoke do not
meet requirements for users wishing to participate in global consensus,
because they are not global consensus networks, since all participating
nodes are not aware of all transactions.
pushes people to other altcoins, no?
hitting the limit and then fees rising as a consequence?
-------------------------------------
Agree with everything you said.  Spot on observations on all counts.
Thank you for speaking up.

Adam

On 1 June 2015 at 13:45, Jérôme Legoupil <jjlegoupil@gmail.com> wrote:


-------------------------------------
On Mon, Jun 22, 2015 at 8:32 PM, Jean-Paul Kogelman <jeanpaulkogelman@me.com


The activation or not rule is purely timestamp based.  Blocks with a
timestamp less than 1452470400 have a limit of 1MB.  There could be an 8MB
block followed by a block that is limited to 1MB.
-------------------------------------
On Mon, Sep 21, 2015 at 03:51:29PM +0200, gb wrote:

Nah, we can always change the scheduling later... But let's first try it out with one time.

W

-------------------------------------



Freundliche Grüsse
---
Jonas Schnelli

°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°
include7 AG
Jonas Schnelli
Mattengasse 27
CH-8005 Zürich
Switzerland
Office: +41 44 500 16 70

Mail: jonas.schnelli@include7.ch
Web: www.include7.ch
V-Card: www.include7.ch/js.vcf
°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°

ACHTUNG
Bitte senden sie uns keine sensitiven Daten in unverschlüsselten E-Mails.
Verwenden Sie hierzu folgenden Link:
https://include7.ch/contact/secureform

°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°


Hi Justus

Is there a reason why Bitcoin-Core and Breadwallet (iOS) is missing?

Thanks
—
</Jonas>
-------------------------------------
Hi Mike,

The reason why I would like to extend BIP70 is because it is currently
not being used in transactions between end-users. BIP70 works very well
in B2C situations, where users buy products from a website. However,
end-users still share Bitcoin addresses.

Before BIP70 was written, I had proposed "Signed URIs", where the
signature is a public proof that a payee requested a payment. This is
one of the main benefits of BIP70, and I still want to bring it to
user-to-user transactions.

I believe one of the main barriers to BIP70 adoption is that bitcoin:
URIs have been extended in a way that requires the request to be hosted
on a webserver. You may complain about the lack of store-and-forward
network, despite the apparent simplicity of creating one such network.
However, that does not mean we should absolutely do things that way and
wait until such a network exists.

Bitcoin addresses do not require a webserver. If we want to build
something that competes with that, we should have at least that level of
convenience.

EC signatures are short, and they can be shared by copy-paste, or added
to a bitcoin: URI. This is a features of my "Signed URIs" proposal, that
was lost in BIP70. Indeed, signed URIs were self-contained. A serialized
payment request can also be made very short, if it is signed by a EC
key, and if it does not include the chain of certificates. Such a
"lightweight request" can be base58-encoded, and easily shared by
copy-paste, or passed in a bitcoin: URI.

Size is another reason why I proposed to use DNSSEC in BIP70 (the first
reason is that the subdomain is signed by the domain, not by an external
CA). Indeed, DNSSEC provides a canonical way to download the chain of
signatures needed to verify a record. Thus, the chain of signatures does
not need to be included in the payment request; it can be downloaded and
archived by the verifier.

Now, I understand that SSL vertificates have distinct advantages over
DNSSEC; for example, CA-signed SSL certificates have a legal value,
which is important for dispute resolution.

Would it be possible to create the same kind of "lightweight payment
requests" using SSL certificates? Probably, if the final signing key is
a EC key, and if the payment request does not include the whole chain of
certificates. (However, that would require an additional infrastructure
to publish the chain of certificates, and I do not think that x509
certification path are unique, which makes things more complicated, but
not impossible).

Sorry if I did not answer point-by-point to your post. I felt that I
failed to explain one of the reasons why I want to use DNSSEC in the
validation of payment requests.

Thomas




Le 14/07/2015 13:45, Mike Hearn a écrit :

-------------------------------------
I don't have strong opinion @ block size topic.

But if there'll be a fork, PLEASE, include SIGHASH_WITHINPUTVALUE (
https://bitcointalk.org/index.php?topic=181734.0) or its alternative. All
developers of lightweight (blockchain-less) clients will adore you!

slush

On Thu, May 7, 2015 at 12:12 AM, Matt Corallo <bitcoin-list@bluematt.me>
wrote:

-------------------------------------
If this is widely deployed + enabled, what is the impact to current wallets
in use?


On Fri, Aug 21, 2015 at 12:46 AM, Matt Corallo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Peter Todd via bitcoin-dev 於 2015-09-17 18:44 寫到:


I think this is the cleanest way to implement the maturity requirement. 
I understand why we need maturity, However, requiring 100 block maturity 
will unfortunately make the system much less appealing since the 
recipient may not like it. A fill-or-kill tx may still be used as the 
initial funding tx to the Lightning Network, as long as the counterparty 
is willing to take the extra risk.

Actually, a fill-or-kill tx is slight safer than a coinbase tx, 
depending on the difference between the absolute kill time and actual 
confirmation time. In a re-org, an orphaned coinbase tx is permanently 
invalidated and has no hope to be included again. However, an orphaned 
fill-or-kill tx may still be confirmed by another miner. If there is 
still a few days until the absolute kill time, a fill-or-kill tx is 
basically as safe as a normal tx.

With possibility of re-org and unpredictable block interval in mind, 
height-based fill-or-kill is not very useful since it is difficult for 
users to determine the actual kill time. If we could abolish the idea of 
height-based fill-or-kill, the resolution of time-based fill-or-kill 
might be improved.


------------------------------------

Chun Wang 於 2015-09-17 18:33 寫到:

The fill-or-kill system is totally optional, using a bit flag in tx 
nVersion to indicate. Everything should be fine unless you are also 
messing with the nVersion


-------------------------------------

Btc Drak 於 2015-09-17 15:12 寫到:

What I'm describing is to implement fill-or-kill as consensus rule. 
Certainly, we could implement it at the P2P network level: everything is 
the same as I described, but the nLockTime2 and nKillTime are for 
reference only and tx validity depends only on the nLockTime. Benevolent 
miners should drop the tx after the suggested kill time but there is no 
guarantee


-------------------------------------

I made a mistake in this example:


The correct nLockTime2 for this example should be 210000/4 = 52500

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

This is very well done.

Have you seen this discussion that I started regarding BIP 63?

https://bitcointalk.org/index.php?topic=1083961.0

I have no response from Peter Todd back on it other than "my time is
better spent focusing on more fundemental issues" and "I've also got
no-one interested in funding stealth address development right now,"
when several people (myself included) offered to send donations to see
the BIP (63) advance, no donation address was posted, so... waiting
for him to act on that.

I'm definitely supportive of seeing what you've written up here as
Reusable payment codes move to draft in https://github.com/bitcoin/bips
When you can, please write up something on bitcointalk as well.


On 04/24/2015 01:00 PM, Justus Ranvier wrote:
ediawiki
- --------
One dashboard for servers and applications across Physical-Virtual-Cloud

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVgE4fAAoJEGxwq/inSG8CjgkH/i0aX4aJaOjrbI2xzWbPeL1T
/APSvSqV0D610ljbw/MuRRFVagnK3lCs73fYolKw9uFG0cnwhIWJ53mCqPWhM5nL
kIejDTHr9jQ2tbXrU2L481Oat1Z6vtdQj7LolXFfD3Ktqz+sqp//gBaC9EEZ5nOq
4oz71Am58pf8+XGhtJk0+4XDXzFNd71bKKY+nMf9f3bwqNX93jHiF48hXwijFPC4
MOZmYRh3Sf5LAVP5p1JY3aJRQv4M/W0L2RDC+GW8Ol997etQSGGLhESihNNPw1m8
GEqJLBmUBkavzsRpZ009czfzL7EiCwsMbOrVw918o2Y9NnVpY9a9cBNB+UJgCmk=
=wAGz
-----END PGP SIGNATURE-----


-------------------------------------

What is the incentive for someone with high level technical skills to 
spend all their time developing and testing code?  Especially since the 
code is generally the boring task of "fixing the plumbing" and won't 
benefit the developer directly ... except they will be blamed if 
something goes wrong.

Russ



-------------------------------------
Bitcoin has a major crossroad ahead regarding a suitable platform for the
average non technical main stream user. Until now the majority of the
available solutions were at two extremes, or DIY your security and privacy
*OR* let a 3rd party service do it for you. The DIY solution is obviously
not scalable, but it seems that 3rd party solutions are not scalable as
well. If we compare for a second a 3rd party services with traditional
banks, it seems banks have two major "advantages" over them. Entry costs
for creating a bank are HUGE so a priori very few people can actually
create such a service, second, their physical and IT security
infrastructure are heavily regulated which insures a minimum of security
level to the end user (and even so money is stolen frequently). Entry costs
and regulation do not exist in the bitcoin space, meaning two programers in
their spare time can create a wallet/ platform and the non technical end
user cannot know if his money is safe, did they hire the right security
expert, did they invest enough in protecting and backing up his keys, etc.

Many services tried to tackle those problems with multisig (2 of 2 and 2 of
3) to create a syntactical 2 factor authentication/ authorisation mechanism
but in reality those solutions didn't really increase security and their
failure point is always a single device. Coupling those said problems with
the fact that bitcoin transactions are irreversible and are a scarce
commodity, trying to insure them the way our money is insured by the
government when we deposit it in the bank becomes a huge problem. Premiums
will be very high and will only grow as the appetite of hackers to steal
coins increase.

I personally believe we have the tools for creating a platform that is both
secure and private but most importantly it does it in a decentralised way.
Creating true 2 (or more) factor authentication/ authorisation schemes can
improve dramatically personal security to a point where 3rd party wallet
services will become a thing of the past. Succeeding in that will mean the
next billion non technical bitcoin users will have a platform to use
securely and a base line for building cool services on top.

Alon Muroch
bitcoinauthenticator.org

-------------------------------------


On 05/29/15 23:48, Gavin Andresen wrote:

If, for example, the majority of miners are in China (they are), and
there is really poor connectivity in and out of China (there is) and a
miner naively optimizes for profit, they will create blocks which are
large and take a while to relay out of China. By simple trial-and-error
an individual large miner might notice that when they create larger
blocks which fork off miners in other parts of the world, they get more
income. Obviously forking off 50% of the network would be a rather
extreme situation and assumes all kinds of simplified models, but it
shows that the incentives here are very far from aligned, and your
simplified good-behavior models are very far from convincing.


The disadvantage is small with 1MB blocks, but already non-zero. 20MB
blocks are much, much worse (lots of things here dont scale linearly,
even just transfer over a high-packet-loss-link). I mentioned this in my
original email as something which doesnt make me comfortable with 20MB
blocks, but something which needs simulation and study, and might
actually be just fine!


Defaults? Dumb designs? Most miners just use the default 750K blocks, as
far as I can tell, other miners probably didnt see transactions relayed
across several hops or so, and a select few miners are doing crazy
things like making their blocks fit in a single packet to cross the gfw,
but that is probably overkill and not well-researched.


Do you have convincing evidence that at 20MB miners will be able to
break even on transaction fees for a long time? (The answer is no
because no one has any idea how bitcoin transaction volumes are going to
scale, period.)


Sure, do you have a value of hashpower which is "secure enough" (which
is a whole other rabbit hole to go down...).


Oh? You mention at http://gavinandresen.ninja/bigger-blocks-another-way
that "I struggle with wanting to stay true to Satoshi’s original vision
of Bitcoin as a system that scales up to Visa-level transaction volume".
That is in direct contradiction.


"it is not a panacea", but everyone in the community seems to be taking
it as one. You've claimed many times that many of the big
webwallet/payment processors/etc have been coming to you and saying they
need bigger block sizes to continue operating. In reality, they dont, it
just makes it easier.

Matt


-------------------------------------
On Mon, Jun 29, 2015 at 5:40 AM, Luke Dashjr <luke@dashjr.org> wrote:

Even accepting the premise that policy is pure local fiat, the
conclusion doesn't follow for me. BIPs about best practices or
especially anything where interop or coordination are, I think,
reasonable uses of the process.

E.g. you might want to know what other kinds of policy are in use if
you're to have any hope of authoring transactions that work at all!

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Right. Wallets are covering malleability in acceptable ways. Normal
user to user payments aren't (or at least shouldn't be) affected by
malleability.

Problems appear in second level and third level malleability, when
Alice sends txB to Bob which spends from txA which is unconfirmed. If
txA changes txid, txB becomes useless and invalidates Alice's payment.
Looking at scriptPubKeys instead of transaction IDs doesn't help in
this context.

This is the reason why some type of contracts are not workable or not
100% safe. One can't pre-sign a refund transaction with an nLockTime
in the future: the payer will provide the funding transaction ID from
which the refund tx will spend, but if the transaction ID of the
funding transaction is affected by malleability (third party
malleability, since the signer doesn't have interest to do so) the
refund tx becomes useless.

On 11/5/2015 10:25 PM, Jorge Timn via bitcoin-dev wrote:
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBCAAGBQJWO9w7AAoJEIN/pSyBJlsR2BsH+gMwxJ/isiWfJF12LJ9s4wat
Bd/K2Ld+Lyk5BRs+6rQzv5NeeYjYC3FtNFV+z1Z1dMDd752cUfEZqQA9dt9nl0E7
BEia3RSFii1k2L/4xwiIWKZM20qoiykou41J56GZrJa9SoP+9kg8iLq8CokahakP
PLjfBrTylJBsgq34foPPaOH9ckOa/RJpx3WHrRFTPhxbTCm1Ezv6jAZmYr9tTi1h
afzU0YayzLUIb9xH8vfODY2qMJ91uguTUZYCGuopDYhom5GMw8zss0kG5FdEZrEQ
Z7srQmKQ0SRMtiSlg6lg3d8TS5Mv1gIp1HcL+gtMFroi38pJS8dXT65nGjg0Epc=
=ZhVA
-----END PGP SIGNATURE-----

-------------------------------------
What do you gain by making PoPs actually valid transactions? You could for
example change the signature hashing algorithm (prepend a constant string,
or add a second hashing step) for signing, rendering the signatures in a
PoP unusable for actual transaction, while still committing to the same
actual transaction. That would also remove the need for the OP_RETURN to
catch fees.

Also, I would call it "proof of transaction intent", as it's a commitment
to a transaction and proof of its validity, but not a proof that an actual
transaction took place, nor a means to prevent it from being double spent.



On Sat, Jun 6, 2015 at 4:35 PM, Kalle Rosenbaum <kalle@rosenbaum.se> wrote:

-------------------------------------
Tier Nolan via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
writes:

Indeed.  There are three believable failure possibilties:

1) You don't implement the rule at all, and don't set the bit.
2) You implement it and set bit, but think some valid block is invalid.
3) You implement it and set bit, but think some invalid block is valid.

#1 is by far the most common, and the proposal is designed so they
*always* get ~2 weeks warning before those drop to SPV security.

Assuming the mining majority isn't buggy (otherwise, it's arguably not a
bug but a feature!) #2 is the worst case: some miners fork off and don't
rejoin.

So there is a slight advantage in doing this early: those buggy miners
no longer contribute to the 95% threshold.  But that's outweighed IMHO
by:

1) We would need another delay at 75% so #1 nodes can upgrade.

2) The new feature won't be exercised much before impliciation, since
   it's useless before then, so it might not find bugs anyway.

In conclusion, I'm not convinced by the extra complexity.

Cheers,
Rusty.

-------------------------------------
On Fri, Jun 19, 2015 at 5:45 AM, Dr Adam Back <adam@cypherspace.org> wrote:


Someone might find it more convenient to consume that in the form of text
instead:
http://diyhpl.us/wiki/transcripts/bitcoin-adam3us-fungibility-privacy/

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
You don't need to ask permission for testnet. Here is one with 100MB blocks:

https://github.com/pstratem/bitcoin/tree/testnet4
On Jun 24, 2015 11:06 PM, "Pindar Wong" <pindar.wong@gmail.com> wrote:

-------------------------------------
On Sat, Aug 8, 2015 at 2:37 PM, Thomas Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Adam, I think he means a multisig escrow transaction where the escrow
is trusted by both parties, and other examples like that.
But I don't see how that is relevant, allowing trust to be involved in
different ways is a feature, but it's optional.
I think the point "you don't need to trust anyone to use Bitcoin" remains.

-------------------------------------
Hi again

I've built a proof-of-concept for Proof of Payment. It's available at
http://www.rosenbaum.se:8080. The site contains links to the source code
for both the server and a Mycelium fork as well as pre-built apk:s.

I'm still very interested in feedback on this, so please let me know what
you think.

Stuff that has come up so far, and my answers:

* Some people think it's too complicated. I disagree. Using transactions as
the data structure actually makes it simple to implement both on the server
and in wallets. Just use existing wallet software to sign and verify PoPs.

* Other ideas on Proof of Payment use a single key from the proven
transaction, for example the first key from the first input of the
transaction. This is problematic when multisig and other P2SH transactions
are used. I also think that it's necessary to use *all* credentials used
for the transaction. Otherwise we wouldn't be sure that the sender actually
have all the needed credentials.

* Another suggestion is that a payment request from BIP70 is used as proof.
That is possible, but it's reusable which makes it inappropriate to send
over networks; If it is stolen somewhere, anyone can use it as many times
they like. As stated in BIP70, the payment request is suitable for dispute
resolution, more like a receipt. On the other hand, I think that PoP would
fit nicely into the workflow of BIP70: a) Read a url for the PoP request,
b) get the (possibly signed) PoP request. c) send the PoP through http POST
to the URL in the PoP request, d) profit!

* A thought of my own: The txid used in the PoP output is not strictly
necessary. It's more of a convenience for the verifier of the PoP. Without
it, the verifier would need to lookup the transaction based on the inputs
of the PoP,

Regards,
Kalle Rosenbaum

2015-03-14 19:16 GMT+01:00 Kalle Rosenbaum <kalle@rosenbaum.se>:

-------------------------------------
On 7/22/2015 9:52 AM, Pieter Wuille via bitcoin-dev wrote:

Count me among those who see allowing bitcoin to become 
space-constrained, without technical reason, as a dramatic change. 
Especially when the reasons cited in support are

  - Various species of vaporware
  - Amateurish economic thinking surrounding fees
  - "We don't support it because not everyone supports it because we 
don't support it because ..." infinite descent



-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.11.2rc1 are now available from:

    https://bitcoin.org/bin/bitcoin-core-0.11.2/test/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.11.2rc1

This is a new minor version release, bringing bug fixes, the BIP65 (CLTV)
consensus change, and relay policy preparation for BIP113.

Preliminary release notes for the 0.11.2 release can be found here:

    https://github.com/bitcoin/bitcoin/blob/0.11/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.11.2.

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWQHgdAAoJEHSBCwEjRsmmkZkH/joklzUWXNCS/CKjfhnDaSAL
kTuGpcBPcmGyLZ+n7YHIwXKi5Jjuy91ADbYKUQHtOI5oDK+5XY0SD5YDfQv+jx8a
m3J5rxePV6VXcXKtNURXRmmk71zGhIZvZ0ynUlgLqvP7WFM+FcH5BJF2sk2amFlK
2WIzJapJMXzOyYehb9ISb2qXtuSGDyevpfeDJVMNIqoQekS1r8jOPXJiT66G4HZZ
SvUMPZAjgOtjKUQK98nF1xzRggkWiP1rjeBVdvlYiTmCopYrNiB5scPmSf2guCrx
7IH5fLbQ7JDow49dcd2ILTYFgMF03HvPvtlwz9dvOx5JYOaCw0He5CnXzZgFmV0=
=uO43
-----END PGP SIGNATURE-----

-------------------------------------

Mailman must take responsibility for the mail itself. It doesn't have to
actually sign with DKIM to do so: for backwards compatibility, spam filters
fall back to other heuristics to try and figure out the 'owner' of the mail
if it doesn't use DKIM. Those heuristics can go wrong of course. Ideally
all mail would be DKIM signed. There's no reason not to do it, really.

Yes mailing lists that edit people's emails resign. For example, from a
recent message to the bitcoinj list

DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        *d=googlegroups.com <http://googlegroups.com>*; s=20120806;
        h=to:from:subject:date:lines:message-id:references:mime-version
         :content-type:user-agent:in-reply-to:x-original-sender
         :x-original-authentication-results:reply-to:precedence:mailing-list
         :list-id:list-post:list-help:list-archive:sender:list-subscribe
         :list-unsubscribe;




Good email clients can extract the same information from the headers
anyway. I filter all my mail based on them, and the headers also contain
unsubscribe instructions. Gmail is capable of using them programmatically.
-------------------------------------
For simplicity, assume total network hashpower is constant. Also, assume the soft fork activates at the beginning of a retarget period.

At the moment the soft fork activates, the effective difficulty is increased (by adding a second independent PoW check that must also be satisfied) which means more hashes on average (and proportionally more time) are required to find a block. At the end of the retarget period,  the difficulty is lowered so that if the second PoW difficulty were to be kept constant the block interval would again average 10 mins.

If we were to keep the second PoW difficulty constant, we would restore the same total PoW-to-time-unit ratio and the retarget difficulty would stabilize again so each block would once more require the same number of hashes (and same amount of time) on average as before.

But we don't keep the second PoW difficulty constant - we increase it so once again more hashes on average are required to find a block by the same proportion as before. And we keep doing this.

Now, the assumption that hashpower is constant is obviously unrealistic. If this is your bone of contention, then yes, I agree my model is overly simplistic.

My larger point was to explore the extent of what's possible with only a soft fork - and we can actually go pretty far and even compensate for these economic shifts by increasing block size and rewards. The whole thing is clearly a huge mess - and I wouldn't recommend actually doing it.



On December 26, 2015 7:33:53 AM PST, "Jorge Timón" <jtimon@jtimon.cc> wrote:
-------------------------------------
Benchmarks?

I cant imagine that's very fast.

On 10/22/2015 02:26 PM, Jeff Garzik via bitcoin-dev wrote:

-------------------------------------
On Oct 5, 2015 2:08 PM, "Clément Elbaz" <clem.ds@gmail.com> wrote:

Given the assumptions above, only of transactions without enough
confirmations.


Not if the wallet waits for enough confirmations.
-------------------------------------
Background: I'm a CS student quickly approaching his research project, and
I'd like to do something meaningful with it.

Essentially, I'd like to know what issues someone up for their bachelor's
degree might actually be able to help on, and where I can start. Obviously
I'm not going to be able to just dive into a 6-year-running project without
some prior research, so I'm looking for a start.

What are some current things that are lacking in Bitcoin core? Or am I
better off making something else for the ecosystem?
-------------------------------------
Jorge,


Sure, most extreme: if secp256k1 or SHA256 starts to show chinks in its armor, or practical quantum computing is getting powerful enough to factor discrete logarithms of those sizes, I don't doubt everyone would go along with a proposal to add new crypto algos.

I do expect there are other possible hardforks that are uncontroversial. Either

- minor issues (maybe solving the time-warp issue with mining) issues planned on the long term
- features that are not politically loaded, on the long term
- major emergencies (anything that is clearly an 'exploit' with regard to coin holders or miners)

Not sure though. The only way to find out is to propose them and see. Maybe wait a bit until things have cooled down...

Note that anything non-critical and non-controversial can be planned and time-locked, say, 5 years ahead, obliviating the need for anyone to quickly upgrade their client.

Wladimir

-------------------------------------
On Sat, May 9, 2015 at 1:45 PM, Peter Todd <pete@petertodd.org> wrote:


You're correct in this point. Future UTXO growth will be coming from all
directions. But I'm a believer in the idea that whatever can be done should
be done.  If we get Bitcoin devs into the mindset now that UTXOs are
expensive to those that have to store them, and that they should be good
netizens and do what they can to limit them, then hopefully that will ideal
will be passed down to future developers. I don't believe consolidating
UTXOs in the wallet is the only solution.. I just think it is a fairly easy
one to implement, and can only help the problem from getting worse in the
future.

--
*James G. Phillips IV*
<https://plus.google.com/u/0/113107039501292625391/posts>
<http://www.linkedin.com/in/ergophobe>

*"Don't bunt. Aim out of the ball park. Aim for the company of immortals."
-- David Ogilvy*

 *This message was created with 100% recycled electrons. Please think twice
before printing.*
-------------------------------------
On Fri, Jan 9, 2015 at 1:20 PM, Mike Hearn <mike@plan99.net> wrote:

I don't agree with your understanding.  Expecting replacement to work
and be enforced is completely unsafe. People (sanely) refuse to use
protocols which are broken by refund malleability, which is a much
narrower expectation for miners than expecting the sequence ratchet to
go one way.


-------------------------------------
On Thu, Jul 23, 2015 at 09:56:36PM +0200, Marcel Jamin wrote:

Lol, no, I'm being dumb. :)

-- 
'peter'[:-1]@petertodd.org
00000000000000000402fe6fb9ad613c93e12bddfc6ec02a2bd92f002050594d
-------------------------------------
I wouldn't say same trade-off because you need the whole 20mb block before you can start to use it where as a 1mb block can be used quicker thus transactions found in tge block quicker etc. As for tge higher rate of orphans, I think this would be complimented by a faster correction rate, so if you're pumping out blocks at a rate of 1 per minute, if we get a fork and the next block comes in 10 minutes and is the decider, it took 10 minutes to determine which block is the orphan. But at a rate of 1 block per 1 minute then it only takes 1 minute to resolve the orphan (obviously this is very simplified) so I'm not so sure that orphan rate is a big issue here. Indeed you would need to draw upon more confirmations for easier block creation but surely that is not an issue?

Why would sync time be longer as opposed to 20mb blocks?
________________________________
From: gabe appleton<mailto:gappleto97@gmail.com>
Sent: ‎26/‎05/‎2015 12:41 PM
To: Thy Shizzle<mailto:thyshizzle@outlook.com>
Cc: Jim Phillips<mailto:jim@ergophobia.org>; Mike Hearn<mailto:mike@plan99.net>; Bitcoin Dev<mailto:bitcoin-development@lists.sourceforge.net>
Subject: Re: [Bitcoin-development] No Bitcoin For You

But don't you see the same trade-off in the end there? You're still
propagating the same amount of data over the same amount of time, so unless
I misunderstand, the costs of such a move should be approximately the same,
just in different areas. The risks as I understand are as follows:

20MB:


   1. Longer per-block propagation (eventually)
   2. Longer processing time (eventually)
   3. Longer sync time

1 Minute:

   1. Weaker individual confirmations (approx. equal per confirmation*time)
   2. Higher orphan rate (immediately)
   3. Longer sync time

That risk-set makes me want a middle-ground approach. Something where the
immediate consequences aren't all that strong, and where we have some idea
of what to do in the future. Is there any chance we can get decent network
simulations at various configurations (5MB/4min, etc)? Perhaps
re-appropriate the testnet?

On Mon, May 25, 2015 at 10:30 PM, Thy Shizzle <thyshizzle@outlook.com>
wrote:

-------------------------------------
Hi Adam,

I have more experience than Gavin of building consumer wallets, so I'll
make an attempt to answer your questions.



I am a wallet developer and I am telling you that it is.




I don't see how this is the case. If an exchange supports extension blocks
and I withdraw from that to a wallet that doesn't, the money will never
arrive from my perspective. Yet the exchange will claim they sent it and
they will wash their hands of the matter. Disaster.

I am not a UX guy

But I am. I've designed both consumer and engineering UI's at Google, and
also more recently for Lighthouse.

Attempting to explain to a user why they sent money that didn't show up on
the other end is a non starter. It's bad enough when things take a long
time to confirm or bugs cause propagation failures. Doing it
deliberately is not going to work. Payments *must* be reliable and wallets
*must* be compatible with each other.

This is one reason why a Lightning style approach also isn't going to work
any time soon. For example, it would require people to abandon Bitcoin
addresses. I pushed for that before, around the P2SH time, and Gavin
correctly intuited that the community wasn't ready for it yet. I'm not sure
much has changed.




I disagree with all of those points. I find Lightning/Stroem etc to be more
dangerous, less flexible, and worse for decentralisation. I explain why
here:

https://medium.com/@octskyward/the-capacity-cliff-586d1bf7715e

You mentioned decentralisation metrics. Gregory's post is ignoring one of
the most important decentralisation metrics, which is number of wallets
made by independent developers. That has got dramatically better over time.
It would get worse if wallets became more complex very suddenly.




companies entirely, which will give you a radically more distorted view of
the consensus. As companies providing services to our community have
serious economic weight, it stands to reason that their opinions would
matter a great deal. Yet on this mailing list I see zero effort to even
recognise their concerns, let alone care about them.
  <https://lists.sourceforge.net/lists/listinfo/bitcoin-development>
Anyway, let me repeat again to make it clear - as someone who has spent
five years writing SPV wallets, I am not on board with extension blocks or
any other Rube Goldberg contraption that exists purely to work around
theoretical objections by Blockstream employees+Peter Todd, which is what
this feels like to me.
-------------------------------------
I would recommend the following solution as a decent compromise between
complexity and privacy:

1) Encourage electrum server operators to have their servers reachable as
tor hidden services (.onion addresses)
2) Make sure server discovery works well with .onion addresses
3) Make the privacy a user configurable setting:
  - None - Allows any server connection type
  - SSL - Requires SSL at least, no plain text
  - Tor - Requires tor, no direct TCP
  - Multi-Tor - Uses a variety of tor paths to reach a variety of servers
(maybe configurable number of servers)

Default should be 'SSL' probably.








On Wed, Jul 22, 2015 at 3:20 PM Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Peter,

You did not address me but libbitcoin. Since our story and your evaluation is probably similar, I chime in.

On Feb 14, 2015, at 2:13 PM, Peter Todd <pete@petertodd.org> wrote:



We have seen that the consensus critical code practically extends to Berkley DB limits or OpenSSL laxness, therefore
it is inconceivable that a consensus library is not the same as Bitcoin Core, less its P2P service rules, wallet and RPC server.


On Feb 14, 2015, at 2:13 PM, Peter Todd <pete@petertodd.org> wrote:



The Core code base is unfriendly to feature extensions because of its criticality, legacy design and ancient technology. It is also a commodity
that the ecosystem takes for granted and free. 

I honestly admire the core team that works and progresses within these limits and perception.

I am not willing to work within the cores legacy technology limits. Does it mean I am dicking around? I think not.
It was my way to go down the rabbit hole by re-digging it and I created successful commercial products on the way.

It is entirely rational for me to focus on innovation that uses the core as a border router for this block chain. 

I am rather thankful for the ideas of the side chains, that enable innovation that is no longer measured on unapologetic compatibility with a given code base, but its services to end user.

Tamas Blummer
Bits of Proof

-------------------------------------
Hello all,

I've created a simulator for Bitcoin mining which goes a bit further than
the one Gavin used for his blog post a while ago. The main difference is
support for links with different latency and bandwidth, because of the
clustered configuration described below. In addition, it supports different
block sizes, takes fees into account, does difficulty adjustments, and
takes processing and mining delays into account. It also simulates longer
periods of time, and averages the result of many simulations running in
parallel until the variance on the result is low enough.

The code is here: https://github.com/sipa/bitcoin-net-simul

The configuration used in the code right now simulates two groups of miners
(one 80%=25%+25%+30%, one 20%=5%+5%+5%+5%), which are well-connected
internally, but are only connected to each other through a slow 2 Mbit/s
link.

Here are some results.

This shows how the group of smaller miners loses around 8% of their
relative income (if they create larger blocks, their loss percentage goes
up slightly further):

Configuration:
  * Miner group 0: 80.000000% hashrate, blocksize 20000000.000000
  * Miner group 1: 20.000000% hashrate, blocksize 1000000.000000
  * Expected average block size: 16200000.000000
  * Average fee per block: 0.250000
  * Fee per byte: 0.0000000154
Result:
  * Miner group 0: 81.648985% income (factor 1.020612 with hashrate)
  * Miner group 1: 18.351015% income (factor 0.917551 with hashrate)

When fees become more important however, and half of a block's income is
due to fees, the effect becomes even stronger (a 15% loss), and the optimal
way to compete for small miners is to create larger blocks as well (smaller
blocks for them result in even less income):

Configuration:
  * Miner group 0: 80.000000% hashrate, blocksize 20000000.000000
  * Miner group 1: 20.000000% hashrate, blocksize 20000000.000000
  * Expected average block size: 20000000.000000
  * Average fee per block: 25.000000
  * Fee per byte: 0.0000012500
Result:
  * Miner group 0: 83.063545% income (factor 1.038294 with hashrate)
  * Miner group 1: 16.936455% income (factor 0.846823 with hashrate)

The simulator is not perfect. It doesn't take into account that multiple
blocks/relays can compete for the same bandwidth, or that nodes cannot
process multiple blocks at once.

The numbers used may be unrealistic, and I don't mean this as a prediction
for real-world events. However, it does very clearly show the effects of
larger blocks on centralization pressure of the system. Note that this also
does not make any assumption of destructive behavior on the network - just
simple profit maximalization.

Lastly, the code may be buggy; I only did some small sanity tests with
simple networks.

-- 
Pieter
-------------------------------------
Thanks Bryan for collating these links in one great list. This is very
helpful and thanks for sharing it.

Feel free to fork https://github.com/EthanHeilman/BlockSizeDebate
edit to add the list of proposals and create a pull request to Ethan.

There's also a miningconsensus.slack.com group to have discussion w.r.t.
fact/source checking, completeness  (e.g. from IRC) etc.

Tks.

p.


On Sat, Jun 13, 2015 at 3:13 AM, Bryan Bishop <kanzure@gmail.com> wrote:

-------------------------------------
As per request of Luke-jr I'm sending a copy of my post on reddit
https://www.reddit.com/r/Bitcoin/comments/3nh0s4/bitcoin_dev_irc_meeting_in_laymans_terms_or_an/
to the mailing list.

This was intended to be a simple explanation of the weekly dev meeting for
people to understand what you guys are working on, not as a summary for
other devs.
However, if this is in any way, shape or form useful for the mailing-list
I'll gladly post a copy of this every week (or a modified version of it).

Any comments, suggestions, etc. are welcome.
Mail me at G1liusbitcoin@gmail.com
Tweet me @G1lius


If you are to skim through this, skip "background" as you likely already
know this.





Please bare in mind I'm not a developer and I'd have problems coding "hello
world!", so some things might be incorrect or plain wrong.
Like any other write-up it likely contains personal biases, although I try
to stay as neutral as I can.

The full IRC-logs can be found here
http://bitcoinstats.com/irc/bitcoin-dev/logs/2015/10/01#l1443726030.0.

There are no decisions being made in these meetings, so if I say "everyone
agrees" this means everyone present in the meeting, that's not consensus,
but since a fair amount of devs are present it's a good representation.

Main topics discussed where:
Mempool limiting
BIP68 + CHECKSEQUENCEVERIFY
CLTV soft fork deployment
libconsensus merge time window


**Mempool limiting**


- background

When a transaction is relayed across the network it is held by the nodes in
memory, until it gets into a block. All these transactions that sit in
memory are called the memorypool or mempool for short.
Like we could see during the spam-attack if there's a big back-log of
transactions that couldn't make it in the blockchain this mempool can get
pretty big resulting in nodes crashing.

To stop this from happening devs are trying to find a way to limit this
mempool, so a mechanism to reject and/or remove transactions from the
mempool. The hard part here is to make it so nodes can't be attacked by
abusing this mechanism.

There are multiple worked out ideas for this, namely:
Limit mempool by throwing away the cheapest txn and setting min realy fee
to it ( https://github.com/bitcoin/bitcoin/pull/6722 )
Mempool limiting with descendant package tracking (
https://github.com/bitcoin/bitcoin/pull/6557 )
exponential rising effective min relay feerate (
https://github.com/bitcoin/bitcoin/pull/6673 )


- meeting comments

devs are leaning towards 6722 (throwing away the cheapest txn and setting
min relay fee to it) because it's the more simpler approach and possibly
less edge-cases.
The idea behind it is to have a mem-pool that gives a good approximation on
what'll be included in the next blocks, meaning higher fee transactions.
This approach also helps to build a fee-estimator.
Some devs propose to include a time-based eviction as well.


- meeting conclusion

6722 should be completed and 6722, 6557 and 6673 should be attacked by the
others to try and find edge-cases.
The default mempool size should be 300Mb.



**Chain limits**

- background

Related to mempool limiting.
Chain in this context means connected transactions. When you send a
transaction that depends on another transaction that has yet to be
confirmed we talk about a chain of transactions.
Miners ideally take the whole chain into account instead of just every
single transaction (although that's not widely implemented afaik). So while
a single transaction might not have a sufficient fee, a depending
transaction could have a high enough fee to make it worthwhile to mine both.
This is commonly known as child-pays-for-parent.
Since you can make these chains very big it's possible to clog up the
mempool this way.
The first unconfirmed transaction is called the ancestor and the
transactions depending on it the descendants. The total amount of
transactions is referred to as "packages".

- meeting comments

All of the mempool limiting approaches are way easier to attack if you have
bigger chain limits.
the reason to have larger descendant packages is you can't control that
yourself, somebody pays you and bob, and bob chains off a million
descendants and he ends up screwing you.
if you have a say 900kb ancestor package limit, then even if the ancestor
fee rate is reasonably high, default mining code is likely going to find
100kb of very high fee txs to include first, and then there won't be room
for your ancestor package.
Morcos proposes 25/250kb for ancestors and 50/500kb for descendants,
meaning max. either 25 transactions or 250kb in size for ancestors.
Most seem to be fine with those limits and even smaller.

-meeting conclusion

morcos writes a chain-limit proposal to post on the mailing list in order
to find possible usecases for large chain transactions.



**CHECKLOCKTIMEVERIFY softfork**

- background

Commonly referred to as: How you thought nLockTime worked before you
actually tried to use it.
There's a fair amount of demand for this and the code is reviewed and has
been running on sidechains alpha for 6 months.
The only real issue is how and when it's merged.
Currently softforks have been done by the isSuperMajority mechanism,
meaning when 95% of the last X blocks has a version number higher than X
the fork is deployed.
A new way of doing this is currently being worked on and that uses all bits
of the version number, appropriately being called versionbits. So instead
of a fork happening when the version is larger than (for example)
00000000011 (3), a fork happens when (for example) the 3rd bit is up (so
00100000011).
This way softforks can be deployed simultaneous and independent of each
other.

- meeting comments

Questions are being posed whether we wait for other time-related BIP's
and/or versionbits, or do it now using isSuperMajority.
If versionbits is deployed later it needs to wait for all supermajority
softforks to be over.
Vladimir van der Laan doesn't want to deploy any soft forks in major
releases (0.12 in this case) so that people explicitly upgrade for the
softfork not for other things.
You could roll out multiple supermajority forks as long as they are
cumulative.
Talks seem to converge to using supermajority to deploy checkLockTimeVerify
and checkSequenceVerify if it's ready by the end of October.

- meeting conclusion

checkLockTimeVerify backports (deployment in older versions) needs to be
reviewed as well as BIP68, 112 and 113 (all the time-related BIP's).



**Libconsensus**

- background

Satoshi wasn't the best programmer out there, which leaves a pretty messy
code. Ideally you'd have the part of the code that influences the network
consensus separately, but in bitcoin it's all intertwined.
Libconsensus is what eventually should become this part. This way people
can more easily make changes in the non-consensus part without fear of
causing a network fork.
This however is a slow and dangerous project of moving lot's of code
around.

- meeting comments

Lot's of discussion on when existing changes should be merged, when the
code should be frozen for next release etc.
In linux changes are merged right after a major release. jtimon notices
this was planned for after 0.10 and 0.11 too, but nothing happened.
There seems to be a lack of planning and overview as to what where has to
go.

- meeting conclusion

jtimon will provide a high level rationale for what and where things should
move so people can make comments and review according to this rationale.


**Participants**


dstadulis     Daniel Stadulis
wumpus Wladimir J. van der Laan
morcos Alex Morcos
gmaxwell     Gregory Maxwell
btcdrak btcdrak
jonasshnelli Jonas Schnelli
maaku Mark Friedenbach
sdaftuar Suhas Daftuar
sipa Pieter Wuille
BlueMatt    Matt Corallo
CodeShark Eric Lombrozo
Luke-Jr Luke Dashjr
bsm117532 Bob McElrath
jgarzik Jeff Garzik
-------------------------------------
On 2/22/2015 9:12 AM, Peter Todd wrote:

Let's see.  I could pay 10 people 1 BTC each with one tx, then 
double-spend it with fees of 2BTC.  Now at least three of the 10 have to 
work together if they want to scorched-earth me, since an individual or 
two-party claw-back wouldn't have high enough fees. Oops!



-------------------------------------
https://en.wikipedia.org/wiki/I_know_it_when_I_see_it

Can we agree n-1 dev Nacks would be a controversial hard fork?

Greg

On Sat, Jun 27, 2015 at 8:15 AM, NxtChg <nxtchg@hush.com> wrote:

-------------------------------------
Some of the people on this mailing list are blindly discussing the
technicalities of a soft/hard fork without realizing that is not Mike's
main intention. At least I perceive (and maybe others too) something else
is happening.

Let me try to clarify: the discussion has nothing to do with technical
arguments. I generally like more hard forks than soft forks (but I won't
explain why because this is not a technical thread), but for CLTV this is
quite irrelevant (but I won't explain why..), and I want CLTV to be
deployed asap.

Mike's intention is to criticize the informal governance model of Bitcoin
Core development and he has strategically pushed the discussion to a
dead-end where the group either:

1) ignores him, which is against the established criteria that all
technical objections coming from anyone must be addressed until that person
agrees, so that a change can be uncontroversial. If the group moves forward
with the change, then the "uncontroversial" criteria is violated and then
credibility is lost. So a new governance model would be required for which
the change is within the established rules.

2) respond to his technical objections one after the other, on never ending
threads, bringing the project to a standstill.

As I don't want 2) to happen, then 1) must happen, which is what Mike
wants. I have nothing for or against Mike personally. I just think Mike
Hearn has won this battle. But having a more formal decision making process
may not be too bad for Bitcoin, maybe it can actually be good.

Best regards
 from a non-developer to my dearest developer friends,
  Sergio.
-------------------------------------
Bitcoin security depends on the enforcement of consensus rules which
is done by economically dependent full nodes.  This is distinct from
miners fullnodes, and balances miners interests, otherwise SPV nodes
and decentralisation of policy would tend degrade, I think.  Therefore
it is important that it be reasonably convenient to run full nodes for
decentralisation security.

Also you may want to read this summary of Bitcoin decentralisation by Mark:

https://www.reddit.com/r/Bitcoin/comments/3h7eei/greg_luke_adam_if_xt_takes_over_and_wins_the/cu53eq3

I think you maybe misunderstanding what the Chinese miners said also,
about 8MB, that was a cap on the maximum they felt they could handle
with current network infrastructure.

I had proposed 2-4-8MB growing over a 4 year time frame with 2MB once
the hard-fork is upgraded by everyone in the network.  (I dont
consider miner triggers, as with soft-fork upgrades, to be an
appropriate roll out mechanism because it is more important that
economically dependent full nodes upgrade, though it can be useful to
know that miners also have upgraded to a reasonable extent to avoid a
temporary hashrate drop off affecting security).

Adam

On 9 September 2015 at 15:00, Marcel Jamin via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Aug 7, 2015 5:55 PM, "Gavin Andresen" <gavinandresen@gmail.com> wrote:
yes, fear of Bad Things Happening as we run up against the 1MB limit is one
of the reasons.

What are the other reasons?

and have seen what happens when networks run out of capacity very seriously.

When "the network runs out of capacity" (when we hit the limit) do we
expect anything to happen apart from minimum market fees rising (above
zero)?
Obviously any consequences of fees rising are included in this concern.
-------------------------------------
Le 28/05/2015 17:53, Gavin Andresen a crit :

I like that idea.

Average is a better choice than median. The median is not well defined
on discrete sets, as shown in your example, and there is no need to be
robust to outliers, thanks to the max size.



-------------------------------------
On Thu, May 7, 2015 at 11:12 AM, Mike Hearn <mike@plan99.net> wrote:


Yes, that is a possibility.




This is absolutely not a trivial change.

It is a trivial *code* change.  It is not a trivial change to the economics
of a $3.2B system.

-- 
Jeff Garzik
Bitcoin core developer and open source evangelist
BitPay, Inc.      https://bitpay.com/
-------------------------------------


On 05/07/15 14:52, Gavin Andresen wrote:

People have been sharing the same objections as on this list for months,
I'm not sure what is new here.


I think this is a huge issue. You've been wandering around telling
people that the blocksize will increase soon for months, when there is
very clearly no consensus that it should in the short-term future. The
only answer to this that anyone with a clue should give is "it will
very, very likely be able to support at least 1MB blocks roughly every
10 minutes on average for the next eleven years, and it seems likely
that a block size increase of some form will happen at some point in the
next eleven years", anything else is dishonest.


-------------------------------------
On Tuesday 11. August 2015 19.47.56 Jorge Timn wrote:

[]

Since you replied to me;

I have to admit I find that a little depressing.
I put forward about 10 reasons in the last 24 hours and all you remember is 
something with fees.  Which, thats the funny part, I never wrote as being a 
problem directly.

 

I would really like to avoid putting blame. I'd like to avoid the FUD 
accusation and calling people paranoid, even yourself, sounds rather bad 
too...

Personally I think its a bad idea to do write the way you do, which is that 
some people have to prove that bad things will happen if we don't make a 
certain change. It polarizes the discussion and puts people into camps. People 
have to choose sides.

I've been reading the blocksize debate for months now and have been 
wondering 
why people here are either for or against, it makes no sense to me.
Neither camp is right, and everyone knows this!
Everyone knows that bigger blocks doesn't solve the scalability problem. 
Everyone knows that you can't get substantial growth using lightning or higher 
fees in, say, the next 12 months.

please reply to this email;
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010129.html

-- 
Thomas Zander

-------------------------------------
On Dec 27, 2015 00:06, "Jonathan Toomim" <j@toom.im> wrote:

hard fork to increase the blocksize for years, I do not think that
mobilizing people to upgrade their nodes is going to be hard.
their full nodes. We may want to request that miners not trigger the fork
until some percentage of visible full nodes have upgraded.

I am generally not interested in a system where we rely on miners to make
that judgement call to fork off nodes that don't pay attention and/or
disagree with the change. This is not because I don't trust them, but
because I believe one of the principle values of the system is that its
consensus system should be hard to change.

I can't tell you what code to run of course, but I can decide what system I
find interesting to build. And it seems many people have signed off on
working towards a plan that does not include a hard fork being scheduled
right now: https://bitcoin.org/en/bitcoin-core/capacity-increases

Cheers,

-- 
Pieter
-------------------------------------
On Thu, Aug 13, 2015 at 6:12 PM, Mark Friedenbach <mark@friedenbach.org> wrote:

I think it's important to allow some time for discussion with the
actual proposed text up; as understandings can shift significantly. :)
Btcdrak already asked me for numbers prior to posting text at all and
I asked him to post text...

-------------------------------------
On Wed, Dec 16, 2015 at 9:44 PM, Eric Lombrozo <elombrozo@gmail.com> wrote:


At least on my part, the title of the 1st email was "It's economics & ..."
and focused on (a) economics and (b) transition issues.  There was no
confounding.  There was a list of real problems and risks taken when 1M is
not lifted in the short term.

Thus "SW is orthogonal" in these emails, because these problems remain
regardless of SW or no, as the 1st email outlined.

The 2nd email addresses the specific assertion of "no 1M hard fork needed,
because SW."
-------------------------------------
"minimum" an interesting topic.

- Traffic levels may not produce a minimum size block
- Miners can always collude to produce a lowered maximum block size, a sort
of minimum maximum



On Sun, Aug 16, 2015 at 12:41 PM, Levin Keller via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Thomas,
  I think this is interesting and has some good thoughts behind it.  For
clarity, are you recommending that the "_oa2" portion of the domain name be
"hidden" as a way to make it easier to delegate just wallet names from a
zone?



On Thu, Jul 23, 2015 at 6:07 AM, Thomas Voegtlin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 

Justin W. Newton
Founder/CEO
NetKi, Inc.

justin@netki.com
+1.818.261.4248
-------------------------------------
That guy is going to easily make back his 150 BTC with interest, if he
is short BTC. Should be a clear signal for the market to trend lower,
probably below $180 by the time of the test.

On 23 August 2015 at 19:08, jl2012 via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On 12/20/2015 3:34 AM, Jeff Garzik via bitcoin-dev wrote:

Also, a change (#6550) has been merged to bitcoin core that removes
merkle branches from the wallet, and if pruning gets turned on (possible
in 0.12 with #6057), it would become quite a bit more difficult to spend
older coins under a change like this.

As a solution I would favor not removing wallet merkle branches.


-------------------------------------
Please note there is now a PR for this BIP[1] and also a pull request for
the opcode CHECKSEQUENCEVERIFY in Bitcoin Core[2].

[1] https://github.com/bitcoin/bips/pull/179
[2] https://github.com/bitcoin/bitcoin/pull/6564
-------------------------------------

I agree.  In fact, I’ll go meta on your meta and suggest that we should first discuss how Bitcoin should be governed in the first place.  Should Bitcoin evolve from the “bottom up,” or from the “top down”?

If one’s answer is from the “top-down,” then the meta-level criteria can be endlessly debated, for they all involve some sort of tradeoff, they all require some sort of compromise.  The “top down” perspective holds that people might make poor choices if given the freedom to easily do so--it holds that the trade-offs must be balanced instead by experts.  

However, if one's answer is from the “bottom up,” then the meta-level criteria is very easy: we do what the people wants. We allow the people to weigh the tradeoffs and then we watch as consensus emerges through a decentralized process, objectively represented by the longest proof-of-work chain.  

Regarding the block size limit debate, at the end of the day it comes down to two things:

1.  How big of a block will my node accept today?

2.  What do I want my node to do if the longest chain includes a block larger than the limit I set?

If one concedes that Bitcoin should be governed from the “bottom up,” then it is already possible to empower each node operator to more easily express his free choice regarding the size of blocks he is willing to accept, while simultaneously ensuring that his node tracks consensus.

Best regards,
Peter


-------------------------------------
Seems like a good change to me.

On Wed, Jan 21, 2015 at 7:32 PM, Rusty Russell <rusty@rustcorp.com.au>
wrote:

-------------------------------------
Here are refutations of the approach in BIP-100 here:
http://gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf

To recap BIP-100:

1) Hard form to remove static 1MB block size limit
2) Add new floating block size limit set to 1MB
3) Historical 32MB message limit remains
4) Hard form on testnet 9/1/2015
5) Hard form on main 1/11/2016
6) 1MB limit changed via one-way lock in upgrade with a 12,000 block
threshold by 90% of blocks
7) Limit increase or decrease may not exceed 2x in any one step
8) Miners vote by encoding 'BV'+BlockSizeRequestValue into coinbase
scriptSig, e.g. "/BV8000000/" to vote for 8M.
9) Votes are evaluated by dropping bottom 20% and top 20%, and then the
most common floor (minimum) is chosen.

8MB limits doubling just under every 2 years makes a static value grow
in a predictable manner.

BIP-100 makes a static value grow (or more importantly potentially
shrink) in an unpredictable manner based on voting mechanics that are
untested in this capacity in the bitcoin network.  Introducing a highly
variable and untested dynamic into an already complex system is
unnecessarily risky.

For example, the largely arbitrary voting rules listed in 9 above can be
gamed.  If I control pools or have affiliates involved in pools that
mine slightly more than 20% of blocks, I could wait until block sizes
are 10MB, and then suddenly vote "/BV5000000/" for 20% of blocks and
"/BV5000001/" for the remaining 10%.  If others don't consistently vote
for the same "/BV#/" value, vote too consistently and have their value
thrown out as the top 20%, I could win the resize to half capacity
"/BV5000001/" because it was the lowest repeated value not in the bottom
20%.

I could use this to force an exodus to my sidechain/alt coin, or to
choke out the bitcoin network.  A first improvement would be to only let
BIP-100 raise the cap and not lower it, but if I can think of a
vulnerability off the top of my head, there will be others on the other
side of the equation that have not been thought of.  Why bother
introducing a rube goldberg machine like voting when a simple 8mb cap
with predictable growth gets the job done, potentially permanently?


On 6/23/2015 9:43 PM, odinn wrote:

-------------------------------------
Peter,

An important use of the core is being border router to proprietary software, that is usually indexing the block chain and mempool. That software is also assuming that double spends are not relayed by the core.

To remain useful as border router, the replace-by-fee patched core should only relay double spend if it actually replaces an earlier transaction, as otherwise the replace logic that is according to your commit more than just fee comparison, would have to be replicated in the proprietary stack and mempool might get out of sync with that of the border router. 

Tamas Blummer
Bits of Proof
-------------------------------------
Thanks Mike, and sorry to answer a bit late; it has been a busy couple
of weeks.

You are correct, a BIP39 seed phrase will not work in Electrum, and vice
versa. It is indeed unfortunate. However, I believe BIP39 should not be
followed, because it reproduces two mistakes I did when I designed the
older Electrum seed system. Let me explain.

The first problem I have with BIP39 is that the seed phrase does not
include a version number.

Wallet development is still in an exploratory phase, and we should
expect even more innovation in this domain. In this context, it is
unwise to make decisions that prevent future innovation.

However, when we give a seed phrase to users, we have a moral obligation
to keep supporting this seed phrase in future versions. We cannot simply
announce to Electrum users that their old seed phrase is not supported
anymore, because we created a new version of the software that uses a
different derivation. This could lead to financial losses for users who
are unaware of these technicalities. Well, at least, that is how I feel
about it.

BIP39 and Electrum v2 have a very different ways of handling future
innovation. Electrum v2 seed phrases include an explicit version number,
that indicates how the wallet addresses should be derived. In contrast,
BIP39 seed phrases do not include a version number at all. BIP39 is
meant to be combined with BIP43, which stipulates that the wallet
structure should depend on the BIP32 derivation path used for the wallet
(although BIP43 is not followed by all BIP39 compatible wallets). Thus,
innovation in BIP43 is allowed only within the framework of BIP32. In
addition, having to explore the branches of the BIP32 tree in order to
determine the type of wallet attached to a seed might be somewhat
inefficient.

The second problem I see with BIP39 is that it requires a fixed
wordlist. Of course, this forbids innovation in the wordlist itself, but
that's not the main problem. When you write a new standard, it is
important to keep this standard minimal, given the goal you want to
achieve. I believe BIP39 could (and should) have been written without
including the wordlist in the standard.

There are two ways to derive a master key from a mnemonic phrase:
 1. A bidirectional mapping between words and numbers, as in old
Electrum versions. Pros: bidirectional means that you can do Shamir
secret sharing of your seed. Cons: It requires a fixed wordlist.
 2. Use a hash of the seed phrase (pbkdf). Pros: a fixed wordlist is not
required. Cons: the mapping isn't bidirectional.

Electrum v1 uses (1). Electrum v2 uses (2).

Early versions of BIP39 used (1), and later they switched to (2).
However, BIP39 uses (2) only in order to derive the wallet keys, not for
its checksum. The BIP39 checksum uses (1), and it does requires a fixed
wordlist. This is just plainly inconsistent. As a result, you have
neither wordlist flexibility, nor Shamir secret sharing.

Having a fixed wordlist is very unfortunate. First, it means that BIP39
will probably never leave the 'draft' stage, until all languages of the
world have been added. Second, once you add a wordlist for a new
language, you cannot change it anymore, because it will break existing
seed phrases; therefore you have to be extremely careful in the way you
design these wordlists. Third, languages often have words in common.
When you add a new language to the list, you should not use words
already used by existing wordlists, in order to ensure that the language
can be detected. It leads to a first come first served situation, that
might not be sustainable in the future.

In order to support the old Electrum v1 seeds, all future versions of
Electrum will have to include the old wordlist. In addition, when
generating new seed phrases, Electrum now has to avoid collisions with
old seed phrases, because the old ones did not have a version number.
This is painful enough, I will not repeat the same errors twice.

Electrum v2 derives both its private keys and its checksum/version
number using a hash of the seed phrase. This means that wordlists can be
added and modified in the future, without breaking existing seed
phrases. It also means that it will be very easy for other wallets to
support Electrum seedphrases: it requires about 20 lines of code, and no
wordlist is required.


Thomas


Le 02/03/2015 16:37, Mike Hearn a écrit :


-------------------------------------
On 07/04/2015 10:35 AM, Tier Nolan wrote:

How do we know if a committed UTXO set is valid? If a majority of the
hashing power is willing to extend an invalid branch, it's reasonable to
assume they'd be willing to commit an invalid UTXO set as well.

In order for the committed UTXO set to be reliable at a minimum it will
need to contain at a source block reference for each item in the set.
That would enable fraud proof to show that the committed UTXO set
contains invalid entries.


I agree the information should be an extra commitment produced by the
miner, rather than changing the format of the transaction, since the
author of a transaction can't always know the required information ahead
of time.


If items in the the proof tree are required to be sorted, then it's easy
to proof that an item is missing.



-- 
Justus Ranvier
Open Bitcoin Privacy Project
http://www.openbitcoinprivacyproject.org/
justus@openbitcoinprivacyproject.org
E7AD 8215 8497 3673 6D9E 61C4 2A5F DA70 EAD9 E623
-------------------------------------
On Thu, Aug 20, 2015 at 10:38:19PM -0700, Peter Todd via bitcoin-dev wrote:

Oh, and we should also point out that Bloom filters have scaling issues,
as each application of the filter has to scan the whole blockchain -
with future blocksize increases these issues increase, in some proposals
quite dramatically. The underlying idea also conflicts with some
proposals to "shard" the blockchain, again suggesting that we need a bit
to handle future upgrades to more scalable designs.

-- 
'peter'[:-1]@petertodd.org
00000000000000000402fe6fb9ad613c93e12bddfc6ec02a2bd92f002050594d
-------------------------------------
On 8/1/2015 1:45 PM, Pieter Wuille via bitcoin-dev wrote:

You've proposed scaling the cap based on technology growth.  There's
still a cap to stop bad things from happening.

Once that is done, why worry so much about whether the uses are
efficient?  Let people work in the space created.  Let them figure out
how to make good things happen in the application space.


-------------------------------------
Pieter Wuille mentions "subsidy fraud" in his recent talk:
https://youtu.be/fst1IK_mrng?t=57m2s

I was unable to google what this is, and the Bitcoin Wiki also does not seem 
to explain it.

If this is a well-known problem, perhaps it would be a good idea to explain it 
somewhere?
-------------------------------------
I haven't seen much discussion on this list of what will happen when the
blockchain forks due to larger blocks. I think the debate surrounding this
issue is a storm in a teacup, because transactions on the smaller chain can
and will appear on the bigger chain also. There is nothing tying
transactions to the blocks they appear in.

Miners will migrate to the bigger chain in search of higher profits due to
higher volume of fees. They can also collect the higher fees of the smaller
chain by including into the bigger chain as many as possible of the
transactions from the smaller chain.

To stop this from happening the smaller chain would somehow need to change
the serialized format of their transactions so the signatures would no
longer be valid across chains.

Incidentally I read somewhere that the losing chain would have their coins
sold down. Trying to sell the smaller chain's coins in the short term at
least is not advisable, as those transactions will appear on the bigger
chain too.
-------------------------------------
Hey while I was listening to the talks I also typed most of the words down.

Here are some talks from the Hong Kong workshop:
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip99-and-uncontroversial-hard-forks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/fungibility-and-scalability/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/zero-knowledge-proofs-for-bitcoin-scalability-and-beyond/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/security-assumptions/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/in-adversarial-environments-blockchains-dont-scale/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/why-miners-will-not-voluntarily-individually-produce-smaller-blocks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/invertible-bloom-lookup-tables-and-weak-block-propagation-performance/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip101-block-propagation-data-from-testnet/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/overview-of-bips-necessary-for-lightning/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/network-topologies-and-their-scalability-implications-on-decentralized-off-chain-networks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-bevy-of-block-size-proposals-bip100-bip102-and-more/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-flexible-limit-trading-subsidy-for-larger-blocks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/validation-cost-metric/

Also, here are some talks from the Montreal workshop:
http://diyhpl.us/wiki/transcripts/scalingbitcoin/alternatives-to-block-size-as-aggregate-resource-limits/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-block-propagation-iblt-rusty-russell/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-failure-modes-and-the-role-of-the-lightning-network/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-load-spike-simulation/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/block-synchronization-time/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/coinscope-andrew-miller/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/competitive-fee-market-urgency/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/issues-impacting-block-size-proposals/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/overview-of-security-concerns/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/relay-network/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/reworking-bitcoin-core-p2p-code-for-robustness-and-event-driven/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/transaction-fee-estimation/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/validation-costs/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-1/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-2/

These are not always exact transcripts because I am typing while I am
listening, thus there are mistakes including typos and listening errors, so
please keep this discrepancy in mind between what's said and what's typed.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------------------------------
On Tue, May 5, 2015 at 10:38 PM, Tier Nolan <tier.nolan@gmail.com> wrote:

Yes, sorry, I changed it just before sending from "what needs to be
satisfied for the validation error to trigger" to "what needs to be
satisfied for the tx to be valid".
You're right.


Yes, this would be the simplest solution. Another option would be to
have a new tx version in which IsFinal(CTransaction) doesn't check the
inputs sequences to be 0xFFFFFFFF for the tx to be final.


Yes.


Well, the semantics of nSequence don't really change completely. In
fact, one could argue that this put it closer to its original
semantics.
But in any case, yes, already signed transaction should remain valid.
No transaction would become invalid, just non-final.
As soon as the height of its inputs plus their respective nSquences
get higher than current height they will become final again.
I cannot think of any use case where a tx becomes invalid forever.
Also, probably most people have usedrelatively low values for
nSequence given the original semantics, just like the relative lock
nSquence will likely be used as well.


To be clear, this proposal is supposed to replace RCLTV, so there
would still be 2 options. But please let's imagine we have infinite
opcodes in this thread and let the "should we design an uglier
scripting langues to save opcodes?" question in the other one.


This gives you less flexibility and I don't think it's necessary.
Please let's try to avoid this if it's possible.




-------------------------------------
On Mon, Sep 28, 2015 at 04:51:22PM +0200, Mike Hearn wrote:

I have read your article. In fact we reviewed it at a NY BitDevs meetup
that I attended.


Can you explain exactly how you think wallets will "know" how to ignore
the invalid chain?

With an advertised soft-fork, e.g. the IsSuperMajority() mechanism,
ignoring the invalid chain is easy: use nVersion to detect invalid
blocks when you know what soft-forks are coming up, and if presented
with an unknown - but advertised - soft-fork at minimum loudly warn the
user. In the case of a hard-fork identical logic can be used. (BIP101
being an example of a hard-fork triggered in a way that can be detected
by SPV clients, both explicitly (BIP101 specific) and implicitly
(general unknown block nVersion warnings))


How so? Miners can always choose to create invalid blocks, thus
attacking SPV wallets; my statement with regard to pull-req #5000 comes
from a risk-based approach, knowing that every invalid block is
expensive and the new concern created by a soft-fork is whether or not
miners will create them accidentally; miners can always create invalid
blocks delibrately.


That's incorrect: Miners bypassing IsStandard() risk creating invalid
blocks in the event of a soft-fork. Equally, we design soft-forks to
take advantage of this.


We seem to be in strong disagreement about which option has "clear,
explicit downsides"

-- 
'peter'[:-1]@petertodd.org
0000000000000000006f2abe95e361b73289e4a79ba3124801896f6b7dc8d977
-------------------------------------
Hi!

My fingers have been itching many times now, this debate
drives me nuts.

I just wish all posters could follow two simple principles:

1. Read up. Yes. All of what has been written. Yes, it will
   take many hours. But if you're rehashing what other
   smarter people have said over and over before, you're
   wasting hundreds of peoples time. Please don't.

2. Be helpful. Suggest alternatives. Just cristizising is
   just destructive. If you want no change, then say so.

Mats


-------------------------------------

Yeah, my proposal is not intended to function correctly with full blocks,
as Bitcoin cannot work at all in such a state. It assumes that fees only
change slowly and that transactions are being cleared normally.
-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Arriving slightly late to the discussion, apologies.

Personally I wouldn't have written that patch, but I know development of
hostile patches happens out of sight, and if it can be written, we have
to presume it will be written eventually. I'd have preferred a patch
that only replaced non-final txes, which is the use-case I have for
transaction replacement, but that's easy to add back in.

I'm certainly not terribly convinced of the security of vanilla
zero-confirmation transactions myself, for reasons including but not
limited to this case. I also think it's important to understand that
people do make irrational decisions, and trusting network security on
everyone behaving perfectly rationally is not a workable model either.

TLDR; me too

Ross

On 12/02/15 20:36, Allen Piscitello wrote:
------------------------------------------------------------------------------
Take a
------------------------------------------------------------------------------
is your

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJU31/yAAoJEJFC5fflM8475YIIAI7nxgxUdkKiMePMqtvPOi25
U+WCxjvIK0ZRTAV30POC7fKLT2mK0gPusSS7LtNJpPKvpC98VcSD5HWE49K80Yo9
9+QI7X7xBau1jjLo+27uOex0bJ6JwP1DSMpC12AQbMmi4FnyG+M5FMkr5/OnSxeF
cd4lT2UF7yTJPRy0+A9LwertL5Sv1yeOJJ9jtWuXgixapmHN+1Zm2VkGnur55V64
vnonlixlUMwnZNxDVoRhjTWm1P/lmCejvmvTRvcBomUlAEgRQF4TtF4YMBYXS97S
5WYrxOHLgTfTWr3FJuOnd+CVBRgZGw3u30ktaSErelyMG19lJOusBPdHTQFkV30=
=eWPj
-----END PGP SIGNATURE-----

-------------------------------------
On Sat, May 30, 2015 at 07:42:16AM +0800, Chun Wang wrote:

Great to hear from you!

Yeah, I'm pretty surprised myself that Gavin never accepted the
compromises offered by others in this space for a slow growth solution,
rather than starting with over an order of magnitude blocksize increase.
This is particularly surprising when his own calculations - after
correcting an artithmetic error - came up with 8MB blocks rather than
20MB.

Something important to note in Gavin Andresen's analysises of this issue
is that he's using quite optimistic scenarios for how nodes are
connected to each other. For instance, assuming that connections between
miners are direct is a very optimistic assumption that depends on a
permissive, unregulated, environment where miners co-operate with each
other - obviously that's easily subject to change! Better block
broadcasting logic helps this in the "co-operation" case, but there's
not much it can do in the worst-case.


Unrelated: feel free to contact me directly if you have any questions
re: the BIP66 upgrade; I hear you guys were planning on upgrading your
mining nodes soon.

-- 
'peter'[:-1]@petertodd.org
00000000000000000db932d1cbd04a29d8e55989eda3f096d3ab8e8d95eb28e9
-------------------------------------
On Fri, Jun 19, 2015 at 5:37 PM, Eric Lombrozo <elombrozo@gmail.com> wrote:

I disagree with this premise. Please, don't take this as an argument
from authority fallacy, but I will cite Satoshi to express what I
think the assumptions while using the system should be:

"As long as a majority of CPU power is controlled by nodes that are
not cooperating to attack the network, they'll generate the longest
chain and outpace attackers."

I can't say for sure what was meant by "attacking the network" in this
context but I personally mean trying to rewrite valid and
proof-of-work-timestamped history.
Unconfirmed transactions are simply not part of history yet. Ordering
unconfirmed transactions in a consensus compatible way without a
universal clock is impossible, that's why we're using proof of work in
the first place.

Alternative policies are NOT attacks on the network.


-------------------------------------
On Thu, May 07, 2015 at 04:05:41PM +0200, Mike Hearn wrote:

As you know I was forwarded that email first, and because I *do* respect
your privacy I consulting with you via private IRC chat first, and as
you wished I didn't publish it. The hacker presumably gave up waiting
for me to do so and published it themselves seven months ago; to make
that clear I linked the source(1) of the email in my message. Those
emails simply are no longer private.

Frankly personal attacks like this - "your hypocrisy really is
bottomless, isn't it?", "Satoshi's hacker had no illusions about your
horrible personality" - simply don't belong on this mailing list and I
think we would all appreciate an apology.

1) https://www.reddit.com/r/Bitcoin/comments/2g9c0j/satoshi_email_leak/

-- 
'peter'[:-1]@petertodd.org
000000000000000012a3e40d5ee5c7fc2fb8367b720a9d499468ceb25366c1f3
-------------------------------------
Nice insight Peter,

This further confirms the real problem, which doesn't have much to do with
blocksize but rather the connectivity of nodes in countries with
not-so-friendly internet policies and deceptive connectivity.


On Thu, Jun 18, 2015 at 6:00 PM, Tom Harding <tomh@thinlink.com> wrote:




-- 
*Yifu Guo*
*"Life is an everlasting self-improvement."*
-------------------------------------
Pieter,

Am 13.06.2015 um 16:39 schrieb Pieter Wuille:

I think we should set the right incentives to invalidate these assumptions. If the fees as well as the security guarantees
on the main chain are highest and fees are dropping with the distance from the main chain on each level of side chains,
wouldn't communities with many internal transactions create their own side chain with low fees? I'd expect geographic
as well as virtual communities to be forming enjoying cheap fees on their 'local' chains and expensive but comparabily rare
'long distance' fees. One would expect geographic chains (e.g. continents) as well as virtual ones (e.g. the Open Bazaar
users' chain) to form. To save fees, a typical user would maintain a wallet in each of her communities which are loaded
and drained with rare expensive transacations, whereas daily business with many transactions is done cheaply within
each community chain. So, indeed, I would argue that side chains equipped with the right cost incentives for cross-chain
transactions would lead to a scalable and efficiently self-organizing network of side chains.

best regards,
Martin


-------------------------------------
I don't think this would work.

If the rule is that one user can only have one vote, how do you prevent
a user running multiple nodes?

Also, how do you verify that a node is indeed a fully validating node
with its own copy of the blockchain?


On 08/25/2015 01:37 PM, Peter Todd via bitcoin-dev wrote:

-------------------------------------
On Monday 5. October 2015 19.41.30 Gregory Maxwell wrote:

You are special only in your eloquent use of the language. Consider yourself 
lucky :)


I would not expect anything less.


Thanks for explaining your thinking.

Fortunately I can say that while we certainly value your opinion, when peoples 
opinions are hard to read, as you indicated they can be, we should look at 
their actions. The group has followed the consensus rule quite rigorously, 
which I applaud.
But next to that people like Black and Laan have given strong verbal 
indications confirming the practice you personally keep explaining is not 
real.


When I was a little boy of maybe 12 years, I remember reading a short story, 
that stuck with me.  It was about a man that had vowed to never lie. He was 
invited to a dinner party and asked to assist with another man's accusation of 
a crime he claimed to not have committed.
The end result was that the accused man was indeed guilty, but he minced his 
words so well that every sentence uttered was true. To the layman he seemed 
truthful and pleasant. Certainly innocent.
But to the man that never lied, his stories quickly fell apart as he himself 
had had years of practice with the same. And the guilty man was jailed.


I really enjoy reading your emails and github posts too, they have an 
eloquence and a brashness.


Quite.

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 05/07/2015 03:27 PM, Jeff Garzik wrote:

On 05/07/2015 03:25 PM, Peter Todd wrote:


In summary, I asked a question neither you, nor Peter Todd, want to
answer and want to actively discourage people from even asking at all.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBAgAGBQJVS4XZAAoJECpf2nDq2eYjwZcP/j4ypIBctC4Wt71KJCx4eJ3u
u8DQAJKKr8BfL/zDu5bwVz0qbIX1+Wv9EwkBSYuYQaLDozDCnlttptr7qNWm62QI
d5Z6HUU+g/Zbk2DSgVK57Hf3G7pzcodRq+fp6O/kNgtdE9OyZnv9giApd6F1Yy7l
wgZxjlpKGMA+qKigHSHIQyu1L2JfWjw7eEiirnDtFaCgTpJqPErigX+2eMdpj8/r
jTP3mEN2qStWYydWfYxfcM68gOZsvFiVBfT7qTkFXSeOdigC4bHMDMew9nqP1hlB
9uo/JESNQ4Z0/WHgDSn9fLbc/UX6SIPVn7vDAj7mZAeyaXYBrXhbHpdqhnOGhDmt
R9aUopGHleY44RujES1rQWSo6D8SWlbmpXThgHU5rlRFKKSCu9/s99s7kVdLFqpS
bGg42qs1LwxDiq2TuMV/9TuP10ibB4mSnKwaglcAHcrbo26ZdMF4T9YwKcEmHIrv
0hCUA0qyvKP3fqfQUVzcssJfWdvjx7/bnwLadrxSOur1IZj+2jJdzPYsT1tSiwL/
XChSN5a00LJWW5+b0ka155sEg8XcBdUECXIQtRpFedCURjeinGuMnEf6gM8NbcNS
yVm5Kptf8BO11r154J93nkc3gU4VFcxudg8smaDcq3amPDkyaNBXQm+rcwIApchL
SOzHWwxtA1q+pLHvxnlk
=Fcdg
-----END PGP SIGNATURE-----
-------------------------------------
I picked up the next available number myself, but can be changed to 
anything, the 74 is unimportant to the proposal.

David Barnes

On 7/17/2015 2:54 PM, Micha Bailey wrote:


-------------------------------------

This depends on how miners are connected.

E.g. suppose there are three miners, A and B have fast connectivity between
then, and C has a slow network.
Suppose that A miners a block and B receives it in 1 second. C receives it
in 6 seconds.
This means that blocks mined by C during these ~5 seconds will be orphaned
because B gets A's block first.
-------------------------------------
On 02/03/2015 01:22 AM, Pavol Rusnak wrote:


It should follow the spec. I know BIP32-hierarchy is short on gap
limits, which is why (amongst other reasons) I expect
BIP32-hierarchy-based wallets migrate to a better standard at some time.


If it follows BIP32, h=bip32 is fine.




-------------------------------------
Hey Gabe,

That's diving into the deep end for sure! :)

That depends on your interests.

Many of the highest priority tasks in Bitcoin Core are rather complicated,
unfortunately, even for people with experience. You can consult the issue
tracker to get a feel for it.

Alternatively, there are lots of wallet apps out there and plenty of more
straightforward projects on them. However they may have less of a research
flavour.
-------------------------------------
Hello all.

We’d like to share an idea we have to dramatically increase the bitcoin
block propagation speed after a new block has been mined for the first time.

Efficient bitcoin block propagation
A proposed solution to provide near-instantaneous block propagation on the
bitcoin network, even with slow network connections or large block sizes.
Increasing mining efficiency for everyone while decreasing transaction
confirmation times and strengthening the distributed nature of bitcoin.

Short summary: we propose to introduce bitcoin-backed guarantees
(“Guarantee Messages”) between miners. This would allow miners to mine on
blocks that are not yet fully transmitted. This reduces the effect of slow
internet connections, leveling the playing field between the 1st world
fiberoptic datacenter miners and the rest of the world. We also believe it
strengthens the bitcoin network by using existing processing power that is
currently wasted into further securing the blockchain, and it reduces the
likelihood of transactions becoming confirmed, then unconfirmed and then
-hopefully- confirmed again (due to different miners finding competing
blocks with different transactions at approx the same time).

It is possible to implement our idea as a fork of bitcoind, or as layer
between the standard bitcoind and the mining equipment. In the future it
could be incorporated in the bitcoin core if and when that becomes a
priority, but that step would not make sense until it becomes a priority.

There are a lot of nuances in this idea, and the first reaction is quite
probably that this is a crazy idea. We have attempted to address the most
important nuances in our proposal, which is currently at v.0.2.

We cannot guarantee that there are no ‘hidden devils in the details’ and we
invite you to be critical in a friendly and constructive manner. We will do
our best to answer all questions that arise.

The ‘official’ proposal is at:
PDF: http://pukaki.bz/efficient-bitcoin-block-propagation-v.0.2.pdf
HTML: http://pukaki.bz/efficient-bitcoin-block-propagation-v.0.2.html

-- Arnoud Kouwenhoven
-------------------------------------
The engineers at Google were well aware that ASN.1 existed. I can assure
you of that, because I was one of them.

The protobuf FAQ has a very polite take on the matter:

   https://developers.google.com/protocol-buffers/docs/faq

This email thread gives more enlightenment:

   https://groups.google.com/forum/#!topic/protobuf/eNAZlnPKVW4

Anyone who has actually had to work with both ASN.1 and protocol buffers
will be able to explain why ASN.1 should not be chosen for any modern
formats. A lot of it boils down to simplicty and quality of
implementations, especially open source implementations.

With respect to the specific concerns Richard raises:

Performance doesn't feel that relevant when you think that:

Performance wasn't a concern.



HTTP transmits files as binary on the wire. So it's binary-clean and,
moreover, HTTP/2 aka SPDY is fully binary and doesn't use text anywhere
except the gzip dictionary.



Luckily, this is also true of protocol buffers. Language support is pretty
good these days.



BIP 70 doesn't contain any code, as far as I know. The protobuf schema
might look like code, but it's not - it's just a description of what fields
a message can contain and their types. This is very relevant for a
specification!

JSON in particular is pretty awful and I don't like it much. It suffers
complexities with things as basic as encoding numbers and strings. It's
very much unsuited to applications where correctness matters and where
you're dealing with binary structures.
-------------------------------------
On Aug 11, 2015 12:52 AM, "Pieter Wuille" <pieter.wuille@gmail.com> wrote:
bitcoin-dev@lists.linuxfoundation.org> wrote:
transport? They
may be
is more
priority when you pay more. Taxi drivers can't pick out higher-paying
customers in advance.

I'm sorry, I missed your "if everyone is paying it". This changes a lot. I
agree with you: if everyone wants to pay much then it becomes unreliable.

But I don't think that is something we can avoid with a small constant
factor block size increase, and we don't do the world a service by making
it look like it works for longer.

Let's grow within bounderies set by technology and centralization pressure
that we can agree on. Let the market decide whether how they will that will
low volume reliable transactions and/or high volume unreliable ones.

-- 
Pieter
-------------------------------------
On Mon, May 25, 2015 at 1:29 PM, Andreas Schildbach
<andreas@schildbach.de> wrote:

Is it really wise to call this a "confirmation"?  All this is really
telling you is one seemingly random peer has relayed the transaction
back to you that you sent to a presumably different seemingly random
peer.

Warren


-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

It would - it assumes you have the set of keys and are sorting before
you derive and send funds to such a P2SH address.

It seems there is scope for further narrowing down how a multisig
scripthash address should be determined - what do people think of
anticipating only compressed keys for scripts?

It's possible to cause confusion if one put forward a compressed key at
some time, and an uncompressed key at another. A different script hash
would be produced even though there is no difference to the keys
involved. The client will not search for this.


Having spoken with Jean-Pierre and Ruben about this for quite some time
now, there is 100% the need for a BIP outlining this. Everyone has had
the idea at some point, and some of us already using it, but people
shouldn't have to go digging in BIP45 for the two lines which mention
it. All we need is a place to put the docs.

I am building up a list of implementations which currently support
sorting, and briefly describing a motivation for such a BIP.


On 16/01/15 10:16, Ruben de Vries wrote:
in that context?
<mailto:laanwj@gmail.com>> wrote:
<bip@mattwhitlock.name <mailto:bip@mattwhitlock.name>> wrote:
they're embedded in Script as a push operation whose payload is the raw
bytes of the big-endian representation of the integer). As far as I
know, DER encoding is only used for signatures. Am I mistaken?
Amsterdam with registration No.:60262060 and VAT No.:NL853833035B01
------------------------------------------------------------------------------

- -- 
Thomas Kerin
- -------------------------

My PGP key can be found here
<http://pgp.mit.edu/pks/lookup?op=get&search=0x3F0D2F83A2966155>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQJ8BAEBCgBmBQJUuT2EXxSAAAAAAC4AKGlzc3Vlci1mcHJAbm90YXRpb25zLm9w
ZW5wZ3AuZmlmdGhob3JzZW1hbi5uZXQ2MzI1MzM4QjJGOTU5OEUzREMzQzc0MzAz
RjBEMkY4M0EyOTY2MTU1AAoJED8NL4OilmFV4GgP/Rr955cDBA34e58lLdjXkqzi
EYDH5QfsTdUQQVUvkK0OBq7RQwkbb7Kn5u6U8UD3hEhaWwQGhrQ/gOJrqM68glma
YfYupugMesTTu4Fxm/AtNv4Cifr29EZB1gu9hBeZGT4FL863+0ShvWHdHvscOcmg
3SGv0De+1bd93j7p+9jyWh/sYpHEdi0lQBMkkCzSzhXPZzoHEglUmVYBRcmrjaag
ycHuQfN5zjM0fJ18R6f7PCOOAhDi9+7xpikDArvHmKb4BZjOuMBTprN2Mzdg98Uz
Rw4LRsLuht5VCnWHvC8+TUUEMUO8QOMrRxLYJSDVGcl0XYXT0EiRfnkqCr5ab8mm
KqLcxpSLxrDGd4OiHwWB7oDsg9tWXwVmyQgFsTLsxaNkL8AFRG59mAhbK9j+0+1E
Bd/pMx0VgGXpn1Urism5YlrR4FZ5USbYn9O0NxhUkQb550qvRtaAQNUVSJPEW0AG
/2pQdFOOqkI1wI0g2L/ZcC+fwBqUok+5MyMTb4NuuvaMDpR7vOeeobIpYLjL0VVZ
dNzfnlCQxGw/7QrFIbvnye8fNIMZZ9qtJx00bvXYizRyUhrF/FrRgwj2DhEjz6xM
3+CHKXNmb0qGg6jKgHvXQFic2DVo3IaNmZtVDBqyqCBKmC/A65rRws5uxIimUsIC
k4af62ZBGpSAhJ4ajCIY
=Ni9V
-----END PGP SIGNATURE-----

-------------------------------------
On Tue, Sep 15, 2015 at 12:10:37AM -0400, Jeff Garzik via bitcoin-dev wrote:

Incidentally, it'd help if we got some insight into why those branches
are being maintained; what features are in those branches that Bitcoin
Core doesn't have?

I've run into a number of cases where companies were maintaining forks
of Bitcoin Core unnecessarily, where a different, loosely coupled,
architecture could do what they needed to do without including the new
logic in the codebase itself.

-- 
'peter'[:-1]@petertodd.org
000000000000000013137b1bd77e352d28fa36309be1c821180eda408bcb745c
-------------------------------------
It's pretty obvious that Dave is suggesting an alternate tie-breaker:


I do see a problem with the proposal. Right now, when a miner sees a
new block with the most work and there are no ties, it is always a
good idea to build on top of it (unless they're in the middle of
building a private chain, or other pathological cases).

With this new heuristic (assuming it is actually followed by a good
chunk of people), a miner can reasonably know whether or not they can
safely mine a sibling of the block instead. When enough widely
propagated transactions exist, and the block to orphan is small,
there's minimal risk in mining a sibling block instead of a child
block (the only extra risk is in someone else mining a child block
right around the time we suceed in mining a siblish block, where we'll
definitely be orphaned instead of ~50% of the time).

Because the risk can be measured and is sometimes very small, it will
then be profitable for a miner to orphan a small non-empty block and
double-spend some confirmed transactions whenever the block confirming
them is easily replaced. This lowers the security of 1-conf
transactions.

Mind you, that risk doesn't apply if we prefer non-empty blocks to
empty blocks and leave it at that, or only switch if the new block
doesn't double spend transactions in the old one, so it's a fixable
issue.

On 11 September 2015 at 12:32, Jorge Timón
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hi everyone,

We just released the whitepaper describing Bitcoin-NG, a new technique for
addressing some of the scalability challenges faced by Bitcoin.
Surprisingly, Bitcoin-NG can simultaneously increase throughput while
reducing latency, and do so without impacting Bitcoin's open architecture
or changing its trust model. This post illustrates the core technique:
     http://hackingdistributed.com/2015/10/14/bitcoin-ng/
while the whitepaper has all the nitty gritty details:
     http://arxiv.org/abs/1510.02037

Fitting NG on top of the current Bitcoin blockchain is future work that we
think is quite possible. NG is compatible with both Bitcoin as is, as well
as Blockstream-like sidechains, and we currently are not planning to
compete commercially with either technology -- we see NG as being
complementary to both efforts. This is pure science, published and shared
with the community to advance the state of blockchains and to help them
reach throughputs and latencies required of cutting edge fintech
applications. Perhaps it can be adopted, or perhaps it can provide the
spark of inspiration for someone else to come up with even better solutions.

We would be delighted to hear your feedback.
- Ittay Eyal and E. Gün Sirer.
-------------------------------------


If Core ships CLTV as is, then XT will have to adopt it - such is the
nature of a consensus system.

This will not change the fact that the rollout strategy is bad and nobody
has answered my extremely basic question: *why* is it being done in this
way, given the numerous downsides?
-------------------------------------
On 3 August 2015 at 08:16, Simon Liu via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



What's the current stance of the Chinese pools on Bitcoin XT, should
Bitcoin Core refuse to increase the block size to 8 MB in a timely fashion?
Would they run it if the economic majority (e.g. Coinbase, Bitpay, etc.)
publicly stated their support for big blocks?
-------------------------------------
On Fri, Jun 19, 2015 at 09:44:08AM -0400, Peter Todd wrote:

Incidentally, because someone asked that message was sent two weeks ago.


Also, a shout-out to Marshal Long of FinalHash for his help with
(FSS)-RBF deployment and for getting F2Pool and myself in touch, as well
as his work in talking getting pools on board with BIP66.

-- 
'peter'[:-1]@petertodd.org
00000000000000000bb4abd88c6b023e9f19a1c1deaac120467279c330a803cf
-------------------------------------
On 14/10/15 19:02, Jeff Garzik via bitcoin-dev wrote:

Even if we assume everybody will try to approach that topic in good
faith, I don't think it's that simple.

A term that's become popular recently is "Bitcoin maximalist", and it's
frequently used as a slur or insult.

I honestly find that to be incomprehensible. If somebody at a Ford board
meeting started talking about how Ford needed to make sure Toyota was
able to sell enough cars, they wouldn't get very far by labelling their
critics as "Ford maximalists".

Anyone who works at Ford and who isn't a Ford maximalist is in the wrong
job.

And yet in Bitcoin, a much development is funded by companies who offer
products which compete with Bitcoin, or at least would be in competition
if Bitcoin were to achieve unlimited success.

I expect this is a minority view on this list, but my position is that
anyone who is not a Bitcoin maximalists has a potential conflict of
interest if they're also involved in Bitcoin development.

I also suspect this issue is a cause of much user dissatisfaction with
Bitcoin development. If Bitcoin users and investors don't trust that the
developers are working toward the unlimited success case, they can and
will revolt.

-------------------------------------
Facebook has a LevelDB fork which is maintained.
It's called RocksDB and the API seems to be nearly the same as for LevelDB,
thus maybe easy to replace: http://rocksdb.org/
https://github.com/facebook/rocksdb

Although I don't know if we might have some negative effects for our
use-case since RocksDB was optimized for big databases running on multiple
cores.


2015-10-22 23:26 GMT+02:00 Jeff Garzik via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
On Mon, Jun 15, 2015 at 1:09 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:


As noted to Adam last night - although I agree it adds complexity - side
chains are one solution that will indeed help with scaling long term.
Similar to the graph you see with git repos and merges, having aggregation
chains that arbitrarily fork and then rejoin the main chain are both
feasible and useful.

That code & future is a ways away from production, so doesn't help us
here.  Still, let's not dismiss it as a solution either.

-- 
Jeff Garzik
Bitcoin core developer and open source evangelist
BitPay, Inc.      https://bitpay.com/
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512


TL;DR disable UPnP in Bitcoin Core as soon as possible, if you still have it enabled.

Upgrading to 0.11.1rc2 or 0.10.3rc2 will also solve the issue, as they bundle a newer libupnpc (as well as disable upnp usage by default.) However these versions are still in the release candidate cycle, there is some risk in using test versions.

See https://bitcoin.org/en/alert/2015-10-12-upnp-vulnerability for details

Wladimir
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWG+rxAAoJEHSBCwEjRsmmh14H/jWEqINoAdb9CNE5pOiFv9FG
X51SCeZ/OCQXJ5qQGgcpMfP1w2fPFJwzrrJFIp9D8MUYXc9f6ZHo0A0Uc8LmPlrW
46Wu/TgN0N5XpJ8yDzDk1GxU3fGhGEX897SOxrt8NEUcrJBC1kaLlG01ma2Mf+VJ
wXsn++pgWO/9CCQzRIBNdJf1a8qnMsyRbryW7IsLNGiR4GRKzt9Hcp/p2vVxYFdD
bjVAWsEFnRga0ho0Kpnp5RxFZxVkL03ls6yj9wqZtlMHVGuyVWiwFqMjOV30wBfv
uENkWe/6veIU+Y3PmbuPJv79kRW2xTGZTl1RIKgJAdxVWPJy58a999AToIs/BWM=
=XC8t
-----END PGP SIGNATURE-----

-------------------------------------
In September, 2014, a collective experiment began in the bitcoin
ecosystem.  Available block space (1MB) began to sometimes fall short of
the space required to mine all of the transactions that would otherwise
have been included.

This chart, posted earlier, shows the onset of the
some-blocks-at-maximum era. http://i.imgur.com/5Gfh9CW.png

Although the average block is only about 400K, real blocks are bigger or
smaller due to the random length of time between blocks (and other
factors).  I look at how often this is predicted to happen.

Recently, transactions have been confirmed at a rate of about
100000/day*.  The average transaction size for the past 6000 blocks has
been 545 bytes.  Using these values,

txesPerMinute = 100000 / 24 / 60 = 69.4
txesInMaxBlock = 999977 / 545 = 1834
minutesToFillBlock = txesInMaxBlock/txesPerMinute = 26.4

Using the theoretical formula for the time before an inter-block
interval of at least a given length **

blockChickenMinutes[x] := 10 (exp(x/10) - x/10 - 1)

we obtain

minutesBetweenFullBlocks = blockChickenMinutes[minutesToFillBlock] = 104

We currently expect a maximum-size block every 1 hour + 44 minutes, on
average.  If the transaction rate doubles, we should expect a
maximum-size block every 14 minutes, on average.  The non-linearity
makes sense, because doubling the average without raising the maximum
requires disproportionately more maximum-size blocks.

This estimate is understated because transaction size and submission
rate have their own distributions.  Using the averages of 545 bytes and
100000/day ignores the fact that for some blocks, there are unusually
big and/or numerous transactions, which increases the block size
variance and causes blocks over the threshold to be encountered more
frequently.

These calculations are confirmed by empirical observation of the most
recent 6000 blocks:

http://i.imgur.com/0pQUsdl.png

In many cases, the miner chose to create a 750KB block, which is
unusually likely to be followed by another 750KB or 1MB block, because
the next interval starts off with a 250KB backlog.  Some backlog
transactions may experience more than 1 block delay in these cases.


* https://www.quandl.com/data/BCHAIN/NTRAN-Bitcoin-Number-of-Transactions

** This is a chicken-crossing-the-road problem. Wait time = (exp(λx) −
λx - 1) / λ
Some discussion at
https://github.com/nanotube/supybot-bitcoin-marketmonitor/pull/68.





-------------------------------------
OK, a few things here:

The Bitcoin network was designed (or should be designed) with the requirement that it can withstand deliberate double-spend attacks that can come from anywhere at any time…and relaxing this assumption without adequately assessing the risk (i.e. I’ve never been hacked before so I can assume it’s safe) is extremely dangerous at best and just horrid security practice at worst. Your users might not thank you for not getting hacked - but they surely will not like it when you DO get hacked…and lack a proper recovery plan.

Furthermore, the protocol itself makes no assumptions regarding the intentions behind someone signing two conflicting transactions. There are many potential use cases where doing so could make a lot of sense. Had the protocol been designed along the lines of, say, tendermint…where signing multiple conflicting blocks results in loss of one’s funds…then the protocol itself disincentivizes the behavior without requiring any sort of altruistic, moralistic assumptions. That would also mean we’d need a different mechanism for the use cases that things like RBF address.

Thirdly, taken to the extreme, the viewpoint of “signing a conflicting transaction is fraud and vandalism” means that if for whatever reason you attempt to propagate a transaction and nobody mines it for a very long time, you’re not entitled to immediately reclaim those funds…they must remain in limbo forever.


- Eric Lombrozo



-------------------------------------
Hi Adam,

Provisional answers below!

- Are you releasing a BIP for that proposal for review?

The work splits like this:

   - Gavin is writing the code and I think a BIP as well

   - I will review both and mostly delegate to Gavin's good taste around
   the details, unless there is some very strong disagreement. But that seems
   unlikely.

   - I have been handling gitian and the patch rebases, the code signing
   and so on, so far. I've also been doing some work to setup the basic
   infrastructure of the project (website etc).


- If the reviewers all say NACK will you take on board their suggestions?

Feedback will be read. There are no NACKS in Bitcoin XT. Patch requests
aren't scored in any way. The final decision rests with the maintainer as
in ~all open source projects.




Yes, I have been working on an article that explains how we got to this
point from my perspective. It is quite long, but only because I want it to
be readable for people who weren't following the debate.

Anyway, I think I've laid out the gist of it over and over again, but to
summarise:

If Bitcoin runs out of capacity *it will break and many of our users will
leave*. That is not an acceptable outcome for myself or the many other
wallet, service and merchant developers who have worked for years to build
an ecosystem around this protocol.




The approach is the same for other forks. Voting via block versions and
then when there's been >X% for Y time units the 1mb limit is
lifted/replaced.





Good question!  I have various thoughts on this, but let's wait and see
what happens first. Perhaps the new chain won't get the majority on it.

In the event that the >1mb chain does eventually win, I would expect Core
to apply the patch and rejoin the consensus rather than lose all its users.
That would take XT back to being a fairly small patchset to improve the
network protocol.



- Do you have contingency plans for what to do if the non-consensus

Where did you get the $3B figure from? The fork either doesn't happen, or
it happens after quite a long period of people knowing it's going to happen
- for example because their full node is printing "You need to upgrade"
messages due to seeing the larger block version, or because they read the
news, or because they heard about it via some other mechanisms.

Let me flip the question around. Do you have a contingency plan if Bitcoin
runs out of capacity and significant user disruption occurs that results in
exodus, followed by fall in BTC price? The only one I've seen is "we can
perform an emergency hard fork in a few weeks"!





Gavin and I have been polling many key players in the ecosystem. The
consensus you seek does exist. All wallet developers (except Lawrence), all
the major exchanges, all the major payment processors and many of the major
mining pools want to see the limit lifted (I haven't been talking to pools,
Gavin has).

This notion that the change has no consensus is based on you polling the
people directly around you and people who like to spend all day on this
mailing list. It's not an accurate reflection of the wider Bitcoin
community and that is one of the leading reasons there is going to be a
fork. A small number of people have been flatly ignoring LOTS of highly
technical and passionate developers who have written vast amounts of code,
built up the Bitcoin user base, designed hardware and software, and yes
built companies.

How do you think that makes Bitcoin Core look to the rest of the Bitcoin
world? How much confidence does that give people?



Of the overall process, I think you can agree we should not be making

This debate will never end until a fork makes it irrelevant. There is no
process for ending it, despite me begging Wladimir to make one.

And there is no haste. We have been debating the block size limit for
*years*. We have known it must be lifted for *years*. I kicked off this
current round of debates after realising that Wladimir's release timeline
wouldn't allow a block size limit to be released before the end of the
year. The reason we're talking about it now and not next year is exactly to
ensure there is plenty of time.






I really wish you were right, and I definitely feel you are one of the more
reasonable ones Adam. But the overwhelming impression I get from a few
others here is that no, they don't want to scale Bitcoin. They already
decided it's a technological dead end. They want to kick end users out in
order to "incentivise" (force) the creation of some other alternative,
claiming that it's still Bitcoin whilst ignoring basic details ... like the
fact that no existing wallets or services would work.

Scaling Bitcoin can only be achieved by letting it grow, and letting people
tackle each bottleneck as it arises at the right times. Not by convincing
ourselves that success is failure.
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Jorge,

You know, it is always insightful to get the perspective of active
participants and Core developers like yourself. As Adam pointed out
earlier, the developers have done mileage in this space and have
already considered most of the conceptual issues and technical
challenges that must resurface in waves as new interested parties join
the list. Allow me, in this response to your message, to make a
proposal to those who may be interested:


Bitcoin's protocol functions and the implications of this innovation
for the future are difficult to grasp, even for the smartest among us.
Then there are also the words of Niels Bohr:

"Prediction is difficult, especially about the future."

They say a lot of time and energy is wasted because we don't know what
we don't know. Years of discussion among those in the list has
established certain axioms that determine the options for Bitcoin
going forward. According to my comprehension, the following are some
of the most relevant for the present discussion (please correct me
where I'm off the mark):

1. A high degree of decentralization is prima optima.

2.  Bitcoin is much more than a payment network. A lot of the
non-payment features are, arguably, what gives Bitcoin most of its
value. Yet, the payment functionality is a major design feature and
all agree that it should scale - subject to axiom 1.

3. The Bitcoin payment network ("Layer 1"), due to technical
constraints imposed by its p2p design, cannot compete with Visa and
other centralized transmission channels for speed or transaction
volume. Nor can it handle the transaction requirements of the world's
population - the scaling required would necessarily render Bitcoin
centralized, insecure and, therefore, worthless.

4. The addition of "layer 2" protocols (such as Lightning and other
sidechains) will allow fast, low-fee (and with virtually instant
confirmation) bitcoin transactions within two years, according to the
developers active in that:
http://www.youtube.com/watch?v=jE_elgnIw3M
http://www.youtube.com/watch?v=fBS_ieDwQ9k

5. This "layering of protocols" simplifies the scaling (blocksize)
debate because it separates
 A) the primary concern for security and fidelity via
decentralization, and
 B) the ideal of universal accessibility via fast, low-fee transactions.
Discussion about scalability can therefore proceed with the knowledge
that Lightning and other "layer 2" sidechains will make Bitcoin
accessible to the global majority - and be fast like Bruce Lee - while
the Bitcoin developers can focus on making Bitcoin Core protocol
(layer 1) the world heavyweight champion - Muhammad Ali.

Since I've maintained your interest up to the final sentence, I say:
as an insurance against a capacity crisis before layer 2 is deployed,
why not implement bip100's 2MB blocksize proposals in a testnet?



On 07/30/2015 04:38 PM, Jorge Timón wrote:
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVuieTAAoJEGwAhlQc8H1meAIH/RHUV72bbMItZOT7rvhEU56r
lqEcvwXBSCYgsh1ieVeTdC/ydJnRSzWsdZxM3D7PEOzutlG+VaJQVSJREItGb2GW
PYiZ3uwSwF64nRq5bZ7aS2pT/Zo1a1yAf4H5rbeyxxoWC+zkmSsmcf73MgmslIuU
7XXHNztCX3glfOctr+J61WEKBw0ItQCTsp9J08yVlj/gvKTi3U2jDcYV5mf/3D0j
pvXl244DG4b+nYetRyyonYbZelSUYfCghNBJhUYZApVmcgfKDRPeX1uWfkl0HuUd
Kc+uZtrhJaUXdRlqc50nOsRSCAK+d4PGClF8JFlzI65+SG7VzkVqc8SkSDfNXfY=
=r4SA
-----END PGP SIGNATURE-----

-------------------------------------
phm got most of this, but...

On Sat, Sep 19, 2015 at 2:53 PM, phm via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Pot is used as money, and they do jail people for it, but it doesn't have
the effect to which you refer. It has the opposite effect, partially
because it enriches suppliers.

The 51% attack is a good point, but they would be taking a huge risk.
Ideas don't die, just people.  For example, they got Ross Ulbricht, not DPR.

Government is the group of people that does things that are not acceptable
if anyone else does them, and that is because people cheer for them when
they do those things, rather than pointing out that they are not
acceptable.  The movie "The Deep Web" shows how bitcoin helps to turn this
misfortune around.
-------------------------------------
"Fast transactions"
Fast transactions implies it is slower than Visa, and Visa is 'instant' by
comparison from the spender's POV. Bitcoin is still very instant because
wallets still send notifications/pings when transactions are first seen,
not when it goes into a block. We shouldn't mislead people into thinking a
transaction literally takes 10 minutes to travel the globe.

Maybe this feels like PR speak. But being too humble about Bitcoin's
attributes isn't a good idea either.

If we're going to look at perception, image and expectations, perhaps we
can start to look at redefining some terminology too. Like confirmations,
which is an arbitrary concept. Where possible we should describe it with
finance terminology.

"0 conf transaction"
0 conf is the 'transaction' - just the act of making an exchange. It
doesn't imply safe and I believe using the word 'settle' in place of
confirmations will automatically click with merchants.

"1st conf"
A 'confirmation' is a 'settlement'. If it is 'settled', it implies final
(except by court order), whereas confirmation usually means 'ah, I've seen
it come through'. I rarely hear any sales clerk call credit card
transactions confirmed. More often you will hear 'approved' instead.
Although 1st conf can be overtaken, so...

"n confirmations"
This term can probably stay since I can't come up with a better word.
Settlements only happen once, putting a number next to it breaks the
meaning of the word. "Settled with 4 confirmations" seems pretty clear.
Alternatively I think instead of displaying a meaningless number we ought
to go by a percentage (the double spend improbability) and go by
'confidence'. "Settled with 92% confidence." Or we can pick an arbitrary
number like 6 and use 'settling...' and 'settled' when reached.
-------------------------------------
On Mon, Oct 5, 2015 at 6:26 PM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


If you are referring to some of Mike's PRs that were either refused or
reverted, it was because they where substantial technical objections to
them. This isn't even in the same ballpark.

Surely you see the absurdity of arguing against soft forks after we
successfully used them already for BIP34 and BIP66?
-------------------------------------
Note that this violates present assumptions about transaction validity,
unless a constraint also exists that any output of such an expiry block is
not spent for at least 100 blocks.

Do you have a clean way of ensuring this?

On Thu, Sep 17, 2015 at 2:41 PM, jl2012 via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I think that it is important to note that Bitcoin XT faces a natural
uphill battle.

Since it is possible to setup atomic inter-fork coin trades. I do not
see how Bitcoin XT could possibly win if Satoshi decides to sell 10000
XTBTC for BTC everyday for the first 100 days after the fork.

In many ways Satoshi gets to decide the winning fork just by his huge
economic investment in Bitcoin.


Here is some simple game-theory for non-consensus forks:

1. Spoil the ballot. Have Bitcoin Core propagate the Bitcoin XT version
string.

2. Encourage all miners to false vote for the Bitcoin XT fork.

- Now people have no-idea what % of the economy Bitcoin XT holds. -
Making it impossible for people to put economic faith behind Bitcoin XT.

3. Setup good Atomic Swap markets.

4. Setup a fork of Bitcoin XT that allows people to easily make a
transaction only on the XT fork (while leaving the original BTC coins
untouched).


This means that the Bitcoin XT fork will be born per-mature. Probably
with only a small % of hashing power behind it (contrary to the almost
100% that falsely claim to support it). It will be embarrassing that for
the goal of larger blocks, XT instead has blocks (before re-adjustment)
every 2h.


The price for XTBTC coins will plummet, Satoshi progressively dumping
his 1M stash over a year or so will make sure that it doesn't recover
either.


I cannot see how Bitcoin XT is but-not in a extremely weak position from
game theory.

I'm sure smarter people than I could come up with even more ways to
disrupt non-consensus forks.

Cam.



On 16/8/2015 6:39 AM, muyuubyou via bitcoin-dev wrote:

-------------------------------------
Maybe Jeff can clarify but my communications with him seemed to imply
he didn't think any kind of difficulty penalty scheme is workable. I
strongly dispute that assertion.

On Thu, Sep 3, 2015 at 7:23 PM, Jorge Timón
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
One more thing I would like to add to this thread: I want to make it unequivocally clear that I believe what is making double-spends easier has relatively little to do with the protocol and almost everything to do with poor software and poor security policy on the merchant end. Perhaps it isn’t prudent to push out changes to the relay policy that make these exploits even easier right now - but we NEED to be applying some kind of pressure on the merchant end to upgrade their stuff to be more resilient so that we have more room for changes on things like relay policy without significant disruption to the network.

- Eric Lombrozo
-------------------------------------
Oh and I realised I stuffed up the subject and it talks about the addr relay but I actually answered my own question on the addr relaying, I had just miss interpreted one document I thought it was talking about subtracting 2 hours before relaying but I see we subtract 2 hours on receipt not relay because the if it hadn't been seen for 60 minutes previously it now becomes 3 hours and we use but don't relay makes sense. 

     On Thursday, 19 February 2015, 22:33, Thy Shizzle <thashiznets@yahoo.com.au> wrote:
   
 

  Hi, plugging away at my C# Bitcoin node "Lego.NET" Thashiznets/Lego.NET now I am currently working on addr relaying. I am as we speak wiring up my DB in Azure, and ready to start plopping net_addrs in my DB, all good however I'm reading two different specification docs that seem to be wildly varying. I mean the first one here Developer Reference - Bitcoin didn't mention that version message now has the 4 byte checksum and no time in the net_addrs and I was getting reject malformed messages until I found the other document which informed me we now use the 4 byte checksum in version and no time in the net-addrs in version message. So I solved that and here is the other doco. I have found other variances like one document said that the heartbeat AND disconnect were 30 minutes, but then in the other document I read that Heartbeat is 30 minutes and disconnect is 90 minutes which seems far more sensible so I went with that and modified my code. Is there any other variations between these two spec docos that perhaps some of you devs know about that I need to look out for! Thanks! Shizzle.
|   |
|   |  |   |   |   |   |   |
| Thashiznets/Lego.NETLego.NET - A C# full node for processing the Bitcoin block chain |
|  |
| View on github.com | Preview by Yahoo |
|  |
|   |

  
|   |
|   |  |   |   |   |   |   |
| Developer Reference - BitcoinBETA: This documentation has not been extensively reviewed by Bitcoin experts and so likely contains numerous errors. Please use the Issue and Edit links on the bot... |
|  |
| View on bitcoin.org | Preview by Yahoo |
|  |
|   |

  
|   |
|   |   |   |   |   |
| Satoshi Client Node Discovery - BitcoinContents 1 Overview 2 Handling Message "getaddr" 3 Discovery Methods 3.1 Local Client's External Address 3.2 Connect Callback Address 3.3 IRC Addresses 3.4 DNS Addresses  |
|  |
| View on en.bitcoin.it | Preview by Yahoo |
|  |
|   |

    

 
   
-------------------------------------
That was easy.


-------------------------------------
On Fri, Jul 31, 2015 at 12:16 PM, Mike Hearn via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

He is not saying that. Whatever the reasons for centralization are, it
is obvious that increasing the size won't help.
In the best case, it will only make it slightly worse. How big of a
"slightly worse" are we willing to risk to increase the size is the
open question.


As far as I know, people just want to change an arbitrary number for
another arbitrary number.
But this arbitrary cap is a cap to centralization, not a tool to make
Bitcoin-Qt more important or to attack concrete Bitcoin companies like
you seem to think.
If you don't think the blocksize cap helps limiting centralization and
you think it would be fine to completely remove it, then it would be
better for the conversation that you said that directly instead of
supporting other arbitrary caps like 8GB (bip101).

I think it would be nice to have some sort of simulation to calculate
a "centralization heuristic" for different possible blocksize values
so we can compare these arbitrary numbers somehow. Even if the first
definition of this "centralization heuristic" is stupid, it would be
better than keep rolling dices and heatedly defend one result over
another.
To reiterate, If you don't think the blocksize cap helps limiting
centralization, please, say so.
If we can't agree on what the limit is for, we will never be able to
agree on whether 1MB (current situation) or 8GB (bip101) is the most
appropriate value to have at a given point in time.


Lightning is nothing more than a better design for trustless payment
channels, but it's really good that you agree that if we want to scale
not everything can be broadcast in-chain.


What he means is that if Bitcoin needs to support a scale that is only
feasible with high degrees of centralization (say, supporting 1 M tx/s
right now), then it has already failed in its decentralization goals.
In fact, with only a few miners, I'm not sure regulators will still
agree Bitcoin transactions are irreversible...
But you are right, we haven't tried to destroy bitcoin by removing the
only available consensus tool to limit centralization yet.
I don't want to try, do you?


Let's go to "most people use bitcoin" first and then think about "many
people ONLY use Bitcoin" later, please.
I believe everybody here thinks that the more people are able to use
Bitcoin, the better.
But that doesn't


Risking destroying Bitcoin through centralization to be able to keep
free transactions for longer it's a very risky gamble.
Doing so explicitly against the will of some of the users by promoting
schism hardfork, and thus risking to economically destroy both Bitcoin
and Bitcoin_new_size (different currencies) in the process is also a
very risky gamble.
So may want to give some example of responsibility yourself to make
these calls to responsibility more credible.
You certainly cannot know what "all the payment processors and
startups plans" are based on, and spreading conspiracy theories about
the evil secret plans of Blockstream (or any other Bitcoin company)
doesn't help in keeping this discussion civilized, contaminates
bitcoin development in general and unhealthily polarizes the whole
Bitcoin ecosystem. Also, I believe is doing a disservice to your
reputation among technical people, but since you don't seem worried
about that, why should I be?


Are you suggesting that bitcoin consensus rules should be designed to
maximize the profits of Bitcoin exchanges?
I assume not, but I'm really having troubles trying to read the
question with another meaning.
Can you rephrase this, please?

-------------------------------------
On Wed, Dec 9, 2015 at 6:59 AM, Mark Friedenbach <mark@friedenbach.org> wrote:

I didn't comment on the transaction output. I have commented on
coinbase outputs and on a hard-fork.

Using an output in the last transaction would break the assumption
that you can truncate a block and still have a valid block. This is
used by some mining setups currently, because GBT does not generate
the coinbase transaction and so cannot know its size; and you may have
to drop the last transaction(s) to make room for it.

That a block can be truncated and still result in a valid block also
seems like a useful property to me.

If the input for that transaction is supposed to be generated from a
coinbase output some blocks earlier, then this may again run into
hardware output constraints in coinbase transactions. (But it may be
better since it wouldn't matter which output created it.). This could
likely be escaped by creating a zero value output only once and just
rolling it forward.

-------------------------------------

Quoting Tier Nolan via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>:


I refrain from calling it the "main chain". I use "original chain" and  
"new chain" instead as I make no assumption about the distribution of  
mining power. This BIP still works when we have a 50/50 hardfork. The  
main point is to protect all users on both chains, and allow them to  
make an informed choice.



Again, as I make no assumption about the mining power distribution,  
the new chain may actually have less miner support. Without any  
protection (AFAIK, for example, BIP100, 101, 102), the weaker new  
chain will get 51%-attacked by the original chain constantly.



I guess the git hash is not known until the code is written? (correct  
me if I'm wrong) As the coinbase message is consensus-critical, it  
must be part of the source code and therefore you can't use any kind  
of hash of the code itself (a chicken-and-egg problem)


This may not be compatible with the other version bits voting mechanisms.


Yes



The flag block itself is a hardfork already and old miners will not  
mine on top of the flag block. So your suggestion won't be helpful in  
this situation.

To make it really meaningful, we need to consume one more bit of the  
'version' field ("notice bit"). Supporting miners will turn on the  
notice bit, and include a message in coinbase ("notice block"). When a  
full node/SPV node find many notice blocks with the same coinbase  
message, they could bet that the subsequent flag block is a legit one.  
However, an attacker may still troll you by injecting an invalid flag  
block after many legit notice blocks. So I'm not sure if it is worth  
the added complexity.




-------------------------------------
On Wed, Jul 15, 2015 at 12:06 PM, Me via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Running normal full nodes only provides extra service to nodes
synchronizing and lightweight clients. It does not "make the network
stronger" in the sense that it does not reduce the trust the participants
need to have in each other.

It's such a misconception that running many nodes somehow helps. It's much
better that you run and control one or a few full nodes which you actually
use to validate your transactions, than to run 1000s of nodes in third
party datacenters. The latter only looks more decentralized.

-- 
Pieter
-------------------------------------
On Sat, Jun 27, 2015 at 2:10 PM, NxtChg <nxtchg@hush.com> wrote:

Fortunately we have a lower limit in the standard mining policy to see
if the skies turn purple when we hit that limit like some people
predict.


But this is NOT a way to see the majority of anything. I can run 1000
nodes, have you heard of sybil attacks?
There's simply no decentralized way of voting that works. Otherwise we
could vote on each block instead of using proof of work.
Miners voting on size is also ridiculous since big miners have an
incentive to completely remove the limit and make smaller miners
unprofitable.


No, this is very important. The majority has no right to dictate on
the minority.
If the majority of bitcoiners wanted demurrage (and we actually had a
working method for "measuring majorities"), the minority would still
say "these are not the rules we signed up for, go make freicoin as a
separate chain".
And that is very reasonable. If some people want a more centralized
version of Bitcoin they can create an altcoin too. Doesn't dogecoin
already have big blocks?

-------------------------------------


In retrospect I regret not having made this note more emphatic:

GUYS, WE’VE KNOWN ABOUT THESE PROBLEMS AND HAVE TALKED ABOUT THEM FOR YEARS ALREADY…AND IT SEEMS PRACTICALLY NOTHING HAS HAPPENED…WTF?!?!?!?
-------------------------------------
This is my biggest headache with practical bitcoin usage. I'd love to hear it if
anyone has any clever solutions to the wallet/utxo locked problem. Spending
unconfirmed outputs really requires a different security model on the part of
the receiver than #confirmations, but isn't inherently bad if the receiver has a
better security model and knows how to compute the probability that an
unconfirmed-spend will get confirmed. Of course the bigger problem is wallet
software that refuses to spend unconfirmed outputs.

I've thought a bit about a fork/merge design: if the change were computed by the
network instead of the submitter, two transactions having the same change
address and a common input could be straightforwardly merged or split (in a
reorg), where with bitcoin currently it would be considered a double-spend.  Of
course that has big privacy implications since it directly exposes the change
address, and is a hard fork, but is much closer to what people expect of a
debit-based "account" in traditional banking.

The fact of the matter is that having numerous sequential debits on an account
is an extremely common use case, and bitcoin is obtuse in this respect.

On May 9, 2015 1:09:32 PM EDT, Jim Phillips <jim@ergophobia.org> wrote:

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------------------------------
Errata + clarity (in bold):
On Wed, Sep 9, 2015 at 9:11 AM, Washington Sanchez <
washington.sanchez@gmail.com> wrote:



-- 
-------------------------------------------
*Dr Washington Y. Sanchez <http://onename.com/drwasho>*
Co-founder, OB1 <http://ob1.io>
Core developer of OpenBazaar <https://openbazaar.org>
@drwasho <https://twitter.com/drwasho>
-------------------------------------
I agree that the use protocol buffer and x509 by BIP70 is a poor choice.

The choice should have been done to maximize portability, not to maximize
efficiency and flexibility.

What I ended up doing for having a similar codebase on all plateform is to
parse a BIP70 messages with the help of a web service that convert it to
JSON.
I don't like this solution since it had a trust dependency, and the
certificate verification become handled by the web service, not the device.
But even if I solved google buffer problem, I would stumble upon having
headache to validate the x509 certificate chain on every plateforms.

A simple BIP70 using JSON + HTTPS would have make things more easy.
I agree that it requires that the merchant own the domain name of the BIP70
endpoint, but I don't consider such a big of a deal, since this is how
e-commerce works.
-------------------------------------
On Sun, May 31, 2015 at 1:29 AM, Matt Whitlock <bip@mattwhitlock.name>
wrote:


I seriously doubt if miners and merchants who's income depends on bitcoin
are going to risk a network split. Gavin isn't pedalling some mempool
policy which doesn't affect consensus. The changes have to be universally
adopted by miners and full nodes. If there is any uncertainty about that
global acceptance, those financially dependent on bitcoin will not take the
risk just to be political. You can see how conservative the mining
community is already by their slow upgrade of Bitcoin Core as it is. Even
if some miners and merchants generally support the idea of bigger blocks,
they most certainly are not going to take the risk of leading a hard fork
when there is substantial risk of it failing.

Until there is actual consensus among the technical community I wouldn't be
too concerned.



I don't think anyone is ignoring the issues, nor that everyone accepts that
blocksize may have to eventually change. The overwhelming technical
majority do not agree there is a problem that needs to be immediately
addressed. It would be far more helpful if we focused on stuff that helps
enable level 2 technologies so that bitcoin can actually scale, (like
R/CLTV and malleability fixes which are being delayed by BIP66 rollout and
pending the new "concurrent soft-forks" proposal).



But of course it would be dealt with if and when it becomes necessary. It's
not like there is blanket opposition to increasing the blocksize ever, it's
the matter of if, when and how; but when is defintely not now.

9. My proposal is that we raise the block size limit *gradually*, using an

Automatic or dynamic blocksize increase risks being very difficult to shut
down if later we find it is negatively impacting the ecosystem... and
that's part of the reluctance with bigger blocks because we still have not
studied the potential downsides enough beyond some sketchy and disputed
calculations and overall it's not addressing scalability at all.




Extending blocksize now would be nothing more than a political move. I have
no idea what will be decided in the end, but I do know that in order for
bitcoin to survive, changes must be based on well thought out and discussed
technical merits and not the result of political pressure. Politics and
good software do not mix.

Drak
-------------------------------------
Gregory Maxwell via bitcoin-dev 於 2015-08-23 21:01 寫到:


I think this comment is more related to BIP68 instead of OP_CSV? Without 
further complicating the BIP68, I believe the best way to leave room for 
improvement is to spend a bit in tx nVersion to indicate the activation 
of BIP68. I have raised this issue before with 
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010043.html 
However, it seems Mark isn't in favor of my proposal

The idea is not to permanently change the meaning of nSequence. 
Actually, BIP68 is "only enforced if the most significant bit of the 
sequence number field is set." So BIP68 is optional, anyway. All I 
suggest is to move the flag from nSequence to nVersion. However, this 
will leave much bigger room for using nSequence for other purpose in the 
future.

AFAIK, nSequence is the only user definable and signed element in TxIn. 
There could be more interesting use of this field and we should not 
change its meaning permanently. (e.g. if nSequence had 8 bytes instead 
of 4 bytes, it could be used to indicate the value of the input to fix 
this problem: https://bitcointalk.org/index.php?topic=181734.0 )


-------------------------------------

Bitcoin is only a partial solution to the Byzantine general problem. 
Users do need to trust that things such as mining and development 
systems work as intended.  Once the user trusts those systems only then 
is the state of the ledger trustless.  Just because the state of ledger 
is decentralized due to mining that does not automatically mean 
everything associated with Bitcoin is "decentralized."  (Some people 
actually claim reddit is decentralized because users can vote.  That 
would mean the US government is also decentralized since there are 
elections but i don't think most people would agree with that definition.)

Centralized and decentralized system are not intrinsically good or bad. 
  Each one has it use cases just like a hammer and a screw driver. 
Claiming otherwise is treating Bitcoin a as religion rather than a 
technology.

Russ


-------------------------------------
On Wed, Aug 19, 2015 at 02:27:10PM -0700, Joseph Poon via bitcoin-dev wrote:

ACK on removing the inversion of nSequence from what would be human
readable.

I don't want to spend the rest of my life mentally having to subtrace
from 0xFFFFFFFF :)

-- 
'peter'[:-1]@petertodd.org
00000000000000000402fe6fb9ad613c93e12bddfc6ec02a2bd92f002050594d
-------------------------------------
Ideally, the metrics that we settle on would be architecture agnostic and have some sort of conversion metric to map it onto any specific architecture. An Intel based architecture is going to perform vastly different from an ARM based one for example.

Simple example: The PS3 PPE and Xbox 360 CPU are RISC processors that run at 3.2GHz, but their non-vector performance is rather poor. You’d be lucky to get about 33% effective utilization out of them (up to 50%, tops, but that’s really pushing it), so if you were to map this onto another architecture, you’d have at least a 3x conversion from this end alone (the other end could also have a scaling factor).

Ultimately, how these values are expressed isn’t the important part. It’s the ability to measure the impact of a change that’s important. If some metric changes by, say, 5%, then it doesn’t really matter if it’s expressed in MIPS, INTOPS, MB or GB. The fact that it changed is what matters and what the effect is on the baseline (that ultimately could be expressed as a certain specific hardware configuration). It would probably be practical to have a number of comparable concrete min spec configurations and even more ideal would be if people in the community would have these systems up and running to do actual on-target performance benchmarks.


jp



-------------------------------------
The choice is very real and on-point.  What should the block size limit
be?  Why?

There is a large consensus that it needs increasing.  To what?  By what
factor?

The size limit literally defines the fee market, the whole damn thing.  If
software high priests choose a size limit of 300k, space is scarce, fees
are bid high.  If software high priests choose a size limit of 32mb, space
is plentiful, fees are near zero.  Market actors take their signals
accordingly.  Some business models boom, some business models fail, as a
direct result of changing this unintentionally-added speedbump.  Different
users value adoption, decentralization etc. differently.

The size limit is an economic policy lever that needs to be transitioned
-away- from software and software developers, to the free market.

A simple, e.g. hard fork to 2MB or 4MB does not fix higher level governance
problems associated with actors lobbying developers, even if a cloistered
and vetted Technical Advisory Board as has been proposed.







On Sun, Jun 14, 2015 at 1:20 AM, Eric Lombrozo <elombrozo@gmail.com> wrote:



-- 
Jeff Garzik
Bitcoin core developer and open source evangelist
BitPay, Inc.      https://bitpay.com/
-------------------------------------
BIP68 currently represents by-height locks as a simple 16-bit integer of
the number of blocks - effectively giving a granularity of 600 seconds
on average - but for for by-time locks the representation is a 25-bit
integer with granularity of 1 second. However this granularity doesn't
make sense with BIP113, median time-past as endpoint for lock-time
calcualtions, and poses potential problems for future upgrades.


There's two cases to consider here:

1) No competing transactions

By this we mean that the nSequence field is being used simply to delay
when an output can be spent; there aren't competing transactions trying
to spend that output and thus we're not concerned about one transaction
getting mined before another "out of order". For instance, an 2-factor
escrow service like GreenAddress could use nSequence with
CHECKSEQUENCEVERIFY (CSV) to guarantee that users will eventually get
their funds back after some timeout.

In this use-case exact miner behavior is irrelevant. Equally given the
large tolerances allowed on block times, as well as the poisson
distribution of blocks generated, granularity below an hour or two
doesn't have much practical significance.


2) Competing transactions

Here we are relying on miners prefering lower sequence numbers. For
instance a bidirectional payment channel can decrement nSequence for
each change of direction; BIP68 suggests such a decrement might happen
in increments of one day.

BIP113 makes lock-time calculations use the median time-past as the
threshold for by-time locks. The median time past is calculated by
taking median time of the 11 previous blocks, which means when a miner
creates a block they have absolutely no control over what the median
time-past is; it's purely a function of the block tip they're building
upon.

This means that granularity below a block interval will, on average,
have absolutely no effect at all on what transaction the miner includes
even in the hypothetical case. In practice of course, users will want to
use significantly larger than 1 block interval granularity in protocols.


The downside of BIP68 as written is users of by-height locktimes have 14
bits unused in nSequence, but by-time locktimes have just 5 bits unused.
This presents an awkward situation if we add new meanings to nSequence
if we ever need more than 5 bits. Yet as shown above, the extra
granularity doesn't have a practical benefit.


Recommendation: Change BIP68 to make by-time locks have the same number
of bits as by-height locks, and multiply the by-time lock field by the
block interval.

-- 
'peter'[:-1]@petertodd.org
000000000000000001a06d85a46abce495fd793f89fe342e6da18b235ade373f
-------------------------------------
FWIW, I had worked on something similar a while back: https://github.com/CodeShark/bitcoin/tree/coinparams_new/altconf <https://github.com/CodeShark/bitcoin/blob/coinparams_new/altconf/bitcoin.conf>

I like the idea in principle…but we should require a new genesis block, different magic bytes, and a different network port at the very least. :)


-------------------------------------
On Tue, Apr 28, 2015 at 11:01 AM, Pieter Wuille <pieter.wuille@gmail.com> wrote:

Agree here - there is no need to time consensus changes with a major
release, as they need to be ported back to older releases anyhow.
(I don't really classify them as software features, but properties of
the underlying system that we need to adopt to)

Wladimir


-------------------------------------
Hello Mike,


Yes I am aware of that; sorry for not mentioning it. I think it is an
interesting proposal, but I would not rely on it today, because it
includes a large share of unproven social experiment.

(Bitcoin too is a social experiment, but so far it has been working)



I agree with that, but I don't think it can be used as a way to justify
how decisions are made today.

The opposition to block size increase comes from two things:
(1) The perceived risk of increased centralization.
(2) Long-term considerations on the need for fee pressure.

I believe you and Gavin have properly addressed (1). Concerning (2), I
think the belief that miners can eventually be funded by a fee market is
wishful thinking. Thus, I am not against the proposed block size increase.

However, the issue of long-term mining incentives remains. So far, the
only proven method to incentivize mining has been direct block reward.

The easiest solution to ensure long-term viability of Bitcoin would be
to put an end to the original block halving schedule, and to keep the
block reward constant (this is what Monero does, btw). That solution is
inflationary, but in practice, users happen to lose private keys all the
time. The rate of coins loss would eventually converge to whatever rate
of emission is chosen, because the care people take of their coins
depends on their value.

Another solution, that does not break Bitcoin's social contract, would
be to keep the original block halving schedule, but to allow miners to
also redeem lost coins (defined as: coins that have not moved for a
fixed number of years. Some time averaging of the lost coins may be
needed in order to prevent non-productive miner strategies). That
solution would create less uncertainty on the actual money supply, and
better acceptability.

I do not expect such a solution to be adopted before miner incentives
become a problem. Neither am I attempting to predict the future; a
completely different solution might be found before the problem arises,
or Bitcoin might stop to exist for some other reason.

However, if I had to decide today, I would choose such a solution,
instead of relying on completely unproven mechanisms.

More important, since we need to decide about block size today, I want
to make it clear that my support is motivated by that long-term
possibility. I believe that the "we will need fee pressure" argument can
reasonably be dismissed, not because we don't know how Bitcoin will work
in 20 years, but because we know how it works today, and it is not
thanks to fee pressure.

Thomas


-------------------------------------
On 02/05/2015 02:08 PM, Paul Puey wrote:

I agree that with manual verification between the parties the worst
problem becomes DOS, which is certainly not catastrophic.

But the objective is to the extent possible improve upon the cumbersome
process of QR code, NFC signal, or textual address scanning. Given that
there would be no way to know you are under attack, with the exception
of manual confirmation, it would seem unwise to ever rely on the
automation. If the automation cannot be relied upon, it may actually
make matters worse. People would either take their chances by relying on
it or go through a more complex process.

In terms of the difficulty of an attack, it's important to recognize
that all attacks (DOS, privacy, integrity) in this scenario can be
fully-automated and executed over the air by a black box at some distance:

* DOS is possible by rebroadcasting a similar request.

* Privacy is compromised by monitoring for payment requests and
correlating them to location and potentially images of parties.

* Integrity is compromised by either:
(1) Rebroadcasting a similar transaction with a bogus address but with
the same leading characters; can't be spent but you lose your money.
(2) Rebroadcasting with a valid address that doesn't match the leading
characters, in the expectation that the user doesn't check manually.

Regarding possible mitigation via BIP-70:

A BIP-70 signed payment request in the initial broadcast can resolve the
integrity issues, but because of the public nature of the broadcast
coupled with strong public identity, the privacy compromise is much
worse. Now transactions are cryptographically tainted.

This is also the problem with BIP-70 over the web. TLS and other
security precautions aside, an interloper on the communication, desktop,
datacenter, etc., can capture payment requests and strongly correlate
transactions to identities in an automated manner. The payment request
must be kept private between the parties, and that's hard to do.

So the initial broadcast needs privacy, but then of course it cannot be
a broadcast - it need to be a narrow cast. That brings us back to
proximity-based establishment.

I think that you could get away with this for a while, simply because of
the narrow fields we are working in presently. But in a bitcoin world it
would be very problematic. For this reason I wouldn't want to encourage
standardization on this approach.

e

On 02/05/2015 02:10 PM, Eric Voskuil wrote:

-------------------------------------
None of those listed were in the context of performance.  Parsing of binary
or text is quite fast these days, and is not really a consideration versus
other needs such as a predictable encoding for a single data
representation.  XML and JSON both can represent the same post-evaluation
user data a million different ways, which is awful for anything you are
signing and hashing.  Text formats also transit binary data very poorly,
leading to unnecessary wrapping and unwrappiing (a programmatic, visibility
& bug; again performance not a primary concern).

This is evident because both XML and JSON have standards efforts under way
to correct some of these problems and make them more deterministic.
However, such standards are not field deployed and widely supported by
parsers and generators alike.




On Mon, Jan 19, 2015 at 2:16 PM, Richard Brady <rnbrady@gmail.com> wrote:



-- 
Jeff Garzik
Bitcoin core developer and open source evangelist
BitPay, Inc.      https://bitpay.com/
-------------------------------------
None that I can see.
In fact I was just about to ask for some details about this part of the
process, so this come just at the right time.

On Fri, Sep 4, 2015 at 1:18 AM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
Try the Online TrID File Identifier
http://mark0.net/onlinetrid.aspx
-------------------------------------
I'm not sure why Bitcoin Core and the rules and policies that it
enforces are being conflated in this thread. There's nothing stopping
us from adding the ability for the user to decide what their consensus
parameters should be at runtime. In fact, that's already in use:
./bitcoind -testnet. As mentioned in another thread, the chain params
could even come from a config file that the user could edit without
touching the code.

I realize that it'd be opening Pandora's Box, and likely met with very
loud and reasonable arguments about the obvious terrible implications,
but it's at least an alternative to the current status quo of Core's
conflation with the consensus rules. The idea really is no different
than suggesting that someone fork the codebase and implement their own
changes, it just cuts out most of the work required.

With that in place, consensus changes would be more about lobbying and
coalitions, and less about pull requests.

Cory

On Wed, Jul 22, 2015 at 6:40 PM, Raystonn via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Inspired by Pieter's Tree Signatures, I believe Merkleized Abstract 
Syntax Trees (MAST) could be implemented with only OP_CAT and OP_EVAL 
(BIP12).

The idea is very simple. Using a similar example in Pieter's paper,

scriptSig = <sig> <serialized sub-script 10> Z1 0 1 1 X6 1 K9 0 
<serialized script>
scriptPubKey = DUP HASH160 <hash serialized script> EQUALVERIFY EVAL
serialized script = 8 PICK SHA256 (SWAP IF SWAP ENDIF CAT SHA256)*4 <R> 
EQUALVERIFY EVAL

This will run the 10-th sub-script, when there are 11 sub-scripts in the 
MAST

I think this is the easiest way to enable MAST since the reference 
implementation for BIP12 is already there. We could enable OP_CAT only 
inside OP_EVAL so this will be a pure softfork.

Ref:
Tree Signatures: https://blockstream.com/2015/08/24/treesignatures/
BIP12: https://github.com/bitcoin/bips/blob/master/bip-0012.mediawiki

-------------------------------------

I have explained why I believe there is some urgency, whereby "some
urgency" I mean, assuming it takes months to implement, merge, test,
release and for people to upgrade.

But if it makes you happy, imagine that this discussion happens all over
again next year and I ask the same question.
-------------------------------------

On Oct 28, 2015, at 12:13 AM, Luke Dashjr <luke@dashjr.org> wrote:


I think the only custom policy that this change would make harder to implement is the current default policy of 5% reserved space. Right now, in e.g. CreateNewBlock, you have two loops, each of which follows a completely different policy, plus additional code for corner cases like ensuring that a tx isn't added twice. If I were a miner and a mediocre programmer (which I actually am, on both accounts), and I wanted to change the mining policy, I would probably take a look at that code, groan, give up, and go sharpen my pickaxe instead.

This change could be written in an abstract way. We could define an API that is calibrated on the whole mempool, then has a method that takes transactions and returns priority scores.

If someone wanted to write a reserved-space algorithm in this priority API scheme, then they could just set it up so that most transactions would get a priority score between e.g. zero and 8999, and any transactions that were supposed to be prioritized would get a priority level over 9000. Easy enough?


-------------------------------------
To clarify, although I have defended the deployment of segwit as a
hardfork, I have no strong opinion on whether to do that or do it as a
softfork first and then do a hardfork to move things out of the
coinbase to a better place.
I have a strong opinion against never doing the later hardfork though.
I would have supported segwit for Bitcoin even if it was only possible
as a hardfork, but there's a softfork version and that will hopefully
accelerate its deployment.
Since the plan seems to be to do a softfork first and a hardfork
moving the witness tree (and probably more things) outside of the
coinbase later, I support the plan for segwit deployment.
In fact, the plan is very exciting to me.

-------------------------------------
On Friday, September 04, 2015 8:13:18 PM Andy Chase via bitcoin-dev wrote:

For hardforks (removing consensus rules), economic consensus: people who 
accept payment in bitcoins weighted by their actual volume of such payments. 
A supermajority subset may arguably be sufficient for some hardforks (which 
don't violate Bitcoin's social contract) since they can effectively compel 
the remaining economy to comply.

For softforks (adding consensus rules), a majority of miners: they can "51% 
attack" miners who don't go along with it.

Anything else does not necessarily need universal agreement, so are 
completely up to the whim of individual software projects. If someone doesn't 
like a decision in Core (for example), they can safely fork the code. If any 
significant amount of people use their fork, then the BIP is accepted whether 
or not Core later adopts it.

Note this "system" is really describing a lack of a system - that is, what 
naturally must happen for changes to occur. Softforks have a relatively 
mature technical method for measuring support and deploying (which I believe 
someone else is already working on a BIP describing), but the same thing is 
impractical for hardforks. Some formal way to measure actual economic 
acceptance seems like a good idea to study, but it needs to be reasonably 
accurate so as to not change the outcome from its natural/necessary result.

Luke

-------------------------------------


In addition, fees are complicated by the fact that they are used as an anti-spam measure for relay nodes…who don’t see ANY direct compensation whatsoever for providing that service. So we really have two different fees being tacked on…but the miners get to keep all of it…and the relay fee is being hard coded into the software.

Fee calculation heuristics for wallets are already far from trivial - this is another issue that needs to be addressed.

- Eric Lombrozo


-------------------------------------


The actual assignment of version bits isn't clear from the
specification. Are you saying that any implementation that wants to
propose a change is encouraged to pick a free version bit and use it?

Furthermore, my proposal addresses the danger of forward-incompatible
changes; a hard-fork can no longer occur as every implementation will
agree on the active the set of rules even if it has not implemented
them. This seems to be lacking in the version bits proposal.

Tomas

-------------------------------------
Am 17.08.2015 um 18:32 schrieb Jorge Timón:

It just creates confusion. Particularly in the media. I also find it
unfair if people abuses Satoshi's name to submit their personal views on
an issue.

- oliver


-------------------------------------


This doesn't work because a large-scale miner can trivially make themselves look like a very large number of much smaller scale miners. Their ability to minimize variance comes from the cumulative totals they control so 10 pools of 1% of the network cumulatively have the same variance as 1 pool with 10% of the network. It's also very easy for miners to relay blocks via different addresses and the cost is minimal. The biggest cost would be in DDoS prevention and a miner that actually split their pool into lots of small fragments would actually give themselves the ability to do quite a lot of DDoS mitigation anyway. If no-one is doing this right now it's simply because they've not had the right incentives to make it worthwhile; if the incentives make it worthwhile then this is pretty trivial to do.

This is one area where anonymity on behalf of transaction validators and block makers essentially makes it pretty-much impossible to maintain any sort of sanctions against antisocial behaviour.

-------------------------------------
On 7 August 2015 at 22:35, Thomas Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

It's not as simple as trusting miners, Bitcoin security needs some
reasonable portion of economic interest to be validating their receipt
of coins against a full node they run.

I do it myself because I dont want to lose money, as do many power
users.  Most bitcoin ecosystem companies do it.  You dont have to run
it all the time, just sync it when you want to check your own coin
receipt with higher assurance.


Even if you are willing to trust others, trusting miners or random
full nodes would be unsafe if not for the reasonable portion of
economic interest validating their own received coins.  That holds
miners honest, otherwise they could more easily present fake
information to SPV users.


Bitcoin's very reason for existence is to avoid that need.  For people
fully happy to trust others with their money, Bitcoin may not be as
interesting to them.


What Pieter said is an accurate summary and non-controversial.

Adam

-------------------------------------
On Tue, Jun 23, 2015 at 4:50 PM, Peter Todd <pete@petertodd.org> wrote:

Recent averages seem to be offering around $0.04...
https://tradeblock.com/blog/bitcoin-network-capacity-analysis-part-2-macro-transaction-trends
With various stress tests indicate needing 10x more...
http://www.reddit.com/r/Bitcoin/search?restrict_sr=on&q=stress+test+fees


Penny stocks? Voting? Notarizing?
There's a whole ecosystem of non-purchase cases
for which non-profit-mining fees are enabling.

-------------------------------------
It is not a bug that you are unable to selectively choose these features
with higher version numbers. The version selection is in there at all
because there is a possibility that there exists already signed
transactions which would violate these new rules. We wouldn't want these
transactions to become unspendable. However moving forward there is no
reason to selectively pick and choose which of these new consensus rules
you want to apply your transaction.
On Aug 8, 2015 11:51 AM, "jl2012 via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Sun, Dec 6, 2015 at 10:50 PM, Bryan Bishop via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



​This was clearly a lot of work...thanks again.​

-- 
Johnathan Corgan
Corgan Labs - SDR Training and Development Services
http://corganlabs.com
-------------------------------------
On Wed, Mar 11, 2015 at 10:12 PM, Thy Shizzle <thashiznets@yahoo.com.au>
wrote:

meh... the fact that you can't derive the seed phrase from the wallet seed,
and that the password key stretching is so weak as to be ineffectual
security theater bugs me. Feels like a pretty big compromise to work on
current generation low power embedded devices when the next generation will
be more than capable. But I understand the motivation for the compromise.

Aaron Voisine
co-founder and CEO
breadwallet.com
-------------------------------------
Hi Paul,

The issue is in the establishment of trust. Anyone can broadcast the initial information.

e

-------------------------------------
On Saturday, November 14, 2015 10:52:12 AM Jorge Timn via bitcoin-dev wrote:

Actually, the economy does not necessarily include miners, and in fact the 
present miner community for the most part does not overlap significantly with 
economic activity. And at the same time, miners also have a tendency to 
upgrade at a different rate than the economy. It might make sense to 
incorporate a miner-trigger, but *only if* the flag is enabled in nodes by an 
option disabled by default, and the BIP clearly specifies that miners must not 
enable it until they perceive complete economic adoption of the change.

Luke

-------------------------------------
On Thursday, October 22, 2015 11:59:11 AM Rusty Russell via bitcoin-dev wrote:

They are not readable, they all say:
-------------------------------------
On Tue, Jun 23, 2015 at 05:24:23PM -0400, Gavin Andresen wrote:

You're the one proposing a change here; we're evaluating the safety of
that change.

An analogous situation is imagine we have two islands, with a suspension
bridge between them. The bridge has just two lanes in either direction,
so obviously there's a limited amount of traffic that can flow across
it. It used to be used to little that people would joyride back and
forth as the toll booths at either end just charged a hundreth of a
penny per trip, or even not at all, but as demand has been increasing
tolls are going up as well.

You've come along with a bold new plan to add fifteen more lanes to that
bridge by expanding the bridge deck, then hire contractors in advance to
double the number of lanes every two years after that with no clear way
of terminating their contract if anything goes wrong. (in just over a
decade our two lane bridge will be a mile wide!)

Of course, obviously if we add enough lanes the cables holding it up
will snap, so we've better carefully analyse the carrying capacity of
the brdige and the threats it faces. For instance, earthquakes happen
every so often - the last one even snapped a few strands in the main
cables, which people claim were fixed... but we don't really know for
sure as a thick layer of paint was quickly slapped over the fix and
no-one's been able to inspect it.

It's perfectly reasonable to ask what kind of earthquakes you expect the
bridge to withstand, as well as peer-reviewed and peer-reproducable
figures about the strength of the cables and the weight of the traffic.
Similarly, we've got the funds to make a test bridge of the same
dimensions and see if it collapses. Any bridge widening proposal that
doesn't have this data is simply incomplete, end of story.

As for the other side, the worst that happens if nothing changes is
usage of this bridge gets proportioned to the most valuable users by the
supply and demand toll system. Some people might decide to take the bus
across rather than an inefficient individual car, some of the
advertising companies running trucks back and forth with billboards on
the side are going to stop doing that. But traffic is still going to get
across. It's not a politically easy position to be in - there's enormous
pressure to quickly "do something" however dangerous - but the actual
effects of doing nothing are ultimately not a big deal.

In civil engineering we have enough experience with disasters to know
that you can't give into political pressure to do potentially dangerous
things until the consequences are well understood; hopefully we'll learn
that in the consensus cryptography space before a big disaster rather
than after.

-- 
'peter'[:-1]@petertodd.org
0000000000000000007fc13ce02072d9cb2a6d51fae41fefcde7b3b283803d24
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Agreed.

On 08/17/2015 07:36 AM, Adam Back via bitcoin-dev wrote:

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJV0/yyAAoJEGxwq/inSG8CaicIALQU//jNkUVRb6uzYFtSNb/y
cWViS5oaberLC2S0pQu2C4CciHSht347luM8kxlp3xL045SgIvvwXBzooH5HE+JE
+NscfC58+2RyQWgPiWrwQ2JSFQgaRzi58fyE8rLSdsLKXjJkbkaol6w2atbpvHP8
Zbm6sAOybLurA+2N9ZtxWosEZEfjT54Sf14+YNQlr5ve3JbYBIbZ8PhWPXwc5P/5
FuwBb/JBszClasWGxmsexrpXfK6Kqy2rOVOWmmYNMjpHR8oYZWKC42HTOXw2lig1
lspqMEXBTv+ppSk1KU5ovwftongX3W0lM5DOj3FXE7frlgcBxeMMFBFBFGnT3ZU=
=vCZH
-----END PGP SIGNATURE-----

-------------------------------------
"However, that is outside the scope of the result that an individual
miner's profit per block is always maximized at a finite block size Q* if
Shannon Entropy about each transaction is communicated during the block
solution announcement.  This result is important because it explains how a
minimum fee density exists and it shows how miners cannot create enormous
spam blocks for "no cost," for example. "

Dear Peter,

This might very well not be the case. Since the expected revenue *<V>* in
our formulas is but a lower bound to the true expected revenue, and the fee
supply curve [image: M_s(Q)\propto 1/\langle V\rangle], if the true
expected revenue doesn't decay faster than the mempool's average
transaction fee (or, more simply, if it doesn't decay to zero) then the
maximum miner surplus will be unbounded and unhealthy fee markets will
emerge.

Best,
Daniele



Daniele Pinna, Ph.D

On Sun, Aug 30, 2015 at 10:08 PM, Peter R <peter_r@gmx.com> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 02/04/2015 02:23 PM, Isidor Zeuner wrote:

Another possible approach is to randomize the number of change outputs
from transaction to transaction.

Doing this, it would be possible to make change outputs that mimic
real spends (low number of s.d.)

- -- 
Support online privacy by using email encryption whenever possible.
Learn how here: http://www.youtube.com/watch?v=bakOKJFtB-k
-----BEGIN PGP SIGNATURE-----

iQIcBAEBCAAGBQJU1DH9AAoJECpf2nDq2eYjt2gP/3gpojJey2URkWWk0sg9dpHU
OsD37TCbrwUaS/K8UMKsuc45FSJU/EeYpaVz9r1Ifm/IeaFYPIX0tEm17n3hkcAG
QPmt/xAZn9GVyPWYKjmVDmx574pqiJLeZh8bP788sZsGc4Gk7NNJniVGLtsmvFCb
ZOtwS8v7UuJZx6awydrpNhw/+SsQn9Xdb8fcLqmFKWDpG2Mlrv+ds34NMlGbfO2r
PqCMw1Y12J0HXLisOCGQNZNdG9mVjKw3MP0GGjUlOM+ibrrorqoO5Ifo2RGuElgw
LZkzzDzg6kO8iuNOV7Jg1lz5WftRjgLRSCcMq4V+793zGJW9BeISeDcKQ2ZlWMXB
Hu83m4vCYOJeECdKGWlhyTmKNNHshsiPz3SBDLxP8uR80UkS3waDIXwLxGX9Pa63
uleaZ2qHQ/0UdC9opN3Snn33M701dHNJH9iXfhf/MVnUZ0FjzsLXaJ0F0208ZxCX
qGCAv5y1ijrDlCLTvakZJRIruXgxNPqtErzP9GtgXeGeDc8tRv00WiM9Olpu0EXd
yjhAZGydcE3Ec2cNo+teWjeDt4Ga4OYDb7i08eegaDuj5MCDcDtlgfwNjdKbre1x
S7pKKDn8V03/WST1x9fWjM04NxeSjJ0yRjOAxkLV/mlDX6lQEYJL/W+MJLvpOnTC
LtZrkSmSTJ7ZR0tMgpAe
=8EVe
-----END PGP SIGNATURE-----
-------------------------------------
Rather than using an inhumanly long hex string from the genesis hash to
distinguish between mainnet and testnet, why not use the network magic
bytes instead? Much shorter, just as distinct.

I'd still prefer a common network name mapping for the sake of humanity.
Few bitcoin library implementations use the same string names for mainnet
and testnet. This BIP could simply define one string name alias for each
supported network and leave mapping to local lingo to the implementors.

-Danny

On Sat, Aug 29, 2015 at 1:10 PM, Jorge Timón <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I'm in favor of relative CHECKLOCKTIMEVERIFY, but I don't have a very
specific reason. I just have a vague worry that there can be "race
conditions" in which a txn with an absolute CHECKLOCKTIMEVERIFY goes
into the blockchain later than one of its signers expected that it
would, and therefore there is a surprisingly short delay between that
transaction going into the blockchain and becoming spendable.

This worry of mine is assuaged by using relative CHECKLOCKTIMEVERIFY instead.

Regards,

Zooko


-------------------------------------
There seemed to be some agreement on IRC - after a bit of haranguing by
myself :) -- that large refactors should (a) occur over a small window of
time and (b) have a written plan beforehand.



On Tue, Sep 22, 2015 at 7:49 PM, Dave Scotese <dscotese@litmocracy.com>
wrote:

-------------------------------------
<p dir="ltr">You really believe they would risk getting orphaned by skipping the longer chain, just to attempt to reduce average block size? No, that doesn't happen today. Laziness in leaving the default size is common. But that is not collusion, nor an attempt to manipulate the block sizes of other miners.<br>
</p>
<div class="gmail_quote">On 24 Jun 2015 10:05 am, Mark Friedenbach &lt;mark@friedenbach.org&gt; wrote:<br type='attribution'><blockquote class="quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><p dir="ltr">They do so by not building on larger blocks</p>
<div class="elided-text">On Jun 23, 2015 9:31 PM, &#34;Raystonn&#34; &lt;<a href="mailto:raystonn&#64;hotmail.com">raystonn&#64;hotmail.com</a>&gt; wrote:<br /><blockquote style="margin:0 0 0 0.8ex;border-left:1px #ccc solid;padding-left:1ex"><p dir="ltr">No, they can lower their own block sizes.  But they cannot currently lower the sizes of blocks mined by others.  That is not the same thing by any stretch of the imagination.</p>
<div class="elided-text">On 23 Jun 2015 8:50 pm, Jeff Garzik &lt;<a href="mailto:jgarzik&#64;gmail.com">jgarzik&#64;gmail.com</a>&gt; wrote:<br /><blockquote style="margin:0 0 0 0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Miners can collude today to lower the block size limit.<div><br /></div><div>In fact, this largely happens already out of laziness - miners often follow the &#34;soft&#34; default limit set by Bitcoin Core, to the point where you can chart when miners upgrade to new software: <a href="http://hashingit.com/analysis/39-the-myth-of-the-megabyte-bitcoin-block">http://hashingit.com/analysis/39-the-myth-of-the-megabyte-bitcoin-block</a></div><div><br /></div><div><br /></div></div><div><br /><div>On Tue, Jun 23, 2015 at 8:05 PM, William Madden <span dir="ltr">&lt;<a href="mailto:will.madden&#64;novauri.com">will.madden&#64;novauri.com</a>&gt;</span> wrote:<br /><blockquote style="margin:0 0 0 0.8ex;border-left:1px #ccc solid;padding-left:1ex">Here are refutations of the approach in BIP-100 here:<br />
<a href="http://gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf">http://gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf</a><br />
<br />
To recap BIP-100:<br />
<br />
1) Hard form to remove static 1MB block size limit<br />
2) Add new floating block size limit set to 1MB<br />
3) Historical 32MB message limit remains<br />
4) Hard form on testnet 9/1/2015<br />
5) Hard form on main 1/11/2016<br />
6) 1MB limit changed via one-way lock in upgrade with a 12,000 block<br />
threshold by 90% of blocks<br />
7) Limit increase or decrease may not exceed 2x in any one step<br />
8) Miners vote by encoding &#39;BV&#39;&#43;BlockSizeRequestValue into coinbase<br />
scriptSig, e.g. &#34;/BV8000000/&#34; to vote for 8M.<br />
9) Votes are evaluated by dropping bottom 20% and top 20%, and then the<br />
most common floor (minimum) is chosen.<br />
<br />
8MB limits doubling just under every 2 years makes a static value grow<br />
in a predictable manner.<br />
<br />
BIP-100 makes a static value grow (or more importantly potentially<br />
shrink) in an unpredictable manner based on voting mechanics that are<br />
untested in this capacity in the bitcoin network.  Introducing a highly<br />
variable and untested dynamic into an already complex system is<br />
unnecessarily risky.<br />
<br />
For example, the largely arbitrary voting rules listed in 9 above can be<br />
gamed.  If I control pools or have affiliates involved in pools that<br />
mine slightly more than 20% of blocks, I could wait until block sizes<br />
are 10MB, and then suddenly vote &#34;/BV5000000/&#34; for 20% of blocks and<br />
&#34;/BV5000001/&#34; for the remaining 10%.  If others don&#39;t consistently vote<br />
for the same &#34;/BV#/&#34; value, vote too consistently and have their value<br />
thrown out as the top 20%, I could win the resize to half capacity<br />
&#34;/BV5000001/&#34; because it was the lowest repeated value not in the bottom<br />
20%.<br />
<br />
I could use this to force an exodus to my sidechain/alt coin, or to<br />
choke out the bitcoin network.  A first improvement would be to only let<br />
BIP-100 raise the cap and not lower it, but if I can think of a<br />
vulnerability off the top of my head, there will be others on the other<br />
side of the equation that have not been thought of.  Why bother<br />
introducing a rube goldberg machine like voting when a simple 8mb cap<br />
with predictable growth gets the job done, potentially permanently?<br />
<div><div><br />
<br />
On 6/23/2015 9:43 PM, odinn wrote:<br />
&gt; -----BEGIN PGP SIGNED MESSAGE-----<br />
&gt; Hash: SHA1<br />
&gt;<br />
&gt; 1) Hard fork not (necessarily) needed<br />
&gt; 2) See Garzik&#39;s BIP 100, better (this is not meant to say &#34;superior to<br />
&gt; your stuff,&#34; but rather simply to say, &#34;Better you should work with<br />
&gt; Garzik to implement BIP-100, that would be good&#34;)<br />
&gt; 3) See points 1 and 2 above<br />
&gt; 4) If still reading... changes should be (as you seem to have been<br />
&gt; trying to lean towards)... lean towards gradual change; hence, changes<br />
&gt; that would flow from this BIP would be better off oriented in a<br />
&gt; process that dies not require the &#34;way you have done it.&#34;<br />
&gt;<br />
&gt; You did address that, to be fair - in your TODO, this link:<br />
&gt; <a href="http://gavinandresen.ninja/time-to-roll-out-bigger-blocks">http://gavinandresen.ninja/time-to-roll-out-bigger-blocks</a><br />
&gt;<br />
&gt; contained the following link:<br />
&gt;<br />
&gt; <a href="http://gavinandresen.ninja/bigger-blocks-another-way">http://gavinandresen.ninja/bigger-blocks-another-way</a><br />
&gt;<br />
&gt; However, in reading that, I didn&#39;t see any meaningful statements that<br />
&gt; would refute the approach in Garzik&#39;s BIP-100.<br />
&gt;<br />
&gt; Maybe a better way to say this is,<br />
&gt;<br />
&gt; Work with Jeff Garzik (which I am sure you are already having such<br />
&gt; discussions in private) as well as the list discussions,<br />
&gt; Move forward on BIP-100 with Garzik and other developers (not such a<br />
&gt; bad plan really) and don&#39;t get caught up in XT.  (If you feel you can<br />
&gt; develop XT further, that is your thing but it would perhaps make you<br />
&gt; lose focus, work together with other developers.)<br />
&gt;<br />
&gt; Relax into the process.  Things will be ok.<br />
&gt;<br />
&gt; Respectfully,<br />
&gt;<br />
&gt; - -O<br />
&gt;<br />
&gt; On 06/22/2015 11:18 AM, Gavin Andresen wrote:<br />
&gt;&gt; I promised to write a BIP after I&#39;d implemented<br />
&gt;&gt; increase-the-maximum-block-size code, so here it is. It also lives<br />
&gt;&gt; at:<br />
&gt;&gt; <a href="https://github.com/gavinandresen/bips/blob/blocksize/bip-8MB.mediawiki">https://github.com/gavinandresen/bips/blob/blocksize/bip-8MB.mediawiki</a><br />
&gt;&gt;<br />
&gt;&gt;  I don&#39;t expect any proposal to please everybody; there are<br />
&gt;&gt; unavoidable tradeoffs to increasing the maximum block size. I<br />
&gt;&gt; prioritize implementation simplicity -- it is hard to write<br />
&gt;&gt; consensus-critical code, so simpler is better.<br />
&gt;&gt;<br />
&gt;&gt;<br />
&gt;&gt;<br />
&gt;&gt;<br />
&gt;&gt; BIP: ?? Title: Increase Maximum Block Size Author: Gavin Andresen<br />
&gt;&gt; &lt;<a href="mailto:gavinandresen&#64;gmail.com">gavinandresen&#64;gmail.com</a> &lt;mailto:<a href="mailto:gavinandresen&#64;gmail.com">gavinandresen&#64;gmail.com</a>&gt;&gt; Status:<br />
&gt;&gt; Draft Type: Standards Track Created: 2015-06-22<br />
&gt;&gt;<br />
&gt;&gt; &#61;&#61;Abstract&#61;&#61;<br />
&gt;&gt;<br />
&gt;&gt; This BIP proposes replacing the fixed one megabyte maximum block<br />
&gt;&gt; size with a maximum size that grows over time at a predictable<br />
&gt;&gt; rate.<br />
&gt;&gt;<br />
&gt;&gt; &#61;&#61;Motivation&#61;&#61;<br />
&gt;&gt;<br />
&gt;&gt; Transaction volume on the Bitcoin network has been growing, and<br />
&gt;&gt; will soon reach the one-megabyte-every-ten-minutes limit imposed by<br />
&gt;&gt; the one megabyte maximum block size. Increasing the maximum size<br />
&gt;&gt; reduces the impact of that limit on Bitcoin adoption and growth.<br />
&gt;&gt;<br />
&gt;&gt; &#61;&#61;Specification&#61;&#61;<br />
&gt;&gt;<br />
&gt;&gt; After deployment on the network (see the Deployment section for<br />
&gt;&gt; details), the maximum allowed size of a block on the main network<br />
&gt;&gt; shall be calculated based on the timestamp in the block header.<br />
&gt;&gt;<br />
&gt;&gt; The maximum size shall be 8,000,000 bytes at a timestamp of<br />
&gt;&gt; 2016-01-11 00:00:00 UTC (timestamp 1452470400), and shall double<br />
&gt;&gt; every 63,072,000 seconds (two years, ignoring leap years), until<br />
&gt;&gt; 2036-01-06 00:00:00 UTC (timestamp 2083190400). The maximum size of<br />
&gt;&gt; blocks in between doublings will increase linearly based on the<br />
&gt;&gt; block&#39;s timestamp. The maximum size of blocks after 2036-01-06<br />
&gt;&gt; 00:00:00 UTC shall be 8,192,000,000 bytes.<br />
&gt;&gt;<br />
&gt;&gt; Expressed in pseudo-code, using integer math:<br />
&gt;&gt;<br />
&gt;&gt; function max_block_size(block_timestamp):<br />
&gt;&gt;<br />
&gt;&gt; time_start &#61; 1452470400 time_double &#61; 60*60*24*365*2 size_start &#61;<br />
&gt;&gt; 8000000 if block_timestamp &gt;&#61; time_start&#43;time_double*10 return<br />
&gt;&gt; size_start * 2^10<br />
&gt;&gt;<br />
&gt;&gt; // Piecewise-linear-between-doublings growth: time_delta &#61;<br />
&gt;&gt; block_timestamp - t_start doublings &#61; time_delta / time_double<br />
&gt;&gt; remainder &#61; time_delta % time_double interpolate &#61; (size_start *<br />
&gt;&gt; 2^doublings * remainder) / time_double max_size &#61; size_start *<br />
&gt;&gt; 2^doublings &#43; interpolate<br />
&gt;&gt;<br />
&gt;&gt; return max_size<br />
&gt;&gt;<br />
&gt;&gt; &#61;&#61;Deployment&#61;&#61;<br />
&gt;&gt;<br />
&gt;&gt; Deployment shall be controlled by hash-power supermajority vote<br />
&gt;&gt; (similar to the technique used in BIP34), but the earliest possible<br />
&gt;&gt; activation time is 2016-01-11 00:00:00 UTC.<br />
&gt;&gt;<br />
&gt;&gt; Activation is achieved when 750 of 1,000 consecutive blocks in the<br />
&gt;&gt; best chain have a version number with bits 3 and 14 set (0x20000004<br />
&gt;&gt; in hex). The activation time will be the timestamp of the 750&#39;th<br />
&gt;&gt; block plus a two week (1,209,600 second) grace period to give any<br />
&gt;&gt; remaining miners or services time to upgrade to support larger<br />
&gt;&gt; blocks. If a supermajority is achieved more than two weeks before<br />
&gt;&gt; 2016-01-11 00:00:00 UTC, the activation time will be 2016-01-11<br />
&gt;&gt; 00:00:00 UTC.<br />
&gt;&gt;<br />
&gt;&gt; Block version numbers are used only for activation; once activation<br />
&gt;&gt; is achieved, the maximum block size shall be as described in the<br />
&gt;&gt; specification section, regardless of the version number of the<br />
&gt;&gt; block.<br />
&gt;&gt;<br />
&gt;&gt;<br />
&gt;&gt; &#61;&#61;Rationale&#61;&#61;<br />
&gt;&gt;<br />
&gt;&gt; The initial size of 8,000,000 bytes was chosen after testing the<br />
&gt;&gt; current reference implementation code with larger block sizes and<br />
&gt;&gt; receiving feedback from miners stuck behind bandwidth-constrained<br />
&gt;&gt; networks (in particular, Chinese miners behind the Great Firewall<br />
&gt;&gt; of China).<br />
&gt;&gt;<br />
&gt;&gt; The doubling interval was chosen based on long-term growth trends<br />
&gt;&gt; for CPU power, storage, and Internet bandwidth. The 20-year limit<br />
&gt;&gt; was chosen because exponential growth cannot continue forever.<br />
&gt;&gt;<br />
&gt;&gt; Calculations are based on timestamps and not blockchain height<br />
&gt;&gt; because a timestamp is part of every block&#39;s header. This allows<br />
&gt;&gt; implementations to know a block&#39;s maximum size after they have<br />
&gt;&gt; downloaded it&#39;s header, but before downloading any transactions.<br />
&gt;&gt;<br />
&gt;&gt; The deployment plan is taken from Jeff Garzik&#39;s proposed BIP100<br />
&gt;&gt; block size increase, and is designed to give miners, merchants,<br />
&gt;&gt; and full-node-running-end-users sufficient time to upgrade to<br />
&gt;&gt; software that supports bigger blocks. A 75% supermajority was<br />
&gt;&gt; chosen so that one large mining pool does not have effective veto<br />
&gt;&gt; power over a blocksize increase. The version number scheme is<br />
&gt;&gt; designed to be compatible with Pieter&#39;s Wuille&#39;s proposed &#34;Version<br />
&gt;&gt; bits&#34; BIP.<br />
&gt;&gt;<br />
&gt;&gt; TODO: summarize objections/arguments from<br />
&gt;&gt; <a href="http://gavinandresen.ninja/time-to-roll-out-bigger-blocks">http://gavinandresen.ninja/time-to-roll-out-bigger-blocks</a>.<br />
&gt;&gt;<br />
&gt;&gt; TODO: describe other proposals and their advantages/disadvantages<br />
&gt;&gt; over this proposal.<br />
&gt;&gt;<br />
&gt;&gt;<br />
&gt;&gt; &#61;&#61;Compatibility&#61;&#61;<br />
&gt;&gt;<br />
&gt;&gt; This is a hard-forking change to the Bitcoin protocol; anybody<br />
&gt;&gt; running code that fully validates blocks must upgrade before the<br />
&gt;&gt; activation time or they will risk rejecting a chain containing<br />
&gt;&gt; larger-than-one-megabyte blocks.<br />
&gt;&gt;<br />
&gt;&gt; Simplified Payment Verification software is not affected, unless<br />
&gt;&gt; it makes assumptions about the maximum depth of a transaction&#39;s<br />
&gt;&gt; merkle branch based on the minimum size of a transaction and the<br />
&gt;&gt; maximum block size.<br />
&gt;&gt;<br />
&gt;&gt; &#61;&#61;Implementation&#61;&#61;<br />
&gt;&gt;<br />
&gt;&gt; <a href="https://github.com/gavinandresen/bitcoinxt/tree/blocksize_fork">https://github.com/gavinandresen/bitcoinxt/tree/blocksize_fork</a><br />
&gt;&gt;<br />
&gt;&gt;<br />
&gt;&gt;<br />
&gt;&gt; _______________________________________________ bitcoin-dev mailing<br />
&gt;&gt; list <a href="mailto:bitcoin-dev&#64;lists.linuxfoundation.org">bitcoin-dev&#64;lists.linuxfoundation.org</a><br />
&gt;&gt; <a href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a><br />
&gt;&gt;<br />
&gt;<br />
&gt; - --<br />
&gt; <a href="http://abis.io">http://abis.io</a> ~<br />
&gt; &#34;a protocol concept to enable decentralization<br />
&gt; and expansion of a giving economy, and a new social good&#34;<br />
&gt; <a href="https://keybase.io/odinn">https://keybase.io/odinn</a><br />
&gt; -----BEGIN PGP SIGNATURE-----<br />
&gt; Version: GnuPG v1<br />
&gt;<br />
&gt; iQEcBAEBAgAGBQJVigtJAAoJEGxwq/inSG8CqZwIAIG3ZQzekfccPxBOMqtim175<br />
&gt; Crov6hrO9FaIzbLljECpUi60RKuDM/fs09ZJsKKIaJPkB5dlJjs4huc206veAIO&#43;<br />
&gt; K2h3DmAcA6W/Thk0C2cV3ewv&#43;OiELDOhpeoddBBLPadAfaBGr4l9ltqWLdBtMCmw<br />
&gt; OtmiWstEuXTao9ApgoFOmybdmCjbfrfhejOOHs/pMiSn5xVE60RK4x2HFTFsHfAN<br />
&gt; fZAeLCuwuN2qWMrVrr&#43;cbpCXjEuE1xZG3WEj7ppYoGR&#43;AgF/Y5/U1j7S4PVpk85s<br />
&gt; CgMkpcWvLnBMmSCrllnRZy1Gfrwk36Pg0rXD/l/NNd0/KTpmPSvkX/bCyzFwbzo&#61;<br />
&gt; &#61;ft62<br />
&gt; -----END PGP SIGNATURE-----<br />
&gt; _______________________________________________<br />
&gt; bitcoin-dev mailing list<br />
&gt; <a href="mailto:bitcoin-dev&#64;lists.linuxfoundation.org">bitcoin-dev&#64;lists.linuxfoundation.org</a><br />
&gt; <a href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a><br />
&gt;<br />
_______________________________________________<br />
bitcoin-dev mailing list<br />
<a href="mailto:bitcoin-dev&#64;lists.linuxfoundation.org">bitcoin-dev&#64;lists.linuxfoundation.org</a><br />
<a href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a><br />
</div></div></blockquote></div><br /></div>
</blockquote></div><br />_______________________________________________<br />
bitcoin-dev mailing list<br />
<a href="mailto:bitcoin-dev&#64;lists.linuxfoundation.org">bitcoin-dev&#64;lists.linuxfoundation.org</a><br />
<a href="https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev">https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev</a><br />
<br /></blockquote></div>
</blockquote></div>
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Um ~ "jurisdiction of wallet provider?"

If that's the (perhaps ot) bit you want to run on this thread then my
comments are:

Get out of web wallet businesses now.  It's not a jurisdictional
question anymore, although I think there used to be very valid long
running debates on where it would be best to do business.  Now it just
feels like you will be bouncing from one place to another -
determining where your exit is as soon as you establish a (physical)
presence, because jurisdictions sense a serious threat from the
advancement of financial cryptography as it will evolve in the next
several years. So you have to be mobile, or do something like what
they are establishing at blueseed (see http://blueseed.com which is
just off coast of San Francisco).  Please perk up and don't just swipe
to delete, read the whole e-mail.  There are some configurations (e.g.
the zero knowledge bit) you can do to mitigate the issues but if you
are asking users to log in and log out of a service that relies on a
web site then in the end you doom them (and any service you provide)
to mandatory storage of customer data and ultimately loss of customer
resources due to identification of the customer.

I think you need to stop quibbling about the details and just get over
it and understand that the problem of web wallet users and
corporations that serve web wallet customers being forced to give up
information constantly to governments means that web wallets are
certainly no longer a viable solution.  And post-cromnibus with the
extra financial surveillance provisions now passed on 3rd party
matters, it's even worse.  This is not subject to debate, it's just a
fact.  Period.  Web wallet corps exist now only on a model that exists
to burn the users.  Convenient?  Yes.  But is it good for the users in
the long haul?  Absolutely not.  Do alternative to the web wallets
exist? Absolutely.

Back off.. Go to p2p.  Stop advocating for webby solutions.  In fact,
I don't think that anyone working for coinbase or bitpay should be,
anymore.  I think that on principle you should withdraw and end your
employment from such services.

Core?  Good.  Electrum Wallet?  good.  Mycelium? Local Trader? Open
Bazaar?  Could be better, but great.  These are the kind of things we
need.  No signups, avoids centralizations, no grabbing your data, no
ID collection and requirements.

As to the issue of auto-updating itself... I think the simplest answer
to this question (personally) is that (go ahead and attack me here)
there shouldn't be auto-updates... but that there should be
auto-notifications for update when (a) update is available, but that
(b) this notification should never "push" the user to update (e.g. the
notification should never say "oh hey user if you don't update by such
and such a date, your wallet will not work or satoshis will die
because of your inaction"
(stays quiet while likely 100-e-mail thread is spawned from this)

- -O

Tamas Blummer:
New Year. New Location. New Benefits. New Data Center in Ashburn, VA.

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEcBAEBCgAGBQJUvsnBAAoJEGxwq/inSG8CGekIAJH4lUdk81sVfQqxZ4sKOKFM
5iAvCD4JNuV+xcCZBiNNr1GxIZEVoDRQYupo7wB1A5uGW+STLHDGsEMuDNyiOcNl
oSsJQFZJabxL7dIn8g89Gw+8J8LtYKEkHHZLk5J5QF0DkRljXjEcOV4KL6WXhdl5
ToV01POMUBbSJsQt2lLznmCvQ+4QW5/GJ9Hk04HIub+kzuil0R23CgRH9QFevC9S
2/RT3NnfGFu+jU5+K/o8RbuUuzExq94x4w266IEmJc0NsLHxnxsg2PefabQbfdzp
P7FU7+D9NsIOaBGTXnQK80kpgRCJ49Gf9HXHKFYg2KCFuqgJYa8DnHm1Xlfo7DQ=
=yS8H
-----END PGP SIGNATURE-----


-------------------------------------
2015-06-01 0:40 GMT+01:00 Pindar Wong <pindar.wong@gmail.com>:
no, let me rephrase: The disadvantage alex refers to only exists if
miners do not have the same network speed.

Just like easy ASIC access, low price electricity, etc are not a free
and open market.



-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



On 08/19/2015 04:06 AM, Jorge Timón wrote:


You must be really tired.

Please read the whole pull request discussing this loathsome subject her
e:

https://github.com/bitcoin-dot-org/bitcoin.org/pull/899

It was fairly detailed and conclusive. I'm not being a dumdum when I
say that it is controversial.


Everybody here is well aware of what this sad proposal is.  See
detailed reply to the moaning and groaning of Hearn on this subject,
where he claimed that "the difference between hard and soft forks is
actually quite small, has got smaller with time and is thus hardly the
policy-founding chasm you seem to think it is."  He was wrong, of
course.  Thus, my reply to that, which I won't bother to quote in
detail but which you can read here:
https://github.com/bitcoin-dot-org/bitcoin.org/pull/899#issuecomment-117
815987



Given the state in which bitcoin is in now, one could say that things
are fairly horrible, but by no means necessitating, as you put it, a
schism hardfork.  It is clear and evidenced by my previous posts and
others that Hearn's efforts are an attack on the bitcoin network.


It's not totally fine at all.  It shouldn't even exist.  People are
doing other unsuspecting users a disservice by even suggesting that it
should be downloaded.

 The big problem is

This is certainly a problem.


#GAVEL

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJV1GevAAoJEGxwq/inSG8CJXwH/3XX7Osr48MmcJ9mrPyOJ4Zn
WxRmdJ99mbO5KhrjD8+eRtFjURQNsQYAJ6ftnQEGaksuprSYCPQQR6WsV9mqRKZ2
/zdSz/5RjdADsjB2FDN6gWpwpyg7tzUpB6D4rCjuL1fcRmieCxwNUxnnFTbedxpE
MdT2oj9uSB7tTvU4AYs2VzJq0QeRn/c0pTJkBDwU0c4PN1tv/IQnOzmS+LnQDQJJ
eaunhpbmphRzqC9Mh8N7pD40ZyVoM5ysxVv86OaqmsOQL4W01CahQKADB4T/pBla
AOPh3LLxp7aamC9aYnBfxIaWXj28BZCwVasDkJdQ/Qg/rV5/Kze7TjJs9G2sowc=
=OvDj
-----END PGP SIGNATURE-----

-------------------------------------
In terms of miners, a strong supermajority is arguably sufficient, even 75%
would be enough.

The near total consensus required is merchants and users.  If (almost) all
merchants and users updated and only 75% of the miners updated, then that
would give a successful hard-fork.

On the other hand, if 99.99% of the miners updated and only 75% of
merchants and 75% of users updated, then that would be a serioud split of
the network.

The advantage of strong miner support is that it effectively kills the fork
that follows the old rules.  The 25% of merchants and users sees a
blockchain stall.

Miners are likely to switch to the fork that is worth the most.  A mining
pool could even give 2 different sub-domains.  A hasher can pick which
rule-set to follow.  Most likely, they would converge on the fork which
paid the most, but the old ruleset would likely still have some hashing
power and would eventually re-target.

On Thu, May 7, 2015 at 9:00 PM, Roy Badami <roy@gnomon.org.uk> wrote:

-------------------------------------
Thanks for all the feedback Eric. You know we value all that you have to say. That's what this forum is for. We're looking for great ideas to harden this protocol and we're not closed to better ideas and we'll improve it as suggestions come up.



   
Paul Puey CEO / Co-Founder, Airbitz Inc
619.850.8624 | http://airbitz.co | San Diego
     



On Feb 5, 2015, at 5:05 PM, Eric Voskuil <eric@voskuil.org> wrote:


Another (unspendable) address can trivially match the prefix. Imagine
someone walking around in a mall with a phone in the pocket with a
malicious app, just disrupting business by causing money to be burned.
Manual verification doesn't fix this attack.


I don't think it would be great to constrain a standard implementation
to low cost purchases or the need for manual verification, but again
manual prefix verification isn't actually a solution.


An appeal to the security of BT bootstrapping isn't exactly flattering.

You know I love Airbitz, and I know you guys are extremely privacy
conscious. I personally would have no problem using this feature under
certain circumstances. My question is only whether it would be wise to
standardize on the proposal as-is.

e


-------------------------------------
On Tur, Aug 11, 2015 at 07:08 AM, *Mark Friedenbach* via bitcoin-dev <

bitcoin-dev at lists.linuxfoundation.org
<https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>>
wrote:





I agree on the importance of having the credible threat of being able to
operate in the underground, and for the reasons you outlined.  However, I
see that threat as being inherent in the now-public-knowledge that a system
like Bitcoin can exist.  The smart governments already know that
Bitcoin-like systems are unstoppable phenomena, that they can operate over
Tor and I2P, that they can and do run without central servers, and that
they can be run on commodity hardware without detection.  Bitcoin itself
does not need to constantly operate in survival-mode, hunkered down, and
always ready for big brother’s onslaught, to benefit from the protection of
the ‘credible threat’.

It’s important to accurately asses the level of threat the Bitcoin system
faces from regulation, legislation, and government ‘operations’.  If we are
too paranoid, we are going to waste resources or forgo opportunities in the
name of, essentially, baseless fear.  When I got involved with this project
in 2012, no one really knew how governments were going to react.  Had an
all out war-on-Bitcoin been declared, I think it’s pretty safe to say the
structure of the network would look different than it does today.  We would
probably be discussing ways to disguise Bitcoin traffic to look like VoIP
calls, not talking about how to best scale the network.  In light of the
current regulatory climate surrounding Bitcoin, I believe the best security
against a state-sponsored / political crackdown to be gained at this time
comes from growing the user base and use cases, as opposed to hardening and
fortifying the protocol.  Uber is a great example of this form of
security-though-adoption, as was mentioned earlier today on this mailing
list.

If there are security or network-hardening measures that don’t come at the
expense of growing the user base and use cases, then there is no reason not
to adopt them.  The recent improvements in Tor routing are a great example
of a security improvement that in no meaningful way slows Bitcoin’s
potential growth.  How does this relate to the Blocksize debate?  Let’s
accept that 8 MB blocks might cause a little bit, and perhaps even a
‘medium bit’ (however that is measured), of centralization.  Although the
network might be slightly more vulnerable to government attack, if millions
more people are able to join the system as a result, I’d wager the overall
security situation would be stronger, owning to greatly decreased risk of
attack.

-Corey (CubicEarth)
-------------------------------------
Also (I am fuzzy on the details for this), Bitcoind will detect when a node
is misbehaving and (I believe) it will blacklist misbehaving nodes for a
period of time so it doesn't continually keep trying to connect to tarpit
nodes, for example.

On Wed, Mar 4, 2015 at 6:13 PM, Kevin Greene <kgreenek@gmail.com> wrote:

-------------------------------------

This isn't about "everyone's coffee".  This is about an absolute minimum
amount of participation by people who wish to use the network.   If our
goal is really for bitcoin to really be a global, open transaction
network that makes money fluid, then 7tps is already a failure.  If even
5% of the world (350M people) was using the network for 1 tx per month
(perhaps to open payment channels, or shift money between side chains),
we'll be above 100 tps.  And that doesn't include all the
non-individuals (organizations) that want to use it.

The goals of "a global transaction network" and "everyone must be able
to run a full node with their $200 dell laptop" are not compatible.  We
need to accept that a global transaction system cannot be
fully/constantly audited by everyone and their mother.  The important
feature of the network is that it is open and anyone *can* get the
history and verify it.  But not everyone is required to.   Trying to
promote a system where the history can be forever handled by a low-end
PC is already falling out of reach, even with our miniscule 7 tps. 
Clinging to that goal needlessly limits the capability for the network
to scale to be a useful global payments system



On 05/07/2015 03:54 PM, Jeff Garzik wrote:

-------------------------------------
I decided to try to certify Wladimir's PGP keys (the old one (2346C9A6)
first, and then the new one (36C2E964), since it was signed with the old
one).

I visited
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/009045.html
to see that the new key was referenced in a message signed by the old one.
I figure it's safe to assume that if the old key actually signed that
message, then the core dev using <laanwj at gmail.com
<https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>> is an
actual core dev (that's all I'd be worried about).  So I copied the text
from ------BEGIN PGP SIGNED MESSAGE----- to -----END PGP SIGNATURE----- to
my clipboard and asked Kleopatra (on Windows) to verify it.  It says the
signature is bad.  If I alter the text of the email (so the signature would
be have to be different to be valid), it says exactly the same thing.  So
maybe something is wrong with Kleopatra on Windows.

However, the SHA256SUMS.asc file I got from the magnet link posted in the
email (below)  verifies just fine using the new key (36C2E964).  So I
figure Kleopatra is not broken.  It recognizes that the old key was used to
create the signature in that old email, but it says it's invalid.  Has
Wladimir been secretly replaced by someone who doesn't have access to the
private key for 2346C9A6?  Can you make a (bad) signature look like it was
made using a key you don't have? The whole reason for signing is so that we
will know if something like that happened.  So did I do something wrong?
(I mean, besides using Windows).

I believe this is the expected result if someone took something Wladimir
signed and ripped off the signature and pasted it below this new message to
make everyone think the new message was genuine.  Maybe Wladimir made an
edit after the signature was attached.  Or maybe it got changed when it
went through the email system.  It would be nice to know.  Anyway, I fell
back on Windows security and ran the install because it said it verified
that the publisher was "The Bitcoin Foundation".


On Fri, Nov 13, 2015 at 5:13 AM, Wladimir J. van der Laan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:




-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
Seems like 3 is something we want to do no matter what and therefore
is the "most future-proof" solution.
I wonder if I can help with that (and I know there's more people that
would be interested).
Where's the current "non-full" nVersion bits implementation?
Why implement a "non-full" version instead of going with the full
implementation directly?


On Wed, Aug 19, 2015 at 8:10 AM, Mark Friedenbach via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------

Well, but RAM is not infinite :-) Effectively what these caps are doing is
setting the minimum hardware requirements for running a Bitcoin node.

That's OK by me - I don't think we are actually going to exhaust the
hardware abilities of any reasonable computer any time soon, but still,
having the software recognise the finite nature of a computing machine
doesn't seem unwise.



Not "any" size because, again, the remote node must buffer things up and
have the transaction data actually in memory in order to digest it. But a
much larger size, yes.

However, that's a bigger change.
-------------------------------------
On Mon, Jul 20, 2015 at 3:43 PM, Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


Mmmm.... you'd have to:

a) Have lost or thrown away the keys to the unspent transaction outputs
b) Have created a locktime'd transaction with a lock time after the
BIP100/101 switchover times
that is more than 100,000 bytes big
c) Have some special relationship with a miner that you trust to still be
around when the transaction
unlocks that would mine the bigger-than-standard transaction for you.

I don't think adding extra complexity to consensus-critical code to support
such an incredibly unlikely
scenario is the right decision here. I think it is more likely that the
extra complexity would trigger a bug
that causes a loss of bitcoin greater than the amount of bitcoin tied up in
locktime'ed transactions
(because I think there are approximately zero BTC tied up in >100K
locktime'ed transactions).


RE: limit size of transaction+parents:  Feature creep, belongs in another
BIP in my opinion. This one
is focused on fixing CVE-2013-2292


-- 
--
Gavin Andresen
-------------------------------------
I see, thanks for clearing that up, I misread what Gavin stated.

On Wed, Dec 9, 2015 at 12:29 AM, Gregory Maxwell <greg@xiph.org> wrote:

-------------------------------------
On 1 August 2015 at 01:17, Gregory Maxwell <gmaxwell@gmail.com> wrote:


Right. Why anyone would want to do this intentionally, however, is not
clear. The coins would be worth less as they wouldn't be fungible across
the chains anymore.

Additionally, new coins will be issued, along with fees, on both

This is something that hadn't entered my mind when I made my assertions. At
the moment I can't see any way to avoid this fact.

Also any transaction whos casual history extends from one of the above

Wallets could detect this and notify the user. Due to Gresham's Law, I'll
admit the observation that these outputs would likely get spent in
preference (as they are less fungible), but whether the payee would be
happy to accept them is a different matter.


Well in your example they wouldn't, because they know that your version
wouldn't be accepted by the economic majority. But it's not as clear cut
for the larger blocks case. They may move over gradually as they see the
new chain gain acceptance, demonstrated by the higher trading volume on it.
-------------------------------------
Yeah. We made a git repo instead, so we don't have to bother with the
exclusive-by-default wiki policies. It's linked in this email chain.

I'll be getting home tomorrow, so I should be able to start back up on
this. A few days from now we should throw this on /r/Bitcoin so we can get
some more public comment on it. They already gave me a few leads to chase.
On Jun 5, 2015 11:34 PM, "Pindar Wong" <pindar.wong@gmail.com> wrote:

-------------------------------------
On Tue, Jul 21, 2015 at 6:58 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

mmmm kay.  Let's try to keep it technical, please.

2MB is a limit that has been discussed as a viable next-step, meeting with
the most consensus.

2MB gets beyond the 1MB hard fork issue, while still remaining within a
safety cap that should ensure the system does not go "off the rails" as
some has predicted.

Security, privacy and centralization are not going to disappear at 2MB.

Further, a limited step gains valuable field data for judging whether
further steps are warranted - thus informing the "better block size
solution" development process.

Finally, as stated in the initial PR, it is intended as a viable fallback
should we reach a point of criticality where the user community feels a
block size increase is warranted, yet cannot reach consensus on a fancy,
all-consuming solution be it 20MB, flexcap, BIP 100, BIP 102, etc.

I am open to suggestions for improving BIP 102.  The goal is a minimum
complexity fallback that others have previously agreed was a useful
kick-the-can compromise - a static 2MB cap.
-------------------------------------
Whats the objective?  Is it to require accidental disclosure of two
private keys to compute the master private key?

Adam

On 21 February 2015 at 13:20, 木ノ下じょな <kinoshitajona@gmail.com> wrote:


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 05/09/2015 02:02 PM, Andrew wrote:

How many individuals and companies do you propose will ever use
Bitcoin (order of magnitude estimates are fine)

Whatever number you select above, please describe approximately how
many lifetime Bitcoin transactions each individual and company will be
capable of performing with a 1 MB block size limit.

-----BEGIN PGP SIGNATURE-----

iQIcBAEBAgAGBQJVTgNcAAoJECpf2nDq2eYjM8AP/2kwSF+HMPR1KdaZsATL4rog
xSS97Q5iEX8StA61jUqHQmpXL5pG6z5DeeKT/liwcMnYnVqOEOLvoVctr3gXfgRz
9GJeTOlmN5l9xBeX/nWa0A2ql0kWZpYolBS1FwYadWReAD8R0X9UeBd9YXLZNy33
Ow9JjwRjKHhsuyrlMP8pRDKlGPoa/U+2aW4FwiysMLa0Gu6dbFjTrp3bHw4Fccpi
X0E/aDN68U4FV+lZ4NzkMsBK9VARzmC8KI0DQ540pqfkcnyoYf0VERl/gslPWhfq
t6Rqa7vHHMqFe82lgCd3ji8Qhsz8oBrDS4u4jqwATvgihgImOB6K85JoKmf3y2JS
jByjMGd4Ep0F80Z2MRhi6HuEoRU69uY2u6l9bZxMjzvLX8sG6QTNk3uLMS3ARXcY
JBjZ/g13DXgcRj01fq05CHbCTJYZgTA9pRZTY+ZKH4r0mu86b9ua7hjvyKHS6q54
uaFmRkNcnKlpCY+fvH/JUdvvmwrA0ETUdHhRyk8vzWIMi+aH4//GwrCmBNRrugzv
9JtQ1BC+tQqtSX2VkFEhAVISitgkBqurVVlGk18FvVKPFO8cnFS/6NWoPE0WLLzW
2pTuhEPjdz9UAHD3RW601rb4C0LbuwVlGO4tYBjyqCmk/vBlES2XIjQKctXZLBEy
eLgn3gMwEXUTU6UdGyvb
=RPhK
-----END PGP SIGNATURE-----
-------------------------------------
Den 30 jun 2015 03:00 skrev "Tom Harding" <tomh@thinlink.com>:
It isn't securable. It is running on the honor system exclusively. It will
be attacked, it will fail, losses will be had, the attackers will walk away
with embarrassingly large sums.
different than banning it for relaying garbage.
somehow.  I didn't offer a complete design, don't claim magical properties,
and certainly didn't mean to imply that nodes passing a test could be
trusted (as you suggest with your "accountable parties").

But the check means nothing at all to you since no information which you
can learn from doing so can be trusted for use in decision making of any
kind, so why do it? Unless you pay for hosting of that particular node
which you test, you have no reason to care for any reason other than simple
statistics.

The claims I made is based on simple economic analysis - if *to be able to
attack* first requires effort and risk that exceed the payoff, you're
unlikely to try. Being legally accountable and identified in advance and
having to build reputation before being trusted with attack-worthy sums is
strongly discouraging.
-------------------------------------
On Mon, Aug 24, 2015 at 05:37:51PM +0000, Matt Corallo wrote:

Yes, it makes sense to not explicitly exclude it. 
Looks good to me.

Wladimir


-------------------------------------
What about trying the dynamic scaling method within the 20MB range + 1 year
with a 40% increase of that cap?  Until a way to dynamically scale is
found, the cap will only continue to be an issue.  With 20 MB + 40% yoy,
we're either imposing an arbitrary cap later, or achieving less than great
DOS protection always.  Why not set that policy as a maximum for 2 years as
a protection against the possibility of dynamic scaling abuse, and see what
happens with a dynamic method in the mean time.  The policy of Max(1MB,
(average size over previous 144 blocks) * 2) calculated at each block seems
pretty reasonable.

As an outsider, the real 'median' here seems to be 'keeping the cap as
small as possible while allowing for larger blocks still'.    We know
miners will want to keep space in their blocks relatively scarce, but we
also know that doesn't exclude the more powerful miners from
including superfluous transactions to increase their effective share of the
network.  I have the luck of not being drained by this topic over the past
three years, so it looks to me as if its two poles of 'block size must
increase' and 'block size must not increase' are forcing what is the clear
route to establishing the 'right' block size off the table.

--Andrew Len
(sorry if anybody received this twice, sent as the wrong email the first
time around).

On Fri, May 29, 2015 at 5:39 AM, Gavin Andresen <gavinandresen@gmail.com>
wrote:

-------------------------------------
On Sep 11, 2015 1:18 PM, "Christophe Biocca" <christophe.biocca@gmail.com>
wrote:

I thought he was proposing a new consesnsus rule. I see, this would be just
a policy validation that everybody would be free to ignore (like the "first
seen" spend conflict tx replacement policy).

I don't see how miners would benefit from running this policy so I would
not expect them to run it in the long run (like the "first seen" spend
conflict tx replacement policy).
If miners don't use it, I don't see how users can benefit from running that
policy themselves.
They will still have to keep waiting some block confirmation to
exponentially reduce the chances of a successful double-spend attack with
each new confirmation (as explained in the bitcoin white paper).


How do you know which of 2 blocks with the same height is "newer"?

sophisticated
-------------------------------------
Ok, I'm going to separate terms: current-libconsensus from theoretical
future-libconsensus (implementing ALL consensus rules).

On Tue, Jul 28, 2015 at 8:40 AM, Eric Voskuil <eric@voskuil.org> wrote:

We want to complete future-libconsensus (decouple all the consensus
rules from the rest of the bitcoin core code) first.
Then we can move future-libconsensus to a subrepository/subtree like
libsecp256k1 and I believe everybody wants this to eventually happen.
Separating current-libconsensus now may make completing
future-libconsensus harder.


What I mean is that once it is separated to a subtree, there's one more step:

Make Bitcoin Core use future-libconsensus' API instead of a subtree.
Decoupling future-libconsensus from Bitcoin Core is one thing, and
Decoupling Bitcoin Core from future-libconsensus is another thing: you
need to decouple Bitcoin Core from all future-libconsensus
implementation internals. For example, script/sign (part of Bitcoin
Core) depends on individual non-API-exposed classes in
current-libconsensus.
Moving from a subtree to a completely separated library is what I
don't know will ever happen, but I don't think this is "unfairly
advantageous" for Bitcoin Core or anything like that: other
implementations can also use future-libcosensus as a subtree instead
of the C API as well.

You have accomplished the goal of separating curren-libconsensus, not
future-libconsensus.
In fact, if you complete the equivalent of future-libconsensus in
libbitcoin and separate that, maybe that's a better place to start
drafting a full API.


future-libconsensus will not have significant changes *once it is
completed*. Currently future-libconsensus is spread around many places
inclusing src/main, so that obviously needs to change before it can be
separated to an independent repo.


Well, Bitcoin Core is "currently the only user of future-libconsensus"
since bitcoin core and future-libconsensus are currently mutually
coupled.
Bitcoin Core will always keep using future-libconsensus. The only
question is whether it will use it through the C API or as a
subtree/subrepository (both options are also available to other
implementations). I don't know if decoupling Bitcoin Core from
future-libconsensus' implementation details enough to be able to
directly use the API is worth it or if anyone will be interested in
doing so. For me this last step is not all that interesting: if we
have an independent repo with a full API that other implementations
can use, I don't really care about Bitcoin Core not going through the
API and using including all the code directly instead.


Great.


As I told you before the reason why current-libconsensus is using
OpenSSL instead of libsecp256k1 is that the very authors of
libsecp256k1 warned that using libsecp256k1 for validation was
consensus risky. As Wladimir said, Pieter Wuille will make an
announcement about this soon.
In any case, as I told you in previous conversations, the plan is to
move from OpenSSL to libsecp256k1 for validation too (so libconsensus
wil drop the OpenSSL dependency and this is just a temporary concern).


Well, the questions about the API are just in english, no need to
deeply know Bitcoin Core's (satoshi client) internals.
But maybe we should have an independent mailing list for
consensus-only things. Not all future-libconsensus users will be
interested in Bitcoin Core-specific discussions, and making them
subscribe and filter seems like an unnecessary burden to
participation.


I think that's precisely what makes it a high priority in the eyes of
all the people working on it or reviewing related changes.
But, yes, I guess "evil-thinking", maybe that's what make it a low
priority for someone evil that wants Bitcoin Core's implementation
have more importance than it shold forever. I prefer not to evil-think
and just attribute it to having other priorities or just apathy about
it.


By "finished" I mean a future-libconsensus that implements ALL
consensus rules. We don't have that yet.


Bitcoin Core is the ONLY "user" of future-libconsensus (which actually
only exists inside Bitcoin Core and it's not exposed).
Current-libconsensus is used by Bitcoin Core and also exposed as an
independent build (not a separated repository yet).

Once future-Bitcoin's API is completed and the code in a different
repo, how is Bitcoin Core using the API instead of the sources
directly of any importance to other implementations?
That's really the part that I cannot understand. It will be a problem
Bitcoin Core, but if other implementations want to have (and maybe
solve later) the same problem they can use a subtree too and start
coupling their code with implementations details from
future-libconsensus.
Why would they want to do that? Again, I have no idea. I don't
understand what the complain is here.


Great. I mean, I wasn't asking about reviewing the commits themselves
(which is also great if you do), but rather on answering the questions
I'm making there, ie: what to expose next (ie VerifyTx or
VerifyHeader)? would this be an acceptable way to expose VerifyHeader
? Which of he step-checks functions is worth exposing too (Bitcoin
Core is currently using some to prevent DoS attacks, for example)?


Well, neither libbitcoincosnensus nor libbitcoin-consensus implements
all the consensus rules.
That's what makes them different from future-libconsensus.
But great, we're confirming more views that we share.


Well, this is where I fear we will never agree. I think "Bitcoin is
different" in this reward and you disagree.
Maybe Pieter's explanation is more convincing to you:
https://youtu.be/PxW5D9xCIsc?t=769
Otherwise, I think I'll stop trying convincing you.
But one last try:

If there's 2 "specification implementations":

1) widely deployed but containing 1 bug
2) not deployed anywhere but more readable and without the bug.

When the bug is found, is it a consensus rule or not?
If it is, it turns out the second implementation wasn't an specification at all.
If it's not, nobody has been ever following consensus rules!!


1) Working on it
2) The Satohi client has been using all along and it will use it
forever (maybe not through the API, but I don't get what the problem
with that is).
3) There will be an announce about this soon.


Once future-libconsensus exists as a separated repository, I don't
think you want to preemptively fork it unless you're actually changing
something.


Ideally schism consensus-rules changes will never happen, since they
effectively divide the currency in 2 and force the users to chose
(apart from other complications).
But if someone is trying to impose a schism fork (from Bitcoin Core,
from the new future-lbiconsensus repo or from anywhere else) it
becomes trivial to protect your implementation against forced changes.
Ideally all consensus changes will be uncontroversial or not happen at all.
But I think about non-ideal cases too.


1) Reasonable.
2) You mean use it through the API? Seriously, why do you care?


To be clear, Bitcoin Core's using future-libconsensus through a
subtree instead of the API is not a preference or a goal: it is just
how things will be just after completing and separating
future-libconsensus. Making it use the API instead of the subtree will
be additional work. I'm not sure I will want/have time to do it and I
don't know of anyone planning to do that (which seems very reasonably
given that a separated future-libconsensus is a dependency for such a
change).


Is it because "fear of consensus bugs is what keeps people on the
satoshi client" and you want to keep things this way?
Sorry, just joking about your previous "cynic" comment. See? Don't
attribute to malice what can be attributed to lack of time.

-------------------------------------
On Wed, Oct 21, 2015 at 8:49 AM, Christian Decker
<decker.christian@gmail.com> wrote:

"sort of"

Using the sighash normalization doesn't allow creating a utxo set or
scanning the blockchain while only transferring ~1/3rd of the data
(allowing for reduced security fast start, and private lite wallets);
it requires txin ID rewriting when the witness changes on a parent
transaction; it requires hashing each transaction multiple times (for
the normalized ID, and the old ID), it requires storing two IDs for
every transaction in the UTXO set. -- but indeed, it's easier to
deploy (though not infinitely easier as I thought before).

-------------------------------------
Bryan Bishop via bitcoin-dev 於 2015-08-30 14:56 寫到:


Thanks for your summary. This one seems particularly interesting. 
However, it does not allow fine adjustment for each input and output 
separately, so I wonder if it really "fully enable any seen or unforseen 
use case of the CTransactionSignatureSerializer." as it claims.

-------------------------------------
On Tue, Sep 01, 2015 at 02:30:17PM +0100, Ahmed Zsales via bitcoin-dev wrote:

As long as it's an open system, one can't require a specific license for everything added to the chain.

You could of course make the BIP advisory, but I'm not sure what that would help. You still wouldn't have any certainty what license the contents of block #XXXX would be under.

Wladimir

-------------------------------------
I suggest we use short-circuit evaluation. If someone complains, we figure it out as we go, maybe depending on the nature of the complaint. If nobody complains, we get it done faster.

We're humans. We have the ability to respond to novel conditions without relying on predetermined rules and algorithms. I suggest we use that ability sometimes.

On Dec 29, 2015, at 4:55 AM, jl2012 <jl2012@xbt.hk> wrote:


-------------------------------------

How would email have looked if it required 300 MW of power to support it for "free" for 10 years?

In practice email was never free- it was paid for by the payments users made to ISPs. ISPs paid for email and network infrastructure from that.

The equivalent analogy here would be to drop fees completely and pay a specific miner to mine all of your transactions as a monthly subscription (which of course doesn't work in a non-permission-based network).

Miners have real (huge) costs - they will be in a lot of pain with reward halving if a few model does not replace that. That in turn poses a huge risk of smaller miners shutting down, which in turn centralises things even more. I would argue that the lack of pool diversity and thus lack of block makers is already the single biggest risk for a decentralised system; avoiding the issue of fees just accelerates this.
-------------------------------------
On 02/02/2015 03:47 PM, vv01f wrote:


Pavol's suggestion saves 2 chars only because its just a date. I think
the creation date should be at least precise to the hour, if not to the
minute.

But anyhow, if everyone prefers a human readble date format I will bow
to the majority.




-------------------------------------
(Claim of large bitcoin ecosystem companies without full nodes) this
says to me rather we have a need for education: I run a full node
myself (intermittently), just for my puny collection of bitcoins.  If
I ran a business with custody of client funds I'd wake up in a cold
sweat at night about the security and integrity of the companies full
nodes, and reconciliation of client funds against them.

However I'm not sure the claim is accurate ($30m funding and no full
node) but to take the hypothetical that this pattern exists, security
people and architects at such companies must insist on the company
running their own full node to depend on and cross check from
otherwise they would be needlessly putting their client's funds at
risk.

The crypto currency security standards document probably covers
requirement for fullnode somewhere
https://cryptoconsortium.github.io/CCSS/ - we need some kind of basic
minimum bar standard for companies to aim for and this seems like a
reasonable start!

Reducing custody in my opinion should also be an aim eg 2 of 2
multisig + timelock seems like a more prudent approach, transaction
throughput permitting.  Right now exchange volume wouldnt fit on
chain, once bitcoin scaling has improved, perhaps it can.  I am
optimistic that within a year Bitcoin scaling and decentralisation
will look much better with current active work on decentralisation,
layer 2 scaling solutions.  As part of that I could see a modest
blocksize increase to smooth out the transition to layer 2.

In terms of a constructive discussion, I think it's interesting to
talk about the root cause and solutions: decentralisation (more
economically dependent full nodes, lower miner policy centralisation),
more layer 2 work.  People interested in scaling, if they havent,
should go read the lightning paper, look at the github and participate
in protocol or code work.  I think realistically we can have this
running inside of a year.  That significantly changes the dynamic.
Similarly a significant part of mining centralisation is artificial
and work is underway that will improve that.

What I mean about decentralisation is if decentralisation simple
metrics were in a healthy place, it would be a simple conversation to
make use of bandwidth improvements (in the range of 15%/year per Cisco
numbers) to get more throughput.  I do think flexcap is interesting as
a way to add one more control variable such that we can have
economically validated scaling.  Pushing fees to zero and increasing
centralisation to levels that weaken security with economically weak
payments is probably not desirable.  Without flexcap it seems the next
best thing we can do is rely on miners to balance user utility against
mining revenue, and it seems plausible that they would in extremis but
to my mind there are factors suggesting this could be problematic
incrementally: miners have not often been responsive to editing
defaults, or reacting to security or soft-fork upgrades; miners may
have some conflict of interest of users, eg they could use switching
cost economics to optimise for miner profit at the expense of user
utility, or attack each other in selfish-miner or other variants as
miners are also pitted against each other while being held honest by
economically dependent full nodes.

Adam

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Some brief thoughts, adding here a suggestion for a dynamic approach
to the issue. (e.g. each additional tx relayed has some thing
associated with it, that is, a "doubling" for each additional tx
relayed that spends a given UTXO, doesn't sound like it would be the
most dynamic approach to the issue; considering that full nodes use
the (UTXOs) to establish if transactions are valid  all inputs to a
transaction must be in the UTXO database for it to be valid, but
rather, would end up ratcheting upward the fee/kB for each additional
tx relayed, as proposed).

A more dynamic fee approach would be a better one, imho, but how is
this to occur?

Quoting from Gavin Andresen's http://gavinandresen.ninja/utxo-uhoh,
"The entire UTXO set doesnt have to be in RAM; it can be stored on an
SSD or spinning hard disk. The access pattern to the UTXO is not
random; outputs that were spent recently are more likely to be
re-spent than outputs that have not been spent in a long time. Bitcoin
Core already has a multi-level UTXO cache, thanks to the hard work of
Pieter Wuille."

The relay nodes would, in this scenario that is proposed here in this
message, be confirming and discarding; the wallet nodes, if I
understand properly, in this scenario, as proposed, should be
retaining (keeping a record of the transactions they've relayed and
using a mempool).

There are some questions here:

- - How is the mempool to be limited?
- - What is the mechanism by which the UTXO set is stored (or proposed
to be stored)?
- - How would dynamic fee determinations be calculated?
- - Please describe more the general purpose messaging network?

Thank you



On 07/18/2015 12:46 PM, Patrick Strateman via bitcoin-dev wrote:

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVq2cFAAoJEGxwq/inSG8CIo4IAJAZ97NvW6Qdjd6HLN8q2074
sEUGdDeonARiQZXLfGyTJVg43Yb6LKPqkjWPQEgl9LLNni8t99iUqu3BJxedRDjd
8x+/F8n5VJrUrn1pXUcbC1aWss1y8JPTO2KpF/WL254IFc8iE8MJf4YF8PDSgy5j
9uW8NvWvdT4dz+rXu9vqfcplz8x7NGQ+CWN2N2JlChhKLMFprXPIx8a7NQwaKdrY
lTpgAJWGMyPGNCmYQprBjIjOfp8tdTLQFlsLUAsXDmEisJX9goRVGMsHOWLTREoA
l3kTgM0WMv6MIG7NOQQcWLD7cZdwWYR9e49hc27VcHt2R/FTepvnwPqo2GtE0tM=
=eRbR
-----END PGP SIGNATURE-----

-------------------------------------
On Tue, May 19, 2015 at 9:28 AM, Christian Decker <
decker.christian@gmail.com> wrote:


The normalized TXID cannot depend on height for other transactions.
Otherwise, it gets mutated when been added to the chain, depending on
height.

An option would be that the height is included in the scriptSig for all
transactions, but for non-coinbase transctions, the height used is zero.

I think if height has to be an input into the normalized txid function, the
specifics of inclusion don't matter.

The previous txid for coinbases are required to be all zeros, so the
normalized txid could be to add the height to the txids of all inputs.
Again, non-coinbase transactions would have heights of zero.



I assumed that since the scriptSig in the coinbase is specifically intended
to be "random" bytes/extra nonce, so putting a restriction on it was
guaranteed to be backward compatible.
-------------------------------------
I've been tossing around an idea in my head that involves time-locked
encryption [0] and I wondered what the devs here think about it. In a
nutshell: the timechain is a serial chain of time-lock encrypted GPG keys
at N minute intervals (meaning that it requires N minutes to decrypt a
single link / key in the chain and each link must be fully decrypted before
decryption can start on the next link.) For those not aware of how
time-lock encryption works it goes something like this:

1. Choose some random, unique text - this is the initialisation vector or
IV.

2. Hash that text -> output.

3. Hash the output -> output.

4. Hash the output -> output.

5. ...

6. Process is repeated for N minutes.

7. Result is then used to generate encryption keys and the public key can
be used to time-lock encrypt an arbitrary number of plaintexts.

8. All intermediary results are discarded -- only the pub key is kept and
giving out the IV forces an individual to have to repeat the same amount of
work used to generate the encryption key.

What's interesting about this is that the keys can be generated in parallel
and then "stitched" together to form a single serial chain of keys. So
potentially, if a person had access to a GPU cluster then they could
generate a years worth of work in only 5 minutes. Now imagine if one were
to stitch these keys together into a chain of keys at five minute intervals
(a structure I refer to as the "timechain"): you could use this structure
to encrypt ECDSA keys which could then be used in multi-signature contract
schemes as a 100% decentralized, trustless way to execute refunds in
contract protocols.

Unexpected benefit: time-lock encryption can be used to build unbreakable
DACs.

Peter Todd has already done work on using Bitcoin to incentivize the
decryption process of time-lock encryption [1] but what he may not be aware
of is how important this process is for the construction of DACs.

Imagine a true peer-to-peer cryptocurrency exchange [2] that time-lock
encrypts a chain of ECDSA keys using the timechain and then sets up
contracts to pay a small portion of their fees "into" the ECDSA keys.
Essentially the exchange has created a DAC that pays its participants to
decrypt itself. This is the incentive for the decryption. The reason for
the incentive is that another chain of keys can be generate at 5 minute
intervals which can be used in contract protocols in place of nTimeLocked
refund transactions (which are vulnerable to transaction malleability.)

Sample contract using the timechain:

3 of 4 multi-sig: Owner, Owner, Recipient, Timelock

Pay N coins to recipient sequentially (micropayment channel) before [time /
date], otherwise fall back on timelock decrypted refund key to give full
leverage back to owner. This is how smart contracts would work using the
timechain for refunds (instead of nTimeLock TXs.)

Using the DAC, it might also be possible to force participants to reveal
their solutions to the decryption of the timechain (otherwise the first
person who starts on the chain would receive all the fees which isn't very
fair.) One way to do this would be to use the public key for the fee ECDSA
key as the IV used to generate the next key on the chain. To spend the fees
would therefore require revealing the public key if the fees were paid to a
pay-to-pub-key-hash transaction.

A further precaution would be to generate the pay to fee transaction in
such a way that the amount needs to be redeemed otherwise another
transaction would burn the coins. (I haven't worked out the full details
for this but similar schemes have been used successfully, for example in
BitHalo [3]. The Lightning Network [4] offers another potential solution.)
Perhaps a custom blockchain or sidechain could also be used to award coins
for successful (and timely solutions) but this is a subject for future work.

In conclusion: I have described a simple way to solve the TX malleability
problem in smart contract protocols without requiring a fork or relying on
a third-party escrow scheme to manage coins. My solution doesn't require
any trust beyond the initial need for the timechain to be generated in a
secure cluster and the solution remains secure so long as participants
stick to using future keys in the chain regardless of how far along
decryption is.

What do you think of the idea so far?

Obviously the biggest flaw here is that the integrity of a timechain can't
be known before-hand but if a timechain were to be generated securely by a
reputable party, the biggest benefit of using it is that it basically runs
itself: it does not require any third-party to manage its functionality and
the entity which originally generated it can completely disappear without
interrupting service. This could, for instance - allow companies to create
entirely secure and reliable systems that couldn't be hacked as the
behaviour of a timechain is deterministic. I think this is a huge
improvement over existing systems which require third-parties to be
perpetually trusted with managing key-pairs on their web servers.

Anyway, that's the basic idea. Let me know what you think.


Sources:

[0] http://www.gwern.net/Self-decrypting%20files

[1]
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg05547.html

[2] http://www.uptrenda.com/uptrenda.pdf

[3] https://bithalo.org/wp-content/uploads/2014/06/whitepaper_twosided.pdf

[4] https://lightning.network/lightning-network-paper-DRAFT-0.5.pdf
-------------------------------------
I have a potential BIP, "Multi-Currency Hierarchy For Use In
Multisignature Deterministic Wallets." I'm requesting discussion on it,
and possibly assignment of a BIP number.

It's located in this github gist:
https://gist.github.com/Kefkius/1aa02945e532f8739023


-------------------------------------
Although consumer to merchant is a use case for BLE I would argue that NFC has a higher chance of providing a better user experience in most cases since, at least on Android, a user can tap their phone without even having a wallet running. The URI handler will launch the wallet for them. However a dedicated, user facing, screen can give certainty that the user is connecting to the correct recipient. 

1. Because it can show an address prefix 
2. It can display the users nickname/handle upon connecting which is only sent to the merchant upon a point to point connection. Not a broadcast. 

The Airbitz wallet already does this on the recipient side. A popup shows the most recent person connecting to the recipient. 


   
Paul Puey CEO / Co-Founder, Airbitz Inc
619.850.8624 | http://airbitz.co | San Diego
     



On Feb 5, 2015, at 3:34 PM, Roy Badami <roy@gnomon.org.uk> wrote:

For peer-to-peer payments, how common do we think that the payment is
of an ad hoc nature rather than to a known contact?

If I want to pay my friends/colleagues/etc over a restaurant table
there's no reason why I couldn't already have their public keys in my
contact list - then it would be pretty straightforward to have a
watertight mechanism where I would know who I was paying.  You could
probably even relatively securely bootstrap a key exchange over SMS,
relying only on the contacts already having each other in their
phonebooks.

As for comsumer-to-merchant transactions where the merchant is a
bricks and mortar merchant, IMHO it absolutely has to be "pay that
terminal over there".  It's the trust model we all currently use -
whether paying cash or card - and it's the only trust model that works
IMHO (and customers and businesses alike are well aware of the risks
of a fraudster standing behind the counter pretending to be an
employee accepting payment - and by and large are pretty good at
mitigating it).  OTOH as we've discussed here before there are many
use cases where the custoemr doesn't actually know or care about the
name of the shop or bar they walked into but is pretty damn sure that
they need to make payment to the person over there behind the counter.

Granted, there are cases taht dont' fall into either of the above -
but they're the cases that are (a) harder to figure out how to
authenticate and consequently (b) the use cases that are going to be
most subject to attempted fraud.

roy

-------------------------------------
Thank you for your response, that does make sense. It's going to be
interesting to follow what is going to happen!

2015-05-14 3:41 GMT+12:00 Gavin Andresen <gavinandresen@gmail.com>:

-------------------------------------

It's worth noting that the original protocol as designed by Satoshi did not
have this limitation. It has evolved this way because of ad-hoc DoS fixes
over time (btw I'm not saying they were the wrong thing to do, as non "ad
hoc" solutions are significantly more work). But it seems like eventually a
different approach to handling DoS attacks based on resource prioritisation
and scheduling will become needed / implemented, and at that point the
original design could be safely brought back to life.
-------------------------------------
Le 08/05/2015 22:33, Mark Friedenbach a crit :


Sorry but I fail to see how a linear identity transform between block
size and difficulty would work.

The miner's reward for finding a block is the sum of subsidy and fees:

 R = S + F

The probability that the miner will find a block over a time interval is
inversely proportional to the difficulty D:

 P = K / D

where K is a constant that depends on the miner's hashrate. The expected
reward of the miner is:

 E = P * R

Consider that the miner chooses a new difficulty:

 D' = D(1 + x).

With a linear identity transform between block size and difficulty, the
miner will be allowed to collect fees from a block of size: S'=S(1+x)

In the best case, collected will be proportional to block size:

 F' = F(1+x)

Thus we get:

 E' = P' * R' = K/(D(1+x)) * (S + F(1+x))

 E' = E - x/(1+x) * S * K / D

So with this linear identity transform, increasing block size never
increases the miners gain. As long as the subsidy exists, the best
strategy for miners is to reduce block size (i.e. to choose x<0).


-------------------------------------
Yifu Guo via bitcoin-dev:

Why not? The blocksize increase eliminates the pressure to seek durable
solutions. But it will turn out differently. Other than we all think.

I really can not imagine that a 20-year plan will be successful. At one
point we like to clear away the auto-increase. I think that is the only
thing for sure at the moment.

I hope that the current protocol and blockchain will be continued.
Perhaps with fewer people. That would settles the matter for the time
being. ;)

- oliver

-------------------------------------

Quoting Peter Todd via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>:


I think I have already answered this with my previous mail. If there  
is consensus among major exchanges and merchants, the preference of  
miners are not particularly relevant. A checkpoint could be  
implemented in a decentralized way to make sure miners of the original  
chain won't be able to overtake the new chain.

Bitcoin has no intrinsic value. Bitcoin has value because people are  
willing to exchange it with something really valuable (e.g. a pizza;  
or USD which could buy a pizza). If most bitcoin-accepting business  
agree to follow BIP102 and ONLY BIP102, then BIP102 is THE Bitcoin,  
and the original chain is just a dSHA256 alt-coin which one can't even  
merge mine with BIP102. Switching to BIP102 is the only economically  
viable choice for miners.

Having said that, a miner voting may still be useful. It is just to  
make sure enough miners are ready for the change, instead of measuring  
their consensus. For example, the new rule will be implemented 1) 1  
week after 70% of miners are ready; or 2) on 1 Feb 2016, whichever  
happens first.

For SPV wallets, they have to strengthen their security model after  
the BIP66 fork, anyway. They should be able to identify potential  
consensus fork in the network and stop accepting incoming txs when it  
is in doubt. My "version 0 flag block" proposal could be a good  
generic way to indicate a hardfork to SPV wallets. (see my previous  
email on this topic)


-------------------------------------
The mail list is public, so it's not like the data on it is somehow
sensitive. Sourcefoge is fine, it has a nice web UI where you can browse
the message and sort/order them as you want, etc.

Why would you want to move to a paid solution? And why would you want
users to have to pay per message? This is the worst idea ever from my
point of view. We want to encourage people to join the community, run
full nodes, ask questions, come with solutions, ideas for improvements
and so on. Everyone should read and write and contribute as much as
possible with ideas in debates. You never know who can have bright ideas
in some contexts.

Bottom line is so far sourceforge handles the mail lists just fine. I
don't see a single advantage another mail list provider / system could
offer, except some headache and extra work for migration. The software
distribution via sourcefoge was cancelled for obvious reasons which I
fully understand and agree to, but it has nothing to do with the mail
lists. We have way more important things to brainstorm about.

On 6/10/2015 7:46 PM, Andy Schroder wrote:


-------------------------------------
On Tue, Dec 8, 2015 at 5:41 PM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


This trick can be improved by only using certain tx counts.  If the number
of transactions is limited to a power of 2 (other than the extra
transactions), then you get a path of length zero.

The number of non-zero bits in the tx count determings how many digests are
required.

https://github.com/TierNolan/bips/blob/aux_header/bip-aux-header.mediawiki

This gets the benefit of a soft-fork, while also keeping the proof lengths
small.  The linked bip has a 105 byte overhead for the path.

The cost is that only certain transaction counts are allowed.  In the worst
case, 12.5% of transactions would have to be left in the memory pool.  This
means around 7% of transactions would be delayed until the next block.

Blank transactions (or just transactions with low latency requirements)
could be used to increase the count so that it is raised to one of the
valid numbers.

Managing the UTXO set to ensure that there is at least one output that pays
to OP_TRUE is also a hassle.
-------------------------------------
Thank you for your feedback. I have written the Abstract and Motivation.

If my English is poor please let me know. Also let me know any other
comments or criticism you may have.

Thank you,
Jona

2015-02-21 22:34 GMT+09:00 Pavol Rusnak <stick@gk2.sk>:




-- 
-----BEGIN PGP PUBLIC KEY BLOCK-----
Comment: http://openpgpjs.org

xsBNBFTmJ8oBB/9rd+7XLxZG/x/KnhkVK2WBG8ySx91fs+qQfHIK1JrakSV3
x6x0cK3XLClASLLDomm7Od3Q/fMFzdwCEqj6z60T8wgKxsjWYSGL3mq8ucdv
iBjC3wGauk5dQKtT7tkCFyQQbX/uMsBM4ccGBICoDmIJlwJIj7fAZVqGxGOM
bO1RhYb4dbQA2qxYP7wSsHJ6/ZNAXyEphOj6blUzdqO0exAbCOZWWF+E/1SC
EuKO4RmL7Imdep7uc2Qze1UpJCZx7ASHl2IZ4UD0G3Qr3pI6/jvNlaqCTa3U
3/YeJwEubFsd0AVy0zs809RcKKgX3W1q+hVDTeWinem9RiOG/vT+Eec/ABEB
AAHNI2tpbm9zaGl0YSA8a2lub3NoaXRham9uYUBnbWFpbC5jb20+wsByBBAB
CAAmBQJU5ifRBgsJCAcDAgkQRB9iZ30dlisEFQgCCgMWAgECGwMCHgEAAC6Z
B/9otobf0ASHYdlUBeIPXdDopyjQhR2RiZGYaS0VZ5zzHYLDDMW6ZIYm5CjO
Fc09ETLGKFxH2RcCOK2dzwz+KRU4xqOrt/l5gyd50cFE1nOhUN9+/XaPgrou
WhyT9xLeGit7Xqhht93z2+VanTtJAG6lWbAZLIZAMGMuLX6sJDCO0GiO5zxa
02Q2D3kh5GL57A5+oVOna12JBRaIA5eBGKVCp3KToT/z48pxBe3WAmLo0zXr
hEgTSzssfb2zTwtB3Ogoedj+cU2bHJvJ8upS/jMr3TcdguySmxJlGpocVC/e
qxq12Njv+LiETOrD8atGmXCnA+nFNljBkz+l6ADl93jHzsBNBFTmJ9EBCACu
Qq9ZnP+aLU/Rt6clAfiHfTFBsJvLKsdIKeE6qHzsU1E7A7bGQKTtLEnhCCQE
W+OQP+sgbOWowIdH9PpwLJ3Op+NhvLlMxRvbT36LwCmBL0yD7bMqxxmmVj8n
vlMMRSe4wDSIG19Oy7701imnHZPm/pnPlneg/Meu/UffpcDWYBbAFX8nrXPY
vkVULcI/qTcCxW/+S9fwoXjQhWHaiJJ6y3cYOSitN31W9zgcMvLwLX3JgDxE
flkwq/M+ZkfCYnS3GAPEt8GkVKy2eHtCJuNkGFlCAmKMX0yWzHRAkqOMN5KP
LFbkKY2GQl13ztWp82QYJZpj5af6dmyUosurn6AZABEBAAHCwF8EGAEIABMF
AlTmJ9QJEEQfYmd9HZYrAhsMAABKbgf/Ulu5JAk4fXgH0DtkMmdkFiKEFdkW
0Wkw7Vhd5eZ4NzeP9kOkD01OGweT9hqzwhfT2CNXCGxh4UnvEM1ZMFypIKdq
0XpLLJMrDOQO021UjAa56vHZPAVmAM01z5VzHJ7ekjgwrgMLmVkm0jWKEKaO
n/MW7CyphG7QcZ6cJX2f6uJcekBlZRw9TNYRnojMjkutlOVhYJ3J78nc/k0p
kcgV63GB6D7wHRF4TVe4xIBqKpbBhhN+ISwFN1z+gx3lfyRMSmiTSrGdKEQe
XSIQKG8XZQZUDhLNkqPS+7EMV1g7+lOfT4GhLL68dUXDa1e9YxGH6zkpVECw
Spe3vsHZr6CqFg==
=/vUJ
-----END PGP PUBLIC KEY BLOCK-----
-------------------------------------
Below are 2 examples why a systematic risk analysis needs to be used. 
The current situation is that you have developers making hyperbolic, 
demonizing statements that users are "spammers" and engaged in Sybil 
"attacks."  Characterizing these activities as spam and Sybil attacks is 
not a systematic analysis, it is closer to the process used at the Salem 
Witch trials.

If this process of demonetization is to take its natural course then 
these statements are "developer attacks" from a developer system that 
lacks proper incentives and is rife with conflicts of interest.

Russ



...




-------------------------------------
I agree with you, Sergio, up until the part about someone having won a battle. There's a difference between sincere technical objections and someone just being a dick. I think in this case this line has been crossed (and I don't think I'm alone here).

- Eric

On October 5, 2015 8:56:33 AM PDT, Sergio Demian Lerner via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------------------------------
On Tue, Nov 3, 2015 at 9:49 PM Luke Dashjr <luke@dashjr.org> wrote:


Ok, so assuming we can get a connected component of upgraded nodes that
relay both the transaction and the associated external scripts then we
could just piggyback the external scripts on top of the normal messages.
Non-upgraded nodes will read the entire two-part message but only parse the
classical transaction, dropping the external script. Validation rules for
upgraded nodes are the same as before: if the attached signatures are
invalid the entire TX is dropped. We have to commit to the external scripts
used during the creation of a block. I think the easiest way to add this
commitment is the coinbase input I guess, and following the transaction
list a new list of signature lists is shipped with the rest of the block.
Non-upgraded will ignore it as before.

Would that work? It all hinges on having upgraded miners in a connected
component otherwise non-upgraded nodes will drop the external scripts on
the way (since they parse and then reconstruct the messages along the
path). But if it works this could be a much nicer solution.



So this is indeed a form of desired malleability we will likely not be able
to fix. I'd argue that this goes more into the direction of double-spending
than a form of malleability, and is mostly out of scope for this BIP. As
the abstract mentions this BIP attempts to eliminate damage incurred by
malleability in the third party modification scenario and in the multisig
scenario, with the added benefit of enabling transaction templating. If we
can get the segregated witnesses approach working all the better, we don't
even have the penalty of increased UTXO size. The problem of singlesig
users doublespending their outputs to update transactions remains a problem
even then.



Sounds very interesting. That would then be a new signature checking opcode
I guess that would allow the transaction hash in the input be replaced by
the hash of the serialized output it is spending? That way the transaction
would not be detached from the coins unless the amount or the scriptpubkey
(containing the address) is modified. So a user may add new outputs and
inputs to an existing transaction like you mentioned. This does not help
someone receiving funds from a sender to build new transactions on top
since the sender may simply doublespend its output before it is confirmed.
I think this is probably best addressed in a separate proposal.


-------------------------------------
I propose to implement dynamic block size limit. Its short summary is here
in doc:

https://docs.google.com/document/d/1ixt0loN7LOF6M_2HXvV0D-3ZCayvcfj0rzVm-h-6ONg/edit

Comments are allowed
--
Maksim Bozhko
-------------------------------------
I agree that naive scaling will likely lead to bad outcomes. They might
have the advantage though, as this would mean not changing Bitcoin.

Level2 and Lightning is not well defined. If you move money to a third
party, even if it is within the constrained of a locked contract, then I
don't think that will solve the issues. Blockchain does not know about
offchain and moving between offchain and onchain requires liquidity and a
pricing mechanism. That is exactly the problem with side-chains. If you
have off-chain transactions on an exchange, they are ID'ed in their system,
subject to KYC/AML.
-------------------------------------
On Mon, Aug 10, 2015 at 2:53 PM, Mike Hearn <hearn@vinumeris.com> wrote:

Well, yes, I guess it's modifying that in the extension BIP.


The point is not having exceptions and treating all supported chains
in the same way in the code.
Having a special case for regtest makes the code more complex, not simpler.

-------------------------------------
A coalescing transaction in my scheme is the same size as a normal
transaction. You only include one UTXO, the rest are implied based on
the presence of the OP_CHECKWILDCARDSIGVERIFY opcode.

The code that determines if a UTXO is spent or not will need to be
modified to include a check to see if any matching coalescing
transactions exist in any later block. Maybe there should be a
"coalescing pool" containing all coalescing transactions that make
such a check faster.

The part I'm not too sure about is the "wildcard signature". I'm not
too versed in cryptography to know how exactly to pull this off, but I
think it should be simple.
You'd just have to some way inject a flag into the signing process
that can be verified later.

I originally wanted the "wildcardness" of the transaction expressed by
the transaction version number.
Basically any input that exists within a "version 2 transaction" is
viewed as a wildcard input. Then I realized whats to stop someone from
modifying the transaction from version 1 to version 2 and stealing
someones funds. The "wildcardness" must be expressed in the signature
so you know that the private key holder intended all inputs to be
included. Hence the need for a new opcode.

btw, this scheme is definitely in the 10x or higher gain. You could
potentially spend an unlimited number of UTXOs this way.

On 11/24/15, Gavin Andresen <gavinandresen@gmail.com> wrote:

-------------------------------------
On Thu, Aug 6, 2015 at 3:40 PM, Gavin Andresen <gavinandresen@gmail.com> wrote:

I'm pretty sure that I can quote Mike Hearn with a sentence extremely
similar to that in this forum or in some of his blog posts (not in
https://medium.com/@octskyward/crash-landing-f5cc19908e32 at a first
glance...).
But yeah, what people said in the past is not very important: people
change their minds (they even acknowledge their mistake some times).
What interests me more it's what people think now.

I don't want to put words in your mouth and you are more than welcome
to correct what I think you think with what you really think.
All I'm trying to do is framing your fears properly.
If I say "all fears related to not raising the block size limit in the
short term can be summarized as a fear of fees rising in the short
term".
Am I correct? Am I missing some other argument?
Of course, problems that need to be solved regardless of the block
size (like an unbounded mempool) should not be considered for this
discussion.


If you pay high enough fees your transactions will be likely mined in
the next block.
So this seems to be reducible to the "fees rising" concern unless I am
missing something.


I think I would have a much better understanding of what "the other
side" thinks if I ever got an answer to a couple of very simple
questions I have been repeating ad nausea:

1) If "not now" when will it be a good time to let fees rise above zero?

2) When will you consider a size to be too dangerous for centralization?
In other words, why 20 GB would have been safe but 21 GB wouldn't have
been (or the respective maximums and respective +1 for each block
increase proposal)?

On Thu, Aug 6, 2015 at 4:21 PM, Gavin Andresen <gavinandresen@gmail.com> wrote:

3) Does this mean that you would be in favor of completely removing
the consensus rule that limits mining centralization by imposing an
artificial (like any other consensus rule) block size maximum?

I've been insistently repeating this question too.
Admittedly, it would be a great disappointment if your answer to this
question is "yes": that can only mean that either you don't understand
how the consensus rule limits mining centralization or that you don't
care about mining centralization at all.

If you really want things to move forward, please, prove it by
answering these questions so that we don't have to imagine what the
answers are (because what we imagine is probably much worse than your
actual answers).
I'm more than willing to stop trying to imagine what "big block
advocates" think, but I need your answers from the "big block
advocates".

Asking repeatedly doesn't seem to be effective. So I will answer the
questions myself in the worse possible way I think a "big block
advocate" could answer them.
Feel free to replace my stupid answers with your own:

----------------------- (FICTION ANSWERS [given the lack of real answers])

3) Does this mean that you would be in favor of completely removing
the consensus rule that limits mining centralization by imposing an
artificial (like any other consensus rule) block size maximum?

Yes, I would remove the rule because I don't care about mining centralization.

2) When will you consider a size to be too dangerous for centralization?
In other words, why 20 GB would have been safe but 21 GB wouldn't have
been (or the respective maximums and respective +1 for each block
increase proposal)?

Never, as said I don't care about mining centralization.
I thought users and Bitcoin companies would agree with a 20 GB limit
hardfork with proper lobbying, but I certainly prefer 21 GB.

1) If "not now" when will it be a good time to let fees rise above zero?

Never. Fees are just an excuse, the real goal is making Bitcoin centralized.

------------------------

I'm quite confident that you will have better answers than those.
Please, let me know what you think.

-------------------------------------



-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

I've seen a lot of talk on this list debating the role of Bitcoin Core and its maintainers WRT consensus, typically focused around whether they can technically force anyone to run their code (of course, they can't.)

I've yet to see the discussion framed in terms of influence and leadership. Which is why I want to highlight:


Perhaps s/helps guarantee/promotes/ , but this stands out as an excellent description of Bitcoin Core's relationship to the Bitcoin network.

Defaults are powerful. Users technically /can/ compile and run any code they like, but very few even bother to change configurable settings. They just want a trusted brand ("Bitcoin Core") that does the right thing out of the box. Bitcoin Core and its maintainers play a valuable /leadership/ role for the network. Whether they can force people to run their code is uninteresting -- people trust them.

That trust is well earned, precisely because they have always promoted the operation and security of the network.

In light of this responsibility it seems unreasonable for anyone to expect Core maintainers to promote patches that endanger network consensus (e.g. user configurable consensus parameters.)

Consensus is order of business #1. If we can't all agree to use the same money then the grand experiment is resolved as a failure. Everyone has consensus parameters they'd (strongly) prefer. Somebody needs to heard us all toward using the same ones, sometimes even in the face of very high costs.

- -Gareth

-----BEGIN PGP SIGNATURE-----
Version: APG v1.1.1

iQFABAEBCgAqBQJVsKPgIxxHYXJldGggV2lsbGlhbXMgPGdhY3J1eEBnbWFpbC5j
b20+AAoJEEY5w2E3jkVEZuEIAIKC9jTO33y4YC/cl1mO/+ux9YUqBlFUpuElKjNe
NLUIqPANrMV3nTjUm666Hk3tVHk8IpYLUU1pRuYBAT17d1t/2bFC4CpfpWssF9Nw
YhoYOKKVMvLUR4DRlkyhMD4YxorJ/TGiuEaFD4K/1s5uKf1+7Vj/BTi+SP+AIAIW
gTbn2CA3T4n8WjDYADE0dqcYSqzt2M1fjXB+Ld95JGLun8m+6lDPhFy/o5aGhBk6
5j86SITT9UtyyA6oaV5NNNgumcNBievnVwjTxjaWm8CBJlJ5jNpW65PQGkoSnCgz
TpYt/wZHcdSqBeNHyno9XaEBSm99Ylk3i2Z1dGQwrSsZU0Q=
=0pac
-----END PGP SIGNATURE-----


-------------------------------------
Who would be performing a Sybil attack against themselves? We're talking about a LAN here. All the nodes would be under the control of the same entity. In that case, you actually want them all connecting solely to a central hub node on the LAN, and the hub node should connect to "diverse and unpredictable" other nodes on the Bitcoin network.


On Monday, 25 May 2015, at 9:46 pm, Kevin Greene wrote:


-------------------------------------
On 02/23/2015 01:49 AM, Andreas Schildbach wrote:

Hi Andreas,

DHKE will not improve the situation. Either we use a simple method to
transfer a session key or a complex method.


DHKE doesn't offer greater forward secrecy than private transfer of a
session key, in fact it's lesser.


I don't see that there is a dilemma. The current proposal has a
significant privacy problem that can be easily resolved, and the
resolution actually makes the implementation simpler.

e


-------------------------------------
On Friday, June 12, 2015 11:01:02 PM Vincent Truong wrote:

Just simplify this? Allow a miner to have their block counted as <max possible 
votes> votes for X provided not a single transaction they include votes 
against it. Then miners can explicitly forego the fees of opposing 
transactions without having to bloat blocks.

Luke


-------------------------------------
I'm a bit confused.  It's been a long time since I looked at protobuf
(and will have to dig into it soon), but I seem to recall it doesn't
have any of the determinism properties you guys just said.  It is
intended to allow you to skip details of the on-the-wire representations
and just send a bunch of named fields between systems.  I thought there
was no guarantee that two identical protobuf structures will get
serialized identically...?




On 01/19/2015 02:57 PM, Richard Brady wrote:

-------------------------------------
Looks like a solid description of what would happen.

I fail to see how this description wouldn't be applicable also to a
20MB-network in some time in the future, say ~3 years from now, if
Bitcoin keeps taking off.
If you agree that it will be harder in the future to change the block
limit again, and we switch to hardcoded 20MB, then aren't we just
going from an immediate relief to a future larger blockage?





-------------------------------------
On Wed, Aug 19, 2015 at 10:04 PM, Eric Lombrozo <elombrozo@gmail.com> wrote:

I think that effort is in progress (again, much slower that I would
like it to be) and it's called libconsensus.
Once we have libconsensus Bitcoin Core it's just another
implementation (even if it is the reference one) and it's not "the
specification of the consensus rules" which is a "privileged" position
that brings all sorts of misunderstandings and problems (the block
size debate is just one example).

-------------------------------------
I would also like to summarize my observation and thoughts after the 
Hong Kong workshop.

1. I'm so glad that I had this opportunity to meet so many smart 
developers who are dedicated to make Bitcoin better. Regular conference 
like this is very important for a young project, and it is particularly 
important for Bitcoin, with consensus as the core value. I hope such a 
conference could be conducted at least once in 2 years in Hong Kong, 
which is visa-friendly for most people in both East and West.

2. I think some consensus has emerged at/after the conference. There is 
no doubt that segregated witness will be implemented. For block size, I 
believe 2MB as the first step is accepted by the super majority of 
miners, and is generally acceptable / tolerable for devs.

3. Chinese miners are requesting consensus among devs nicely, instead of 
using their majority hashing power to threaten the community. However, 
if I were allowed to speak for them, I think 2MB is what they really 
want, and they believe it is for the best interest of themselves and the 
whole community

4. In the miners round table on the second day, one of the devs 
mentioned that he didn't want to be seen as the decision maker of 
Bitcoin. On the other hand, Chinese miners repeatedly mentioned that 
they want several concrete proposals from devs which they could choose. 
I see no contradiction between these 2 viewpoints.

Below are some of my personal views:

5. Are we going to have a "Fee Event" / "Economic Change Event" in 2-6 
months as Jeff mentioned? Frankly speaking I don't know. As the fee 
starts to increase, spammers will first get squeezed --- which could be 
a good thing. However, I have no idea how many txs on the blockchain are 
spam. We also need to consider the effect of halving in July, which may 
lead to speculation bubble and huge legitimate tx volume.

6. I believe we should avoid a radical "Economic Change Event" at least 
in the next halving cycle, as Bitcoin was designed to bootstrap the 
adoption by high mining reward in the beginning. For this reason, I 
support an early and conservative increase, such as BIP102 or 2-4-8. 2MB 
is accepted by most people and it's better than nothing for BIP101 
proponents. By "early" I mean to be effective by May, at least 2 months 
before the halving.

7. Segregated witness must be done. However, it can't replace a 
short-term block size hardfork for the following reasons:
(a) SW softfork does not allow higher volume if users are not upgrading. 
In order to bootstrap the new tx type, we may need the help of 
altruistic miners to provide a fee discount for SW tx.
(b) In terms of block space saving, SW softfork is most efficient for 
multisig tx, which is still very uncommon
(c) My most optimistic guess is SW will be ready in 6 months, which will 
be very close to halving and potential tx volume burst. And it may not 
be done in 2016, as it does not only involve consensus code, but also 
change in the p2p protocol and wallet design

8. Duplex payment channel / Lightning Network may be viable solutions. 
However, they won't be fully functional until SW is done so they are 
irrelevant in this discussion

9. No matter what is going to be done / not done, I believe we should 
now have a clear road map and schedule for the community: a short-term 
hardfork or not? The timeline of SW? It is bad to leave everything 
uncertain and people can't well prepared for any potential radical 
changes

10. Finally, I hope this discussion remains educated and evidence-based, 
and no circling

-------------------------------------
Hi All,

Here are some final results of testing with the reference implementation
for compressing blocks and transactions. This implementation also
concatenates blocks and transactions when possible so you'll see data
sizes in the 1-2MB ranges.

Results below show the time it takes to sync the first part of the
blockchain, comparing Zlib to the LZOx library.  (LZOf was also tried
but wasn't found to be as good as LZOx).  The following shows tests run
with and without latency.  With latency on the network, all compression
libraries performed much better than without compression.

I don't think it's entirely obvious which is better, Zlib or LZO. 
Although I prefer the higher compression of Zlib, overall I would have
to give the edge to LZO.  With LZO we have the fastest most scalable
option when at the lowest compression setting which will be a boost in
performance for users that want peformance over compression, and then at
the high end LZO provides decent compression which approaches Zlib,
(although at a higher cost) but good for those that want to save more
bandwidth.

Uncompressed 60ms 	Zlib-1 (60ms) 	Zlib-6 (60ms) 	LZOx-1 (60ms) 	LZOx-999
(60ms)
219 	299 	296 	294 	291
432 	568 	565 	558 	548
652 	835 	836 	819 	811
866 	1106 	1107 	1081 	1071
1082 	1372 	1381 	1341 	1333
1309 	1644 	1654 	1605 	1600
1535 	1917 	1936 	1873 	1875
1762 	2191 	2210 	2141 	2141
1992 	2463 	2486 	2411 	2411
2257 	2748 	2780 	2694 	2697
2627 	3034 	3076 	2970 	2983
3226 	3416 	3397 	3266 	3302
4010 	3983 	3773 	3625 	3703
4914 	4503 	4292 	4127 	4287
5806 	4928 	4719 	4529 	4821
6674 	5249 	5164 	4840 	5314
7563 	5603 	5669 	5289 	6002
8477 	6054 	6268 	5858 	6638
9843 	7085 	7278 	6868 	7679
11338 	8215 	8433 	8044 	8795



These results from testing on a highspeed wireless LAN (very small latency)

Results in seconds 	
	
	
	
	
Num blocks sync'd 	Uncompressed 	Zlib-1 	Zlib-6 	LZOx-1 	LZOx-999
10000 	255 	232 	233 	231 	257
20000 	464 	414 	420 	407 	453
30000 	677 	594 	611 	585 	650
40000 	887 	782 	795 	760 	849
50000 	1099 	961 	977 	933 	1048
60000 	1310 	1145 	1167 	1110 	1259
70000 	1512 	1330 	1362 	1291 	1470
80000 	1714 	1519 	1552 	1469 	1679
90000 	1917 	1707 	1747 	1650 	1882
100000 	2122 	1905 	1950 	1843 	2111
110000 	2333 	2107 	2151 	2038 	2329
120000 	2560 	2333 	2376 	2256 	2580
130000 	2835 	2656 	2679 	2558 	2921
140000 	3274 	3259 	3161 	3051 	3466
150000 	3662 	3793 	3547 	3440 	3919
160000 	4040 	4172 	3937 	3767 	4416
170000 	4425 	4625 	4379 	4215 	4958
180000 	4860 	5149 	4895 	4781 	5560
190000 	5855 	6160 	5898 	5805 	6557
200000 	7004 	7234 	7051 	6983 	7770



The following show the compression ratio acheived for various sizes of
data.  Zlib is the clear
winner for compressibility, with LZOx-999 coming close but at a cost.

range 	Zlib-1 cmp%
	Zlib-6 cmp% 	LZOx-1 cmp% 	LZOx-999 cmp%
0-250b 	12.44 	12.86 	10.79 	14.34
250-500b  	19.33 	12.97 	10.34 	11.11
600-700 	16.72 	n/a 	12.91 	17.25
700-800 	6.37 	7.65 	4.83 	8.07
900-1KB 	6.54 	6.95 	5.64 	7.9
1KB-10KB 	25.08 	25.65 	21.21 	22.65
10KB-100KB 	19.77 	21.57 	14.37 	19.02
100KB-200KB 	21.49 	23.56 	15.37 	21.55
200KB-300KB 	23.66 	24.18 	16.91 	22.76
300KB-400KB 	23.4 	23.7 	16.5 	21.38
400KB-500KB 	24.6 	24.85 	17.56 	22.43
500KB-600KB 	25.51 	26.55 	18.51 	23.4
600KB-700KB 	27.25 	28.41 	19.91 	25.46
700KB-800KB 	27.58 	29.18 	20.26 	27.17
800KB-900KB 	27 	29.11 	20 	27.4
900KB-1MB 	28.19 	29.38 	21.15 	26.43
1MB -2MB 	27.41 	29.46 	21.33 	27.73


The following shows the time in seconds to compress data of various
sizes.  LZO1x is the
fastest and as file sizes increase, LZO1x time hardly increases at all. 
It's interesing
to note as compression ratios increase LZOx-999 performs much worse than
Zlib.  So LZO is faster
on the low end and slower (5 to 6 times slower) on the high end.

range 	Zlib-1 	Zlib-6 	LZOx-1 	LZOx-999 cmp%
0-250b    	0.001 	0 	0 	0
250-500b   	0 	0 	0 	0.001
500-1KB     	0 	0 	0 	0.001
1KB-10KB    	0.001 	0.001 	0 	0.002
10KB-100KB   	0.004 	0.006 	0.001 	0.017
100KB-200KB  	0.012 	0.017 	0.002 	0.054
200KB-300KB  	0.018 	0.024 	0.003 	0.087
300KB-400KB  	0.022 	0.03 	0.003 	0.121
400KB-500KB  	0.027 	0.037 	0.004 	0.151
500KB-600KB  	0.031 	0.044 	0.004 	0.184
600KB-700KB  	0.035 	0.051 	0.006 	0.211
700KB-800KB  	0.039 	0.057 	0.006 	0.243
800KB-900KB  	0.045 	0.064 	0.006 	0.27
900KB-1MB   	0.049 	0.072 	0.006 	0.307


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey Angel,

On 08/11/2015 02:14 AM, Angel Leon via bitcoin-dev wrote:

This seems to be a really good idea... May I add in here something
that's been dismissed before but I will mention it again anyway...

http://is.gd/DiFuRr "dynamic block size adjustment"
My sense has been that something like this could be coupled with
Garzik's BIP 100.  For some reason I keep getting attacked for saying
this.

/RantOff


- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJVypQUAAoJEGxwq/inSG8CZ3wH/1BsvmusOaCHnQMSrRAhUSTy
owzhWC/7AWUuav7MSr8upLK29oGnL+R4+PqmyuXmSBQDjwJmtlS11GXvEP3/BqzA
Qxz6EkEw/BRPe1YgvsS9LJNe90aIsiJRgeGls3XIirdzE0b6PSFn4GB0hn0XLU5Y
rNA6WlWHlZ5c/J26sxbEUdWzQZd7cTybJgmUVwqjqaviBpvg5iGIHhN/dHglqRvm
9/IscT3MNneXqD5QYwNvYUG2yjgRLgggnIufU37r8Rt+S6xJ0TK++R5hhKS5fF61
7xBQGDuczI+/mCGMc88JZxEIAV/H63SfIrD3bjqcfDnGXDl9SmrfdiIwyy3Yitc=
=zbGE
-----END PGP SIGNATURE-----

-------------------------------------
Once again my attempt to summarize and explain the weekly bitcoin developer
meeting in layman's terms.
Link to last weeks layman's summarization:
https://www.mail-archive.com/bitcoin-dev@lists.linuxfoundation.org/msg02445.html



*Disclaimer*

Please bare in mind I'm not a developer and I'd have problems coding "hello
world!", so some things might be incorrect or plain wrong.
Like any other write-up it likely contains personal biases, although I try
to stay as neutral as I can.
There are no decisions being made in these meetings, so if I say "everyone
agrees" this means everyone present in the meeting, that's not consensus,
but since a fair amount of devs are present it's a good representation.
The dev IRC and mailinglist are for bitcoin development purposes. If you
have not contributed actual code to a bitcoin-implementation, this is
probably not the place you want to reach out to. There are many places to
discuss things that the developers read, including this sub-reddit.


link to this week logs (
http://bitcoinstats.com/irc/bitcoin-dev/logs/2015/10/08#l1444330778.0 )
link to meeting minutes (
https://docs.google.com/document/d/1hCDuOBNpqrZ0NLzvgrs2kDIF3g97sOv-FyneHjQellk/edit
)


Main topics discussed this week where:

Mempool limiting: chain limits
Low-S change
CLTV & CSV review
Creation of bitcoin discuss mailing list


**off-topic but important notice**

This issue ( https://github.com/feross/buffer/pull/81 ) has made most JS
bitcoin software vulnerable to generating incorrect public keys.
"This is an ecosystem threat with the potential to cause millions of
dollars in losses that needs higher visibility; though it's not a bitcoin
core / bitcoin network issue.
Common, critical, JS code is broken that may cause the generation of
incorrect pubkeys (among other issues). Anyone who cares for a JS
implementation should read that PR."


**Mempool limiting: chain limits**

- background

(c/p from last week)
Chain in this context means connected transactions. When you send a
transaction that depends on another transaction that has yet to be
confirmed we talk about a chain of transactions.
Miners ideally take the whole chain into account instead of just every
single transaction (although that's not widely implemented afaik). So while
a single transaction might not have a sufficient fee, a depending
transaction could have a high enough fee to make it worthwhile to mine both.
This is commonly known as child-pays-for-parent.
Since you can make these chains very big it's possible to clog up the
mempool this way.
The first unconfirmed transaction is called the ancestor and the
transactions depending on it the descendants. The total amount of
transactions is reffered to as "packages".

- since last week

As said in "Chain limits" last week Morcos did write a proposal about
lowering the default limits for transaction-chains.
2 use cases came up which are currently in use or happened before:
As example: someone buys bitcoin from a website and can spend those bitcoin
in the marketplace of the same website without waiting for confirmation in
order to improve the bitcoin user-experience. This leaves a sequential
transaction chain. They don't need to chain more than 5 transactions deep
for this, and it falls within the proposed limits.
What's not within the proposed limits is the chain of +/- 100 transactions
a company had during the spam-attacks. These where simply increased
activities by end-users while not enough UTXO's where available (3 to be
precise)(UTXO: unspent transaction output, an output that can be used as
input for a new transaction).
Notably this is with the best practices of using confirmed transactions
first.
Ways this can be solved from the company's end is to have more UTXO's
available before hand, bundling transactions (which requires delaying
customer's request) or using replace-by-fee to add payees (which saves
blockchain space, is cheaper in fees and gets transactions through quicker,
but is not widely deployed by miners atm).
Bare in mind these proposals are for default values for the memorypool, not
in any way hard limits.


- meeting comments

Sense of urgency. Quoting sipa: "my mempool is 2.5G... we better get some
solution!"
Current attack analysis assumes child-pays-for-parent mining, it should
probably be done again without.
Higher limits on number of transactions increase attack-vectors.
Proposed number of transactions gets some push-back, total size limit not.
Mixing default values (for example having a 50% of a 10/10 limit and 50% of
a 100/100 limit) wastes bandwidth while there are too many factors that
limit utility of long chains as well.
25 transaction limit ought to be enough for everyone (for now).

- meeting conclusion

Review & test "Limit mempool by throwing away the cheapest txn and setting
min relay fee to it" ( https://github.com/bitcoin/bitcoin/pull/6722 )
Provide support for "Lower default limits for tx chains" (
https://github.com/bitcoin/bitcoin/pull/6771 ) aka convince people 25
should be enough.



**Low-S change**

- background

This is in regards to the recent malleability attack. Which is caused by a
value 'S' in the ECDSA signature which can be 2 values, a high and low
value and still be valid. Resulting in different transaction id's. more
info:
http://blog.coinkite.com/post/130318407326/ongoing-bitcoin-malleability-attack-low-s-high
A solution for this is to require nodes to have the "low-s" encoding for
signatures.
Downside is that it will block most transactions made by sufficiently out
of date software (+/- pre-march 2014)
This does not replace the need for BIP62, it only eliminates the cheap DOS
attack.


- meeting comments

95% of transactions already confirm to this, and more fixes have been
applied since.
BlueMatt has a node which several people are running that auto-malleates to
low-s transactions.
Questions whether we release it ASAP or wait for the next release and get
it to a couple of miners in the meantime (possibly with
auto-lowS-malleating)


- meeting conclusion

Contact miners about "Test LowS in standardness, removes nuisance
malleability vector" ( https://github.com/bitcoin/bitcoin/pull/6769 )
Release scheduled for the end of the month, together with likely
check-lock-time-verify and possibly check-sequence-verfiy.



**CLTV & CSV backport review**

- background

CLTV: checkLockTimeVerify
CSV: checkSequenceVerify
Both new time-related OP-codes.
Been discussed heavily last week.


- meeting comments

CSV doesn't seem ready enough for release later this month.
There's no clarity on how things look when all 3 time related pull-requests
are merged.
There's a number of people still reviewing the pull-requests.
Uncertainty and confusion about whether the semantics are final or not (in
regards to using bits from nSequence). nSequence are 4 bytes intended for
sequencing time-locked transactions, but this never got used.
Now these bytes are being repurposed for a mixture of things. Currently the
plan is: " bits 0..15 are the relative locktime, bit 30 determines units
(0: height, 1: time w/ 512s granularity), and bit 31 toggles BIP 68 (0: on,
1: off). bits 16..29 are masked off and can take any value."

- meeting conclusion

Clarification from maaku regarding nSequence for BIP68. (after the meeting
he explained he was waiting for opinions, but not enough people seemed to
know the issue at hand)
Continue review of pull requests 6312 (
https://github.com/bitcoin/bitcoin/pull/6312 ), 6564 (
https://github.com/bitcoin/bitcoin/pull/6564 ) and 6566 (
https://github.com/bitcoin/bitcoin/pull/6566 )


**Creation of bitcoin discuss mailing list**

- background

The bitcoin-dev mailing list is intented for technical discussions only.
There's things that don't belong there but need to be discussed anyway.
Now this is done in bitcoin-dev, but the volume of this is getting too big.

There's recently also an influx of really inappropriate posts, level
kindergarden (
https://www.mail-archive.com/bitcoin-dev@lists.linuxfoundation.org/msg02539.html
).


- meeting comments

No clarity about who are the moderators.
Next week there'll be a bitcoin-discuss list created.
Decisions are needed as to who'll become the moderators for that and
bitcoin-dev.
Decisions are needed as to what will be the list and moderation policies.


- meeting conclusion

The bitcoin-discuss list will be created as well as a simple website
listing all the lists and corresponding policies.
A meeting is scheduled on monday to discuss the moderation and policies of
said lists.


**Participants**

morcos           Alex Morcos
gmaxwell         Gregory Maxwell
wumpus           Wladimir J. van der Laan
sipa             Pieter Wuille
BlueMatt         Matt Corallo
btcdrak          btcdrak
petertodd        Peter Todd
warren           Warren Togami
phantomcircuit   Patrick Strateman
dstadulis        Daniel Stadulis
GreenIsMyPepper  ?? Jospeh Poon ??
bsm117532        Bob McElrath
-------------------------------------
You just proved his point once again with yet another ad hominem :)

Good job.

On 6/25/2015 at 10:56 AM, "cipher anthem"  wrote:+1 on this!
 I have come across Milly a couple of times on reddit and disqus and
she basically dismisses anyone who doesn't agree with her opinions.
always labeling them "cultish". Please ignore her so you can stay
productive.    
-------------------------------------
Den 13 mar 2015 20:57 skrev "Kalle Rosenbaum" <kalle@rosenbaum.se>:
payment. I came up with an idea I call Proof of Payment (PoP) and I would
highly appreciate your comments. Has something like this been discussed
somewhere before?
have paid for something. For example:
any device.
this period you can upload new content to the sign whenever you like using
PoP.
of the T-shirt is selected among the transactions to that address. You
exchange the T-shirt for a PoP for the winning transaction.
accounts, no e-mails, etc) being involved.
type (P2SH, P2PKH, etc.).

Relevant: https://idemix.wordpress.com/

Anonymous Credentials allows an issuer to declare that you have certain
rights. For example, upon paying the service provider could issue you the
credentials for using their service up until a certain date.

When challenged to prove a statement about what credentials you have, you
can prove the fact that you've got the right credentials without revealing
anything else. You don't even reveal you're the same person as the last
time, if you prove the right to access a VPN multiple times there's no data
in it that links the different sessions together.

The main difference is that issuance of Anonymous Credentials aren't
"atomic" with the payment transactions, which can open up the risk for
certain types of dishonest behavior by the seller. You could however use a
proof in court of having paid for the credentials but not getting them
issued to you (maybe throw in usage of Factom to log issuance of
credentials?). With this construction of using both these methods, you add
stronger privacy for the usage of the services while simultaneously keeping
a degree of accountability for the payment.

The Zerocoin developers also got a paper on a blockchain version,
"Distributed Anonymous Credentials".
-------------------------------------
On Thu, May 7, 2015 at 11:25 AM, Mike Hearn <mike@plan99.net> wrote:

Can you please elaborate on what terrible things will happen if we
don't increase the block size by winter this year?
I assume that you are expecting full blocks by then, have you used any
statistical technique to come up with that date or is it just your
guess?
Because I love wild guesses and mine is that full 1 MB blocks will not
happen until June 2017.


We've successfully reached consensus for several softfork proposals already.
I agree with others that hardfork need to be uncontroversial and there
should be consensus about them.
If you have other ideas for the criteria for hardfork deployment all I'm ears.
I just hope that by  "What we need to see right now is leadership" you
don't mean something like "when Gaving and Mike agree it's enough to
deploy a hardfork" when you go from vague to concrete.



Oh, so your answer to "bitcoin will eventually need to live on fees
and we would like to know more about how it will look like then" it's
"no bitcoin long term it's broken long term but that's far away in the
future so let's just worry about the present".
I agree that it's hard to predict that future, but having some
competition for block space would actually help us get more data on a
similar situation to be able to predict that future better.
What you want to avoid at all cost (the block size actually being
used), I see as the best opportunity we have to look into the future.


Free transactions are a gift from miners that run an altruistic policy.
That's great but we shouldn't rely on them for the future. They will
likely disappear at some point and that's ok.
In any case, he's not complaining about the lack of free transactions,
more like the opposite.
He is saying that's very easy to get free transactions in the next
block and blocks aren't full so there's no incentive to include fees
to compete for the space.
We can talk a lot about "a fee market" and build a theoretically
perfect fee estimator but we won't actually have a fee market until
there's some competition for space.
Nobody will pay for space that's abundant just like people don't pay
for the air they breath.


Ok, this is my plan: we wait 12 months, hope that your estimations are
correct (in case that my guess was better than yours, we keep waiting
until June 2017) and start having full blocks and people having to
wait 2 blocks for their transactions to be confirmed some times.
That would be the beginning of a true "fee market", something that
Gavin used to say was his #1 priority not so long ago (which seems
contradictory with his current efforts to avoid that from happening).
Having a true fee market seems clearly an advantage.
What are supposedly disastrous negative parts of this plan that make
an alternative plan (ie: increasing the block size) so necessary and
obvious.
I think the advocates of the size increase are failing to explain the
disadvantages of maintaining the current size. It feels like the
explanation are missing because it should be somehow obvious how the
sky will burn if we don't increase the block size soon.
But, well, it is not obvious to me, so please elaborate on why having
a fee market (instead of just an price estimator for a market that
doesn't even really exist) would be a disaster.


-------------------------------------
On Sun, Sep 27, 2015 at 02:50:31PM -0400, Peter Todd via bitcoin-dev wrote:


There appears to be common agreement on that.

The only source of some controversy is how to deploy: versionbits versus
IsSuperMajority. I think the versionbits proposal should first have code
out there for longer before we consider it for concrete softforks. Haste-ing
along versionbits because CLTV is wanted would be risky.



As you say, the underlying code has been merged for months in master, and #6351
seems to have had quite some eyes on it already.

It does need to be made sure that the backports are correct, however.
Although the tests do provide some assurance, I think those two pulls
require more review.

After they are merged, a 0.10.3 and 0.11.1 release can be rolled out (with RC
cycle).


Wladimir


-------------------------------------
Looks like a neat solution, Tier.
-------------------------------------
On Sat, Jan 10, 2015 at 12:18 PM, Ivan Jelincic <parazyd@archlinux.info> wrote:

Yes. It concerns CVE-2014-8275.

Which in https://www.openssl.org/news/openssl-1.0.1-notes.html is under:

Major changes between OpenSSL 1.0.1j and OpenSSL 1.0.1k [8 Jan 2015]

Wladimir


-------------------------------------
You're only strengthening Gigas' point about the mailing list by posting
derisive emails. Take your nonconstructive comments elsewhere.

- Jameson

On Fri, Jun 19, 2015 at 4:01 PM, Brian Hoffman <brianchoffman@gmail.com>
wrote:

-------------------------------------

You seem to be analysing a different attack - I mean that if someone
has enough hashrate to do a selfish mining attack, then setting up a
system that has no means to reduce block-size risks that at a point
where there is excess block-size they can use that free transaction
space to amplify selfish mining instead of collecting transaction
fees.

Adam

-------------------------------------
I find it to be an admirable goal to try to keep node operation costs low
and accessible to the average user. On the other hand, if we are able to
keep the resource requirements of nodes at the level of, say, whatever the
latest Raspberry Pi model on a residential Internet connection can handle,
I'm not sure how helpful it will be if the demand for inclusion in blocks
results in transaction fees prices out more users. Stated differently, if
the cost or contention of using the network rises to the point of excluding
the average user from making transactions, then they probably aren't going
to care that they can run a node at trivial cost.

If we're approaching the block size from a resource usage standpoint, it
seems to me that someone is going to be excluded one way or another. Not
raising the block size will exclude some users from sending transactions
while raising the block size will exclude some users from running nodes.
The latter seems preferable to me because more users will grow the
ecosystem, which should increase the value of the ecosystem, which should
increase the cost that entities are willing to pay to run nodes.

I see two primary points of view / objectives clashing in this debate:

1) Decentralization and stability even if it retards growth of the ecosystem
2) Push the system's load as far as we are comfortable in order to
accommodate the growth it is experiencing

It's clear to me that Core developers have a responsibility to maintain a
stable platform for the ecosystem. I think it's less clear that they have a
responsibility to grow it or ask node operators to expend more resources in
order to support more users. As an operator of several nodes, I can
anecdotally state that I find their resource usage to be trivial and I
welcome more load.

- Jameson

On Thu, Jul 30, 2015 at 11:12 AM, Jorge Timón <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Sat, May 30, 2015 at 3:32 PM, Matt Corallo <bitcoin-list@bluematt.me>
wrote:


"good behavior" models? I intentionally modeled what should be a worst-case.

If you have a specific network topology you want to model, please email me
details and I'll see what worst case is. Or, even better, take my
simulation code and run it yourself (it's C++, easy to compile, easy to
modify if you think it is too simple).

I get frustrated with all of the armchair "but what if..."
how-many-miners-can-dance-on-the-head-of-a-pin arguments.





No, they're not. They are only at a disadvantage when THEY mine bigger
blocks.

I guess I wasn't clear in the "do bigger miners have an advantage" blog
post.



I spent last week doing simulation and study. Please, do your own
simulation and study if you don't trust my results. There are big
full-scale-bitcoin-network-simulations spinning up that should have results
in a month or two, also, but there will ALWAYS be "but we didn't think
about what if THIS happens" scenarios that can require more simulation and
study.



Last night's transaction volume test shows that most miners do just go
along with defaults:
  http://bitcoincore.org/~gavin/sizes_358594.html



Mining is a competitive business, the marginal miner will ALWAYS be going
out of business.

That is completely independent of the block size, block subsidy, or
transaction fees.

The question is "will there be enough fee+subsidy revenue to make it
unprofitable for an attacker to buy or rent enough hashpower to
double-spend."

It is obvious to me that bigger blocks make it more likely the answer to
that question is "yes."




Mike Hearn wrote about that just a couple days ago:
  https://medium.com/@octskyward/hashing-7d04a887acc8
(See "How much is too much" section)



I have said repeatedly that if it was left completely up to me I would go
back to Satoshi's original "there is no consensus-level blocksize limit".

20MB is a compromise.


... and now you're pissing me off. I have NEVER EVER said that they need
bigger blocks to continue operating. Please stop being overly dramatic.

They believe that bigger blocks are better for Bitcoin.

Brian Armstrong at Coinbase, in particular, said that smaller blocks drive
centralization towards services like Coinbase ("look ma! No blockchain
transaction!" <-- if you pay a Coinbase merchant from your Coinbase
wallet), but he supports bigger blocks because more transactions on our
existing decentralized network is better.

-- 
--
Gavin Andresen
-------------------------------------
On 07/01/2015 03:54 AM, Jeffrey Paul wrote:

If that's the purpose of this list, then it is misleadingly named.

If development of Bitcoin Core, the application, is to be considered
independent from development of Bitcoin, the protocol, then Bitcoin Core
development needs its own list.

-------------------------------------
2015-06-06 17:32 GMT+02:00 Peter Todd <pete@petertodd.org>:

Actually, I suggested that on this list on april 27, but shortly after
rejected my own idea:

#######################
"Or a really high lock_time, but it would not make it invalid, just delayed."

Ok, this was a bad idea, since nodes would have to keep it in memory.
Please disregard that idea...
########################

Now I think I rejected it on based on a misunderstanding. Nodes will
not put them in their mempool unless it's value is near in time,
right? From the 0.9.0 release notes: "Accept nLockTime transactions
that finalize in the next block".

In that case this is a really nice option.



-------------------------------------
Rather than (promising to, and when they don't actually, at least
pretending to) use the first-seen block, I propose that a more
sophisticated method of choosing which of two block solutions to accept.
Essentially, a miner receiving two solutions at the same height would
compute a weighted sum of bitcoin-days-destroyed (transactions received
earlier get higher weights) of whatever transactions are in a block *and
also* were in the miner's mempool *before* the first solution arrived.
Whichever block has more wins.

This strategy avoids allowing miners to use private transactions to mess
with the blockchain.  It also makes an empty block far less attractive
because it is easily replaced, all the way until the next block locks it
in.  Any block-selection heuristic can be gamed, but I believe that using a
weighted sum of BTCDD is harder to game than using block propagation timing.

I asked Can Bitcoin Days Destroyed be a better resolution mechanism for
competing blocks?
<http://bitcoin.stackexchange.com/questions/39226/can-bitcoin-days-destroyed-be-a-better-resolution-mechanism-for-competing-blocks>
on the stackexchange bitcoin site in order to collect objections to and
problems with this idea, and have not found any that I haven't addressed.
The best objection is that *maybe* empty blocks and selfish mining are
either good for bitcoin, or else they are so minimally bad that no effort
ought to be expended in preventing them.

If anyone here thinks this is a good idea, and no one can offer reasons
it's a bad idea, I will probably start working on an implementation.  I'm
really slow though, so ping me if it looks like fun to you.

notplato
-------------------------------------
On Thu, Aug 27, 2015 at 11:08:32PM +0100, Btc Drak wrote:

On Thu, Aug 27, 2015 at 11:11:10PM +0100, Btc Drak via bitcoin-dev wrote:

I thought we had decided that the masking thing doesn't work as
intended?

To recap, XT nodes are producing blocks with nVersion=0b001...111

You're suggesting that we apply a mask of ~0b001...111 then trigger the
soft-fork on nVersion >= 0b0...100 == 4, with miners producing blocks with
nVersion=0b0...1000

That will work, but it still uses up a version bit. The reason why is
blocks with nVersion=0b001...000 - the intended deployment of the
nVersion bits proposal - will be rejected by the nVersion >= 4 rule,
hard-forking them off the network. In short, we have in fact "burnt" a
version bit unnecessarily.

If you're going to accept hard-forking some people off the network, why
not just go with my stateless nVersion bits w/ time-expiration proposal
instead? The only case where it leads to a hard-fork is if a soft-fork
has been rejected by the time the upgrade deadline is reached. It's easy
to set this multiple years into the future, so I think in practice it
won't be a major issue for non-controversial soft-forks.

Equally, spending the time to implement the original stateful nVersion
bits proposal is possible as well, though higher risk due to the extra
complexity of tracking soft-fork state.

-- 
'peter'[:-1]@petertodd.org
000000000000000008ba8215b2b644e33a98a762fd40710bc5e8c7f1b0e78375
-------------------------------------
Quick observation: block transmission would be compress-once, send-multiple-times, which makes the tradeoff a little better.

-------------------------------------
On 02/14/2015 05:13 AM, Peter Todd wrote:

done

https://github.com/libbitcoin/libbitcoin-consensus


You seriously made me laugh out loud with this one Peter.

e

-------------------------------------

On 6/27/2015 at 3:18 PM, "Greg Sanders" <gsanders87@gmail.com> wrote:


That requires an assumption that all developers are perfectly representing the whole community.

And no shady lobbying behind the scenes too.



-------------------------------------
On Sat, Jul 11, 2015 at 11:24:48AM +0200, Jorge Timn wrote:

You're missing something really critical about what F2Pool/AntPool were
(are?) doing: They're finding out about new blocks not by getting block
headers from just anywhere, but by connecting to other pools' via
stratum anonymously and determining what block hash they're telling the
hashers at the pool to work on. (e.g. what prevblockhash is in the block
header of shares being generated)

If other pools try to fake this information they're immediately and
directly losing money, because they're telling their own hashers to make
invalid blocks. This of course has a high chance of being detected, and
can easily be FUDed into "STOP MINING AT FOO POOL!" reardless of what
the ivory tower game theory might say. The only hope the pools have is
to somehow identify which connections correspond to other pools with
high reliability and target just those connections - good luck on that.


Anyway, all this concern about SPV mining is misguided: relying purely
on SPV w/ low #'s of confirmations just isn't very smart. What SPV can
do - at least while the inflation subsidy is still high - is give
reasonable protection against your third-party-run trusted full nodes
from lying to you, simply because doing so has well-defined costs in
terms of energy to create fake blocks. Targetting enough people at once
to make a fake block a worthwhile investment is difficult, particularly
when you take into account how timing works in the defenders favor - the
attacker probably only has a small % of hashing power, so they're going
to wait a long time to find their fake block. Between that and a trusted
third party-run full node you're probably reasonably safe, for now.

-- 
'peter'[:-1]@petertodd.org
0000000000000000086007e31decd6eb80e07f77271ef50c69e1e6342161f4e5
-------------------------------------
On Wed, Aug 19, 2015 at 11:27 PM, Joseph Poon <joseph@lightning.network> wrote:

That policy code should be simple to change, but thank you for pointing it out.
Also thank you for declaring your position (indifference) on the subject.

-------------------------------------
On Tue, Aug 11, 2015 at 9:37 PM, Michael Naber via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


The question is not what the technology can deliver. The question is what
price we're willing to pay for that. It is not a boolean "at this size,
things break, and below it, they work". A small constant factor increase
will unlikely break anything in the short term, but it will come with
higher centralization pressure of various forms. There is discussion about
whether these centralization pressures are significant, but citing that
it's artificially constrained under the limit is IMHO a misrepresentation.
It is constrained to aim for a certain balance between utility and risk,
and neither extreme is interesting, while possibly still "working".

Consensus rules are what keeps the system together. You can't simply switch
to new rules on your own, because the rest of the system will end up
ignoring you. These rules are there for a reason. You and I may agree about
whether the 21M limit is necessary, and disagree about whether we need a
block size limit, but we should be extremely careful with change. My
position as Bitcoin Core developer is that we should merge consensus
changes only when they are uncontroversial. Even when you believe a more
invasive change is worth it, others may disagree, and the risk from
disagreement is likely larger than the effect of a small block size
increase by itself: the risk that suddenly every transaction can be spent
twice (once on each side of the fork), the very thing that the block chain
was designed to prevent.

My personal opinion is that we should aim to do a block size increase for
the right reasons. I don't think fear of rising fees or unreliability
should be an issue: if fees are being paid, it means someone is willing to
pay them. If people are doing transactions despite being unreliable, there
must be a use for them. That may mean that some use cases don't fit
anymore, but that is already the case.

-- 
Pieter
-------------------------------------


I’m just late to the party I guess.  Thanks for the links.

-------------------------------------
On Tuesday, 26 May 2015, at 1:15 am, Peter Todd wrote:

The Porcupine Freedom Festival ("PorcFest") in New Hampshire last summer. I strongly suspect that it's the largest gathering of Bitcoin users at any event that is not specifically Bitcoin-themed. There's a lot of overlap between the Bitcoin and liberty communities. PorcFest draws somewhere around 1000-2000 attendees, a solid quarter of whom have Bitcoin wallets on their mobile devices.

The backhaul was a 3G cellular Internet connection, and the local Bitcoin node and network router were hosted on a Raspberry Pi with some Netfilter tricks to restrict connectivity. The net result was that all Bitcoin nodes (lightweight and heavyweight) on the local Wi-Fi network were unable to connect to any Bitcoin nodes except for the local node, which they discovered via DNS. I also had provisions in place to allow outbound connectivity to the API servers for Mycelium, Blockchain, and Coinbase wallets, by feeding the DNS resolver's results in real-time into a whitelisting Netfilter rule utilizing IP Sets.

For your amusement, here's the graphic for the banner that I had made to advertise the network at the festival (*chuckle*): http://www.mattwhitlock.com/bitcoin_wifi.png


-------------------------------------
Awesome graph!

It would be interesting if you could use your Excel voodoo (or whatever graph app you use) to add density as a heat map, making areas with more blocks red and regions with less blocks blue…



.·´¯`·.¸¸.·´¯`·.¸¸.·´¯`·.¸¸.·´¯`·.¸¸.·´¯`·.¸><(((º>

Richard Moore ~ Founder
Genetic Mistakes Software inc.
phone: (778) 882-6125
email: ricmoo@geneticmistakes.com <mailto:ricmoo@geneticmistakes.com>
www: http://GeneticMistakes.com <http://geneticmistakes.com/>
-------------------------------------
On Tue, Jun 9, 2015 at 1:52 PM, Raystonn . <raystonn@hotmail.com> wrote:


It doesn't have to be enforced. As long as a reasonable percentage of hash
rate is following that policy an attacker that tries to flood the network
will fail to prevent normal transaction traffic from going through and will
just end up transferring some wealth to the miners.

Although the existing default mining policy (which it seems about 70% of
hashpower follows) of setting aside some space for high-priority
transactions regardless of fee might also be enough to cause this attack to
fail in practice.

-- 
--
Gavin Andresen
-------------------------------------
On Fri, Jun 26, 2015 at 2:34 PM, Milly Bitcoin <milly@bitcoins.info> wrote:

Yes, I understand that it may be difficult to define
"uncontroversial", as I explain in
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/008936.html


Can you provide anything to back your claim?
Note that even if that's true, still, Bitcoin core != Bitcoin consensus rules.


Well, yes, github is centralized and so it is bitcoin core development.
But bitcoin core developers don't decide hardfork changes.
So far, softfork changes have been made because they have been
considered "uncontroversial", not because there's any centralized
negotiating table or voting process to decide when to force every user
to adapt their software to new consensus rules.

-------------------------------------
Hello,

I'm not sure if anyone has submitted a request for gmane to monitor and 
archive this new list, but I did. Maybe it's not best for many people to 
re-request that? For those who aren't aware, gmane has a nice HTTP/HTML 
archive as well as a newsgroup proxy that is also very nice for reading 
the archive. They'll also subscribe the list to mail-archive.com for you.

I've also noticed that gmane 
(http://dir.gmane.org/gmane.comp.bitcoin.devel) does not have exactly 
the same archive for the old bitcoin-development list 
(http://sourceforge.net/p/bitcoin/mailman/bitcoin-development/). It's 
missing a few months at the beginning. The new list archive does contain 
the same thing as the old list 
(https://lists.linuxfoundation.org/pipermail/bitcoin-dev/).

gmane has an option to import old archives, so I'm seeing if they can do 
that with the https://lists.linuxfoundation.org/pipermail/bitcoin-dev/ 
list, since it is complete. In order to avoid confusion on what list is 
the most complete, if they can, I may request them delete or rename 
their old, incomplete archive at 
http://dir.gmane.org/gmane.comp.bitcoin.devel . It's possible they could 
even make this new list use that same name after deleting or renaming 
that old archive.


There is also bitcoin-list 
(http://sourceforge.net/p/bitcoin/mailman/bitcoin-list/). I think that 
one may be discontinued? That list appears to be older and contains some 
of Satoshi's original e-mails. On gmane though 
(http://dir.gmane.org/gmane.comp.bitcoin.user), it is missing the first 
few years of archives. Do we want to try and preserve this historical 
correspondence on any way before sourceforce disappears?




Regarding message footers and the subject prefix of [bitcoin-dev], it 
would be cool if mailman allowed people to change this on a per-user basis.

Andy Schroder

On 06/21/2015 05:29 PM, Warren Togami Jr. wrote:

-------------------------------------
The main motivation is to try and stop a single entity running lots of 
nodes in order to harvest transaction origin IPs. That's what's behind 
this.

Probably the efforts are a waste of time.. if someone has to keep a few 
hundred copies of the blockchain around in order to keep IP specific 
precomputed data around for all the IPs they listen on then they'll just 
buy a handful of 5TB HDs and call it a day.. still some of the ideas 
proposed are quite interesting and might not have much downside.

Rob


On 2015-03-27 15:16, Matt Whitlock wrote:



-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On 2015-06-19 10:39, Peter Todd wrote:

     Yesterday F2Pool, currently the largest pool with 21% of the hashing
     power, enabled full replace-by-fee (RBF) support after discussions 
with
     me. This means that transactions that F2Pool has will be replaced if 
a
     conflicting transaction pays a higher fee. There are no requirements 
for
     the replacement transaction to pay addresses that were paid by the
     previous transaction.


Intentional fraud is a bad thing to add to a financial protocol.

A user who creates conflicting transactions, one that pays someone else 
and another which does not pay them, and broadcasts both of them, has 
just self-incriminated themselves by producing prima facie evidence of 
fraud.

It may be the case that since Bitcoin spans multiple legal jurisdictions 
and can be use anonymously that the victims of such fraud can not rely 
on legal recourse, and it may also be the case that proof of work is how 
Bitcoin deals with the aforementioned factors, but regardless 
un-prosecutable fraud is still fraud and anyone who encourages it should 
be recognied as a bad actors.

Committing vandalism and encouraging fraud to prove a point may be 
something the network can't stop on a technical level, but there's no 
reason not to call it out for what it is.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCgAGBQJVhCsXAAoJECpf2nDq2eYjA08P/ApDFcIGws55TsgDFxPhDpN+
Iq9a06mPbXVjUfRxP5ZwmJuiM+XzHQ4QL3C2BH0OETatIV+bh7GP2mGHPcUAISYt
1j4TKhurnC+mqN+YAsiI5hQsws8DvPYXBTYYn0savaJTbq6/Q77+xvfRgNxofcPW
EHpnl/5wcmYGgp3mVyStGJ+qIP17yywzCLnSA3WEPaZG/9/FPrIq3Ptw2+RHod79
nzDiFBiKLK8E5NPbdbXS+gkjkkBA/QeCzZObpMOeWMriu/PIifVi8KssLSznnEwx
r7hiv6ISW47BTzkRbjxmXmGep3wfl8MjH7BZq3g0uyiApMdmjohIJ2lyuvOXdh7s
47+4r2xA8gG+z0aQTmCx5TS75T0Hnj3I78ZtCVr31Ip2OLbNI1mQ2gPR2zaoZkUZ
atp2XCssHDlY2s30k5hAnIHxuN6CkyGkZCECSuv46Z3ok6ll/nIP80qB7BBzVlP1
xfSOPZh57J31U8PxZBZcwgdRg+HBiExvg484grE+h18izxcrjNfPRSWP4+7nEZtK
LN7JL7YcmhVfhqKTSd6+C4bD2LsKsrcMiUhH1xHkD/hzAxc7egL6lgYTHJjU+yPu
BTIh0VHJxBgroHB45Vq6loa4B3l4ZCl4Ykw8Opm7NJIfueJ0l0ySyJXi6ix4bjVf
ZRF0Ot9RP0M0fHEwOpT6
=s0w/
-----END PGP SIGNATURE-----



-------------------------------------
On Thu, May 28, 2015 at 5:22 PM, s7r <s7r@sky-ip.org> wrote:


Yes, Alice is assumed to be the one who funded the channel.  It is a single
direction channel (Alice to Bob).




Assuming the deposit is 1 BTC.

When the channel is created, Alice can broadcast the refund transaction
immediately and the get her money back 150 blocks later.

The full scriptPubKey for the refund transaction would be

OP_IF
    <150> OP_RELATIVE_CHECKLOCKTIME_VERIFY OP_DROP <Alice's public key 2>
OP_CHECKSIGVERIFY
OP_ELSE
    OP_2 <Alice's public key 3> <Bob's public key 2> OP_2
OP_CHECKMULTISIGVERIFY
OP_ENDIF

This means that Alice can spend the output after 150 blocks but with both
signatures Bob and Alice can spend the output without the delay.

She can send money to Bob by spending the non-locked output of the refund
transaction (0.01BTC for Bob and 0.99BTC for Alice).

Bob has a transaction that pays him 0.01BTC and pays Alice 0.99BTC from the
refund transaction and is signed by Alice, but still requires his
signature.  Only Bob can make the transaction valid.

It can be spent as soon as the refund transaction is broadcast.

He has the refund transaction, so he can start the process whenever he
wishes.

Assume the channel runs for a while, and Alice sends 0.3BTC total.

Bob has a transaction which pays him 0.3BTC and Alice 0.7BTC.  He also has
some that pay him less than 0.3, but there is no point in him using those
ones.

Alice decides she wants to close the channel, so asks bob to sign his final
transaction and broadcast it and the refund transaction.

If Bob refuses to do that, then Alice can just broadcast the refund
transaction.

If Bob still refuses to broadcast his final transaction, then Alice gets
1BTC and he gets nothing, after 150 blocks.

This means he will send his final transaction before the 150 blocks have
passed.  This gets him 0.3 and Alice 0.7.

Bob can close the channel immediately and Alice can force it to be closed
within 150 blocks (~1 day).



Protection against that type of fraud isn't covered by channels.  They are
just to make sure money is handed over.



Does the explanation above help?

With some risks.

As long as Bob is online and sees the refund transaction being broadcast by
Alice, then there is no risk to him.

Alice can close the transaction whenever she wants, so there is no holdup
risk for her.



I mean with OP_CHECKLOCKTIMEVERIFY.

She could say that TXA pays to her in 6 months.

If TXA ends up mutated after being broadcast, then she would have to wait
the 6 months.  It's better than nothing and maybe Bob would sign the
mutated transaction.
-------------------------------------
On 06/02/2015 04:03 AM, Mike Hearn wrote:

A mining pool is not a person, a full node is not a miner, and
cooperation is not control.

http://bravenewcoin.com/news/number-of-bitcoin-miners-far-higher-than-popular-estimates/

The entire Bitcoin ecosystem cooperates, that is what consensus means.
Establishing proof of that cooperation is the purpose of Bitcoin.

Decentralization is about keeping control out of the hands of the state
(any entity that would substitute violence for consensus). Nobody has
the power to compel the cooperation of individual miners in a pool. When
state power is applied to a pool operator the miners (people) retain
their vote.

e

-------------------------------------
I am breaking this into a couple of pieces as my first response has been in
a moderator queue for some time because it is too long.


TL;DR version - Wallet Name Service has always been a decentralized and
distributed service that it no way requires you to ever touch the Netki
infrastructure.  We want to work with the community, as we have been from
the beginning, to come up with the best standard possible.

Longer answers inline below.



On Tue, Jul 14, 2015 at 12:07 PM, Riccardo Spagni <ric@spagni.net> wrote:

believe the trade offs should be on perceived privacy versus censorship
resistance and centralization.


By having a limited number of proxies people need to go through to easily
implement, be it the 4 you recommend, or 53, you actually have a very
limited number of actors for an authority or hacker to go to in order to be
able to install/force logging, or censorship.  This very centralization
forces us back to a model where we need to trust a very small number of
actors in order for the system to operate as designed.  This, to me,
appears to be the opposite of the goals of the bitcoin ecosystem.  To
ensure this point is clear, I strongly believe recommending people focus
all lookups through 4 centralized "proxies" is a bad idea and counter to
bitcoin's ideals.


The fact that hackers or state actors need to corrupt only a small number
of servers/services in order to gain global visibility into all queries, I
believe, breaks any perceived privacy gains from using DNSCrypt.  A very
small number of hacks or subpoenas and everyone's records are fair game in
one place.


For the highly privacy conscious they can, today do their DNS lookups over
a non logging VPN connection without forcing everyone else through a
handful of centralized servers.  Or they can use DNSCrypt optionally
themselves.  All of our tools have always been open source and folks can
modify them for their own desired uses, or submit pull requests with their
own ideas.

We'd love to hear others thoughts on this.  While I believe that for now
the centralization trade offs required to use DNSCrypt today (via a limited
number of proxies) outweigh any perceived privacy benefits it provides, we
are always open to what others in the community believe and have made
modifications to how things work before as a result of feedback from
industry participants.







I think DANE is a great idea.  We were just discussing that with Andreas
S., and are currently looking at whether we want to add this as optional
versus mandatory, based on how widely available DANE is for folks using
services like Cloudflare, Akamai, etc for their DNS, which many providers
in the space today are.

Of course, the security conscious could setup DANE on the URL we use AS
IS.  There is no need to create a special kv pair for this as is done in
OpenAlias.  As you know, DNSSEC and HTTPS support this today out of the box.

The CA validation, in our case, is an ADDITIONAL signature based validation
to the DNSSEC chain, not a replacement for it.


 [CONTINUED]
-------------------------------------
On Sun, Jun 28, 2015 at 7:29 PM, Gavin Andresen <gavinandresen@gmail.com> wrote:

Unlike other payment channels designs, the lightning payment channel
network allows you to pay to people that you haven't sent a pre-fund
to.
There's must be a path in the network from you to the payee.
That's simpler with only a few hubs although too few hubs is bad for privacy.


Worried about financial institutions using Bitcoin? No. Who said that?


Remember the hubs cannot steal any coins.


I don't see how people could pay coffees with bitcoin in the long term
otherwise.
Bitcoin IOUs from a third party (or federation) maybe, but not with
real p2p btc.

-------------------------------------
As I understand, there is already a consensus among core dev that block 
size should/could be raised. The remaining questions are how, when, how 
much, and how fast. These are the questions for the coming Bitcoin 
Scalability Workshops but immediate consensus in these issues are not 
guaranteed.

Could we just stop the debate for a moment, and agree to a scheduled 
experimental hardfork?

Objectives (by order of importance):

1. The most important objective is to show the world that reaching 
consensus for a Bitcoin hardfork is possible. If we could have a 
successful one, we would have more in the future

2. With a slight increase in block size, to collect data for future 
hardforks

3. To slightly relieve the pressure of full block, without minimal 
adverse effects on network performance

With the objectives 1 and 2 in mind, this is to NOT intended to be a 
kick-the-can-down-the-road solution. The third objective is more like a 
side effect of this experiment.


Proposal (parameters in ** are my recommendations but negotiable):

1. Today, we all agree that some kind of block size hardfork will happen 
on t1=*1 June 2016*

2. If no other consensus could be reached before t2=*1 Feb 2016*, we 
will adopt the backup plan

3. The backup plan is: t3=*30 days* after m=*80%* of miner approval, but 
not before t1=*1 June 2016*, the block size is increased to s=*1.5MB*

4. If the backup plan is adopted, we all agree that a better solution 
should be found before t4=*31 Dec 2017*.

Rationale:

t1 = 1 June 2016 is chosen to make sure everyone have enough time to 
prepare for a hardfork. Although we do not know what actually will 
happen but we know something must happen around that moment.

t2 = 1 Feb 2016 is chosen to allow 5 more months of negotiations (and 2 
months after the workshops). If it is successful, we don't need to 
activate the backup plan

t3 = 30 days is chosen to make sure every full nodes have enough time to 
upgrade after the actual hardfork date is confirmed

t4 = 31 Dec 2017 is chosen, with 1.5 year of data and further debate, 
hopefully we would find a better solution. It is important to 
acknowledge that the backup plan is not a final solution

m = 80%: We don't want a very small portion of miners to have the power 
to veto a hardfork, while it is important to make sure the new fork is 
secured by enough mining power. 80% is just a compromise.

s = 1.5MB. As the 1MB cap was set 5 years ago, there is no doubt that 
all types of technology has since improved by >50%. I don't mind making 
it a bit smaller but in that case not much valuable data could be 
gathered and the second objective of this experiment may not be 
archived.

--------------------

If the community as a whole could agree with this experimental hardfork, 
we could announce the plan on bitcoin.org and start coding of the patch 
immediately. At the same time, exploration for a better solution 
continues. If no further consensus could be reached, a new version of 
Bitcoin Core with the patch will be released on or before 1 Feb 2016 and 
everyone will be asked to upgrade immediately.

-------------------------------------
On Sun, Feb 22, 2015 at 08:02:03AM +0000, Adam Back wrote:

FWIW I've been advocating this kind of thing in various forms for
literally years, including to hold fidelity bonded banks honest - what
you now call 'federated sidechains' - and most recently Feb 12th on
#bitcoin-dev:

19:56 < petertodd> leakypat: now, do note that an advanced version [of replace-by-fee scorched earth] could be to make another tx that alice and bob setup in advance such that if alcie doublespends, bob gets the money *and* alice pays a bunch of cash to miners fees
19:57 < petertodd> leakypat: this would work espectially well if we improved the scripting system so a script could evaluate true based on proof-of-doublespend
19:58 < leakypat> Yeah, proof of double spend would ideally be done at the protocol level
19:59 < petertodd> leakypat: if satoshi hadn't make the multiple things that CHECKSIG does into one opcode it'd be really easy, but alas...

Implementing it as a general purpose scripting language improvement has
a lot of advantages, not least of which is that you no longer need to
rely entirely on inherently unreliable P2P networking: Promise to never
create two signatures for a specific BIP32 root pubkey and make
violating that promise destroy and/or reallocate a fidelity bond whose
value is locked until some time into the future. Since the fidelity bond
is a separate pool of funds, detection of the double-spend can happen
later.

Equally, that *is* what replace-by-fee scorched-earth does without the
need for a soft-fork, minus the cryptographic proof and with a bit less
flexibility.


Is releasing a version of Bitcoin Core with different IsStandard() rules
than the previous version vandalism? Is mining with a different policy
than other people vandalism? Is mining at a pool that gets sybil
attacked vandalism? Are my replace-by-fee tools an act of vandalism?
Because every one of those things causes people to get double-spent in
the real world, even losing tens of thousands of dollars until they get
some sense and stop treating unconfirmed transactions as confirmed.

Is it vandalism if you decide to host a wedding right next to a hairpin
corner at a rally race and complain to me that mud is getting on the
pretty white dresses? Is it vandalism if I tell that wedding party to
fuck off before someone gets hurt? Is it vandalism if some racers take
the mudguards off for a few laps to see if we can encourage them to
leave before someone gets *actually* hurt? Or someone decides that the
solution is to pave the track over and hold a bicycle race instead...

-- 
'peter'[:-1]@petertodd.org
000000000000000017c2f346f81e93956c538531682f5af3a95f9c94cb7a84e8
-------------------------------------
On Sat, Jun 27, 2015 at 07:46:55PM +0200, Benjamin wrote:

There's lots of markets where there is no assured quality of service,
and where the bids others are making aren't known. Most financial
markets work that way - there's only ever probabalistic guarantees that
for a given amount of money you'll be able to buy a certain amount of
gold at any given time for instance. Similarly for nearly all
commodities the infrastructure required to mine those commodities has
very little room for short, medium, or even long-term production
increases, so whatever the production supply is at a given time is
pretty much fixed.

-- 
'peter'[:-1]@petertodd.org
0000000000000000007fc13ce02072d9cb2a6d51fae41fefcde7b3b283803d24
-------------------------------------
On Thu, Jul 23, 2015 at 1:42 AM, Cory Fields via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

For what is worth, here's yet another piece of code from the "doing
nothing" side:

https://github.com/bitcoin/bitcoin/pull/6382

It allows you to create a regtest-like testchain with a maximum block
size chosen at run time.
Rusty used a less generic testchain for testing 8 MB blocks:

http://rusty.ozlabs.org/?p=509

Unfortunately I don't know of anybody that has used my patch to test
any other size (maybe there's not that much interest in testing other
sizes after all?).

I'm totally in favor of preemptively adapting the code so that when a
new blocksize is to be deployed, adapting the code is not a problem.
Developers can agree on many changes in the code without users having
to agree on a concrete block size first.
I offer my help to do that. That's what I'm trying to do in #6382 and
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/008961.html
but to my surprise that gets disregarded as "doing nothing" and as
"having a negative attitude", when not simply ignored.

-------------------------------------
On Thu, Jul 23, 2015 at 1:43 PM, Eric Lombrozo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Increasing block size only temporarily addresses one significant issue -
Larger block sizes don't scale the network, they merely increase how much
load we allow the network to bear. On the flip side, the scalability
proposals will still require larger blocks if we are ever to support
anything close to resembling "mainstream" usage. This is not an either/or
proposition - we clearly need both.

- Jameson

-------------------------------------
It's not really clear why this is better than BIP 44 as it already
stands. You have the same fields, but they are just in a different
order. Couldn't you just use the existing BIP 44 hierarchy, but add
the convention that "wallet/account N" is the same wallet in each
supported currency?

For example, if I have a wallet called "business expenses", which
happens to be wallet m / 44' / 0' / 5', for Bitcoin, then the same
wallet would be m / 44' / 3' / 5' for Dogecoin, and m / 44' / 2' / 5'
for Litecoin.

I am trying to think of examples where your proposal is better than
BIP 44, but I can't think of any. Even backup recovery works fine. I
assume that your idea is to continue iterating over the different
wallet indices as long as you are finding funds in *any* currency.
Well, you can still do that with BIP 44. The fields are in a different
order, but that doesn't affect the algorithm in any way.

Maybe you have some deeper insight I'm not seeing, but if so, you need
to clearly explain that in your motivation section. The current
explanation, "This limits the possible implementations of
multi-currency, multisignature wallets," is pretty vauge. Also, there
is nothing in this spec that addresses the multisignature use-case.
The BIP 45 spec does a lot of extra work to make multisignature work
smoothly.

I'm not trying to criticize your proposal. I'm just trying to
understand what it's trying to accomplish.

-William Swanson


On Wed, Apr 8, 2015 at 12:05 AM, Kefkius <kefkius@maza.club> wrote:


-------------------------------------
Hi all,

it's a very useful approach to also model fees and you came up with an interesting scenario.
Assuming that you meant that the groups are only connected with a single link,
I've recreated the scenario with Gavin's simulation and got similar results.
The group with the large hashrate does profit overall, but the miners which are not directly
connected to the small group loose:
https://github.com/jonasnick/bitcoin_miningsim/blob/master/analysis/README.md#two-groups-well-connected-internally-but-connected-to-each-other-with-a-single-poor-connection

Moreover, it's important to note that this is not an equilibrium because these miners are better off when they create their own
connections to the small group (see the plot below the other one).
This means that your scenario is not the result of a cartel but the result of a long-term network partition.
When assuming partitions, there are quite a few scenarios where big miners can profit from creating big
blocks. For example, one 40% miner and two groups of three 10% miners, where both groups are connected to the big
miner but they are not connected to each other.
https://github.com/jonasnick/bitcoin_miningsim/blob/master/analysis/README.md#one-big-miner-and-two-partitioned-groups

Best,
Jonas


On 06/12/2015 06:51 PM, Pieter Wuille wrote:




-------------------------------------
I'm OK with a smaller size + a formula that ramps it up over time. We are
far from having enough demand to fill 10MB blocks, let alone 20MB today.

To put it in perspective, to be feeling squeezed inside 10MB within two
years, we would need to double usage five times. I wish I knew a way to
make that happen. So the chances of us going to 20MB blocks full of real
transactions any time soon is close to zero short of some amazing killer
app that takes the world by storm (in which case: yay, nice problem to
have). As long as capacity significantly outpaces organic growth, we should
avoid problems.

The reason to pick 20MB then is merely one of expedience: we have to pick a
number, 20 is tested and seems to work, and we don't want to get caught by
surprise if demand does outstrip expectations.

Still, I question the underlying logic. We have no idea what connectivity
into China will look like a few years from now: it's seems to be a function
of politics rather than hardware trends. It might go down rather than up.
So 10 vs 20 feels a bit arbitrary. We can't let the Chinese government
dictate how Bitcoin is used, that would never be accepted by the rest of
the world. But if we optimistically assume things don't get worse, and 10
== more acceptance, then alright - it should make no difference in practice.
-------------------------------------
On Mon, Aug 24, 2015 at 06:15:39PM +0000, Eric Lombrozo via bitcoin-dev wrote:

You don't necessarily need to send everyone the same nServices bits.
E.g. you could give whitelisted peers special privileges.

But only advertize the intersection of your supported services (eg those you offer to the general public) in `addr` messages.

Wladimir

-------------------------------------
On Wed, May 6, 2015 at 5:12 PM, Matt Corallo <bitcoin-list@bluematt.me> wrote:

Well, there has been significant public discussion in #bitcoin-wizards
on irc.freenode.net which is available in public logs, specifically
about why increasing the max block size is kicking the can down the
road while possibly compromising blockchain security. There were many
excellent objections that were raised that, sadly, I see are not
referenced at all in the recent media blitz. Frankly I can't help but
feel that if contributions, like those from #bitcoin-wizards, have
been ignored in lieu of technical analysis, and the absence of
discussion on this mailing list, that I feel perhaps there are other
subtle and extremely important technical details that are completely
absent from this--and other-- proposals. I have some rather general
thoughts to offer.

Secured decentralization is the most important and most interesting
property of bitcoin. Everything else is rather trivial and could be
achieved millions of times more efficiently with conventional
technology. Our technical work should be informed by the technical
nature of the system we have constructed.

I suspect that as bitcoin continues to grow in all dimensions and
metrics, that we will see an unending wave of those who are excited by
the idea of Something Different in the face of archaic, crumbling
software and procedures in the rest of the financial world. Money has
found its way into every aspect of human life. There's no doubt in my
mind that bitcoin will always see the most extreme campaigns and the
most extreme misunderstandings. Like moths to a flame or water in the
desert, almost everyone is excited by ANY status quo change
whatsoever. This is something that we have to be vigilante about,
because their excitement is motivation to do excellent work, not
simply any work. For some who are excited about general status quo
changes that bitcoin represents, they may not mind if bitcoin
decentralization disappears and is replaced with just a handful of
centralized nodes. Whereas for development purposes we must hold
ourselves to extremely high standards before proposing changes,
especially to the public, that have the potential to be unsafe and
economically unsafe. We have examples from NASA about how to engineer
extremely fault tolerant systems, and we have examples from Linux
about how to have high standards in open-source projects. Safety is
absolutely critical, even in the face of seemingly irrational
excuberance of others who want to scale to trillions of daily coffee
transactions individually stored forever in the blockchain.

When designing bitcoin or even some other system, an important design
target is what the system should be capable of. How many transactions
should the system perform? What is the correct number of transactions
for a healthy, modern civilization to perform every day? And how fast
should that (not) grow? Should we allow for 2 billion trillion coffee
transactions every day, or what about 100 trillion transactions per
second? I suspect that these sorts of questions are entirely
unanswerable and boring. So in the absence of technical targets to
reach during the design phase, I suspect that Jeff Garzik was right
when he pointed out a few months ago that bitcoin is good at
settlement and clearing. There are many potential technical solutions
for aggregating millions (trillions?) of transactions into tiny
bundles. As a small proof-of-concept, imagine two parties sending
transactions back and forth 100 million times. Instead of recording
every transaction, you could record the start state and the end state,
and end up with two transactions or less. That's a 100 million fold,
without modifying max block size and without potentially compromising
secured decentralization.

The MIT group should listen up and get to work figuring out how to
measure decentralization and its security :-). Maybe we should be
collectively pestering Andrew Miller to do this, too. No pressure,
dude. Getting this measurement right would be really beneficial
because we would have a more academic and technical understanding to
work with. I would also characterize this as high priority next to the
"formally verified correctness proofs for Script and
libbitcoinconsensus".

Also, I think that getting this out in the open on this mailing list
is an excellent step forward.

- Bryan
http://heybryan.org/
1 512 203 0507


-------------------------------------
On Wed, Aug 19, 2015 at 11:31 AM, Jorge Timón <jtimon@jtimon.cc> wrote:



What problem am I missing if we just mask of the offending bits. For my own
project which uses auxpow (and thus has weird nVersion), I also used the
bitmasking method to get rid of auxpow version bits before making the
standard integer comparisons to deploy BIP66 using IsSuperMajority():

    if ((block.nVersion & 0xff) >= 4 && CBlockIndex::IsSuperMajority(...))
{ //...}
-------------------------------------
We can use nVersion & 0x8 to signal support, while keeping the consensus
rule as nVersion >= 4, right? That way we don't waste a bit after this all
clears up.
On Aug 18, 2015 10:50 PM, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I've just tagged 0.10.0rc2 in git.

To fetch, build, and test (see also doc/build-*.md):
```
git clone -b v0.10.0rc2 https://github.com/bitcoin/bitcoin.git bitcoin-0.10
cd bitcoin-0.10
./autogen.sh
./configure
make
make check
```

Note: This includes the changes required for interoperability with
OpenSSL 1.0.1k.

Notable changes relative to v0.10.0rc1:

- 4e7c219 Catch UTXO set read errors and shutdown
- a3a7317 Introduce 10 minute block download timeout
- 12b7c44 Improve robustness of DER recoding code
- 76ce5c8 fail immediately on an empty signature
- 2d375fe depends: bump openssl to 1.0.1k
- ace39db consensus: guard against openssl's new strict DER checks
- 263b65e tests: run sanity checks in tests too
- e2677d7 Fix smartfees test for change to relay policy
- b7a4ecc Build: Only check for boost when building code that requires it
- 867c600 Catch LevelDB errors during flush
- 008138c Bugfix: only track UTXO modification after lookup
- 3022e7d Require sufficent priority for relay of free transactions
- 06fdf32 bitcoin-tx: Fix JSON validation of prevtxs
- 58fda4d Update seed IPs, based on bitcoin.sipa.be crawler data
- 94b362d On close of splashscreen interrupt verifyDB
- 1eadfd9 Bugfix: prioritisetransaction: Do some basic sanity checking on txid
- 18021d0 Remove bitnodes.io from dnsseeds.
- b790d13 English translation update
- 8543b0d Correct tooltip on address book page
- 87d43a3 rpcserver: attempt to fix uncaught exception.
- 06ca065 Fix CScriptID(const CScript& in) in empty script case

Wladmir


-------------------------------------
Bitcoin has no elections; it has no courts. If not through attempting a
hard-fork, how should we properly resolve irreconcilable disagreements?

On Sat, Aug 15, 2015 at 6:07 PM, Eric Lombrozo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I'd argue that at the point where there's consistently more transactions 
than the network can handle, there are two significant risks. Firstly, 
that people don't care enough to pay the transaction fees required to 
get their transaction prioritised over another's, and secondly that as 
transactions start outright failing (which will happen with enough 
transactions backlogged) the network is considered unreliable, the 
currency illiquid, and there's a virtual "bank rush" to get into a more 
usable currency.

I understand the desire to use current demand to model future, however I 
feel there's a lack of understanding of just how inadequate the main 
chain is as a global clearance network. My go-to example for this is 
CHIPS (US-only, inter-bank only clearance) which already handles 
slightly over 3 transactions per second on average across a year 
(https://www.theclearinghouse.org/~/media/tch/pay%20co/chips/reports%20and%20guides/chips%20volume%20through%20may%202015.pdf?la=en). 
If Bitcoin is to be used across a wider portion of the world's 
population, and/or beyond clearance between financial institutions, it 
needs larger blocks. This is not about handling the several orders of 
magnitude more transactions that would be required to replace credit 
cards or cash, but simply to enabling other technologies to perform that 
scaling.

Also, and I'm aware most on this list do understand the situation better 
than this, I find it immensely frustrating to see people suggesting that 
Greece or other large groups should adopt Bitcoin, while there's clearly 
inadequate support (on chain or off) to do so.

Ross

On 26/06/2015 19:34, Pieter Wuille wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Another point building on Justus's remarks that I'll make.... (below)

Justus Ranvier via bitcoin-dev:


One example that came to mind as I was reading this was, when I
presented an idea that I thought would be good for integration into
Bitcoin Core, explaining in various ways why I felt it would be
worthwhile to explore, I eventually had someone tell me I should go
and develop the idea first as either some sort of independent wallet,
or to demonstrate it would work via an alt.  (This has now occurred,
as a successful implementation of my micro-donations idea has been
demonstrated in an alt.)  I have to wonder, however, when I eventually
bring the micro-donation ideas back in such a form that they could
again be considered in bitcoin-dev, whether or not they would
seriously be considered, in part due to this effect which Justus
Ranvier has described in part ~ that is to say, the effect of people
engaging in the use of "maximalist" or some other label (or labels) as
limiting the extent of discourse which people can engage in.  (I
realize that wasn't exactly where you were going with this Justus, but
I'm just expanding upon the notion of how some labels and categories
can be used to suppress real discussion.)  Or, for example, if people
see me as "conflicted," and someone else doesn't, and I'm confused
about why someone would see me as "conflicted," where does that leave
one?  Quite possibly, stuck in a morass of unproductive commentary (or
maybe just being ignored by moderators who might see quite a few
people as "conflicted").


Another thing to consider, although the person(s) proposing the list
moderation policy and conduct document will certainly not want to hear
it, is that the list might be better off without a policy document
that is enforced by moderators.  (An "about" section for what the list
is about, its purpose, and how people are supposed to treat each
other, is probably good... but the enforcement angle that I'm seeing
is probably a bad idea.)  What we stand for here is more than making
people comfortable while technical issues are discussed on a list.
The idea of keeping a protocol free of financial censorship, in
concept, extends to language as well, and thus people should be able
to be free in how they write and speak, even when their peers on the
list don't like what they see in others' expressions.

I recommend removal of the enforcement and moderator sections.
(Technically, there are mods for it already... I suppose... the
question is how you disclose in a "Purpose" or "About" section that
refers to this list who the mods are, or rather, what the roles are of
each person involved in a way that is minimally invasive and lets the
list flow.)


- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEcBAEBCgAGBQJWH2YLAAoJEGxwq/inSG8C0aAH/AqYWgZEyRM5d1rAwjt6jNrf
Vqkd+kBCu0+0CQRXHUwJpK07IzFm5CwzSGIwri/VWT+1t/27Lk1Kt9iV4+zxOZhO
RFyo4gmJ6GApZ7N6wlIWD9R2hFdg9Q+taZHgRXiMDMqi8MOJjf5tMAXnYjbMQrSr
ntLY3ESFF0yF3ZGIIptNI4atv6UdhL2po7p+F5GMa7VZp7/e3zw96Uxmd2wkZN0R
3G5VHR2gscn3PooykpH/nhpH4mk0eFsWomuwWXAxfo2JjMhuyIXU0KnUs7ibpfPT
qtOmBW/7DI//IeRJpstAnbc22g6YOqCKrMDgNe0HgVjnmugNpY1/wRh29m+WCpA=
=felI
-----END PGP SIGNATURE-----

-------------------------------------
to supplement mining revenue and so those who do not have access to cheap
or free power to mine;"

why?
wouldn't a bigger block size actually allow for more transactions per
block, therefore more fees to be collected, and the cost spread out among
many more users (thus still keeping tx fees low). If anything, wouldn't
bigger blocksizes are needed to suplement the losses of coinbase rewards
being halfed.

http://twitter.com/gubatron

On Mon, Aug 17, 2015 at 12:39 PM, Ahmed Zsales via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On this day, the Bitcoin network was crawled and reachable nodes surveyed to find their maximum throughput in order to determine if it can safely support a faster block rate. Specifically this is an attempt to prove or disprove the common statement that 1MB blocks were only suitable slower internet connections in 2009 when Bitcoin launched, and that connection speeds have improved to the point of obviously supporting larger blocks.


The testing methodology is as follows:

 * Nodes were randomly selected from a peers.dat, 5% of the reachable nodes in the network were contacted.

 * A random selection of blocks was downloaded from each peer.

 * There is some bias towards higher connection speeds, very slow connections (<30KB/s) timed out in order to run the test at a reasonable rate.

 * The connecting node was in Amsterdam with a 1GB NIC. 

 
Results:

 * 37% of connected nodes failed to upload blocks faster than 1MB/s.

 * 16% of connected nodes uploaded blocks faster than 10MB/s.

 * Raw data, one line per connected node, kilobytes per second http://pastebin.com/raw.php?i=6b4NuiVQ


This does not support the theory that the network has the available bandwidth for increased block sizes, as in its current state 37% of nodes would fail to upload a 20MB block to a single peer in under 20 seconds (referencing a number quoted by Gavin). If the bar for suitability is placed at taking only 1% of the block time (6 seconds) to upload one block to one peer, then 69% of the network fails for 20MB blocks. For comparison, only 10% fail this metric for 1MB blocks.

-------------------------------------
Very cool! This will certainly help make Lightning Network testable on
the main-chain and permit channels to remain open indefinitely. I'm
looking forward to it.

On Thu, Aug 13, 2015 at 12:06:44PM +0100, Btc Drak via bitcoin-dev wrote:

I haven't tested the details of this, but is there another bit available
for use in the future for the relative blockheight?

I strongly believe that Lightning needs mitigations for a systemic
supervillan attack which attemps to flood the network with transactions,
which can hypothetically be mitigated with something like a timestop
bit (as originally suggested by gmaxwell).

Summary: If a block is flagged as timestopped (whether automatically or
by vote or other mechanism), then an auxillary blockheigh is frozen and
does not increment. This auxillary blockheight is only used for
accounting in timestopped height computation (and isn't used for
anything else). So as the real blockheight increments, the auxillary
blockheight can sometimes stop and stay the same. If a transaction has a
timestop bit enabled, then the transaction's OP_CSV relative height is
dependent upon the auxillary height, not the real block height. This
allows for a large backlog of transactions which must occur before a
particular (relative) block height to enter into the blockchain.

I'm not sure if it's out of scope, but it could make sense to consider
the possibility for additional state(s) with relative height computation
today. Ideally, there'd be some kind of "version" byte which can be
recontextualized into something later, but I don't know how that could
cleanly fit into the data structure/code.

-- 
Joseph Poon

-------------------------------------
September 15 2015 6:04 AM, "Luke Dashjr" <luke@dashjr.org> wrote:

My proposal is about the current signing process (which exists event it it's not perfect) but it could also work with a new signing message system tomorrow. It more about give users an easier way to access existing tools than the "sign message thing" itself.

BTW I'm aware of privacy issues, but could you elaborate on why the use case your are referring to doesn't actually work?
Here are a use of bitcoin signatures ( https://bitcointalk.org/index.php?topic=497545.0 ) to speak about a real case.

--
Arthur

-------------------------------------
I'm quite puzzled by the response myself, it doesn't seem to address some
of the (more serious) concerns that Adam put out, the most important
question that was asked being the one regarding personal ownership of the
proposed fork:

"How do you plan to deal with security & incident response for the duration
you describe where you will have control while you are deploying the
unilateral hard-fork and being in sole maintainership control?"

I do genuinely hope that whomever (now and future) wishes to fork the
protocol reconsider first whether they are truly ready to test/flex their
reputation/skills/resources in this way... Intuitively, to me it seems
counterproductive, and I don't fully believe it is within a single
developer's talents to manage the process start-to-finish (as it is
non-trivial to hard-fork successfully, others have rehashed this in other
threads)...

That being said I think it appropriate if Adam's questions were responded
in-line when Mike is feeling up to it. I think that the answers are
important for the community to hear when such a drastic change is being
espoused.

Faiz

On Mon, Jun 15, 2015 at 4:56 PM, Bryan Bishop <kanzure@gmail.com> wrote:

-------------------------------------
Good idea. I think this could be even better:

instead of using third party, send partially signed TX from computer
to smartphone. In case, you are paranoid, make 3oo5 address made of
two cold storage keys, one on desktop/laptop, one on smartphone, one
using third party.
If it isn't enough, add requirement of another four keys, so you have
three desktops with different OS (Linux, Windows, Mac) and three
mobile OS (Android, iOS, Windows Phone), third party and some keys in
cold storage. Also, I forgot HW wallets, so at least Trezor and
Ledger. I believe this scheme is unpenetrable by anyone, including
NSA, FBI, CIA, NBU...

Jokes aside, I think leaving out third party is important for privacy reasons.

Stay safe!

2015-02-02 18:40 GMT+01:00 Brian Erdelyi <brian.erdelyi@gmail.com>:


-------------------------------------
On Thu, Jun 4, 2015 at 6:02 PM, Daniel Stadulis <dstadulis@gmail.com> wrote:


Indeed.

Take this somewhere else, Mr. Berg.  This is a technical mailing list.

-- 
Jeff Garzik
Bitcoin core developer and open source evangelist
BitPay, Inc.      https://bitpay.com/
-------------------------------------
On Thu, Feb 12, 2015 at 1:18 PM, Mike Hearn <mike@plan99.net> wrote:

How many thousands of BTC must be stolen by miners before you'd agree
that it has, in fact, happened?
(https://bitcointalk.org/index.php?topic=321630.0)

On Thu, Feb 12, 2015 at 3:27 PM, Jeff Garzik <jgarzik@bitpay.com> wrote:

I just wanted to pull this out and say that I agree with this
completely; to the point where I'm continually surprised to see people
expressing other views (but they do).

I don't have much opinion about replace-by-fee; It has pluses and
minuses. In the past I've considered it a "oh perhaps best to not talk
about that" idea. I think making zero conf actively less secure would
be generally regrettable, though it might make building alternatives
for fast and acceptably safe transactions more attractive sooner. I do
favor a version of replace by fee that adds the extra constraint that
all prior outputs must be paid equal or more; which would capture many
of the 'opps paid too little' without opening up the malicious double
spends quite as much (so soon).

One challenge is that without rather smart child-pays-for-parent logic
the positive argument for replace by fee doesn't really work.

On Thu, Feb 12, 2015 at 12:52 PM, Alex Mizrahi <alex.mizrahi@gmail.com> wrote:

As a point for historical accuracy: PPC was actively attacked with
stake grinding and had to use developer signed blocks to prevent the
attacker from mining all the blocks and then later made a hard fork to
make it harder, and retains the developer block signing to stop it.

This doesn't contradict your point, which I agree with: an absence of
attacks doesn't mean an absence of vulnerability, and people counting
on things that they wouldn't if they understood them better is
something to avoid. And the prior point about game theory is one I
think some people have a hard time with: partipants are looking out
for their own interests, not some global optimum.  It may not be the
case that everyone (or even anyone) is maximally short sighted; but
it's even more unreasonable to assume that no one will ever break rank
and do something selfish.

I don't know that RBF even needs to be debated on these terms, since
there is an argument for RBF as good even if we assume miners are all
fully protocol conforming.


-------------------------------------
Thanks - several good suggestions, including some in common.  Will comment
& revise today.

Currently in "collecting" mode, to avoid duplicative comments in multiple
locales.



On Thu, Sep 3, 2015 at 3:57 AM, <jl2012@xbt.hk> wrote:

-------------------------------------
Michael Naber wrote:

Everyone here is excited about the potential of Bitcoin and would
aspirationally like it to reach its full potential as fast as
possible.  But the block-size is not a free variable, half those
parameters you listed are in conflict with each other.  We're trying
to improve both decentralisation and throughput short-term while
people work on algorithmic improvements mid-term.  If you are
interested you can take a look through the proposals:

http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/008603.html

Note that probably 99% of Bitcoin transactions already happen
off-chain in exchanges, tipping services, hosted wallets etc.  Maybe
you're already using them, assuming you are a bitcoin user.
They constitute an early stage layer 2, some of them even have on
chain netting and scale faster than the block-chain.

You can also read about layer 2, the lightning network paper and the
duplex micropayment channel paper:

http://lightning.network/lightning-network-paper-DRAFT-0.5.pdf
http://www.tik.ee.ethz.ch/file/716b955c130e6c703fac336ea17b1670/duplex-micropayment-channels.pdf

and read the development list and look at the code:

http://lists.linuxfoundation.org/pipermail/lightning-dev/
https://github.com/ElementsProject/lightning

Adam


On 27 June 2015 at 16:39, Michael Naber <mickeybob@gmail.com> wrote:

-------------------------------------
Airbitz has developed and implemented a method for communicating a bitcoin
URI across Bluetooth (BLE) or any other P2P, mid range, wireless, broadcast
medium. The currently documented implementation is available in our iOS and
Android mobile wallet (updated Android version with BLE coming in about 1
week). We would like to have the BIP pulled into Github for review and
discussion. Here is the current BIP:


BIP: TBD

Title: P2P Wireless URI transfer

Authors: Thomas Baker <tom’at’airbitz.co>, Paul Puey <paul’at’airbitz.co>

Contributors: Joey Krug <joeykrug’at’gmail.com>

Status: proposal

Type: Standards Track

Created: 2015-01-12

Table of Contents

   -

   Abstract
   -

   Motivation
   -

   Specification
   -

   Compatibility
   -

   Examples
   -

   References

Abstract

This is a protocol for peer-to-peer wireless transfer of a URI request
using an open broadcast or advertisement channel such as Bluetooth,
Bluetooth Low Energy, or WiFi Direct.
Motivation

There are disadvantages for a merchant (requester) and customer (sender) to
exchange a URI request using QR codes that can be eliminated by using
wireless broadcast or advertisements.

Current QR code scan method to transfer a request URI from merchant
(Requester) to customer (Sender) is cumbersome. A usual scenario is a
merchant with a POS terminal for order entry and a separate tablet for
transacting payments with bitcoin, and a customer with a smartphone. After
the order is entered, the merchant enters payment request information into
the tablet, generates the QR code representing the URI, and presents this
to the customer. The customer prepares to scan the QR code with their
smartphone by maneuvering the camera to the tablet. The tablet screen must
be relatively clean, point at the customer, and held steady. The smartphone
camera lens must be clean, point at the tablet screen, come into range, and
held steady to focus and wait for a QR scan. Environmental conditions such
as bright outdoor sunlight, indoor spot lights, or significant distance
between QR code and camera can create difficult and cumbersome experiences
for users.

Using a wireless local broadcast allows the merchant to just enter the
payment and wait. The tablet and smartphone are not maneuvered to align in
any way. The customer observes broadcast listings, selects the appropriate
one from possible simultaneous broadcasts from other POS stations nearby,
examines the URI request details such as amount, and decides whether to
send funds, initiating a bitcoin network transfer. The merchant and
customer then receive the transaction confirmations and are done with the
sale. Merchant and customer devices are kept private and secured in their
own possession.

The URI and other broadcast identification (Joe’s Grill #1) only contain
public information. However, a copycat broadcaster acting as MITM might
duplicate the broadcast simultaneously as the merchant, attempting to lure
the customer to send funds to the copycat. That attack is mitigated with
this broadcast method because of the partial address in the broadcast.
Specification

Requester generates a bitcoin URI request of variable length, and a limited
descriptive identifier string. Requester then broadcasts the URI’s partial
public address (<paddress>) plus identifier (<id>) over a publicly visible
wireless channel.

Sender scans for broadcasts on their device, examines and selects the
desired request by the identifier and partial address. This connects a data
channel to Requester.

Requester sends full URI back over the data channel.

Sender device ensures <paddress> is part of the full URI public address and
checks the full address integrity. Checking the broadcast and full URI
integrity prevents a copycat device within range from copying the partial
address and fooling the customer into sending funds to the copycat instead.

Below is a description of the protocol through Bluetooth Smart (Low Energy).

Requestor      Sender     - Bitcoin transaction roles

Peripheral     Central    - Bluetooth GAP definitions

  Mode           Mode

1   |------------->|       - Requestor Advertises partial bitcoin: URI +
Name

   |     ...      |

2   |<-------------|       - Subscribe then send sender's Name, requesting
a response

3   |------------->|       - ACK

4   |<-------------|       - request Read Characteristic from peripheral

5   |------------->|       - Sender receives full bitcoin: URI


   1.

   Peripheral advertises over a service UUID a BLE extended advertisement
   with a Scan Response containing the partial address of a bitcoin URI and a
   Name, any plain text. The entire response is limited to 26 characters. The
   first 10 make up the first 10 characters of the bitcoin URI public address
   where to send bitcoin, and must be present. The remaining characters are
   any plain text such as “The Habit 1” or “Starbucks-Reg 1”, more human
   readable information. The partial address serves as a check against a
   nearby attacker who may try to lure a Sender into sending payment to a
   separate wallet by advertising a similar Scan Response but cannot replicate
   a public address with the same leading 10 characters and different trailing
   characters.
   2.

   When the Central scans the advertisement, it may display the Scan
   Response in a human readable listing using the two pieces of information.
   If Central chooses this advertisement to receive the full request, it then
   subscribes to the service and writes the characteristic (a second UUID)
   with it’s own name, or a blank if not sending a name, to the Peripheral.
   3.

   Peripheral gets a characteristic write request of the Central’s name,
   and acknowledges the receipt by sending a server response.
   4.

   Central receives a characteristic write (from the response) and
   immediately requests the entire bitcoin URI by issuing a read request on
   that characteristic.
   5.

   Peripheral receives the read request and sends the entire bitcoin URI
   over that characteristic up to 512 bytes.

This ends the proposed specification as the bitcoin URI transfer is
complete. The Sender would then normally confirm the request and decide
whether to initiate the fund transfer.
Compatibility

There are no prior BIPs covering this.
Examples

Airbitz iOS Bluetooth Low Energy to Bluetooth Low Energy request transfer.
References



[image: logo]
*Paul Puey* CEO / Co-Founder, Airbitz Inc
+1-619-850-8624 | http://airbitz.co | San Diego
<http://facebook.com/airbitz>  <http://twitter.com/airbitz>
<https://plus.google.com/118173667510609425617>
<https://go.airbitz.co/comments/feed/>  <http://linkedin.com/in/paulpuey>
<https://angel.co/paul-puey>
*DOWNLOAD THE AIRBITZ WALLET:*
  <https://play.google.com/store/apps/details?id=com.airbitz>
<https://itunes.apple.com/us/app/airbitz/id843536046>
-------------------------------------
This is true, but the device doesn't know if the LAN it's on is a safe
network or a hotel wifi, for example. So there would be a tricky UX there.
You'd have to ask the user during set up if this is a trusted LAN or not;
or something like that. That may not be an issue though depending on the
nature of the product. For example, Chromecast doesn't need any security
protections against trolls on the same LAN. I guess it just depends on what
you're planning to build.

On Mon, May 25, 2015 at 9:56 PM, Matt Whitlock <bip@mattwhitlock.name>
wrote:

-------------------------------------
Hi

Following earlier posts on Proof of Payment I'm now proposing the following
BIP (To read it formatted instead, go to
https://github.com/kallerosenbaum/poppoc/wiki/Proof-of-Payment-BIP).

Regards,
Kalle Rosenbaum

<pre>
  BIP: <BIP number>
  Title: Proof of Payment
  Author: Kalle Rosenbaum <kalle@rosenbaum.se>
  Status: Draft
  Type: Standards Track
  Created: <date created on, in ISO 8601 (yyyy-mm-dd) format>
</pre>

== Abstract ==

This BIP describes how a wallet can prove to a server that it has the
ability to sign a certain transaction.

== Motivation ==

There are several scenarios in which it would be useful to prove that you
have paid for something. For example:

* A pre-paid hotel room where your PoP functions as a key to the door.
* An online video rental service where you pay for a video and watch it on
any device.
* An ad-sign where you pay in advance for e.g. 2 weeks exclusivity. During
this period you can upload new content to the sign whenever you like using
PoP.
* Log in to a pay site using a PoP.
* A parking lot you pay for monthly and the car authenticates itself using
PoP.
* A lottery where all participants pay to the same address, and the winner
is selected among the transactions to that address. You exchange the prize
for a PoP for the winning transaction.

With Proof of Payment, these use cases can be achieved without any personal
information (user name, password, e-mail address, etc) being involved.

== Rationale ==

Desirable properties:

# A PoP should be generated on demand.
# It should only be usable once to avoid issues due to theft.
# It should be able to create a PoP for any payment, regardless of script
type (P2SH, P2PKH, etc.).
# It should prove that you have enough credentials to unlock all the inputs
of the proven transaction.
# It should be easy to implement by wallets and servers to ease adoption.

Current methods of proving a payment:

* In BIP0070, the PaymentRequest together with the transactions fulfilling
the request makes some sort of proof. However, it does not meet 1, 2 or 4
and it obviously only meets 3 if the payment is made through BIP0070. Also,
there's no standard way to request/provide the proof. If standardized it
would probably meet 5.
* Signing messages, chosen by the server, with the private keys used to
sign the transaction. This could meet 1 and 2 but probably not 3. This is
not standardized either. 4 Could be met if designed so.

If the script type is P2SH, any satisfying script should do, just like for
a payment. For M-of-N multisig scripts, that would mean that any set of M
keys should be sufficient, not neccesarily the same set of M keys that
signed the transaction. This is important because strictly demanding the
same set of M keys would undermine the purpose of a multisig address.

== Specification ==

=== Data structure ===

A proof of payment for a transaction T, here called PoP(T), is used to
prove that one has ownership of the credentials needed to unlock all the
inputs of T. It has the exact same structure as a bitcoin transaction with
the same inputs and outputs as T and in the same order as in T. There is
also one OP_RETURN output inserted at index 0, here called the pop output.
This output must have the following format:

 OP_RETURN <version> <txid> <nonce>

{|
! Field        !! Size [B] !! Description
|-
| &lt;version> || 2        || Version, little endian, currently 0x01 0x00
|-
| &lt;txid>    || 32       || The transaction to prove
|-
| &lt;nonce>   || 6        || Random data
|}

The value of the pop output is set to the same value as the transaction fee
of T. Also, if the outputs of T contains an OP_RETURN output, that output
must not be included in the PoP because there can only be one OP_RETURN
output in a transaction. The value of that OP_RETURN output is instead
added to the value of the pop output.

An illustration of the PoP data structure and its original payment is shown
below.

<pre>
  T
 +----------------------------------------------+
 |inputs       | outputs                        |
 |       Value | Value   Script                 |
 +----------------------------------------------+
 |input0 1     |     0   pay to A               |
 |input1 3     |     2   OP_RETURN <some data>  |
 |input2 4     |     1   pay to B               |
 |             |     4   pay to C               |
 +----------------------------------------------+

  PoP(T)
 +----------------------------------------------------------+
 |inputs       | outputs                                    |
 |       Value | Value   Script                             |
 +----------------------------------------------------------+
 |input0 1     |     3   OP_RETURN <version> <txid> <nonce> |
 |input1 3     |     0   pay to A                           |
 |input2 4     |     1   pay to B                           |
 |             |     4   pay to C                           |
 +----------------------------------------------------------+
</pre>

The PoP is signed using the same signing process that is used for bitcoin
transactions.

The purpose of the nonce is to make it harder to use a stolen PoP; Once the
PoP has reached the server, that PoP is useless since the server will
generate a new nonce for every PoP request.

Since a PoP is indistinguishable from a bitcoin transaction, there is a
risk that it, accidently or maliciously, enters the bitcoin p2p network. If
T is still unconfirmed, or if a reorg takes place, chances are that PoP(T)
ends up in a block, invalidating T. Therefore it is important that the
outputs of the PoP are the same as in T. The zero transaction fee in PoP(T)
is to minimize the incentives for miners to select PoP(T) for inclusion.

=== Process ===

# A proof of payment request is sent from the server to the wallet. The PoP
request contains:
## a random nonce
## a destination where to send the PoP, for example a https URL
## data hinting the wallet which transaction to create a proof for. For
example:
##* txid, if known by the server
##* PaymentRequest.PaymentDetails.merchant_data (in case of a BIP0070
payment)
##* amount, label, message or other information from a BIP0021 URL
# The wallet identifies a transaction T, if possible. Otherwise it asks the
user to select among the ones that match the hints in 1.iii.
# The wallet creates an unsigned PoP (UPoP) for T, and asks the user to
sign it.
# The user confirms
# The UPoP(T) is signed by the wallet, creating PoP(T).
# The PoP is sent to the destination in 1.ii.
# The server receiving the PoP validates it and responds with “valid” or
“invalid”.
# The wallet displays the response in some way to the user.

'''Remarks:'''

* The method of transferring the PoP request at step 1 is not specified
here. Instead that is specified in separate specifications. See [btcpop
scheme BIP](btcpop scheme BIP).
* The nonce must be randomly generated by the server for every new PoP
request.

=== Validating a PoP ===

The server needs to validate the PoP and reply with "valid" or "invalid".
That process is outlined below. If any step fails, the validation is
aborted and "invalid" is returned:

# Check the format of the PoP. It must pass normal transaction checks,
except that the inputs may already be spent.
# Check the PoP output at index 0. It must conform to the OP_RETURN output
format outlined above.
# Check that the rest of the outputs exactly corresponds to the outputs of
T and that they appear in the same order as in T. An exception to this is
that any OP_RETURN outputs of T must not be included in the PoP. All output
value from the OP_RETURN must instead be included in the PoP output.
# Check that the nonce is the same as the one you requested.
# Check that the inputs of the PoP are exactly the same as in transaction
T, and in the same order.
# Check the scripts of all the inputs, as would be done on a normal
transaction.
# Check that the txid in the PoP output is the transaction you actually
want proof for. If you don’t know exactly what transaction you want proof
for, check that the transaction actually pays for the product/service you
deliver.
# Return "valid".

== Security considerations ==

* Someone can intercept the PoP-request and change the PoP destination so
that the user sends the PoP to the bad actor.
* Someone can intercept the PoP-request and change for example the txid to
trick the user to sign a PoP for another transaction than the intended.
This can of course be avoided if the user is actually looking at the UPoP
before signing it. The bad actor could also set hints for a transaction,
existing or not, that the user didn’t make, resulting in a broken service.
* Someone can steal a PoP and try to use the service hoping to get a
matching nonce. Probability per try: 1/(2^48). The server should have a
mechanism for detecting a brute force attack of this kind, or at least slow
down the process by delaying the PoP request by some 100 ms or so.
* Even if a wallet has no funds it might still be valuable as a generator
for PoPs. This makes it important to keep the security of the wallet after
it has been emptied.
* Transaction malleability may cause the server to have another transaction
id than the wallet for the payment. In that case the wallet will not be
able to prove the transaction for the server. Wallets should not rely on
the transaction id of the outgoing transaction. Instead it should listen
for the transaction on the network and put that in its list of transactions.

The first two issues are the same attack vector as for traditional, ie
BIP0021, bitcoin payments. They could be mitigated by using secure
connections.

== Reference implementation ==

[https://github.com/kallerosenbaum/poppoc poppoc on GitHub]

[https://github.com/kallerosenbaum/wallet Mycelium fork on GitHub]

== References ==

[https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki BIP0021]:
URI Scheme

[https://github.com/bitcoin/bips/blob/master/bip-0070.mediawiki BIP0070]:
Payment Protocol

[[btcpop scheme BIP]]
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

You wanted advice... you got it

Jonathan Toomim (Toomim Bros):

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEcBAEBCgAGBQJWHZrZAAoJEGxwq/inSG8CZzQIAKsqKs//Wydv60nXgy5AWAPU
qZ9HuyyWXDKljxzv/Ky5jS7o7B8Ivhnt6zWvkpMTF/R9MLpGrS9jBxXZjHF//ET0
L+eoVrmxwt+rgSjIPSGU/ftF8Jnh1sELecR8FMuCaFR87xraR/7FsJF/233RLWFg
+scNiFEgttyizFNgSq2r1/N3G5e603qXfh0+reaabDX3E+8+PKyUqVaG5E+TUEW0
NIkqi7MuEYd+/Q0SGAYyY/j2BQnebsTB2TbupE/soJkAYqYbCQR8TtrctmwLXTL0
GN+WyWwLYpMio3+7a6oQJ67TBcFxCVmF81zxKM1VIoT0u39VVWeYD1YfxEYFN9Y=
=a6kH
-----END PGP SIGNATURE-----

-------------------------------------
Shouldn't a odbc jdbc jconnect or equivalent be totally transparent for the consensus code? I mean, the client would write or store the data communicating to the driver provided by the vendor. Using the schema bitcoin suggests adapted to many different vendors (one table schema for Oracle, other for mysql, etc with their slight syntax particularities), installed in the machine with the node and from that communication to the driver  the storage would be totally controlled by the third party rdbms. 
Regarding bugs or risk of fork, does not have actual client any defense against someone forking core and slightly changing the actual database used maybe wrongly and creating a fork by themselves? 
Does the client have any way to verify that what is stored is correct? Maybe inserting a column with a hash of what is stored in each row and another column with a incremental row by row hash composed by the hash of each row and the previous column one., so any tampering in a previous row can be verified up to where is not consistent.
I just imagine what would be for people to be able to access easily (with the thousands of software packages already bought and licensed by ALL companies in the world that already use open standard connectivity or equivalents)., the bitcoin blockchain. 
SUBSCRIPTION: for a couple decades replication servers have allowed a publish/subscription model using replication agents. If I am a guy working on a lever in the warehouse with my pda I do not need on my pda all the company info or maybe all the blockchain. If a company., that has already licensed a rdbms package with dozens of related software packages needs one guy to suscribe to something on the bitcoin blockchain, he can either use one of the purchased methods in their company and access the company database that holds blockchain data or hire a rare bitcoin developer that will create a interfaz bitcoin for a specific need up to the millions of needs out there. 
PUBLISHING Maybe even to have a publishing daemon that would allow those companies and their software packages to write things in the bitcoin blockchain provided of couse that they fund the agent with a small bitcoin amount to send transactions and they comply with the database constraint of being the owners of the private key. The publishing agent would check for changes every X minutes on that specific address  in the db and if funded it would publish "send" the transaction through the bitcoin client. People would be able to publish info on the decentralized ledger from 90% of enterprise software packages.,paying ofc  and with the small delay of the publishing agent checking for changes. In fact the db would allow publishing info while the publishing agent could just take its time publishing at its own rate like a slow write cache.
In any case shouldn't even actual consensus be shielded from a malfunctioning or Ill forked database from core client

El 17 de noviembre de 2015 16:24:42 CET, Tom Harding <tomh@thinlink.com> escribió:

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------------------------------
When you assume you make an ass out of you and me. That page doesn't even exist in my router, I don't have 2wire. The router I have is the one everyone is getting from uverse.

Literally everything you said is incorrect. It is completely on topic as it pertains to Bitcoin Core functionality, 42 connections is nothing, and that's not how you fix it. If you're gonna call me out for being wrong at least be correct! I spent a lot of time fixing this and the info is useful cause this problem happens to a lot of people, so why obfuscate it with this nonsense. 

AT&T u-verse as it is now is near impossible for bitcoin nodes, 42 in the world is absolutely pathetic considering they are among the top 2 ISPs in the USA. Thanks for proving my point.

 

 

-----Original Message-----
From: Matt Whitlock <bip@mattwhitlock.name>
To: hurricanewarn1 <hurricanewarn1@aol.com>
Cc: bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
Sent: Wed, Sep 2, 2015 3:21 pm
Subject: Re: [bitcoin-dev] AT&T has effectively banned Bitcoin nodes via utilizing private subnets.


I've been trying to keep our discussion off-list because it is off-topic, but
you keep adding the list back on in your
replies.

http://steamforge.net/wiki/images/2/29/Settings-Firewall-Advanced.png

Settings
Protocols

That's all you had to do.


On Wednesday, 2 September 2015, at
9:44 am, Zach G via bitcoin-dev wrote:
them. Clearly that is a problem, do you even know about AT&T or are you in
another country? Cause that statement is utterly ridiculous given the fact there
are hundreds of millions of people using AT&T. I was simply sharing my knowledge
on this issue since it poses a threat to the health of the bitcoin network, no
need for personal attacks. 
firewall in the DVR that is uncontrolled and all ports are blocked via private
subnets and no fixed public IP allowed unless you pay. I confirmed every one of
these details with AT&T technicians or I wouldn't be saying them.

<bip@mattwhitlock.name>
Wed, Sep 2, 2015 5:34 am
banned Bitcoin nodes via utilizing private subnets.
BitNodes, 42 Bitcoin nodes are running on AT&T's
https://getaddr.bitnodes.io/nodes/?q=AT%26T
nothing wrong with AT&T's default network configuration.
things you've been writing strongly suggest that you aren't very
knowledgeable
wild accusations
to verify your
Zach G via bitcoin-dev
said no block source
literally getting thottled
connection to get block source. EVERY
the router firewall did nothing. I was
a major security risk.
Bitcoin Python modules, so I login to my computer
was flat out rejecting the connection. I could not run
got fixed, and of course needed the block source to even do

list. Bitcoin Core was crippled and unusable due to the AT&T
they tried hard to get me to buy monthly subscriptions to get the
This makes it likely that Bitcoin Core is unusable for most AT&T
and other ISPs, hence the massive node decline. I'm sure this disrupts
of other people besides Bitcoiners too, hence the monthly subscriptions
geared towards people who can't figure out their connection situation.
wasn't a
hackers and
almost every
down 15% in the
before this gets
https://getaddr.bitnodes.io/dashboard/?days=365 6,000
is and it's constantly declining.

 
-------------------------------------
On Sun, Feb 15, 2015 at 11:40:24PM +0200, Adam Gibson wrote:

The problem with that statement is I trust a merchant that I went into
a store and made a payment with personally more than I trust the firmware
on my hard drive [1].

The attack surface of devices in your computer is huge. A motivated attacker
just needs to get an intern into a company that makes some kind of component
or system that's in your computer, cloud server, hardware wallet, or what 
have you that has firmware capable of reading your private keys.

With the possibility of mass trojaned hardware, if we are going to trust 
the system, it must somehow allow reversal through a human-in-the-loop.
 

We built 'reliable' TCP on top of unreliable ethernet networks. My experience
with networking was if you tried to guarantee message delivery at the lowest
level, the system got exceedingly complicated, expensive, and brittle.

Most applications, in particular paying someone you already trust, are quite
happy running on reversible systems, and in some cases more reliable and 
lower risk. (carrying non-reversible cash is generally considered risky)

The problem is that if the base currency is assumed to be non-reversible, 
then it's brittle and becomes 'too big to fail'.

Where the blockchain improves on everything else is in transparency. If you
reverse transactions a lot, it will be obvious from an analysis. I would much
rather deal with a known, predictable, and relatively continous transaction
reversal rate (percentage) than have to deal with sudden failures where 
some anonymous bad actor makes off with a fortune.

We already have zero-conf double-spend transaction reversal, why not explicitly
extend that a little in a way that senders and receivers have a choice to 
use it, or not?


[1] http://www.reuters.com/article/2015/02/16/us-usa-cyberspying-idUSKBN0LK1QV20150216


-------------------------------------
People are wondering why there is such a division in the bitcoin community
at this time. Bitcoin as a whole is only so big. The bitcoin discussion
communities are spread out, but there are only a few popular ones.
BitcoinTalk and Reddit seem to be the largest, most popular where bitcoin
discussion happens consistently and from almost everyone in the community
(devs, users, btc companies, merchants, and so on). I think we all know
where this is going. If not, here you go. BitcoinTalk and Reddit are
managed by a single person: Theymos. It's become quite clear over the past
several days/weeks, that Theymos is highly censoring bitcoin discussion,
mainly on Reddit, but also BitcoinTalk. If this single person is able to
censor content, and hush the debate, he (and whatever his agenda is), wins.
How can we as a community discuss these proposed changes, if the discussion
is from a one sided point of view? There doesn't seem to be a solution at
this time, but I find it dissapointing that many (in this very email list)
aren't discussing this important part of the issue. Maybe it's becuase
Theymos is anti-bigger blocks, which seems to coincide with the Blockstream
point of view, which many of the core devs belong to.


-------------------------------------------------

ONLY AT VFEmail! - Use our Metadata Mitigator to keep your email out of the NSA's hands!
$24.95 ONETIME Lifetime accounts with Privacy Features!  
15GB disk! No bandwidth quotas!
Commercial and Bulk Mail Options!  
-------------------------------------
On Tuesday 11. August 2015 21.27.46 Jorge Timn wrote:

Fees rising due to scarcity has nothing to do with the problem. Its a 
consequence that is irrelevant to me.

Bad situations are roughly divided into two parts;
 * technical
 * marketing.

The technical part is that we already know of several technical 
solutions we 
will need when we have a forever growing backlog. Without them, nodes 
will 
crash.
On top of that, we can expect a lot of new problems we don't know yet.

IT experts are serious when they say that they avoid maxing out a 
system like 
the plague.


Marketing wise full blocks means we can only serve 3 transactions a 
second. 
Which is beyond trivial. All the banks, nasdaq, countries, businesses 
etc etc 
now contemplating using Bitcoin itself will see this as a risk too big to 
ignore and the 1Mb Bitcoin will loose 99% of its perceived value.

If you want fees to rise, then it should be viable to be used, withing 6 
months, for something bigger than the economic size of Iceland. 
(=random 
smallest country I know).

-- 
Thomas Zander

-------------------------------------
On Thursday, November 12, 2015 7:47:50 PM Matt Corallo via bitcoin-dev wrote:

This should be optional, at least for 0.12.


We should not be influencing miner policy by changing defaults.

Luke

-------------------------------------
Bitcoin was/is a disruptive technology for credit card payment processors,
and replace-by-fee/stag-hunt is a disruptive technology for Bitcoin payment
processors.

I think whether you call it scorched earth is a bit more of a reflection of
whether you stand to make money, or lose money from the distruption.

Personally, I think 'first-seen' is a dangerous scorched-earth policy that
only benefits the people who own the internet routers that determine what
is seen first.

But from the standpoint of consensus, can we at least agree that it's a
*node policy* decision, and the market particpants should be free to choose
whichever policy works best for them?

Otherwise, I think the only way to make 'first-seen' work is by adding 
a timestamp to CTransaction.

On Sat, Feb 21, 2015 at 05:47:28PM -0500, Jeff Garzik wrote:

-- 
----------------------------------------------------------------------------
Troy Benjegerdes                 'da hozer'                  hozer@hozed.org
7 elements      earth::water::air::fire::mind::spirit::soul        grid.coop

      Never pick a fight with someone who buys ink by the barrel,
         nor try buy a hacker who makes money by the megahash



-------------------------------------
Two years ago I presented a new way to create a fee market that does not
depend on the block chain limit.

This proposal has not been formally analyzed in any paper since then,
but I think it holds a good promise to untangle the current problem
regarding increasing the tps and creating the fee market. BTW, think the
maximum tps should be increased, but not by increasing the block size,
but by increasing the block rate (I'll expose why in my next e-mail).

The original post is here (I was overly optimistic back then):
https://bitcointalk.org/index.php?topic=147124.msg1561612#msg1561612

I'll summarize it here again, with a little editing and a few more
questions at the end:

The idea is simple, but requires a hardfork, but is has minimum impact
in the code and in the economics.

Solution: Require that the set of fees collected in a block has a
dispersion below a threshold. Use, for example, the Coefficient of
Variation (http://en.wikipedia.org/wiki/Coefficient_of_variation). If
the CoVar is higher than a fixed threshold, the block is considered invalid.

The Coefficient of variation is computed as the standard deviation over
the mean value, so it's very easy to compute. (if the mean is zero, we
assume CoVar=0). Note that the CoVar function *does not depend on the
scale*, so is just what a coin with a floating price requires.

This means that if there are many transactions containing high fees in a
block, then free transactions cannot be included.
The core devs should tweak the transaction selection algorithm to take
into account this maximum bound.

*Example*

If the transaction fee set is: 0,0,0,0,5,5,6,7,8,7
The CoVar is 0.85
Suppose we limit the CoVar to a maximum of 1.

Suppose the transaction fee set is: 0,0,0,0,0,0,0,0,0,10
Then the CoVar is 3.0

In this case the miner should have to either drop the "10" from the fee
set or drop the zeros. Obviously the miner will drop some zeros, and
choose the set: 0,10, that has a CoVar of 1.

*Why it reduces the Tx spamming Problem?*

Using this little modification, spamming users would require to use
higher fees, only if the remaining users in the community rises their
fees. And miners won't be able to include an enormous amounts of
spamming txs.

*Why it helps solving **the tragedy-of-the-commons fee "problem"?*

As miners are forced to keep the CoVar below the threshold, if people
rises the fees to confirm faster than spamming txs, automatically
smamming txs become less likely to appear in blocks, and fee-estimators
will automatically increase future fees, creating a the desired feedback
loop.

*Why it helps solving the block size problem?*

Because if we increase the block size, miners that do not care about the
fee market won't be able to fill the block with spamming txs and destroy
the market that is being created. This is not a solution against an
attacker-miner, which can always fill the block with transactions.

*Can the system by gamed? Can it be attacked?*

I don't think so. An attacker would need to spend a high amount in fees
to prevent transactions with low fees to be included in a block.
However, a formal analysis would be required. Miller, Gun Sirer, Eyal..
Want to give it a try?
*
Can create a positive feedback to a rise the fees to the top or push
fess to the bottom?

*Again, I don't think so. This depends on the dynamics between the each
node's fee estimator and the transaction backlog. MIT guys?

*Doesn't it force miners to run more complex algorithms (such as linear
programming) to find the optimum tx subset ?

*Yes, but I don't see it as a drawback, but as a positive stimulus for
researchers to develop better tx selection algorithms. Anyway, the
greedy algorithm of picking the transactions with highest fees fees
would be good enough.

*
PLEASE don't confuse the acronym CoVar I used here with co-variance.*

Best regard,
  Sergio.



-------------------------------------
On 08/19/2015 04:27 PM, Jorge Timón wrote:

I don't see this happening any time soon, and I'm not sure why we should
wait for it.


You might consider this as feedback from your customer base.


That's a false dichotomy. We never would have considered forking Bitcoin
Core, and still wouldn't. Why would we set ourselves up for this
disruption, which would inevitably lead to us factoring the consensus
portions of libconsensus out of /bitcoin at the 11th hour?

We have to operate as if it can happen at any time. Otherwise we have
relinquished control of this vote and failed our users. Given that
operating assumption, it is much safer for us to have already done this
work (which we did). [It also provides a forcing function for us to
review in detail any consensus changes that get pushed out.]

My question is why you would not embrace an independent consensus
repository? Your work to evolve it doesn't change.


OK

e

-------------------------------------
On Tue, Jan 20, 2015 at 8:35 PM, Pieter Wuille <pieter.wuille@gmail.com> wrote:

I'd like to request a BIP number for this.

-- 
Pieter


-------------------------------------


On 08/21/15 22:06, Peter Todd wrote:

Had a discussion on IRC and with Pieter, and I kinda agree that the more
optimal way is for DNS seeds to, instead of returning NODE_NETWORK
nodes, return any node which responds to getaddr, allowing clients to
connect to a few DNS seeds by name, do a getaddr, then disconnect (like
Bitcoin Core does now if you're using Tor). They can then select the
peers they want based on nServices.


Meh, whatever, justification is already provided well enough without
having to go into "but if we did this long into the future"  arguments.


Ehh, I was going more for the oldest mention.


-------------------------------------
On Wed, Feb 4, 2015 at 2:23 PM, Isidor Zeuner
<cryptocurrencies@quidecco.de> wrote:


Adding/subtracting a randomized offset amount is one way, but there
have also been more sophisticated ideas to obfuscate the amount, e.g.
by adding multiple change outputs or even distributing over multiple
transactions (potentially coinjoined for further privacy).

Mike Hearn had some ideas regarding obfuscation of payment amounts,
which still make sense, and he wrote about them here:
https://medium.com/@octskyward/merge-avoidance-7f95a386692f

Wladimir


-------------------------------------
Hello,

I've just uploaded Bitcoin Core 0.11.0rc1 executables to:

https://bitcoin.org/bin/bitcoin-core-0.11.0/test/

The source code can be found in the source tarball or in git under the tag 'v0.11.0rc1'

Preliminary release notes can be found here:

https://github.com/bitcoin/bitcoin/blob/0.11/doc/release-notes.md

Thanks to everyone that participated in development or in the gitian build process,

Wladimir



-------------------------------------
That's not what I said. We don't seem able to communicate with each other
efficiently, probably my fault since English is not my native language. But
I don't want to use more of my time (or yours) in this discussion, since
it's clearly unproductive.
On Jul 28, 2015 6:45 PM, "Tom Harding" <tomh@thinlink.com> wrote:

-------------------------------------
People have already been testing big blocks on testnets.

The biggest problem here isn’t whether we can test the code in a fairly sterile environment. The biggest problem is convincing enough people to switch without:

1) Pissing off the other side enough to the point where regardless of who wins the other side refuses to cooperate
2) Screwing up the incentive model, allowing people to sabotage the process somehow
3) Setting a precedent enabling hostile entities to destroy the network from within in the future
etc…

These kinds of things seem very hard to test on a testnet.


-------------------------------------
Hi,

I am the developer for Ninki Wallet which has recently been listed on
bitcoin.org.

FWIW I support the approach of the existing bitcoin core dev team having
observed their work for the last year and their attitude towards consensus,
a clear process for change and discussion of changes.

I think, like everyone else, that the block size limit will have to be
increased in the future, but I don't like to see this done in a gung-ho way
without an established consensus on how bitcoin will scale *with*
decentralization and maintain it's key value property.

I would not support any group that attempts to fork the blockchain in the
manner proposed using XT. I strongly urge Gavin to withdraw from this
standoff and work with the bitcoin core devs via the existing and
successful bip process.

Thanks

Ben
-------------------------------------
On Fri, Jul 03, 2015 at 10:43:14PM -0700, Raystonn wrote:

Nodes can and do lie about what version they are all the time.

Fact is, SPV means you're trusting other people to check the rules for
you. In this particular case bitcoinj could have - and should have -
checked the BIP66 soft-fork rules, but in general there's no easy
solution to this problem.

-- 
'peter'[:-1]@petertodd.org
00000000000000000a43884e675843f56df90feffeabf56c4e7350f96b623f00
-------------------------------------


Then please enlighten me. You're unable to download block templates from a
trusted node outside of the country because the bandwidth requirements are
too high? Or due to some other problem?

With respect to "now it's your turn". Let's imagine the hard fork goes
ahead. Let us assume that almost all exchanges, payment processors and
other businesses go with larger blocks, but Chinese miners do not.

Then you will mine coins that will not be recognised on trading platforms
and cannot be sold. Your losses will be much larger than from orphans.

This can happen *even* if Chinese miners who can't/won't scale up are >50%
hashrate. SPV clients would need a forced checkpoint, which would be messy
and undesirable, but it's technically feasible. The smaller side of the
chain would cease to exist from the perspective of people actually trading
coins.

If your internet connectivity situation is really so poor that you cannot
handle one or two megabits out of the country then you're hanging on by
your fingernails anyway: your connection to the main internet could become
completely unusable at any time. If that's really the case, it seems to me
that Chinese Bitcoin is unsustainable and what you really need is a
China-specific alt coin that can run entirely within the Chinese internet.
-------------------------------------

He hasn't ignored you, and he wasn't responding to your email specifically
but rather the general attitude displayed in this forum for the last
several months (and I'd argue the last year or so).

Your data is interesting but ultimately tell us what we already know - that
the next bottleneck after the hard coded limit could easily be propagation
speed. The solution is likely to be a better protocol. Matt's custom
network already has optimised things, rolling some of those ideas into the
P2P protocol may be a good place to start, or something fancier like IBLTs.

Regardless, the *next* bottleneck is not the protocol, it's the hard cap.

So the conclusion remains unchanged: Bitcoin must grow, and solutions for
scaling it up will be found as the need arises.
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512



On 18 December 2015 11:52:19 GMT-08:00, "Jorge Timón via bitcoin-dev" <bitcoin-dev@lists.linuxfoundation.org> wrote:


FWIW all these median time based schemes should be using median time past: the point is to use a time that the block creator has no direct control of, while still tying the rule to wall clock time for planning purposes.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWdG/r
AAoJEMCF8hzn9Lncz4MH/iYJv6aB9rvfvy1KuSSHAQDQ++6j7Flmk2n8f/S4jt4q
92MZnKDw09HxUJiWvwREi81wHpq4JedgK1Z/+8m3wlK+jaIyWZ7Su+Jm+EqsoOSJ
Sx6oisbyFlhVEUAdaG/XOX/K0mqh01NSvGGpoQjHAYzcG3pI03OC4G7Qg4WGeZLx
O0yb387DmK/of52JGJcei3TUx0w8Up/GdXDqerLxioH7fhGhtGCj0vyD4LugnNLQ
hka5g+hri27YltfaRxncNQ0nZT4rAfgRgRH1Qi3kHnc6ZgRcRjjb36TyrWjZ34eb
9+YDAirFwu8HGmi7lfxh9DDtVjPZCwKal7/rNeRI744=
=7f+W
-----END PGP SIGNATURE-----


-------------------------------------
On Wed, Dec 16, 2015 at 5:09 PM, Jeff Garzik <jgarzik@gmail.com> wrote:


Illustration:  If SW is deployed via soft fork, the count of nodes that
validate witness data is significantly lower than the count of nodes that
validate non-witness data.  Soft forks are not trustless operation, they
depend on miner trust, slowly eroding the trustless validation of older
nodes over time.

Higher security in one data area versus another produces another economic
value distinction between the two goods in the basket, and creates a "pay
more for higher security in core block, pay less for lower security in
witness" dynamic.

This economic distinction is not present if SW is deployed via hard fork.
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On 10/14/2015 6:19 PM, Paul Sztorc wrote:

Probably yes. But probably no. Having less hashing power is not good,
and it's unrelated to scalability and decentralization, it's related
to security. Of course we could argue that the hashing power is not
super decentralized at this moment but it's unrelated to the topic.

I'd rather have less decentralized big amount of hashing power as
opposite to less hashing power.

One theory, very close to yours, is that if Bitcoin transactions
demand grows so high that we need the lightning network, there should
be plenty of on chain transactions for miners to collect fees from.

I haven't yet seen the incentives of everyone involved in lightning
network (payment channel end points, hub operators, miners, etc.) but
would it make sense to enforce a % of the fees collected by on payment
hubs to be spent as miner fees, regardless if the transactions from
that hub go on the main chain or not?
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBCAAGBQJWHtksAAoJEIN/pSyBJlsR9Y0H+QE/XdW7yauhrNJtp2eIBPg9
zVUanzR2LT0zAkeF5/Xsx3PFoypALOV7R0YNL29jI3F2XkZA8v24wfNvPi0DETcC
ZOxw4G1erIEjjj51Qz4M7okjQecJxPHOJ+Nz6iNZEDFcZG2b15phCRSQKZwSHP+b
Erw6a4NPs1foieZyk260KSOB8lFs9e8bUJfXd4FfA7l60RA9582K6p05aqVtehFW
ONTe8ULv8F0ba+EzVyTodzzY6ehjD+uc31zL6mDFIbiW+InivFbfi2uDVN1BP/US
m99lLHvDEthnkTokFrbDu81kXdD0lHwIu4O0EMzCnw2E0vWi3sGKd+M0P0sv4WA=
=1qxh
-----END PGP SIGNATURE-----

-------------------------------------
I think something that anyone who isn't validating should be aware of is
that cgminer(which powers the vast majority of the current mining network)
doesn't allow for a pool to revert to mining on the previous block due to
the way chain tracking is implemented.

https://github.com/ckolivas/cgminer/blob/master/cgminer.c#L4727

On Fri, Dec 4, 2015 at 4:43 PM, Rusty Russell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
I really suggest you look into the layer2 systems Adam pointed to, as you
appear to be misinformed about their properties. There are many proposals
which really do achieve global consensus using the block chain, just in a
delayed (and cached) fashion that is still 100% safe.

It is possible to go off-chain without losing the trustlessness and
security of the block chain.

On Sat, Jun 27, 2015 at 9:09 AM, Michael Naber <mickeybob@gmail.com> wrote:

-------------------------------------
On Monday, October 19, 2015 2:01:04 PM Christian Decker via bitcoin-dev wrote:

This doesn't completely close malleability (which should be documented in the 
BIP), so I'm not sure it's worth the cost, especially if closing malleability 
later on would need more. How about specifying flags upfront in the UTXO-
creating transaction specifying which parts the signature will cover? This 
would allow implementation of fully malleability-proof wallets.

Additionally, you have a flag to control whether the opcode behaves as VERIFY 
or not. Non-VERIFY is not possible as a softfork (without doing a second/new 
P2SH) since it can be negated.

Luke

-------------------------------------
On Tue, Aug 18, 2015 at 11:06 PM, Danny Thorpe via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

I would expect any uncontroversial hardfork to be deployed in testnet3
before it is deployed in bitcoin's main chain.

In any case, you can already do these tests using
https://github.com/bitcoin/bitcoin/pull/6382
Note that even if the new testchains are regtest-like (ie cheap proof
of work) you don't need to test them "in-a-box": you can run them from
many different places.
Rusty's test ( http://rusty.ozlabs.org/?p=509 ) could have been
perfectly made using #6382, it just didn't existed at the time.

-------------------------------------
Hi Chris, I don't speak for Peter, but here's my opinion on the matter
anyway.

On Mon, Aug 17, 2015 at 05:44:56PM -0400, Chris Pacia via bitcoin-dev wrote:

With SPV, it is possible to create a transaction that spends from
non-existent coins. With sufficient hashpower, you can construct an SPV
proof which sends 1,000 bitcoin to the victim. The attack is
"overloadable" in the sense that the attacker is never out of money
(they never needed to have 1,000 BTC in the first place). Whereas if the
victim is running a full node, the attacker must be signing and spending
real outputs in their control, there is a possibility in a re-org that
the victim will eventually get their money if it gets re-orged back.

On a more fundamental level, the SPV attack isn't on re-orging real/live
transactions, it's an attack on *how much money you currently have*. If
the client is using SPV, they never had the money in the first place
when attacked, irrespective of re-orgs.

It is possible to attack thousands of people at once (everyone gets
1,000 bitcoin in false transactions) with a fraction of the hashpower
(lie in wait until you get a sufficiently long chain of blocks). If you
wished to attack a full-node, it requires you orphaning a chain of valid
blocks *live*, meaning you have to send real coins in a real transaction
to the victim first. With SPV validation, you only need to construct a
chain of invalid blocks off the current blockheight *whenever*. This
means you can attack with substantially less hashpower; you don't need
51% of the hashpower to attack SPV wallets. It may be economically
unviable to attack a single victim with a full node within a very short
timeframe, but it can be economically viable to attack thousands of
victims doing SPV validation in a long timeframe.

Note I'm not arguing that SPV should be compeletely avoided, I don't
have a solid opinion on that (and some threats can definitely be
mitigated in various ways, and I certainly like/appreciate the
convenience of SPV), but the current SPV security model is definitely
weaker than running a full node (if you're handling a lot of money, you
should be running a full node), are these issues not well-known by all
in the bitcoin community?

-- 
Joseph Poon

-------------------------------------
I like the idea but I think we should leave at least 16 bits of the
version fixed as an extra-nonce.
If we don't then miners may use them as a nonce anyway, and mess with
the soft-fork voting system.
My original proposal was this: https://github.com/bitcoin/bitcoin/pull/5102

Best regards



-------------------------------------


For the record, there’s pretty much unanimous agreement that running a full node should be a requirement at the higher levels of certification (if not the lower ones as well). I’m not sure exactly what pushback you’re referring to.



-------------------------------------
This BIP was assigned number 113.

I have updated the text accordingly and added credits to Gregory Maxwell.

Please see the changes in the pull request:
https://github.com/bitcoin/bips/pull/182

On Sat, Aug 22, 2015 at 1:57 AM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
It sounds like the main issue is this is a web wallet server of some kind.
If the clients were SPV then they'd be checking their own balances and
downloading their own tx history, which would mean the coordination tasks
could be done by storing encrypted blobs on the server rather than the
server itself having insight into what's going on (see: Subspace).

So whilst you might be able to use some scheme to avoid the server knowing
the xpubkey, if the server still knows all addresses and all transactions
because the clients are web wallets ..... is there any point? It seems like
maybe going from server knows everything to server knows 95% of everything:
maybe not worth the engineering cost.
-------------------------------------
Here is a short review of previously-proposed and exotic SIGHASH types.

SIGHASH_MULTIPLE
SIGHASH_LIST:
https://bitcointalk.org/index.php?topic=252960.0
https://bitcointalk.org/index.php?topic=212555.0

SIGHASH_MULTIPLE
SIGHASH_WITHINPUTVALUE:
https://bitcointalk.org/index.php?topic=191003.0

SIGHASH_WITHINPUTVALUE:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-January/007185.html

SIGHASH_NOINPUT:
https://github.com/Roasbeef/bitcoin/commit/4b3c3f1baf7985208ceb6887261ee150ab6e3328
https://github.com/Roasbeef/btcd/commit/67830e506fa135d5239177340918cea39909e6a4
http://lightning.network/lightning-network-paper.pdf

SIGHASH_NORMALIZED
SIGHASH_NOINPUT:
http://diyhpl.us/wiki/transcripts/sf-bitcoin-meetup/2015-02-23-scaling-bitcoin-to-billions-of-transactions-per-day/

SIGHASH_WITHOUT_PREV_SCRIPTPUBKEY
SIGHASH_WITHOUT_PREV_VALUE
SIGHASH_WITHOUT_INPUT_TXID
SIGHASH_WITHOUT_INPUT_INDEX
SIGHASH_WITHOUT_INPUT_SEQUENCE
SIGHASH_WITHOUT_OUTPUT_SCRIPTPUBKEY
SIGHASH_WITHOUT_OUTPUT_VALUE
SIGHASH_WITHOUT_INPUTS
SIGHASH_WITHOUT_OUTPUTS
SIGHASH_WITHOUT_INPUT_SELF
SIGHASH_WITHOUT_OUTPUT_SELF
SIGHASH_WITHOUT_TX_VERSION
SIGHASH_WITHOUT_TX_LOCKTIME
SIGHASH_SIGN_STACK_ELEMENT:
https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md

Similarly, petertodd has asked for a SIGHASH_DONT_SIGN_TXID before to
make OP_CODESEPARATOR more useful.

SIGHASH_DANGEROUSLYPROMISCUOUS:
http://gnusha.org/bitcoin-wizards/2015-04-17.log

SIGHASH_DOUBLE:
06:41 < lorenzoasr> maybe a SIGHASH_DOUBLE that signs INPUT[i] and
OUTPUT[i] and OUTPUT[i+1] could be very helpful

Some sighash types briefly proposed by petertodd in #bitcoin-dev:
SIGHASH_NLOCKTIMEVERIFY
SIGHASH_SUM (for merging multiple payments)

And finally one from wumpus (#bitcoin-dev):
SIGHASH_UNICORN

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
Hello all,

here I'm going to try to address a part of the block size debate which has
been troubling me since the beginning: the reason why people seem to want
it.

People say that larger blocks are necessary. In the long term, I agree - in
the sense that systems that do not evolve tend to be replaced by other
systems. This evolution can come in terms of layers on top of Bitcoin's
blockchain, in terms of the technology underlying various aspects of the
blockchain itself, and also in the scale that this technology supports.

I do, however, fundamentally disagree that a fear for a change in economics
should be considered to necessitate larger blocks. If it is, and there is
consensus that we should adapt to it, then there is effectively no limit
going forward. This is similar to how Congress voting to increase the
copyright term retroactively from time to time is really no different from
having an infinite copyright term in the first place. This scares me.

Here is how Gavin summarizes the future without increasing block sizes in
PR 6341:

rise; very-low-fee transactions will fail to get confirmed at all.
stop submitting transactions
growth and adoption

Is it fair to summarize this as "Some use cases won't fit any more, people
will decide to no longer use the blockchain for these purposes, and the
fees will adapt."?

I think that is already happening, and will happen at any scale. I believe
demand for payments in general is nearly infinite, and only a small portion
of it will eventually fit on a block chain (independent of whether its size
is limited by consensus rules or economic or technological means).
Furthermore, systems that compete with Bitcoin in this space already offer
orders of magnitude more capacity than we can reasonably achieve with any
blockchain technology at this point.

I don't know what subset of use cases Bitcoin will cater to in the long
term. They have already changed - you see way less betting transactions
these days than a few years ago for example - and they will keep changing,
independent of what effective block sizes we end up with. I don't think we
should be afraid of this change or try to stop it.

If you look at graphs of block sizes over time (for example,
http://rusty.ozlabs.org/?p=498), it seems to me that there is very little
"organic" growth, and a lot of sudden changes (which could correspond to
changing defaults in miner software, introduction of popular
sites/services, changes in the economy). I think these can be seen as the
economy changing to full up the available space, and I believe these will
keep happening at any size effectively available.

None of this is a reason why the size can't increase. However, in my
opinion, we should do it because we believe it increases utility and
understand the risks; not because we're afraid of what might happen if we
don't hurry up. And from that point of view, it seems silly to make a huge
increase at once...

-- 
Pieter
-------------------------------------
Some thoughts, hope this is not off-topic.

Maybe we should summarise the security assumptions and design
requirements.  It is often easier to have clear design discussions by
first articulating assumptions and requirements.

Validators: Economically dependent full nodes are an important part of
Bitcoin's security model because they assure Bitcoin security by
enforcing consensus rules.  While full nodes do not have orphan
risk, we also dont want maliciously crafted blocks with pathological
validation cost to erode security by knocking reasonable spec full
nodes off the network on CPU (or bandwidth grounds).

Miners: Miners are in a commodity economics competitive environment
where various types of attacks and collusion, even with small
advantage, may see actual use due to the advantage being significant
relative to the at times low profit margin

It is quite important for bitcoin decentralisation security that small
miners not be significantly disadvantaged vs large miners.  Similarly
it is important that there not be significant collusion advantages
that create policy centralisation as a side-effect (for example what
happened with "SPV mining" or validationless mining during BIP66
deployment).  Examples of attacks include selfish-mining and
amplifying that kind of attack via artificially large or
pathologically expensive to validate blocks.  Or elevating orphan risk
for others (a miner or collusion of miners is not at orphan risk for a
block they created).

Validators vs Miner decentralisation balance:

There is a tradeoff where we can tolerate weak miner decentralisation
if we can rely on good validator decentralisation or vice versa.  But
both being weak is risky.  Currently given mining centralisation
itself is weak, that makes validator decentralisation a critical
remaining defence - ie security depends more on validator
decentralisation than it would if mining decentralisation was in a
better shape.

Security:

We should consider the pathological case not average or default behaviour
because we can not assume people will follow the defaults, only the
consensus-enforced rules.

We should not discount attacks that have not seen exploitation to
date.  We have maybe benefitted from universal good-will (everybody
thinks Bitcoin is cool, particularly people with skills to find and
exploit attacks).

We can consider a hierarchy of defences most secure to least:

1. consensus rule enforced (attacker loses block reward)
2. economic alignment (attacker loses money)
3. overt (profitable, but overt attacks are less likely to be exploited)
4. meta-incentive (relying on meta-incentive to not damage the ecosystem only)

Best practices:

We might want to list some best practices that are important for the
health and security of the Bitcoin network.

Rule of thumb KISS stuff:

We should aim to keep things simple in general and to avoid creating
complex optimisation problems for transaction processors, wallets,
miners.

We may want to consider an incremental approach (shorter-time frame or
less technically ambitious) in the interests of simplifying and
getting something easier to arrive at consensus, and thus faster to
deploy.

We should not let the perfect be the enemy of the good.  But we should
not store new problems for the future, costs are stacked in favour of
getting it right vs A/B testing on the live network.

Not everything maybe fixable in one go for complexity reasons or for
the reason that there is no clear solution for some issues.  We should
work incrementally.

Adam

-------------------------------------
What of this prior effort, proposing B-with-horizontal-bar (Ƀ)?
http://bitcoinsymbol.org/

They argue that B-with-2-vertical-bars is easily confused with the Thai
Bhat currency symbol, which is a B with a single vertical bar.

I'm not terribly fond of the B-with-horizontal-bar as a symbol, but it does
have the advantage that it is already in the Unicode glyph set, already
available on most Unicode enabled devices.

-Danny

On Sat, Sep 5, 2015 at 7:11 AM, Ken Shirriff via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

If you (e.g. Chainalysis) or anyone else are doing surveillance on the
network and gathering information for later use, and whether or not
the ultimate purpose is to divulge it to other parties for compliance
purposes, you can bet that ultimately the tables will be turned on
you, and you will be the one having your ass handed to you so to
speak, before or after you are served, in legal parlance.  Whether or
not the outcome of that is meaningful and beneficial to any concerned
parties and what is the upshot of it in the end depends on on what you
do and just how far you decide to take your ill-advised enterprise.

Chainalysis and similar operations would be, IMHO, well advised to
cease operations.  This doesn't mean they will, but guess what:

Shot over the bow, folks.

Jan Mller:
Dive into the World of Parallel Programming The Go Parallel Website,
Dive into the World of Parallel Programming The Go Parallel Website,
sponsored

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEcBAEBCgAGBQJVD34mAAoJEGxwq/inSG8CvrQH/28Rt26oGdo9rS+PaR1fIQ1p
Jwks11Axsmu5x3emTgIz0xUJ6zz/4ERM0LeNLBpfSFwZyLbuCgw1uiJplT+9uPgY
hPXb9OTNejfWZJjYc3i6rNjf2SNc5E3/4PtgeOI6lI/SsGQ6ineNm6gFjwe8xVpt
wCLOPetzCukQegXluFZZdALnPDf4H9yAeSsrfX2h2iCBAJ3qd9f1DP7+e6hvr+xr
POVBjlRYtnSd/viKJ2IhMbRvnqd86pRNAKEWrjZp0CIkGyY7wh4nqtYErZi4TcOK
H7yhU8o4/mgTNSIYdLTOSMlRi+nTMPWUD2jvO/Z9i9VTR9afn8E7j7iHD6QPMB0=
=vdbG
-----END PGP SIGNATURE-----


-------------------------------------
On Thu, Aug 6, 2015 at 1:15 PM, Jorge Timón <jtimon@jtimon.cc> wrote:



Two answers:

1. If you are willing to wait an infinite amount of time, I think the
minimum fee will always be zero or very close to zero, so I think it's a
silly question.

2. The "market minimum fee" should be determined by the market. It should
not be up to us to decide "when is a good time."



Sure, if keeping up with transaction volume requires a cluster of computers
or more than "pretty good" broadband bandwidth I think that's too far.
That's where original 20MB limit comes from, otherwise I'd have proposed a
much higher limit.




Although I've been very clear with my criterion, no, I don't think all
blocksize increase proposals should have to justify "why this size" or "why
this rate of increase." Part of my frustration with this whole debate is
we're talking about a sanity-check upper-limit; as long as it doesn't open
up some terrible new DoS possibility I don't think it really matters much
what the exact number is.




It prevents trivial denial-of-service attacks (e.g. I promise to send you a
1 Terabyte block, then fill up your memory or disk...).

And please read what I wrote: I said that the block limit has LITTLE effect
on MINING centralization.  Not "no effect on any type of centralization."

If the limit was removed entirely, it is certainly possible we'd end up
with very few organizations (and perhaps zero individuals) running full
nodes.

-- 
--
Gavin Andresen
-------------------------------------
It is possible to softfork. Just use Iceland time. Iceland time = UTC 
without DST

Btc Drak via bitcoin-dev 於 2015-09-18 16:34 寫到:


-------------------------------------
On Thu, Aug 27, 2015 at 12:22 AM, Daniele Pinna via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Maybe this helps undesrtanding the risks of a contentious/schism
hardfork: https://github.com/bitcoin/bips/pull/181/files#diff-e331b8631759a4ed6a4cfb4d10f473caR137
That section can be greatly improved though.


I have only skimmed the document, but I believe its conclusions (or
some of them) are right for the current propagation code.
The assumption may not stand if we move to something like IBLT though.
I believe that document also proves that it is
irrational/noncompetitive for miners to include ANY free transactions
at all (like they currently do).
But the analysis is about the effect of the maximum block size on
fees: there's more effects of that consensus rule.
The most important one being that it limits mining centralization (and
centralization in general).
This is true for at least 2 reasons: one is related to block
propagation and it is what is usually described.
At a bigger scale (even with crazy assumptions like constant time
infinite bandwidth, instant [superluminal] communication, zkSNARKS
block validity proofs ) minimum CPU costs will be something to limit
through the maximum block size consensus rule. There's a scale at
which the minimum CPU costs for a miner to be competitive may be so
high that some small miners without the resources to meet that minimum
will become unprofitable.
Admittedly we're not near that scale yet, but if something to take into account.


There are some simulations. See:
http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners

The goal of https://github.com/bitcoin/bitcoin/pull/6382 is to allow
people to do more realistic simulations (by using real full nodes).
That doesn't mean that more simplified simulations are worthless, but
I didn't want people to have to create their own testchain every time
they want to simulate a different size, like rusty had to do for:

http://rusty.ozlabs.org/?p=509

So the "incapacity to quantify the advantages that large miners have
over smaller ones" doesn't really exist.
It would be nice to have more data about this (more sizes, more
network topologies, etc) though.


Although smaller subsidies will remove some problems we currently
have, for example, SPV mining (there's no incentive to SPV mine
worthless empty blocks), I don't understand your claim that they will
also solve mining centralization problems related to block
propagation.


I really dislike basing the consensus rules on predictions about
future technology. For all I know, a terrible war could destroy half
of the global internet infrastructure in the next 5 years.
I prefer simpler increments like in bip102 (although I don't have the
data to know if 2MB is safe mining-centralization-wise at this point
[when mining centralization is pretty bad]).
Arguments against that kind of change are usually along the lines
"then we will have to repeat this same discussion in 1 or 2 years".
I believe that with the proper simulation tools being deployed and a
better general understanding of what the concerns are, the
conversation should be much simpler the next time.


Actually some simulations show they in fact have incentive to do just
that in some cases.
But more importantly, we shouldn't assume that all attackers are
rational miners. Maybe a potential attacker is a secret service or a
financial cartel attempting to destroy Bitcoin for whatever reason.


I truly hope that the discussion can move forward into more productive
territories after the workshop, and I'm particularly interested in
Peter R's presentation, even if I haven't found the time to read his
paper yet. Even if fees are not the main reason why we want to have a
block size maximum, fees are certainly very relevant and anything that
he has mathematically proven in that regard will be useful to this
discussion.


-------------------------------------
But don't you see the same trade-off in the end there? You're still
propagating the same amount of data over the same amount of time, so unless
I misunderstand, the costs of such a move should be approximately the same,
just in different areas. The risks as I understand are as follows:

20MB:


   1. Longer per-block propagation (eventually)
   2. Longer processing time (eventually)
   3. Longer sync time

1 Minute:

   1. Weaker individual confirmations (approx. equal per confirmation*time)
   2. Higher orphan rate (immediately)
   3. Longer sync time

That risk-set makes me want a middle-ground approach. Something where the
immediate consequences aren't all that strong, and where we have some idea
of what to do in the future. Is there any chance we can get decent network
simulations at various configurations (5MB/4min, etc)? Perhaps
re-appropriate the testnet?

On Mon, May 25, 2015 at 10:30 PM, Thy Shizzle <thyshizzle@outlook.com>
wrote:

-------------------------------------
Gavin,
They are not analogous.

Increasing performance and making other changes that will help allow
scaling can be done while at small scale or large scale.
Dealing with full blocks and the resultant feedback effects is something
that can only be done when blocks are full.  It's just too complicated a
problem to solve without seeing the effects first hand, and unlike the
block size/scaling concerns, its binary, you're either in the situation
where demands outgrows supply or you aren't.

Fee estimation is one example, I tried very hard to make fee estimation
work well when blocks started filling up but it was impossible to truly
test and in the small sample of full blocks we've gotten since the code
went live, many improvements made themselves obvious.  Expanding mempools
is another issue that doesn't exist at all if supply > demand.   Turns out
to also be a difficult problem to solve.

Nevertheless, I mostly agree that these arguments shouldn't be the reason
not to expand block size, I think they are more just an example of how
immature all of this technology is, and we should be concentrating on
improving it before we're trying to scale it to world acceptance levels.
The saddest thing about this whole debate is how fundamental improvements
to the science of cryptocurrencies (things like segregated witness and
confidential transactions) are just getting lost in the circus debate
around trying to cram a few more users into the existing system sooner
rather than later.



On Mon, Aug 10, 2015 at 10:12 AM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Expanding on pay-with-diff and volatility (closing comment),

Users and miners will have significant difficulty creating and/or
predicting a stable block size (and fee environment) with pay-with-diff
across the months.  The ability of businesses to plan is low.  Chaos and
unpredictability are bad in general for markets and systems.  Thus the
binary conclusion of "not get used" or "volatility"






On Thu, Sep 3, 2015 at 10:31 AM, Jeff Garzik <jgarzik@gmail.com> wrote:

-------------------------------------
I think the overlap of people who want to run a serious mining operation
and people who are unable to afford a slightly above average internet
connection is infinitesimally small.

2015-09-09 20:51 GMT+02:00 Jorge Timón <jtimon@jtimon.cc>:

-------------------------------------
On Mon, Dec 7, 2015 at 4:02 PM, Gregory Maxwell wrote:

ACK.

One of the interesting take-aways from the workshops for me has been
that there is a large discrepancy between what developers are doing
and what's more widely known. When I was doing initial research and
work for my keynote at the Montreal conference (
http://diyhpl.us/~bryan/irc/bitcoin/scalingbitcoin-review.pdf -- an
attempt at being exhaustive, prior to seeing the workshop proposals ),
what I was most surprised by was the discrepancy between what we think
is being talked about versus what has been emphasized or socially
processed (lots of proposals appear in text, but review efforts are
sometimes "hidden" in corners of github pull request comments, for
example). As another example, the libsecp256k1 testing work reached a
level unseen except perhaps in the aerospace industry, but these sorts
of details are not apparent if you are reading bitcoin-dev archives.
It is very hard to listen to all ideas and find great ideas.
Sometimes, our time can be almost completely exhausted by evaluating
inefficient proposals, so it's not surprising that rough consensus
building could take time. I suspect we will see consensus moving in
positive directions around the proposals you have highlighted.

When Satoshi originally released the Bitcoin whitepaper, practically
everyone-- somehow with the exception of Hal Finney-- didn't look,
because the costs of evaluating cryptographic system proposals is so
high and everyone was jaded and burned out for the past umpteen
decades. (I have IRC logs from January 10th 2009 where I immediately
dismissed Bitcoin after I had seen its announcement on the
p2pfoundation mailing list, perhaps in retrospect I should not let
family tragedy so greatly impact my evaluation of proposals...). It's
hard to evaluate these proposals. Sometimes it may feel like random
proposals are review-resistant, or designed to burn our time up. But I
think this is more reflective of the simple fact that consensus takes
effort, and it's hard work, and this is to be expected in this sort of
system design.

Your email contains a good summary of recent scaling progress and of
efforts presented at the Hong Kong workshop. I like summaries. I have
previously recommended making more summaries and posting them to the
mailing list. In general, it would be good if developers were to write
summaries of recent work and efforts and post them to the bitcoin-dev
mailing list. BIP drafts are excellent. Long-term proposals are
excellent. Short-term coordination happens over IRC, and that makes
sense to me. But I would point out that many of the developments even
from, say, the Montreal workshop were notably absent from the mailing
list. Unless someone was paying close attention, they wouldn't have
noticed some of those efforts which, in some cases, haven't been
mentioned since. I suspect most of this is a matter of attention,
review and keeping track of loose ends, which can be admittedly
difficult.

Short (or even long) summaries in emails are helpful because they
increase the ability of the community to coordinate and figure out
what's going on. Often I will write an email that summarizes some
content simply because I estimate that I am going to forget the
details in the near future, and if I am going to forget them then it
seems likely that others might.... This creates a broad base of
proposals and content to build from when we're doing development work
in the future, making for a much richer community as a consequence.
The contributions from the scalingbitcoin.org workshops are a welcome
addition, and the proposal outlined in the above email contains a good
summary of recent progress. We need more of this sort of synthesis,
we're richer for it. I am excitedly looking forward to the impending
onslaught of Bitcoin progress.

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
On Thu, May 07, 2015 at 12:59:13PM -0400, Gavin Andresen wrote:

Sounds like you're saying we are bumping up against a 1MB limit. However
other than the occasional user who has sent a transaction with an
extremely low/no fee, what evidence do we have that this is or is not
actually impacting meaningful usage form the user's point of view?

Do we have evidence as to how users are coping? e.g. do they send time
sensitive transactiosn with higher fees? Are people conciously moving
low value transactions off the blockchain? Equally, what about the story
with companies? You of course are an advisor to Coinbase, and could give
us some insight into the type of planning payment processors/wallets are
doing.  For instance, does Coinbase have any plans to work with other
wallet providers/payment processors to aggregate fund transfers between
wallet providers - an obvious payment channel application.

-- 
'peter'[:-1]@petertodd.org
00000000000000000232164c96eaa6bf7cbc3dc61ea055840715b5a81ee8f6be
-------------------------------------
I know you will ignore this as usual, but the entire replace-by-fee folly
is based on your fundamental misunderstanding of miner incentives.

Miners are *not* incentivised to earn the most money in the next block
possible. They are incentivised to maximise their return on investment.
Making Bitcoin much less useful reduces demand for the bitcoins they are
mining, reducing coinbase and fee income in future blocks. Quite possibly,
to the point where those miners are then making a loss.

Your "scorched earth" plan is aptly named, as it's guaranteed to make
unconfirmed payments useless. If enough miners do it they will simply break
Bitcoin to the point where it's no longer an interesting payments system
for lots of people. Then miners who have equipment to pay off will be
*really* screwed, not to mention payment processors and all the investors
in them.

I'm sure you can confuse a few miners into thinking your ideas are a
super-duper way to maximise their income, and in the process might
facilitate a pile of payment fraud. But they aren't. This one is about as
sensible as your "let's never increase the block size"  and "let's kill SPV
clients" crusades - badly thought out and bad for Bitcoin.
-------------------------------------
Hey Jorge,

He is not saying that. Whatever the reasons for centralization are, it

It's not obvious. Quite possibly bigger blocks == more users == more nodes
and more miners.

To repeat: it's not obvious to me at all that everything wrong with Bitcoin
can be solved by shrinking blocks. I don't think that's going to suddenly
make everything magically more decentralised.

The 8mb cap isn't quite arbitrary. It was picked through negotiation with
different stakeholders, in particular, Chinese miners. But it should be
high enough to ensure organic growth is not constrained, which is good
enough.

I think it would be nice to have some sort of simulation to calculate


Centralization is not a single floating point value that is controlled by
block size. It's a multi-faceted and complex problem. You cannot "destroy
Bitcoin through centralization" by adjusting a single constant in the
source code.

To say once more: block size won't make much difference to how many
merchants rely on payment processors because they aren't using them due to
block processing overheads anyway. So trying to calculate such a formula
won't work. Ditto for end users on phones, ditto for developers who want
JSON/REST access to an indexed block chain, or hosted wallet services, or
miners who want to reduce variance.

None of these factors have anything to do with traffic levels.

What people like you are Pieter are doing is making a single number a kind
of proxy for all fears and concerns about the trend towards outsourcing in
the Bitcoin community. Everything gets compressed down to one number you
feel you can control, whether it is relevant or not.


That isn't what I said at all Jorge. Let me try again.

Setting up an exchange is a lot of risky and expensive work. The motivation
is profit, and profits are higher when there are more users to sell to.
This is business 101.

If you remove the potential for future profit, you remove the motivation to
create the services that we now enjoy and take for granted. Because if you
think Bitcoin can be useful without exchanges then let me tell you, I was
around when there were none. Bitcoin was useless.
-------------------------------------
On Mon, Aug 17, 2015 at 10:13 PM, GC <slashdevnull@hotmail.com> wrote:

check out the #1 podcast in higher education on podomatic.com, you may find
that it's more awareness than paranoia.  There are other resources too,
like GnosticMedia, SchoolSucksProject, and Corbett Report.  These programs
are not addressing bitcoin specifically or even generally.  They simply
show that people with high intelligence do not always have the best
interests of the rest of their species in mind when they engineer
solutions.  For example, taxation is a form of parasitic human cannibalism,
not in the eating of flesh, but in the consuming of life force.  The
methods of farming humans to tolerate such a system are quite advanced.
Learning is the answer.  Defend yourself for the sake of everyone else.

Dave
-------------------------------------
Thanks a lot Cory for following through the test case and producing a patch.

I confirm that libconsensus is now running stable within the Bits of Proof stack,
in-line with test cases we use to verify the java implementation of the script engine,
that are BTW borrowed from Bitcoin Core.

The performance of libconsensus is surprisingly close to the java one.
Validating a 2-of-2 a multi-sig  transaction runs at 1021 ops/sec with java and 1135 ops/sec
in libconsensus. This is on a 2.2GH i7 laptop (4 hyper threading cores used by 8 threads).
Another nice demonstration why one should not trade in advances
of languages for the last decades for a marginal gain of performance with C/C++,
I assume thereby that Bouncy Castle’ EC lib s not superior to OpenSSL's.

I disagree that the problem was rare in the real-world, it should affect any modern
implementation that validates transactions parallel in multiple threads.

Aborting also does not make the problem less severe in my opinion.
Therefore hope the pull will be included into Core with next release.

I can’t assign a timeline to “near future" secp256k1 integration. Can you?

Tamas Blummer



-------------------------------------

I agree and would say that this is the only prediction of bitcoin's future
we can be absolutely sure of: more users equals more decentralization as
long as the cost of running a node is not prohibitively high.

It's incredibly cheap today and won't be too high with any of the current
proposals for the time being. If the "laws" of Nielsen & co suddenly don't
apply anymore, we can always react to that with another hardfork reducing
the rate of growth. Hardforks are way easier if the network is in danger
and the necessary change is obvious and non-controversial (e.g. "reduce
blocksize limit growth").

As long as hobbyists can participate in running the network and it's
affordable for everyone to transact on it, bitcoin will grow and its
decentralization with it, however you measure it.

2015-07-31 14:15 GMT+02:00 Mike Hearn via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

-------------------------------------
It should actually be straightforward to softfork RCLTV in as a negative CLTV.
All nLockTime are >= any negative number, so a negative number makes CLTV a 
no-op always. Therefore, it is clean to define negative numbers as relative 
later. It's also somewhat obvious to developers, since negative numbers often 
imply an offset (eg, negative list indices in Python).

Luke


-------------------------------------

You perform a valuable service with your demonstration, but you
neglected to include the txid's to show that you actually did it.

Your advice is must-follow for anyone relying on an unconfirmed tx: it
must pay a good fee and be highly relayable/minable.


On 7/14/2015 8:29 PM, simongreen--- via bitcoin-dev wrote:


-------------------------------------
I also need to argue for increasing the default block limit to the full 1MB in the next release.  We’re already hitting that limit in bursts of transactions, which puts pressure on the average displayed in the below graphs.

From: raystonn@hotmail.com 
Sent: Monday, June 01, 2015 11:39 AM
To: Mike Hearn ; Adam Back 
Cc: Bitcoin Dev 
Subject: Re: [Bitcoin-development] soft-fork block size increase (extensionblocks)


No, not at these block size limits.  The closer we get to the maximum block size, the slower we grow the average block size toward it.  Number of transactions per day is of course highly correlated with average block size.  Based on these graphs we can expect that hitting 1 million transactions per day will be impossible without raising the maximum block size.


https://blockchain.info/charts/avg-block-size?showDataPoints=false&show_header=true&daysAverageString=7&timespan=all&scale=1&address=



https://blockchain.info/charts/n-transactions?showDataPoints=false&timespan=all&show_header=true&daysAverageString=7&scale=1&address= 



From: Mike Hearn 
Sent: Monday, June 01, 2015 11:01 AM
To: Adam Back 
Cc: Bitcoin Dev 
Subject: Re: [Bitcoin-development] soft-fork block size increase (extensionblocks)

  (at reduced security if it has software that doesnt understand it) 

Well, yes. Isn't that rather key to the issue?  Whereas by simply increasing the block size, SPV wallets don't care (same security and protocol as before) and fully validating wallets can be updated with a very small code change.

  A 1MB client wont even understand the difference between a 1MB and 8MB
  out payment. 

Let's say an old client makes a payment that only gets confirmed in an extension block. The wallet will think the payment is unconfirmed and show that to the user forever, no?

Can you walk through the UX for each case?

  If I am not misremembering, I think you've sided typically
  with the huge block, big data center only end of the spectrum.  

It would be Satoshi, that argued that.

I think there must be a communication issue here somewhere. I'm not sure how this meme has taken hold amongst you guys, as I am the guy who wrote the scalability page back in 2011:

https://en.bitcoin.it/wiki/Scalability


It says:

  The core Bitcoin network can scale to much higher transaction rates than are seen today, assuming that nodes in the network are primarily running on high end servers rather than desktops. 


By "much higher rates" I meant VISA scale and by "high end server" I meant high end by today's standards not tomorrows. There's a big difference between a datacenter and a single server! By definition a single server is not a datacenter, although it would be conventional to place it in one. But even with the most wildly optimistic growth imaginable, I couldn't foresee a time when you needed more than a single machine to keep up with the transaction stream. 


And we're not going to get to VISA scale any time soon: I don't think I've ever argued we will. If it does happen it would presumably be decades away. Again, short of some currently unimagined killer app.


So I don't believe I've ever argued this, and honestly I kinda feel people are putting words in my mouth.


--------------------------------------------------------------------------------
------------------------------------------------------------------------------



--------------------------------------------------------------------------------
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development@lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development
-------------------------------------

I don't intend to do that, and I don't think I am - I know what the
difference between a soft and hard fork is and am not trying to confuse or
blur the two.

To reiterate: this current BIP implements a soft fork. I am not debating
that. I am saying it should use a hard fork instead. This will ensure no
repeat of the P2SH case where invalid blocks were being found for weeks (or
was it months?) after the new rules kicked in, thus exposing SPV wallets
and old nodes to unnecessary risk for no benefit.

Additionally, I am making it clear that there's no consensus for rolling
out the new opcode in this way. As you say, the mechanism has issues. If
you read the comments when I wrote my article, you can see that others
share the same concerns:

https://www.reddit.com/r/Bitcoin/comments/3griiv/on_consensus_and_forks_by_mike_hearn
-------------------------------------
It would appear that the Bitcoin Foundation has decided that their
next two seats would be decided by miners.   (More information
available at: https://bitcoinfoundation.org/forum/index.php?/topic/1255-blockchain-voting/#entry13453
)

Unfortunately, they seem to have not provided the software needed to
participate.

I've taken Luke DashJr's somewhat notorious IsNotorious patch, which
he's used previously to block things like the Horse Stapler Battery
dust-spam attacks and re-purposed it so miners can avoid casting votes
in the election that they don't intend to cast.

Not really an ideal fit, but the code has the benefit of having been
run in production for some time; a necessity for deployment on short
notice.

A patch (against git master, but should apply to 0.10 cleanly at least
and probably other versions) is available at:

https://people.xiph.org/~greg/bcf2012.patch

Let me know if you have any trouble applying it, I'll be glad to do my
civic duty and do what I can to help people participate with the
system was clearly intended by the design.

[Please note that I am relying on some claims from reddit for some of
the candidate addresses (
http://www.reddit.com/r/Bitcoin/comments/2x3ffk/bitcoin_foundation_runoff_voting_live_stats_2015/
) because the official voting software is more or less completely
busted for me and I can only see some of the candidates. If any are
wrong, please let me know.]

Cheers.


-------------------------------------
Am 17.08.2015 um 13:44 schrieb Jorge Timón:

To avoid such discussions.

- oliver

-------------------------------------
On 12/08/2015 09:12 AM, Gavin Andresen via bitcoin-dev wrote:

If such a change is going to be deployed via a soft fork instead of a
hard fork, then the coinbase is the worst place to put the segwitness
merkle root.

Instead, put it in the first output of the generation transaction as an
OP_RETURN script.

This is a better pattern because coinbase space is limited while output
space is not. The next time there's a good reason to tie another merkle
tree to a block, that proposal can be designated for the second output
of the generation transaction.

-------------------------------------
I'm not against it (mostly active committers as you are), my challenge is
getting the PGP keys scattered all over the place.
If you want put your thoughts, then let me know you pgp.

On Fri, Aug 21, 2015 at 12:45 PM, Eric Lombrozo <elombrozo@gmail.com> wrote:

-------------------------------------
2015-10-02 15:14 GMT+02:00 jl2012 via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

Or simply stop pursuing this silly distraction.


-------------------------------------






Manually quoting a reply from Andreas that was sent privately while the 
e-mail list was 2 days delayed delivering messages ....

On 02/25/2015 02:45 AM, Andreas Schildbach wrote:

This bs: is not a bad idea. Is bts: any better/clearer than bs:?


I'm going to agree with Andreas on this. The other thing is we are 
making the resource name derived from the public key, so we are not even 
directly sending the resource name.



We are planning to send a unique public key of the payee via NFC. See 
other e-mails now that the e-mail list finally forwarded them through 
the other day.
















Now for Eric's e-mail... More below.


On 02/24/2015 09:09 PM, Eric Voskuil wrote:


See my comments above.



This may be true. Andreas, do you agree? I feel like there was something 
in your app where it did not currently compare the domain name to domain 
name the digital signature in the payment request used though. Maybe 
this was only for bluetooth though? However, can we trust DNS though? 
Seems like it is not too hard to get an alternate signed certificate for 
a domain name, and if you can serve false DNS and/or change TCP/IP 
routing, then your secure link can break down?






I get what you are saying, but I don't know that 2 taps with the same 
public key is the same as 1000 uses of the same public key?





If you'll check the proposed specification, the headers in each message 
(before the serialized payment request data is sent), are consistent 
from message to message.

https://github.com/AndySchroder/bips/blob/master/tbip-0074.mediawiki#Specification





I think we probably also want to combine new UUID's with Schildbach's 
suggestion (above) to use a new "bs:" (which I suggested maybe "bts:") 
protocol scheme.




-------------------------------------
On Fri, May 29, 2015 at 5:39 PM, Raystonn . <raystonn@hotmail.com> wrote:


I don't think so.  The lower security is the potential centralisation
risk.  If you have your money in the "root" chain, then you can watch it.
You can probably also watch it in a 20MB chain.

Full nodes would still verify the entire block (root + extended).  It is a
"nuclear option", since you can make any changes you want to the rules for
the extended chain.  The only safe guard is that people have to voluntarly
transfer coins to the extended block.

The extended block might have 10-15% of the total bitcoins, but still be
useful, since they would be the ones that move the most.  If you want to
store your coins long term, you move them back to the root block where you
can watch them more closely.

It does make things more complex though.  Wallets would have to list 2
balances.
-------------------------------------

Platforms that support HTTPS but not certificate handling are rare - I know
HTML5 is such a platform but such apps are inherently dependent on the
server anyway and the server can just do the parsing and validation work
itself. If WinRT is such a platform, OK, too bad.

The embedding of the certificates is not arbitrary or pointless, by the
way. It's there for a very good reason - it makes the signed payment
request verifiable by third parties. Effectively you can store the signed
message and present it later to someone else, it's undeniable. Combined
with the transactions and merkle branches linking them to the block chain,
what you have is a form of digital receipt ... a proof of purchase that can
be automatically verified as legitimate. This has all kinds of use cases.

Because of how HTTPS works, you can't easily prove to a third party that a
server gave you a piece of data. Doing so requires staggeringly complex
hacks (see tls notary) and when we designed BIP70, those hacks didn't even
exist. So we'd lose the benefit of having a digitally signed request.

Additionally, doing things this way means BIP70 requests can be signed by
things which are not HTTPS servers. For example you can sign with an email
address cert, an EV certificate i.e. a company, a certificate issued by
some user forum, whatever else we end up wanting. Not every payment
recipient can be identified by a domain name + dynamic session.



That's a bit melodramatic. BitcoinJ is able to use the Android, JRE,
Windows and Mac certificate stores all using the same code or very minor
variants on it (e.g. on Mac you have to specify you want the system store
but it's a one-liner).

Yes, that's not *every* platform. Some will require custom binding glue and
it depends what abstractions and languages you are using.



There is code to do iOS using the Apple APIs here:

https://github.com/voisine/breadwallet/blob/master/BreadWallet/BRPaymentProtocol.m#L391



WinRT is a minority platform in the extreme, and all the other platforms
you mentioned have the necessary APIs. Java abstracts you from them. So I
think you are encountering this problem because you desire to target WinRT
and other platforms with a single codebase. That's an unusual constraint.

AFAIK the only other people who encountered this are BitPay, because they
want to do everything in Javascript which doesn't really provide any major
APIs.



Yes, there are pros and cons to bundling a custom root store.



It can do OCSP checks, yes, although I believe no wallets currently do so.
A better solution would be to implement an OCSP stapling extension to BIP70
though.
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256



On 21 February 2015 17:47:28 GMT-05:00, Jeff Garzik <jgarzik@bitpay.com> wrote:

I think you guys are reading too much into the name... Replace-by-fee is called "replace-by-fee" because it considers whether to replace or not based on fee; the idea came about in an discussion about replacement based on nSequence.

I forget whether it was myself or John Dillon who came up with the name "scorched earth", but it just refers to the game theory behind the *specific* idea of the receiver combating a zeroconf double-spend by sending all the funds to fees. Scorched earth as in "You're trying to defraud me, so I'm not going yo play this game or negotiate, I'm just going to immediately do what is most likely to make you lose the maximum amount of money to punish you for your vandalism."


I'm not so convinced, precisely because we've seen zeroconf fail in pretty bad ways; the people most vulnerable to losses have generally changed the way they operate. (e.g. ATM's that no longer rely on zeroconf security, instead waiting for confirmations, installing cameras, etc.)

My #1 concern right now is person-to-person trading, and people doing that tend to wait for confirmations or otherwise protect themselves. (e.g. reputation systems)


Agreed. Deploying it has been something I've made into a long, drawn out, protracted process for precisely that reason. OTOH I sometimes wonder if I've gone too far with that - the services that themselves try to guarantee zeroconf right now through metrics are themselves highly centralised, and there's a big risk of them driving mining centralisation itself when they fail.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCAAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJU6S2N
AAoJEMCF8hzn9LncrFUH/1xhuPhYJnjTCxhpv2h5ZJOT3wLsrU1oEDmD5fWy/4wG
7ppr3EiHNX7nB42fgeSGZF8fW1VuBjivJa9ra3IvFysFfaD40Kyre2FTnN03+vTC
Upa5ykPzOMqZIHkSf8N1xMbz4SXHHPWu8wPMzj/QGvUpllNiOWn/6Vooqrcp7f6Y
NJFykSq+vDNMOUWCiJG8hhoKiOcZhTH0Aj9qPcGs9WhgsF7wDAX7pg6iO6Y5qmt5
LdFcut2caL6mIxpExm0F9V+lyeam/3gvAU3eecHY77KOxRxFTO1xfQXEJFTWN92h
+M9BXQZ1UifjTZWMzK0kp3SRJuVSXg4KOAapQFBLTzU=
=3Mmw
-----END PGP SIGNATURE-----



-------------------------------------
This sounds like a cool competition; it is also off-topic for this mailing
list, which is focused on bitcoin protocol and reference implementation
development.


On Wed, Sep 30, 2015 at 2:37 AM, Richard Olsen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Sun, Jun 28, 2015 at 10:27 PM, Warren Togami Jr. <wtogami@gmail.com> wrote:

Still broken, in both the web "archives" and the "mbox" archives.
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/009252.html
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June.txt.gz

Note also, as outlined with other issues in the OP, the below attachment
will not appear in the "mbox" archives, thus becoming lost to mirrors and
users rightly attempting to use the "mbox" archives as their canonical source
for local seeding or MUA access.
-------------------------------------
I completely agree and I share your frustration.
The importance of modularization is often disregarded but in my
opinion it has a deep positive impact in the long term: more people
are able to contribute with code and review (in the areas they know
better), the risks associated with each change become more clear
(there was a time when almost any change implied consensus risks),
more alternative code bases can be implemented on top of the basic
ones without fear of consensus bugs, etc.

When I first read some of the code in 2011, I concluded that almost
everything was in main.cpp (which I found ridiculous from a software
engineering perspective). When I started to contribute with code in
2014, main.cpp was still (and still is in my opinion) giant, but the
modularization had greatly improved thanks to changes like moving the
serialization code out of main (thank you very much for that). We
still have a lot of work ahead, but we've certainly advanced a lot.

Unfortunately we cannot force reviewers to pay more attention to
modularization PRs, many of them are usually more interested in
changes that add or remove functionality in the short term. This
problem gets exacerbated when modularization changes are required to
be done in small increments to make them more easily reviewable and
less disruptive to other open PRs, since it's harder for people to see
the big picture and the rationale for those small changes (that often
don't hcange functionality or performance at all).

I know we are not alone on this and people like Wladimir, Pieter, Cory
and Jonas Schnelli (at least, probably more people do) deeply care
about modularization, even if I subjectively and selfishly interpret
the lack of review on some of my PRs as a symptom of the opposite.
So I suggest that people who think this is a high priority join and
review each other's PRs on the subject.

Currently I focus on 3 modularization areas:

1) Chainparams: supporting multiple chains (ie multiple testchains is
all what Bitcoin cares about) is a great goal but there's still many
barriers to create a new testchain. I started this work with #3824,
but even after #6382 there are still more things to do.

2) Consensus: separating the consensus code, Matt Corallo had the idea
of also exposing it in libconsensus. I started with #3839, the latest
things I still have open are #6591 and #6445, please review.

3) Policy: separate node local policy code. Luke Dashjr started with
#5071, I started with #6335 (after several failed attempts), the next
little step blocking many other changes I have ready for way too long
is #6068 (#6424 also helps), please review.

I know Jonas Schnelli is focusing on the wallet. Cory Fields has
recently focused on checkpoints and chainparams.

Now that I know that you also care about modularization I will ask you
for review as well, I hope not to be too annoying like I've been with
Wladimir and Cory some times (and I usually am with some of my
coworkers at blockstream). Please do the same with me: point me to any
modularization PR you have opened.

Regarding your next post, I agree that an additional "Layer" field in
BIPs could be useful. Maybe you should start a BIP for that?

On Sun, Aug 23, 2015 at 3:23 AM, Eric Lombrozo <elombrozo@gmail.com> wrote:

-------------------------------------

Yeah, though FYI Luke informed me last week that I somehow managed to take
out the change to the user-agent string in Bitcoin XT, presumably I made a
mistake during a rebase of the rebranding change. So the actual number of
XT nodes is a bit higher than counting user-agent strings would suggest.

I sort of neglected XT lately. If we go ahead with this then I'll fix
things like this.
-------------------------------------
By the way, now that I remember why I subscribed to the libbitcoin
list I want to share it with you.
I met Amir Taaki in person in a spanish hackmeeting and had the chance
to talk a lot with him, very interesting person whose input in this
blocksize matter I would greatly appreciate. He explained some of his
concerns with Bitcoin Core (Bitcoin-qt at the time) and he
specifically named 2 persons: Mike Hearn and Gavin Andresen. If I
remember correctly, Hearn had recently proposed a blacklisting scheme
for Bitcoin.

I remember I said something along the lines:
"Mike Hearn has certainly proposed some nasty things but I don't think
other devs will ever accept that kind of changes in Bitcoin-qt.
Regarding Gavin, I believe he is someone that can be trusted even if
he visited the CIA. If anything, I think he is overly conservative
about some changes, but that's very understandable given how fragile
Bitcoin is (specially at this early stage)".

Looking back, I now realize that his concerns were not exaggerated at
all and I was clearly wrong thinking Gavin was overly conservative.
He was also worried about the payment protocol and we agreed to
disagree there (maybe I should read all the payment protocol stuff
more deeply).

I don't want this to be taken as an argument of authority "Mike and
Gavin cannot be trusted because Amir didn't trust them", just as a
curious anecdote.
Amir, I wouldn't like to put words in your mouth: that's why I cc'ed
you so you can correct me in case my memory is failing.

-------------------------------------
On Sat, May 09, 2015 at 12:09:32PM -0500, Jim Phillips wrote:

You can't assume that UTXO growth will be driven by walles at all; the
UTXO set's global consensus functionality is incredibly useful and will
certainly be used by all manner of applications, many having nothing to
do with Bitcoin.

As one of many examples, here's a proposal - with working code - to use
the UTXO set to get consensus over TLC certificate revocations. The
author, Christopher Allen, was one of the co-authors of the SSL
standard:

https://github.com/ChristopherA/revocable-self-signed-tls-certificates-hack

There's nothing we can do to stop these kinds of usages other than
forcing users to identify themselves to get permission to use the
Bitcoin blockchain. Using the Bitcoin blockchain gives their users
significantly better security in many cases than any alternatives, so
there's strong incentives to do so. Finally, the cost many of these
alternative uses are willing to pay pre UTXO/transaction is
significantly higher than the fees many existing Bitcoin users can pay
to make transactions.

-- 
'peter'[:-1]@petertodd.org
00000000000000000e7980aab9c096c46e7f34c43a661c5cb2ea71525ebb8af7
-------------------------------------

Next two weekly developer meetings would fall on:

- Thursday December 24th
- Thursday December 31th

In my timezone they're xmas eve and new year's eve respectively, so at least I
won't be there, and I'm sure they're inconvenient for most people.

So: let's have a two week hiatus, and continue January 7th.

Wladimir

-------------------------------------
Note: my stupid email client didn't indent Peter Todd's quote correctly. 
The first paragraph is his, the second is my response.

------ Original Message ------
From: "Eric Lombrozo" <elombrozo@gmail.com>
To: "Peter Todd" <pete@petertodd.org>; "Emin Gün Sirer" 
<el33th4x0r@gmail.com>
Cc: nbvfour@gmail.com; "Bitcoin Dev" 
<bitcoin-dev@lists.linuxfoundation.org>
Sent: 12/26/2015 12:23:38 AM
Subject: Re[2]: [bitcoin-dev] We need to fix the block withholding 
attack

-------------------------------------

That is an interesting point.  That is a feature of Bitcoin, not a bug. 
  If the user did have rights to sue someone then the system would not 
be decentralized.  User rights = someone else has a liability for 
violating those rights.

As it is now a user would have the right to sue all the miners, node 
operators, and developers collectively.  Of course that is not realistic 
which is the way a decentralized system should be.  If you want to try 
to define specific entities that have liability then they must be in 
control or otherwise they would not be liable.

Russ







-------------------------------------
On 8/20/2015 11:07 PM, Peter Todd via bitcoin-dev wrote:



Just checked mine and found 20.4% bitcoinj connections.



-------------------------------------
On 11/5/15, Eric Voskuil via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

I disagree. I think blockchain APIs are a good thing for
decentralization. There aren't just 3 or 4 blockexplorer APIs out
there, there are dozens. Each API returns essentially the same data,
so they are all interchangeable. Take a look at this python package:
https://github.com/priestc/moneywagon

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I suggest revising these items for clarity (and I'm guessing on the first
one)

    Calculate hardLimit by examining the coinbase scriptSig votes of the
previous 12,000 blocks, and taking the 20th percentile.
    A new hardLimit may not increase or decrease by more than 1.2x beyond
the prior hardLimit.

to:

    The new hardLimit is calculated by sorting the coinbase scriptSig votes
of the last 12,000 blocks from lowest to highest and using the vote of the
2400th block.
    If the vote of the 2400th block is a change of less than 20%, use it as
the new hardLimit.  Otherwise, change the hardLimit to be closer to that
vote, to either 120% or 80% of the current hardLimit.

I don't understand #5, 75% rule.  Shouldn't invalid version 4 blocks always
be rejected?

notplato
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQEcBAEBAgAGBQJV58/5AAoJEL8dSijmIbHt16IH/0jAr3v1HjWW7N1awNxeAABs
GIvOFYuZAcPkZvWZQc4JRAppglqeBfYqWl2gpyywSBK1SXjsY8zdo3t7xAK/IJfB
05hnv1GGutG3dLTzJBEXaPx62SLukepC1pzEH7rlwWvVuE9zcRqVE1eGbBEUjA9c
sGPr0z9BNeLoTbllyl3Jndz9N2Vnd6bBTxRgBlfkm/Y5ovc+GhyKZyX3Pdmj5Pga
E6foOsvqNXQJqPl8WCODsnfPSshyb7YRNFrBB9A+tpjvj4UMc8PxOpL6IX/nJpOt
jlfRoKVw2YBEodvda+9P6S54GlGFazyHhwJ11F5YCNnWW1bKoQrqJU6ofgmyxMM=
=QWra
-----END PGP SIGNATURE-----


On Wed, Sep 2, 2015 at 8:33 PM, Jeff Garzik via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
Any attempt to 'fix' this problem, would most likely require changes to all
mining software, why not just make mining more decentralized in general?

For example, allow anyone to submit proofs of work to Bitcoind that are
some fraction of the network difficulty and receive payment for them if
they're valid.  This would also encourage the proliferation of full nodes
since anyone could solo mine again.  Then, the next coinbase transaction
could be split among, say, the top 100 proofs of work.

Eligius already does their miner payouts like this.

If you want to fix an issue with mining, fix the selfish mining issue first
as it's a much larger and more dangerous potential issue.

I don't believe it was ever clearly established whether Eligius suffered a
block withholding attack or was just the victim of a miner with (what was,
at the time) a large amount of faulty hardware, however, from the
Bitcointalk threads at the time I believe it was assumed to be the latter.

--Adam


On Sat, Dec 19, 2015 at 8:44 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Bitcoin Core version 0.10.1 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.10.1/>

The distribution is also available as torrent:

   https://bitcoin.org/bin/bitcoin-core-0.10.1/bitcoin-0.10.1.torrent

   magnet:?xt=urn:btih:b6f8da60aaf2007cd6db631637951ae673e31044&dn=bitcoin-core-0.10.1&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Fopen.demonii.com%3A1337&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2F

The source code can be found in git under the tag `v0.10.1`, or in `bitcoin-0.10.1.tar.gz` in the distribution.

This is a new minor version release, bringing bug fixes and translation 
updates. It is recommended to upgrade to this version.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

Upgrading and downgrading
=========================

How to Upgrade
--------------

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes for older versions), then run the
installer (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or
bitcoind/bitcoin-qt (on Linux).

Downgrade warning
------------------

Because release 0.10.0 and later makes use of headers-first synchronization and
parallel block download (see further), the block files and databases are not
backwards-compatible with pre-0.10 versions of Bitcoin Core or other software:

* Blocks will be stored on disk out of order (in the order they are
received, really), which makes it incompatible with some tools or
other programs. Reindexing using earlier versions will also not work
anymore as a result of this.

* The block index database will now hold headers for which no block is
stored on disk, which earlier versions won't support.

If you want to be able to downgrade smoothly, make a backup of your entire data
directory. Without this your node will need start syncing (or importing from
bootstrap.dat) anew afterwards. It is possible that the data from a completely
synchronised 0.10 node may be usable in older versions as-is, but this is not
supported and may break as soon as the older version attempts to reindex.

This does not affect wallet forward or backward compatibility.

Notable changes
===============

This is a minor release and hence there are no notable changes.
For the notable changes in 0.10, refer to the release notes for the
0.10.0 release at https://github.com/bitcoin/bitcoin/blob/v0.10.0/doc/release-notes.md

0.10.1 Change log
=================

Detailed release notes follow. This overview includes changes that affect external
behavior, not code moves, refactors or string updates.

RPC:
- `7f502be` fix crash: createmultisig and addmultisigaddress
- `eae305f` Fix missing lock in submitblock

Block (database) and transaction handling:
- `1d2cdd2` Fix InvalidateBlock to add chainActive.Tip to setBlockIndexCandidates
- `c91c660` fix InvalidateBlock to repopulate setBlockIndexCandidates
- `002c8a2` fix possible block db breakage during re-index
- `a1f425b` Add (optional) consistency check for the block chain data structures
- `1c62e84` Keep mempool consistent during block-reorgs
- `57d1f46` Fix CheckBlockIndex for reindex
- `bac6fca` Set nSequenceId when a block is fully linked

P2P protocol and network code:
- `78f64ef` don't trickle for whitelisted nodes
- `ca301bf` Reduce fingerprinting through timestamps in 'addr' messages.
- `200f293` Ignore getaddr messages on Outbound connections.
- `d5d8998` Limit message sizes before transfer
- `aeb9279` Better fingerprinting protection for non-main-chain getdatas.
- `cf0218f` Make addrman's bucket placement deterministic (countermeasure 1 against eclipse attacks, see http://cs-people.bu.edu/heilman/eclipse/)
- `0c6f334` Always use a 50% chance to choose between tried and new entries (countermeasure 2 against eclipse attacks)
- `214154e` Do not bias outgoing connections towards fresh addresses (countermeasure 2 against eclipse attacks)
- `aa587d4` Scale up addrman (countermeasure 6 against eclipse attacks)
- `139cd81` Cap nAttempts penalty at 8 and switch to pow instead of a division loop

Validation:
- `d148f62` Acquire CCheckQueue's lock to avoid race condition

Build system:
- `8752b5c` 0.10 fix for crashes on OSX 10.6

Wallet:
- N/A

GUI:
- `2c08406` some mac specifiy cleanup (memory handling, unnecessary code)
- `81145a6` fix OSX dock icon window reopening
- `786cf72` fix a issue where "command line options"-action overwrite "Preference"-action (on OSX)

Tests:
- `1117378` add RPC test for InvalidateBlock

Miscellaneous:
- `c9e022b` Initialization: set Boost path locale in main thread
- `23126a0` Sanitize command strings before logging them.
- `323de27` Initialization: setup environment before starting QT tests
- `7494e09` Initialization: setup environment before starting tests
- `df45564` Initialization: set fallback locale as environment variable

Credits
=======

Thanks to everyone who directly contributed to this release:

- Alex Morcos
- Cory Fields
- dexX7
- fsb4000
- Gavin Andresen
- Gregory Maxwell
- Ivan Pustogarov
- Jonas Schnelli
- Matt Corallo
- mrbandrews
- Pieter Wuille
- Ruben de Vries
- Suhas Daftuar
- Wladimir J. van der Laan

And all those who contributed additional code review and/or security research:
- 21E14
- Alison Kendler
- Aviv Zohar
- Ethan Heilman
- Evil-Knievel
- fanquake
- Jeff Garzik
- Jonas Nick
- Luke Dashjr
- Patrick Strateman
- Philip Kaufmann
- Sergio Demian Lerner
- Sharon Goldberg

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).



-------------------------------------
<p dir="ltr">As I said, this doesn't help against malicious nodes.  But it helps on a case such as today's.<br>
</p>
<div class="gmail_quote">On 3 Jul 2015 10:45 pm, Peter Todd &lt;pete@petertodd.org&gt; wrote:<br type='attribution'><blockquote class="quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">



<div>
<div>On Fri, Jul 03, 2015 at 10:43:14PM -0700, Raystonn wrote:<br />
&gt; &lt;p dir&#61;&#34;ltr&#34;&gt;The SPV clients should be checking node versions.  This is for wallet authors to implement.  End-users should just stay current with their chosen wallet software.&lt;br&gt;<br />
<br />
Nodes can and do lie about what version they are all the time.<br />
<br />
Fact is, SPV means you&#39;re trusting other people to check the rules for<br />
you. In this particular case bitcoinj could have - and should have -<br />
checked the BIP66 soft-fork rules, but in general there&#39;s no easy<br />
solution to this problem.<br />
<br />
-- <br />
&#39;peter&#39;[:-1]&#64;petertodd.org<br />
00000000000000000a43884e675843f56df90feffeabf56c4e7350f96b623f00<br />
</div>
</div>

</blockquote></div>
-------------------------------------

On Dec 9, 2015, at 7:50 AM, Jorge Timn <jtimon@jtimon.cc> wrote:

Okay, I might just not understand how a sigwit payment would look to current software yet. I'll add learning about that to my to-do list...
-------------------------------------

On 6/27/2015 at 2:04 PM, "Jorge Timón" <jtimon@jtimon.cc> wrote:


It is, until it actually happens. Before that, anything is a speculation. That's why risk is attached to both "doing nothing" and "raising the limit".

For example, there's another risk that a lot of people will be disappointed in a system that can't scale (or adapt to any significant changes, for that matter).

Yes, there is the "exit" option, but that path would probably be a lot messier and unwarranted.

Various people perceive these risks differently and there is no clear mechanism currently to somehow gauge what the majority wants. So it's tempting to just give up and say: let's do nothing.

In this situation, doing a "software fork" seems like the only way to actually see how many people/interests are in favor of bigger blocks.

(Whether the majority has a moral right to dictate the minority is a tough philosophical question, which should probably be left out of this discussion :)



-------------------------------------
On Fri, Jun 12, 2015 at 01:21:46PM -0400, Gavin Andresen wrote:

Then simulate first the relay network assuming 100% of txs use it, and
secondly, assuming 100%-x use it.

For instance, is it in miners' advantage in some cases to sabotage the
relay network? The analyse say yes, so lets simulate that. Equally even
the relay network isn't instant.

-- 
'peter'[:-1]@petertodd.org
0000000000000000127ab1d576dc851f374424f1269c4700ccaba2c42d97e778
-------------------------------------
So I checked, and the code described *does not* run when behind a
proxy of any kind, including tor:

https://github.com/bitcoinxt/bitcoinxt/commit/73c9efe74c5cc8faea9c2b2c785a2f5b68aa4c23#diff-11780fa178b655146cb414161c635219R265

At least based on my admittedly weak understanding of how the internal works.

Hopefully I save the next reader of your post from also having to dig
around to find the code and realize this is a false alert.

On Tue, Aug 18, 2015 at 6:36 PM F L via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:


-------------------------------------
Hello,

I've just uploaded Bitcoin Core 0.10.1rc2 executables to:

https://bitcoin.org/bin/bitcoin-core-0.10.1/test/

The source code can be found in git under the tag 'v0.10.1rc2'

The only change in comparison to rc1 is a fix by Gavin Andresen:

- `1c62e84` Keep mempool consistent during block-reorgs

Thanks to everyone that participated in the gitian build process,

Wladimir



-------------------------------------
Could we see a PR that adds it to BIP 66?   Perhaps we'd all agree quickly
that its so simple we can just add it...
In either case it doesn't seem strictly necessary to me that it was
non-standard before it becomes a soft-fork...


On Tue, Feb 3, 2015 at 7:00 AM, Wladimir <laanwj@gmail.com> wrote:

-------------------------------------
LN transactions are a substitute good for on-chain transactions.

Therefore, demand for on-chain transactions will decrease as a result of
LN, meaning that fees will be lower than they would otherwise be.

However, the two are also perfect compliments, as LN transactions cannot
take place at all without periodic on-chain transactions.

The demand for *all* Bitcoin transactions (LN and otherwise) is itself a
function of innumerable factors, one of which is the question "Which
form of money [Bitcoin or not-Bitcoin] do I think my trading partners
will be using?". By supporting a higher rate of (higher-quality) Bitcoin
transactions, the net result is highly uncertain, but will probably be
that LN actually increases trading fees.

On 10/14/2015 6:14 AM, s7r via bitcoin-dev wrote:
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
Btc Drak via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:

OK, having implemented lightning test code against the initial proposal,
I can give the following anecdata:

- I screwed up inversion in my initial implementation.  Please kill it.

- 256 second granularity would be be fine in deployment, but a bit
  painful for testing (I currently use 60 seconds, and "sleep 61").  64
  would work better for me, and works roughly as minutes.

- 1 year should be sufficient as a max; my current handwave is <= 1 day
  per lightning hop, max 12 hops, though we have no deployment data.

- We should immediately deploy an IsStandard() rule which insists that
  nSequence is 0xFFFFFFFF or 0, so nobody screws themselves when we
  soft fork and they had random junk in there.

Aside: I'd also like to have nLockTime apply even if nSequence !=
0xFFFFFFFF (another mistake I made).  So I'd like an IsStandard() rule
to say it nLockTime be 0 if an nSequence != 0xFFFFFFFF.  Would that
screw anyone currently?

Thanks,
Rusty.

-------------------------------------
Thanks for CC'ing me Mike. Having trouble receiving maillist list posts.

Even if a user could get the BIP70 URL in the URI, they would still need
internet to access the URL. This BLE spec doesn't preclude BIP70, but can
work with it while still allowing individuals without a certificate to
broadcast a request.

The issue of confused payments becomes less so if the Recipient broadcasts
a name along with the 10 digit public addr prefix. Only if there is a name
conflict will the user have to be concerned with the prefix. The name can
be something like

Mikes Coffee #1 and it can have a "Register #1" at the counter. A customer
facing screen can also show the 10 digit prefix.


[image: logo]
*Paul Puey* CEO / Co-Founder, Airbitz Inc
+1-619-850-8624 | http://airbitz.co | San Diego
<http://facebook.com/airbitz>  <http://twitter.com/airbitz>
<https://plus.google.com/118173667510609425617>
<https://go.airbitz.co/comments/feed/>  <http://linkedin.com/in/paulpuey>
<https://angel.co/paul-puey>
*DOWNLOAD THE AIRBITZ WALLET:*
  <https://play.google.com/store/apps/details?id=com.airbitz>
<https://itunes.apple.com/us/app/airbitz/id843536046>




On Thu, Feb 5, 2015 at 12:28 PM, Mike Hearn <mike@plan99.net> wrote:

-------------------------------------


Number of posts, August:

72,	Jorge Timón
36,	Hector Chu
32,	Thomas Zander
27,	Pieter Wuille
24,	Eric Lombrozo
23,	Mark Friedenbach
18,	Adam Back
18,	Btc Drak
18,	Peter Todd
17,	jl2012 
16,	odinn
15,	Gavin Andresen
12,	Venzen Khaosan
12,	Michael Naber
11,	Anthony Towns
10,	Tom Harding
10,	Gregory Maxwell

Everybody else less than 10.


-------------------------------------
Hi,

Hope it is OK to post this on the list, was not sure where else to post 
for answers from Bitcoin-Qt client developers.

As part of the Open Bitcoin Privacy Project’s ongoing wallet privacy 
measurement efforts, we’ve selected the Bitcoin-Qt client v0.11.0 for 
inclusion into our 2015 mid year survey.

While our volunteers will be performing a series of functional tests by 
interacting with your application directly, several of the features we’d 
like to examine are not easily discernible by non-developers, and for 
this reason we’re asking for your help.

If you can answer the following questions about your wallet’s behavior 
it will assist us with the process of accurately rating your wallet’s 
privacy features.

	Transaction Formatting

1.	Does your application take any steps to create ambiguity between 
transactions which unavoidably spend from multiple addresses at the same 
time and intentional mixing transactions?
2.	What algorithms does your application use for ordering inputs and 
outputs in a transaction? In particular, how do you handle the change 
output and do you take into account common practices of other wallet 
applications when determining ordering?
3.	Does your application minimize the harmful effects of address reuse 
by spending every spendable input (“sweeping”) from an address when a 
transaction is created?
4.	Does your application fully implement BIP 62?

Mixing

5.	If your application supports mixing:
a.	What is the average number of participants a user can expect to 
interact with on a typical join transaction?
b.	Does your application attempt to construct join transactions in a way 
that avoids distinguishing them from non-join transactions?
c.	Does your application perform any kind of reversibility analysis on 
join transactions prior to presenting them to the user for confirmation?
d.	Is the mixing technique employed secure against correlation attacks 
by the facilitator, such as a CoinJoin server or off-chain mixing 
service?
e.	Is the mixing technique employed secure against theft of funds by the 
facilitator or its participants?

Donations

6.	If your application has a fee or donation to the developers feature:
a.	What steps do you take to make the donations indistinguishable from 
regular spend in terms of output sizes and destination addresses?

Balance Queries and Tx Broadcasting

7.	Please describe how your application obtains balance information in 
terms of how queries from the user’s device can reveal a connection 
between the addresses in their wallet.
a.	Does the application keep a complete copy of the blockchain locally 
(full node)?
b.	Does the user’s device provide a filter which matches some fraction 
of the blockchain while providing a false positive rate (bloom or prefix 
filters)?
i.	If so, approximately what fraction of the blockchain does the filter 
match in a default configuration (0% - 100%)?
c.	Does the user’s device query all of their addresses at the same time?
d.	Does the user’s device query addresses individually in a manner that 
does not allow the query responder to correlate queries for different 
addresses?
e.	Can users opt to obtain their balance information via Tor (or 
equivalent means)?
8.	Does the applications route outgoing transactions independently from 
the manner in which it obtains balance information? Can users opt to 
have their transactions submitted to the Bitcoin network via Tor (or an 
equivalent means) independently of how they obtain their balance 
information?
9.	If your application supports multiple identities/wallets, does each 
one connect to the network as if it were completely independent from the 
other?
a.	Does the application ever request balance information for addresses 
belonging to multiple identities in the same network query?
b.	Are outgoing transactions from multiple identities routed 
independently of each other to the Bitcoin network?
c.	When an identity/wallet is deleted, does the deletion process 
eliminate all evidence from the user's device that the wallet was 
previously installed?

	Network Privacy

10.	When a user performs a backup operation for their wallet, does this 
generate any automatic network activity, such as a web query or email?
11.	Does your application perform any lookup external to the user’s 
device related to identifying transaction senders or recipients?
12.	Does you application connect to known endpoints which would be 
visible to an ISP, such as your domain?
13.	If your application connects directly to nodes in the Bitcoin P2P 
network, does it either use an unremarkable user agent string (Bitcoin 
Core. BitcoinJ, etc), or randomize its user agent on each connection?

	Physical Access

14.	Does the application uninstall process for your application 
eliminate all evidence from the user's device that the application was 
previously installed? Does it also eliminate wallet data?
15.	Does your application use techniques such as steganography to store 
persistent wallet metadata in a form not identifiable as belong to a 
Bitcoin wallet application?
16.	Please describe the degree to which users can use passwords/PINs to 
protect their data:
a.	Can the user set a password/PIN to protect their private keys?
b.	Can the user set a password/PIN to protect their public keys and 
balance information?
c.	Can the user set a password/PIN to encrypt other wallet metadata, 
such as address books and transaction notes?
d.	Does the application use a single password/PIN to cover all protected 
data, or does it allow the use of multiple passwords/PINs?

Custodianship

17.	Do you as a wallet provider ever have access to unencrypted copies 
of the user’s private keys, public keys, or any other wallet metadata 
which may be used to associate a user with their transactions or 
balances?

	Telemetry Data

18.	If your application reports telemetry data, such as usage 
information or automatic crash reporting, does the user have the 
opportunity to review and approve all information transmitted before it 
is sent?

	Source Code and Building

19.	Can a user of your application compile the application themselves in 
a manner that produces a binary version identical to the version you 
distribute (deterministic build system)?

Thank you for assisting us with this effort to measure privacy progress 
in the Bitcoin wallet space.  If at all possible, please return this 
survey before 2015/08/13 to ensure the score for your application will 
be as accurate as possible.

Sincerely,

Wei
Open Bitcoin Privacy Project Contributor

-------------------------------------


Agreed. But I believe the economic and security arguments I gave regarding fees and incentives still hold and are largely separate from the scalability issue. Please correct me if I overlooked something.



An increase in block size at this time will exacerbate security concerns around nodes relying on other nodes to validate (particularly miners and wallets). It’s not really a matter of having limited developer resources that need to be budgeted, as you seem to suggest.

Regarding developments on properly handling fees, there must exist the economic need for it before there’s an earnest effort to solve it. Increasing the block size right now will, in all likelihood, delay this effort. I’d much prefer to first let the fee market evolve because it’s a crucial component to the protocol’s design and its security model…and so we can get a better sense for fee economics. Then we might be able to figure out better approaches to block size changes in the future that makes sense economically…perhaps with mechanisms that can dynamically adjust it to reflect resource availability and network load.
-------------------------------------
On 6/20/2015 5:54 PM, Eric Lombrozo wrote:

There's no need to worry about causing more problems by relaying
double-spends.  After a year of watching, it's clear that already only
20% of hash power strictly obeys first-seen.

http://i.imgur.com/0bYXrjn.png

It may be surprising that
 - The period of ambiguity is very short - just 2 seconds
   (this makes sense, given the .5s median propagation time)
 - Fast double-spends between 2 and 15 seconds are less successful
 - The steady-state 80% respend success rate is reached after just 15
seconds

The >30s data point includes txes that were respent after a long time,
sometimes months.  Those longer-term respends are to be expected, as
people reclaim stuck txes.

Paying attention to double-spends is an opportunity for wallets and
merchants .  With 140 Bitcoin XT nodes online, you're probably already
receiving them.  Most wallets, including vanilla core, don't even alert
when a double-spend of a wallet transaction appears in a block - even
though there may still be time to withhold delivery of the goods/services.

If FSS RBF gains miner share, fewer successful zero-conf double-spends
will occur.  Only radical twisted logic finds that to be an undesirable
result.




-------------------------------------
We should make sure to consider how BIP34 affects normalized transaction ids, since the height of the block is included in the scriptSig ensuring that the txid will be different. We wouldn't want to enable replay attacks in the form of spending coinbase outputs in the same way they were spent from a previous block. 

So maybe normalized txids should strip the scriptSigs of all transactions except for coinbase transactions? This seems to make sense, since coinbase transactions are inherently not malleable anyway. 

Also, s7r linked to my 'Build your own nHashType' proposal (although V2 is here: https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal_v2.md). I just wanted to add that I think even with normalized ids, it could still be useful to be able to apply these flags to choose which parts of the transaction become signed. I've also seen vague references to some kind of a merklized abstract syntax tree, but am not fully sure how that would work. Maybe someone on here could explain it? 

Best,
Stephen





-------------------------------------
So, if consensus shouldn't really be between the developers (which is 
fine), should we empower users to control consensus? I've been working 
on a fork framework anyway, which can support reasonably arbitrary 
consensus changes (currently against block height, but moving towards 
block time). Theoretically it could be modified to load consensus 
parameters (which block size would have to be added to) from disk at 
startup, rather than having them hard-coded.

Is that considered desirable? Will raise as a PR if so. If not, where do 
we draw a line between developer and user consensus?

Ross

On 22/07/2015 17:52, Pieter Wuille via bitcoin-dev wrote:

-------------------------------------
A minimum block size does nothing to prevent the problems that come
from schism hardforks.
But also a minimum block size can be trivially cheated as recently
explained on this list:

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010317.html

"[...] miners can just pay to themselves to follow the minimum size
block rule without risking anything.
As long as they have a single matured satoshi they can just pay to
themselves with it as many times as they need in the same block."

It is good to search previous post before proposing or asking
something (it could have been proposed/asked earlier):

http://www.catb.org/esr/faqs/smart-questions.html


On Sun, Aug 23, 2015 at 1:30 AM, Bdimych Bdimych via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
Hello,

There was overwhelming response that weekly IRC meetings are a good thing.

Thanks to the doodle site we were able to select a time slot that everyone (that voted) is available:

    Thursday 19:00-20:00 UTC, every week, starting September 24 (next Thursday)

I created a shared Google Calendar here:
https://www.google.com/calendar/embed?src=MTFwcXZkZ3BkOTlubGliZjliYTg2MXZ1OHNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ

The timezone of this calendar is Reykyavik (Iceland) which is UTC+0. However, you can use the button on the lower right to add the calendar to your own calendar, which will then show the meeting in your own timezone.

See you then,

Wladimir


-------------------------------------


Some of the risks are pretty hard to quantify. But I think this misses the bigger point - it very well *might* be possible to safely raise this limit or even get rid of it by first fixing some serious issues with the protocol. But over six years into the project and these issues continue to be all-but-ignored by most of the community (including at least a few core developers). I don’t think it’s really a matter of whether we agree on whether it’s good to raise the block size limit, Gavin. I think it’s a matter of a difference in priorities.

- Eric
-------------------------------------
On Thu, Jun 25, 2015 at 06:33:44PM -0400, Peter Todd wrote:

I've opened a pull-req to deploy CHECKLOCKTIMEVERIFY via the
IsSuperMajority() mechanism:

    https://github.com/bitcoin/bitcoin/pull/6351

    Final step towards CLTV deployment on mainnet.

    I've copied the logic and tests from the previous BIP66 (DERSIG)
    soft-fork line-by-line for ease of review; any code review applicable to
    BIP66 should be applicable to BIP65.

    Once merged I'll prepare a backport of the soft-fork logic for the
    v0.10.x branch as well.

-- 
'peter'[:-1]@petertodd.org
00000000000000000dbc12bdcb4d0a340272edd649d24849f86a20d075f0dba1
-------------------------------------

Thank you for this reference.  Interesting to see that there is a tool to generate a vanity bitcoin address.

I am still researching viruses that are designed to manipulate a bitcoin address.  I suspect they are primitive in that they use a hardcoded rogue bitcoin address as opposed to dynamically generating one.

As a start, this would help protect against malware that uses a static rogue bitcoin address.  The next thing would be for the malware to brute-force the legitimate bitcoin address and generate a rogue bitcoin address that would produce the same 8 digit code.  Curious to know how long this brute force would take?  Or perhaps, before converting to 8 digits there is some other hashing function that is performed.

Brian Erdelyi
-------------------------------------
jl2012@xbt.hk writes:

Ah thanks!  I missed the version bump in BIP68.


Yes, but Mark pointed out that it has uses, so I withdraw the
suggestion.

Thanks,
Rusty.

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

June 28th, 2015
Mailman's obscure_addresses option is now disabled.  It has been pretty
useless
as a spam mitigation measure anyway.

This is a test message to see if mailman continues to break --clearsign GPG
signatures for messages that contain an e-mail address like test@example.com
.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQQcBAEBCAAGBQJVkKy7AAoJELEXnrc0fcENmiIf/1Qgb/XFDmGl5RsZnMWh+kCX
EBjT/ouufgURTrOt6XPhwnxJAYMKPtobg4MvjosqKFNKzyz3Io9TxkWXFXFLajLt
uZ/RPck0b6qYPXW02m6on1Zj8VIKrREtm4CEH/2q7iHkjsWIHULo+K6emrJQtIOs
DzhUEniPPVDJVIq2pZJGKHq87ydmnFMYhXjzCu2e0N7CBFw/uxtUEsvkd0uuognm
YVjnrCjS8CLzNgEkx8z6R1WWz7psDQeLr+d8BP/6TVrgEOaeX4pAy9mhqdPjGutV
C6X3SyMlSA/PDJFqzT5Upe4UKwKWGq7AOBz7OpzpYIqJ69NKsIjCgh+t6qCisDZO
O3XFUIidmKZsWE422InFLhUtqzwZHq56J/eKKiYz53BSYtTOPIs4+RPlm1Gzs7gP
cEqoDYobdlT2H1vCvNficcbH5nWmooErUxT06tXjGqfPHlm+OVuOyVTwuin+gD5H
winqtNlkSCbG1MjhUoLEJVxLHP7t6i5e6EKPQrvP0XycKuO2PMlVkYYFBGMKEFF/
sL68wyV/0PzGRA7ggcBmcOGHC7ys2sOroLZCWNDnOrQnRFHvQgDCQpj3uMFMO7yf
8E+0CIKO0vB/Ljxse51+s/NZTX8uTZu525+hSG8cuLY1FTl5Zk3oRH1HGUJ9wbWh
Hjx6827uh543+AHWgBwEpQ/8LkSlc0UN6txNmYn4DJgEksqP80iaPkvXAEesSUJ8
OtAPrWOMZXcSsp7HZkDXuJyMUerGvZcdIVvVNsLOWLWa2c0kyDdxubGflhGN/Hkq
67pJ5J/aeraw7BdfRi78CUWYXpVBlMaQ60jT2LV9XQxsqSnZb3+VgN9vyvQi/gih
enr+l8r4esM7TOsGYHMWbQkHgnm/wOMiMc56xlqeIi/+cRuJF3Mg5LnbqIUU1Ox0
S+luGASWg1R2yU4n0EWQ5sx1dmslgvJe6lDKlQXEX4e4d5F0vhnyFiDVhChMCpGv
ALK8MiUZGD2y9fIPB+Yypq7mH9NiQwQIJtpbVgVd2pBEl0XWkbs6F5GNq90ChE7x
hWiyu2N/iug2r7zKt5KtZob9hIljQ6N/HAk+O1lLPUktX5HBt8X+KHyX22GMLIAu
g4tWk2P7R0Sh2rYSd43S3CMF01eaGI2oWU0rj2KtG1HqQMVOIVjdxYR3tpmC69Tb
MyCq7xe+zOcAiWkAeb1cJjgahrnDBR663zSsqZVnAyG9GQVvPY6fZKxYoY2B+xgP
tEi8FTL3ag0PbdUJF8SQugx+R3HNBS+r1AGgOUEBtrrH9IqBAaOf3N+rMFLUl/HW
ypTs5fj6A49CkncmQQGcyj3BwlEjhr4wF59Bpxn/KTwT5H2Sdu1QEuB2G+gwqYs=
=bOwk
-----END PGP SIGNATURE-----


On Sat, Jun 27, 2015 at 4:54 PM, Jeff Garzik <jgarzik@gmail.com> wrote:

-------------------------------------
Hi Martin,

You're on the right lines. Your writeup is pretty similar to the high level
overview given here though:

https://en.bitcoin.it/wiki/Contracts#Example_2:_Escrow_and_dispute_mediation

To make 2-of-3 dispute mediation works requires implementing a wallet that
supports it, and the tools mediators need to manage incoming tickets, etc.
The BIP70 extension is probably the smallest part of the project.


On Sat, Jan 31, 2015 at 2:30 AM, Martin Habovštiak <
martin.habovstiak@gmail.com> wrote:

-------------------------------------
On Mon, May 25, 2015 at 08:44:18PM +0200, Mike Hearn wrote:

This can cause problems as until those transactions confirm, even more
of the user's outputs are unavailable for spending, causing confusion as
to why they can't send their full balance. It's also inefficient, as in
the case where the user does try to send a small payment that could be
satisfied by one or more of these small UTXO's, the wallet has to use a
larger UTXO.

With replace-by-fee however this problem goes away, as you can simply
double-spend the pending defragmentation transactions instead if they
are still unconfirmed when you need to use them.

-- 
'peter'[:-1]@petertodd.org
00000000000000000aa9033c06c10d6131eafa3754c3157d74c2267c1dd2ca35
-------------------------------------
Adam,

Thank you for your comments. We will address them in the next update.

Privacy is an area you have been championing for many years and your input
in this area has created the environment for Bitcoin to exist. Anonymity is
not something we would be willing to compromise, despite the general
public's endless willingness to give away their intimate life histories and
contact details to Facebook and other social media.

If you have the will to read a longish response, we have tried to expand a
little on the rationale for the type of data we believe will be useful and
valuable, in addition to broadly addressing the privacy issues.

Arguably, the greater the cryptographic protections on the ownership of a
transaction, the more value will be placed on the nature of transactions.
Without wishing to frame the conversation on specifics or particular
sectors, here are some distinctions to address some of the previously noted
Orwellian fears on attaching identity markers to Bitcoin transactions.

* We have no desire to create tools that can analyze which brand of baked
beans Bob may prefer compared to Alice. That is a matter of extreme detail
for supermarkets, their suppliers and internal point of sale systems.  When
you make a store purchase, your basket of goods are wrapped up in a single
payment which appears on your credit card or bank statement. Knowing on an
aggregated basis that people spend around $120 / week on retail shopping
and that Saturday is the busiest period for that activity is more valuable
for payment processors than knowing the contents of Bob's basket of goods.

As a retailer or supplier, I would buy that data in order to plan
inventory, marketing budgets, promotional activity, staffing levels,
logistics, factory production, bank borrowing, store expansion planning,
etc.

* Now consider petrol. Bitcoin is very well suited for fuel purchases,
especially for small independent petrol filling stations. Aggregated data
would help the entire supply chain that serves the consumer, if businesses
at the front end had access to data for when demand was at peaks and
troughs. Extend that from individual petrol stations to regional, national
and international consumer purchases and you have the basis of the market
pricing oil based on demand per period and per country. Again, we don't
care that it is actually Bob who fills up every Sunday so he has enough
fuel to last him the week or to track him along his driving holiday.

* Now consider remittances. While the global headlines are that it is a
$500bn a year industry. Few people know that remittances are based on a
relatively small number of remittance corridors that make up the bulk of
the market.  These corridors are based around people leaving small towns
and villages in poor areas and travelling to work in locations based on
knowing someone or a family member who used to live in their area and are
doing well in xyz location because they can see the beneficial impact on
the recipients quality of life. At the coal face, remittances are a word of
mouth grey market sector and part of the reason that WU and the like can
charge so much is because the last mile of remittances are in areas where
monetary infrastructure and logistics are difficult to serve and they can
get away with setting high prices. They prosper because they use data to
organise themselves better. Banks have no desire to serve this market
because you end up clogging up branches every Friday or Saturday with
people that transact relatively small sums compared to those that are
banked and get annoyed waiting. Having access to Bitcoin transaction data
on this sector would help Bitcoin businesses to understand the end points
of this market and serve it better and focus their promotional activities.
We have no desire to identify that Bob's cousin José is working illegally
in Texas.

These are three sectors where there are millions of small businesses that
would, for the first time, be able to access global, national and regional
industry data figures typically reserved for large businesses due to the
high cost of acquiring or commissioning research.

While increasing anonymity or having zero knowledge proofs in transactions
is desirable so that I can keep my Bitcoin salary payments private and my
membership of Ashley Madison out of the news, it would be helpful to know
that when I want to spend my salary that the world around me is organised
enough to serve my needs.

The block chain is ledger. It will contain a global data set that could end
up being one of the most valuable databases in the world. Why not use it to
fund Bitcoin's security infrastructure and growing bandwidth challenges?

Regards,

Ahmed






On Sun, Aug 23, 2015 at 5:05 PM, Adam Back <adam@cypherspace.org> wrote:

-------------------------------------
Hi Jan,

This is really nice work.

WRT the Schroder and Schildbach proposal, the generalization of the "r"
and "payment_url" parameters makes sense, with only the potential
backward compat issue on payment_url.


Yes, this design is problematic from a privacy standpoint. Anyone within
the rather significant range of the Bluetooth terminal is able to
capture payment requests and correlate them to people. In other words it
can be used to automate tainting.

The problem is easily resolved by recognizing that, in the envisioned
face-to-face trade, proximity is the source of trust. Even in the above
proposal the "h" parameter is trusted because it was obtained by
proximity to the NFC terminal. The presumption is that this proximity
produces a private channel.

As such the "tap" should transfer a session key used for symmetric block
cipher over the Bluetooth channel. This also resolves the issue of
needing to formulate the payment request before the NFC.

As an aside, in other scenarios, such as an automated dispenser, this
presumption does not hold. The merchant is not present to guard against
device tampering. Those scenarios can be secured using BIP70, but cannot
guarantee privacy.

The other differences I have with the proposal pertain to efficiency,
not privacy or integrity of the transaction:

The proposed resource name is redundant with any unique identifier for
the session. For example, the "h" parameter is sufficient. But with the
establishment of a session key both as I propose above, the parties can
derive a sufficiently unique public resource name from a hash of the
key. An additional advantage is that the resource name can be
fixed-length, simplifying the encoding/decoding.

The MAC address (and resource name) should be encoded using base58. This
is shorter than base16, is often shorter than base64, better
standardized and does not require URI encoding, and is generally
available to implementers.

There is no need for the establishment of two Bluetooth services.

I would change the payment_url recommendation so that the list order
represents a recommended ordering provided by the terminal for the wallet.

I wrote up my thoughts on these considerations last year and recently
revised it by adding a section at the end to incorporate the "r" and
"payment_url" generalizations from Andreas and Andy.

https://github.com/evoskuil/bips/tree/master/docs

e


On 02/22/2015 11:08 AM, Jan Vornberger wrote:

-------------------------------------
On 1/17/2015 12:45 PM, Rune Kjr Svendsen wrote:

Will success be defined by "BitPay Payment Channels Accepted Here" signs 
appearing in shop windows?



-------------------------------------
On Wed, Sep 16, 2015 at 06:29:28PM -0400, Peter Todd via bitcoin-dev wrote:


This is the same point I have been making to Jeff privately.

Refactors are a means to an end: a more modular, reusable and maintainable codebase. This goal is that new functionality can be plugged in more easily, and rebase work for e.g. functionality built on top can go down, not up, if it just hooks into well-defined interfaces here and there.

Although there has been a lot of progress, bitcoind's design is still too monolithic. To add a more involved feature, like say a new index over the block chain data, code needs to be touched all over the place. This change interacts with all other functionality, potentially breaking the base node functionality - risk for users that do NOT use the functionality. This increases risk and review time.

- *If possible* functionality should be built without changing bitcoind's code at all. An external process should be able to keep up to date with the chain, notice reorgs, and process block data accordingly. If bitcoind's interface does not allow that, or it is too difficult, that is what should be fixed. 
- *if not possible* then a change should at least touch the code in as few places as possible, and integrate with e.g. signal notification.

To name an example of it done right, IMO: Monero's 'simplewallet'. It is a command-line utility wallet that communicates with the node software, and remembers where it was in the chain, and processes changes to the chain state since its last invocation when it 'refreshes'. 
What is nice is that one can run an arbitary number of simplewallets against one node daemon, and unlike bitcoind's wallet it doesn't need to run as always-on daemon itself. It can be invoked when the user wants to do something with the wallet, or see if there are new transactions.

An index could be implemented entirely externally in a similar way, while still fully handling reorgs.

What one needs for that, I think, is a library that communicate with the node, and which offers functionality abstractly be similar to 'git pull': give me the tree path from my current known tip to the best tip, and supply the block hashes (and block data) along the way. 

My long-term vision of bitcoind is a P2P node with validation and blockchain store, with a couple of data sources that can be subscribed to or pulled from.

Wladimir

-------------------------------------
One correction inline below.

e

On 02/22/2015 02:39 PM, Eric Voskuil wrote:

The MAC address (and session key) should be encoded using base58. This


-------------------------------------
In building some CLTV-based contracts, it is often also useful to have a
method of requiring, instead of locktime-is-at-least-N,
locktime-is-at-least-N-plus-the-height-of-my-input. ie you could imagine
an OP_RELATIVECHECKLOCKTIMEVERIFY that reads (does not pop) the top
stack element, adds the height of the output being spent and then has
identical semantics to CLTV.
A slightly different API (and different name) was described by maaku at
http://www.reddit.com/r/Bitcoin/comments/2z2l91/time_to_lobby_bitcoins_core_devs_sf_bitcoin_devs/cpgc154
which does a better job of saving softfork-available opcode space.

There are two major drawbacks to adding such an operation, however.

1) More transaction information is exposed inside the script (prior to
CLTV we only had the sigchecking operation exposed, with a CLTV and
RCLTV/OP_CHECK_MATURITY_VERIFY we expose two more functions).

2) Bitcoin Core's mempool invariant of "all transactions in the mempool
could be thrown into one overside block and aside from block size, it
would be valid" becomes harder to enforce. Currently, during reorgs,
coinbase spends need checked (specifically, anything spending THE
coinbase 100 blocks ago needs checked) and locktime transactions need
checked. With such a new operation, any script which used this new
opcode during its execution would need to be re-evaluated during reorgs.

I think both of these requirements are reasonable and not particularly
cumbersome, and the value of such an operation is quite nice for some
protocols (including settings setting up a contest interval in a
sidechain data validation operation).

Thoughts?

Matt

On 10/01/14 13:08, Peter Todd wrote:


-------------------------------------
I'm reading it.

First comment: since a Bitcoin block time is only greater than the median
of the last 11 blocks, a miner could choose the key block time in order to
generate about 400 miniblocks, instead of the average 60 blocks. Not very
bad, but should be taken into account.




On Wed, Oct 14, 2015 at 3:02 PM, Emin Gün Sirer <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Wed, Aug 12, 2015 at 1:21 PM, Venzen Khaosan <venzen@mail.bihthai.net> wrote:

No, sorry it's 2 concerns/risks with 2 sub-concerns/risks each:

1) Potential indirect consequence of rising fees.

1.1) Lowest fee transactions (currently free transactions) will become
more unreliable.
1.2) People will migrate to competing systems (PoW altcoins) with lower fees.

2) Software problem independent of a concrete block size that needs to
be solved anyway, often specific to Bitcoin Core (ie other
implementations, say libbitcoin may not necessarily share these
problems).

2.1) Bitcoin Core's mempool is unbounded in size and can make the program
crash by using too much memory.

2.2) There's no good way to increase the fee of a transaction that is
taking too long to be mined without the "double spending" transaction
with the higher fee being blocked by most nodes which follow Bitcoin
Core's default policy for conflicting spends replacements (aka "first
seen" replacement policy).


I believe thisbelongs in 2, not really sure if 2.1 or 2.2 since it's
related to both.


I believe "fear of exchange rate declining" can probably be added to
any concern/risk "leaf", so we should probably leave that for the end
or just omit it.


I'm not sure I understood this but seems related to the exchange rate.

-------------------------------------
On Thu, Aug 27, 2015 at 9:39 AM, prabhat <prabhatkr@gmail.com> wrote:


This is a development list; organizations like https://coincenter.org/ work
on high-level policy issues.

Last I heard, competent law enforcement organizations said they were
perfectly capable of tracking down criminals using Bitcoin using
traditional investigative techniques (like infiltrating criminal
organizations or setting up honeypots). Given how many "dark markets" have
either disappeared or been taken down, it seems they are correct.

-- 
--
Gavin Andresen
-------------------------------------
Or can’t you create a transaction that’s still within the op count and sig ops limits but is larger than 1MB?


-------------------------------------
On Tue, Jun 23, 2015 at 4:46 PM, Peter Todd <pete@petertodd.org> wrote:


... but the effect is only significant if they have an absurdly
low-bandwidth connection and do NOTHING to work around it (like rent a
server on the other side of the bandwidth bottleneck and write some code to
make sure you're creating blocks that will propagate quickly on both sides
of the bottleneck).


Why do you think connectivity is a centralizing effect? It is just one
factor in the profitability-of-mining equation. A location with bad
connectivity (the US, maybe) but 10% cheaper electricity might be just as
good as one with great connectivity but more expensive electricity.

Having lots of variables in the profitability equation is a decentralizing
force, it means there is very likely to be several different places in the
world / on the net where mining is equally profitable.



Long term the p2p protocol will evolve to incorporate those optimizations,
so will require no co-operation.







Are you familiar with the terms "Gish Gallop" and "Moving the Goalposts" ?

I have written quite a lot about the kind of resources needed to run a full
node, and have asked you, specifically, several times "how much do you
think is too much" and received no answer.

-- 
--
Gavin Andresen
-------------------------------------
On Sat, Dec 19, 2015 at 03:17:03AM +0800, Chun Wang via bitcoin-dev wrote:

If size is calculated from the median time past, which is fixed for a
given block and has no dependency on the block header's nTime field,
does that solve your problem?

By "median time past" I mean the median time for the previous block.

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Decentralization depends on the context and does not have a definition
in a form that it was demanded... I can confirm we have people in our
community which do understand decentralization, and quite good
actually, just there is no definition if the form demanded.

It is known that ~90% (at least of the nodes accepting incoming
connections) are running Bitcoin Core software. This does not mean
that Bitcoin is somehow less decentralized. Bitcoin Core is open
source, it has many contributors from all over the world and there are
many pull requests - most of them do get merged if you check the
commit history. It is widely used because the quality of the code is 5
stars. There are other implementations as well, they are just not
widely used. This does not mean one is not free to write his own
implementation of the Bitcoin protocol (assuming he follows the
consensus rules of the network). The biggest problem is convincing
users to adopt that implementation, which is a normal thing which
happens in general, not only related to software implementations.

The problem is there is no other implementation out there which comes
near the quality of the code in Bitcoin Core. I am actually eager to
try other implementations as well, but something serious, because
Bitcoin itself is a payment protocol not something to play with.

This is the reason why a lot of developers contribute to Bitcoin Core
rather than writing their own implementation. This only makes Bitcoin
Core stronger, better, and obviously the result is that it has
majority in the ecosystem for good reasons. If I'm experienced in a
certain segment related to software developing, I am better of in
contributing to Bitcoin Core just with the part I know instead of
writing from scratch my own implementation.

On 9/1/2015 2:32 AM, Peter R via bitcoin-dev wrote:
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBCAAGBQJV5OeqAAoJEIN/pSyBJlsRRsoIAMmdyeE+Sro14NIHy6jQqTH3
JdkhUg6lg7S58tqs7ahQ/U2QGMPLaQae9yv3NidKpyqzL0YXtc2+r7RDBp0p2L4O
ieBJfJRBDwjjHYun+h7VTkPRbFGoBs/vwtTahd+uxUjwdEhiOxI2Q8pY8dLbdmJz
5lyA3TIcOVy3FjGYp3ji8aBQkw4o9OZbgmY/iCmVONgup96+81/FdR8P6wwdi3tg
Hep+4iU5Z+RHVE0sQhJDgl8Rw2oY6cmfxOCdFalRAASfZClkMfZok7eDE5yWtUbE
tn9tEP82tc3OwZCC+XvpVggVWnCp/rGZFslfTdiWXWeLXhs+JLf0hWet4/SWCT0=
=zQ9s
-----END PGP SIGNATURE-----

-------------------------------------
It generally doesn't matter that every node validate your coffee
transaction, and those transactions can and will probably be moved onto
offchain solutions in order to avoid paying the cost of achieving global
consensus. But you still don't get to set the cost of global consensus
artificially. Market forces will ensure that supply will meet demand there,
so if there is demand for access to global consensus, and technology exists
to meet that demand at a cost of one cent per transaction -- or whatever
the technology-limited cost of global consensus happens to be -- then
that's what the market will supply.

It would be like if Amazon suddenly said that they were going to be
charging $5 / gb / month to store data in s3. Can't do it. Technology
exists to bring about cloud storage at $0.01 / GB / month, so they don't
just get to set the price different from the capabilities of technology or
they'll get replaced by a competitor. Same applies to Bitcoin.




On Tue, Aug 11, 2015 at 1:48 PM, Mark Friedenbach <mark@friedenbach.org>
wrote:

-------------------------------------
In sending the first-signed transaction to another for second signature, how does the first signer authenticate to the second without compromising the  independence of the two factors?

Sent from my iPhone



-------------------------------------
Why should miners only be able to vote for "double the limit" or "halve" the limit? If you're going to use bits, I think you need to use two bits:

	0 0 = no preference ("wildcard" vote)
	0 1 = vote for the limit to remain the same
	1 0 = vote for the limit to be halved
	1 1 = vote for the limit to be doubled

User transactions would follow the same usage. In particular, a user vote of "0 0" (no preference) could be included in a block casting any vote, but a block voting "0 0" (no preference) could only contain transactions voting "0 0" as well.

Incidentally, I love this idea, as it addresses a concern I immediately had with Jeff's proposal, which is that it hands control exclusively to the miners. And your proposal here fixes that shortcoming in a economically powerful way: miners lose out on fees if they don't represent the wishes of the users.


On Friday, 12 June 2015, at 2:11 pm, Peter Todd wrote:


-------------------------------------
On Fri, Dec 18, 2015 at 8:56 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

So hypothetically if wallets/payments processors/full nodes adoption
will take 6 month to get to 50% after the segwit soft-fork activation, this
means that actual network capacity will be increased by:

1.75 x 0.5 + 1 x 0.5 = 1.375

after six month.

An hard-fork on the others side would bring 1.75 since the activation, am I
right?
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Excellent - thank you.

Jeff Garzik via bitcoin-dev:

- -- 
http://abis.io ~
"a protocol concept to enable decentralization
and expansion of a giving economy, and a new social good"
https://keybase.io/odinn
-----BEGIN PGP SIGNATURE-----

iQEbBAEBCgAGBQJV5+uQAAoJEGxwq/inSG8CVBUH9A5GtIj3pLxZRlX0oDxSbIWJ
2830HURoeb40ShBlhbzO1nHiJtPhRPWqByZETQcuElBagMPreSKI5VZxJ1xaNOI3
o6yo9ujeLNlge1j53TOq8uQCXKnwrVsjS3yQkXlo+IX+Vihin5c/D4Xn9y97OqwQ
CixVswCJrrRrGHj6YaFsfAx+epaJ/aT4djoB0XjH9PKJI5b0cPGSBDipHbuVn3nd
FZidPAS/hHI0Sw3k0EHtYudjBXBbMi2hCad37asrg2cIF/sFbCA/BSkpuIi5agzY
50Wp8xm3gd4WWjEn/svhw2AIgH7R/1Yk2/qFImob5iXMm7sU1OUMHD325kN2dg==
=7a0Z
-----END PGP SIGNATURE-----

-------------------------------------
let me continue my conversation:

as the development of this transactions would be indiscated

as a ByteArray of


On Fri, May 8, 2015 at 3:11 PM, Damian Gomez <dgomez1092@gmail.com> wrote:

-------------------------------------

People often miss the fundamental reasons Bitcoin exists,
the various conjoined ethos behind its creation. This is to be
expected, it's so far ouside any thinking or life process they've
ever had to do or been exposed to. It's also partly why figuring
out what to do or code or adopt, is hard. And certainly not made
any easier by the long term need and the current value at stake.

Creating a system in which a Botswanan can give a few bits
of their impoverished wages to their friend in Mumbai without
it being gated, permitted, hierarchied, middlemanned, taxed,
tracked, stolen and feed-upon until pointless... this simply
doesn't compute for these people. Their school of thought is
centralization, profit, control and oppression. So of course they
see txrate ramming up against an artificial wall as perfectly fine,
it enables and perpetuates their legacy ways.

Regardless of whichever technical way the various walls are torn down,
what's important is that they are. And that those who are thinking
outside the box do, and continue to, take time to school these
legacy people such that they might someday become enlightened
and join the ethos.

Otherwise might as well work for ICBC, JPMC, HSBC, BNP, MUFG
and your favorite government. Probably not as much fun though.

-------------------------------------
On Sat, Aug 29, 2015 at 9:01 PM, Matt Whitlock via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

I would really prefer chain=<chainID> over network=<chainPetnameStr>
By chainID I mean the hash of the genesis block, see
https://github.com/jtimon/bitcoin/commit/3191d5e8e75687a27cf466b7a4c70bdc04809d39
I'm completely fine with doing that using an optional parameter (for
backwards compatibility).

I agree with Andreas Schildbach that respecting the most commonly used
schemes is desirable.
So my preference would be:

/tx/3b95a766d7a99b87188d6875c8484cb2b310b78459b7816d4dfc3f0f7e04281a?chain=000000000933ea01ad0ee984209779baaec3ced90fa3f408719526f8d77f4943

(a tx in testnet)

/block/00000000000000000b0d504d142ac8bdd1a2721d19f423a8146d0d6de882167b?chain=000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f

(a block in bitcoin's mainnet)

-------------------------------------
On Friday, September 18, 2015 8:24:50 PM Btc Drak via bitcoin-dev wrote:

Not everyone does crazy clock-changing. Using such a time system for 
scheduling seems to inconvenience the wrong position. (although perhaps 
arguably better since most people probably use DST) :p

(Aside, if Google Calendar can't support standard UTC, that sounds like an 
argument against using Google Calendar...)


Tonal time works nice any consistently. :D

Luke

-------------------------------------
Pieter: I kind of see your point (but I think you're missing some key
points). You mean just download all the headers and then just verify the
transactions you filter out by using their corresponding merkle trees,
right? But still, I don't think that would scale as well as with the tree
structure I propose. Because, firstly, you don't really need the headers of
the sibling chains. You just need the headers of the parent chains since
the parent verifies all the siblings. All you really need in a typical
(non-mining) situation is the headers or full blocks in one path going down
the tree starting from the root chain. So that means O(log n) needs to be
stored (headers or blocks) (n the number of transaction on the network).
With big blocks, you still need O(n) headers. I know headers are small, but
still they take up space and verification time. Also, since you are storing
the full blocks on the chains you want, you are validating the headers of
those blocks and you are sure that you are seeing all transactions on those
blocks. And if certain addresses must stay on those blocks, you will know
that you are catching all of the transactions corresponding to those
blocks. If you just filter out based on addresses or other criteria, you
can be denied some of those transactions by full nodes, and you may not
know about it. Say for example, your government representative publishes on
of his public addresses that is used for paying for expenses. Then with my
system, you can be sure to catch every transaction being spent from that
address (or UTXO or whatever you want to call it). If you just filter on
any transaction that includes that address, you may not catch all of those
transactions. Same with incoming funds.

There are also advantages for mining decentralization as I have explained
in my previous posts. So still not sure you are right here...

Thanks

On Mon, Jun 15, 2015 at 5:18 PM, Mike Hearn <mike@plan99.net> wrote:




-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
-------------------------------------
As softforks almost certainly require backports to older releases and other
software anyway, I don't think they should necessarily be bound to Bitcoin
Core major releases. If they don't require large code changes, we can
easily do them in minor releases too.
On Apr 28, 2015 12:51 PM, "Peter Todd" <pete@petertodd.org> wrote:

-------------------------------------
I was curious about there being only 10 single-byte opcodes left.  There
are ten single-byte OP_NOPx opcodes defined, but there are 15 opcodes that
"simply *do not exist anymore* in the protocol" because they are scary (had
bugs that "could crash any Bitcoin node if exploited" or "allowed anyone to
spend anyone's bitcoins").  There are also 66 single-byte values that are
currently reserved, 186 - 252 (0xba - 0xfc).

If the name OP_CHECKSEQUENCEVERIFY should not be changed, each of us has a
single best reason not to change it.  Finding other reasons suggests that
one's top reason isn't good enough.  See Nassim Taleb's book, Antifragile,
if that claim makes you curious.  The same goes for changing it.  In any
case, it is 178 (0xb2) and app developers can call it whatever they want.

It seems trivial to me since the following, in script.h, would neither slow
compilation nor confuse anyone, but could lead the curious to explore the
history and expand their knowledge:
OP_NOP3 = 0xb2,
OP_CHECKSEQUENCEVERIFY = OP_NOP3,
OP_CHECKMATURITYVERIFY = OP_NOP3, // A comment defending the alternative
name

I don't know the consensus here on leaving breadcrumbs in code comments
(and enum/variable names) for curious coders to use as inspiration for
studying the history, but I advocate it, since modern IDEs are fairly
well-equipped to make skipping or hiding comments easy.


On Wed, Nov 25, 2015 at 3:05 PM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
I agree with the simplicity of this approach and with removing the
reduction step... it's unlikely the block size would ever need to be
reduced, only increased with demand?

I like this solution better than either kicking the can, or raising the
block size based on chain height (another dynamic solution).

-Chris


On Tue, Aug 18, 2015 at 4:58 PM, Danny Thorpe via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Monday, 25 May 2015, at 8:41 pm, Mike Hearn wrote:

I see this behavior all the time. I am using the latest release, as far as I know. Version 4.30.

The same behavior occurs in the Testnet3 variant of the app. Go in there with an empty wallet and receive one payment and wait for it to confirm. Then send a payment and, before it confirms, try to send another one. The wallet won't let you send the second payment. It'll say something like, "You need x.xxxxxx more bitcoins to make this payment." But if you wait for your first payment to confirm, then you'll be able to make the second payment.

If it matters, I configure the app to connect only to my own trusted Bitcoin node, so I only ever have one active connection at most. I notice that outgoing payments never show as "Sent" until they appear in a block, presumably because the app never sees the transaction come in over any connection.


-------------------------------------
On 20.12.2015 18:00, Emin Gn Sirer via bitcoin-dev wrote:

Block withholding attacks do not differentiate between small and large
pools. When Eligius and BTCGuild got hit with this, they were far from
the biggest pools at the time.

When my pool, Bitminter, got a new large miner who found 1 block where
average luck would have had them find 3, one of the other miners claimed
they must be withholding blocks. Even if there is no logic or evidence
behind it, after one person cries wolf the others get nervous. This way
even the possibility of block withholding can keep smaller pools from
growing. It takes more hashpower to put a dent in a bigger pool, so you
will see less such panic.


Three guys with 1 TH/s, 2 TH/s and 100 GH/s meet at a conference and
decide to start a private pool? Obviously that doesn't work. Maybe three
people with huge warehouses of miners would work together if they knew
and trusted each other.

Those small miners need to mine with people they don't know to get an
acceptable variance.

If you kill off mining pools then small miners have no way to achieve
acceptable variance and they will disappear. There will only be big
warehouse miners left, the ones who are big enough to solo mine.

That's not helping decentralization.


I agree. It's very disappointing how most miners and pools handle this
(BTCGuild being the exception). But I do not think block withholding is
a good tool. It can easily destroy small pools, but it won't put a dent
in a pool that goes over 50%.

Block withholding is a tool big pools can use to put smaller competitors
out of business.

And even if it was effective I would not use block withholding to attack
other pools.


Is it? Is there any example of block withholding leading to more
decentralized mining?

If I remember right, GHash being too big ended with BitFury moving some
of their hashpower out of the pool. I don't know where that hashpower
went and whether the problem was solved or merely hidden.

GHash profitability being very low for some time wasn't due to block
withholding, it was a bug that some miners abused to get paid for the
same work multiple times. This made it look like a lot of work was done
while finding few blocks.


Block withholding didn't solve the problem back then. And guess what,
those painful days are here right now. China is at 65% and block
withholding isn't solving it.

I was disappointed when GHash got too big and refused to do anything. It
was sad when their miners didn't do anything. Then they used
double-spends to scam a casino. I was shocked that noone cared. Now two
thirds of the bitcoin hashpower is within the control of a single
government. This time I expected noone would care - but I'm still
disappointed. I'm also surprised at the irrational behavior; there are
so many who go out of their way to put their own investments in danger.

For a long time now many miners and pools have been irresponsible with
the hashpower. But block withholding just makes it worse.

Regards,
Geir H. Hansen, Bitminter mining pool


-------------------------------------
I feel compelled to re-share Mike Hearn's counter-argument *against *
replace-by-fee:
https://medium.com/@octskyward/replace-by-fee-43edd9a1dd6d

Please carefully consider the effects of replace-by-fee before applying
Peter's patch.

On Sun, May 3, 2015 at 9:36 PM, Peter Todd <pete@petertodd.org> wrote:

-------------------------------------
On Wed, May 06, 2015 at 11:41:37PM +0000, Matt Corallo wrote:

I'll also point out that miners with the goal of finding more blocks
than their competition - a viable long-term strategy to increase market
share and/or a short-term strategy to get more transaction fees -
actually have a perverse incentive(1) to ensure their blocks do *not*
get to more than ~30% of the hashing power. The main thing holding them
back from doing that is that the inflation subsidy is still quite high -
better to get the reward now than try to push your competition out of
business.

It's plausible that with a limited blocksize there won't be an
opportunity to delay propagation by broadcasting larger blocks - if
blocks propagate in a matter of seconds worst case there's no
opportunity for gaming the system. But it does strongly show that we
must build systems where that worst case propagation time in all
circumstances is very short relative to the block interval.

1) http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg03200.html

-- 
'peter'[:-1]@petertodd.org
000000000000000004dc867e4541315090329f45ed4dd30e2fd7423a38a72c0e
-------------------------------------


Blockchain reorgs are part of the consensus rules. We’re talking not about forks caused by network partitions…but forks caused by the use of distinct consensus rules.


You cannot merge two chains that have incompatible transactions in them without throwing away one of the two conflicting transactions (along with all dependencies). In the reorg process, this occurs naturally…and we allow for it by using confirmation count as a metric of irreversibility. Until one chain wins (by overwhelming consensus) or all chains include a particular transaction in question, we cannot treat that transaction as irreversible. Propose a model in which we can still reliably measure irreversibility in the presence of multiple chains and you might have a point.
-------------------------------------
Tets
-------------------------------------
On Monday 10. August 2015 16.55.40 Jorge Timn via bitcoin-dev wrote:

The actual fee is irrelevant, the amount of transactions is relevant.

Have you ever been to a concert that was far away from public transport? They 
typically set up bus shuttles, or taxis to get people back into town 
afterwards.
The result there is always you end up waiting forever and it actually may be 
easier to just walk instead of wait.
The amount you pay is irrelevant if everyone is paying it. There still is more 
demand than there is capacity.

At the concert the amount of people will stop after some time, and you'd get 
your bus. But in the scenarios created here the queues will never stop.

So, no, its not unreliable for cheap free transactions.
Its unreliable for all types of transactions.

-- 
Thomas Zander

-------------------------------------
There are no good short-term scaling solutions...this is a very hard problem that necessarily requires a lot of out-of-the-box thinking, something 2015 has seen a LOT of...and I'm optimistic about the ideas presented thus far.

At least SW *is* a scaling solution (albeit most of the important benefits are long term). The issue of fee events has nothing to do with scaling - it has to do with economics...specifically whether we should be subsidizing transactions, who should pay the bill for it, etc. My own personal opinion is that increasing validation costs works against adoption, not for it...even if it artificially keeps fees low - and we'll have to deal with a fee event sooner or later anyhow. You may disagree with my opinion, but please, let's stop confounding the economic issues with actual scaling.

On December 16, 2015 6:21:22 PM PST, Jeff Garzik via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------------------------------
On 02/06/2015 12:59 AM, Roy Badami wrote:

Certainly, which brings us back to proximity.

Which reminds me - it's important to keep in mind the scenario that
arises when there is no person present to represent the receiver. Such
as a vending machine purchase.

Proximity in these cases is insufficient, as the receiver is not able to
prevent application of a fraudulent NFC device or replacement of a
static QR code. In these cases BIP-70 becomes essential.

e

-------------------------------------
I think at this point I'd like to bring back my original suggestion of
using DHKE (Diffie-Hellman) or simlar. I know we'd still need to
transmit some secret that could be eavesdropped, but at least the
session could not be decrypted from recordings.

Anyway, establishing a "mostly secure" session is clearly an improvement
to no protection at all. If we can't find a solution to the dilemma of
how to exchange the secret, I suggest going ahead with what we have and
make the best from it.


On 02/23/2015 08:36 AM, Andy Schroder wrote:




-------------------------------------
Mike, I am not denying it is impossible to do all of that.
Just that it is not a trivial stuff to do to make it works everywhere, and
I think that it is not a good thing for a client side technology.
BIP70 has its use, and I understand why there is case where it is good to
ship the certs in the message and not depends on the transport.

But a standard that just use JSON and HTTPS, even if less flexible that
BIP70, would make it easier and sufficient for today's use case.

On Wed, Jan 28, 2015 at 5:55 PM, Mike Hearn <mike@plan99.net> wrote:

-------------------------------------
On Thu, Aug 13, 2015 at 4:42 PM, Joseph Poon via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


This proposal includes no such provision.

Since we talked about it, I spent considerable time thinking about the
supposed risk and proposed mitigations. I'm frankly not convinced that it
is a risk of high enough credibility to worry about, or if it is that a
protocol-level complication is worth doing.

The scenario as I understand it is a hub turns evil and tries to cheat
every single one of its users out of their bonds. Normally a lightning user
is protected form such behavior because they have time to broadcast their
own transactions spending part or all of the balance as fees. Therefore
because of the threat of mutually assured destruction, the optimal outcome
is to be an honest participant.

But, the argument goes, the hub has many channels with many different
people closing at the same time. So if the hub tries to cheat all of them
at once by DoS'ing the network, it can do so and spend more in fees than
any one participant stands to lose. My issue with this is that users don't
act alone -- users can be assured that other users will react, and all of
them together have enough coins to burn to make the attack unprofitable.
The hub-cheats-many-users case really is the same as the
hub-cheats-one-user case if the users act out their role in unison, which
they don't have to coordinate to do.

Other than that, even if you are still concerned about that  scenario, I'm
not sure timestop is the appropriate solution. A timestop is a
protocol-level complication that is not trivial to implement, indeed I'm
not even sure there is a way to implement it at all -- how do you
differentiate in consensus code a DoS attack from regular old blocks
filling up? And if you could, why add further complication to the consensus
protocol?

A simpler solution to me seems to be outsourcing the response to an attack
to a third party, or otherwise engineering ways for users to
respond-by-default even if their wallet is offline, or otherwise assuring
sufficient coordination in the event of a bad hub.
-------------------------------------
On Sat, Feb 14, 2015 at 3:23 PM, Tamas Blummer <tamas@bitsofproof.com> wrote:

Right now libconsensus' only dependency is openSSL. Most of the
testing in libsecp256k1 has been in signing rather than verifying
signatures (please, anyone with more knowledge in the library don't
hesitate to correct me or clarify things). But eventually openSSL will
be completely replaced by libsecp256k1.
It does not store anything, 0.1 is just a dynamic library with a c API
to a single function: VerifyScript().
This function saves the hassle of reimplementing signature checking
(which is a really hard part) and reimplementing an interpreter that
must function in exactly the same way in many as many other nodes with
different software and/or hardware.
Guido van Rossum can say "some behaviours in python the language are
not specified, so it is ok if cpython and pypy do different things,
they're still both running python which is more abstract than any of
its implementation".
But a consensus system like bitcoin doesn't have the luxury of leaving
consensus rules unspecified. And the simplest way to fully specify a
language interpreter is by implementing it.
But coupling the consensus rules specification with a bigger project
like bitcoin core can result in implementation details of that bigger
project accidentally and unexpectedly becoming consensus rules. This
is what happened with bdb and nobody wants that to happen again,
that's the whole point.
Note that many parts of the bitcoin protocol (like the p2p messages)
are NOT part of the consensus rules.
You can have a look at
https://github.com/jtimon/bitcoin/commits/consensus2 and maybe you
would be surprised about how small they actually are. This branch is
incomplete and still a mess that needs to be cleaned up. And none of
that is included in libconsensus yet.
I was planning on writing a post here asking for feedback on the
interfaces for these higher level checks. I'm just putting the code
together in the same module, but obviously class CCoinsViewCache
cannot be an argument in functions of a c API.


Nobody is attacking alternative implementations. This tool was created
mostly with alternative implementations in mind.
So input from them it's very welcomed on how to continue libconsensus
(or of course correct any flaws in verifyScript if there's any).
I just wanted to wait to have some more code to make things easier to
explain (and have a clearer idea of it myself).
There's a more limited branch on "next steps for libconsensus" in #5669.


Sure, I think he is complaining that at the moment that's probably the
only safe way to operate with alternative implementations and still
have full node guarantees.


Sidechains are completely orthogonal to this discussion and, in fact,
it would be good to have libconsensuses for sidechains too, since
their nodes also need to come to consensus.


-------------------------------------
BIP70 is a protocol for getting a user's wallet client communicate with a
merchant's server in order to agree on details like where to send the
payment, how much to send, what the shipping address is, sending a receipt
back, and much more using various extensions that adds more functionality.

There could even be advanced functionality for automatically negotiating
terms. One example could be selecting a multisignature arbitrator both
sides trust. Another could be to agree on the speed and type of delivery.
Many more types of decisions could be automatically agreed upon.

But as it is now, it is designed to be initiated at the time of payment. If
you always want next-day delivery from online stores then you won't always
know if that's an option until you've filled the digital basket and gone
through checkout. If you only want to shop with an arbitrator involved same
thing applies.

Everything that BIP70 enables happens at the last step only, as it is right
now.

If there could be a BIP70 HTML tag on web shops that automatically
triggered your wallet as soon as you visit the page, it would be possible
for a browser extension that talks to your wallet to tell you right away if
the web shop you're currently looking at has terms you consider acceptable
or not (note: if your wallet client isn't installed on or linked to that
same machine, a visible Qr code would be an acceptable alternative which
you can scan in advance before you start shopping). This notification can
even be automatically updated as you add and remove things from your cart
and details like shipping options change.

This would massively simplify the shipping experience and make every web
shop feel like Amazon.

Of course this has privacy implications and increases exposure to potential
wallet exploits, but the wallet can ask you if you intend to shop or not at
each site before it even connects and send any information at all in order
to mitigate both of those problems. This way it should be reasonably safe.

Another option would be to automatically connect but limit what data is
sent in order to remain privacy preserving, until the user agrees to send
private information.

This second method would also open up for the merchant to other send
relevant information such as details about various certifications from
third parties, which can include a certification that shows they have been
been audited and approved by by entity X for purpose Y. If your wallet has
that entity whitelisted it will show you that certificate (for example
"Acme Audits have audited and approves of Merchant M's privacy policy and
data protection"). With a list of predefined types of certifications that
the wallet understand and accepts, it could (by choice of the user) require
a certificate to be present to even allow you to make a purchase (lack of
required certifications would result in automatic denial). No certificate =
your wallet never proceed to send private information.

Thoughts?

- Sent from my tablet
-------------------------------------
On Wed, May 13, 2015 at 9:31 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

That's great.  So, basically the multi-level refund problem is solved by
this?
-------------------------------------
On Thu, Dec 17, 2015 at 8:44 PM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

You can always schism hardfork miners out...

-------------------------------------
Hi list

Found this on reddit: http://impulse.is/

PDF: http://impulse.is/impulse.pdf

I'd love to hear this list's thoughts.

/runeks
-------------------------------------
I fully expect that new layers will someday allow us to facilitate higher
transaction volumes, though I'm concerned about the current state of the
network and the fact that there are no concrete timelines for the rollout
of aforementioned high volume networks.

As for reasoning behind why users will still need to settle on-chain even
with the existence of high volume networks, see these posts:

http://sourceforge.net/p/bitcoin/mailman/message/34119233/

http://sourceforge.net/p/bitcoin/mailman/message/34113067/

Point being, the scalability proposals that are currently being developed
are not magic bullets and still require the occasional on-chain settlement.
Larger blocks will be necessary with or without the actual scalability
enhancements.

- Jameson

On Thu, Jul 30, 2015 at 12:36 PM, Bryan Bishop <kanzure@gmail.com> wrote:

-------------------------------------
On Fri, Aug 21, 2015 at 02:01:06AM -0400, Jeff Garzik wrote:

I run a number of high speed nodes and while I don't have historical
logs handy over time, I've noticed a drop from about %5-%10 SPV clients
at any one time to closer to %1 (Matt: you have a few TB of logs saved
don't you?)

Also, as I mentioned, just look at the popularity of wallets such as
Mycelium that are not adopting bloom filters, but going with SPV
verification of block headers w/ lookup servers.

Anyway, look at the analogous implementation of NODE_GETUTXO's, which
helpfully has provided the infrastructure for wallets that need bloom
filters to find appropriate nodes to connect too - we certainely aren't
seeing any shortages of nodes for those wallets to use.

-- 
'peter'[:-1]@petertodd.org
00000000000000000402fe6fb9ad613c93e12bddfc6ec02a2bd92f002050594d
-------------------------------------
On Fri, Oct 2, 2015 at 8:30 AM, Daniele Pinna via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

They discuss a very old version of the Cuckoo cycle paper, and I
believe none of their analysis is applicable to the most recent
revision. :(

In any case, I commented more about functions of this class here:
https://www.reddit.com/r/Bitcoin/comments/3n5nws/research_paper_asymmetric_proofofwork_based_on/cvl922x

I don't believe changing the POW function is impossible in principle,
but I expect it would only happen due to problems with the composition
of current hash-power and not even if it were universally agreed that
some other construction were technically better (though that is a high
bar.)

-------------------------------------
Hey everyone,

as with the current "max block size" debate I was wondering: Is anyone here
in favor of a minimum block size (say 2 MB or so)? If so I would be
interested in an exchange (maybe off-list) of ideas. I am in favor of a
lower limit and am giving it quite a bit of thought at the moment.

Cheers

Levin
-------------------------------------
As now we have some concrete proposals 
(https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009808.html), 
I think we should wrap up the endless debate with voting by different 
stakeholder groups.

---------------------------------
Candidate proposals

Candidate proposals must be complete BIPs with reference implementation 
which are ready to merge immediately. They must first go through the 
usual peer review process and get approved by the developers in a 
technical standpoint, without political or philosophical considerations. 
Any fine tune of a candidate proposal may not become an independent 
candidate, unless it introduces some “real” difference. “No change” is 
also one of the voting options.
---------------------------------
Voter groups

There will be several voter groups and their votes will be counted 
independently. (The time frames mentioned below are just for example.)

Miners: miners of blocks with timestamp between 1 to 30 Sept 2015 are 
eligible to vote. One block one vote. Miners will cast their votes by 
signing with the bitcoin address in coinbase. If there are multiple 
coinbase outputs, the vote is discounted by output value / total 
coinbase output value.
Many well-known pools are reusing addresses and they may not need to 
digitally sign their votes. In case there is any dispute, the digitally 
signed vote will be counted.

Bitcoin holders: People with bitcoin in the UTXO at block 372500 (around 
early September) are eligible to vote. The total “balance” of each 
scriptPubKey is calculated and this is the weight of the vote. People 
will cast their votes by digital signature.
Special output types:
Multi-sig: vote must be signed according to the setting of the 
multi-sig.
P2SH: the serialized script must be provided
Publicly known private key: not eligible to vote
Non-standard script according to latest Bitcoin Core rules: not eligible 
to vote in general. May be judged case-by-case

Developers: People with certain amount of contribution in the past year 
in Bitcoin Core or other open sources wallet / alternative 
implementations. One person one vote.

Exchanges: Centralized exchanges listed on Coindesk Bitcoin Index, 
Winkdex, or NYSE Bitcoin index, with 30 days volume >100,000BTC are 
invited. This includes Bitfinex, BTC China, BitStamp, BTC-E, itBit, 
OKCoin, Huobi, Coinbase. Exchanges operated for at least 1 year with 
100,000BTC 30-day volume may also apply to be a voter in this category. 
One exchange one vote.

Merchants and service providers: This category includes all bitcoin 
accepting business that is not centralized fiat-currency exchange, e.g. 
virtual or physical stores, gambling sites, online wallet service, 
payment processors like Bitpay, decentralized exchange like 
Localbitcoin, ETF operators like Secondmarket Bitcoin Investment Trust. 
They must directly process bitcoin without relying on third party. They 
should process at least 100BTC in the last 30-days. One merchant one 
vote.

Full nodes operators: People operating full nodes for at least 168 hours 
(1 week) in July 2015 are eligible to vote, determined by the log of 
Bitnodes. Time is set in the past to avoid manipulation. One IP address 
one vote. Vote must be sent from the node’s IP address.

--------------------
Voting system

Single transferable vote is applied. 
(https://en.wikipedia.org/wiki/Single_transferable_vote). Voters are 
required to rank their preference with “1”, “2”, “3”, etc, or use “N” to 
indicate rejection of a candidate.
Vote counting starts with every voter’s first choice. The candidate with 
fewest votes is eliminated and those votes are transferred according to 
their second choice. This process repeats until only one candidate is 
left, which is the most popular candidate. The result is presented as 
the approval rate: final votes for the most popular candidate / all 
valid votes

After the most popular candidate is determined, the whole counting 
process is repeated by eliminating this candidate, which will find the 
approval rate for the second most popular candidate. The process repeats 
until all proposals are ranked with the approval rate calculated.

--------------------
Interpretation of results:

It is possible that a candidate with lower ranking to have higher 
approval rate. However, ranking is more important than the approval 
rate, unless the difference in approval rate is really huge. 90% support 
would be excellent; 70% is good; 50% is marginal; <50% is failed.

--------------------
Technical issues:

Voting by the miners, developers, exchanges, and merchants are probably 
the easiest. We need a trusted person to verify the voters’ identity by 
email, website, or digital signature. The trusted person will collect 
votes and publish the named votes so anyone could verify the results.

For full nodes, we need a trusted person to setup a website as an 
interface to vote. The votes with IP address will be published.

For bitcoin holders, the workload could be very high and we may need 
some automatic system to collect and count the votes. If people are 
worrying about reduced security due to exposed raw public key, they 
should move their bitcoin to a new address before voting.

Double voting: people are generally not allowed to change their mind 
after voting, especially for anonymous voters like bitcoin holders and 
solo miners. A double voting attempt from these classes will invalidate 
all related votes.

Multiple identity: People may have multiple roles in the Bitcoin 
ecology. I believe they should be allowed to vote in all applicable 
categories since they are contributing more than other people.


-------------------------------------
Bitcoin's participants can improve their ability to stay on a valuable
and censorship resistant blockchain by individually and informally
absorbing cultural wisdom regarding "rough consensus".  This does not
require writing any formal rules about what rough consensus is.  It is
a matter of participation with an understanding.

  https://www.ietf.org/tao.html#rfc.section.2

    In many ways, the IETF runs on the beliefs of its participants.
    One of the "founding beliefs" is embodied in an early quote about
    the IETF from David Clark: "We reject kings, presidents and
    voting.  We believe in rough consensus and running code".

A June 2015 bitcoin-dev thread, arguing about consensus, included the
usual range of responses; ranging from claims that any objection must
block consensus to a definition based on US Justice Stewart's "I'll
know it when I see it".  (It's funny because it's true.  We can
explain it better, though.)

  "Concerns Regarding Threats by a Developer to Remove Commit Access
  from Other Developers"
  http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-June/008772.html

An August 2015 cryptography-list thread presents the idea that rough
consensus can be used as a tool for hindering progress.  The specific
threat was that two protocol options could be made to seem equally
good.  To solve this example, identify that as the problem, then
engage a judgement to pick one solution "good enough" (but that does
not lead to a dead-end for other goals of the project), and go with
it.  There is room, within "rough consensus", for such action to
defend against the attack; as you can see from other excerpts in this
message.

  "[Cryptography] asymmetric attacks on crypto-protocols - the rough
  consensus attack"
  http://www.metzdowd.com/pipermail/cryptography/2015-August/026151.html

To learn about forming a useful "rough consensus", see the very
readable "Tao of the IETF", and RFC 7282.

  "The Tao of the IETF"
  https://www.ietf.org/tao.html
    (previously RFC 4677)

  RFC 7282
  "On Consensus and Humming in the IETF"
  https://tools.ietf.org/html/rfc7282

Strong objections don't block rough consensus:

  https://www.ietf.org/tao.html#getting.things.done

    Rough consensus has been defined in many ways; a simple version is
    that it means that strongly held objections must be debated until
    most people are satisfied that these objections are wrong.

  https://tools.ietf.org/html/rfc7282

    Having full consensus, or unanimity, would be ideal, but we don't
    require it: Requiring full consensus allows a single intransigent
    person who simply keeps saying "No!" to stop the process cold.  We
    only require rough consensus: If the chair of a working group
    determines that a technical issue brought forward by an objector
    has been truly considered by the working group, and the working
    group has made an informed decision that the objection has been
    answered or is not enough of a technical problem to prevent moving
    forward, the chair can declare that there is rough consensus to go
    forward, the objection notwithstanding.

The working group chair's responsibility is different from that of
either a vote counter or a benign dictator:

  http://tools.ietf.org/html/rfc2418

    Note that 51% of the working group does not qualify as "rough
    consensus" and 99% is better than rough.  It is up to the Chair to
    determine if rough consensus has been reached.

  https://tools.ietf.org/html/rfc7282

    3.  Rough consensus is achieved when all issues are addressed, but
         not necessarily accommodated

      [...]

      If the chair finds, in their technical judgement, that the issue
      has truly been considered, and that the vast majority of the
      working group has come to the conclusion that the tradeoff is
      worth making, even in the face of continued objection from the
      person(s) who raised the issue, the chair can declare that the
      group has come to rough consensus.  (And even though this is
      framed in terms of a "vast majority", even that is not
      necessarily true.  This point is discussed in more detail in
      Sections 6 and 7.)

      [...]

      The chair of a working group who is about to find that there is
      only rough consensus is going to have to decide that not only
      has the working group taken the objection seriously, but that it
      has **fully examined the ramifications** of not making a change
      to accommodate it, and that the outcome does not constitute a
      failure to meet the technical requirements of the work.

      [...]

    6.  One hundred people for and five people against might not be
         rough consensus

      [...] one of the great strengths of using consensus over voting:
      It isn't possible to use "vote stuffing" (simply recruiting a
      large number of people to support a particular side, even people
      who have never participated in a working group or the IETF at
      all) to change the outcome of a consensus call.  As long as the
      chair is looking for outstanding technical objections and not
      counting heads, vote stuffing shouldn't affect the outcome of
      the consensus call.

    7.  Five people for and one hundred people against might still be
         rough consensus

      [...Sybil attack] it is within bounds for the chair to say, "We
      have objections, but the objections have been sufficiently
      answered, and the objectors seem uninterested in participating
      in the discussion.  Albeit rough in the extreme, there is rough
      consensus to go with the current solution."

      [...] it is likely that if a working group got this
      dysfunctional, it would put the whole concept of coming to rough
      consensus at risk.  But still, the correct outcome in this case
      is to look at the very weak signal against the huge background
      noise in order to find the rough consensus.

Working group chairs can help direct discussion:

  https://www.ietf.org/tao.html#rfc.section.4.1

    Sometimes discussions get stuck on contentious points and the
    chair may need to steer people toward productive interaction and
    then declare when rough consensus has been met and the discussion
    is over.

Some working groups segregate the role of forming a consensus from
communicating the consensus:

  https://www.ietf.org/tao.html#rfc.section.4.2

    Another method that some Working Groups adopt is to have a Working
    Group "secretary" to handle the juggling of the documents and the
    changes.  The secretary can run the issue tracker if there is one,
    or can simply be in charge of watching that all of the decisions
    that are made on the mailing list are reflected in newer versions
    of the documents.

Bitcoin Core is neither an IETF working group, nor should it aim to
curate its network protocol ruleset as one.  The IETF uses a steering
group, formal variance procedures, an appeals board, and a director
(to send even higher appeals to).  All of those positions could become
points of attack, if Bitcoin were to attempt to use or copy them.
That said, most IETF appeal routes are merely authorized to undo a
prior ruling of consensus, opening for reconsideration prior dismissed
points of argument (on their technical merits).  In Bitcoin, if
developers know what to work on, and can speak clearly enough to the
economic majority, then the system is working; regardless of whether
any role exists taking all the responsibility that an IETF working
group chair would take.

It is absolutely the case that resolving excessive roughness in shared
consensus takes more work than either votes or dictatorship.  It is
also the case that rough consensus is a good defense against
committing to decisions with subtle undesirable long-term effects.
That is why the IETF cares about it, and that same long-term threat is
important in Bitcoin's ecosystem as well.


/// References and Selected IETF Excerpts ///

  "The Tao of the IETF"
  https://www.ietf.org/tao.html

    A 2012 continuation of 2006's RFC 4677, itself first published in
    1994.


  BCP 25
  http://tools.ietf.org/html/rfc2418
    (1998)

    3.3. Session management

      Working groups make decisions through a "rough consensus"
      process.  IETF consensus does not require that all participants
      agree although this is, of course, preferred.  In general, the
      dominant view of the working group shall prevail.  (However, it
      must be noted that "dominance" is not to be determined on the
      basis of volume or persistence, but rather a more general sense
      of agreement.)  Consensus can be determined by a show of hands,
      humming, or any other means on which the WG agrees (by rough
      consensus, of course).  Note that 51% of the working group does
      not qualify as "rough consensus" and 99% is better than rough.
      It is up to the Chair to determine if rough consensus has been
      reached.

      In the case where a consensus, which has been reached during a
      face-to-face meeting, is being **verified on a mailing list**,
      the people who were in the meeting and expressed agreement must
      be taken into account.  If there were 100 people in a meeting
      and only a few people on the mailing list disagree with the
      consensus of the meeting then the consensus should be seen as
      being verified.  Note that enough time should be given to the
      verification process for the mailing list readers to understand
      and consider any objections that may be raised on the list.  The
      normal two week last-call period should be sufficient for this.

      [...]

      To facilitate making forward progress, a Working Group Chair may
      wish to decide to reject or defer the input from a member, based
      upon the following criteria:

        - Old

          The input pertains to a topic that already has been resolved
          and is redundant with information previously available;

        - Minor

          The input is new and pertains to a topic that has already
          been resolved, but it is felt to be of minor import to the
          existing decision;

        - Timing

          The input pertains to a topic that the working group has not
          yet opened for discussion; or

        - Scope

          The input is outside of the scope of the working group
          charter.

    [...]


  RFC 2026
  "The Internet Standards Process -- Revision 3"
  http://tools.ietf.org/html/rfc2026#section-6.5

    6.5 Conflict Resolution and Appeals
    [...]


  RFC 7282
  "On Consensus and Humming in the IETF"
  https://tools.ietf.org/html/rfc7282

    1.  Introduction

      [...] our credo is that we don't let a single individual dictate
      decisions (a king or president), nor should decisions be made by
      a vote, nor do we want decisions to be made in a vacuum without
      practical experience.  Instead, we strive to make our decisions
      by the consent of all participants, though allowing for some
      dissent (rough consensus), and to have the actual products of
      engineering (running code) trump theoretical designs.

      Having full consensus, or unanimity, would be ideal, but we
      don't require it: Requiring full consensus allows a single
      intransigent person who simply keeps saying "No!" to stop the
      process cold.  We only require rough consensus: If the chair of
      a working group determines that a technical issue brought
      forward by an objector has been truly considered by the working
      group, and the working group has made an informed decision that
      the objection has been answered or is not enough of a technical
      problem to prevent moving forward, the chair can declare that
      there is rough consensus to go forward, the objection
      notwithstanding.

    2.  Lack of disagreement is more important than agreement

      [...] **determining** consensus and **coming to** consensus are
      different things than **having** consensus [emphasis in
      original].

      [...]If at the end of the discussion some people have not gotten
      the choice that they prefer, but they have become convinced that
      the chosen solution is acceptable, albeit less appealing, they
      have still come to consensus.  Consensus doesn't require that
      everyone is happy and agrees that the chosen solution is the
      best one.  Consensus is when everyone is sufficiently satisfied
      with the chosen solution, such that they **no longer have
      specific objections** to it.

      [...] "Can anyone not live with choice A?" is more likely to
      only hear from folks who think that choice A is impossible to
      engineer given some constraints.  Following up with, "What are
      the reasons you object to choice A?" is also essential.

      [...]

      There is also an important point to be made about reaching
      consensus and "compromising": Unfortunately, the word
      "compromise" gets used in two different ways, and though one
      sort of compromising to come to consensus is good (and
      important), the other sort of compromising in order to achieve
      consensus can actually be harmful.  As mentioned earlier,
      engineering always involves balancing tradeoffs, and figuring
      out whether one engineering decision makes more sense on balance
      compared to another involves making engineering "compromises":
      We might have to compromise processor speed for lower power
      consumption, or compromise throughput for congestion resistance.
      Those sorts of compromises are among **engineering choices**,
      and they are **expected and essential**.  We always want to be
      weighing tradeoffs and collectively choosing the set that best
      meets the full set of requirements.

      However, there is another sense of "compromise" that involves
      compromising between people, not engineering principles.  For
      example, a minority of a group might object to a particular
      proposal, and even after discussion still think the proposal is
      deeply problematic, but decide that they don't have the energy
      to argue against it and say, "Forget it, do what you want".
      That surely can be called a compromise, but a chair might
      mistakenly take this to mean that they agree, and have therefore
      come to consensus.  But really all that they've done is
      capitulated; they've simply given up by trying to appease the
      others.  That's not coming to consensus; there still exists an
      outstanding unaddressed objection.  Again, if the objection is
      only that the choice is not ideal but is otherwise acceptable,
      such a compromise is fine.  But **conceding** when there is a
      real outstanding technical objection **is not coming to
      consensus**.

      [...]

      Coming to consensus is when everyone (including the person
      making the objection) comes to the conclusion that either the
      objections are valid, and therefore make a change to address the
      objection, or that the objection was not really a matter of
      importance, but **merely a matter of taste**.  Of course, coming
      to full consensus like that does not always happen.  That's why
      in the IETF, we talk about "rough consensus".

    3.  Rough consensus is achieved when all issues are addressed, but
not necessarily accommodated

      [...]

      If the chair finds, in their technical judgement, that the issue
      has truly been considered, and that the vast majority of the
      working group has come to the conclusion that the tradeoff is
      worth making, even in the face of continued objection from the
      person(s) who raised the issue, the chair can declare that the
      group has come to rough consensus.  (And even though this is
      framed in terms of a "vast majority", even that is not
      necessarily true.  This point is discussed in more detail in
      Sections 6 and 7.)

      [...]

      The chair of a working group who is about to find that there is
      only rough consensus is going to have to decide that not only
      has the working group taken the objection seriously, but that it
      has **fully examined the ramifications** of not making a change
      to accommodate it, and that the outcome does not constitute a
      failure to meet the technical requirements of the work.

      In order to do this, the chair will need to have a good idea of
      the purpose and architecture of the work being done, perhaps
      referring to the charter of the working group or a previously
      published requirements document, or even consulting with other
      experts on the topic, and then the chair will use **their own
      technical judgement** to make sure that the solution meets those
      requirements.  It is possible that the chair can come to the
      wrong conclusion, and the chair's conclusion is always
      appealable should that occur, but the chair must use their
      judgement in these cases.  What can't happen is that the chair
      bases their decision solely on hearing a large number of voices
      simply saying, "The objection isn't valid."  That would simply
      be to take a vote.  A **valid justification needs to me made**.

      [...] Indeed, RFC 2418 adds on to [old talk of balloting] by
      stating, "Note that 51% of the working group does not qualify as
      'rough consensus' and 99% is better than rough."  This document
      actually disagrees with the idea that simply balloting or
      otherwise looking at percentages can "determine" consensus.
      While counting heads might give a good guess as to what the
      rough consensus will be, doing so can allow important minority
      views to get lost in the noise.  One of the strengths of a
      consensus model is that minority views are addressed, and using
      a rough consensus model should not take away from that.  That is
      why this document talks a great deal about looking at open
      issues rather than just counting the number of people who do or
      do not support any given issue.  Doing so has some interesting
      and surprising implications that are discussed in subsequent
      sections.

      Any finding of rough consensus needs, at some level, to provide
      a **reasoned explanation** to the person(s) raising the issue of
      why their concern is not going to be accommodated.  A good
      outcome is for the objector to **understand the decision taken
      and accept the outcome**, even though their particular issue is
      not being accommodated in the final product.

      Remember, if the objector feels that the issue is so essential
      that it must be attended to, they always have the option to file
      an appeal.  A technical error is always a valid basis for an
      appeal. [...]

    4.  Humming should be the start of a conversation, not the end

      [...] a show of hands might leave the impression that the number
      of people matters in some formal way.

    5.  Consensus is the path, not the destination

      We don't try to reach consensus in the IETF as an end in itself.
      We use consensus-building as a tool to get to the best technical
      (and sometimes procedural) outcome when we make decisions.
      Experience has shown us that traditional voting leads to gaming
      of the system, "compromises" of the wrong sort as described
      earlier, important minority views being ignored, and, in the
      end, worse technical outcomes.

    6.  One hundred people for and five people against might not be
rough consensus

      [...] one of the great strengths of using consensus over voting:
      It isn't possible to use "vote stuffing" (simply recruiting a
      large number of people to support a particular side, even people
      who have never participated in a working group or the IETF at
      all) to change the outcome of a consensus call.  As long as the
      chair is looking for outstanding technical objections and not
      counting heads, vote stuffing shouldn't affect the outcome of
      the consensus call.

      [...]

      Even if no particular person is still standing up for an issue,
      that doesn't mean an issue can be ignored.  As discussed
      earlier, simple capitulation on an issue is not coming to
      consensus.  But even in a case where someone who is not an
      active participant, who might not care much about the fate of
      the work, raises a substantive issue and subsequently
      disappears, the issue needs to be addressed before the chair can
      claim that rough consensus exists.

    7.  Five people for and one hundred people against might still be
rough consensus

      [...Sybil attack] it is within bounds for the chair to say, "We
      have objections, but the objections have been sufficiently
      answered, and the objectors seem uninterested in participating
      in the discussion.  Albeit rough in the extreme, there is rough
      consensus to go with the current solution."

      [...] it is likely that if a working group got this
      dysfunctional, it would put the whole concept of coming to rough
      consensus at risk.  But still, the correct outcome in this case
      is to look at the very weak signal against the huge background
      noise in order to find the rough consensus.

    9.  Security Considerations

      "He who defends with love will be secure." -- Lao Tzu

-------------------------------------
I am fully in support of the plan laid out in "Capacity increases for the
bitcoin system".

This plan provides real benefit to the ecosystem in solving a number of
longstanding problems in bitcoin. It improves the scalability of bitcoin
considerably.

Furthermore it is time that we stop bikeshedding, start implementing, and
move forward, lest we lose more developers to the toxic atmosphere this
hard-fork debacle has created.

On Mon, Dec 21, 2015 at 12:33 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Dec 9, 2015 7:41 AM, "Jonathan Toomim via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

size of fraud proofs considerably, makes the whole design more elegant and
less kludgey, and is safer for clients who do not upgrade in a timely
fashion.

I agree, although I disagree with the last reason.

assumptions of non-upgraded clients (including SPV wallets). I think that
for these clients, no data is better than invalid data. Better to force
them to upgrade by cutting them off the network than to let them think
they're validating transactions when they're not.

I don't undesrtand. SPV nodes won't think they are validating transactions
with the new version unless they adapt to the new format. They will be
simply unable to receive payments using the new format if it is a softfork
(although as said I agree with making it a hardfork on the simpler design
and smaller fraud proofs grounds alone).

bitcoin-dev@lists.linuxfoundation.org> wrote:
-------------------------------------
I agree that NFC is the best we have as far as a trust anchor that you 
are paying the right person. The thing I am worried about is the privacy 
loss that could happen if there is someone passively monitoring the 
connection. So, in response to some of your comments below and also in 
response to some of Eric Voskuil's comments in another recent e-mail:

Consider some cases:

If NFC is assumed private, then sending the session key over the NFC 
connection gives the payer and the payee assumed confidence that that a 
private bluetooth connection can be created.

If the NFC actually isn't private, then by sending the session key over 
it means the bluetooth connection is not private. An eavesdropper can 
listen to all communication and possibly modify the communication, but 
the payer and payee won't necessarily know if eavesdropping occurs 
unless communication is also modified (which could be difficult to do 
for a really low range communication).

If we send a public key of the payee over the NFC connection (in place 
of a session key) and the NFC connection is assumed trusted (and is 
unmodified but actually monitored by an eavesdropper) and use that 
public key received via NFC to encrypt a session key and send it back 
via bluetooth, to then initiate an encrypted bluetooth connection using 
that session key for the remaining communication, then the payee still 
receives payment as expected and the payer sends the payment they 
expected, and the eavesdropper doesn't see anything.

If we send a public key of the payee over the NFC connection (in place 
of a session key) and the NFC connection is assumed trusted (and is 
actually modified by an eavesdropper) and use that public key received 
via NFC to encrypt a session key and send it back via bluetooth, to then 
initiate an encrypted bluetooth connection using that session key for 
the remaining communication, then the payee receives no payment and the 
attack is quickly identified because the customer receives no product 
for their payment and they notify the payee, and hopefully the problem 
remedied and no further customers are affected. The privacy loss will be 
significantly reduced and the motive for such attacks will be reduced. 
It's possible a really sophisticated modification could be done where 
the attacker encrypts and decrypts the communication and then relays to 
each party (without them knowing or any glitches detected), but I guess 
I'm not sure how easy that would be on such a close proximity device?

Erick Voskuil mentioned this same problem would even occur if you had a 
hardwired connection to the payment terminal and those wires were 
compromised. I guess I still think what I am saying would be better in 
that case. There is also more obvious physical tampering required to 
mess with wires.

I'm not sure if there is any trust anchor required of the payer by the 
payee, is there? Eric also mentioned a need for this. Why does the payer 
care who they are as long as they get a payment received? Just to avoid 
a sophisticated modification" that I mention above? I can see how this 
could be the case for a longer range communication (like over the 
internet), but I'm not convinced it will be easy on really short ranges? 
It's almost like the attacker would be better off to just replace the 
entire POS internals than mess with an attack like that, in which case 
everything we could do locally (other than the payment request signing 
using PKI), is useless.

I'm not a cryptography expert so I apologize if there is something 
rudimentary that I am missing here.

Andy Schroder

On 02/22/2015 08:02 PM, Andreas Schildbach wrote:


-------------------------------------
Where would you verify that?

On 2/3/2015 10:03 AM, Brian Erdelyi wrote:

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 02/22/2015 10:17 AM, Natanael wrote:

You just disproved your own argument.

It is possible to predict risk, and therefore to price the risk.

You also noted that for some Bitcoin users, the price of that risk is
too high for the types of transactions in which wish to engage.

In what way does that translated into a universal requirement for
everybody to use multisignature notaries?

Surely the users who can afford the risk can use zero conf if they
like, and those who can't can use multisig notaries?
-----BEGIN PGP SIGNATURE-----

iQIcBAEBAgAGBQJU6gLmAAoJECpf2nDq2eYj8/AQAJfMtBqjo1Z2Z0A7OhE9iaYD
PqWXdRaCFwyV49RSDrRROrB9Vc7CENQsweHBSnNEmSj6la/YfjyobmaR5BMtTq73
ZaXOFYSGVa9S0j+1qTvz2MorBd6ocxckdunfN7N/uVb4NQRYTHUT8N7AyJgRFYO9
ElQU/8TcNCSRqSQc3z8rnUc8eN1+DgqkMDHM754huOgA0fz0OlxnLCddcCvLr0t7
ZPCtZI94FWQSWhzTK2oa41hh01xG+Eg5GhqGzM7WBqM6+d/CgNcUVeMnVOkkhgav
AmlE81Km9R4AlrsGT/CcGgaC+FvBhqmDYHAGOUG3hLP+MXMe4qA5TRoRKHFvq4Gw
nF6q+leI7z/TkKeiDcyEKKen5cU01SnZlVRnncccIxsjzNjCiBdXOTP6o0pTd34j
5VJQ04mF4sla5AaaSDtsbkZuMdqIZDMn1tWxbmXRQ2cUbCGoi4yYiUlqjetrs4e1
i7NopccLNVDwjGRRnaSs4KkpuW7s23XwKm6WVehrP7S9s1Bqc+84C/rL1G4IF3Ul
vOz+dfxpS+yeGdEDOxb92voKo+fvL/N1sH2+cqTemuYWArDOn1kK/qKdaEfnl9p2
VcPJWuik6Ywomg4fCWmTQWcDxbWiUT/Gb/niONOYQ6iJG7mU4SH9LFBDd8qV+ljN
RqUYrOBf/PaMneNxwJp+
=w36r
-----END PGP SIGNATURE-----
-------------------------------------
On Thu, Mar 12, 2015 at 02:16:38AM +0000, Thy Shizzle wrote:

Agreed, but I don't know the full background on this.


That's true for generating new mnemonics (i.e. same entropy can
generate any combinations of words), but not for converting a mnemonic
to a seed (i.e. a specific wordlist/passphrase should always generate
the same seed).  I agree that it's true that a static wordlist is
required once people have started using BIP39 for anything real and
changing the word lists will invalidate any existing mnemonics (unless
your 'new' wordlist simply substitutes one word for another and the
index mapping is made public ... which means it's not really an
arbitrary word list).


I don't see how this can work given the BIP39 spec as it is today
(there's simply no room for a version in the bits).  I do think
versioning would be nice, but as of now, I'm in the camp that thinks
complete wallet interoperability is a bit of a myth -- so long as you
can fundamentally move into/out of wallets at will.

-Neill.






-------------------------------------
On Sat, Aug 29, 2015 at 1:29 AM, Mark Friedenbach via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Mark and Jorge,

I am very glad you have brought up this particular objection because
it's something I thought about but was unclear if it was an opinion
that would be shared by others. I chose to omit it from the proposal
to see if it would come up during peer review.

I feel that giving miners a blank cheque to increase blocksize, by any
means, goes against a key design of bitcoin's security model. Full
nodes keep miners honest by ensuring by validating their blocks. Under
any voting-only scheme there is no way for full nodes to keep miners
in cheque because miner have free reign to increase the blocksize.

This problem can be solved by introducing a hard cap on blocksize. By
introducing an upper limit miners now have the freedom to increase
blocksize but only within defined parameters.  Remember my proposal
allows blocksize to increase and decrease in such a way that miners
must collectively agree if they want the size to increase.

I believe the idea of a hard upper limit has become rather politicised
but is essential to the security model of bitcoin.

With respect to the flexicap idea where miners can create a larger
block by paying extra difficulty, I believe that proposal has a
critical flaw because, as Gavin pointed out, it makes it very
expensive (and risky) to include a few extra transactions. I believe
it suffers from tragedy of the commons because there is no incentive
for the mining community to reach consensus. Each and every block is
going to be a gamble, "should we include a few extra transactions at
the risk of losing the block?". Under my proposal miners can
collectively agree to change the blocksize. Let's say they want a 10%
increase, they can collude together to make that increase and once
reached, it remains until they want to change it again. Yet, the upper
hard limit keeps the ultimate control of the maximum block size
squarely in the hands of full nodes.

Whilst the exact number may be up for discussion, I would propose an
initial upper limit of 8MB, so under my proposal the blocksize would
be flexible between 1MB and 8MB.

An alternative methodology to voting in the coinbase would be to
change the vote to be the blocksize itself

1. miners pay extra difficulty to create a larger block.
2. every 2016 blocks the average or median of the last 2016 blocks is
calculated and becomes the new maximum blocksize limit.

This would retain incentive to collude to increase blocksize, as well
as the property of costing to increase while being free to propose
decrease.

It would still require an upper blocksize limit in order for full
nodes to retain control. Without an upper limit, any proposal is going
to break the security model as full nodes give up some oversight
control over miners.

Another way of looking at these ideas is we're raising blocksize hard
limit (to 8MB or whatever is decided), but making a soft of "softer"
or inner limit part of consensus. Such a concept is not really
departing from the current idea of a soft limit except to make it
consensus enforced. Obviously it's not identical, but I think you can
see the similarities.

Does that make sense?

-------------------------------------
Makes sense.. So with that said, I'd propose the following criteria for
selecting UTXOs:

1. Select the smallest possible set of addresses that can be linked in
order to come up with enough BTC to send to the payee.
2. Given multiple possible sets, select the one that has the largest number
of UTXOs.
3. Given multiple possible sets, choose the one that contains the largest
amount of total BTC.
4. Given multiple possible sets, select the one that destroys the most
bitcoin days.
5. If there's still multiple possible sets, just choose one at random.

Once the final set of addresses has been identified, use ALL UTXOs from
that set, sending appropriate outputs to the recipient(s), a new change
address, and a mining fee.

Miners should be cognisant of and reward the fact that the user is making
an effort to consolidate UTXOs. They can easily spot these transactions by
looking at whether all possible UTXOs from each input addresses have been
used. Since most miners use Bitcoin Core, and its defaults, this test can
be built into Bitcoin Core's logic for determining which transactions to
include when mining a block.

--
*James G. Phillips IV*
<https://plus.google.com/u/0/113107039501292625391/posts>
<http://www.linkedin.com/in/ergophobe>

*"Don't bunt. Aim out of the ball park. Aim for the company of immortals."
-- David Ogilvy*

 *This message was created with 100% recycled electrons. Please think twice
before printing.*

On Sat, May 9, 2015 at 3:38 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

-------------------------------------
The discussion on block size increase has brought some attention to the
other elephant in the room: Long-term mining incentives.

Bitcoin derives its current market value from the assumption that a
stable, steady-state regime will be reached in the future, where miners
have an incentive to keep mining to protect the network. Such a steady
state regime does not exist today, because miners get most of their
reward from the block subsidy, which will progressively be removed.

Thus, today's 3 billion USD question is the following: Will a steady
state regime be reached in the future? Can such a regime exist? What are
the necessary conditions for its existence?

Satoshi's paper suggests that this may be achieved through miner fees.
Quite a few people seem to take this for granted, and are working to
make it happen (developing cpfp and replace-by-fee). This explains part
of the opposition to raising the block size limit; some people would
like to see some fee pressure building up first, in order to get closer
to a regime where miners are incentivised by transaction fees instead of
block subsidy. Indeed, the emergence of a working fee market would be
extremely reassuring for the long-term viability of bitcoin. So, the
thinking goes, by raising the block size limit, we would be postponing a
crucial reality check. We would be buying time, at the expenses of
Bitcoin's decentralization.

OTOH, proponents of a block size increase have a very good point: if the
block size is not raised soon, Bitcoin is going to enter a new, unknown
and potentially harmful regime. In the current regime, almost all
transaction get confirmed quickly, and fee pressure does not exist. Mike
Hearn suggested that, when blocks reach full capacity and users start to
experience confirmation delays and confirmation uncertainty, users will
simply go away and stop using Bitcoin. To me, that outcome sounds very
plausible indeed. Thus, proponents of the block size increase are
conservative; they are trying to preserve the current regime, which is
known to work, instead of letting the network enter uncharted territory.

My problem is that this seems to lacks a vision. If the maximal block
size is increased only to buy time, or because some people think that 7
tps is not enough to compete with VISA, then I guess it would be
healthier to try and develop off-chain infrastructure first, such as the
Lightning network.

OTOH, I also fail to see evidence that a limited block capacity will
lead to a functional fee market, able to sustain a steady state. A
functional market requires well-informed participants who make rational
choices and accept the outcomes of their choices. That is not the case
today, and to believe that it will magically happen because blocks start
to reach full capacity sounds a lot like like wishful thinking.

So here is my question, to both proponents and opponents of a block size
increase: What steady-state regime do you envision for Bitcoin, and what
is is your plan to get there? More specifically, how will the
steady-state regime look like? Will users experience fee pressure and
delays, or will it look more like a scaled up version of what we enjoy
today? Should fee pressure be increased jointly with subsidy decrease,
or as soon as possible, or never? What incentives will exist for miners
once the subsidy is gone? Will miners have an incentive to permanently
fork off the last block and capture its fees? Do you expect Bitcoin to
work because miners are altruistic/selfish/honest/caring?

A clear vision would be welcome.


-------------------------------------
On Dec 8, 2015 7:08 PM, "Wladimir J. van der Laan via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
completely
can test as well.

Testnet4 ?
-------------------------------------
Trimmed some of the thread to clarity

On 10/08/2015 19:40, Luke Dashjr wrote:
True, but as Jorge points out, it's generally better not to have special 
cases.

It was something I was thinking about with the BIP 66 fork, that it 
could be used as a safety measure, but could also be used to find 
merchants & exchanges who are accepting coins on the wrong branch of a 
fork (and therefore are susceptible to double-spend attacks). For fork 
detection it's probably safer for the client to provide a recent block 
hash with the payment response.

I think genesis hash collisions are probably acceptable; or, the 
duplicate coins are so far rarely long-lived and it's therefore not a 
major concern at least. The server should reject any attempt to pay on 
the wrong chain, in hindsight, as it will try to relay on the network it 
expects and the transaction will be rejected.


That could definitely be done, for example by making the genesis field 
"repeated", so it specifies all potential networks. The response would 
need to indicate which hash it used, but that could be chain tip (with 
height in a further field), and provide fork detection.

Ross


-------------------------------------
I think we should just do it, and include it with the other DERSIG changes
for 0.10.

On Tue, Feb 3, 2015 at 1:15 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

-- 
--
Gavin Andresen
-------------------------------------
I posted a new draft of the proposal:
http://blockhawk.net/bitcoin-dev/bipwiki.html

The subsections still need to be fleshed out a bit more. I'd love any
comments or suggestions.

On Mon, Aug 24, 2015, 4:30 PM Eric Lombrozo <elombrozo@gmail.com> wrote:

-------------------------------------


------ Original Message ------
From: "Milly Bitcoin via bitcoin-dev" 
<bitcoin-dev@lists.linuxfoundation.org>
To: bitcoin-dev@lists.linuxfoundation.org
Sent: 9/20/2015 3:02:32 PM
Subject: Re: [bitcoin-dev] Scaling Bitcoin conference micro-report

Almost none of these merchants depend on Bitcoin in any significant way 
for revenue...and that's likely to remain the case for a good while. 
Merchants that have chosen to accept Bitcoin are typically using a 
handful of payment processors, again...chokepoints. And almost none of 
them are contributing any network resources back to Bitcoin.

Exchanges are indeed serious chokepoints. But increasing the number of 
users will probably have relatively little effect on this unless we also 
increase the number of exchanges and decentralize the exchanges. If all 
we had to do is increase the number of users, the same argument could be 
used to claim that banks would be less susceptible to governmental 
crackdowns if they just had more account holders.

Exchange decentralization is indeed another thing we must work towards - 
but that's probably beyond the scope of the more pressing issue which is 
building consensus in Bitcoin development.

I've pointed out this weakness of Bitcoin *numerous* times. That I 
failed to mention it here does not mean it hasn't been discussed 
elsewhere. Some of us have also been actively working towards developing 
a more modular, layered architecture and better implementations that 
will afford greater decentralization in software development with less 
need for critical code reviews, less pushback from downstream developers 
who must continuously rebase, a better process for building consensus in 
the community, and simpler app migration.

We need to increase the basic infrastructure nodes by a factor much 
larger than 2 or 3...more like 100 or 1000...and it's entirely doable 
with properly aligned incentives.



-------------------------------------
Yes that is completely doable for the next crawl, however I am not sure how much that reflects the behavior bitcoind would see when making connections. Nodes do not make any attempt to sync with close peers, which is an undesirable property if you are attempting to be sybil resistant. With some quick playing around it seems that you do get the expected speedup with close proximity, but it's not a particularly huge difference at present. I'll keep working on it and see where I get. 



-------------------------------------
Sure, and you did indeed say that.
-------------------------------------
On Wed, Sep 23, 2015 at 3:24 PM, Gregory Maxwell <gmaxwell@gmail.com> wrote:


I'm assuming the optimized protocol would be forward-error-coded (e.g.
using IBLTs)  and NOT require the full solution (or follow-on weak blocks)
to be exactly the same.



Yup, although I don't get the 'merge mined' bit; the weak blocks are
ephemeral, probably purged out of memory as soon as a few full blocks are
found...



I don't see any incentive problems, either. Worst case is more miners
decide to skip validation and just mine a variation of the
highest-fee-paying weak block they've seen, but that's not a disaster--
invalid blocks will still get rejected by all the non-miners running full
nodes.

If we did see that behavior, I bet it would be a good strategy for a big
hashrate miner to dedicate some of their hashrate to announcing invalid
weak blocks; if you can get your lazy competitors to mine it, then you
win....

-- 
--
Gavin Andresen
-------------------------------------


On 08/20/15 21:26, Tamas Blummer wrote:

I'm not suggesting pluggable networking, I'm suggesting (and I think
everyone thinks the design should be) NO networking. The API is
ValidationResult libconsensus.HeyIFoundABlock(Block) and
ListOfBlocksToDownloadNext libconsensus.HeyIFoundAHeaderList(ListOfHeaders).


Are you suggesting to support altcoins? I dont think anyone cares about
supporting that.


I think you'd be very pleasantly surprised. It sounds like you havent
dug into Bitcoin Core validation code in years.


Hmm? The result would be an obviously correct consensus implementation
that everyone could use, instead of everyone going off and writing their
own and either being wrong, or never updating in the case of forks. Its
a huge deal to allow people to focus on making their libraries have good
APIs/Wallets/etc instead of focusing on making a working validation
engine (though maybe for that the p2p layer needs to also be in a library).


We have one, it just needs a few already obvious performance improvements.


There are a number of good development tools for C++ that allow this....


-------------------------------------
Understood. That is unfortunate, but not the end of the world. If you
could please give feedback also to these last comments / questions:

How far are we at this moment from BIP62? Can an user send a
non-malleable tx now, if enforces some additional rules?

As for the security of the system, it does not fully rely on txids being
non malleable, but see this quote from my previous email:

[QUOTE]
I am trying to build a bitcoin contract which will relay on 3 things:
- coinjoin / txes with inputs from multiple users which are signed by
all users after they are merged together (every user is sure his coins
will not be spent without the other users to spend anything, as per
agreed contract);
- pre-signed txes with nLockTime 'n' weeks. These txes will be signed
before the inputs being spent are broadcasted/confirmed, using the txid
provided by the user before broadcasting it. Malleability hurts here.
- P2SH

Another thing I would like to confirm, the 3 pieces of the bitcoin
protocol mentioned above will be supported in _any_ future transaction
version or block version, regardless what changes are made or features
added to bitcoin core? The contract needs to be built and left unchanged
for a very very long period of time...
[/END QUOTE]

Can you comment on the quote please?

So, basically transaction malleability could affect the system in the
way that a pre-signed tx which offers the insurance and which is sent to
the user before the user sends the coins (spending user's coins back to
him after a certain period of time) could be invalidated. The insurance
tx signature will still be good, but invalid overall since the input
(txid) being spent does not exist (was altered / modified). The coins
won't be stolen or lost, but a new tx needs to be signed with the
altered (new) txid, for the system to work.

So, an user creates a transaction TX1 sending the coins to the server
but does not broadcast it. Instead, he provides the txid of TX1 to the
server. Server generates another transaction TX2 which spends TX1 back
to the user, with an nLockTime. User checks and if everything ok
broadcasts TX1. In case the txid of TX1 will be altered/modified, TX2
will become invalid (since it will be spending an inexistent input), and
the server will need to re-create and sign TX2 with the new
(altered/modified) txid of TX1, as per agreed contract. Should the
server disappear after user broadcasts TX1 and before the
altered/modified txid of TX1 gets confirmed, user's coins are forever
locked. It is true that no third party can benefit from this type of
attack, only the user will result with coins locked, but it is something
which could be used by competition to make a service useless / annoying
/ too complicated or less safe to use.

How could I mitigate this?

Thanks you for your time and help.

On 4/17/2015 12:02 PM, Pieter Wuille wrote:


-------------------------------------
On Mon, Aug 31, 2015 at 10:49 AM, NxtChg <nxtchg@hush.com> wrote:

This is a technical list dedicated to technical discussions revolving
around the bitcoin protocol and related technologies. What I am saying
is your discussion topic is not relevant to the long established theme
of this list. Please have a bit of decorum and respect for people who
joined this list so they could participate in and review
technical/academic topics.

Clearly there *should* be a general bitcoin discussion list somewhere,
I don't disagree (and I have repeatedly asked for one to be created),
but regardless, this list isn't it. The flood of off topic and
non-technical posts make it almost impossible to follow any technical
proposals.

-------------------------------------
First of all I would like to say... LOL

Second Andrew LeCody is correct, this is off topic.

On 08/18/2015 04:31 PM, F L via bitcoin-dev wrote:

-------------------------------------
Den 8 mar 2015 02:36 skrev "Pavol Rusnak" <stick@gk2.sk>:
[...]
https://github.com/trezor/trezor-emu/commit/9f612c286cc7b8268ebaec4a36757e1c19548717

Reminds me of FIDO's U2F protocol.

http://fidoalliance.org/specifications
https://www.yubico.com/products/yubikey-hardware/fido-u2f-security-key/

It ties into the browser SSL session to make sure only the correct server
can get the correct response for the challenge-response protocol, so that
credentials phishing is blocked and worthless. A unique keypair is
generated for each service for privacy, so that you can't easily be
identified across services from the usage of the device alone (thus safe
for people with multiple pseudonyms).
-------------------------------------
Both wallet and server side implementations will be based on existing
code in me-friendly language (C++>Python>anything else). I don't have
a time for it right now but Crypto hackathon in Parallel Polis
(http://cryptohack.org/) seems like good opportunity for it. I will
let you know then.

2015-02-01 14:43 GMT+01:00 Mike Hearn <mike@plan99.net>:


-------------------------------------
Dear Greg,

I am moving our conversation into public as I've recently learned that you've been forwarding our private email conversation verbatim without my permission [I received permission from dpinna to share the email that proves this fact: http://pastebin.com/jFgkk8M3].
The proof is not "problematic."  Right now you're providing an example of what Mike Hearn refers to as "black-and-white" thinking.  Just because the proof makes simplifying assumptions, doesn't mean it's not useful in helping us to understand the dynamics of the transaction fee market.  Proofs about physical systems need to make simplifying assumptions because the physical world is messy (unlike the world of pure math).  

My proof assumes very reasonably that block solutions contain information (i.e., Shannon Entropy) about the transactions included in a block.  As long as this is true, and as long as miners act rationally to maximize their profit, then the fee market will remain "healthy" according to the definitions given in my paper.  This is the case right now, this is the case with the Relay Network, and this would be the case using any implementation of IBLTs that I can imagine, so long as miners retain the ability to construct blocks according to their own volition.  The "healthy fee market" result follows from the Shannon-Hartley theorem; the SH-theorem describes the maximum rate at which information (Shannon Entropy) can be transmitted over a physical communication channel.   

You are imagining an academic scenario (to use your own words: "perhaps of little practical relevance") where all of the block solutions announcements contain no information at all about the transactions included in the blocks.  Although I agree that the fee market would not be healthy in such a scenario, it is my feeling that this also requires miners to relinquish their ability to construct blocks according to their own volition (i.e., the system would already be centralized).  I look forward to reading a white paper where you show:

(a) Under what assumptions/requirements such a communication scheme is physically possible.

(b) That such a configuration is not equivalent to a single entity[1] controlling >50% of the hash power.

(c) That the network moving into such a configuration is plausible.

Lastly, I'd like to conclude by saying that we are all here trying to learn about this new amazing thing called Bitcoin.  Please go ahead and write a paper that shows under what network configuration my results don't hold.  I'd love to read it!  This is how we make progress in science!!

Sincerely, 
Peter

[1] For example, if--in order to achieve such a configuration with infinite coding gain--miners can no longer choose how to structure their blocks according to their own volition, then I would classify those miners as slaves rather than as peers, and the network as already centralized.


Link to forwarded email pastebin: http://pastebin.com/jFgkk8M3

-------------------------------------
http://xtnodes.com/
From: Brian Hoffman 
Sent: Monday, June 15, 2015 3:56 PM
To: Faiz Khan 
Cc: Bitcoin Dev 
Subject: Re: [Bitcoin-development] questions about bitcoin-XT code fork &non-consensus hard-fork

Who is actually planning to move to Bitcoin-XT if this happens? 

Just Gavin and Mike?




On Jun 15, 2015, at 6:17 PM, Faiz Khan <faizkhan00@gmail.com> wrote:


  I'm quite puzzled by the response myself, it doesn't seem to address some of the (more serious) concerns that Adam put out, the most important question that was asked being the one regarding personal ownership of the proposed fork: 

  "How do you plan to deal with security & incident response for the duration you describe where you will have control while you are deploying the unilateral hard-fork and being in sole maintainership control?"

  I do genuinely hope that whomever (now and future) wishes to fork the protocol reconsider first whether they are truly ready to test/flex their reputation/skills/resources in this way... Intuitively, to me it seems counterproductive, and I don't fully believe it is within a single developer's talents to manage the process start-to-finish (as it is non-trivial to hard-fork successfully, others have rehashed this in other threads)... 

  That being said I think it appropriate if Adam's questions were responded in-line when Mike is feeling up to it. I think that the answers are important for the community to hear when such a drastic change is being espoused. 

  Faiz

  On Mon, Jun 15, 2015 at 4:56 PM, Bryan Bishop <kanzure@gmail.com> wrote:

    On Mon, Jun 15, 2015 at 3:55 PM, Mike Hearn <mike@plan99.net> wrote:

      Re: anyone who agrees with noted non-programmers Mike&Gavin must be non-technical, stupid, uninformed, etc .... OK, go ahead and show them the error of their ways. Anyone can write blogs.

    I worry that if this is the level of care you take with reading and (mis)interpreting Adam's messages, that you might not be taking extreme care with evaluating consensus changes, even while tired or sleeping. I encourage you to evaluate both messages and source code more carefully, especially in the world of bitcoin. However, this goes for everyone and not just you. Specifically, when Adam mentioned your conversations with non-technical people, he did not mean "Mike has talked with people who have possibly not made pull requests to Bitcoin Core, so therefore Mike is a non-programmer". Communication is difficult and I can understand that, but we really have to be more careful when evaluating each other's messages; technical miscommunication can be catastrophic in this context. On the topic of whether you are a programmer, I suspect that ever since you built CIA.vc we have all known you're a programmer, Mike.


    - Bryan
    http://heybryan.org/
    1 512 203 0507

    ------------------------------------------------------------------------------

    _______________________________________________
    Bitcoin-development mailing list
    Bitcoin-development@lists.sourceforge.net
    https://lists.sourceforge.net/lists/listinfo/bitcoin-development


    -- 


    My regards,

    Faiz Khan


  ------------------------------------------------------------------------------

  _______________________________________________
  Bitcoin-development mailing list
  Bitcoin-development@lists.sourceforge.net
  https://lists.sourceforge.net/lists/listinfo/bitcoin-development



--------------------------------------------------------------------------------
------------------------------------------------------------------------------



--------------------------------------------------------------------------------
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development@lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development
-------------------------------------
On Thu, Feb 12, 2015 at 10:27:53AM -0500, Jeff Garzik wrote:

I find Peter's proposal relatively mild. I'd prefer that instead of
exchanges going bankrupt, that there be direct blockchain support
for key revocation and 'burning' stolen coins, and an economic 
ecosystem that supports insurance underwriters that pay out when
someone inevitably gets hacked. This would definitely be 'scorched
earth' at one level, but I think would make for a far more
transparent and friendly system.


Most money/payment systems include some method to reverse or undo
payments made in error. In these systems, the longer settlement times
you mention below are a feature, not a bug, and give more time for 
a human to react to errors and system failures.


I see a world in which we have many blockchains, along with not-quite
blockchain things like ripple that approximate that vision you have 
of 'credits'. But we cannot have one chain to rule them all, for there
are inherent engineering trade-offs that one chain can never resolve.

There appear to be some things we will never come to a consensus on, 
such as transaction reversibility, or what the optimal money supply
algorithm is. However we might learn a great deal from sharing code
and ideas. So in that line, see my thoughts on reversibility [3][4]

 
I'll say the same about not stuffing everthing on top of the same 
blockchain. We might very well have coffee shops that take coffecoin.
But Bitcoin will never be able to scale out horizontally like altcoins
can.
 
[3] https://bitbucket.org/tmagik/catoshi/issue/24
[4] https://bitbucket.org/tmagik/catoshi/issue/27

 

-- 
----------------------------------------------------------------------------
Troy Benjegerdes                 'da hozer'                  hozer@hozed.org
7 elements      earth::water::air::fire::mind::spirit::soul        grid.coop

      Never pick a fight with someone who buys ink by the barrel,
         nor try buy a hacker who makes money by the megahash



-------------------------------------
Three things:

1) Hostility is generally the result of perceived hostility.  If you assume
the best intentions of another person, you will eventually find yourself in
one of two places.  Either you will find truth with that person (becuase
they are also seeking it), or you will drive them away (because you will
ask questions that can't be answered by someone trying to deceive).

2) The Wiki says "The current Core developers are Wladimir J. van der Laan,
Gavin Andresen, Jeff Garzik, Gregory Maxwell, and Pieter Wuille."  I've
seen no hostility from any of these people.

3) The people who are threatened by Bitcoin aren't stupid enough to ignore
#1.  Can anyone imagine that they have not hired highly skilled
psychological warfare agnts to do everything they can to "help" assault
what we decentralization enthusiasts have been working for?

About #2: I'm actually blind to hostility, and that is an intentional
affectation in response to my recognition of #1 and #3 together.  If you
feel another person has expressed a bad idea, just ignore it.  If you feel
they might be misleading others, post a reply about what you know to clear
up any possible misconceptions.  There is no point in identifying
individuals who are being hostile, or pointing out hostility, or being
divisive.  Let the rest of us recognize it on our own.  Maybe send
something like what I'm writing now.

PS: If anyone is interested in conspiracy theories, I had written this into
my gmail compose window and (presumably) hit a wrong key which caused the
thread to be marked as spam and deleted my whole reply.  It hadn't even
saved a draft.  I've never seen gmail not save a draft before.

On Mon, Aug 17, 2015 at 9:55 AM, Eric Lombrozo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------------------------------
you cannot have it.

Neither do you or anyone else.


And it allows the minority to hold the majority hostage.

about to take them with community-owned Bitcoin and Other People's Money!

The same can be said about the other camp.

BitcoinXT is not going to fork the chain on a specific date no matter what.
People will be able to vote via block versions and once a sufficient
majority supports the extensions, everyone else will have a grace period to
upgrade. Only after that is a very small minority at risk of losing money.

That being said, I'd rather see a solution that everyone agrees on. My
personal opinion/hope is that Mike and Gavin are just applying pressure
where it's needed. But in the end, they can do whatever they want if they
have the necessary support. Permissionless innovation is one of bitcoins
virtues. In the end, only adoption will decide what bitcoin is and isn't.

2015-06-16 7:18 GMT+02:00 Venzen <venzen@mail.bihthai.net>:

-------------------------------------
On Wed, Jun 17, 2015 at 11:59 AM, Peter Todd <pete@petertodd.org> wrote:


I'm sorry for the distraction with the mailing list problems.

Taking an ecosystem view, the miners are important, so are all the other
participants who rely on it and  invest time, effort and energy to make
Bitcoin work and work well.

I am in contact with Primavera and it would appear that the Cyberport is
available for use on October 14 and 15 (Wed/Thursday).

Last November, this was where the Global Bitcoin Summit (Hong Kong)
<http://www.cyberport.hk/en/about_cyberport/our_5_centres/collaboration_centre/collaboration_news/2146>
was hosted with the participation of many of China's leading
Bitcoin-related companies. There is a meeting now in Shanghai.

It would be an honour to host a more technical meeting to discuss BIP100,
101 et al. should be interest to do so.

p.


-------------------------------------
Can you please add a discussion of the tradeoffs of decentralization vs
block size?

On Mon, Jun 22, 2015 at 11:18 AM, Gavin Andresen <gavinandresen@gmail.com>
wrote:

-------------------------------------
On Mon, Aug 17, 2015 at 1:02 AM, Cameron Garnham via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Some XTBTC advocates may sell all their BTC for XTBTC and viceversa.
But I'm afraid that what most currency speculators (thus most Bitcoin
holders) will do is just sell both all their BTC and XTBTC for fiat,
and wait for things to settle before deciding whether to re-enter or
not.
This could result in both currencies' prices going down to 1 usd cent,
nobody knows.


Unfortunately it also puts Bitcoin core in an extremely weaker
position than it was before the Schism hardfork.
Even if XT fails in making blocks bigger, it may destroy Bitcoin.
That's probably not the goal of Bitcoin XT, but I don't think Andresen
and Hearn fully undesrtand the risks of a Schism hardfork (not to
mention their "followers" in the interwebs).

Since we want to discard the assumption that Hearn and Andresen want
to make Bitcoin centralized or destroy it, it's reasonable to conclude
that have serious misunderstandings on how the global consensus works.
This is consistent with some of their strong positions on Bitcoin Core
policy defaults (like maintaining the first seen spending-conflict
replacement policy [the dumbest possible one after "last seen"]
forever).

On Mon, Aug 17, 2015 at 2:33 PM, Eric Lombrozo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

Yes, it seems the simplest way to permanently separate your BTC from
your XTBTC is to move them all in transactions bigger than 1MB. You
may need too many outputs to increase the size (thus also hurting the
utxo size in Bitcoin XT), but that's just a side effect.

-------------------------------------
Maybe you are confused with a compilation notice that would say "All 
Content Copyright and other rights reserved by its Respective Owners" or 
something similar.  That is not the same thing as claiming ownership 
using the "c" inside the circle.

There is also a difference between claiming a copyright for individual 
works as part of a compilation as opposed to claiming a copyright on the 
compilation itself (which is what the current notice is).

Russ


On 10/6/2015 1:08 AM, Milly Bitcoin wrote:



-------------------------------------
It makes economic sense to include a transaction on the Lightning Network,
iff the the fee to include the transaction on the blockchain is more than
than the Time Value of Money of the encumbered funds on the Lightening
Nodes amortized across the number of users pushing funds through a LN node.

Large-value transactions are going to hit the blockchain while smaller,
predictable, closer-to-median transaction-rates transactions will go into
the Lightning Network

Blockchain: Car Purchase, House Purchase, Unexpected Medical Expense
Lightning Network: Utility Bill, Groceries, Rent, Mortgage.


This is optimal because the network has minimized the set of
costs/externalities to the minimum necessary to conduct a series of
transactions
“It's tough to make predictions, especially about the future.”
The effect on fees is going to be hard to predict.
1.) One part depends on user behavior around the dynamics of bid-side
demand of fees. I.e. If there is a health ratio of
-users who want 1-block- times but are unwilling/unable to bid up the fee
of their transactions to push out other 1-block-confirmation transactions (AKA
how firm is that fee support presently and under dynamic conditions)
to
-users who take their transactions off the blockchain to LN

2.) New classes of transactions will be possible that aren't possible today.

3.) What market effect will the financial/technical potential of 'instant'
transaction (after a network-joining-intro period) have on Bitcoin's
utility/price/adoption?

It would be elucidating if any blockchain data scientists could study the
effect of the fee market when high-volume exchanges unexpectedly halted
trading.


I believe a more specific question to ask is: What will happen when there
isn't a convincing economic reason for a large majority of hashing power to
be bolstering PoW defense on the main blockchain?  Right now we have a
pretty good handle on the amount of hashing power that's pointed at
extending/defending the 'main' chain but don't have as good intel on how
much idle hashing power there is.  Idle hashing power becomes more of a
threat in market scenarios where chain-extending PoW is scarce (late-game
Bitcoin).

My humble prediction is that the necessary number of block confirmation
will go up and there were be non negligible mining power idle ready to
defend actors' preferred chain.  If this is the scenario that plays out, I
don't think it'll be very concerning;  large-value transaction that will be
on the blockchain have more flexible time-settlement tolerance (no one
needs their home-buying escrow to settle in <= 1 day) and lower-value
transaction that users want/need to be confirmed quickly will be confirmed
'instantly' over Lightning Network or another Bitcoin-anchored protocol.

P.S. I see lots of concern with respect to fee reduction directed at LN
while today there are already off-chain databases that remove fee
pressure.  Like the LN / off-chain databases or not, they will exists.

Daniel Stadulis


On Wed, Oct 14, 2015 at 8:37 AM, Bryan Bishop via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
[continued]





We looked at doing this in a single lookup as you did.  With one or two
currencies this can be potentially more efficient.  As the number of
supported currencies and addresses under a single name grows, however, this
solution becomes potentially more problematic.  Please follow the use cases
below:

Use case 1:  Wallet Name = "bob.foo.bar" or OpenAlias = "bob.foo.bar"

The only currency supported is bitcoin, and there are no colored coin
formats supported.

OpenAlias case:

1 packet lookup to "bob.foo.bar"
1 packet response with bitcoin address

= 2 packets


Wallet Name case:

1 packet lookup to "_wallet.bob.foo.bar"
1 packet response with supported address types
1 packet lookup to "_btc._wallet.bob.foo.bar"
1 packet response with bitcoin address

= 4 packets

Wallet Name Case 1a:

The wallet doing the lookup knows it wants bitcoin, so it skips the
supported addresses lookup

1 packet lookup to "_btc._wallet.bob.foo.bar"
1 packet response with bitcoin address

= 2 packets

In this use case we might create more traffic, but it could also be reduced
by doing smart lookups.


Use case 2:  Wallet Name = "bob.foo.bar" or OpenAlias = "bob.foo.bar"

Many currencies and colored coin addresses are supported under the same
name, lets say 100.  When you count different currencies and colored coin
types, it could easily be hundreds, or over a thousand.


OpenAlias case:

1 packet lookup to "bob.foo.bar"
100 packet responses with various addresses

= 101 packets


Wallet Name case:

1 packet lookup to "_wallet.bob.foo.bar"
1 packet response with supported address types
1 packet lookup to "_btc._wallet.bob.foo.bar"
1 packet response with bitcoin address

= 4 packets

Wallet Name Case 2a:

The wallet doing the lookup knows it wants bitcoin, so it skips the
supported addresses lookup

1 packet lookup to "_btc._wallet.bob.foo.bar"
1 packet response with bitcoin address

= 2 packets


While you may end doing "less lookups" under Open Alias, as it scales, you
end up causing a significant amount of extra, unnecessary traffic.

In addition to the obvious impact of being orders of magnitude more
wasteful than necessary, it also creates privacy "leakage" by returning
someone 100 different addresses when they only asked for one.

Finally, because a single packet UDP transaction for a DNS lookup can
create possibly hundreds of packets in response, the service can
essentially become an amplifier for DDoS attacks.  (If I spoof the source
address of my target with a query to a lookup that issues hundreds of
packets in response to one packet, and I can have a real impact :( )





I think this is a good point, and one we weighed.  When we were making our
original decisions.  Given the importance of ensuring that the lookups
return the correct value, and the known vulnerabilities in DNS without
DNSSEC, coupled with the fact that ICANN has mandated all zones and
registries move to DNSSEC, our belief was and is that it was worth the
trade off that there were cases where existing domains would not work.


It is important to note, that ICANN has "required" for some years that
registrars and registries support DNSSEC on the domains they register.  I
personally believe we shouldn't delay use of DNSSEC until their registries
had come up to current required Internet standards.  (Here are ICANN's
registrar requirements showing the DNSSEC requirement, btw:
https://www.icann.org/resources/pages/approved-with-specs-2013-09-17-en#operation
)

That said, what do others in the industry think?  We are basing our current
standard on our believed best practices, and defaulted to "first, lose no
money", given the irreversibility of bitcoin.





I think "DNSSEC is hard" is a bit of a boogey man that's not really true.
We are working on developing registrar by registrar instructions of how to
do this, and we have typically found that if you are setting up DNS by
yourself, adding DNSSEC doesn't take a lot of additional time, maybe an
hour or so depending on your registrar.

This known concern, however, is why when we launched our product (based on
our standard record formats) that we wanted to launch it with a variety of
options for people.

In addition to these options there are also other hosting providers, and
certain registrars that will allow you to setup DNSSEC on their DNS
platforms.


1> One could choose the "0 click" installation process of just getting a
free Wallet Name underneath their provider's name space.  This option has
been free to end users in all cases so far, and I expect it always will be
in the future (although that is up to the partner, so some may choose to
charge).

2> If they wanted some provider independence, but someone to manage things
for them, they can register a name through us and manage everything via our
web interface.  This can cost them as little as $1.95/yr through us.

3> Finally, for the "do it yourselfer" who wants full control themselves,
they can simply follow the formats on our developer page and do the whole
thing themselves.  If they do this by registering their own ".bit" this
will cost them less than $0.25.  If they have an existing domain name, even
less.







That's some interesting data, and runs counter to the research of the IETF
DNS working group.  If you are willing to share your data, I can put you in
touch with the appropriate folks there to share your research.  I'd also
love to see it!




I'd argue that we aren't locking "huge portions" of the Internet.  You are
correct that about 15% of TLD's are not yet signed, even though they were
required to be by ICANN.

As I said above, I believe the requirement to not lose money and the fact
that other options are available for those running on TLD's that are out of
compliance, is worth the trade off that some existing names won't work
until their TLD's come into compliance with current Internet standards.

And, as we covered above, we don't force anyone to use our registrar, nor
any of our services in order for Wallet Names to work for them.  We never
have.  It's just not who we are.
-------------------------------------
Hi Thomas,

Re: NetKi, I think any proposal in this space has to be an open standard,
almost by the definition of what it is. At any rate, it may be worth
talking to them. They have signed up to implement their system at least.

I did understand that your proposal does not rely on email - for instance a
web forum could issue username@reddit.com type aliases, even if those are
not also email accounts. I am just continuing the comparison against email
address certs.

It's also the case that a domain can use the DKIM setup without actually
offering email accounts. They can just have a web form or API that triggers
sending of the signed email (or simply, providing the signed headers
themselves). Thus the same system can be used transparently by both email
providers and other sites that don't give their users email addresses, but
would still like to use the same system.

Hardly anyone uses email certificates today, so I don't think it would


No, but obviously we'd like to change that! The holdup is not the
certificate side of things, really, but rather the lack of a
store-and-forward network for signed payment requests. I keep asking
someone to build one but I fear the problem is almost too simple ......
everyone who looks at this decides to solve 12 other problems
simultaneously, it gets complicated, then they never launch :(

Once there's a simple and robust way to get PaymentRequest's from one end
user to another, even when that first user is offline, then getting an
email cert issued is no big deal.

That does not look so... not until (1) BIP70 wallets integrate with

Any solution that separates identity providers from certificate issuers
would have these requirements, though. And as many identity providers today
do not wish to become CAs too, it seems fundamental.

I don't think it's such a problem, mind you. The crt.sh website is actually
a frontend to the CT protocol, which is a somewhat blockchain like audit
log that's eventually intended to contain all issued certificates. Right
now, of course, they focus on SSL certs because those are the most common
and important. If other kinds of certs became more widely used, support in
the infrastructure would follow.



Don't get me wrong - I would like to see a way for a domain to delegate
BIP70 signing power to a third party. For instance, this would mean payment
processors like BitPay could sign on the behalf of the merchant, and the
merchant identity would then show up in wallets. The "chain a cert off a
domain cert" trick would be a good way to do that, though rather than
hacking the X.509 stack to validate invalid stuff, at this point it may as
well be a custom (better) cert format. There's little reason to use X.509
beyond backwards compatibility.

But the most popular identity providers today either don't care about
Bitcoin at all, or worse, are developing competitors to it. So for real
adoption to occur, we must have solutions that do not require identity
provider cooperation. I realise this is a business reason rather than a
technical reason, but it's a very strong one - so bootstrapping off
existing infrastructure with a split CA/ID provider design still makes much
more sense to me.
-------------------------------------
Seems like a lot of effort and goodwill is being wasted on contention over
things we don't really need to agree upon. In order to help us better
prioritize, I propose adding an extra attribute to BIPs indicating their
"level" which is split into five as follows:

1. Consensus (hard/soft fork)
2. Peer Services
3. RPC
4. Implementations
5. Applications

I posted an example of what such a table might look like here: http://
blockhawk.net/bitcoin-dev/bipwiki.html

If other folks also think this is a good idea I'll start working on a BIP
draft for this.
-------------------------------------
I've been tossing around an idea in my head that involves time-locked
encryption [0] and I wondered what the devs here think about it. In a
nutshell: the timechain is a serial chain of time-lock encrypted GPG keys
at N minute intervals (meaning that it requires N minutes to decrypt a
single link / key in the chain and each link must be fully decrypted before
decryption can start on the next link.) For those not aware of how
time-lock encryption works it goes something like this:

1. Choose some random, unique text - this is the initialisation vector or
IV.

2. Hash that text -> output.

3. Hash the output -> output.

4. Hash the output -> output.

5. ...

6. Process is repeated for N minutes.

7. Result is then used to generate encryption keys and the public key can
be used to time-lock encrypt an arbitrary number of plaintexts.

8. All intermediary results are discarded -- only the pub key is kept and
giving out the IV forces an individual to have to repeat the same amount of
work used to generate the encryption key.

What's interesting about this is that the keys can be generated in parallel
and then "stitched" together to form a single serial chain of keys. So
potentially, if a person had access to a GPU cluster then they could
generate a years worth of keys in only 5 minutes. Now imagine if one were
to stitch these keys together into a chain of keys at five minute intervals
(a structure I refer to as the "timechain"): you could use this structure
to encrypt ECDSA keys which could then be used in multi-signature contract
schemes as a 100% decentralized, trustless way to execute refunds in
contract protocols.

Unexpected benefit: time-lock encryption can be used to build unbreakable
DACs.

Peter Todd has already done work on using Bitcoin to incentivize the
decryption process of time-lock encryption [1] but what he may not be aware
of is how important this process is for the construction of DACs.

Imagine a true peer-to-peer cryptocurrency exchange [2] that time-lock
encrypts a chain of ECDSA keys using the timechain and then sets up
contracts to pay a small portion of their fees "into" the ECDSA keys.
Essentially the exchange has created a DAC that pays its participants to
decrypt itself. This is the incentive for the decryption. The reason for
the incentive is that another chain of keys can be generate at 5 minute
intervals which can be used in contract protocols in place of nTimeLocked
refund transactions (which are vulnerable to transaction malleability.)

Sample contract using the timechain:

3 of 4 multi-sig: Owner, Owner, Recipient, Timelock

Pay N coins to recipient sequentially (micropayment channel) before [time /
date], otherwise fall back on timelock decrypted refund key to give full
leverage back to owner. This is how smart contracts would work using the
timechain for refunds (instead of nTimeLock TXs.)

Using the DAC, it might also be possible to force participants to reveal
their solutions to the decryption of the timechain (otherwise the first
person who starts on the chain would receive all the fees which isn't very
fair.) One way to do this would be to use the public key for the fee ECDSA
key as the IV used to generate the next key on the chain. To spend the fees
would therefore require revealing the public key if the fees were paid to a
pay-to-pub-key-hash transaction.

A further precaution would be to generate the pay to fee transaction in
such a way that the amount needed to be redeemed before a certain
time-frame otherwise another transaction would burn the coins. (I haven't
worked out the full details for this yet but similar schemes have been used
successfully, for example in BitHalo [3] and the Lightning Network [4]
offers another potential solution.) Perhaps a custom blockchain or
sidechain could be used to award coins for successful (and timely
solutions) but this is a subject for future work.

In conclusion: I have described a simple way to solve the TX malleability
problem in smart contract protocols without requiring a fork or relying on
a third-party escrow services to hold keys. My solution doesn't require any
trust beyond the initial need for the timechain to be generated in a secure
environment and the solution remains secure so long as participants stick
to using future keys in the chain regardless of how far along the
decryption is.

Critique / flaws

Obviously the biggest flaw here is that the integrity of a timechain can't
be known before-hand but if a timechain were to be generated securely by a
reputable party, the biggest benefit of using it is that it basically runs
itself: it does not require any third-party to manage its functionality and
the entity which originally generated it can completely disappear without
interrupting service. This could, for instance - allow companies to create
entirely secure and reliable systems that couldn't be hacked as the
behaviour of a timechain is deterministic. I think this is a huge
improvement over existing systems which require third-parties to be
perpetually trusted with managing key-pairs on their web servers.

You could even use collateral as a way to incentivize the reliable
construction of the timechain by collecting collateral into a multi-sig
controlled by a number of neutral parties and only releasing the coins back
to the entity if the chain behaves as expected. I imagine some kind of
signed copy of a GPU cluster bill + proof of code executed would be
additional proof.

Anyway, that's the basic idea. Let me know what you think.


Sources:

[0] http://www.gwern.net/Self-decrypting%20files

[1]
https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg05547.html

[2] http://www.uptrenda.com/uptrenda.pdf

[3] https://bithalo.org/wp-content/uploads/2014/06/whitepaper_twosided.pdf

[4] https://lightning.network/lightning-network-paper-DRAFT-0.5.pdf
-------------------------------------
I wrote an article that explains the hashing assurance contract concept:

https://medium.com/@octskyward/hashing-7d04a887acc8

(it doesn't contain an in depth protocol description)
-------------------------------------
On Wed, Sep 30, 2015 at 10:17 PM, Jeff Garzik <jgarzik@gmail.com> wrote:

An extra way to look at this is that even absent any rule changes--
users who are asleep at the switch may lose effective security over
time because attackers learn new tricks against existing
vulnerabilities. Security requires a bit of vigilance, inherently.

In many specific cases I think it's hard-to-impossible to articulate a
concrete way that security is lost by users at all, excluding some
small amplification of orphan blocks.


On Wed, Sep 30, 2015 at 9:06 PM, Mike Hearn via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:

This is the outcome guaranteed for absentee miners with a hard fork,
but it is not guaranteed for a soft fork.


Miners who have changed their code in inadvisable ways can produce
invalid blocks as a result. There are many seemingly innocuous ways
one can produce invalid blocks, and miners have stumbled on a few of
them over the years.

Pedantically, modifying IsStandard() will not have this effect:
Unknown NOPs are now handled via a script validation flag--
SCRIPT_VERIFY_DISCOURAGE_UPGRADABLE_NOPS.  Experience (e.g. with
STRICTDER) has show that script validation flags are much more robust
to casual twiddling than IsStandard is.

The only way that script validation flags have been observed getting
bypassed in the field was a miner that had disabled all signature
validation completely (and whom had a not-completely-negligible amount
of hashpower. :( )... as it's a lot more clear that you might be
exposing yourself to trouble if you mess with the validation flags.


IIRC; There is no released version of Bitcoin that has IsStandard
which has failed failed to treat the NOPs as non-standard.

There was a brief time in git master between when IsStandardness was
relaxed and NOPs were addressed via a validation flag but I am
reasonably confident that didn't make it into a release.

Regardless, anyone actually running that code of that vintage would
already be incompatible with the current network already due to prior
soft forks.

And as a matter of fact, invalid CLTVs don't currently appear to get
mined. Checking this again pre-release would be a good checklist item.
For prior soft-forks we've monitored and tested for this (with the
goal of going and yelling at any broken miners to fix their behavior).

-------------------------------------
On 11/05/2015 03:03 PM, Adam Back via bitcoin-dev wrote:

This side of the security model seems underappreciated, if not poorly
understood. Weakening is not just occurring because of the proliferation
of non-validating wallet software and centralized (web) wallets, but
also centralized Bitcoin APIs.

Over time developers tend to settle on a couple of API providers for a
given problem. Bing and Google for search and mapping, for example. All
applications and users of them, depending on an API service, reduce to a
single validator. Imagine most Bitcoin applications built on the
equivalent of Bing and Google.

e

-------------------------------------
Another option for how to deal with block withholding attacks: Give the miner who finds the block a bonus. This could even be part of the coinbase transaction.

Block withholding is effective because it costs the attacker 0% and costs the pool 100%. If the pool's coinbase tx was 95% to the pool, 5% (1.25 BTC) to the miner, that would make block withholding attacks much more expensive to the attacker without making a huge impact on reward variance for small miners. If your pool gets attacked by a block withholding attack, then you can respond by jacking up the bonus ratio. At some point, block withholding attacks become unfeasibly expensive to perform. This can work because the pool sacrifices a small amount of variance for its customers by increasing the bonus, but the block attacker sacrifices revenue. This should make the attacker give up before the pool does.

This system already exists in p2pool, although there the reward bonus for the block's finder is only 0.5%.

This must have been proposed before, right? Anyone know of a good analysis of the game theory math?
-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Sure, and I share your view - I want to know what is happening in the
Bitcoin development space when I scan this email account.

Unfortunately, one moral imbecile keeps polluting this space.

I am confident that I have the debating resources and mental savvy to
show this individual to be an intellectual lightweight and a bankrupt
speaker that should shut his mouth, for good, in this list.

I'm not sure where your own allegiances lie, but if you think XT is a
bright idea, you can go there and participate to your heart's content.
In the meantime, this is a Bitcoin Core developer list and
self-proclaimed enemies of Core should not post here or pretend to
have consensus-blocking powers, as O'Hearn keeps capitulating.

Let me put this aspiring lord in his place and then we talk again. I
will knock him down in round 1 and knock him out in round 3. If you
love this man, go to xt-dev list and enjoy and spend your coins in
that alternate chain.

Venzen Khaosan



On 10/07/2015 05:26 PM, Patrick Mccorry (PGR) wrote:
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (GNU/Linux)

iQEcBAEBAgAGBQJWFPZRAAoJEGwAhlQc8H1mKkUH/jAYVoTMT7OW0bg2dquYBmiF
pNbO5oaqO1F3QGLqlLs/S8h/rxkY0tfaae86HEZ/fVNn9r2k6aykmrrztAZdgOT+
p9a7Po/kdd5BPoJ1F84cTxaAiMtl7y8oPyT/np7Ky+Nnyxs2di5eKdV3UK4gKzpt
6xsCR16CinGC8LPveCkGHNgqlk0Of4sNpnkhW945oO2nJzVB3TogvhJQvecZ3hw9
MzIxj59lXI8EL9qhkE0Ufaia1fHfRorCU0qt7mAOxHaZt4Wy3CpgGqrrRGsqXdud
KC3xePRpCaRmLqyUQVXjXdlvANMnXje5dTg9LH0Gseuhb26RGfq7Z3lQ+3kacVo=
=gHMy
-----END PGP SIGNATURE-----

-------------------------------------
Oh, now I got the 'soft-fork' alternative. If that means that *senders* to
Trezor need to be nice guys and use some special outputs, then it's,
obviously, no-go solution.

I understand political aspect around hard-fork. Anyway, are there any other
pending projects waiting for hard-fork? Maybe we should join our effort in
some way.

M.

On Fri, Jan 23, 2015 at 5:27 PM, Alan Reiner <etotheipi@gmail.com> wrote:

-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Hello all,

BitPay has just released our first whitepaper on ChainDB, a new
peer-to-peer database system backed by the Bitcoin blockchain.  This
paper outlines our intended consensus mechanism, proof of fee.

Please take a look at the paper here: https://bitpay.com/chaindb.pdf

We are seeking comments and feedback on this mechanism.  I am happy to
answer any questions about the paper itself.

Sincerely,

Eric Martindale
-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - https://gpgtools.org

iQIcBAEBCgAGBQJVW2E9AAoJEHLoNvKeOhrJkLwP/1J14yGlZzddp4ApGRFsnnIz
t8U9uZVvjsqxseYv6Pw3ZStQRkuBgcPDcQwMexeBi/0Z5K34LOM1565XRLtNG2sb
AeLHG11ZLNK9SQSga2B0yc95uXs2Zje7Z+A+Q+h7HjhnkcQKbuLA+kB2+ZJv1CA3
dV/5A0oCMBbZukzuFkbgmnhCaNwYjWY15UbwksKb2c3ktuLxZ5zUq/ZI+W+0PZsN
Px2m/qkKb0UiUfbZU5Zva8HSI8lxQrEm/dkv4voglwlG3M7fvmgXcUi+8q0VslDi
2Bx99rhpBaC79eHDUouhTNvLykP7Hal4KdyuzShlNBN+Z6AQyoeOdAQhk9YNw/iq
c/tyiw6fFQVjEOJuJfetl2thByI+/hNH2m70BRXnaOtM+rQ4iIeaR7KevMi+WyYr
+X9M6eqaYvkVXD1y0lEDCfsatYIhLUcXUVkM8gAdXF2yatqfCHENVYdZu9EDhYNa
zC/N2akO+XNmj0a4mder3Oy0/j7vHTXq8HLHGFbCy3S3nld+A0Qe0/JTo/Vj1IZX
REyBnWsaguIE8l/I/+423rzQYKlSEwP5j+V/ObTYouVCwmy+uJC8evNCI8T/APy9
Y04ocYLb2DnKLDOD8mlf+huf4x9WwK8+CdF/wm2g1SxLBchy5lkrmhbbD846HiRF
m7EvzfRGI5zweCNIyx9Y
=nDJ2
-----END PGP SIGNATURE-----



-------------------------------------
Jorge Timón 於 2015-08-30 14:56 寫到:

The reason for 60% of block were generated in China is same as the 
reason for 60% of your clothes were made in China. The electricity there 
is the cheapest on the planet. Many dams were built in the past 10 years 
and now they have huge amount of surplus electricity due to economic 
downturn.

Not sure if you are aware of this thread: 
https://bitcointalk.org/index.php?topic=1072474.0 . Could you imagine 
this in any developed country? As long as mining is largely dependent on 
energy, there is no hope to break the balance/imbalance.

Bandwidth is probably only a few percent of miners' cost. There is no 
evidence that the current level of centralization is a result of block 
size. Instead, clear evidence has shown that centralization is a result 
of pool mining*, invention of ASIC, and disparity of energy cost. (* 
People started pool mining in 2010 because they wanted lower variance, 
not because of the inability to run a full node)



Even if we could quantify the level of centralization, it is a continuum 
and we must compromise between utility and centralization. Unless 
BIP101/103 is adopted, adjusting the hard cap always require a hardfork. 
For obvious technical and political reasons we can't have hardfork too 
frequently. Therefore, we need to leave some leeway: the hard cap may be 
a bit too high for today, but we are sure that technology will catch up 
in the near future.

Assuming we have plenty amount of "benevolent" miners, they will keep 
the block size low unless there is a real demand for larger block space. 
This is different from setting an individual soft limit, as that will 
lead to block size scarcity and therefore higher tx fee, which may be 
good for all miners. And as we say "miners can always decrease the block 
size with softfork or 51% attack", BIP100 materializes this possibility 
in a much smoother way.

I say "lucky" because I wholeheartedly believe it is good to keep the 
block as small as we really need. We can't do this by an equation so I 
would prefer to leave the power to miners (and they always have this 
power, anyway).

Jeff and Satoshi discussed this in 2010, although the flame throwing 
debate did not start until 2013.


The problem is the definition of "urgency" itself is controversial. And 
I believe an urgent hardfork should only be done as a bug fix, not 
implementation of a new feature. Block size increase should be a planned 
feature, as we don't want the tx fee raised to 10USD, before suddenly 
dropping to 0.01USD with the hardfork.

I don't think it's urgent today because my free tx always get mined. I 
don't know what is urgent and different people have different 
definition, but in general I think that should be measured by tx fee in 
USD. 0.001USD/byte may be intolerable for many people. (It's about 
$0.0001/byte now, and $0.0005/byte when it was $1200/BTC). It's not 
difficult to reach this level given the halving and potential bull 
market is coming.


As I argued above, it's already too late when things become really 
urgent. That will lead to serious market disruption, and the uncertainty 
could be very harmful to the development of the bitcoin economy.


As I said above, strictly limiting the block size may have little effect 
on mining centralization (because block size at this level is not a 
determining factor), while it may seriously suppress the utility.

I'm all for mining decentralization but block size is just not the right 
way to improve the situation.


I agree with you, but the balance between centralization and utility is 
also important. (and I believe the difference between 1MB and 8MB is 
tolerable, at least I must keep my full node running at this level)

I also have an idea to have a "decentralizedcoin", with very small 
blocks and everyone could mine with a CPU. That would be interesting if 
it is backed by famous devs in this area and is not 
yet-another-scamcoin.

-------------------------------------
I should add that a strategy of “let’s avoid fee pressure as much as possible. let’s avoid even thinking about how we’ll transition as much as possible.” strikes me as at least a tad bit myopic.

- Eric Lombrozo


-------------------------------------
On 10/5/2015 4:05 PM, Steven Pine via bitcoin-dev wrote:

"troll" and, even worse, "concern troll" are terms generally used by 
teenagers on places like Reddit to complain about someone who doesn't 
agree with them. It is not rally a valid term to use in technical 
discussions. Several of the developers on here act as bullies by 
wielding power they have accumulated in a a system which they claim is 
decentralized.  It is not clear at all so your premise is faulty.


What exactly do you expect?  Bitcoin is not a charity, it is built on 
incentives.


Only a very small minority of the developers have "integrity and trust." 
Most are pretty irrational and untrustworthy if you look at their 
discussions outside of their technical expertise.  Bitcoin is not 
supposed to have a model where users are not forced to trust a small 
group such as the core developers.  It sounds to me like you suggest 
giving that up the idea of decentralization so you can gain control over 
the "official" software releases.

Russ

Russ







-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On Thu, Jun 18, 2015 at 03:49:06PM +0200, Mike Hearn wrote:

It looks as if you entirely missed my point. I'm The Decider for *code issues* regarding Bitcoin Core. Consensus issues should not be considered part of that, they span multiple implementations.

So I'm *not* the decider for anything that concerns the behavior of the global consensus, and I cannot be, as I have explained in the previous post, and as Sipa explained in his.

Speaking of process, let me remind you that there is a BIP processs: https://github.com/bitcoin/bips/blob/master/bip-0001.mediawiki 

If you think it's not clear enough, which may explain why you did not even attempt to follow it for your block size increase, feel free to make improvements.

Wladimir
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJVgtAOAAoJEHSBCwEjRsmmLPUH/1ug5pvLz6ptIhvuROclV7Jh
z0Szk5FOqfg4ejT3nYV5LRV5WNHUGDdFnHZJRFsKYH9B0LFgOlnkc488Qg6hBb+1
rf5zEF/D2X4MhPIx6GqI++gvhDzdBH2t9yxbU7LVZALo7+wtW+ms5eHHFs8WrU0z
m7NgiZRen4cpQUiBWHlt0PojmXBVZQNU0CD6ErcOpQXhN8J0sb0l0DuFswQgUqxk
rrNe3LvKp89xT0kDxyzQts/CeIG/8kQYLwIJ1QQDXvYayj2aHHYMkSEWfDlew3IC
zQkFgHCTGihGHPFeow+dnuW1DI1l92yPYNDLbxivSam3X+qCAGzUTOWTFE+iprk=
=tE4K
-----END PGP SIGNATURE-----


-------------------------------------
On Tue, Oct 20, 2015 at 12:23 AM s7r via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:


The normalization involves two steps:
 - strip the scriptSig scripts in the inputs, i.e., the only part whose
integrity is not guaranteed by the signature itself, by replacing the
scripts with empty strings (var length string of size 0)
 - replace the hashes referencing the outputs being spent with the
normalized hashes of the transaction that created the outputs. This is done
recursively down to the first v2 transactions.

The second part is not yet explained in the draft, but I will amend it as
soon as possible.



Non-coinbase transactions can still not be replayed since the normalized
transaction still includes a the normalized transaction hashes of claimed
outputs, hence any attempt to replay a transaction would fail since the
outputs were already spent. For coinbase transactions it is indeed possible
that we create multiple transactions with the same hash (only one of which
would be spendable), hence we do not strip coinbase transactions and rely
on BIP 34 to make the coinbase transactions unique (except for blocks 91842
and 91880 which are the reason we introduced BIP 34 in the first place).
Clarifying the way the normalized transaction ID is computed should remove
any ambiguities I hope.



Yes, if the computation of the normalized transaction ID includes replacing
input hashes with their normalized counterpart makes a chain of any depth
non-malleable.

HTH,
Christian

-------------------------------------
On Tuesday, 26 May 2015, at 11:22 am, Danny Thorpe wrote:

The "First-Seen-Safe" replace-by-fee presently being discussed on this list disallows fraudulent payment reversals, as it disallows a replacing transaction that pays less to any output script than the replaced transaction paid.


-------------------------------------
On Sun, Aug 30, 2015 at 10:01:00PM +0200, Daniele Pinna via bitcoin-dev wrote:

FWIW I did a quick math proof along those lines awhile back too using
some basic first-year math, again proving that larger miners earn more
money per unit hashing power:

http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg03272.html

-- 
'peter'[:-1]@petertodd.org
000000000000000010b552c5f5c18705ccb1b21c550c08872089f89076840d6d
-------------------------------------
Please take the lightning 101 discussion to another thread.

The main point I was trying to make was that Mike is clearly misrepresenting the views of a great number of people who have deep, intimate knowledge of how things work and are almost certainly not primarily motivated by their own potential for profits.


-------------------------------------
Bitcoin Core version 0.9.5 is now available from:

  https://bitcoin.org/bin/0.9.5/

This is a new minor version release, with the goal of backporting BIP66. There
are also a few bug fixes and updated translations. Upgrading to this release is
recommended.

Please report bugs using the issue tracker at github:

  https://github.com/bitcoin/bitcoin/issues

How to Upgrade
===============

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes for older versions), then run the
installer (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or
bitcoind/bitcoin-qt (on Linux).

Notable changes
================

Mining and relay policy enhancements
------------------------------------

Bitcoin Core's block templates are now for version 3 blocks only, and any mining
software relying on its `getblocktemplate` must be updated in parallel to use
libblkmaker either version 0.4.2 or any version from 0.5.1 onward.
If you are solo mining, this will affect you the moment you upgrade Bitcoin
Core, which must be done prior to BIP66 achieving its 951/1001 status.
If you are mining with the stratum mining protocol: this does not affect you.
If you are mining with the getblocktemplate protocol to a pool: this will affect
you at the pool operator's discretion, which must be no later than BIP66
achieving its 951/1001 status.

0.9.5 changelog
================

- `74f29c2` Check pindexBestForkBase for null
- `9cd1dd9` Fix priority calculation in CreateTransaction
- `6b4163b` Sanitize command strings before logging them.
- `3230b32` Raise version of created blocks, and enforce DERSIG in mempool
- `989d499` Backport of some of BIP66's tests
- `ab03660` Implement BIP 66 validation rules and switchover logic
- `8438074` build: fix dynamic boost check when --with-boost= is used

Credits
--------

Thanks to who contributed to this release, at least:

- 21E14
- Alex Morcos
- Cory Fields
- Gregory Maxwell
- Pieter Wuille
- Wladimir J. van der Laan

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).



-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

That would be very wrong and cause a lot of problems and 'political
chaos' without solving at least one (technical) problem in exchange.

Bitcoin Core is a good quality code. It is open source and free.
Anyone can contribute and submit small changes, improvements.
Controversial changes are not easily merged not because the
maintainers do not want, but because they represent a threat to the
entire ecosystem, one way or the other. We have to very carefully
balance the gains and the risks. If we try to never reach a consensus
on purpose, this will only cause instability, and a possible result
could be that we will end up having many more weaker implementations
running in the network, decreasing the security overall and for everyone.

While I do agree with some of your points of view and I am happy to
see you advocate for 'more decentralization', please let me point you
in a better direction (I think): there is a much bigger problem than >
~90% of the full nodes running Bitcoin Core software - it is
*centralized mining (e.g. a lot of hashing power behind a single full
mining node)*.

On 9/1/2015 5:16 AM, Peter R wrote:
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.22 (MingW32)

iQEcBAEBCAAGBQJV5iFqAAoJEIN/pSyBJlsRMvIH/RiE8BhlXPbNOQW01HBJTBOD
3H4bgaZoXuxSq2B1F4zKa/FvKJKtq7BGR3hLEj5tascqZTE2YsksRqmEednFNvbL
XOliCjees6nI/oz/aYFuz9rFoKH4cxA7bJmbvieqGSOqDt7rtClaO2JzBycilngS
F5pVGjKlprprTn4XUS8R40rfYVFbYyxaMnWBOnkgEpEAbtEvNRcASSW4HQoxuGRL
6E8mzp8f23zAv6ENxKEfQoIf5SBBfYf8v2xV+YY9JcFjwh4MAQ7zFazsChh83D42
eI01jfuh58f0DS6qGmjb++N+a/mbgmQhIC4yV4iRZKiIHp9o2xKlSv4NyEJIHlM=
=JnYI
-----END PGP SIGNATURE-----

-------------------------------------
Actually, I have to think about this merge-mining thing a bit more. I'm
starting to think it's better to do without merge-mining at all. As I
explained in the forum post, the parent will put the hashes of its children
headers as transactions inside its blocks. Thus parents will have an
incentive to validate the children not by merge mining, but by collecting
fees from the children for putting those transactions inside (fees that can
be spent at the children chains). So, ya no merge mining needed for my
proposal. But I will think about it a bit more :)

On Tue, Jun 16, 2015 at 6:43 PM, Andrew <onelineproof@gmail.com> wrote:




-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
-------------------------------------
Dear All,

Just a note that we've extended the last day for submissions by two days.

i.e. Proposals should be submitted to proposals@scalingbitcoin.org by November
11th 23:59 UTC

Cheers,

p.


On Thu, Nov 5, 2015 at 11:32 PM, Jeremy via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

-------------------------------------
On Thu, Apr 09, 2015 at 10:56:20PM -0400, Stephen Morse wrote:

I wrote up how to do this on #bitcoin-wizards, Dec 9th 2014:

17:13 < petertodd> hearn: even now you can use OP_CODESEPARATOR to
implement efficient payword schemes
17:14 < petertodd> hearn: early on you could have used it to do some
really useful after-the-fact signing delegation by wrapping a IF ENDIF
around the CODESEPARATOR introduced into the middle of the
scriptSig/scriptPubKey pair - shame we got rid of that without thinking
the design through
17:15 < petertodd> hearn: e.g. "create a signature that delegates
signing authority to another pubkey"
17:15 < petertodd> probably all 100% accidental... but a nice accident
17:16 < hearn> it's probably for the best. i can imagine such things
being a surprise for implementations not expecting them. a script 2.0
effort that incorporates lots of neat features but still lets script 1.0
work would be nice to have, one day
17:17 < petertodd> satoshi belived in 1 implementation, and by putting
CODESEPARATOR into the scriptSig/scriptPubKey concatenation you had to
opt-in to making that feature possible to use in any particular
scriptPubKey
17:17 < petertodd> w/o the mis-matched ENDIF you can't pull off that
trick because you can't turn CODESEPARATOR off
17:19 < petertodd> to be explicit: scriptPubKey: ENDIF <pubkey>
CHECKSIG, then the normal case is scriptSig: <signature> 1 IF
17:19 < petertodd> they concatenate to  <signature> 1 IF ENDIF <pubkey>
CHECKSIG, CODESEPARATOR is evaluated, and the signature is evaluated on
the script ENDIF <pubkey> CHECKSIG
17:20 < petertodd> to delegate signing authority after the fact sign a
signature on the script <pubkey2> 0 IF ENDIF <pubkey> CHECKSIG
17:21 < petertodd> (remember that CODESEPARATORS are removed by
SignatureHash())
17:22 < petertodd> oops, I mean: <pubkey2> CHECKSIGVERIFY 0 IF ENDIF
<pubkey> CHECKSIG
17:22 < petertodd> anyway, to finally spend it, create another signature
with pubkey2 signing the script <pubkey2> CHECKSIGVERIFY 0 IF ENDIF
<pubkey> CHECKSIG again, and finally spend it with the scriptSig:
<pubkey-sig> <pubkey2-sig> CODESEPARATOR <pubkey2> 0 IF
17:24 < petertodd> after concatenation the script: <pubkey-sig>
<pubkey2-sig> CODESEPARATOR <pubkey2> 0 IF CODESEPARATOR ENDIF <pubkey>
CHECKSIG is evaluated, the inner signature satisfies, and the outer
signature is satisfied only if the scriptPubKey was essentially changed
after the fact to also require the inner, second, pubkey2 to be
satisfied
17:26 < petertodd> a nice use-case would, forinstance, have been to have
a signing robot be able to create signatures offline for a given txout
with SIGHASH_SINGLE such that you had a spending limit enforced, and
exactly who was then allowed to spend the funds - say a department of a
company - could be picked after the fact without re-spending the txout
17:33 < petertodd> gmaxwell: re: script validation state, a good model
would be to have the tx input to EvalScript() essentially be a
CMerkleTx, and the prevout scriptPubKey be the prevout CTxOut (*maybe*
the prevout tx itself... bit dubious there...)

-- 
'peter'[:-1]@petertodd.org
00000000000000000e7980aab9c096c46e7f34c43a661c5cb2ea71525ebb8af7
-------------------------------------
On Tue, Jun 16, 2015 at 6:18 AM, Venzen <venzen@mail.bihthai.net> wrote:



The main principle of open source software is that anyone can fork the code
if they wish.  They don't do it very often, but they can.

This means that if a project dies, someone can take it over.  If some of
the devs want to take things in a different direction, they can.  Users can
decide which version they prefer.

The software itself is what is valuable.

In the case of bitcoin, the blockchain is also (very) valuable.  Simply
splitting into two projects is not possible for bitcoin.

Otherwise, the discussion would have ended already, those who want a larger
block would simply create a fork of the software and create an alt chain.

The fundamental problem is that there is no clear way to make this decision
once and for all.

An agreed set of rules for a hard fork would be a nice thing to have, but
it is hard to have rules about how to change fundamental rules.

I think using the soft fork rules (maybe with a higher threshold than 95%)
plus a delay is a reasonable compromise on hard fork rules.

Even then, it would be nice to include users of the software too.  Peter
Todd's suggestion of encoding a vote in transactions is a step in that
direction (YES transactions in YES blocks and NO transactions in NO blocks).




Nobody owns it, so there is no court of final appeal.

If miners vote >95% for the fork, users could still refuse to accept the
change.

Maybe the sequence could be

version 3 blocks means no opinion
version 4 blocks means NO to fork
version 5 blocks means YES to fork & YES transactions
version 6 blocks means YES to fork & NO transactions

Transaction matching rule:

version 1, 2, 3 transactions means no opinion (can be in any block)
version 4 transactions means YES to fork (cannot be in version 6 blocks)
version 5 transactions means NO to fork (cannot be in version 5 blocks)

Rules
0) if 750 of the last 1000 blocks are version 5 or 6 blocks, tx matching
rule activates for version 5 & 6 blocks
1) if 950 of the last 1000 blocks are version 5 or 6 blocks, then version 4
blocks are rejected
2) if 750 of the last 1000 blocks are version 4 blocks, then version 5 & 6
blocks are rejected
3) if 750 of the last 1000 blocks are version 5 transactions and 950 of the
last 1000 are version 5 or 6, then the fork is accepted
4) 25,000 blocks after 3 is accepted, hard fork actually takes effect

Once miner acceptance is achieved, then only version 5 and 6 blocks are
allowed.  The split between version 5 and 6 blocks should be roughly in
proportion to the number of transactions of each kind produced.

75% of miners can kill the fork by producing version 4 blocks, but 95% is
needed for acceptance.  Even then, transaction volume needs to support the
fork.  I think 75% is reasonable here.  (95% of miners and 75% of
merchants/users is a pretty strong majority).



They are still suggesting some kind of fork threshold process (or at least
that is what is being suggested)

If their system requires 95% miner approval, they aren't taking unilateral
action.  Miners are though if they vote in favour.
-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Hi Jean Pierre,

This is a problem I've considered before, though I have to say I prefer
your solution.

The problem is, how can a person who restores his wallet from just a
seed restore
all his multi-signature addresses with other parties?

Your proposal is nice because all participants are equal, and it
minimalizes the data
required for recovery because it's deterministic, and the (extended)
public key is the first
piece of metadata you'll ask for from others.. Let it be the only thing
we need!

Regards amending BIP45 - BIP's are not amended after the fact, however
bad it may be
in retrospect. It might be best to write a BIP specifying a
"pseudorandom & deterministic
path generation for HD/multi-signature accounts"

TK

On 04/10/15 16:18, Jean-Pierre Rupp via bitcoin-dev wrote:

- -- 
My PGP key can be found here: <https://thomaskerin.io/me.pub.asc>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCgAGBQJWEWDlAAoJEAiDZR291eTlFAkQALgiiKX+VzLOwLK13S0EcE0v
RZeC8hqS5AEi/wpOYC2H0TFaHhDqDgy7Pt7nTt/vfOr9QFbJm076I/iFIhLPPAWf
rRg5kzL6ebOyX1NLmALcNgE9L+Jwz09kdzLUj+xZesJfu1AiSMgND38vFq4CmRfg
YSnWI4iMSP3OoMO5Akjq7m9Ww/lENPDmxTrz2ET9KwKPEkjrdt3c0ipQcs+/vGuU
RfRCUwxcdu/0nl5JhltxMV6wUMjdJ3AGbamWZpL+vA+jT5paOd4ORjc64huQGtFQ
W7l8ynbbVqtXlYYs9mXCMm70316sdo5ZpOXzQmplwtuHWVYt9ssS1aLkBoLYCBtU
i95Ki79S2ooeIjDEqI6FKpgVnLTmUbhudg/vk7eA0+RoNh3SBEHV2HmZ5yTBNtjk
P2a2tRmrbe3CmrdogbJzaweZenzoR82PziF7DAb/2JqtccPSdTW/GrAGyCoe0O5B
PId/iELHKpQepvybp+5PI6q2Atgzut4ze+a2vBiXjbiU3j0sX0XWg5fu9R9Ea1Bw
5+BY71GSa20OTDYEsp5esrl5/AUFj4ivB2OWFok77nGi2rTK+rKL3qMvbmYjAKUV
rWN4m6r8pU2hdhBCEJkXjg57whiMYn5w7ILlrbK5lLEu5qo0txoRtBPaid+y4mkK
moZU0LtvSQSX6ZaojQ/v
=Vs6z
-----END PGP SIGNATURE-----


-------------------------------------
POW is by design the voting mechanism for the valid chain continuation.

Many rightfully dislike that the same voting mechanism is used on the validity rules, since ideally
validators (non-mining full nodes), SPV user and even those having an investment in their cold wallet
would all have a vote.

That ideal voting mechanism is not yet in the protocol.

Before XT we used discussions and an informal consensus of those with commit access to github to evolve Bitcoin.
The decision, not the discussion, is now suggested to be replaced with POW vote with XT.

It is not hard to see problems with both approaches.

If XT comes closer to miner majority, validators will also be forced to take side, so they will be able to express
their vote. I think that most Bitcoin entrepreneurs will pick XT if Core has no comparable offer
to scale transactions per second.

XT, Not-XT and a Core with some not-BIP101 offer will potentially set the stage for the perfect hard fork storm.

I still believe, that the idea of Bitcoin is powerful enough to weather that storm.

Tamas Blummer


-------------------------------------
On Mon, Mar 16, 2015 at 10:22:13PM +0000, Matt Corallo wrote:

Depending on what you mean by "identical" this isn't actually reorg
safe. For instance consider this implementation:

    nLockTime = stack[-1] + prevout.nHeight
    if (nLockTime > txTo.nLockTime):
        return False

Used with this scriptPubKey:

    10 RCLTV DROP <pubkey> CHECKSIG

If I create that output in tx1 which is mined at height 42 I can spend
it in a tx2 at height > 42+10 by setting tx2's nLockTime to >42+10, for
instance 53. However if a reorg happens and tx1 ends up at height 43
after the reorg I'm stuck - tx2's nLockTime is set at 42.

Thus RCTLV is only reorg safe if the height is compared against the
actual block height of the block containing the spending transaction,
not the spending transaction's nLockTime.


Yup, definitely kinda ugly.

If the above style of RCTLV was used, one possibility might be to make
the relative locktime difference be required to be at least 100 blocks,
same as the coinbase maturity, and just accept that it's probably not
going to cause any problems, but could in an extremely big reorg. But
re-orgs that big might be big enough that we're screwed anyway...

With the 100 block rule, during a sufficiently large reorg that
coinbases become unavailble, simply disconnect entire blocks - all
txouts created by them.


So to be clear, right now the minimal interface to script execution is
simply:

    int bitcoinconsensus_verify_script(const unsigned char *scriptPubKey, unsigned int scriptPubKeyLen,
                                       const unsigned char *txTo        , unsigned int txToLen,
                                       unsigned int nIn, unsigned int flags, bitcoinconsensus_error* err);

Where scriptPubKey is derived from the unspent coin in the UTXO set and
txTo is the transaction containing the script that is being executed.
The UTXO set itself currently contains CCoins entries, one for each
transaction with unspent outputs, which basically contain:

    nVersion - tx nVersion
    nHeight  - Height of the block the transaction is contained in.
    vout     - Unspent CTxOut's of the transaction.

The block nTime isn't directly available through the UTXO set, although
it can be found in the block headers. This does require nodes to have
the block headers, but at 4MB/year growth it's reasonable to assume the
UTXO set will grow faster.

Script execution does not have direct access to the current block
height/block time, however it does have indirect access via nLockTime.

Thus we have a few possibilities:

1) RCLTV against nLockTime

Needs a minimum age > COINBASE_MATURITY to be safe.


2) RCLTV against current block height/time

Completely reorg safe.


3) GET_TXOUT_HEIGHT/TIME <diff> ADD CLTV

To be reorg safe GET_TXOUT_HEIGHT/TIME must fail if minimum age <
COINBASE_MATURITY. This can be implemented by comparing against
nLockTime.


All three possibilities require us to make information about the
prevout's height/time available to VerifyScript(). The only question is
if we want VerifyScript() to also take the current block height/time - I
see no reason why it can't. As for the mempool, keeping track of what
transactions made use of these opcodes so they can be reevaluated if
their prevouts are re-organised seems fine to me.


Absolute CLTV
=============

If we are going to make the block height/time available to
VerifyScript() to implement RCLTV, should absolute CLTV should continue
to have the proposed behavior of checking against nLockTime? If we go
with RCLTV against current block height/time, I'm going to vote no,
because doing so needlessly limits it to only being able to compare
against a block height or a block time in a single transaction.
Similarly it can complicate multi-party signatures in some
circumstances, as all parties must agree on a common nLockTime.


Time-based locks
================

Do we want to support them at all? May cause incentive issues with
mining, see #bitcoin-wizards discussion, Jul 17th 2013:

https://download.wpsoftware.net/bitcoin/wizards/2013/07/13-07-17.log

-- 
'peter'[:-1]@petertodd.org
0000000000000000015e09479548c5b63b99a62d31b019e6479f195bf0cbd935
-------------------------------------

The SPV wallet author would if they wanted their wallet to function.


Aaron Voisine
co-founder and CEO
breadwallet.com

On Mon, Jun 15, 2015 at 10:28 PM, <justusranvier@riseup.net> wrote:

-------------------------------------
On Tuesday 11. August 2015 21.51.59 Pieter Wuille via bitcoin-dev wrote:

Thats one usage of the form unreliable.
Yes, if people start getting their transactions thrown out because of full 
blocks or full memory pools, then its unreliable to send stuff.

Much more importantly is the software is unreliable at such loads. Bitcoin 
core will continue to grow in memory consumption, and eventually crash. Or, 
worse, crash the system its running on.
We know of some issues in the software with regards to running at > 100% 
capacity, I'm sure we'll find more when it actually happens.

IT experts are serious when they say that they avoid maxing out a system like 
the plague.

This, btw, is a good scenario where more centralization ends up happening when 
blocks are always full and people need to upgrade their client every week to 
keep up with the bugfixes.

-------------------------------------
So if you picked up the BLE broadcast request. All you know is that
*someone* within 100m is requesting bitcoin at a certain address. Not
necessarily who. The *name* is both optional, and possibly just a *handle*
of the user. If I'm sitting 5 ft away from someone at dinner and wanted to
pay them via BLE, I might see "Monkey Dude" on my list and simply ask him
"is that you?" If so, I send it. If there are two "Monkey Dude's" Then I
have to bother with the address prefix, but not otherwise.


[image: logo]
*Paul Puey* CEO / Co-Founder, Airbitz Inc
+1-619-850-8624 | http://airbitz.co | San Diego
<http://facebook.com/airbitz>  <http://twitter.com/airbitz>
<https://plus.google.com/118173667510609425617>
<https://go.airbitz.co/comments/feed/>  <http://linkedin.com/in/paulpuey>
<https://angel.co/paul-puey>
*DOWNLOAD THE AIRBITZ WALLET:*
  <https://play.google.com/store/apps/details?id=com.airbitz>
<https://itunes.apple.com/us/app/airbitz/id843536046>




On Thu, Feb 5, 2015 at 1:46 PM, Eric Voskuil <eric@voskuil.org> wrote:

-------------------------------------
The current block size debate has brought up an important, albeit often neglected observation. Full nodes suffer from a tragedy of the commons problem and therefore are likely to continue decreasing as a percentage of total Bitcoin nodes. This also results in a vicious circle as more and more people use SPVs, the burden on existing full nodes will increase even without a block size increase, which will further reduce the number of full nodes . A few people have mentioned it in blogs or reddit, but the topic is generally quickly overshadowed by posts along the lines of  "RAISE the blocksize already!".
Full nodes bear the full cost of validating/relaying/storing the blockchain and servicing SPV clients but gain nothing financially from it, yet they serve an important role in validating transactions and keeping miner dishonesty in check. If there were few independent full nodes, it would be possible for 3-4 of the biggest mining pools to collude and do literally whatever they wanted with the protocol, including inflating the money supply, freezing funds or even confiscating funds, because who would know? And even if someone running a full node did voice out, the majority of users on SPV/Coinbase/etc.. would be powerless to do anything about it and would likely bear with the changes to protect status quo, just as is the case with current fiat regimes where people bear with QE/Inflation/bail outs because they are so dependent on the current system that they would rather tolerate any injustice than to have the system go down and bring them with it. This is the primary reason why many in the technical community are against drastic blocksize increases, as this will only worsen the problem of decentralization as this cost increases. And as long as full nodes are running on charity, i'm fully in agreement with the conservative block size camp. 
However, it is important to note that this seems to be an economic problem instead of a technical one. I cannot deny the argument from the big block side that technically, the hardware/bandwidth required to run full nodes supporting considerably larger blocks (4MB-8MB) is not out of reach of many individuals around the globe. The core issue in my opinion is that of incentive, because at the end of the day, running a full node is not free and at larger blocks costs will not be trivial. In my opinion, its perhaps our insistence that full nodes cant be incentivised that contributes to centralization pressures and discourages increasing of blocksize even though the technology exists to support it.
Technically, existing hardware is capable of validating/processing blocks in the region of an order of magnitude larger than the current limit. Bandwidth requirements for running a validating full node are also not very high if you are not mining, as you can afford to wait a couple of minutes to download your block. This is obviously not the case for miners who need to download new blocks asap to avoid idle hash power or as has been seen in the recent fork, SPV mining (which is extremely undesirable for the network). IBLT should help greatly in reducing the propagation time of new blocks and ease peak bandwidth requirements. But im not worried about the miners, they are after all being financially compensated for what they are doing and investing in more bandwidth(either locally or running a full node remotely) can be seen as a cost of the business as long as the cost of running a full node is insignificant to the cost of hashing equipment to keep barriers to mining low. 

Before the concept lightning, there did not seem to be any trustless way of feasibly paying small micropayments to full nodes for their services. However, with payment channels and lightning, this may no longer be an issue. A node could advertise it's rates to a SPV nodes upon connection and the SPV could either agree or look for another node with lower fees. If implemented, fees are likely to be trivial(few satoshis per request) as competition will drive down fees close to the cost of running a full node. This should spur an increase in the number of full nodes and increase decentralization of the network.
I just wanted to float the idea and hear comments/feedback/critiques of this idea.
 		 	   		   		 	   		  
-------------------------------------
Things apparently aren't bad enough to prevent the majority from clamoring
for larger blocks.

If the majority agreed that things had got worse till this point, and that
this was to be blamed on the block size, they would be campaigning for the
other direction. Even yourselves aren't asking for a reduction in the block
size, as you know full well that you would be laughed out.

On 4 August 2015 at 12:27, Pieter Wuille <pieter.wuille@gmail.com> wrote:

-------------------------------------
I'd rather replace the whole nSequence thing with an explicit relative locktime with clear semantics...but I'm not going to fight this one too much.

On September 16, 2015 6:40:06 PM EDT, Btc Drak via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------------------------------
I traveled around in China for a couple weeks after Hong Kong to visit with miners and confer on the blocksize increase and block propagation issues. I performed an informal survey of a few of the blocksize increase proposals that I thought would be likely to have widespread support. The results of the version 1.0 census are below.

My brother is working on a website for a version 2.0 census. You can view the beta version of it and participate in it at https://bitcoin.consider.it. If you have any requests for changes to the format, please CC him at m@toom.im.

https://docs.google.com/spreadsheets/d/1Cg9Qo9Vl5PdJYD4EiHnIGMV3G48pWmcWI3NFoKKfIzU/edit#gid=0

Or a snapshot for those behind the GFW without a VPN:
http://toom.im/files/consensus_census.pdf

HTML follows:

Miner	Hashrate	BIP103	2 MB now (BIP102)	2 MB now, 4 MB in 2 yr	2-4-8 (Adam Back)	3 MB now	3 MB now, 10 MB in 3 yr	BIP101
F2Pool	22%	N/A	Acceptable	Acceptable	Preferred	Acceptable	Acceptable	Too fast
AntPool	23%	Too slow	Acceptable	Acceptable	Acceptable	N/A	N/A	Too fast
Bitfury	18%	N/A	Acceptable	Probably/maybe	Maybe	N/A	Probably too fast	Too fast
BTCC Pool	11%	N/A	Acceptable	Acceptable	Acceptable	Acceptable	Acceptable, I think	N/A
KnCMiner	7%	N/A	Probably?	Probably?	"We like 2-4-8"	Probably?	N/A	N/A
BW.com	7%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Slush	4%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
21 Inc.	3%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Eligius	1%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
BitClub	1%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
GHash.io	1%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Misc	2%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Certainly in favor			74%	56%	63%	33%	22%
Possibly in favor			81%	81%	81%	40%	33%	0%
Total votes counted			81%	81%	81%	40%	51%	63%
F2Pool: Blocksize increase could be phased in at block 400,000. No floating-point math. No timestamp-based forking (block height is okay). Conversation was with Wang Chun via IRC.
AntPool/Bitmain: We should get miners and devs together for few rounds of voting to decide which plan to implement. (My brother is working on a tool which may be useful for this. More info soon.) The blocksize increase should be merged into Bitcoin Core, and should not be implemented in an alternate client like BitcoinXT. A timeline of about 3 months for the fork was discussed, though I don't know if that was acceptable or preferable to Bitmain. Conversation was mostly with Micree Zhan and Kevin Pan at the Bitmain HQ. Jihan Wu was absent.
Bitfury: We should fix performance issues in bitcoind before 4 MB, and we MUST fix performance issues before 8 MB. A plan that includes 8 MB blocks in the future and assumes the performance fixes will be implemented might be acceptable to us, but we'll have to evaluate it more before coming to a conclusion. 2-4-8 "is like parachute basejumping - if you jump, and was unable to fix parachute during the 90sec drop - you will be 100% dead. plan A) [multiple hard forks] more safe." Conversation was with Alex Petrov at the conference and via email.
KnC: I only had short conversations with Sam Cole, but from what I can tell, they would be okay with just about anything reasonable.
BTCC: It would be much better to have the support of Core, but if Core doesn't include a blocksize increase soon in the master branch, we may be willing to start running a fork. Conversation was with Samson Mow and a few others at BTCC HQ.
The conversations I had with all of these entities were of an informal, non-binding nature. Positions are subject to change. BIP100 was not included in my talks because (a) coinbase voting already covers it pretty well, and (b) it is more complicated than the other proposals and currently does not seem likely to be implemented. I generally did not bring up SegWit during the conversations I had with miners, and neither did the miners, so it is also absent. (I thought that it was too early for miners to have an informed opinion of SegWit's relative merits.) I have not had any contact with BW.com or any of the smaller entities. Questions can be directed to j@toom.im.

-------------------------------------
On 2 May 2015 at 00:57, Marc D. Wood <metamarc@metamarket.biz> wrote:


Is there any relation between this and the work satoshi was putting into
the core before he left?

https://github.com/bitcoin/bitcoin/commit/5253d1ab77fab1995ede03fb934edd67f1359ba8

-------------------------------------

It was an example. Adam Back's extension blocks proposal would, in fact,
allow for a soft forking change that creates more subsidy than is valid (or
does anything else) by hiding one block inside another.

Anyway, I think you got my point.



I'm pretty sure Gregory did not use such an example because it's dead
wrong. You cannot verify the size of a coinbase without being a fully
verifying node because you need to know the fees in the block, and
calculating that requires access to the entire UTXO set.

This sort of thing is why I get annoyed when people lecture me about SPV
wallets and the things they "should" do. None of you guys has built one. I
keep seeing wild statements about theoretical unicorn wallets that nobody
has even designed, and how all existing wallets are crappy and insecure
because they don't meet your ever shifting goal posts.

To everyone making such statements I say: go away and build an SPV wallet
of your own from scratch. Then you will understand the engineering
tradeoffs involved much better, and be in a much better position to debate
what they should or should not be doing.

And bear in mind if it weren't for the work myself and a few others did on
SPV wallets, everyone would be using web wallets instead. Then you'd all
just complain about that instead.



Making it a hard fork instead is changing one line of code (ignoring the
code to set up the flag day, which can be based on the code for BIP101). If
it comes down to it, then I'll do the work to change that one line. But
obviously I'd need to see agreement from the maintainers that such a pull
req would be merged first.

The example is this: find someone that accepts 1-block confirmed
transactions in return for something valuable. There are plenty of them out
there. Once the soft fork starts, send a P2SH transaction that defines a
new output controlled by OP_CLTV. It will be incorporated into the UTXO set
by all miners because it's opaque (p2sh).

Now send a transaction that pays the merchant, and make it spend your
OP_CLTV output with an invalid script. New nodes will reject it as a rule
violator. Old nodes won't. So at some point an old miner will create a
block containing your invalid transaction, the merchant will think they got
paid, they'll give you the stuff and the fraud is done.



This is just embarrassing - do any of you guys at Blockstream actually use
Bitcoin in the real world? Virtually all payments that aren't moving money
into/out of exchange wallets are 0-confirm in reality. I described a
1-confirm attack above, but really ... come on.
-------------------------------------
