On Wed, Aug 17, 2016 at 2:14 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> I'm not aware of any ECC-enabled smart-cards that can sign the specific
> curve
> that Bitcoin uses, not to mention the fact that those smartcards generally
> only
> speak higher level protocols than raw signature generation, precluding the
> signing of bitcoin transactions.
>

any Java Card supporting ECC can sign on user supplied Weierstrass curve
parameters - you can find a good shopping list at
http://www.fi.muni.cz/~xsvenda/jcsupport.html (look for ALG_ECDSA_SHA256 on
javacard.crypto.signature). The NXP JCOP platform (found in Yubico Neo) is
a popular choice, and then you can add your own custom logic for validation.

-- 
Nicolas Bacca | CTO, Ledger

-------------------------------------
On Wed, Nov 16, 2016 at 1:58 PM, Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Are checkpoints good now? Are hard forks okay now?
>

I think that at least one checkpoint should be included.  The assumption is
that no 50k re-orgs will happen, and that assumption should be directly
checked.

Checkpointing only needs to happen during the headers-first part of the
download.

If the block at the BIP-65 height is checkpointed, then the comparisons for
the other ones are automatically correct.  They are unnecessary, since the
checkpoint protects all earlier block, but many people would like to be
able to verify the legacy chain.

This makes the change a soft-fork rather than a hard fork.  Chains that
don't go through the checkpoint are rejected but no new chains are allowed.

-------------------------------------
On Wed, May 11, 2016 at 03:16:58PM -0700, Simon Liu via bitcoin-dev wrote:
> > giving one
> > manufacturer/licenser a huge influence in who is successful in a market
> > that we're all relying on remaining rather flat.
> 
> Central planning is a slippery slope.  Let the market decide the winners
> and losers.  It's not feasible to hard fork every time an innovation or
> perceived unfair advantage appears in the space.

That's why we're asking the market right now, and any actual hard-fork to make
AsicBoost irrelevant would be voted on by miners themselves and in turn, the
economic majority, again letting the market collectively decide.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Dear list,

As we know, it would be desirable for Alice, running an SPV client, to tip (say $5) anyone who can prove to her that a given block has invalid content.

If no one takes these tips, then this is weak evidence that the entire block is valid. Alice gets validation, full nodes can get paid...this idea goes back to Satoshi's whitepaper.

In my view, "alerts" are relatively straightforward: a new OP CODE (details below) st. the txn only succeeds if it references invalid block content on a "pretender block".

However, my background reading seems to reveal that "fraud proofs" (as they are now called) require some kind of tremendous engineering overhaul. Can anyone point me to these large problem(s)?

Regards,
Paul Sztorc


------------------------------------

Fraud Proof, Simple (?)


1. "OP FraudProof", which:
	1. Contains arguments [a] block number (from Alice), [b] block header, and [c] merkle path from header to an invalid transaction*.
	2. Checks to see if the provided header _is_ in the position which Alice requested.
	2. Checks to see if the header _is_ valid (ie, has sufficient work).
	3. Checks to see if the merkle path _does_ lead from the header to "something invalid"*.

2. This OP Code can then be used in a transaction of the form:
	Inputs:
		1 from Alice
		0.2 from X**
	Output:
		1.2 to Alice, timelocked
		(or)
		1.2 to X, OP FraudProof .


3. Alice could sign this txn and circulate it, waiting for "X" to add the second signature. 

"Eric", for example, might sign. As soon as Alice get's Eric's signature, she [1] assumes the block *is* invalid, and [2] stops offering to buy FraudProofs on it.

If Eric does not deliver the fraud proof, Alice gets her money back + 0.2 BTC from Eric (for wasting her time). Alice can't lose -- she either buys a fraud proof for 1, or she gets a free 0.2.

Eric can't lose either. Either he doesn't sign (and nothing happens), or he places himself in a position to trade a FraudProof for 1 BTC.

- FraudProof can use "OP Equal" to request fraud for a certain block.
- This can all happen through the lightning network.

* "invalid transaction" is defined either [1] as a script which fails, or [2] a double-spend (headers/paths to 2 txns spending the same input). This definition does not catch bad coinbase transactions, but this doesn't concern me. Those outputs aren't spendable for 100 blocks, and anyway, SPV clients could be programmed to never accept them (it would be annoying, but possible).

** For simplicity, I assume that "FraudProof sellers" will pre-identify themselves (and their unspent outputs, etc, by making them "watching only" or whatever).

---

Now, I wouldn't describe this as a "weekend project", but I wouldn't describe it as an "engineering overhaul" either. Just a new OP Code, and code to create / scan for these "Alert Transactions". So, if the idea is 5+ years old, what's the hold up?

I've also heard that segwit will help, but don't understand why.




-------------------------------------
> I don't understand why subscriptions would need to be built into the
protocol.

Simple: Because the PaymentRequest is somewhat counter-intuitively a
/response/ to a customer initiated action.   It's not something the
merchant can initiate (of course, logically this makes sense... how can a
merchant know how to connect to some random android app).

Customers initiate all InvoiceRequests  BIP0075 clarifies this.   BIP0070
merely says that the customer "somehow indicates they are ready to pay".
BIP0075 formalizes a standard way to do this.

In no way do merchants initiate anything (of course).   Subscription
information must reside in the customers wallet, in response to a
merchant's advice to set up subscription.   Tacking parameters on to a
PaymentRequest or PaymentAck is the only good way to do this within BIP
70/75.

The only thing to hash out is exactly what fields to tack on and what they
mean.  ( subscription amount / currency / interval / interval_type ...
can't think of anything else )

Wallets are responsible for initiating the subscriptions on behalf of the
user.  Recommendations on how to do this should go into the spec.

Of course any wallet can, with BIP0075 add support for subscriptions
without any spec - just let the user set them up manually.   But it would
be nice if a user didn't have to enter the main parameters for
subscriptions... too easy to get times amounts, etc wrong.


On Wed, Jun 22, 2016 at 4:11 PM, James MacWhyte <macwhyte@gmail.com> wrote:

> Thomas,
>
> I like your idea about expanding Bitcoin URI's to include signatures. For
> BIP75 store and forward servers we are already thinking the DNS record
> would have the user's public key as well as the URL of their store and
> forward endpoint, so as soon as that becomes a standard you could use it
> just for the public key part. Expanding the Bitcoin URI should be done as
> well, for people who want to go the simpler route and not rely on servers.
>
> Erik, Andy, everyone else,
>
> I don't understand why subscriptions would need to be built into the
> protocol. With BIP75 the merchant could automatically issue a
> PaymentRequest message every X amount of time, and the customer's wallet
> would either display the request like normal or be set to pre-authorize
> requests from the merchant. If the merchant goes out of business, the
> requests would stop coming. This sounds like a UI issue and not a
> protocol-level requirement.
>
> If you think I'm wrong, please explain why :)
>
> On Wed, Jun 22, 2016 at 12:35 PM Erik Aronesty via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> - Payment channels seem clearly inappropriate for things like monthly
>> subscriptions, the use of nlocktime, etc.
>>
>> - Merchants cannot send requests to users for future payments, because
>> users don't run servers that they can connect to.  That's why BIP0070 works
>> the way it does.
>>
>> - Need to have an interval for subscriptions, at a minimum, and stored in
>> the wallet so next months payment can go out on time
>>
>> - Support for varying currency conversion needs to be baked in to
>> wallets.   Fortunately, by adding advisory subscription info to the
>> paymentrequest, this is left up to the wallet to
>> secure/validate/repeat/convert/etc. as needed for each subscription.
>>
>> - The UI you describe is nice - but not unique to the solution.
>>
>>
>>
>>
>> On Wed, Jun 22, 2016 at 12:20 PM, Andy Schroder <info@andyschroder.com>
>> wrote:
>>
>>> I understand the need for people to make repeated payments to
>>> individuals in real life that they know, without the payee every even
>>> taking the effort to make a formal payment request (say you're just paying
>>> a family member of friend back for picking something up for you at the
>>> store, and you've already payed them many times before).
>>>
>>> For a subscription, wouldn't it be better to promote payment channels or
>>> just send another payment request? I've been brainstorming recently about a
>>> model where service providers could deliver invoices, receipts, and payment
>>> requests in a standardized and secure way. In addition to having a send,
>>> receive, and transaction history tab in your bitcoin wallet, you'd also
>>> have an open payment channels tab (which would include all applications on
>>> your computer that have an open real time payment channel, such as a wifi
>>> access point, web browser, voip provider, etc.), as well as a "bills to
>>> pay" tab. Since everything would be automated and consolidated locally, you
>>> wouldn't have to deal with logging into a million different websites to get
>>> the bills and then pay them. If it were this easy, why would you ever want
>>> to do a recurring payment from a single payment request? I understand why
>>> you may think you want to given current work flows, but I'm wondering if it
>>> may be better to just skip over to a completely better way of doing things.
>>>
>>>
>>> Andy Schroder
>>>
>>>
>>> On 06/22/2016 11:30 AM, Erik Aronesty wrote:
>>>
>>>> My conclusion at the bottom of that post was to keep BIP 75 the same,
>>>> don't change a bit, and stick any subscription information (future payment
>>>> schedule) in the PaymentACK.   Then the wallet then re-initiates an invoice
>>>> (unattended or attended.. up to the user), after the subscription interval
>>>> is passed. Subscriptions are pretty important for Bitcoin to be used as a
>>>> real payment system.
>>>>
>>>
>>>
>>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>

-------------------------------------
I have been reading recently through the history of soft forks provided by
Bitcoin Core:
https://bitcoin.stackexchange.com/questions/43538/where-can-i-find-a-record-of-blockchain-soft-forks
.

It has led me to think that there is a deceiving notion that soft forks do
not force Bitcoin users to upgrade software. Yes, it's true that the past
soft forks still allow old nodes to accept blocks under the tighter rules
as valid, but what about miners who are still using old software? What
about users who want to make a transaction using the old rules? Those
people are no longer able to do those things. And if they want to do those
things, a hard fork will result.

Remember what happened when BIP 66 was activated? Luckily, it was short
lived, but this is just the beginning. If you keep tightening the rules,
you are building up more and more pressure for a split in the network to
occur. You can call this split a "hard fork" or just a "fork", but it is
dangerous either way, and it leads to basically the creation of two coins
when before we just had one, people instantly lose value, and the trust in
Bitcoin's store of value dies.

Obviously every one can debate about what should be the definition of a
soft fork, but whatever that is, I think it is unacceptable how sloppily
the past soft forks have been deployed. I can think of many ways in which
we could have these new features that the soft forks provided, but without
forcing the new rules, and simply making them features that can be used on
an individual miner or transaction signer basis. Is there a document from
Bitcoin Core that outlines the philosophy of soft forks and why it is
acceptable to force the tightening of rules and cause such risks? And
please give me another reason other than "it removes a few if statements
from the code".

Now that Segregated witness is scheduled to be deployed on November 15, we
should take a look at this "soft fork" as well. I like the idea of
Segregated Witness, but from conversations on Reddit and IRC, I see people
saying that this soft fork will be like the others: requiring a hard fork
in order to revert it. Is this true? I am getting conflicting messages by
reading the BIP. It says that if all transactions are non-segwit, then a
node will validate the block as before. But if we pass the threshhold
(usually 95 % for 1000 blocks) will miners mining non-segwit blocks be
ignored? This is not good... I really think we should make it optional.
Miners will have an incentive to mine segwit blocks, since it allows for
more transactions per block, so why force them? What if we want to slightly
modify the Segwit protocol in the future? What if we want to replace segwit
with something much different? We will be forced to do a hard fork in order
to do that.

Now, we can't go back in time and fix the deployment of the soft forks, but
I do propose one clean way to fix things: Remove all the previously "soft
forked" rules for non segwit transactions, and require them only for segwit
transactions. But make segwit optional! In addition to what I talked about
above, this may also relieve some tensions of people who are not
comfortable with segwit and are thinking of joining a hard fork like the
Bitcoin Unlimited project.

Unless people can give me a good explanation as to why we are deploying
soft forks in such forceful manner, or Bitcoin Core accepts my proposal,
then I will have no choice but to create a new client (I'm thinking to call
it Bitcoin Authentic), that will be just as Bitcoin Core but will always
follow the chain with the most work regardless of whether soft fork rules
are respected, and I would put at least CHECKLOCKTIMEVERIFY as mandatory
within segwit transactions.

-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647

-------------------------------------
On Wed, Mar 2, 2016 at 4:27 PM, Paul Sztorc via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> For example, it is theoretically possible that 100% of miners (not 50%
> or 10%) will shut off their hardware. This is because it is revenue
> which ~halves, not profit.


It depends on how much is sunk costs and how much is marginal costs too.

If hashing costs are 50% capital and 50% marginal, then the entire network
will be able to absorb a 50% drop in subsidy.

50% capital costs means that the cost of the loan to buy the hardware
represents half the cost.

Assume that for every $100 of income, you have to pay $49 for the loan and
$49 for electricity giving 2% profit.  If the subsidy halves, then you only
get $50 of income, so lose $48.

But if the bank repossesses the operation, they might as well keep things
running for the $1 in marginal profit (or sell on the hardware to someone
who will keep using it).

Since this drop in revenue is well known in advance, businesses will spend
less on capital.  That means that there should be less mining hardware than
otherwise.

A 6 month investment with 3 months on the high subsidy and 3 months on low
subsidy would not be made if it only generated a small profit for the first
3 and then massive losses for the 2nd period of 3 months.  For it to be
made, there needs to be large profit during the first period to compensate
for the losses in the 2nd period.

-------------------------------------
On Aug 17, 2016 00:36, "Russell O'Connor" <roconnor@blockstream.io> wrote:

> Can I already do something similar with replace by fee, or are there
limits on that?

BIP125 and mempool eviction both require the replacing transaction to have
higher fee, to compensate for the cost of relaying the replaced
transaction(s).

-- 
Pieter

-------------------------------------
On Monday, 17 October 2016 03:11:23 CEST Johnson Lau wrote:
> > Honestly, if the reason for the too-short-for-safety timespan is that
> > you
> > want to use BIP9, then please take a step back and realize that SegWit
> > is a contriversial soft-fork that needs to be deployed in a way that is
> > extra safe because you can't roll the feature back a week after
> > deployment. All transactions that were made in the mean time turn into
> > everyone-can- spent transactions.
> 
> No one should use, nor anyone is advised to use, segwit transactions
> before it is fully activated. 

Naturally, I fully agree.

It seems I choose the wrong words, let me rephrase;

You can't roll the SegWit back a week after people are allowed to send 
segwit transactions (lock-in + fallow period). All transactions that were 
made in the mean time turn into everyone-can- spent transactions.

Because the network as a whole and any implementation is unable to roll back 
in an environment where SegWit is a contriversial soft-fork, it is super 
important to make sure that it is properly supported by all miners. This 
takes time and the risk you take by pushing this is that actual real people 
loose actual real money because of the issue I outlined inthe previous 
paragraph.


> Having 2 months or 2 weeks of grace period
> makes totally no difference in this regard. If anyone tried to use segwit
> tx during your proposed 2 months grace period, all those txs were still
> everyone-can-spent.
> 
> All you are advocating is just stalling the process with no improvement in
> security.

I hope the above explains the actual security issue better.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
Hi Jeremy,

> My understanding of the paper is that the blinding factor would be included
> in the extra data which is incorporated into the ring signatures used in the
> range proof.

Yep, that is a possibility. The blinding factor could be encrypted
with the public key of the receiver. Thus it is only visible for the
receiver who can then check that the correct amount has been sent.

> Although, since I think the range proof is optional for single output
> transactions (or at least, one output per transaction doesn't require a
> range proof since there's only one possible value that it can be to make the
> whole thing work, and that value must be in range, I'm not entirely sure how

I understand and agree.

> you'd transmit it then, though in any case, since using it will pretty much
> require segwit, adding extraneous data isn't much of a problem.  In both
> cases, I imagine the blinding factor would be protected from outside
> examination via some form of shared secret generation... Although that would
> require the sender to know the recipient's unhashed public key; I don't know
> of any shared secret schemes that will work on hashed keys.

Here you lost me.
Why do we need to create a shared secret? Is this shared secret used
as the blinding factor?
Also I think the sender knows the unhashed public key of the receiver.
The only reason not to include it in the transaction script is that an
external observer is unable to see the receiver directly in the
blockchain.

Best
Henning


On Tue, Feb 09, 2016 at 04:12:37PM -0600, Jeremy Papp via bitcoin-dev wrote:
> My understanding of the paper is that the blinding factor would be included
> in the extra data which is incorporated into the ring signatures used in the
> range proof.
> 
> Although, since I think the range proof is optional for single output
> transactions (or at least, one output per transaction doesn't require a
> range proof since there's only one possible value that it can be to make the
> whole thing work, and that value must be in range, I'm not entirely sure how
> you'd transmit it then, though in any case, since using it will pretty much
> require segwit, adding extraneous data isn't much of a problem.  In both
> cases, I imagine the blinding factor would be protected from outside
> examination via some form of shared secret generation... Although that would
> require the sender to know the recipient's unhashed public key; I don't know
> of any shared secret schemes that will work on hashed keys.
> 
> Jeremy Papp
> 
> On 2/9/2016 7:12 AM, Henning Kopp via bitcoin-dev wrote:
> >Hi all,
> >
> >I am trying to fully grasp confidential transactions.
> >
> >When a sender creates a confidential transaction and picks the blinding
> >values correctly, anyone can check that the transaction is valid. It
> >remains publically verifiable.
> >But how can the receiver of the transaction check which amount was
> >sent to him?
> >I think he needs to learn the blinding factor to reveal the commit
> >somehow off-chain. Am I correct with this assumption?
> >If yes, how does this work?
> >
> >All the best
> >Henning
> >
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 

-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------
On Mon, May 9, 2016 at 8:57 AM, Tom via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> The moderators failed to catch his aggressive tone while moderating my post
> (see archives) for being too aggressive.
>

IIRC you were previously informed by moderators (on the same reddit thread
to which you refer) that it would seem you had canceled your email from the
moderation queue, contrary to your retelling above. This is now reaching
far into off-topic and further posts on this subject should be sent to
bitcoin-discuss@lists.linuxfoundation.org or
bitcoin-dev-owners@lists.linuxfoundation.org instead of the bitcoin-dev
mailing list.

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
On Sat, Jan 2, 2016 at 10:37 AM, Jorge Timón
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> > Updates from IRC discussion:
>
> Is there a link to the IRC discussion?

prior-block possession proofs, fraud proofs, non-fraud correctness
proofs, commitments and segwit:
https://botbot.me/freenode/bitcoin-core-dev/2015-12-28/?msg=56907496&page=2

- Bryan
http://heybryan.org/
1 512 203 0507


-------------------------------------
How are you collecting fees from the transactions in the block?

On Sat, Jan 2, 2016 at 8:51 PM, joe2015--- via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On 2016-01-03 02:46, Marco Falke wrote:
>>
>> 2015-12-30 17:27 GMT+01:00  <joe2015@openmailbox.org>:
>>>
>>> On 2015-12-30 18:33, Marco Falke wrote:
>>>>
>>>>
>>>> This is an interesting approach but I don't see how this is a soft
>>>> fork. (Just because something is not a hard fork, doesn't make it a
>>>> soft fork by definition)
>>>> Softforks don't require any nodes to upgrade. [1]
>>>> Nonetheless, as I understand your approach, it requires nodes to
>>>> upgrade. Otherwise they are missing all transactions but the coinbase
>>>> transactions. Thus, they cannot update their utxoset and are easily
>>>> susceptible to double spends...
>>>>
>>>> Am I missing something obvious?
>>>>
>>>> -- Marco
>>>>
>>>>
>>>> [1] https://en.bitcoin.it/wiki/Softfork#Implications
>>>
>>>
>>>
>>> It just depends how you define "softfork".  In my original write-up I
>>> called
>>> it a "generalized" softfork, Peter suggested a "firm" fork, and there are
>>> some suggestions for other names.  Ultimately what you call it is not
>>> very
>>> important.
>>>
>>> --joe.
>>
>>
>> joe, indeed it is not important how you call it, but please, let's not
>> call it "soft fork".
>
>
> This kind of fork (whatever it is called) has all the traditional properties
> of a softfork except meaningful backwards compatibility for non-upgraded
> clients.  So I think it is reasonable to call it a softfork with some
> qualification.
>
>> Besides my initial question about the coinbase
>> tx, I was also wondering how non-updated nodes would verify the
>> collected fees without the actual txs at hand. (They only have the
>> coinbase tx, don't they?)
>
>
> Yes this appears to be an oversight in my proof-of-concept implementation.
> The unintended consequence being that all transactions would have to be
> zero-fee...
>
> The simplest fix would be make the new rules add the fees implicitly.  There
> are other solutions.
>
>> Moreover, I can't see the benefits over a hard fork. A hard fork is
>> much cleaner in regard to code changes. As one of the intends of
>> "generalized soft forks" is to force user to update, at least a hard
>> fork doesn't lie about the fact. Am I missing any obvious advantages
>> of a "generalized soft fork" over a "clean" hard fork?
>
>
> A "firm soft fork" also does not lie about that fact -- you must upgrade.  I
> don't see it dishonest if it was never claimed otherwise.
>
> I agree that hardforks can be "cleaner".
>
> However the obvious disadvantage of a hardfork is the risk of the network
> splitting between upgraded and non-upgraded clients.  This is not a problem
> if there is 100% consensus behind the hardfork, but I am not sure if 100% is
> realistically achievable for contentious issues such as the blocksize limit.
>
> If 100% consensus is never achieved, then the options are:
> 1. Never upgrade and keep the blocksize limit unchanged forever.
> 2. Use a firm softfork to resolve the deadlock.
> 3. Hardfork anyway and split the network.
>
> My argument is simply that 2 is better than 3 and possibly 1.
>
> --joe
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
Maybe bitcoin-discuss should have been opt-out rather than opt-in.

Dear moderators, what is the subscription count to bitcoin-discuss,
and bitcoin-dev?


-------------------------------------
As the title suggests, I would like to formally request the assignment of a 
BIP number for my FT spec.

Thank you!


Source; 

https://github.com/zander/bips/blob/FlexTrans/bip-9999.mediawiki

<pre>
  BIP: ??
  Title: Flexible Transactions
  Author: Tom Zander <tomz@freedommail.ch>
  Status: Draft
  Type: Standards Track
  Created: 2016-07-27
</pre>

==Abstract==

This BIP describes the next step in making Bitcoin's most basic element,
the transaction, more flexible and easier to extend. At the same time this
fixes all known cases of malleability and resolves significant amounts of
technical debt.

==Summary==

Flexible Transactions uses the fact that the first 4 bytes in a transaction
determine the version and that the majority of the clients use a
non-consensus rule (a policy) to not accept transaction version numbers
other than those specifically defined by Bitcoin.
This BIP chooses a new version number, 4, and defines that the data
following the bytes for the version is in a format called Compact Message
Format (CMF). CMF is a flexible, token based format where each token is a
combination of a name, a format and a value. Because the name is added we
can skip unused tokens and we can freely add new tokens in a simple manner
in future. Soft fork upgrades will become much easier and cleaner this
way.

This protocol upgrade cleans up past soft fork changes like BIP68 which
reuse existing fields and do them in a much better to maintain and easier
to parse system. It creates the building blocks to allow new features to be
added much cleaner in the future.

It also shows to be possible to remove signatures from transactions with
minimal upgrades of software and still maintain a coherent transaction
history. Tests show that this can reduce space usage to about 75%.

==Motivation==

Token based file-formats are not new, systems like XML and HTMl use a
similar system to allow future growth and they have been quite successful
for decades in part because of this property.

Bitcoin needs a similar way of making the transaction future-proof because
re-purposing not used fields for new features is not good for creating
maintainable code.

Next to that this protocol upgrade will re-order the data-fields which
allows us to cleanly fix the malleability issue which means that future
technologies like Lightning Network will depend on this BIP being deployed.

At the same time, due to this re-ordering of data fields, it becomes very
easy to remove signatures from a transaction without breaking its tx-id,
which is great for future pruning features.


=== Tokens ===

In the compact message format we define tokens and in this specification we
define how these tokens are named, where they can be placed and which are
optional.  To refer to XML, this specification would be the schema of
a transaction.

CMF tokens are triplets of name, format (like PositiveInteger) and value.
Names in this scope are defined much like an enumeration where the actual
integer value (id, below) is equally important to the written name.
If any token found that is not covered in the next table will make the
transaction that contains it invalid.

{| class="wikitable"
|-
! Name !! id !! Format !! Default Value !! Description
|-
|TxEnd         ||  0 ||BoolTrue ||  Required    ||A marker that is the last 
byte in the txid calculation
|-
|TxInPrevHash  ||  1 ||ByteArray||  Required    ||TxId we are spending
|-
|TxPrevIndex   ||  2 ||Integer  ||      0       ||Index in prev tx we are 
spending (applied to previous TxInPrevHash)
|-
|TxInScript    ||  3 ||ByteArray||  Required    ||The 'input' part of the 
script
|-
|TxOutValue    ||  4 ||Integer  ||  Required    ||Amount of satoshi to 
transfer
|-
|TxOutScript   ||  5 ||ByteArray||  Required    ||The 'output' part of the 
script
|-
|LockByBlock   ||  6 ||Integer  ||  Optional    ||BIP68 replacement
|-
|LockByTime    ||  7 ||Integer  ||  Optional    ||BIP68 replacement
|-
|ScriptVersion ||  8 ||Integer  ||      2       ||Defines script version for 
outputs following
|-
|NOP_1x        || 1x || . ||  Optional    ||Values that will be ignored by 
anyone parsing the transaction
|}


=== Scripting changes ===

In the current version of Bitcoin-script, version 1, there are various
opcodes that are used to validate the cryptographic proofs that users have
to provide in order to spend outputs.

The OP_CHECKSIG is the most well known and, as its name implies, it
validates a signature.
In the new version of 'script' (version 2) the data that is signed is
changed to be equivalent to the transaction-id. This is a massive
simplification and also the only change between version 1 and version 2 of
script.

=== Serialization order===

The tokens defined above have to be serialized in a certain order for the
transaction to be well-formatted.  Not serializing transactions in the
order specified would allow multiple interpretations of the data which
can't be allowed.
There is still some flexibility and for that reason it is important for
implementors to remember that the actual serialized data is used for the
calculation of the transaction-id. Reading and writing it may give you a
different output and when the txid changes, the signatures will break.

At a macro-level the transaction has these segments. The order of the
segments can not be changed, but you can skip segments.

{| class="wikitable"
!Segment !! Description
|-
|   Inputs   || Details about inputs.
|-
|  Outputs   || Details and scripts for outputs
|-
| Additional || For future expansion
|-
| Signatures || The scripts for the inputs
|-
|   TxEnd    || End of the transaction
|}

The TxId is calculated by taking the serialized transaction without the
Signatures and the TxEnd and hashing that.


{| class="wikitable"
!Segment !! Tags !! Description
|-
|Inputs||TxInPrevHash and TxInPrevIndex||Index can be skipped, but in any 
input the PrevHash always has to come first
|-
|Outputs||TxOutScript, TxOutValue||Order is not relevant
|-
|Additional||LockByBlock  LockByTime NOP_1x
|-
|Signatures||TxInScript||Exactly the same amount as there are inputs
|-
|TxEnd||TxEnd
|}

TxEnd is there to allow a parser to know when one transaction in a stream
has ended, allowing the next to be parsed.

Notice that the token ScriptVersion is currently not allowed because we
don't have any valid value to give it. But if we introduce a new script
version it would be placed in the outputs segment.

=== Script v2 ===

The default value of ScriptVersion is number 2, as opposed to the version 1
of script that the is in use today.  The version 2 is mostly identical
to version one, including upgrades made to it over the years and in the 
future. The only exception is that the OP_CHECKSIG is made dramatically
simpler.  The input-type for OP_CHECKSIG is now no longer configurable, it is
always '1' and the content that will be signed is the txid.

TODO: does check-multisig need its own mention?


=== Block-malleability ===

The effect of leaving the signatures out of the calculation of the
transaction-id implies that the signatures are also not used for the
calculation of the merkle tree.  This means that changes in signatures
would not be detectable. Except naturally by the fact that missing or
broken signatures breaks full validation. But it is important to detect
modifications to such signatures outside of validating all transactions.

For this reason the merkle tree is extended to include (append) the hash of
the v4 transactions (and those alone) where the hash is taken over a
data-blob that is build up from:

1. the tx-id
2. the CMF-tokens 'TxInScript'


=== Future extensibility ===

The NOP_1x wildcard used in the table explaining tokens is actually a list
of 10 values that currently are specified as NOP (no-operation) tags.

Any implementation that supports the v4 transaction format should ignore
this field in a transaction. Interpreting and using the transaction as if
that field was not present at all.

Future software may use these fields to decorate a transaction with
additional data or features. Transaction generating software should not
trivially use these tokens for their own usage without cooperation and
communication with the rest of the Bitcoin ecosystem as miners certainly
have the option to reject transactions that use unknown-to-them tokens.


==Reference Implementation==

Bitcoin Classic includes this in its beta releases and a reference
implementation can be found at;

https://github.com/bitcoinclassic/bitcoinclassic/pull/186


==Deployment==

To be determined

==References==

[https://github.com/bitcoinclassic/documentation/blob/master/spec/compactmessageformat.md] 
CMF



-------------------------------------
On 1 October 2016 at 08:02, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Saturday, October 01, 2016 4:01:04 AM Rusty Russell wrote:
>
> > - Otherwise <bits> of hash is compared to lower <bits> of blockhash.
>
> Lower in what endian? Why only that endian? Why only lower? I can see a
> possible use case where one wants to look at only the high bits to ensure
> their transaction is only valid in a block with at least a certain
> difficulty...


Why not use segwit versioning for all this stuff? That lets you re-enable
the bitwise operations like OP_AND, permitting arbitrary bit-masks.
Further, the "at least a certain difficulty" problem suggests a solution by
extending the validity of opcodes like OP_LESSTHAN etc. to 256-bit inputs.

-------------------------------------
This has been reviewed by merchants, miners and exchanges for a couple of
weeks, and has been implemented and tested as part of the Bitcoin Classic
and Bitcoin XT implementations.

Constructive feedback welcome; argument about whether or not it is a good
idea to roll out a hard fork now will be unproductive, so I vote we don't
go there.

Draft BIP:
  https://github.com/gavinandresen/bips/blob/bump2mb/bip-bump2mb.mediawiki

Summary:
  Increase block size limit to 2,000,000 bytes.
  After 75% hashpower support then 28-day grace period.
  With accurate sigop counting, but existing sigop limit (20,000)
  And a new, high limit on signature hashing

Blog post walking through the code:
  http://gavinandresen.ninja/a-guided-tour-of-the-2mb-fork

Blog post on a couple of the constants chosen:
  http://gavinandresen.ninja/seventyfive-twentyeight

-- 
--
Gavin Andresen

-------------------------------------
It's good you bring that point, and it's very interesting to analyze what
happened then.

We shared our findings with some core developers much earlier than the BIP
proposal. Wether they kept it secret or they shared it with some ASIC
manufacturers is something I don't know. I even mentioned my wishes to try
to give the patent to public domain.

I remember the reason we proposed the BIP is because ASICBoost actually
does NOT require that BIP at all. And that BIP was not a consensus change,
but just a semantic re-interpretation.

ASICBoost can roll the nVersion field or the Merkle root hash. Doing the
former currently generates a strange warning message on nodes and can be
confusing, but doing the later makes ASICBoost completely stealthy. That
BIP could help the community to monitor its use in non-confusing way to the
users. What is worse? I think forcing it to be stealthy is worse.

I never opposed changing Bitcoin to be more decentralized, but hard-forking
a change to the PoW function may be contentious and that path of thought
must be walked very carefully.

Regards

-------------------------------------
Someone dropped this document in -wizards the other day:
http://5pdcbgndmprm4wud.onion/mimblewimble.txt
http://diyhpl.us/~bryan/papers2/bitcoin/mimblewimble.txt

Some commentary:
http://gnusha.org/bitcoin-wizards/2016-08-02.log
http://gnusha.org/bitcoin-wizards/2016-08-03.log
https://www.reddit.com/r/Bitcoin/comments/4vub3y/mimblewimble_noninteractive_coinjoin_and_better/

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
Not everyone who uses centralized exchanges are there to obtain the
currency though. A large portion are speculators who need to be able to
enter and exit complex positions in milliseconds and don't care about
decentralization, security, and often even the asset that they're buying.

Try telling everyone who currently uses Btc-e to go do their margin trading
over lightning channels, for example. They're not going to want to do that
because these exchanges are already meeting their needs perfectly well, and
like I argued before -- it would be very hard to do that as efficiently
with any other design (there are major drawbacks for traders with a
decentralized exchange.)

Like it or not, these exchanges play an integral role in the current
Bitcoin eco-system since they allow us to most efficiently discover price
and help improve liquidity. A decentralized exchange isn't going to stop
any more centralized exchanges from being hacked even if they are more
secure simply because traders don't want to use them.

(Sorry for the duplicate message Erik, I haven't used many mailing lists
before. I think I have the hang of it now though :) )

On Mon, Aug 8, 2016 at 8:59 AM, Erik Aronesty <erik@q32.com> wrote:

> I still feel like you're better off getting rid of "hot wallets" and use
> lightning-esqe networks to route orders.  I don't think either speed or
> flexibility is an issue there.
>
> IMO, the point of Bitcoin is to avoid the centralization that seems to be
> happening on the network now.   By making "hot wallets" more "secure", we
> encourage things to keep heading downhill with massive centralized
> crappy-security exchanges.
>
> Because, ultimately, there's no security that will prevent an inside
> job.   And all of these thefts have, in my opinion, been at least partly
> inside jobs.
>
> And centralization is the actually demon that needs slaying here.
>
> A client-side library with P2P order routing, tether.to + bitcoin ....
> and you've got a decentralized exchange... with orders matched to users
> directly, and channel-trades executed instantly.   And "market makers"
> running nodes to facilitate routing, etc.
>
> No center... nothing to shut down or sue... and no one holds your funds.
> That's a real Bitcoin exchange.
>
>
>
> On Sun, Aug 7, 2016 at 1:35 AM, Matthew Roberts via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> I'm wondering if we're fully on the same page here. What I was thinking
>> was that this protection mechanism would be applied to the coins in the hot
>> wallet (I wasn't talking about moving coins from the cold wallet to the hot
>> wallet -- though such a mechanism is also needed.)
>>
>> With the hot wallet you would have an output script that only allowed
>> coins to be sent to a new transaction whose output script was then only
>> redeemable after N confirmations (the output is relative time-locked) but
>> which can also be recovered to a fixed fail-safe address before the
>> time-lock is reached (exactly like TierNolan already listed only the
>> time-locked destination shouldn't be completely fixed.) So the private key
>> for this hot wallet can still sign valid transactions to withdraw coins to
>> any known destination and these transactions still reach the blockchain.
>>
>> The key difference from a regular transaction is that the destination
>> only has access to the coins -after- the relative time-lock is reached (N
>> blocks after first confirm) so everyone knows where withdrawals are suppose
>> to be going and how many coins are being withdrawn at any given time.
>> Deposits to the hot wallet would therefore need to be encumbered by the
>> same protection so that from then on this time-lock to redeem coins can be
>> applied to every new transaction trying to move coins (withdrawn by a user
>> of the exchange or sent to the cold wallet.)
>>
>> Notice we don't care about the destination in the TX script for the hot
>> wallet because to process user's withdrawals we can't know ahead of time
>> where they need to be sent (so it isn't possible to use a fixed address
>> here – though you might want to remove the clearing phase and set a fixed
>> address for coins sent from the hot wallet to the cold wallet.) The benefit
>> here comes from being able to see what withdrawals are being cleared,
>> matching those up to our expectations, and being able to "cancel"
>> withdrawals if they look suspicious, and you get the benefits for transfers
>> made from the hot wallet to the cold wallet and visa-versa.
>>
>>
>> This approach is good for a number of crucial services:
>>
>> 1. Wallets could be built that grouped coins into different "accounts"
>> with different time-frames required for clearing / unlocking coins. Your
>> savings or investment account would say -- take up to a week to clear --
>> whereas your everyday account used for smaller purchases (with less money)
>> would only take a few hours. This could all be linked up to services that
>> notified you of your money being moved + made any phone calls needed to
>> verify any larger transfers.
>>
>> The service could also be entrusted with the “cancellation” key which can
>> only be used to move money to your offline fail-safe address. This would be
>> quite an interesting way to mitigate fraud without the user having to be
>> trusted to do anything (except I suppose – not storing their recovery keys
>> online … but this could be partially solved with BIP 32-style “master”
>> public keys + hardware wallets + multi-sig, N factor auth, etc ...)
>>
>> 2. Gambling websites that process a lot of Bitcoins also have a hot
>> wallet which could be better protected by this.
>>
>> 3. Various other e-commerce websites also accept Bitcoins directly. (Deep
>> web markets come to mind -- hey, people breaking the law need good security
>> too.)
>>
>> 4. Provable dead man's switches on the protocol level is another idea --
>> no need to keep special time-locked transactions around and rely on them to
>> be broadcast = more reliable escrow services.
>>
>> 5. And obviously exchange hot (and cold) wallets - enemy number 1.
>>
>> I hope that makes sense. I think I initially managed to confuse a lot of
>> people by talking about revoking transactions / “settlement layers”, etc.
>> But IMO: all of this needs to take place on the blockchain with a new set
>> of OP_CODES and other than the fixed address issue with OP_SPENDTO, I think
>> the general idea would still work.
>>
>>
>> tl; dr, A pseudo-reversal mechanism for transactions would mean that
>> stolen private keys were no longer such an issue. This is desperately
>> needed for exchanges, wallets, and other services that are forced to manage
>> private keys, and whose users (I argue) already expect for this to be
>> possible (or at least will when they're hacked.)
>>
>>
>>
>>
>> On Sat, Aug 6, 2016 at 9:13 PM, Tier Nolan via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> On Sat, Aug 6, 2016 at 11:39 AM, s7r via bitcoin-dev <
>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>> * reversal of transactions is impossible
>>>>
>>>
>>> I think it would be more accurate to say that the requirement is that
>>> reversal doesn't happen unexpectedly.
>>>
>>> If it is clear in the script that reversal is possible, then obviously
>>> the recipient can take that into consideration.
>>>
>>>
>>>> * keep private keys private and safe. Lose them, it's like losing cash,
>>>> you can just forget about it.
>>>>
>>>
>>> Key management is a thing.  Managing risk by keeping some keys offline
>>> is an important part of that.
>>>
>>>
>>>> * while we try hard to make 0-conf as safe as possible (if there's no
>>>> RBF flag on the transaction), we make it almost impossible or very very
>>>> expensive to reverse a confirmed transaction.
>>>>
>>>
>>> BitGo has an "instant" system where they promise to only sign one
>>> transaction for a given output.  If you trust BitGo, then this is safe from
>>> double spending, since a double spender can't sign two transactions.
>>>
>>> If BitGo had actually implemented a daily withdrawal limit, then their
>>> system ends up similar to cold storage.  Only 10% of the funds at Bitfinex
>>> could have been withdrawn before manual intervention was required (with
>>> offline keys).
>>>
>>> Who will accept
>>>> such an input and treat it as a payment if it can be reversed during the
>>>> settlement layer?
>>>
>>>
>>> Obviously, if a payment is reversible, then you treat it as a reversible
>>> payment.  The protection here relates to moving coins from the equivalent
>>> of cold storage to hot storage.
>>>
>>> It is OK if it takes longer, since security is more important than
>>> convenience for coins in cold storage.
>>>
>>>
>>>> The linked page describes that merchants will never accept payments from
>>>> 'vaults', and it will take 24 hours for coins to be irreversible moved
>>>> outside the 'vault'.
>>>
>>>
>>> This relates to the reserves held by the exchange.  A portion of the
>>> funds are in hot storage with live keys.  These funds can be stolen by
>>> anyone who gets access to the servers.  The remaining funds are held in
>>> cold storage and they cannot be accessed unless you have the offline keys.
>>> These funds are supposed to be hard to reach and require manual
>>> intervention.
>>>
>>> I think this is a wrong approach. hacks and big losses are sad, but all
>>>> the time users / exchanges are to blame for wrong implementations or
>>>> terrible security practices.
>>>>
>>>
>>> Setting up offline keys to act as firebreaks is part of good security
>>> practices.
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>

-------------------------------------
I agree with the prohibition of +1s.  The core competency of those who
provide this list are moderation and technology, not managing a process
through which "involved people [indicate] whether they're for or against
it."

That is certainly an excellent function, but it can be offered by anyone
who wants to run a system for collecting and displaying those indications.
The email list itself is intended to be information rich, and such
"approval voting" is not information-rich enough in my view.

It is a shame that the moderated messages require so many steps to
retrieve.  Is it possible to have the "downloadable version" from
https://lists.ozlabs.org/pipermail/bitcoin-dev-moderation/ for each month
contain the text of the moderated emails?  They do contain the subjects, so
that helps.

On Wed, Jan 20, 2016 at 6:25 PM, xor--- via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Thursday, January 21, 2016 11:20:46 AM Rusty Russell via bitcoin-dev
> wrote:
> > So, what should moderation look like from now on?
>
> The original mail which announced moderation contains this rule:
> > - Generally discouraged: [...], +1s, [...]
>
> I assume "+1s" means statements such as "I agree with doing X".
>
> Any sane procedure of deciding something includes asking the involved
> people
> whether they're for or against it.
> If there are dozens of proposals on how to solve a particular technical
> problem, how else do you want to decide it than having a vote?
> It's very strange that this is not allowed - especially if we consider that
> the Bitcoin community is in a state of constant dissent currently.
> The effect is likely that you push the actual decision-making to IRC, which
> less people have access to (since it's difficult to bear the high traffic),
> and thus form some kind of "inner circle" - which makes decisions seem even
> more as if they're being dictated.
>
> So please consider allowing people to say whether they agree with something
> something or don't.
>
>
> Other than that, thanks for the good latency of moderation, I guess you're
> doing hard work there :)
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
Den 12 okt. 2016 01:33 skrev "John Hardy via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org>:
> Sidechains seem an inevitable tool for scaling. They allow Bitcoins to be
transferred from the main blockchain into external blockchains, of which
there can be any number with radically different approaches.
>
> In current thinking I have encountered, sidechains are isolated from each
other. To move Bitcoin between them would involve a slow transfer back to
the mainchain, and then out again to a different sidechain.
>
> Could we instead create a protocol for addressable blockchains, all using
a shared proof of work, which effectively acts as an Internet of
Blockchains?

More of a treechain / clusterchain, then?

> Instead of transferring Bitcoin into individual sidechains, you move them
into the master sidechain, which I'll call Angel. The Angel blockchain sits
at the top of of a tree of blockchains, each of which can have radically
different properties, but are all able to transfer Bitcoin and data between
each other using a standardised protocol.
>
> Each blockchain has its own address, much like an IP address. The Angel
blockchain acts as a registrar, a public record of every blockchain and its
properties. Creating a blockchain is as simple as getting a
createBlockchain transaction included in an Angel block, with details of
parameters such as block creation time, block size limit, etc. A
decentralised DNS of sorts.
>
> Mining in Angel uses a standardised format, creating hashes which allow
all different blockchains to contribute to the same Angel proof of work.
Miners must hash the address of the blockchain they are mining, and if they
mine a hash of sufficient difficulty for that blockchain they are able to
create a block.
>
> Blockchains can have child blockchains, so a child of Angel might have
the address aa9, and a child of aa9 might have the address aa9:e4d. The
lower down the tree you go, the lower the security, but the lower the
transaction fees. If a miner on a lower level produces a hash of sufficient
difficulty, they can use it on any parents, up to and including the Angel
blockchain, and claim fees on each.
>
> Children always synchronise and follow all parents (and their
reorganisations), and parents are aware of their children. This allows you
to do some pretty cool things with security. If a child tries to withdraw
to a parent after spending on the child (a double spend attempt) this will
be visible instantly, and all child nodes will immediately be able to
broadcast proof of the double spend to parent chain nodes so they do not
mine on those blocks. This effectively means children can inherit a level
of security from their parents without the same PoW difficulty.
>
> There are so many conflicting visions for how to scale Bitcoin. Angel
allows the free market to decide which approaches are successful, and for
complementary blockchains with different use cases, such as privacy, high
transaction volume, and Turing completeness to more seamlessly exist
alongside each other, using Bitcoin as the standard medium of exchange.
>
> I wrote this as a TLDR summary for a (still evolving) idea I had on the
best approach to scale Bitcoin infinitely. I've written more of my thoughts
on the idea at
https://seebitcoin.com/2016/09/introducing-buzz-a-turing-complete-concept-for-scaling-bitcoin-to-infinity-and-beyond/
>
> Does anybody think this would be a better, more efficient way of
implementing sidechains? It allows infinite scaling, and standardisation
allows better pooling of resources.

I've got a similar idea since quite a while back, but I've never really
written it down in full. Here one link:

http://www.metzdowd.com/pipermail/cryptography/2015-January/024338.html

Some thoughts on how to design it;

The basic idea is to compress the validation data maximally, and yet
achieve Turing completeness for an arbitary number of interacting chains,
or "namespaces".

The whole thing is checkpointed and uses Zero-knowledge proofs to enable
secure pruning, making it essentially a rolling blockchain with complete
preservation of history. It grows approximately linearly with
non-deprecated state.

This latest checkpoint's header + the following headers and accompanying
Zero-knowledge proofs would together act as the root for the system.

Having that is all you would need to confirm that any particular piece of
data from the blockchain is correct, given that it comes with enough
metadata to trace it all the way to the root. (Merkle tree hashes, ZKP:s,
etc).

Every chain would be registered under a unique name (the root chain would
essentially just deal with registering chain names + their rules), and
would define its own external API towards other chains, and it would define
its own rules for how its data can be updated and when. Every single
interaction with a chain is done by an atomic program (transaction), and
all sets of validated changes must be conflict-free (especially across
chains). Everything would practically be composed of a hierarchy of
interacting programs.

Every set of programs (transactions) can be transformed into a "diff" on
the blockchain state plus an accompanying Zero-knowledge proof. The proofs
can even be chained, such that groups of users of one chain can create a
proof for their own changes, submit it to some chain coordinator who does
another compressing merge and proof generation, to then send it to the
miners who merges the collective changes for all chains and generates a
proof for the root.

Obviously that validation can get inefficient if many chains interact, as
you can't simply just look for conflicting UTXO:s in programs (unless the
chain designers are *really* smart with their conflict resolution
mechanisms). Either you have to use programmatic locking, very slow block
rates or chose to not guarantee that any particular action has succeeded
(essentially turning validated programs (transactions) into *requests* to
the API up towards the root, to eventually be resolved later with responses
propagated back down, instead of having them be direct changes to the
state).

The latter option requires a lot more interaction by the client to get the
intended behavior in many circumstances. The first two can both kill
performance (nobody wants small programs with a few round-trips to take a
week to execute).

I really do hope it can be resolved effectively. I'm guessing some serious
restrictions on the API:s would be necessary. You would want most programs
to be provably independent (such as not accessing the same resources) to be
able to easily just create a small checkpoint of the global state and
generate a proof for it. Programs simultaneously accessing resources that
don't guarantee commutativity for all actions would likely be to be rate
limited.

Best case scenario: some genius manages to create the equivalent of
Lightning Network (with in-chain arbitration authority assigned to chosen
servers in the chain definitions, and cross-chain negotiation between those
authorities when programs use the API) for processing the programs in near
real-time, and quickly settling on what changes to commit to the root.
Programs would practically need to be designed to be networked
(multi-stage) so that the servers can let them negotiate over their API:s
across all chains, until the server has a complete set of changes without
conflicts to commit to the root.

-------------------------------------
Matt Corallo <lf-lists@mattcorallo.com> writes:
> Indeed, anything which uses P2SH is obviously vulnerable if there is
> an attack on RIPEMD160 which reduces it's security only marginally.

I don't think this is true?  Even if you can generate a collision in
RIPEMD160, that doesn't help you since you need to create a specific
SHA256 hash for the RIPEMD160 preimage.

Even a preimage attack only helps if it leads to more than one preimage
fairly cheaply; that would make grinding out the SHA256 preimage easier.
AFAICT even MD4 isn't this broken.

But just with Moore's law (doubling every 18 months), we'll worry about
economically viable attacks in 20 years.[1]

That's far enough away that I would choose simplicity, and have all SW
scriptPubKeys simply be "<0> RIPEMD(SHA256(WP))" for now, but it's
not a no-brainer.

Cheers,
Rusty.

[1] Assume bitcoin-network-level compute (collision in 19 days) costs
    $1B to build today.  Assume there will be 100 million dollars a day
    in vulnerable txs, and you're on one end of all of them (or can MITM
    if you find a collision), *and* can delay them all by 10 seconds,
    and none are in parallel so you can attack all of them.  IOW, just
    like a single $100M opportunity for 3650 seconds each year.

    Our machine has a 0.11% chance of finding a collision in 1 hour, so
    it's worth about $110,000.  We can build it for that in about 20
    years.


-------------------------------------
On Sun, Oct 2, 2016 at 6:46 PM, Russell O'Connor via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> But I would argue that in this scenario, the only way it
>> would become invalid is the equivalent of a double-spend... and therefore
>> it
>> may be acceptable in relation to this argument.
>>
>
> The values returned by OP_COUNT_ACKS vary in their exact value depending
> on which block this transaction ends up in.  While the proposed use of this
> operation is somewhat less objectionable (although still objectionable to
> me), nothing stops users from using OP_EQUALVERIFY and and causing their
> transaction fluctuate between acceptable and unacceptable, with no party
> doing anything like a double spend.  This is a major problem with the
> proposal.
>

Transactions that redeem an output containing (or referencing by means of
P2WSH) an OP_COUNT_ACKS are not broadcast by the network. That means that
the network cannot be DoS attacked by flooding with a transaction that will
not verify due to being too late.
The only parties that can include the redeem transaction are the miners
themselves.
Therefore I see no problem that an OP_COUNT_ACKS scriptSig transaction is
invalidated after the liveness times expires.
If there is no expiration, then polls can last forever and the system fails
to provide DoS protection for block validation since active polls can
accumulate forever.

-------------------------------------
On Wednesday 23 Mar 2016 16:24:12 Jonas Schnelli via bitcoin-dev wrote:
> Hi
> 
> I have just PRed a draft version of two BIPs I recently wrote.
> https://github.com/bitcoin/bips/pull/362

I suggest running a spellchecker ;)

Some questions;

* why would you not allow encryption on non-pre-approved connections?
* we just removed (ssl) encryption from the JSON interface, how do you suggest 
this encryption to be implemented without openSSL?
* What is the reason for using the p2p code to connect a wallet to a node?
I suggest using one of the other connection methods to connect to the node. 
This avoids a change in the bitcoin protocol for a very specific usecase.
* Why do you want to do a per-message encryption (wrapping the original)? 
Smaller messages that contain predictable content and are able to be matched 
to the unencrypted versions on the wire send to other nodes will open this 
scheme up to various old statistical attacks.

> Responding peers must ignore (banning would lead to fingerprinting) the 
requesting peer after 5 unsuccessfully authentication tries to avoid resource 
attacks.

Any implementation of that kind would itself again be open to resource 
attacks.
Why 5? Do you want to allow a node to make a typo?


> To ensure that no message was dropped or blocked, the complete communication 
must be hashed (sha256). Both peers keep the SHA256 context of the encryption 
session. The complete <code>enc</code> message (leaving out the hash itself) 
must be added to the hash-context by both parties. Before sending a 
<code>enc</code> command, the sha256 context will be copied and finalized.

You write "the complete communication must be hashed" and every message has a 
hash of the state until it is at that point.
I think you need to explain how that works specifically.




-------------------------------------
> #Basic idea:
> 
> Ideally, all miners would begin hashing the next block at exactly the same
> time. Miners with a head start are more profitable, and the techniques that
> help miners receive and validate blocks quickly create centralization
> pressure.
> 
> What if there was something that acted like the starting flag at a race,
> which could suddenly wave and cause all of the miners to simultaneously
> begin hashing the next block?
> 
> #Implementation:
> 
> Let a sync flag be a message consisting of:
> 
> 1. Hash of the previous block.
> 2. Bitcoin address
> 3. Nonce
> 
> This tiny message could propagate through the network at maximum speed. If
> miners had to include the hash of this flag in the next block, then all
> miners wait for this flag, and when it suddenly spread through the network,
> all miners could simultaneously begin hashing the next block.

What you describe in this part of your message can be done with no forks 
whatsoever and I think that this is enough. Don't really see the reason for 
any change in funding.

The idea of sending out a block header is essentially what I called 
"optimistic mining" and has been described in more detail in my blog here;
http://zander.github.io/posts/Innovation%20-%20OnlineScaling/

The video explains with graphics too...

You may find this interesting :)


-------------------------------------
I assume this has been well discussed in at some point in the Bitcoin
community, so I apologize if I'm repeating old ideas.

Problem exploitable nodes:
It is plausible that people running these versions of bitcoind may not
be applying patches. Thus, these nodes may be vulnerable to known
exploits. I would hope none of these nodes are gateway nodes for
miners, web wallets or exchanges. How difficult would it be to crawl
the network to find vulnerable nodes and exploit them? What percentage
of the network is running vulnerable versions of bitcoind?

Problem eclipsable nodes:
Currently a bitcoind node disconnects from any node with a version
below MIN_PEER_PROTO_VERSION. Such nodes become be ripe for an eclipse
attack because they are partitioned from the newer nodes, especially
when they are "freshly obsolete". I have not examined how protocol
versioning works in detail so I could be missing something.

One option could be that after a grace period:
1. to still connect to obsolete nodes and even to transmit blockheaders,
2. but to stop sending the full-blocks and transactions to these
nodes, thereby alerting the operator that something is wrong and
causing them to upgrade.
It may make sense to create this as a rule, if your longest chain
consists of only blockheaders and no one will tell you the
transactions for over 1000 blocks you are obsolete, spit out an error
message and shutdown.

This would not address the issue of alt-coins which are forked from
old vulnerable versions of bitcoind, but that is probably out of
scope.

On Thu, Dec 15, 2016 at 1:48 PM, Jorge Timón via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Thu, Dec 15, 2016 at 4:38 AM, Juan Garavaglia via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> Older node versions may generate issues because some upgrades will make
>> several of the nodes running older protocol versions obsolete and or
>> incompatible. There may be other hard to predict behaviors on older versions
>> of the client.
>
> Hard to predict or not, you can't force people to run newer software.
>
>> In order to avoid such wide fragmentation of "Bitcoin Core” node versions
>> and to help there be a more predictable protocol improvement process, I
>> consider it worth it to analyze introducing some planned obsolescence in
>> each new version. In the last year we had 4 new versions so if each version
>> is valid for about 1 year (52560 blocks) this may be a reasonable time frame
>> for node operators to upgrade. If a node does not upgrade it will stop
>> working instead of participating in the network with an outdated protocol
>> version.
>
> When you introduce anti-features like this in free software they can
> be trivially removed and they likely will.
>
>> These changes may also simplify the developer's jobs in some cases by
>> avoiding them having to deal with ancient versions of the client.
>
> There's a simpler solution for this which is what is being done now:
> stop maintaining and giving support for older versions.
> There's limited resources and developers are rarely interested in
> fixing bugs for very old versions. Users shouldn't expect things to be
> backported to old versions (if developers do it and there's enough
> testing, there's no reason not to do more releases of old versions, it
> is just rarely the case).
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
My mistake.
El mar 22, 2016 3:49 AM, <jl2012@xbt.hk> escribió:

> Do you mean BIP141?
>
>
>
> Your example is an error by BIP141:
>
>
>
> 1*4 + 79999*1 = 80003 > 80000
>
>
>
> *From:* bitcoin-dev-bounces@lists.linuxfoundation.org [mailto:
> bitcoin-dev-bounces@lists.linuxfoundation.org] *On Behalf Of *Sergio
> Demian Lerner via bitcoin-dev
> *Sent:* Monday, 21 March, 2016 10:51
> *To:* bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
> *Subject:* [bitcoin-dev] BIP147 minor error
>
>
>
> The BIP147 reads:
>
> *Sigop cost* is defined. The cost of a sigop in traditional script is 4,
> while the cost of a sigop in witness program is 1.
>
> The new rule is total *sigop cost* ≤ 80,000.
>
> But the code implements:
>
> if (nSigOps + (nWitSigOps + 3) / 4 > MAX_BLOCK_SIGOPS)
>
>  ... error....
>
> Which is not the same.
>
>
> For example:
>
> nSigOps = 1
> nWitSigOps =79999
>
> Is not an error by BIP definition but it's an error by the implemented
> code.
>
> Regards, Sergio.
>

-------------------------------------
On Saturday, December 10, 2016 9:29:09 PM Tier Nolan via bitcoin-dev wrote:
> On Sun, Dec 4, 2016 at 7:34 PM, Johnson Lau via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> > Something not yet done:
> > 1. The new merkle root algorithm described in the MMHF BIP
> 
> Any new merkle algorithm should use a sum tree for partial validation and
> fraud proofs.

PR welcome.

> Is there something special about 216 bits?  I guess at most 448 bits total
> means only one round of SHA256.  16 bits for flags would give 216 for each
> child.

See https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki#Merkle_tree_algorithm

But yes, the 448 bits total target is to optimise the tree-building.

> Even better would be to make the protocol extendable.  Allow blocks to
> indicate new trees and legacy nodes would just ignore the extra ones.  If
> Bitcoin supported that then the segregated witness tree could have been
> added as a easier soft fork.

It already is. This is a primary goal of the new protocol.

> The sum-tree could be added later as an extra tree.

Adding new trees means more hashing to validate blocks, so it'd be better to 
keep it at a minimum.

Luke


-------------------------------------
On Fri, Jan 08, 2016 at 02:00:11PM +1030, Rusty Russell via bitcoin-dev wrote:
> Pieter Wuille via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
> writes:
> > Yes, this is what I worry about. We're constructing a 2-of-2 multisig
> > escrow in a contract. I reveal my public key A, you do a 80-bit search for
> > B and C such that H(A and B) = H(B and C). You tell me your keys B, and I
> > happily send to H(A and B), which you steal with H(B and C).
> 
> FWIW, this attack would effect the current lightning-network "deployable
> lightning" design at channel establishment; we reveal our pubkey in the
> opening packet (which is used to redeem a P2SH using normal 2of2).
> 
> At least you need to grind before replying (which will presumably time
> out), rather than being able to do it once the channel is open.
> 
> We could pre-commit by exchanging hashes of pubkeys first, but contracts
> on bitcoin are hard enough to get right that I'm reluctant to add more
> hoops.

Note how this is a good example where trying to avoid the relatively
small amount of complexity of having two different segregated witness
schemes to allow for 128bit security could lead to a significant amount
of upper level complexity trying to regain security. I wouldn't be
surprised at all if this upper level complexity leads to exploits; at
the very least it'll lead to a lot of wasted mental effort from
cryptographers concerned about the potential weakness, both within and
external to the Bitcoin development community.

-- 
'peter'[:-1]@petertodd.org
000000000000000004aea2cfdb89c4816b7a42208dca1f3cfd66a1c9b5df4506

-------------------------------------
On Sat, Oct 15, 2016 at 4:21 PM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > My suggestion (sorry for not explaining it better) was that for BIPS to
>> > be a public domain (aka CC0) and a CC-BY option and nothing else.
>>
>> Indeed, we agree that BIPs should be licensed as permissive as
>> possible. Still, I wonder why you chose otherwise with BIP 134.
>> (Currently OPL and CC-BY-SA)
>
> OPL was the only allowed option apart from CC0.

I think you are misunderstanding what is allowed and what is required...

BIP1: "Each BIP must either be explicitly labelled as placed in the
public domain (see this BIP as an example) or licensed under the Open
Publication License"

So BIP1 *requires* PD or OPL but does not forbid other licenses. For
example, you are free to multi license OPL (and additionally: BSD,
MIT, CC0, ...)

BIP2: "Each new BIP must identify at least one acceptable license in
its preamble."

So BIP2 *requires* an acceptable license but does not forbid other
choices. For example, you are free to choose: BSD (and additionally:
PD, CC-BY-SA, WTFPL, BEER, ...)


>> BIP 2 does not forbid you to release your work under PD in
>> legislations where this is possible
>
> It does, actually.

Huh, I can't find it in the text I read. The text mentions "not
acceptable", but I don't read that as "forbidden".

>
>> One
>> of the goals of BIP 2 is to no longer allow PD as the only copyright
>> option.
>
> That's odd as PD was never the only copyright option.

Right. Though, up to now the majority of the BIP authors chose PD as
the only option.

Marco


-------------------------------------
> 2) In order to prevent significant blowups in the cost to validate
> [...] and transactions are only allowed to contain
> up to 20 non-segwit inputs. [...]

There is two kind of hard fork, the one who breaks things, and the one who
does not.
Restricting the non-segwit inputs would disrupt lots of services, and
potentially invalidating
hash time locked transactions, which is a very bad precedent.
So I'm strongly against this particular point.

> scriptPubKeys are now limited to 100 bytes in
> size and may not contain OP_CODESEPARATOR, scriptSigs must be push-only
> (ie no non-push opcodes)

Same problem for native multisig, however potentially less important than
the previous point.

-------------------------------------


On 25 August 2016 14:26:21 GMT-04:00, Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>On Thu, Aug 25, 2016 at 2:27 PM, Christian Decker via bitcoin-dev
><bitcoin-dev@lists.linuxfoundation.org> wrote:
>> If however
>> he is planning to use it as a foothold to further compromise your
>> company, send spam or similar, he will likely try to avoid these
>> tripwires. [...]
>
>Depends on the value of their activity compared to the value of the
>coins.
>Spamming doesn't pay much.
>
>Covert tripwires would obviously be better, but if shared tripwires
>allow you to have 100x the funds available it could be a good
>trade-off.

Also, having a overt tripwire doesn't preclude having covert tripwires as well.


In any case, this all deserves a Standard™ to make sure intruders know where to look to find the funds. Maybe /var/honeypot...


-------------------------------------
On Wednesday, November 16, 2016 9:01:00 PM Peter Todd via bitcoin-dev wrote:
> So, conceptually, another way to deal with this is to hardcode a blockhash
> where we allow blocks in a chain ending with that blockhash to _not_ follow
> BIP65, up until that blockhash, and any blockchain without that blockhash
> must respect BIP65 for all blocks in the chain.
> 
> This is a softfork: we've only added rules that made otherwise valid chains
> invalid, and at the same time we are still accepting large reorgs (albeit
> under stricter rules than before).
> 
> I'd suggest we call this a exemption hash - we've exempted a particular
> blockchains from a soft-forked rule that we would otherwise enforce.

While this is technically a softfork, I think it may behave somewhat like a 
hardfork if we're not careful. Particularly, is the chain up to the block 
immediately before the checkpoint itself valid on its own, or does it simply 
become retroactively valid when it hits that checkpoint?

P.S. Your PGP signature is invalid?


-------------------------------------
On Wednesday, February 17, 2016 2:28:31 AM Alex Morcos wrote:
> On Tue, Feb 16, 2016 at 6:46 PM, Luke Dashjr <luke@dashjr.org> wrote:
> > On Tuesday, February 16, 2016 8:20:26 PM Alex Morcos via bitcoin-dev 
wrote:
> > > # The feefilter message is defined as a message containing an int64_t
> > 
> > where
> > 
> > > pchCommand == "feefilter"
> > 
> > What happened to extensibility? And why waste 64 bits for what is almost
> > certainly a small number?
> 
> I thought that extensibility was already sufficient with the command string
> system.  If we come up with a better version of the feefilter later we can
> just give it a different command name.

We shouldn't need a new protocol [extension] for every new policy. Obviously 
this can't be perfectly flexible, but supporting different feerate definition 
versions is trivial and obvious.

> > > # The fee filter is additive with a bloom filter for transactions so if
> > an
> > > SPV client were to load a bloom filter and send a feefilter message,
> > > transactions would only be relayed if they passed both filters.
> > 
> > This seems to make feefilter entirely useless for wallets?
> 
> I don't follow this comment?  Transactions aren't synced with the wallet
> unless they are accepted into the mempool.  Sending feefilter messages
> should not reduce the number of transactions that are accepted to the
> mempool.

In Core, they aren't (but Core never uses bloom filters anyway) - because 
otherwise it would leak privacy. But light clients (particularly overlapping 
with those that use bloom filters!) have no privacy in the first place, so 
they have no reason to use this rule.

Luke


-------------------------------------
Sipa, you are probably the most competent to answer this. Could you please
tell us your opinion? For me, this is straightforward, backward compatible
fix and I like it a lot. Not sure about the process of changing "Final" BIP
though.

Slush

On Wed, Apr 20, 2016 at 6:32 PM, Jochen Hoenicke via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hello Bitcoin Developers,
>
> I would like to make a proposal to update BIP-32 in a small way.
>
> TL;DR: BIP-32 is hard to use right (due to its requirement to skip
> addresses).  This proposal suggests a modification such that the
> difficulty can be encapsulated in the library.
>
> #MOTIVATION:
>
> The current BIP-32 specifies that if for some node in the hierarchy
> the computed hash I_L is larger or equal to the prime or 0, then the
> node is invalid and should be skipped in the BIP-32 tree.  This has
> several unfortunate consequences:
>
> - All callers of CKDpriv or CKDpub have to check for errors and handle
>   them appropriately.  This shifts the burden to the application
>   developer instead of being able to handle it in the BIP-32 library.
>
> - It is not clear what to do if an intermediate node is
>   missing. E.g. for the default wallet layout, if m/i_H/0 is missing
>   should m/i_H/1 be used for external chain and m/i_H/2 for internal
>   chain?  This would make the wallet handling much more difficult.
>
> - It gets even worse with standards like BIP-44.  If m/44' is missing
>   should we use m/45' instead?  If m/44'/0' is missing should we use
>   m/44'/1' instead, using the same addresses as for testnet?
>   One could also restart with a different seed in this case, but this
>   wouldn't work if one later wants to support another BIP-43 proposal
>   and still keep the same wallet.
>
> I think the first point alone is reason enough to change this.  I am
> not aware of a BIP-32 application that handles errors like this
> correctly in all cases.  It is also very hard to test, since it is
> infeasible to brute-force a BIP-32 key and a path where the node does
> not exists.
>
> This problem can be avoided by repeating the hashing with slightly
> different input data until a valid private key is found.  This would
> be in the same spirit as RFC-6979.  This way, the library will always
> return a valid node for all paths.  Of course, in the case where the
> node is valid according to the current standard the behavior should be
> unchanged.
>
> I think the backward compatibility issues are minimal.  The chance
> that this affects anyone is less than 10^-30.  Even if it happens, it
> would only create some additional addresses (that are not seen if the
> user downgrades).  The main reason for suggesting a change is that we
> want a similar method for different curves where a collision is much
> more likely.
>
> #QUESTIONS:
>
> What is the procedure to update the BIP?  Is it still possible to
> change the existing BIP-32 even though it is marked as final?  Or
> should I make a new BIP for this that obsoletes BIP-32?
>
> What algorithm is preferred? (bike-shedding)  My suggestion:
>
> ---
>
> Change the last step of the private -> private derivation functions to:
>
>  . In case parse(I_L) >= n or k_i = 0, the procedure is repeated
>    at step 2 with
>     I = HMAC-SHA512(Key = c_par, Data = 0x01 || I_R || ser32(i))
>
> ---
>
> I think this suggestion is simple to implement (a bit harder to unit
> test) and the string to hash with HMAC-SHA512 always has the same
> length.  I use I_R, since I_L is obviously not very random if I_L >= n.
> There is a minimal chance that it will lead to an infinite loop if I_R
> is the same in two consecutive iterations, but that has only a chance
> of 1 in 2^512 (if the algorithm is used for different curves that make
> I_L >= n more likely, the chance is still less than 1 in 2^256).  In
> theory, this loop can be avoided by incrementing i in every iteration,
> but this would make an implementation error in the "hard to test" path
> of the program more likely.
>
> The other derivation functions should be updated in a similar matter.
> Also the derivation of the root node from the seed should be updated
> in a similar matter to avoid invalid seeds.
>
> If you followed until here, thanks for reading this long posting.
>
>   Jochen
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Since ScalingBitcoin is close, I think this is a good moment to publish our
proposal on drivechains. This BIP proposed the drivechain we'd like to use
in RSK (a.k.a. Rootstock) two-way pegged blockchain and see it implemented
in Bitcoin. Until that happens, we're using a federated approach.
I'm sure that adding risk-less Bitcoin extensibility through
sidechains/drivechains is what we all want, but it's of maximum importance
to decide which technology will leads us there.
We hope this work can also be the base of all other new 2-way-pegged
blockchains that can take Bitcoin the currency to new niches and test new
use cases, but cannot yet be realized because of current
limitations/protections.

The full BIP plus a reference implementation can be found here:

BIP (draft):
https://github.com/rootstock/bips/blob/master/BIP-R10.md

Code & Test cases:
https://github.com/rootstock/bitcoin/tree/op-count-acks_devel
(Note: Code is still unaudited)

As a summary, OP_COUNT_ACKS is a new segwit-based and soft-forked opcode
that counts acks and nacks tags in coinbase fields, and push the resulting
totals in the script stack.

The system was designed with the following properties in mind:

1. Interoperability with scripting system
2. Zero risk of invalidating a block
3. No additional computation during blockchain management and
re-organization
4. No change in Bitcoin security model
5. Bounded computation of poll results
6. Strong protection from DoS attacks
7. Minimum block space consumption
8. Zero risk of cross-secondary chain invalidation

Please see the BIP draft for a more-detailed explanation on how we achieve
these goals.

I'll be in ScalingBitcoin in less than a week and I'll be available to
discuss the design rationale, improvements, changes and ideas any of you
may have.

Truly yours,
Sergio Demian Lerner
Bitcoiner and RSK co-founder

-------------------------------------
On Thursday 22 Sep 2016 13:10:49 Christian Decker via bitcoin-dev wrote:
> On Thu, Sep 22, 2016 at 10:56:31AM +0200, Tom via bitcoin-dev wrote:
> > On Wednesday 21 Sep 2016 18:01:30 Gregory Maxwell via bitcoin-dev wrote:
> > > On Tue, Sep 20, 2016 at 5:15 PM, Tom via bitcoin-dev
> > > 
> > > <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > > > BIP number for my FT spec.
> > > 
> > > This document does not appear to be concretely specified enough to
> > > review or implement from it.
> > > 
> > > For example, it does not specify the serialization of "integer"
> > 
> > It refers to the external specification which is linked at the bottom.
> > In that spec you'll see that "Integer" is the standard var-int that
> > Bitcoin
> > has used for years.
> 
> I think BIPs should be self-contained, or rely on previous BIPs,
> whenever possible. Referencing an external formatting document should
> be avoided 

If luke-jr thinks I should submit CMF as a BIP, I can certainly do that.
Luke, what do you think?

I don't have a preference either way.

> > > nor does it specify how the
> > > presence of the optional fields are signaled
> > 
> > How does one signals an optional field except of in the spec? Thats the
> > job of a specification.
> 
> So the presence is signaled by encountering the tag, which contains
> both token type and name-reference. The encoder and decoder operations
> could be described better.

I'm sorry, I'm not following you here. Is there a question?


> > > nor the cardinality of
> > > the inputs or outputs.
> > 
> > Did you miss this in the 3rd table ?  I suggest clicking on the github
> > bips
> > repo link as tables are not easy to read in mediawiki plain format that
> > the
> > email contained.
> 
> Minor nit: that table is not well-formed.

I am not very well versed in mediawiki tables, and I found github has some 
incompatibilities too.
The markdown one looks better;
https://github.com/bitcoinclassic/documentation/blob/master/spec/transactionv4.md

> As was pointed out in the
> normalized transaction ID BIP, your proposal only addresses
> third-party malleability, since signers can simply change the
> transaction and re-sign it.

I have to disagree. That is not malleability. Creating a new document and re-
signing it is not changing anything. Its re-creating. Something that the owner 
of the coin has every right to do.

> This is evident from the fact that inputs
> and outputs do not have a canonical order and it would appear that
> tokens can be re-ordered in segments. 

Sorry, what is evident? You seem to imply that it is uncommon that you can 
create two transactions of similar intent but using different bytes.
You would be wrong with this implication as this is very common. You can just 
alter the order of the inputs, for instance.

I am unable to see what the point is you are trying to make. Is there a 
question or a suggestion for improvement here?

> Dependencies of tokens inside a
> segment are also rather alarming (TxInPrevHash <-> TxInPrevIndex,
> TxOutScript <-> TxOutValue).

Maybe you missed this line; 
  TxInPrevHash and TxInPrevIndex
   Index can be skipped, but in any input the PrevHash always has
   to come first

If you still see something alarming, let me know.
You can look at the code in Bitcoin Classic and notice that it really isn't 
anything complicated or worrying.


> Finally, allowing miners to reject transactions with unknown fields
> makes the OP_NOPs unusable 

Hmm, it looks like you are mixing terminology and abstraction-levels.  OP_NOP 
is a field from script and there is no discussion about any rejection based on 
script in this BIP at all.

Rejection of transactions is done on there being unrecognised tokens in the 
transaction formatting itself.

Thank you for your email to my BIP, I hope you got the answers you were 
looking for :)


-------------------------------------
I think it does not make sense to try to get this standardized for
current Bitcoin transactions. They are just too complex.

What might be interesting is to have something similar for Segwit and
Lightning transactions.

* TREZOR performs extended validation of the inputs, when all of
prev-txs are streamed into the device and validated. Your standard does
not tackle this at all and I don't think it's worthy to make this
standard unnecessarily complicated.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
Ahh. I see. Thanks, I must have missed that when going through the BIP.
Guess I need to read more carefully next time.

Thanks,
Andrew

On Fri, Jan 22, 2016 at 11:11 PM David A. Harding <dave@dtrt.org> wrote:

> On Fri, Jan 22, 2016 at 04:36:58PM +0000, Andrew C via bitcoin-dev wrote:
> > Spending a time locked output requires setting nSequence to less than
> > MAX_INT but opting into RBF also requires setting nSequence to less than
> > MAX_INT.
>
> Hi Andrew,
>
> Opt-in RBF requires setting nSequence to less than MAX-1 (not merely
> less than MAX), so an nSequence of exactly MAX-1 (which appears in
> hex-encoded serialized transactions as feffffff) enables locktime
> enforcement but doesn't opt in to RBF.
>
> For more information, please see BIP125:
>
>     https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>
> -Dave
>
>

-------------------------------------
Segwit requires work from exchanges, wallets and services in order for
adoption to happen. This is because segwit changes the rules regarding
the Transaction data structure. A blocksize increase does not change
the Transaction rules at all. The blocksize increase is a change to
the Block structure. Most wallets these days are Block agnostic.

Essentially, if a client has been built using a library that abstracts
away the block, then that client's *code* does not need to be updated
to handle this blocksize limit change. An example is any service using
the Bitcore javascript library. Any wallet built using Bitcore does
not need any changes to handle a blocksize upgrade. I have one project
that is live that was built using Bitcore. Before, during, and after
the fork, I do not need to lift a finger *codewise* to keep my project
still working. Same goes for projects that are built using
pybitcointools, as well as probably a few other libraries.

A wallet using Bitcore also has to work in tandem with a blockchan
api. Bitcore itself does not provide any blockchain data, you have to
get that somewhere else, such as a Node API. That API has to be based
on a Node that is following the upgraded chain. My wallet for instance
is built on top of Bitpay Insight. If bitpay doesn't upgrade their
Node to follow the 2MB chain, then I must either...

1) Change my wallet to use my own Bitpay Insight. (Insight is open
source, so you can host you own using any Node client you want)
2) Switch to another API, such as Toshi or Bockr.io, or
Blokchain.Info, or ... (there are dozens to choose from)

A blockchain service such as a blockexplorer does need to be upgraded
to handle a blocksize hardfork. The only work required is updating
their node software so that the MAX_BLOCKSIZE parameter is set to 2MB.
This can be done by either changing the source code themselves, or by
installing an alternate client such as XT, Classic, or Unlimited.

On 2/6/16, Adam Back via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Hi Gavin
>
> It would probably be a good idea to have a security considerations
> section, also, is there a list of which exchange, library, wallet,
> pool, stats server, hardware etc you have tested this change against?
>
> Do you have a rollback plan in the event the hard-fork triggers via
> false voting as seemed to be prevalent during XT?  (Or rollback just
> as contingency if something unforseen goes wrong).
>
> How do you plan to monitor and manage security through the hard-fork?
>
> Adam
>
> On 6 February 2016 at 16:37, Gavin Andresen via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> Responding to "28 days is not long enough" :
>>
>> I keep seeing this claim made with no evidence to back it up.  As I said,
>> I
>> surveyed several of the biggest infrastructure providers and the btcd
>> lead
>> developer and they all agree "28 days is plenty of time."
>>
>> For individuals... why would it take somebody longer than 28 days to
>> either
>> download and restart their bitcoind, or to patch and then re-run (the
>> patch
>> can be a one-line change MAX_BLOCK_SIZE from 1000000 to 2000000)?
>>
>> For the Bitcoin Core project:  I'm well aware of how long it takes to
>> roll
>> out new binaries, and 28 days is plenty of time.
>>
>> I suspect there ARE a significant percentage of un-maintained full
>> nodes--
>> probably 30 to 40%. Losing those nodes will not be a problem, for three
>> reasons:
>> 1) The network could shrink by 60% and it would still have plenty of open
>> connection slots
>> 2) People are committing to spinning up thousands of supports-2mb-nodes
>> during the grace period.
>> 3) We could wait a year and pick up maybe 10 or 20% more.
>>
>> I strongly disagree with the statement that there is no cost to a longer
>> grace period. There is broad agreement that a capacity increase is needed
>> NOW.
>>
>> To bring it back to bitcoin-dev territory:  are there any TECHNICAL
>> arguments why an upgrade would take a business or individual longer than
>> 28
>> days?
>>
>>
>> Responding to Luke's message:
>>
>>> On Sat, Feb 6, 2016 at 1:12 AM, Luke Dashjr via bitcoin-dev
>>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> > On Friday, February 05, 2016 8:51:08 PM Gavin Andresen via bitcoin-dev
>>> > wrote:
>>> >> Blog post on a couple of the constants chosen:
>>> >>   http://gavinandresen.ninja/seventyfive-twentyeight
>>> >
>>> > Can you put this in the BIP's Rationale section (which appears to be
>>> > mis-named
>>> > "Discussion" in the current draft)?
>>
>>
>> I'll rename the section and expand it a little. I think standards
>> documents
>> like BIPs should be concise, though (written for implementors), so I'm
>> not
>> going to recreate the entire blog post there.
>>
>>>
>>> >
>>> >> Signature operations in un-executed branches of a Script are not
>>> >> counted
>>> >> OP_CHECKMULTISIG evaluations are counted accurately; if the signature
>>> >> for a
>>> >> 1-of-20 OP_CHECKMULTISIG is satisified by the public key nearest the
>>> >> top
>>> >> of the execution stack, it is counted as one signature operation. If
>>> >> it
>>> >> is
>>> >> satisfied by the public key nearest the bottom of the execution
>>> >> stack,
>>> >> it
>>> >> is counted as twenty signature operations. Signature operations
>>> >> involving
>>> >> invalidly encoded signatures or public keys are not counted towards
>>> >> the
>>> >> limit
>>> >
>>> > These seem like they will break static analysis entirely. That was a
>>> > noted
>>> > reason for creating BIP 16 to replace BIP 12. Is it no longer a
>>> > concern?
>>> > Would
>>> > it make sense to require scripts to commit to the total accurate-sigop
>>> > count
>>> > to fix this?
>>
>>
>> After implementing static counting and accurate counting... I was wrong.
>> Accurate/dynamic counting/limiting is quick and simple and can be
>> completely
>> safe (the counting code can be told the limit and can "early-out"
>> validation).
>>
>> I think making scripts commit to a total accurate sigop count is a bad
>> idea-- it would make multisignature signing more complicated for zero
>> benefit.  E.g. if you're circulating a partially signed transaction to
>> that
>> must be signed by 2 of 5 people, you can end up with a transaction that
>> requires 2, 3, 4, or 5 signature operations to validate (depending on
>> which
>> public keys are used to do the signing).  The first signer might have no
>> idea who else would sign and wouldn't know the accurate sigop count.
>>
>>>
>>> >
>>> >> The amount of data hashed to compute signature hashes is limited to
>>> >> 1,300,000,000 bytes per block.
>>> >
>>> > The rationale for this wasn't in your blog post. I assume it's based
>>> > on
>>> > the
>>> > current theoretical max at 1 MB blocks? Even a high-end PC would
>>> > probably take
>>> > 40-80 seconds just for the hashing, however - maybe a lower limit
>>> > would
>>> > be
>>> > best?
>>
>>
>> It is slightly more hashing than was required to validate block number
>> 364,422.
>>
>> There are a couple of advantages to a very high limit:
>>
>> 1) When the fork is over, special-case code for dealing with old blocks
>> can
>> be eliminated, because all old blocks satisfy the new limit.
>>
>> 2) More importantly, if the limit is small enough it might get hit by
>> standard transactions, then block creation code (CreateNewBlock() /
>> getblocktemplate / or some external transaction-assembling software) will
>> have to solve an even more complicated bin-packing problem to optimize
>> for
>> fees paid.
>>
>> In practice, the 20,000 sigop limit will always be reached before
>> MAX_BLOCK_SIGHASH.
>>
>>
>>>
>>> >
>>> >> Miners express their support for this BIP by ...
>>> >
>>> > But miners don't get to decide hardforks. How does the economy express
>>> > their
>>> > support for it? What happens if miners trigger it without consent from
>>> > the
>>> > economy?
>>
>>
>> "The economy" does support this.
>>
>>
>>>
>>> >
>>> > If you are intent on using the version bits to trigger the hardfork, I
>>> > suggest
>>> > rephrasing this such that miners should only enable the bit when they
>>> > have
>>> > independently confirmed economic support (this means implementations
>>> > need a
>>> > config option that defaults to off).
>>
>>
>> Happy to add words about economic majority.
>>
>> Classic will not implement a command-line option (the act of running
>> Classic
>> is "I opt in"), but happy to add one for a pull request to Core, assuming
>> Core would not see such a pull request as having any hostile intent.
>>
>>
>>> >
>>> >> SPV (simple payment validation) wallets are compatible with this
>>> >> change.
>>> >
>>> > Would prefer if this is corrected to "Light clients" or something.
>>> > Actual SPV
>>> > wallets do not exist at this time, and would not be compatible with a
>>> > hardfork.
>>
>>
>> Is there an explanation of SPV versus "Light Client" written somewhere
>> more
>> permanent than a reddit comment or forum post that I can point to?
>>
>>>
>>> >
>>> >> In the short term, an increase is needed to continue the current
>>> >> economic
>>> >> policies with regards to fees and block space, matching market
>>> >> expectations
>>> >> and preventing market disruption.
>>> >
>>> > IMO this sentence is the most controversial part of your draft, and it
>>> > wouldn't suffer a loss to remove it (or at least make it subjective).
>>
>>
>> Happy to remove.
>>
>>>
>>> > I would also prefer to see any hardfork:
>>> >
>>> > 1. Address at least the simple tasks on the hardfork wishlist (eg,
>>> > enable some
>>> >    disabled opcodes; fix P2SH for N-of->15 multisig; etc).
>>
>>
>> Those would be separate BIPs. (according to BIP 1, smaller is better)
>>
>> After this 2MB bump, I agree we need to agree on a process for the next
>> hard
>> fork to avoid all of the unnecessary drama.
>>
>>> > 2. Be deployed as a soft-hardfork so as not to leave old nodes
>>> > entirely
>>> >    insecure.
>>
>>
>> I haven't been paying attention to all of the
>> "soft-hardfork/hard-softfork/etc" terminology so have no idea what you
>> mean.
>> Is THAT written up somewhere?
>>
>> --
>> --
>> Gavin Andresen
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
On 5/17/16, Eric Lombrozo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Nice!
>
> We’ve been talking about doing this forever and it’s so desperately needed.
>

"So desperately needed"? How do you figure? The UTXO set is currently
1.5 GB. What kind of computer these days doesn't have 1.5 GB of
memory? Since you people insist on keeping the blocksize limit at 1MB,
the UTXO set growth is stuck growing at a tiny rate. Most consumer
hardware sold thee days has 8GB or more RAM, it'll take decades before
the UTXO set come close to not fitting into 8 GB of memory.

Maybe 30 or 40 years from not I can see this change being "so
desperately needed" when nodes are falling off because the UTXO set is
to large, but that day is not today.


-------------------------------------

> 
>>> the fact that we do this has a rather odd result: a transaction spending a witness output with an unknown version is valid even if the transaction doesn’t have any witnesses!
>> 
>> I don’t see any reason to have such check. We simply leave unknown witness program as any-one-can-spend without looking at the witness, as described in BIP141.
> 
> It will lead to a special case in code that does things with witness
> transactions, as we can spend a witness output without a witness.

It is trivial to softfork a new rule to require the witness must not be empty for a witness output. However, does it really make the code simpler?
> 
> Thus you could summarize the argument for the P2PKH special case as "We don't
> want to make it possible to use 160-bit commitments for multisig, which _might_
> need 256-bit security. But we do want to special-case pubkeys to save a few
> bytes.”

Actually I would like to see even shorter hash and pubkey to be used. Storing 1 BTC for a few months does not really require the security level of P2PKH.

> 
>>> we haven’t explicitly ensured that signatures for the new signature hash can’t be reused for the old signature hash
>> 
>> How could that be? That’d be a hash collision.
> 
> Nope. The problem is it might not be a hash collission, if the actual bytes
> signed can be interpreted in two different ways, by different types of
> signature hashes.
> 
> This is the same reason the signmessage functionality prepends the message
> being signed with the "Bitcoin Signed Message:\n" magic string.
> 

In BIP143 sig, first 4 bytes is nVersion, and the next 32 bytes (hashPrevouts) is a hash of all prevouts. (in the case of ANYONECANPAY, the bytes following would be zero, as you proposed)

In the original sig, first 4 bytes in nVersion, next 4 bytes is number of inputs, and the next 32 bytes is a txid.

For a signature to be valid for both schemes, the last 28 bytes of the hashPrevouts must also be the first 28 bytes of a valid txid. This is already impossible. And this is just one of the many collisions required. In such case SHA256 would be insecure and adding a zero after the nVersion as you suggest would not be helpful at all.

-------------------------------------
On 2016/12/18 12:07, Alice Wonder via bitcoin-dev wrote:
> I almost did not update to 0.13.0 because the test suite was failing due
> to python errors. How to fix them was posted on bitcointalk.
> 
> 0.13.1 came with new python errors in the test suite. So I just said
> fuck it.
> 
> When the test suite actually works in my fairly standard environment
> (CentOS) in the distributed release, I will upgrade.

Can you post more info about the problems you're seeing, how you fixed
them, your environment, etc., or at least post an issue on Github? I'm
sure somebody would be happy to help. Some info on how to reproduce the
problems would be very helpful. :)

Thanks.

-- 
---
Douglas Roark
Cryptocurrency, network security, travel, and art.
https://onename.com/droark
joroark@vt.edu
PGP key ID: 26623924


-------------------------------------
On Tue, Feb 16, 2016 at 6:46 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Tuesday, February 16, 2016 8:20:26 PM Alex Morcos via bitcoin-dev wrote:
> > # The feefilter message is defined as a message containing an int64_t
> where
> > pchCommand == "feefilter"
>
> What happened to extensibility? And why waste 64 bits for what is almost
> certainly a small number?
>

I thought that extensibility was already sufficient with the command string
system.  If we come up with a better version of the feefilter later we can
just give it a different command name.  This seemed to encapsulate a fairly
complete idea for now.  As for the 8 bytes, it didn't seem necessary to me
to over optimize with a custom encoding for what amounts to well under 20%
of ping traffic.  (pings are sent every 2 mins per peer, feefilters on
average every 10 mins, but when the quantized value to be sent would be the
same it is skipped)


> > # The fee filter is additive with a bloom filter for transactions so if
> an
> > SPV client were to load a bloom filter and send a feefilter message,
> > transactions would only be relayed if they passed both filters.
>
> This seems to make feefilter entirely useless for wallets?
>
>
I don't follow this comment?  Transactions aren't synced with the wallet
unless they are accepted into the mempool.  Sending feefilter messages
should not reduce the number of transactions that are accepted to the
mempool.


> Luke
>

-------------------------------------
To conclude discussion on BIP 2, I have opened a pull request to implement it 
and mark it active. Note this implies activation and implementation of BIP 123 
as well: https://github.com/bitcoin/bips/pull/478

I plan to merge this on December 14th. If there are any hard objections to 
this change, please bring it up on the bitcoin-dev mailing list before then. 
Further reviews of the implementation are welcome in the meantime. Please 
refrain from requesting further changes to the BIPs themselves unless it is a 
blocker/show-stopper or trivial (not changing the meaning).

In the process of implementing BIP 2, I came across a number of BIPs which 
managed to get into the repository without a proper license. Authors of any of 
these BIPs should open a pull request adding the necessary Copyright section 
and License header(s). (If there are other contributors to the document in the 
BIP git logs, I will try to reach out to them to get permission. If you have 
accepted contributions from anyone not documented in git as an Author, please 
mention this in the PR explicitly.)

These BIPs need a license:
 001  BIP Purpose and Guidelines
 010  Multi-Sig Transaction Distribution
 011  M-of-N Standard Transactions
 012  OP_EVAL
 013  Address Format for pay-to-script-hash
 014  Protocol Version and User Agent
 015  Aliases
 016  Pay to Script Hash
 021  URI Scheme
 030  Duplicate transactions
 031  Pong message
 032  Hierarchical Deterministic Wallets
 033  Stratized Nodes
 034  Block v2, Height in Coinbase
 035  mempool message
 039  Mnemonic code for generating deterministic keys
 043  Purpose Field for Deterministic Wallets
 044  Multi-Account Hierarchy for Deterministic Wallets
 045  Structure for Deterministic P2SH Multisignature Wallets
 047  Reusable Payment Codes for Hierarchical Deterministic Wallets
 061  Reject P2P message
 062  Dealing with malleability
 064  getutxo message
 066  Strict DER signatures
 067  Deterministic Pay-to-script-hash multi-signature addresses through
          public key sorting
 068  Relative lock-time using consensus-enforced sequence numbers
 070  Payment Protocol
 071  Payment Protocol MIME types
 072  bitcoin: uri extensions for Payment Protocol
 073  Use "Accept" header for response type negotiation with Payment Request
          URLs
 075  Out of Band Address Exchange using Payment Protocol Encryption
 101  Increase maximum block size
 102  Block size increase to 2MB
 103  Block size following technological growth
 106  Dynamically Controlled Bitcoin Block Size Max Cap
 120  Proof of Payment
 121  Proof of Payment URI scheme
 123  BIP Classification

Thanks,

Luke


-------------------------------------
I sent my previous email ONLY to bitcoin-discuss@lists.linuxfoundation.org
and it waited in the moderation queue.  I don't know when moderation was
added to this list, but it seems to me that it's a misstep.

On Mon, Oct 10, 2016 at 12:38 AM, Henning Kopp via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi all,
>
> I totally agree with the assessment of the situation. Previously I
> learned a lot about bitcoin on this list. There were a lot of great
> ideas regarding the protocol and the surrounding ecosystem. Now there
> is mainly talk about code and BIPs, which is the main purpose of a
> developer list.
> I do not feel that we should clog bitcoin-dev again with
> non-development talk but rather find a way to get bitcoin-discuss
> going. My impression is that bitcoin-discuss has not reached a
> critical mass of contributors. The question is how we can change that.
>
> All the best
> Henning
>
> On Sun, Oct 09, 2016 at 12:26:07PM +0200, Jeremy via bitcoin-dev wrote:
> > Hi bitcoin-dev,
> >
> > I'm well aware that discussion of moderation on bitcoin-dev is
> > discouraged*. However, I think that we should, as a year of moderation
> > approaches, discuss openly as a community what the impact of such policy
> > has been. Making such a post now is timely given that people will have
> the
> > opportunity to discuss in-person as well as online as Scaling Bitcoin is
> > currently underway. On the suggestion of others, I've also CC'd
> > bitcoin-discuss on this message.
> >
> > Below, I'll share some of my own personal thoughts as a starter, but
> would
> > love to hear others feelings as well.
> >
> > For me, the bitcoin-dev mailing list was a place where I started
> > frequenting to learn a lot about bitcoin and the development process and
> > interact with the community. Since moderation has begun, it seems that
> the
> > messages/day has dropped drastically. This may be a nice outcome overall
> > for our sanity, but I think that it has on the whole made the community
> > less accessible. I've heard from people (a > 1 number, myself included)
> > that they now self-censor because they think they will put a lot of work
> > into their email only for it to get moderated away as trolling/spam.
> Thus,
> > while we may not observe a high rate of moderated posts, it does mean the
> > "chilling effect" of moderation still manifests -- I think that people
> not
> > writing emails because they think it may be moderated reduces the rate of
> > people writing emails which is a generally valuable thing as it offers
> > people a vehicle through which they try to think through and communicate
> > their ideas in detail.
> >
> > Overall, I think that at the time that moderation was added to the list,
> it
> > was probably the right thing to do. We're in a different place as a
> > community now, so I feel we should attempt to open up this valuable
> > communication channel once again. My sentiment is that we enacted
> > moderation to protect a resource that we all felt was valuable, but in
> the
> > process, the value of the list was damaged, but not irreparably so.
> >
> > Best,
> >
> > Jeremy
> >
> >
> > * From the email introducing the bitcoin-dev moderation policy,
> "Generally
> > discouraged: shower thoughts, wild speculation, jokes, +1s, non-technical
> > bitcoin issues, rehashing settled topics without new data, moderation
> >  concerns."
> >
> >
> > --
> > @JeremyRubin <https://twitter.com/JeremyRubin>
> > <https://twitter.com/JeremyRubin>
>
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
> --
> Henning Kopp
> Institute of Distributed Systems
> Ulm University, Germany
>
> Office: O27 - 3402
> Phone: +49 731 50-24138
> Web: http://www.uni-ulm.de/in/vs/~kopp
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
On Jun 15, 2016 12:53, "Daniel Weigl via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> That would be a big privacy leak, imo. As soon as both outputs are spent,
its visible
> which one was the P2WPKH-in-P2SH and which one the pure P2WPKH and as a
consequence
> you leak which output was the change and which one the actual sent output
>
> So, i'd suggest to even make it a requirement for "normal"
send-to-single-address transactions
> to always use the same output type for the change output (if the wallet
is able to recognize it)

Indeed, and you can go even further. When there are multiple "sending"
outputs, pick one at random, and mimic it for the change output. This means
that if you have a P2PKH and 3 P2SH sends, you'll have 25% chance for a
P2PKH change output, and 75% chance for a P2SH output.

You can go even further of course, if you want privacy that remains after
those sends get spent. In that case, you also need to match the template of
the redeemscript/witnessscript. For example, if the send you are mimicking
is a 2-of-3, the change output should also use 2-of-3.

-- 
Pieter

-------------------------------------
On Tue, Jan 26, 2016 at 9:42 AM, Luzius Meisser via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> This post serves to convince you of the economic benefits of smoothing
> the payout of fees across blocks. It incentivizes decentralization and
> supports the establishment of a fee market.
>
> Idea: currently, the total amount of fees collected in a block is paid
> out in full to whoever mined that block. I propose to only pay out,
> say, 10% of the collected fees, and to add the remaining 90% to the
> collected fees of the next block. Thus, the payout to the miner
> constitutes a rolling average of collected fees from the current and
> past blocks. This
> *reduces the marginal benefit of including an additional transaction into
> a block* by an order of magnitude and thus
> aligns the incentives of individual miners better with those of the
> whole network. As a side-effect,
>
> *the disadvantage of mining with a slow connection is reduced.*


I do not believe your logic is correct.  Reducing the marginal benefit of
including an additional transaction is problematic because it
simultaneously increases the orphan risk while it reduces the reward.  90%
of the fee going to the next block would also create new incentive problems
like mining an empty block to minimize the chance of losing 90% of the fees
from the previous block to an orphan.  Another major issue with mandatory
sharing is if the miner doesn't want to share, nothing stops them from
taking payment out-of-band and confirming the transaction with little or no
fees visible in the block.

I had been thinking recently about fee deferral for a different reason.  In
the future when the subsidy is much smaller in proportion to the fees,
there may be little incentive to confirm on top of someone else's block in
cases when the expected value of replacing the current tip is higher.  I
think smoothing fees between the current and subsequent 5 blocks (for
example) might reduce the incentive of this type of behavior.  The main
risk here might be in weakening too far the incentive of adding more
transactions to the current block, as I believe your 10% current and 90%
subsequent reward split would do.  I think my idea of a mandatory split
between six blocks might also be a failure because of the high incentive to
conduct out-of-band payments.


> Benefits:
>
2. This is a step towards a free fee market. In an ideal market,
> prices form where supply and demand meet, with the fees asymptotically
> approaching the marginal costs of a transaction. Currently, supply is
> capped and only demand can adjust. Should we ever consider to let
> miners decide about supply, it is
>
> *essential that their marginal benefit of including an additional
> transaction is aligned with the global marginal cost incurred by that
> additional transaction.* Fee
> smoothing is a step in this direction.
>

While I don't agree with the rest of your logic, it is agreeable that you
care about aligning the miner's supply incentives with the global marginal
cost.  If you believe that is an important goal, you might like the Flex
Cap approach as presented by Mark Friedenbach at Scaling Bitcoin Hong Kong.

Under the general idea of the Flex Cap approach block size is no longer
fixed, it can be bursted higher on a per-block basis if the miner is
willing to defer a tiny portion of the current block subsidy to pay out to
the miner of later blocks.  If conditions are such that there is genuine
demand then some are willing to pay higher fees for time preference.  Some
formula would balance the cost and reward in some manner like: add the
value of newly included fees, subtract the expected marginal cost of orphan
risk, then subtract the portion of subsidy deferred.  Flex cap has periodic
block size retargets to allow for a temporary limit to rise or fall to
something resembling actual market demand.  This temporary limit is never a
"wall" that can be hit as miners can choose to burst past it if the cost is
worth the reward.

Flex Cap is an area of ongoing research that I strongly believe would
benefit Bitcoin in the long-term.  For this reason it requires careful
study and simulations to figure out specifics.

3. The incentive to form mining pools is reduced. Currently,
> solo-mining yields a very volatile income stream due to the random
> nature of mining, leading to the formation of pools. This volatility
> will increase to even higher levels once the amount of Bitcoins earned
> per block is dominated by (volatile) collected fees and not by
> (constant) freshly minted coins, thus increasing the economic pressure
> to join a large pool. Fee smoothing reduces that volatility and
> pressure.
>

You seem to not recognize that orphan cost is a major reason why pools are
attractive.

Warren Togami

-------------------------------------
On Saturday, October 01, 2016 4:01:04 AM Rusty Russell wrote:
> Prefer a three-arg version (gbits-to-compare, blocknum, hash):
> - If <bits> is 0 or > 256, invalid.
> - If the hash length is not (<bits> + 7) / 8, invalid.

This means zero padding on-chain, which would be undesirable.
Rather "at most" and have the consensus implementation do the padding.

> - If the hash unused bits are not 0, invalid.

Why?

> - Otherwise <bits> of hash is compared to lower <bits> of blockhash.

Lower in what endian? Why only that endian? Why only lower? I can see a 
possible use case where one wants to look at only the high bits to ensure 
their transaction is only valid in a block with at least a certain 
difficulty...

> This version also lets you play gambling games on-chain!
> 
> Or maybe I've just put another nail in CBAH's coffin?

Or maybe resurrected it...

Luke


-------------------------------------
Since the release of sidechains alpha, confidential transactions[1] by Greg
Maxwell have show how they could greatly improve transaction privacy and
fungibility of bitcoin. Unfortunately without a hardfork or pegged
sidechain it was not easy to enable them in bitcoin.

The segregated witness[2] proposal by Pieter Wuille allows to reduce the
blockchain to a mere utxo changeset while putting all cryptographic proofs
(redeemscript/pubkeys/signatures) for the inputs into a witness part.
Segwit also allows upgradable scripting language. All can be done with a
soft fork.

We propose an upgrade to segwit to allow transactions to have both
witnessIns and witnessOuts.

We also propose 3 new transactions types: blinding, unblinding and
confidential. Valid blocks containing any of these new transactions MUST
also include a mandatory special output in their coinbase transaction and a
new special confidential base transaction.

The basic idea for confidential transaction is to use 0 value inputs and
outputs while having the encrypted amounts (petersen-commitment +
range-proof) in the witnessOut part. These transactions are valid under old
rules (but currently non-standard). For blinding, unblinding and miner fees
we use a single anyone-can-spend output (GCTXO) which will be updated in
every block containing confidential transactions.

Blinding transaction:
  Ins:
    All non-confidential inputs are valid
  Outs:
  - 0..N: (new confidential outputs)
    amount: 0
    scriptPubkey: OP_2 <0x{32-byte-hash-value}>
    witnessOut: <0x{petersen-commitment}> <0x{range-proof}>
  - last:
    amount: 0
    scriptPubkey: OP_RETURN OP_2 {blinding-fee-amount}
  Fee: Sum of the all inputs value
The last output's script is also a marker of the transaction being a
blinding tx. After the soft fork, a block is invalid if the miner claims
the fees for himself instead of putting it into a special coinbase output.


Coinbase transaction:
If the block contains blinding transactions, it MUST send the sum of all
their fees to a new output: GCTXO[coinbase]
The scriptPubkey does not really matter since it will be only spendable
under strict rules in the same block's confidential base transaction. Maybe
OP_TRUE.


Unblinding transaction:
  Ins:
    prev: CTXO[n]
    scriptSig: (empty)
    witnessIn: <signature> <0x{redeemscript}>
  Outs:
  - 0..N:
    amount: 0
  scriptPubkey: OP_RETURN OP_2 {amount-to-be-unblinded} {p2sh-destination}
    witnessOut: (empty)
  - last:
    amount: 0
    scriptPubkey: OP_RETURN OP_2 {unblinding-fee-amount}
  Fee: 0

This transaction remove removes the confidential outputs from the utxo set.
This outpoint itself is not spendable (it's OP_RETURN), but the same block
will contain a confidential base transaction created by the miner that will
satisfy the amount and p2sh-destination (refunded using GCTXO).
Confidential transaction:
  Ins:
  - 0..N:
    prev: CTXO[n]
    scriptSig: (empty)
    witnessIn: <signature> <0x{redeemscript}>
  Outs:
  - 0..N:
    amount: 0
    scriptPubkey: OP_2 <0x{32-byte-hash-value}>
    witnessOut: <0x{petersen-commitment}> <0x{range-proof}>
  - last:
    amount: 0
    scriptPubkey: OP_RETURN OP_2 {confidential-fee-amount}
  Fee: 0

All inputs and outputs and have amount 0 and are everyone can spend V2
segwit, thus valid under old rules. Transaction valid under new rules
obviously only if petersen commitment and range-proof in witnessOut valid.
Minerfee for this transaction is expressed as one extra output:


Confidential base transaction:
  Ins:
    GCTXO[last_block],
    GCTXO[coinbase]
  Outs:
    0: GCTXO[current_block]
    amount: {last_block + coinbase - unblindings}
    scriptPubkey: OP_TRUE
    1..N:
    amount/scriptPubkey: as requested by unblinding transactions in this
block
  Fee:
    Sum of all the explicit OP_RETURN OP_2 {...} expressed fees from
    confidential transactions in this block

This special transaction in last position in every block that contains at
least one of the new transaction types. Created by the miner of the block
and used to do the actual unblinding and redeeming transaction fees for all
confidential transactions.

There will always be only 1 GCTXO in the utxo set. This allows for full
accountability for 21 million bitcoin. Should a vulnerability in CT be
discovered all unconfidential bitcoins remain safe. Under these new rules,
a block is only valid if all amounts/commitments/range-proofs match. A a
miner trying use GCTXO other than allowed in the single confidential base
transaction
will be orphaned.

[1] https://people.xiph.org/~greg/confidential_values.txt
[2]
https://github.com/CodeShark/bips/blob/segwit/bip-codeshark-jl2012-segwit.mediawiki


Sorry for the form, this is just a quick draft of a thought I had today.
Please comment.

Felix Weis

-------------------------------------
On Tue, Sep 20, 2016 at 5:15 PM, Tom via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> BIP number for my FT spec.

This document does not appear to be concretely specified enough to
review or implement from it.

For example, it does not specify the serialization of "integer" (is it
a 32 bit word in network byte order or?) nor does it specify how the
presence of the optional fields are signaled nor the cardinality of
the inputs or outputs. For clearly variable length elements
('bytearray') no mention is made of their length encoding. etc.

Without information like this, I don't see how someone could
realistically begin reviewing this proposal.

The motivation seems unclear to me as well: The scheme is described as
'flexible' but it appears to remove flexibility from the existing
system. The "schema" appears to be hardcoded and never communicated.
If the goal is to simply have a more compact on the wire
representation, this could be done without changing the serialization
used for hashing or the serialization used for costing.


-------------------------------------
On Thu, Aug 25, 2016 at 02:54:47AM +0000, James MacWhyte via bitcoin-dev wrote:
> I've always assumed honeypots were meant to look like regular, yet
> poorly-secured, assets. If the intruder could identify this as a honeypot
> by the strange setup (presigned, non-standard transactions lying around)
> and was aware that the creator intended to doublespend as soon as the
> transaction was discovered, wouldn't they instead prefer to not touch
> anything and wait for a non-bait target to appear? Is the assumption here
> that the intruder wouldn't know this is a honeypot, or that they would know
> and it's just assumed that they would rather take their chances on this
> instead of causing some other trouble?

That strongly depends on the value of the compromised machine to the
attacker. If he has syphoned all the data from it and has no further
use for it then the he will probably trip the tripwire to get the
coins even though this will make the compromise apparent. If however
he is planning to use it as a foothold to further compromise your
company, send spam or similar, he will likely try to avoid these
tripwires. In which case a classic honeypot, that attempts to look
like a regular system is what you're looking for.


-------------------------------------
On Thursday, 22 September 2016 14:27:29 CEST Peter Todd wrote:
> CSV uses per-input sequence numbers; you only have a per-tx equivalent.

I think you misunderstand tagged systems at a very basic level.  You think 
that html can only use a bold tag <b> once in a document? Thats equivalent 
to what you are saying.

Your comment is rather embarrassing, I have to point out. You may want to 
read a bit more before you comment more.


-------------------------------------
You need more detail for it to be a BIP.

New Header

new_header.prev = hash of previous header's bitcoin header
new_header.small_nonce = 4 byte nonce
new_header.big_nonce = 8 byte nonce

new_header.... (Can contain any new fields desired)

Fake Block

block.version = 4
block.prev = new_header.prev
block.merkle = calculate_merkle(coinbase)
block.timestamp = block.getPreviousBlock().median_time_past + 1
block.bits = calculate_bits()
block.nonce = new_header.small_nonce
block.tx_count = 1

Coinbase

coinbase.version = 1
coinbase.tx_in_count = 0
coinbase.tx_out_count = 1
coinbase.tx_out[0].value = 0
coinbase.tx_out[0].pk_script = "OP_RETURN"

This is a "nuclear option" attack that knocks out the main chain.  The
median time past will increase very slowly.  It only needs to increase by 1
every 6th blocks.  That gives an increase of 336 seconds for every
difficulty update.  This will cap the update rate, so give an increase of
4X every doubling.

The new headers will end up not meeting the difficulty, so they will
presumably just repeat the last header?

If the bitcoin chain stays at constant difficulty, then each quadrupling
will take more time.

After 2 weeks: 4XDiff   (2 weeks per diff period)
After 10 weeks: 16XDiff (8 weeks per diff period)
After 42 weeks: 256XDiff (32 weeks per diff period)


On Wed, Feb 24, 2016 at 5:52 AM, James Hilliard via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> https://github.com/bitcoin/bips/pull/340
>
> BIP: ?
> Title: 2016 Multi-Stage Merge-Mine Headers Hard-Fork
> Author: James Hilliard <james.hilliard1@gmail.com>
> Status: Draft
> Type: Standards Track
> Created: 2016-02-23
>
> ==Abstract==
>
> Use a staged hard fork to implement a headers format change that is
> merge mine incompatible along with a timewarp to kill the previous
> chain.
>
> ==Specification==
>
> We use a block version flag to activate this fork when 3900 out of the
> previous 4032 blocks have this the version flag set. This flag locks
> in both of the below stages at the same time.
>
> Merge Mine Stage: The initial hard fork is implemented using a merge
> mine which requires that the original pre-fork chain be mined with a
> generation transaction that creates no new coins in addition to not
> containing any transactions. Additionally we have a consensus rule
> that requires that ntime be manipulated on the original chain to
> artificially increase difficulty and hold back the original chain so
> that all non-upgraded clients can never catch up with current time.
> The artificial ntime is implemented as a consensus rule for blocks in
> the new chain.
>
> Headers Change Stage: This is the final stage of the hard fork where
> the header format is made incompatible with merge mining, this is
> activated ~50,000 blocks after the Merge Mine Stage and only at the
> start of the 2016 block difficulty boundary.
>
> ==Motivation==
>
> There are serious issues with pooled mining such as block withhold
> attacks that can only be fixed by making major changes to the headers
> format.
>
> There are a number of other desirable header format changes that can
> only be made in a non-merge mine compatible way.
>
> There is a high risk of there being two viable chains if we don't have
> a way to permanently disable the original chain.
>
> ==Rationale==
>
> Our solution is to use a two stage hard fork with a single lock in period.
>
> The first stage is designed to kill off the previous chain by holding
> back ntime to artificially increase network difficulty on the original
> chain to the point where it would be extremely difficult to mine the
> 2016 blocks needed to trigger a difficulty adjustment. This also makes
> it obvious to unupgraded clients that they are not syncing properly
> and need to upgrade.
>
> By locking in both stages at the same time we ensure that any clients
> merge mining are also locked in for the headers change stage so that
> the original chain is dead by the time the headers change takes place.
>
> We timewarp over a year of merge mining to massively increase the
> difficulty on the original chain to the point that it would be
> incredibly expensive to reduce the difficulty enough that the chain
> would be able to get caught up to current time.
>
> ==Backward Compatibility==
>
> This hardfork will permanently disable all nodes, both full and light,
> which do not explicitly add support for it.
> However, their security will not be compromised due to the implementation.
> To migrate, all nodes must choose to upgrade, and miners must express
> supermajority support.
>
> ==Reference Implementation==
>
> TODO
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Concept ACK.

For the details of executing the plan, I think the following is less disruptive:

1. Send a message with (max sequence - 1), notifying all nodes that the key will be retired on or before a date. People with systems relying on this key should either upgrade or ignore the revocation message. We don't know the actual date because the key is shared by many people.

With the max - 1 sequence, no message except the max sequence revocation message may override this message. 

2. Send the revocation message at the pre-announced time, if no one have done that before

3. After a few months or so, publish the private key.

 >  
 > One of the facilities in the alert system is that you can send a 
 > maximum sequence alert which cannot be overridden and displays only a 
 > static key compromise text message and blocks all other alerts. I plan 
 > to send a triggering alert in the not-distant future (exact time to be 
 > announced well in advance) feedback on timing would be welcome. 
 >  
 > There are likely a few production systems that automatically shut down 
 > when there is an alert, so this risks some small one-time disruption 
 > of those services-- but none worse than if an alert were sent to 
 > advise about a new system upgrade. 
 >  
 > At some point after that, I would then plan to disclose this private 
 > key in public, eliminating any further potential of reputation attacks 
 > and diminishing the risk of misunderstanding the key as some special 
 > trusted source of authority. 
 >  
 > Cheers, 
 > _______________________________________________ 
 > bitcoin-dev mailing list 
 > bitcoin-dev@lists.linuxfoundation.org 
 > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev 
 > 




-------------------------------------
It is always possible I'm being dense, but I still don't understand how
this proposal makes a chain-forking situation better for anybody.

If there are SPV clients that don't pay attention to versions in block
headers, then setting the block version negative doesn't directly help
them, they will ignore it in any case.

If the worry is full nodes that are not upgraded, then a block with a
negative version number will, indeed, fork them off the the chain, in
exactly the same way a block with new hard-forking consensus rules would.
And with the same consequences (if there is any hashpower not paying
attention, then a worthless minority chain might continue on with the old
rules).

If the worry is not-upgraded SPV clients connecting to the old,
not-upgraded full nodes, I don't see how this proposed BIP helps.

I think a much better idea than this proposed BIP would be a BIP that
recommends that SPV clients to pay attention to block version numbers in
the headers that they download, and warn if there is a soft OR hard fork
that they don't know about.

It is also a very good idea for SPV clients to pay attention to timestamps
in the block headers that the receive, and to warn if blocks were generated
either much slower or faster than statistically likely. Doing that (as
Bitcoin Core already does) will mitigate Sybil attacks in general.

-- 
--
Gavin Andresen

-------------------------------------
I have an objection about "BIP comments" in BIP2. I think BIPs should be
self contained, but the specification recommends posting comments to the
Bitcoin Wiki (bitcoin.it). I think this is a bad idea and external sources
are bound to go stale over time as can be evidenced by a number of existing
BIPs which link to external content that has long since expired. Comments
should be made instead using the Wiki feature at bitcoin/bips itself (which
can be enabled in the administration settings).

On Tue, Mar 8, 2016 at 7:04 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> It has been about 1 month since BIP 2 finished receiving comments, so I
> believe it is an appropriate time to begin the process of moving it to
> Final
> Status. Toward this end, I have opened a pull request:
>
>     https://github.com/bitcoin/bips/pull/350
>
> The current requirement for this is that "the reference implementation is
> complete and accepted by the community". Given the vagueness of this
> criteria,
> I intend to move forward applying BIP 2's more specific criteria to itself:
>
> > A process BIP may change status from Draft to Active when it achieves
> rough
> > consensus on the mailing list. Such a proposal is said to have rough
> > consensus if it has been open to discussion on the development mailing
> list
> > for at least one month, and no person maintains any unaddressed
> > substantiated objections to it. Addressed or obstructive objections may
> be
> > ignored/overruled by general agreement that they have been sufficiently
> > addressed, but clear reasoning must be given in such circumstances.
>
> Furthermore, there is a reference implementation in the mentioned PR.
>
> Please review the latest draft BIP and provide any objections ASAP.
> If there are no outstanding objections on 2016 April 9th, I will consider
> the
> current draft to have reached rough consensus and update its Status to
> Final
> by merging the PR.
>
> Thanks,
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Out of curiosity, what is the technical reason a normal ECC-enabled 
smart-card cannot be used for the hardware signing component of a wallet 
app? (Since if it can, its standardization must have been discussed.)

Debian wiki gives a list of such cards with related opensource software 
to access them.

Regards



-------------------------------------
On Saturday, 24 September 2016 06:36:00 CEST Luke Dashjr via bitcoin-dev 
wrote:
> * OPL will no longer be an acceptable license. Many in the community feel
> that prohibiting publication is unacceptable for BIPs, and I haven't
> heard any arguments in favour of allowing it.

My suggestion would be that we replace OPL as an allowed license with one 
or two Creative Commons licenses. Following the suggestion from the OPL 
creators themselves.
According to Wikipedia;

> Open Publication License was created by the Open Content Project in 1999 
> as public copyright license for documents. The license was superseded
> in 2003/2007 by the Creative commons licenses.

I'd suggest saying that "Share alike" is required and "Attribution" is 
optional.

Executive summary; give the user the choice (next to public domain) between 
CCO and BY-SA
see;
https://en.wikipedia.org/wiki/
Creative_Commons_license#Seven_regularly_used_licenses


-------------------------------------
"*Abstract: *Digital currencies like Bitcoin rely on cryptographic
primitives to operate. However, past experience shows that cryptographic
primitives do not last forever: increased computational power and advanced
cryptanalysis cause primitives to break frequently, and motivate the
development of new ones. It is therefore crucial for maintaining trust in a
crypto currency to anticipate such breakage.
We present the first systematic analysis of the effect of broken primitives
on Bitcoin. We identify the core cryptographic building blocks and analyze
the various ways in which they can break, and the subsequent effect on the
main Bitcoin security guarantees. Our analysis reveals a wide range of
possible effects depending on the primitive and type of breakage, ranging
from minor privacy violations to a complete breakdown of the currency.
Our results lead to several observations on, and suggestions for, the
Bitcoin migration plans in case of broken cryptographic primitives."

https://eprint.iacr.org/2016/167

-------------------------------------
On Mon, Sep 19, 2016 at 09:13:40AM -0700, Tom Harding via bitcoin-dev wrote:
> 
> On 9/17/2016 9:20 PM, Peter Todd via bitcoin-dev wrote:
> > The probability that all N blocks are found by dishonest miners is q^N,
> 
> That's the probability that dishonest miners find N blocks in a row
> immediately.  What you want is the probability that they can build a
> chain N blocks long, taking the random-walk into account.
> 
> So use Satoshi's formula from bitcoin.pdf, section 11.  The results are
> remarkably different.  In particular, q=.5 is totally insecure, since
> for any N, both factions are guaranteed to eventually possess a chain of
> length N anchored at x at some point during the wild reorg melee.

Ah! That's a good point; my analysis only applies to the case where you're
assuming the dishonest miners aren't willing to lose revenue from the attack by
mining a less-work chain with blocks that won't end up in the main chain. I
should state that assumption more clearly.

If the dishonest miners are willing to spend money to create an invalid
timestamp the analysis is quite different. In OpenTimestamps a timestamp
doesn't contain the actual block headers - just a block height - so verifiers
are expected to have a working Bitcoin node. If that Bitcoin node is in sync
with the most-work work chain there's no risk: the blocks created by the
dishonest miners won't be part of the most-work chain, and validation of the
timestamp will fail.

In the case where the verifier is not in sync with the most-work chain, an
attacker can sybil attack the verifier's node and cause them to think that the
blocks committing the invalid timestamp are in fact the most-work chain. This
case is no different than a payee being sybil attacked, so we can use the same
analysis we would in that circumstance.

This also means that timestamps definitely shouldn't contain the block headers
of the blocks allegedly confirming them - that's an extremely weak proof given
the relative ease of creating a block, particularly when you take into account
that the same block could be used to create an unlimited number of fake
timestamps. OpenTimestamps doesn't do this, but it wouldn't hurt to make this
point 100% clear.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
It is **essential** that emergency code be prepared. This code must be
able to lower the difficulty by a large factor.

---

This halving-difficulty-drop problem can, with some bad luck, get quite
disastrous, very quickly.

( I did a micro-study of this problem here, for those who are unaware:
http://www.truthcoin.info/blog/mining-heart-attack )

For example, it is theoretically possible that 100% of miners (not 50%
or 10%) will shut off their hardware. This is because it is revenue
which ~halves, not profit. If miners are all equal, difficulty causes
their profit margin to narrow over time (for example, if BTC revenues
are $100, and amortized fixed costs are $10, then difficulty adjustments
will cause total energy costs to rise to ~ $89, such that total
pre-halving profit is $1 for everyone...post-halving, profit is -$49 for
everyone).

So, if miners are homogenous the result is disastrous. Fortunately,
miners are probably still somewhat heterogenous. However, we don't know
how their power contracts (or their hardware turnover) are
scheduled...many miners might (?) have already planned, in private, to
close down (or substantially reduce) operations after the halving.

As the coinbase rewards are currently orders of magnitude larger than
tx-fees, fees are unlikely to be able to compensate for this. Users may
decide to simply hold-off on transacting until fees decrease.

Worse, if the price crashes (possibly as a result of uncertainty
surrounding this episode), it will begin to affect miner-revenue.

As a result, miners may decide to temporarily halt mining until the
difficulty falls naturally.

But such a temporary halt is also (potentially) disastrous. Recall the
simple fact that difficulty adjustments are measured in blocks, not time
(it appears that we have exactly 1015 blocks between the halving block
and the next difficulty adjustment block). If excessive difficulty
chokes the system, next difficulty adjustment may *never* arrive naturally.

In this worst-case (but somewhat plausible) scenario, we will be
*forced* to lower the difficulty via hard fork, and we will be forced to
do so very very QUICKLY, as word will be spreading that the Bitcoin
system has broken!

If a specific hard fork is not coded and tested for this, in advance,
the delay might be accompanied by endless [contentious] conversations
about what else should be included in this hard fork.

Worse, since all users will need to upgrade, there will be uncertainty
over contentious versions, malicious agents may try to tamper with
versions (to steal Bitcoins), etc. We should consider pushing a version
out for users to upgrade, in advance of the halving, as soon as possible.



What a disaster! I certainly hope it does not happen, but if it does we
should have already agreed on what to do.


One choice is "which number do we set the difficulty to?". Half may be
too much, or too little. However, allow me to suggest that, if this
disastrous scenario occurs, we shouldn't take any chances, and reduce
difficulty by a huge proportion...80% or so. The difficulty will then
quickly begin to increase again...we can warn users of the increased
orphan risk, and that they should wait for many confirmations (which
should be happening faster).

So, "Allow the alert key to reduce the difficulty by 80%, exactly once
on one of the 1015 blocks between halving and difficulty adjustment."

And we should consider smoothing the rewards (as described in my post,
can be done via soft fork) to prevent this from happening again. In
microeconomics literature, 'kinks' in incentive-systems are
almost-universally agreed to be very undesirable.

Paul


On 3/2/2016 10:42 AM, Luke Dashjr via bitcoin-dev wrote:
> On Wednesday, March 02, 2016 2:56:14 PM Luke Dashjr via bitcoin-dev wrote:
>> so it may even be possible to have such a proposal ready in time to be
>> deployed alongside SegWit  to take effect in time for the upcoming subsidy
>> halving.
> 
> Lapse of thinking/clarity here. This probably isn't a practical timeframe for 
> deployment, unless/until there's an emergency situation. So if the code were 
> bundled with SegWit, it would need some way to avoid its early activation 
> outside of such an emergency (which could possibly be detected in code, in 
> this case).
> 
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
Eerrrr....let me revise that last paragraph.  That's 12 *GB* of filters at
today's block height (at fixed false-positive rate 1e-6.  Compared to block
headers only which are about 33 MB today.  So this proposal is not really
compatible with such a wallet being "light"...

Damn units...

Bob McElrath via bitcoin-dev [bitcoin-dev@lists.linuxfoundation.org] wrote:
> I like this idea, but let's run some numbers...
> 
> bfd--- via bitcoin-dev [bitcoin-dev@lists.linuxfoundation.org] wrote:
> > A Bloom Filter Digest is deterministically created of every block
> 
> Bloom filters completely obfuscate the required size of the filter for a desired
> false-positive rate.  But, an optimal filter is linear in the number of elements
> it contains for fixed false-positive rate, and logarithmic in the false-positive
> rate.  (This comment applies to a RLL encoded Bloom filter Greg mentioned, but
> that's not the only way)  That is for N elements and false positive rate
> \epsilon:
> 
>     filter size = - N \log_2 \epsilon
> 
> Given that the data that would be put into this particular filter is *already*
> hashed, it makes more sense and is faster to use a Cuckoo[1] filter, choosing a
> fixed false-positive rate, given expected wallet sizes.  For Bloom filters,
> multiply the above formula by 1.44.
> 
> To prevent light clients from downloading more blocks than necessary, the
> false-positive rate should be roughly less than 1/(block height).  If we take
> the false positive rate to be 1e-6 for today's block height ~ 410000, this is
> about 20 bits per element.  So for todays block's, this is a 30kb filter, for a
> 3% increase in block size, if blocks commit to the filter.  Thus the required
> size of the filter commitment is roughly:
> 
>     filter size = N \log_2 H
> 
> where H is the block height.  If bitcoin had these filters from the beginning, a
> light client today would have to download about 12MB of data in filters.  My
> personal SPV wallet is using 31MB currently.  It's not clear this is a bandwidth
> win, though it's definitely a win for computing load on full nodes.
> 
> 
> [1] https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf
> 
> --
> Cheers, Bob McElrath
> 
> "For every complex problem, there is a solution that is simple, neat, and wrong."
>     -- H. L. Mencken 
> 
> 
> 
> !DSPAM:5733934b206851108912031!



> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> !DSPAM:5733934b206851108912031!

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


-------------------------------------
On 14/05/16 18:14, Jonas Schnelli wrote:
> AFAIK: Bip39 import (cross-wallet) is not supported by Schildbachs
> android wallet [1] and Electrum [2] and Breadwallet [3].

They are not BIP44 compatible wallets. This thread is about BIP44.

> * What if the "old" wallet has used more then 1000 addresses? I guess

They are not following the spec and are thus not BIP44 compatible.

> * If I import a bip39 mnemonic into a hardware wallet (assume Trezor or
> Keepkey) I have to type in the words into my computer which bypasses
> some of the security my hardware wallet provides me (MITM seed attack).
> Together with the point above this reduces the security of a wallet (in
> particular cold storage significant).

You should send all your coins to the new seed anyway, but I agree this
might be tricky for non-power users.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
Thanks Greg for the testing!

Note that to those who are reviewing the doc, a few minor tweaks to
wording and clarification have been made to the git version, so please
review there.

On 05/03/16 05:02, Gregory Maxwell wrote:
> On Mon, May 2, 2016 at 10:13 PM, Matt Corallo via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> Hi all,
>>
>> The following is a BIP-formatted design spec for compact block relay
>> designed to limit on wire bytes during block relay. You can find the
>> latest version of this document at
>> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.
> 
> Thanks Matt!
> 
> I've been testing this for a couple weeks (in various forms).  I've
> been getting over 96% reduction in block-bytes sent. I don't have a
> good metric for it, but bandwidth spikes are greatly reduced. The
> largest blocktxn message I've seen on a node that has been up for at
> least a day is 475736 bytes. 94% of the blocks less than 100kb must be
> sent in total.
> 
> In the opportunistic mode my measurements are showing 73% of blocks
> transferred with 0.5 RTT even without prediction, 87% if up to 4
> additional transactions are predicted, and 91% for 30 transactions (my
> rough estimate for the 10k maximum prediction suggested in the BIP.
> 


-------------------------------------
On Thu, Mar 31, 2016 at 09:41:40PM -0700, Timo Hanke via bitcoin-dev wrote:
> Hi.
> 
> I'd like to announce a white paper that describes a very new and
> significant algorithmic improvement to the Bitcoin mining process which has
> never been discussed in public before. The white paper can be found here:
> 
> http://www.math.rwth-aachen.de/~Timo.Hanke/AsicBoostWhitepaperrev5.pdf

What steps are you going to take to make sure that this improvement is
available to all ASIC designers/mfgs on a equal opportunity basis?

The fact that you've chosen to patent this improvement could be a
centralization concern depending on the licensing model used. For example, one
could imagine a licensing model that gave one manufacture exclusive rights.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
<html><head></head><body><div style="font-family: Verdana;font-size: 12.0px;"><div style="font-family: Verdana;font-size: 12.0px;">
<div style="font-family: Verdana;font-size: 12.0px;">
<div>
<div>Good Afternoon</div>

<div>I&#39;m writing this to ask the other bitcoin developers to carry the torch of a project/bip I have been working on more specifically a set of new op_codes to allow data efficient quantum proof transactions on the bitcoin network.</div>

<div>&nbsp;</div>

<div>Sadly due to unforeseen circumstances my skills and focus and slipped from bitcoin and programming and errors crept in so, if any core dev&#39;s would like to take this on I would obliged. I will answer any questions about the project just email me. I still will continue just at a slower pace and will only submit pulls for peer checking.</div>

<div>&nbsp;</div>

<div>you will find it at:</div>

<div>github.com/Alonzo-Coeus/bitcoin</div>

<div>and the bip at:</div>

<div><a href="https://github.com/Alonzo-Coeus/bips/blob/master/bip-alonzocoeus-quantumsafe.mediawiki" target="_blank">https://github.com/Alonzo-Coeus/bips/blob/master/bip-alonzocoeus-quantumsafe.mediawiki</a></div>

<div>&nbsp;</div>

<div>Thank you, Alonzo</div>

<div>ps: write access will be granted to members of github.com/bitcoin if they would like to take this on</div>
</div>
</div>
</div></div></body></html>


-------------------------------------
On Sat, Feb 06, 2016 at 12:45:14PM -0500, Gavin Andresen via bitcoin-dev wrote:
> On Sat, Feb 6, 2016 at 12:01 PM, Adam Back <adam@cypherspace.org> wrote:
> 
> >
> > It would probably be a good idea to have a security considerations
> > section
> 
> 
> Containing what?  I'm not aware of any security considerations that are any
> different from any other consensus rules change.

I covered the security considerations unique to hard-forks on my blog:

https://petertodd.org/2016/soft-forks-are-safer-than-hard-forks

> > , also, is there a list of which exchange, library, wallet,
> > pool, stats server, hardware etc you have tested this change against?
> >
> 
> That testing is happening by the exchange, library, wallet, etc providers
> themselves. There is a list on the Classic home page:
> 
> https://bitcoinclassic.com/

How do we know any of this testing is actually being performed? I don't
currently know of any concrete testing actually done.

> > Do you have a rollback plan in the event the hard-fork triggers via
> > false voting as seemed to be prevalent during XT?  (Or rollback just
> > as contingency if something unforseen goes wrong).
> >
> 
> The only voting in this BIP is done by the miners, and that cannot be faked.

Are you unaware of Not Bitcoin XT?

https://bitcointalk.org/index.php?topic=1154520.0

> I can't imagine any even-remotely-likely sequence of events that would
> require a rollback, can you be more specific about what you are imagining?
> Miners suddenly getting cold feet?

See above.

Also, as the two coins are separate currencies and can easily trade
against each other in a 75%/25% split, it would be easy for the price to
diverge and hashing power to move.

In fact, I've been asked multiple times now by exchanges and other
players in this ecosystem for technical advice on how to split coins
across the chains effectively (easily done with nLockTime). Notably, the
exchanges who have asked me this - who hold customer funds on their
behalf - have informed me that their legal advice was that the
post-hard-fork coins are legally speaking separate currencies, and
customers must be given the opportunity to transact in them separately
if they choose too.  Obviously, with a 75%/25% split, while block times
on the other chain will be slower, the chain is still quite useful and
nearly as secure as the main chain against 51% attack; why I personally
have suggested a 99% threshold:

http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012309.html

(remember that the threshold can always be soft-forked down)

It's also notable that millions of dollars of Bitcoin are voting agsast
the fork on the proof-of-stake voting site Bitcoinocracy.com While
obviously not comprehensive, the fact that a relatively obscure site
like it can achieve participation like that, even without an easy to use
user friendly interface.

> > How do you plan to monitor and manage security through the hard-fork?
> >
> 
> I don't plan to monitor or manage anything; the Bitcoin network is
> self-monitoring and self-managing. Services like statoshi.info will do the
> monitoring, and miners and people and businesses will manage the network,
> as they do every day.

Please provide details on exactly how that's going to happen.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698

-------------------------------------
On Tue, Aug 23, 2016 at 8:54 PM, Kenneth Heutmaker via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> SPV is kinda broken if the wallet doesn’t do this detection. If your wallet connects only to nodes that don’t support bloom filtering, the wallet never gets updates. We have had a spike in users reporting that their wallet isn't getting updated. To compound the problem, they rescan the blockchain and lose all of their transaction history. It has caused much panic among less technical users.
>
> We believe that failing to detect the NODE_BLOOM bit is the culprit, although it is non-deterministic, so we aren't certain.

There are almost no NODE_BLOOM supporting bloom-off nodes on the
network currently. So, while supporting this is important, I am
doubtful that its the current problem you've suffered.

There are a great many fake nodes which appear to exist purely to
monitor transactions. Many do not implement enough of the protocol to
support scanning or transaction relay. (and, in fact, relaying
transactions would make monitoring less effective).

You can't count on peers on a peer to peer network to be honest and
cooperative. Implementations need to work hard to be robust to abusive
peers. Unfortunately, the design of the bloom filtering is such that
it isn't always easy (or even possible) to be robust.


-------------------------------------
On Jan 7, 2016 5:22 PM, "Gavin Andresen via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> On Thu, Jan 7, 2016 at 6:52 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:
>>
>> Bitcoin does have parts that rely on economic arguments for security or
privacy, but can we please stick to using cryptography that is up to par
for parts where we can? It's a small constant factor of data, and it
categorically removes the worry about security levels.
>
> Our message may have crossed in the mod queue:
>
> "So can we quantify the incremental increase in security of
SHA256(SHA256) over RIPEMD160(SHA256) versus the incremental increase in
security of having a simpler implementation of segwitness?"

There are several clever ways to exploit even chosen prefix collisions
using the scripting language. One could search for collisions where one
message is some data and the other is a jump over a critical check.

>
> I believe the history of computer security is that implementation errors
and sidechannel attacks are much, much more common than brute-force breaks.
KEEP IT SIMPLE.

Ask the Iranian nuclear program. Or those brainwallet users.
>
> (and a quibble:  "do a 80-bit search for B and C such that H(A and B) =
H(B and C)"  isn't enough, you have to end up with a C public key for which
you know the corresponding private key or the attacker just succeeds in
burning the funds)
>
>
> --
> --
> Gavin Andresen
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Fri, Jan 8, 2016 at 7:02 AM, Rusty Russell <rusty@rustcorp.com.au> wrote:

> Matt Corallo <lf-lists@mattcorallo.com> writes:
> > Indeed, anything which uses P2SH is obviously vulnerable if there is
> > an attack on RIPEMD160 which reduces it's security only marginally.
>
> I don't think this is true?  Even if you can generate a collision in
> RIPEMD160, that doesn't help you since you need to create a specific
> SHA256 hash for the RIPEMD160 preimage.
>
> Even a preimage attack only helps if it leads to more than one preimage
> fairly cheaply; that would make grinding out the SHA256 preimage easier.
> AFAICT even MD4 isn't this broken.
>

It feels like we've gone over that before, but I can never remember where
or when. I believe consensus was that if we were using the broken MD5 in
all the places we use RIPEMD160 we'd still be secure today because of
Satoshi's use of nested hash functions everywhere.


> But just with Moore's law (doubling every 18 months), we'll worry about
> economically viable attacks in 20 years.[1]


> That's far enough away that I would choose simplicity, and have all SW
> scriptPubKeys simply be "<0> RIPEMD(SHA256(WP))" for now, but it's
> not a no-brainer.


Lets see if I've followed the specifics of the collision attack correctly,
Ethan (or somebody) please let me know if I'm missing something:

So attacker is in the middle of establishing a payment channel with
somebody. Victim gives their public key, attacker creates the innocent
fund-locking script  '2 V A 2 CHECKMULTISIG' (V is victim's public key, A
is attacker's) but doesn't give it to the victim yet.

Instead they then generate about 2^81scripts that are some form of
pay-to-attacker ....
... wait, no that doesn't work, because SHA256 is used as the inner hash
function.  They'd have to generate 2^129 to find a cycle in SHA256.

Instead, they .. what? I don't see a viable attack unless RIPEMD160 and
SHA256 (or the combination) suffers a cryptographic break.


-- 
--
Gavin Andresen

-------------------------------------
This BIP is unnecessary, in my opinion.

I'm going to take issue with items (2) and (3) that are the motivation for
this BIP:

" 2. Full nodes and SPV nodes following original consensus rules may not be
aware of the deployment of a hardfork. They may stick to an
economic-minority fork and unknowingly accept devalued legacy tokens."

If a hardfork is deployed by increasing the version number in blocks (as is
done for soft forks), then there is no risk-- Full and SPV nodes should
notice that they are seeing up-version blocks and warn the user that they
are using obsolete software.

It doesn't matter if the software is obsolete because of hard or soft fork,
the difference in risks between those two cases will not be understood by
the typical full node or SPV node user.

" 3. In the case which the original consensus rules are also valid under
the new consensus rules, users following the new chain may unexpectedly
reorg back to the original chain if it grows faster than the new one.
People may find their confirmed transactions becoming unconfirmed and lose
money."

If a hard or soft fork uses a 'grace period' (as described in BIP 9 or BIP
101) then there is essentially no risk that a reorg will happen past the
triggering block. A block-chain re-org of two thousand or more blocks on
the main Bitcoin chain is unthinkable-- the economic chaos would be
massive, and the reaction to such a drastic (and extremely unlikely) event
would certainly be a hastily imposed checkpoint to get everybody back onto
the chain that everybody was using for economic transactions.


Since I don't agree with the motivations for this BIP, I don't think the
proposed mechanism (a negative-version-number-block) is necessary. And
since it would simply add more consensus-level code, I believe the
keep-it-simple principle applies.


-- 
--
Gavin Andresen

-------------------------------------


>> On Jun 29, 2016, at 12:22 AM, Gregory Maxwell <gmaxwell@gmail.com> wrote:
>> 
>> On Tue, Jun 28, 2016 at 9:59 PM, Eric Voskuil <eric@voskuil.org> wrote:
>> Passing the session ID out of band is authentication. As this is explicitly not part of BIP151 it cannot be that BIP151 provides the tools to detect a attack (the point at issue).
> 
> It provides the ID, the rest is meat.

The rest is "authentication".

> Users can compare session IDs
> via whatever communications channels they already use after the fact
> and discover if they were or are being MITMed.
> 
>>>> It requires a secure channel and is authentication. So BIP151 doesn't provide the tools to detect an attack, that requires authentication. A general requirement for authentication is the issue I have raised.
>>> 
>>> One might wonder how you ever use a Bitcoin address, or even why we might guess these emails from "you" aren't actually coming from the NSA.
>> 
>> The sarcasm is counterproductive Greg. By the same token I could ask how you ever use Bitcoin given that the P2P protocol is not encrypted or authenticated.
> 
> I think I was unclear. A bitcoin address needs to be sent over a secure channel, which we do not provide. Yet sending funds to addresses instead of anyone_can_spend is pretty useful.
> 
> Similarly, I can guess that messages claiming to you are probably from you when many people can independently check, even if they don't usually. The fact tampered messages might be detected is a big disincentive from trying.

You were perfectly clear. Did I give some indication that I did not understand what you meant?

>> The blockchain and mempool are a cache of public data. Transmission of a payment address to a payer is not a comparable scenario.
> 
> The precise timing and ordering of transactions being relayed is _not_
> public data.

Posting txs to the network is a client-server scenario. The set of txs arriving at an arbitrary node, including the order of arrival, is by definition public information. The only possible way it could be considered private is if the entire network was private.

So where does the private timing become public? First hop, second, third?

Encryption and authentication cannot prevent timing attacks against a person posting txs to the network unless the entire network is "secured". That is not possible without centralized access control.

Encrypting the P2P network doesn't resolve this problem, nor does authentication, nor does Tor. I would prefer we advance an actual solution to this significant problem than advance a false sense of security while creating both complexity and the likely evolution of node identity.

e

-------------------------------------
> Most people?

I'm talking about services that are built to handle multiple accounts, like
exchanges and payment processors.


> You realize that you need to set up bitcoind to make an
> external request on every reception of funds on any address in the whole
> system.
>
No, you don't. You can write a script that repeatedly asks bitcoind for the
block height, and when it increments you know a new block has been
confirmed. So then you request the transaction list from the latest block,
and check each confirmed transaction against your database of receive/watch
addresses. If there is a match, you record the transaction info in your
database so you can use it as an input later to create a spend transaction.

You could also use something like Bitpay's Insight to make interfacing with
bitcoind easier.


> It can't possibly scale, also we don't have the time to build an account
> system for every bitcoind service. Imagine the loss of time, it's huge and
> grows exponentially with adoption, or rather there is no adoption!
>
What are you trying to build?


> Also external systems are not be trusted, specially not bitgo, did you
> read any news in the last 24 hours?
>
I prefer to wait until all facts are in before I pass judgement. I think
BitGo is an excellent company with a great track record. If used properly,
they are extremely secure. If you are worried about storing funds there
long time, don't--just use them to detect incoming payments and move your
funds elsewhere for long term storage.


> /m
>
> On Wed, 3 Aug 2016, James MacWhyte wrote:
>
> >
> > From what I've seen, most people build their own account system
> separately
> > (including fee management) and just use bitcoind to send, receive, and
> > verify transactions. It's not meant to be a drop-in solution for running
> an
> > entire bitcoin deposit and withdrawal system, it just provides the bare
> > tools required to build your own. If you need a pre-built solution, there
> > are companies that provide those types of services as a platform (BitGo,
> for
> > example).
> >
> >
> > On Wed, Aug 3, 2016, 11:25 Marc Larue via bitcoin-dev
> > <bitcoin-dev@lists.linuxfoundation.org> wrote:
> >       Hi,
> >
> >       I have 2 problems with bitcoind that separately are not a
> >       problem but
> >       together they make the platform unusable for many projects.
> >
> >       If I have accounts I need to make sure the account holders do
> >       not
> >       overcharge their account. To do this I can now use
> >       "createrawtransaction()
> >       + fundrawtransaction() + signrawtransaction()" and then make
> >       sure the
> >       transaction can be paid by an account.
> >
> >       But since you deprecated the accounts and there is no
> >       sendrawtransactionfrom() method; I either have to build my own
> >       account
> >       system (this is no picknick btw, since you need to track all
> >       incoming
> >       funds to all addresses and having an integrated account system
> >       in bitcoind
> >       is 100% necessary to do this effectively).
> >
> >       Or I might be able to go ahead and speculate that you will not
> >       be able to
> >       untangle the account code and hack my bitcoind to have a
> >       sendfrom with a
> >       fixed fee parameter that overrides the size multiplication and I
> >       just do
> >       the math before I send hoping that the transactions go through
> >       (this is
> >       bad but better than having accounts overcharge because they send
> >       dust that
> >       induce high fees).
> >
> >       I understand the privacy problems with using accounts for
> >       off-chain
> >       microstransactions but currently it's the best workable option.
> >
> >       I hope you understand that I'm not trolling here, I have been
> >       mining since
> >       2011 on FPGAs and built bitcoinbankbook.com 2 years ago. When I
> >       descovered
> >       that once transactions will require fees (back then they didn't)
> >       and that
> >       your system is not able to handle fees with accounts, I stopped
> >       developing
> >       everything related to bitcoin.
> >
> >       There are probably 100s if not 1000s of developers in the same
> >       situation.
> >
> >       You can't just deprecate accounts like that because nobody likes
> >       the code.
> >       Without accounts bitcoind is only a person-to-person manual
> >       client.
> >
> >       To build many-to-many automatic "organisations" on top of
> >       bitcoind you
> >       need accounts and you need fees that are predictable.
> >
> >       Kind Regards,
> >
> >       /marc
> >       _______________________________________________
> >       bitcoin-dev mailing list
> >       bitcoin-dev@lists.linuxfoundation.org
> >       https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> >
> >
>

-------------------------------------
Hello all,

Today we are submitting some updates to BIP75:

-- Example use cases have been reworded to more accurately describe the
goal of this BIP and how the technology works.
-- ECDSA and PGP have been added to the supported public key infrastructure
(PKI) types to increase flexibility and use cases.
-- Versioning has been added to make future changes and backwards
compatibility easy to manage.
-- Other minor, technical details have been added or changed to improve
performance and reduce ambiguity during implementation.

You can see the PR here: https://github.com/bitcoin/bips/pull/439

Thank you,
James

-------------------------------------
On Fri, Feb 26, 2016 at 5:35 AM, Jonathan Toomim via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> An improvement that I've been thinking about implementing (after
> Blocktorrent) is an option for batched INVs. Including the hashes for two
> txes per IP packet instead of one would increase the INV size to 229 bytes
> for 64 bytes of payload -- that is, you add 36 bytes to the packet for every
> 32 bytes of actual payload. This is a marginal efficiency of 88.8% for each
> hash after the first. This is *much* better.
>
> Waiting a short period of time to accumulate several hashes together and
> send them as a batched INV could easily reduce the traffic of running
> bitcoin nodes by a factor of 2,

Copying my response to you from BitcoinTalk
(https://bitcointalk.org/index.php?topic=1377345.msg14013294#msg14013294):

Uh. Bitcoin has done this since the very early days. The batching was
temporarily somewhat hobbled between 0.10 and 0.12 (especially when
you had any abusive frequently pinging peers attached), but is now
fully functional again and it now manages to batch many transactions
per INV pretty effectively. Turn on net message debugging and you'll
see the many INVs that are much larger than the minimum. The average
batching size (ignoring the trickle cut-through) is about 5 seconds
long-- and usually gets about 10 transactions per INV. My measurements
were with these fixes in effect; I expect the blocksonly savings would
have been higher otherwise.

2016-02-26 05:47:08 sending: inv (1261 bytes) peer=33900
2016-02-26 05:47:08 sending: inv (109 bytes) peer=32460
2016-02-26 05:47:08 sending: inv (37 bytes) peer=34501
2016-02-26 05:47:08 sending: inv (217 bytes) peer=33897
2016-02-26 05:47:08 sending: inv (145 bytes) peer=41863
2016-02-26 05:47:08 sending: inv (37 bytes) peer=35725
2016-02-26 05:47:08 sending: inv (73 bytes) peer=20567
2016-02-26 05:47:08 sending: inv (289 bytes) peer=44703
2016-02-26 05:47:08 sending: inv (73 bytes) peer=13408
2016-02-26 05:47:09 sending: inv (649 bytes) peer=41279
2016-02-26 05:47:09 sending: inv (145 bytes) peer=42612
2016-02-26 05:47:09 sending: inv (325 bytes) peer=34525
2016-02-26 05:47:09 sending: inv (181 bytes) peer=41174
2016-02-26 05:47:09 sending: inv (469 bytes) peer=41460
2016-02-26 05:47:10 sending: inv (973 bytes) peer=133
2016-02-26 05:47:10 sending: inv (361 bytes) peer=20541

Twiddling here doesn't change the asymptotic efficiency though; which
is what my post is about.

[I'm also somewhat surprised that you were unaware of this; one of the
patches "classic" was talking about patching out was the one restoring
the batching... due to a transaction deanonymization service (or
troll) claiming it interfered with their operations.]


-------------------------------------
On Tuesday, January 26, 2016 3:01:13 AM Toby Padilla wrote:
> > As I explained, none of those reasons apply to PaymentRequests.
> 
> As they exist today PaymentRequests allow for essentially the same types of
> transactions as non-PaymentRequest based transactions with the limitation
> that OP_RETURN values must be greater. In that sense they're basically a
> pre-OP_RETURN environment. OP_RETURN serves a purpose and it can't be used
> with PaymentRequest transactions.

OP_RETURN can be used, but you need to burn coins. I don't see any benefit to 
changing that. It is better that coins are burned.

> > I have no idea what you are trying to say here.
> 
> I think if you think through how you would create an OP_RETURN transaction
> today without this BIP you'll see you need a key at some point if you want
> a zero value.

You *always* need a key, to redeem inputs... regardless of values.

Luke


-------------------------------------
On Sunday, 16 October 2016 19:35:52 CEST Matt Corallo wrote:
> You keep calling flexible transactions "safer", and yet you haven't
> mentioned that the current codebase is riddled with blatant and massive
> security holes.

I am not afraid of people finding issues with my code, I'm only human. Would 
appreciate you reporting actual issues instead of hinting at things here.
Can't fix things otherwise :)

But, glad you brought it up, the reason that FT is safer is because of the 
amount of conceps that SegWit changes in a way that anyone doing development 
on Bitcoin later will need to know about them in order to do proper 
development.
I counted 10 in my latest vlog entry.  FT only changes 2.

Its safer because its simpler.

> For example, you seem to have misunderstood C++'s memory
> model - you would have no less than three out-of-bound, probably
> exploitable memory accesses in your 80-LoC deserialize method at
> https://github.com/bitcoinclassic/bitcoinclassic/blob/develop/src/primitiv
> es/transaction.cpp#L119 if you were to turn on flexible transactions (and
> I only reviewed that method for 2 minutes). 

The unit test doesn't hit any of them. Valgrind only reports such possibly 
exploitable issues in secp256k and CKey::MakeNewKey. The same as in Core.

I don't doubt that your 2 minute look shows stuff that others missed, and 
that valgrind doesn't find either, but I'd be really grateful if you can 
report them specifically to me in an email off list (or github, you know the 
drill).
More feedback will only help to make the proposal stronger and even better. 
Thanks!

> If you want to propose an
> alternative to a community which has been in desperate need of fixes to
> many problems for several years, please do so with something which would
> not take at least a year to complete given a large team of qualified
> developers.

I think FT fits the bill just fine :)  After your 2 minute look, take a bit 
longer and check the rest of the code. You may be surprised with the 
simplicity of the approach.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Tuesday, August 16, 2016 10:10:01 AM Johnson Lau via bitcoin-dev wrote:
> Specification
> 
> Every signature passed to OP_CHECKSIG, OP_CHECKSIGVERIFY, OP_CHECKMULTISIG,
> or OP_CHECKMULTISIGVERIFY, to which ECDSA verification is applied,

Not 20-byte witness v0 programs?

> These operators all perform ECDSA verifications on pubkey/signature pairs,
> iterating from the top of the stack backwards. For each such verification,
> if the signature does not pass the IsLowDERSignature check,

"the IsLowDERSignature check" is not well-defined. Probably intend to 
reference the previous paragraph?

Luke


-------------------------------------
On Thursday 24 Mar 2016 13:20:48 Chris via bitcoin-dev wrote:
> As far as the use cases others mentioned, connecting and SPV wallet to
> your full node is certainly one. It would make it easy to, say, connect
> the android bitcoin-wallet to your own node. I've hacked on that wallet
> to make it connect to my .onion node, but it's very slow border-line
> unusable. Basic encryption and authentication would make that viable.

What about using some interface, much like the JSON one (but more likely the 
zeroMQ one) instead? Would that not solve the problem?

I'm thinking that would not be a replacement for a full-node-connection but in 
addition.

Which means that some questions can be asked over that channel that you need 
authentication for. It would be a much better separation of concerns.


-------------------------------------
On Jun 23, 2016 14:10, "Peter Todd" <pete@petertodd.org> wrote:

> Right, so you accept that we'll exert some degree of editorial control;
the
> question now is what editorial policies should we exert?

No, I do not. I am saying that some degree of editorial control will
inevitably exist, simply because there is some human making the choice of
assigning a BIP number and merging. My opinion is that we should try to
restrict that editorial control to only be subject to objective process,
and not be dependent on personal opinions.

> My argument is that rejecting BIP75 is something we should do on
> ethical/strategic grounds. You may disagree with that, but please don't
troll
> and call that "advocating censorship"

I think that you are free to express dislike of BIP75. Suggesting to remove
it for that reason is utterly ridiculous to me, whatever you want to call
it.

-- 
Pieter

-------------------------------------
On Tue, Aug 16, 2016 at 6:30 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

> On Aug 17, 2016 00:23, "Russell O'Connor via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> > If one's goal is to mess with an transaction to prevent it from being
> mined, it is more effective to just not relay the transaction rather than
> to mess with the witness.  Given two transactions with the same txid and
> different witness data, miners and good nodes ought to mine/relay the
> version with the lower cost (smaller?) witness data.
>
> That implies that everyone will see both versions and be able to make that
> choice. Unfortunately, those two versions will be definition be in conflict
> with each other, and thus only one will end up paying a fee. We're can't
> relay two transactions for the price of one, or we'd expose the p2p network
> to a very cheap DDoS attack: just send increasingly small versions of the
> same transaction.
>
Can I already do something similar with replace by fee, or are there limits
on that?

-------------------------------------
Have you checked AudioModem out:
https://github.com/applidium/AudioModem

Or Chirp:
http://www.chirp.io/faq/

Or this Network World article (particularly the last portion on bitcoin):
http://www.networkworld.com/article/2956450/smartphones/sending-data-over-sound-revisited.html
and
http://www.networkworld.com/article/2689597/opensource-subnet/how-a-tv-network-is-being-used-to-move-financial-transactions.html

Instead of re-inventing the wheel, perhaps take a look at them.  Or at
least to see their design choices and rationale.

Cheers!


On Tue, Aug 9, 2016 at 7:06 PM, Daniel Hoffman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I have updated the GitHub a lot (changed tones to be less chirpy, fixed
> some smalls) and made a couple of samples (see attachment for MP3 and FLAC
> of both tone tables, first 16 then 4). Is this good enough to warrant an
> official BIP number? I haven't built a decoder yet, but it seems like the
> encoder is working properly (looked at Audacity, seems like it is working),
> and some people on reddit want to "allow for decoding experiments"
> <https://www.reddit.com/r/btc/comments/4wsn7v/bip_proposal_addresses_over_audio_thoughts/d69m3st>
>
> What suggestions do you all have for it?
>
> On Mon, Aug 8, 2016 at 8:50 PM, Daniel Hoffman <danielhoffman699@gmail.com
> > wrote:
>
>> It wouldn't be feasible in the vast majority of cases, but I can't think
>> of a reason why it can't be built into the standard.
>>
>> On Mon, Aug 8, 2016 at 5:59 PM, Trevin Hofmann via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> Would it be feasible to transmit an entire BIP21 URI as audio? If you
>>> were to encode any extra information (such as amount), it would be useful
>>> to include a checksum for the entire message. This checksum could possibly
>>> be used instead of the checksum in the address.
>>>
>>> Trevin
>>>
>>> On Aug 8, 2016 3:06 PM, "Justin Newton via bitcoin-dev" <
>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>> Daniel,
>>>>    Thanks for proposing this.  I think this could have some useful use
>>>> cases as you state.  I was wondering what you would think to adding some
>>>> additional tones to optionally denote an amount (in satoshis?).
>>>>
>>>> (FYI, actual link is here:  https://github.com/Dako300/BIP )
>>>>
>>>> Justin
>>>>
>>>> On Mon, Aug 8, 2016 at 2:22 PM, Daniel Hoffman via bitcoin-dev <
>>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>>
>>>>> This is my BIP idea: a fast, robust, and standardized for representing
>>>>> Bitcoin addresses over audio. It takes the binary representation of the
>>>>> Bitcoin address (little endian), chops that up into 4 or 2 bit chunks
>>>>> (depending on type, 2 bit only for low quality audio like american
>>>>> telephone lines), and generates a tone based upon that value. This started
>>>>> because I wanted an easy way to donate to podcasts that I listen to, and
>>>>> having a Shazam-esque app (or a media player with this capability) that
>>>>> gives me an address automatically would be wonderful for both the consumer
>>>>> and producer. Comes with error correction built into the protocol
>>>>>
>>>>> You can see the full specification of the BIP on my GitHub page (
>>>>> https://github.com/Dako300/BIP-0153).
>>>>>
>>>>> _______________________________________________
>>>>> bitcoin-dev mailing list
>>>>> bitcoin-dev@lists.linuxfoundation.org
>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>>
>>>> Justin W. Newton
>>>> Founder/CEO
>>>> Netki, Inc.
>>>>
>>>> justin@netki.com
>>>> +1.818.261.4248
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev@lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Mon, Oct 17, 2016 at 01:17:39PM +0200, Tom Zander via bitcoin-dev wrote:
> On Sunday, 16 October 2016 17:19:37 CEST Andrew C wrote:
> > On 10/16/2016 4:58 PM, Tom Zander via bitcoin-dev wrote:
> > > Lets get back to the topic. Having a longer fallow period is a simple
> > > way to be safe.  Your comments make me even more scared that safety is
> > > not taken into account the way it would.
> > 
> > Can you please explain how having a longer grace period makes it any
> > safer? Once the fork reaches the LOCKED_IN status, the fork will become
> > active after the period is over. How does having a longer grace period
> > make this any safer besides just adding more waiting before it goes
> > active? 
> 
> As Marek wrote just minutes before your email came in; companies will not 
> roll out their updates until it locks in. Peter Todd says the same thing.
> So your assumption that the lock-in time is the END of the upgrading is 
> false. Thats only the case for miners.
> 
> The entire point here is that SegWit is much bigger than just full nodes 
> (including miner).
> An entire ecosystem of Bitconin will have a need to upgrade.
> 
> I understand people saying that non-miners don't *need* to upgrade in order 
> to avoid being kicked of the network, but truely, thats a bogus argument.
> 
> People want to actually participate in Bitcoin and that means they need to 
> understand all of it. Not just see anyone-can-spend transactions.
> I think its rather odd to think that developers on this list can decide
> the risk profile that Bitcoin using companies or individuals should have.

Please don't misleadingly reference/quote me.

I made it quite clear in my last post that because segwit is a backwards
compatible soft-fork, the vast majority of code out there will not have to
change; my own OpenTimestamps being a good example. All I'll have to do to
prepare for segwit is upgrade the (pruned) full nodes that the OpenTimestamps
servers depend on to determine what's the most-work valid chain, and in the
event I was concerned about compatibility issues, I could easily run my
existing nodes behind updated segwit-supporting (pruned) nodes.

Like most users, my OpenTimestamps code doesn't "fully understand" transactions
at all - I rely on my full node to do that for me. What it does understand
about transactions and blocks remains the same in segwit. I can receive
transactions from segwit users with lite-client security without any action at
all, and full-node security once I upgrade my full nodes (or run them behind
upgraded nodes).

Your proposed alternative to segwit - flexible transactions - has none of these
beneficial properties. And as Matt Corallo reported, it's no-where near ready
for deployment: three buffer overflows in 80 lines of code is a serious
problem.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
> All practical single-use seals will be associated with some kind of
> condition,
> such as a pubkey, or deterministic expression, that needs to be satisfied
> for
> the seal to be closed.


I think it would be useful to classify systems w.r.t. what data is
available to condition.
I imagine it might be useful if status of other seals is available.


> Secondly, the contents of the proof will be able to
> commit to new data, such as the transaction spending the output associated
> with
> the seal.
>

So basically a "condition" returns that "new data", right?
If it commits to a data in a recognizable way, then it's practically a
function which yields a tuple (valid, new_data).
If an oracle doesn't care about data then you can convert it to a predicate
using a simple projection.
But from point of view of a client, it is a function which returns a tuple.

It might help if you describe a type of the condition function.

Some related work on UTXO-based smart contracts:

1. Typecoin described in the paper
"Peer-to-peer Affine Commitment using Bitcoin" Karl Crary and Michael J.
Sullivan Carnegie Mellon University PLDI ’15, Portland June 17, 2015

I don't see the paper in open access and I've lost my copy, but there are
slides: https://www.msully.net/stuff/typecoin-slides.pdf

The paper is written by programming language researchers, and thus use
fairly complex constructs.
The idea is to use the language of linear logic, but it's actually
implemented using type-oriented programming.
So, basically, they associate logical propositions with transaction
outputs. Transactions proof that output-propositions logically follow from
input-propositions.
The paper first describes as a colored coin kind of a system, where color
values are propositions/types.
But in the implementation part it became more like a metacoin, as it uses a
complete transaction history.
A setup with a trusted server is also mentioned.

The interesting thing about Typecoin is that a contract language is based
on logic, which makes it powerful and -- I guess -- analyzable. However,
the paper doesn't mention any performance details, and I guess it's not
good.
Another problem is that it looks very unusual to people who aren't used to
type-oriented programming.

2. Generic coins
Seeing how much Typecoin people had to struggle to describe a Bitcoin-style
system I decided to describe a generalized Bitcoin-style system, so it can
be easily referenced in research. Sadly all I got so far is a draft of an
introduction/definition sections:
https://github.com/chromaway/ngcccbase/wiki/gc

In the first section I described a transaction graph model which is
supposed to be general enough to describe any kind of a transaction graph
system with explicit dependencies and no "spooky action at distance". As it
turns out, any such system can be defined in terms of few predicate
functions, however, using these functions directly might be very
inefficient.

The next section introduces a coin-based model. A coin-based system can be
described using a single function called coin kernel which is applied to a
transaction and a list of input coinstates.
It is then described how to go from a coin-based model to a
transaction-graph model.
The reverse should also be possible if we add additional restrictions on a
transaction-graph model, it's probably enough to define that coin can be
spent only once. (Partial coin spends were described in Freimarkets.)

There is a fairly shitty prototype in Haskell:
https://github.com/baldmaster/ColorCoin

3. flexichains
This is a prototype done by me more recently, the interesting thing about
it is that it unifies account-based and UTXO-based models in a single model.

We first introduce a notion of record. A record can be of an arbitrary
type, the only restriction is that it must have a key which must be unique
within a system.
Then transaction model can be introduced using two function:
  txDependencies returns a list of keys of records transaction depends on
  applyTx takes a transaction and a list of records it depends on and
returns either a list of records or an error.

A list of records includes
 * new records which are created by a transaction
 * updated records will have the same key but different content

A simple account-based system can be implement using tuples (pubkey,
balance, last_update) as records.
In an UTXO-based system records are transaction output, and they should
include a spent flag. (Obviously, records with spent flag can be pruned.)
A system with custom smart contracts can be implemented by adding some sort
of a function or bytecode to records.

A Haskell prototype is here:
https://bitbucket.org/chromawallet/flexichains/src/21059080bed6?at=develop
(It's kinda broken and incomplete, though.)

-------------------------------------
There is no reason to use a timestamp beyond 4 bytes. Just let it overflow. If a blockchain is stopped for more than 2^31 seconds, it’s just dead.

> On 5 Dec 2016, at 19:58, Tom Zander via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> On Sunday, 4 December 2016 21:37:39 CET Hampus Sjöberg via bitcoin-dev 
> wrote:
>>> Also how about making timestamp 8 bytes?  2106 is coming up soon 
>> 
>> AFAICT this was fixed in this commit:
>> https://github.com/jl2012/bitcoin/commit/
> fa80b48bb4237b110ceffe11edc14c813
>> 0672cd2#diff-499d7ee7998a27095063ed7b4dd7c119R200
> 
> That commit hacks around it, a new block header fixes it. Subtle difference.
> 
> -- 
> Tom Zander
> Blog: https://zander.github.io
> Vlog: https://vimeo.com/channels/tomscryptochannel
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev




-------------------------------------
Hi Eric

> I haven't seen much discussion here on the rationale behind BIP 151. Apologies if I missed it. I'm trying to understand why libbitcoin (or any node) would want to support it.
> 
> I understand the use, when coupled with a yet-to-be-devised identity system, with Bloom filter features. Yet these features are client-server in nature. Libbitcoin (for example) supports client-server features on an independent port (and implements a variant of CurveCP for encryption and identity). My concern arises with application of identity to the P2P protocol (excluding Bloom filter features).
> 
> It seems to me that the desire to secure against the weaknesses of BF is being casually generalized to the P2P network. That generalization may actually weaken the security of the P2P protocol. One might consider the proper resolution is to move the BF features to a client-server protocol.
> 
> The BIP does not make a case for other scenarios, or contemplate the significant problems associated with key distribution in any identity system. Given that the BIP relies on identity, these considerations should be fully vetted before heading down another blind alley.


In my opinion, the question should be "why would you _not_ encrypt".


1) Transaction censorship
ISPs, WIFI provider or any other MITM, can holdback/censor unconfirmed
transactions. Regardless if you are a miner or a validation/wallet node.

2) Peer censorship
MITM can remove or add entries from a "addr" message.

3) Fingerprinting
ISPs or any other MITM can intercept/inject fingerprinting relevant
messages like "mempool" to analyze the bitcoin network.

4) SPV
For obvious reasons regarding BF (see BIP or above).

5) Goundwork for a "client-server" model over the P2P channel
Fee estimation, bloom-filters, or any other message type that requires
authentication.

I would not reduce BIP151 to only solve the BF privacy/censorship problem.

If we agree that censorship-resistance is one of the main properties of
Bitcoin, then we should definitively use a form of end-to-end encryption
between nodes. Built into the network layer.

There are plenty of other options to solve this problem. stunnel,
Bernsteins CurveCP, VPN, etc. which are available since years.
But the reality has shown that most bitcoin traffic is still unencrypted.
Example: IIRC non of the available SPV wallets can "speak" on of the
possible encryption techniques. Encrypting traffic below the application
layer is extremely hard to set up for non-experienced users.

On top of that, encryption allows us to drop the SHA256 checksum per p2p
message which should result in a better performance on the network layer
once BIP151 is deployed.

I agree that BIP151 _must_ be deployed together with an authentication
scheme (I'm working on that) to protect again MITM during encryption
initialization.

---
</jonas>


-------------------------------------
>> Additionally, BIP 111 (NODE_BLOOM service bit) has been implemented in Bitcoin
>> Core and derivatives; it is unclear if used by clients yet. Can developers of
>> such clients please comment and let me know: 1) if their software supports
>> this BIP already; 2) if not, do they intend to support it in the future?
>> If and only if there are any clients using this service bit already, I will
>> update BIP 111 to Final Status in 2 weeks also.
> 
> Multibit is adding detection of the NODE_BLOOM bit in the next 2-3 weeks.
> 
> SPV is kinda broken if the wallet doesnt do this detection. If your wallet connects only to nodes that dont support bloom filtering, the wallet never gets updates. We have had a spike in users reporting that their wallet isn't getting updated. To compound the problem, they rescan the blockchain and lose all of their transaction history. It has caused much panic among less technical users.
> 
> We believe that failing to detect the NODE_BLOOM bit is the culprit, although it is non-deterministic, so we aren't certain.
> 
> I imagine that other SPV wallets are having similar issues. BIP 111 really isnt optional at this point, so it should be marked final.

SPV Wallets should definitively update to respect NODE_BLOOM. Bloom
filtering is CPU and disk intense and some node operators have disabled
it (or will disabled it) because there is no direct p2p network-health
benefit.

SPV wallets should probably also make use of the new DNS seeder filter
option.
It is running at least on seed.bitcoin.sipa.be and
seed.bitcoin.jonasschnelli.ch.

The filter option allows SPV Wallets to only get nodes that signal
support for NODE_BLOOM.

The syntax is

   x<networkservice_flags_filter_as_int>.seed.bitcoin....

Example for NODE_NETWORK together with NODE_BLOOM
dig x5.seed.bitcoin.jonasschnelli.ch
(NETWORK = (1 << 0), NODE_BLOOM = (1 << 2)) = (bin00000101 = (int)5)


</jonas>


-------------------------------------
On Wednesday, August 24, 2016 1:47:08 PM Andreas Schildbach via bitcoin-dev 
wrote:
> FWIW, BIP44 also doesn't encode a seed birthday. This needed so that SPV
> wallets do not need to scan from the beginning of the blockchain.
> 
> That doesn't mean BIP44 could not be final. There are some wallets that
> interoperate on that standard and that's fine.

Right. The Status doesn't depend on whether it is a good idea or not, only 
whether or not people are de facto using it.

BIP 2's BIP Comments would have provided a place for Thomas and yourself to 
criticise the BIP, but unfortunately this was too controversial.

> I think BIP43 should be made final as well, if it isn't already.

BIP 43 merely advises other BIPs how they might do things, so it goes into the 
Draft->Active Status flow rather than Draft->Accepted->Final.

Luke


-------------------------------------
I'm currently working on a wallet called multiexplorer. You can check
it at https://multiexplorer.com/wallet

It supports all the BIPs, including the ones that lets you export and
import based on a 12 word mnemonic. This lets you easily import
addresses from one wallet to the next. For instance, you can
copy+paste your 12 word mnemonic from Coinbase CoPay into
Multiexplorer wallet and all of your address and transaction history
is imported (except CoPay doesn't support altcoins, so it will just be
your BTC balance that shows up). Its actually pretty cool, but not
everything is transferred over.

For instance, some people like to add little notes such as "paid sally
for lunch at Taco Bell", or "Paid rent" to each transaction they make
through their wallet's UI. When you export and import into another
wallet these memos are lost, as there is no way for this data to be
encoded into the mnemonic.

For my next project, I want to make a stand alone system for archiving
and serving these memos. After it's been built and every wallet
supports the system, you should be able to move from one wallet by
just copy+pasting the mnemonic into the next wallet without losing
your memos. This will make it easier for people to move off of old
wallets that may not be safe anymore, to more modern wallets with
better security features. Some people may want to switch wallets, but
since its much harder to backup memos, people may feel stuck using a
certain wallet. This is bad because it creates lock in.

I wrote up some details of how the system will work:

https://github.com/priestc/bips/blob/master/memo-server.mediawiki

Basically the memos are encrypted and then sent to a server where the
memo is stored. An API exists that allows wallets to get the memos
through an HTTPS interface. There isn't one single memo server, but
multiple memo servers all ran by different people. These memo servers
share data amongst each other through a sync process.

The specifics of how the memos will be encrypted have not been set in
stone yet. The memos will be publicly propagated, so it is important
that they are encrypted strongly. I'm not a cryptography expert, so
someone else has to decide on the scheme that is appropriate.


-------------------------------------
We can already warn users of a hardfork when a block is invalid (at
least) because of the highest bit in nVersion (as you say, because it
is forbidden since bip34 was deployed). It seems the softfork serves
only to warn about soft-hardforks, assuming it chooses to use this
mechanism (which a malicious soft hardfork may not do). In fact, you
could reuse another of the prohibited bits to signal a soft-hardfork
while distinguishing it from a regular hardfork. And this will also
serve for old nodes that have not upgraded to the softfork. But, wait,
if you signal a soft-hardfork with an invalid bit, it's not a
soft-hardfork anymore, is it? It's simply a hardfork.
Your softfork would result in soft hardforks being hardforks for nodes
that upgraded to this softfork, but softforks for older nodes.
Is this the intended behaviour? if so, why?

I would rather have a simpler BIP that doesn't require a softfork
(whether it recommends soft-hardforks to use one of the currently
invalid bits, but a different one than from hardforks or not, but I
also don't see the reason why soft-hardforks should appear as invalid
blocks for older nodes instead of using regular softfork warning
[besides, in this case, after the "unkown softfork" warning you will
get only empty blocks, which may make you suspicious]).


-------------------------------------
On 01/13/2016 12:37 AM, Jorge Timón wrote:
> On Tue, Jan 12, 2016 at 8:17 PM, Eric Voskuil <eric@voskuil.org> wrote:
>> Jorge, first, thanks again for your work on this.
>>
>> Without creating and using a public blockchain interface in phase 2, how
>> will you isolate the database dependency from consensus critical code?
>> Is it that the interface will exist but you will recommend against its use?
> 
> The interface will exist but it will be a C++ interface that fits
> better with Bitcoin Core internals.
> See an initial draft of what could be the storage interface:
> https://github.com/jtimon/bitcoin/blob/1717db89c6db17ea65ddbd5eb19034315db0b059/src/consensus/storage_interfaces_cpp.h
> 
> Phase 3 will consist on discussing and refining that interface to also
> define the C interfaces using structs of function pointers instead of
> classes (see https://github.com/jtimon/bitcoin/blob/2bcc07c014e5dd000030e666be047dfa11f54c10/src/consensus/interfaces.h
> for an early draft) that is needed for the "final" C API.
> But since I think there will be more discussion and work defining
> those interfaces, I would rather start with ANY interface that allows
> us to decouple the consensus code from chain.o and coins.o, which we
> don't want to be built as part of the consensus building package
> (which is used for building both libbitcoinconsensus and Bitcoin
> Core).

Okay.

> Future potential users are more than welcomed to draft their own C
> APIs and that experience should be useful for phase 3.
> I was expecting you, for example, to include the whole consensus code
> (even if it lacks a C API) in
> https://github.com/libbitcoin/libbitcoin-consensus for better testing
> of the equivalent code in libbitcoin. You are kind of taking the C API
> part out already, so this time you will just have less things to
> delete/ignore.

Generalization of the store interface may be more challenging than you
anticipate, but the objective makes sense.

>> This work presumes that the users of the library reject the argument
>> that the database implementation is consensus critical code. Faithful
>> reproduction of stored data is a prerequisite for a validity. But a
>> common store implementation is only slightly more reasonable for this
>> library than a common RAM implementation.
> 
> This is a concern that has been risen repeatedly.
> I am aware that faithful reproduction of the stored data is a
> prerequisite for consensus validity. On the other hand, my presumption
> is that a libbitcoinconsensus that forces its users to a given unifrom
> storage will likely had much less users and any alternative
> implementation that wants to implement its own custom storage would
> have to necessarily reimplement the consensus validation code.
> Doing it this way is more flexible. We can relatively easily implement
> another library (if I remember correctly, last time we talked about it
> we reffered to it as "libconsensus plus", but there's probably better
> names) also takes care of storage for the users that don't want to
> take the risks of reimplementing the storage (probably just using
> Bitcoin Core's structures).
> 
> Unlike me, Luke Dashjr, for example, advocated for the
> storage-dependent version, but I believe that implementing both
> versions was an acceptable solution to him.
> It is certainly an acceptable solution for me. I don't want to force
> anyone that doesn't want or need to take the risks reimplementing the
> consensus storage part to do so. But at the same time I really believe
> that it would be a mistake to not allow it optionally.
> 
> Does that make sense?

We would not offer libbitcoinconsensus integration if it required us to
incorporate the store. These are distinct logical components, as are p2p
networking and client-server networking (e.g. RPC), for example. I would
not think of these as multiple versions of libbitcoinconsensus but
instead as distinct components of a bitcoin node. It doesn't make sense
to me that you would ship this as two consensus variants. I would work
toward shipping independent component libraries (i.e. consensus and store).

e


-------------------------------------
Replies inline.

On May 10, 2016 5:23:55 PM EDT, Rusty Russell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>Gregory Maxwell <greg@xiph.org> writes:
>> On Tue, May 10, 2016 at 5:28 AM, Rusty Russell via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> I used variable-length bit encodings, and used the shortest encoding
>>> which is unique to you (including mempool).  It's a little more
>work,
>>> but for an average node transmitting a block with 1300 txs and
>another
>>> ~3000 in the mempool, you expect about 12 bits per transaction. 
>IOW,
>>> about 1/5 of your current size.  Critically, we might be able to fit
>in
>>> two or three TCP packets.
>>
>> Hm. 12 bits sounds very small even giving those figures. Why failure
>> rate were you targeting?
>
>That's a good question; I was assuming a best-case in which we have
>mempool set reconciliation (handwave) thus know they are close.  But
>there's also an alterior motive: any later more sophisticated approach
>will want variable-length IDs, and I'd like Matt to do the work :)

Yea, there's already an ongoing discussion of that, and the UDP stuff will definitely want something different than the current proposals.

>In particular, you can significantly narrow the possibilities for a
>block by sending the min-fee-per-kb and a list of "txs in my mempool
>which didn't get in" and "txs which did despite not making the
>fee-per-kb".  Those turn out to be tiny, and often make set
>reconciliation trivial.  That's best done with variable-length IDs.
>
>> (*Not interesting because it mostly reduces exposure to loss and the
>> gods of TCP, but since those are the long poles in the latency tent,
>> it's best to escape them entirely, see Matt's udp_wip branch.)
>
>I'm not convinced on UDP; it always looks impressive, but then ends up
>reimplementing TCP in practice.  We should be well within a TCP window
>for these, so it's hard to see where we'd win.

Not at all. The goal with the UDP stuff I've been working on is not to provide reliable transport. Like the relay network, it is assumed some percent of blocks will fail to transit properly, and you will use some other transport to figure out how to get the block. Indeed, a big part of my desire for diversity in network protocols is to enable them to make tradeoffs in reliability/privacy/etc.

>>> I would also avoid the nonce to save recalculating for each node,
>and
>>> instead define an id as:
>>
>> Doing this would greatly increase the cost of a collision though, as
>> it would happen in many places in the network at once over the on the
>> network at once, rather than just happening on a single link, thus
>> hardly impacting overall propagation.
>
>"Greatly increase"?  I don't see that.
>
>Let's assume an attacker grinds out 10,000 txs with 128 bits of the
>same
>TXID, and gets them all in a block.  They then win the lottery and get
>a
>collision.  Now we have to transmit ~48 bytes more than expected.

I assume what Greg was referring to the idea that if there is a conflict, a given block will require an extra round trip when being broadcast between roughly each peer, compounding the effect across each hop.

>> Using the same nonce means you also would not get a recovery gain
>from
>> jointly decoding using compact blocks sent from multiple peers (which
>> you'll have anyways in high bandwidth mode).
>
>Not quite true, since if their mempools differ they'll use different
>encoding lengths, but yes, you'll get less of this.

... Assuming different encoding lengths aren't just truncated, but ok :).

>> With a nonce a sender does have the option of reusing what they got--
>> but the actual encoding cost is negligible, for a 2500 transaction
>> block its 27 microseconds (once per block, shared across all peers)
>> using Pieter's suggestion of siphash 1-3 instead of the cheaper
>> construct in the current draft.
>>
>> Of course, if you're going to check your whole mempool to reroll the
>> nonce, thats another matter-- but that seems wasteful compared to
>just
>> using a table driven size with a known negligible failure rate.
>
>I'm not worried about the sender: The recipient needs to encode all the
>mempool.
>
>>> As Peter R points out, we could later enhance receiver to brute
>force
>>> collisions (you could speed that by sending a XOR of all the txids,
>but
>>> really if there are more than a few collisions, give up).
>>
>> The band between "no collisions" and "infeasible many" is fairly
>> narrow.  You can add a small amount more space to the ids and
>> immediately be in the no collision zone.
>
>Indeed, I would be adding extra bits in the sender and not implementing
>brute force in the receiver.  But I welcome someone else to do so.
>
>Cheers,
>Rusty.
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev@lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
Please use ASCII quotes in the Title. It is also too long (max size 44 
characters).

Add missing headers:
  Layer: Consensus (hard fork)
  Comments-Summary: No comments yet.
  Comments-URI: TBD
  Status: Draft
  Type: Standards Track
  License: PD

It must be made at least technically sound. The BIP talks about adjusting the 
maximum block size, but the specification only affects MAX_BLOCK_BASE_SIZE, 
which does not actually affect the max block size at all. Either the 
specification needs to implement the described goal (adjusting max block size) 
or the motivation needs to be adjusted to explain why MAX_BLOCK_BASE_SIZE is 
being adjusted.

It is missing a section on Backward Compatibility. This should address at 
least the fact that this is *NOT* backward compatible, and ideally propose a 
mechanism for establishing agreement from the entire community for its 
deployment. Similarly, there is talk of 75%, but the algorithm presented does 
not in fact implement 75%.

Finally, I am about to set BIP 2 to Active, so it would be preferable to 
choose a copyright license from the choices in BIP 2.

When you're ready, feel free to open a pull request on 
https://github.com/bitcoin/bips/ with the BIP in mediawiki format, named:
    bip-tkhan-block75.mediawiki

Thanks,

Luke


On Wednesday, December 14, 2016 7:55:20 PM t. khan wrote:
> Hi Luke,
> 
> Following is a BIP for submission. Please let me know if any
> modifications/additions are required.
> 
> Thank you,
> 
> -t.k.
> 
> --------
> 
> BIP: ??
> Title: ‘Block75’ - Managing max block size the same way we do difficulty
> Author: t.khan <teekhan42@gmail.com>
> Created: 2016-12-13
> 
> 
> Abstract
> Automatic adjustment of max block size with the target of keeping blocks
> 75% full, based on the average block size of the previous 2016 blocks. This
> would be done on the same schedule as difficulty.
> 
> 
> Motivation
> The every two-week and automatic adjustment of difficulty has proven to be
> a reasonably effective and predictable way of managing how quickly blocks
> are mined. It works well because humans aren’t involved,  except for
> setting the original target of a 10 minute per block average, and therefore
> it isn’t political or contentious. It’s simply a response to changing
> network resources.
> 
> Bitcoin needs a reasonably effective and predictable way of managing the
> maximum block size, which both allows moderate growth and keeps max block
> size as small as possible, thereby preventing wild swings in max block size
> or transaction fees.
> 
> It’s clear at this point that human beings should not be involved in the
> determination of max block size, just as they’re not involved in deciding
> the difficulty. Any solution to block size scaling which sets an arbitrary
> max block size (1MB, 2MB, 8MB, etc.) is by its very design a temporary
> solution and should be avoided. Any solution which passes the decision on
> to miners/pool operators for a vote will, by its very design, be political
> and contentious and should be avoided.
> 
> A permanent solution that is simply a response to changing transaction
> volumes is the goal of Block75.
> 
> 
> Specification
> The max block size would be recalculated every 2016 blocks, along with
> difficulty, using this simple formula:
> 
> new max block size = 1,000KB + (average size of last 2016 blocks - 750KB)
> 
> Further details:
> 
> MAX_BLOCK_BASE_SIZE = 1000000 //this line stays the same
> TARGET_CAPACITY = 750000 //new line representing 75%
> 
> AVERAGE_OVER_CAP = average block size of last 2016 blocks minus
> TARGET_CAPACITY
> 
> To check if a block is valid, ≤ (MAX_BLOCK_BASE_SIZE + AVERAGE_OVER_CAP)
> 
> 
> Rationale
> The 75% full block target was selected as it represents the middle ground
> between blocks being too small (average 100% full) and blocks being
> unnecessarily large (average 50% full). It can absorb short-term spikes in
> transaction volume of up to 33% and also limits the growth of max block
> size to 250KB over the previous period.
> 
> The 2016 blocks (two week) period was selected as it has been shown to be
> reasonably adaptive to changing network resources. The frequent and gradual
> adjustments that result from this will be relatively easy for miners and
> node operators to predict and adapt to, as any unforeseen consequences will
> be visible well in advance. It also minimizes any effect a malicious party
> could have in an attempt to manipulate block size.
> 
> Copyright
> This work is placed in the public domain.


-------------------------------------
>From my experience working with coin selection algorithms, there are
three "goals" to it:

1. Minimize cost
2. Maximize privacy
3. Minimize UTXO footprint

You can build a coin selection algorithm that achieves 1 and 3, but
will sacrifice 2. If you want coin selectin to maximize your privacy,
it will happen at the expense of UTXO footprint and fees. Minimizing
cost usually also minimizes UTXO footprint but not always. To
completely minimize UTXO footprint, you sacrifice a bit on cost, and a
lot on privacy.

On 9/21/16, Andreas Schildbach via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On 09/21/2016 02:58 PM, Murch via bitcoin-dev wrote:
>
>> Android Wallet for Bitcoin
>
> The correct name is Bitcoin Wallet, or Bitcoin Wallet for Android (if
> you want to refer to the Android version).
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
I'm hoisting this from some private feedback I sent on the segregated
witness BIP:

I said:

"I'd also use RIPEMD160(SHA256()) as the hash function and save the 12
bytes-- a successful preimage attack against that ain't gonna happen before
we're all dead. I'm probably being dense, but I just don't see how a
collision attack is relevant here."

Pieter responded:

"The problem case is where someone in a contract setup shows you a script,
which you accept as being a payment to yourself. An attacker could use a
collision attack to construct scripts with identical hashes, only one of
which does have the property you want, and steal coins.

So you really want collision security, and I don't think 80 bits is
something we should encourage for that. Normal pubkey hashes don't have
that problem, as they can't be constructed to pay to you."
... but I'm unconvinced:

"But it is trivial for contract wallets to protect against collision
attacks-- if you give me a script that is "gavin_pubkey CHECKSIG
arbitrary_data OP_DROP" with "I promise I'm not trying to rip you off, just
ignore that arbitrary data" a wallet can just refuse. Even more likely, a
contract wallet won't even recognize that as a pay-to-gavin transaction.

I suppose it could be looking for some form of "gavin_pubkey
somebody_else_pubkey CHECKMULTISIG ... with the attacker using
somebody_else_pubkey to force the collision, but, again, trivial contract
protocol tweaks ("send along a proof you have the private key corresponding
to the public key" or "everybody pre-commits pubkeys they'll use at
protocol start") would protect against that.

Adding an extra 12 bytes to every segwit to prevent an attack that takes
2^80 computation and 2^80 storage, is unlikely to be a problem in practice,
and is trivial to protect against is the wrong tradeoff to make."

20 bytes instead of 32 bytes is a savings of almost 40%, which is
significant.

The general question I'd like to raise on this list is:

Should we be worried, today, about collision attacks against RIPEMD160 (our
160-bit hash)?

Mounting a successful brute-force collision attack would require at least
O(2^80) CPU, which is kinda-sorta feasible (Pieter pointed out that Bitcoin
POW has computed more SHA256 hashes than that). But it also requires
O(2^80) storage, which is utterly infeasible (there is something on the
order of 2^35 bytes of storage in the entire world).  Even assuming
doubling every single year (faster than Moore's Law), we're four decades
away from an attacker with THE ENTIRE WORLD's storage capacity being able
to mount a collision attack.


References:

https://en.wikipedia.org/wiki/Collision_attack

https://vsatglobalseriesblog.wordpress.com/2013/06/21/in-2013-the-amount-of-data-generated-worldwide-will-reach-four-zettabytes/


-- 
--
Gavin Andresen

-------------------------------------
On Sun, Oct 16, 2016 at 10:58 AM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> The fallow period sounds waaaay to short. I suggest 2 months at minimum
> since anyone that wants to be safe needs to upgrade.
>

I asked a lot of businesses and individuals how long it would take them to
upgrade to a new release over the last year or two.

Nobody said it would take them more than two weeks.

If somebody is running their own validation code... then we should assume
they're sophisticated enough to figure out how to mitigate any risks
associated with segwit activation on their own.

-- 
--
Gavin Andresen

-------------------------------------
An alternative soft fork would be to require that miners pay some of the
coinbase to a CLTV locked output (that is otherwise unlocked).  This allows
the release of the funds to be delayed.

-------------------------------------


> On Jun 30, 2016, at 6:52 PM, Peter Todd <pete@petertodd.org> wrote:
> 
>> On Thu, Jun 30, 2016 at 05:22:08PM +0200, Eric Voskuil via bitcoin-dev wrote:
>> 
>>> On Jun 30, 2016, at 2:43 PM, Jonas Schnelli <dev@jonasschnelli.ch> wrote:
>>> 
>>>>>> The core problem posed by BIP151 is a MITM attack. The implied solution (BIP151 + authentication) requires that a peer trusts that another is not an attacker.
>>>>> 
>>>>> BIP151 would increase the risks for MITM attackers.
>>>>> What are the benefits for Mallory of he can't be sure Alice and Bob may
>>>>> know that he is intercepting the channel?
>>>> 
>>>> It is not clear to me why you believe an attack on privacy by an anonymous peer is detectable.
>>> 
>>> If Mallory has substituted the ephemeral keys in both directions, at the
>>> point where Alice and Bob will do an authentication, they can be sure
>>> Mallory is listening.
>> 
>> I understand the mechanics of a tunnel between trusting parties that have a secure side channel. But this assumes that no other peer can connect to these two nodes. How then do they maintain the chain?
>> 
>> The "middle" in this sense does not have to be the wire directly between these two peers. It can be between either of them and any anonymous connection they (must) allow.
>> 
>> Of course this creates pressure to expand their tunnel. Hence the problem of expanding node identity in an effort to preserve privacy. The protection will remain weak until the entire network is "secure". At that point it would necessarily be a private network.
>> 
>> As Pieter rightly observes, there are and always will be tunnels between trusting nodes. Often these are groups of nodes that are in collaboration, so logically they are one node from a system security standpoint. But if people become generally reliant on good node registration, it will become the registrar who controls access to the network. So my concern rests I this proposal becoming widely adopted.
> 
> To be clear, are you against Bitcoin Core's tor support?
> 
> Because node-to-node connections over tor are encrypted, and make use of onion
> addresses, which are self-authenticated in the exact same way as BIP151 proposes.

BIP151 is self-admittedly insufficient to protect against a MITM attack. It proposes node identity to close this hole (future BIP required). The yet-to-be-specified requirement for node identity is the basis of my primary concern. This is not self-authentication.

> And we're shipping that in production as of 0.12.0, and by default Tor onion support is enabled and will be automatically setup if you have a recent version of Tor installed.
> 
> Does that "create pressure to expand node identity"?

The orthogonal question of whether Tor is safe for use with the Bitcoin P2P protocol is a matter of existing research.

e

-------------------------------------
> Only large merchants are able to maintain such an infrastructure; (even
> Coinbase recently failed at it, they forgot to update their
> certificate). For end users that is completely unpractical.
>

Payment protocol is for when you buy stuff from purse.io, not really needed
for face-to face transfers, end users, IMO.


> The same benefit can be achieved without the complexity of BIP70, by
> extending the Bitcoin URI scheme. The requestor is authenticated using
> DNSSEC, and the payment request is signed using an EC private key. A
> domain name and an EC signature are short enough to fit in a Bitcoin URI
> and to be shared by QR code or SMS text.
>
>  bitcoin:address?amount=xx&message=yyy&name=john.example.com&sig=zzz
>

I agree.  A TXT record at that name could contain the pubkey.


> That extension is sufficient to provide authenticated requests, without
> requiring a https server. The signed data can be serialized from the
> URI, and DNSSEC verification succeeds without requesting extra data from
> the requestor. The only assumption is that the verifier is able to make
> DNS requests.
>

The problem is that there's no way for a merchant to *refuse *a payment
without a direct communication with the merchant's server.    Verify first
/ clear later is the rule.   Check stock, ensure you can deliver, and clear
the payment on the way out the door.

Also, as a merchant processing monthly subscriptions, you don't want the
first time you hear about a user's payment to be *after *it hits the
blockchain.  You could add a refund address to deal with it after the
fact... stuff a refund address int OP_RETURN somehow?

bitcoin:address?amount=xx&currency=ccc&message=yyy&name=john.example.com
&offset=3d&interval=1m&sig=zzz

... But what if the merchant simply goes out of business.  No OP_RETURN
will help you here.   You'll be posting transactions into a dead wallet.
You could have some way of posting a "ping" transaction, and then
monitoring for a valid response.   But this is "spamming the blockchain for
communications".

No, I think BIP075 is fine.   You just need to extend the *PaymentAck *with
a single field, instead of just having a memo.

next_payment_days : integer

The wallet, when it sees this field, re-initiates an invoice request after
the selected number of days, after presenting the user with the content of
the memo field which will presumably explain the subscription.   Wallet
vendors can let users "auto approve" vendors as needed.

This is, I think, the absolute minimum needed to update BIP0070/0075 for
subscriptions.

-------------------------------------
As for your stages idea, I generally like the idea (and mentioned it may
be a good idea in my proposal), but am worried about scheduling two
hard-forks at once....Lets do our first hard-fork first with the things
we think we will need anytime in the visible future that we have
reasonable designs for now, and talk about a second one after we've seen
what did/didnt blow up with the first one.

Anyway, this generally seems reasonable - it looks like most of this
matches up with what I said more specifically in my mail yesterday, with
the addition of timewarp fixes, which we should probably add, and Luke's
header changes, which I need to spend some more time thinking about.

Matt

On 02/09/16 14:16, jl2012--- via bitcoin-dev wrote:
> I would like to present a 2-3 year roadmap to a better header format and
> bigger block size
> 
> Objectives:
> 
> 1. Multistage rule changes to make sure everyone will have enough time to
> upgrade
> 2. Make mining easier, without breaking existing mining hardware and the
> Stratum protocol
> 3. Make future hardfork less disruptive (with Luke-Jr's proposal)
> 
> Stage 1 is Segregated Witness (BIP141), which will not break any existing
> full or light nodes. This may happen in Q2-Q3 2016
> 
> Stage 2 is fixes that will break existing full nodes, but not light nodes:
> a. Increase the MAX_BLOCK_SIZE (the exact value is not suggested in this
> roadmap), potentially change the witness discount
> b. Anti-DoS rules for the O(n^2) validation of non-segwit scripts
> c. (optional) Move segwit's commitments to the header Merkle tree. This is
> optional at this stage as it will be fixed in Stage 3 anyway
> This may happen in Q1-Q2 2017
> 
> Stage 3 is fixes that will break all existing full nodes and light nodes:
> a. Full nodes upgraded to Stage 2 will not need to upgrade again, as the
> rules and activation logic should be included already
> b. Change the header format to Luke-Jr's proposal, and move all commitments
> (tx, witness, etc) to the new structure. All existing mining hardware with
> Stratum protocol should work.
> c. Reclaiming unused bits in header for mining. All existing mining chips
> should still work. Newly designed chips should be ready for the new rule.
> d. Fix the time warp attack
> This may happen in 2018 to 2019
> 
> Pros:
> a. Light nodes (usually less tech-savvy users) will have longer time to
> upgrade
> b. The stage 2 is opt-in for full nodes.
> c. The stage 3 is opt-in for light nodes.
> 
> Cons:
> a. The stage 2 is not opt-in for light nodes. They will blindly follow the
> longest chain which they might actually don't want to
> b. Non-upgraded full nodes will follow the old chain at Stage 2, which is
> likely to have lower value.
> c. Non-upgraded light nodes will follow the old chain at Stage 3, which is
> likely to have lower value. (However, this is not a concern as no one should
> be mining on the old chain at that time)
> 
> -------------------------------
> An alternative roadmap would be:
> 
> Stage 2 is fixes that will break existing full nodes and light nodes.
> However, they will not follow the minority chain
> a. Increase the MAX_BLOCK_SIZE, potentially change the witness discount
> b. Anti-DoS rules for the O(n^2) validation of non-segwit scripts
> c. Change the header format to Luke-Jr's proposal, and move all commitments
> (tx, witness, etc) to the new structure.
> This may happen in mid 2017 or later
> 
> Stage 3 is fixes that will break all existing full nodes and light nodes. 
> a. Full nodes and light nodes upgraded to Stage 2 will not need to upgrade
> again, as the rules and activation logic should be included already
> b. Reclaiming unused bits in header for mining. All existing mining chips
> should still work.
> c. Fix the time warp attack
> This may happen in 2018 to 2019
> 
> Pros:
> a. The stage 2 and 3 are opt-in for everyone
> b. Even failing to upgrade, full nodes and light nodes won't follow the
> minority chain at stage 2
> 
> Cons:
> a. Non-upgraded full/light nodes will follow the old chain at Stage 3, which
> is likely to have lower value. (However, this is not a concern as no one
> should be mining on the old chain at that time)
> b. It takes longer to implement stage 2 to give enough time for light node
> users to upgrade
> 
> -------------------------------
> 
> In terms of safety, the second proposal is better. In terms of disruption,
> the first proposal is less disruptive
> 
> I would also like to emphasize that it is miners' responsibility, not the
> devs', to confirm that the supermajority of the community accept changes in
> Stage 2 and 3.
> 
> Reference:
> Matt Corallo's proposal:
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012403.
> html
> Luke-Jr's proposal:
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012377.
> html
> 
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
>  I love seeing data!  I was considering 0.10 nodes as 'unmaintained'
because it has been a long time since the 0.11 release.

https://packages.gentoo.org/packages/net-p2p/bitcoin-qt

The Gentoo package manager still has 0.10.2 as the most recent stable
version. Getting a later version of the software on a gentoo setup requires
explicitly telling the package manger to grab a later version. I don't know
what percent of nodes are Gentoo 0.10.2, but I think it's evidence that
0.10 should not be considered 'unmaintained'. People who update their
software regularly will be running 0.10 on Gentoo.

> many of whom have privately told me they are willing and able to run an
extra node or three (or a hundred-and-eleven) once there is a final release.

I'm not clear on the utility of more nodes. Perhaps there is significant
concern about SPV nodes getting enough bandwidth or the network struggling
from the load? Generally though, I believe that when people talk about the
deteriorating full node count they are talking about a reduction in
decentralization. Full nodes are a weak indicator of how likely something
like a change in consensus rules is to get caught, or how many people you
would need to open communication with / extort in order to be able to force
rules upon the network. Having a person spin up multiple nodes doesn't
address either of those concerns, which in my understanding is what most
people care about. My personal concern is with the percentage of the
economy that is dependent on trusting the full nodes they are connected to,
and the overall integrity of that trust. (IE how likely is it that my SPV
node is going to lie to me about whether or not I've received a payment).

I will also point out that lots of people will promise things when they are
seeking political change. I don't know what percentage of promised nodes
would actually be spun up, but I'm guessing that it's going to be
significantly less than 100%. I have similar fears for companies that claim
they have tested their infrastructure for supporting 2MB blocks. Talk is
cheap.

-------------------------------------
On Thu, Nov 17, 2016 at 12:10 AM, Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Both of these cases resulted from exact duplicate txs, which BIP34 now
> precludes. However nothing precludes different txs from having the same
> hash.
>

The only way to have two transactions have the same txid is if their
parents are identical, since the txids of the parents are included in a
transaction.

Coinbases have no parents, so it used to be possible for two of them to be
identical.

Duplicate outputs weren't possible in the database, so the later coinbase
transaction effectively overwrote the earlier one.

The happened for two coinbases.  That is what the exceptions are for.

Neither of the those coinbases were spent before the overwrite happened.  I
don't even think those coinbases were spent at all.

This means that every activate coinbase transaction has a unique hash and
all new coinbases will be unique.

This means that all future transactions will have different txids.

There might not be an explicit rule that says that txids have to be unique,
but barring a break of the hash function, they rules do guarantee it.

-------------------------------------

> Le 25/08/2016  09:39, Jonas Schnelli via bitcoin-dev a crit :
>> (I think this case if not completely unrealistic):
>>
>> What would happen, if a user gave out 21 addresses, then address0 had
>> receive funds in +180 days after generation where address21 had receive
>> funds immediately (all other addresses never received a tx).
>>
>> In a scan, address0 would be detected at <address-birthday>+180 days
>> which would trigger the resize+20 of the address-lookup-window, but, we
>> would require to go back 180day in order to detect received transaction
>> of address21 (new lookup-window) in that case.
>>
>> Or do I misunderstand something?
>>
>>
> 
> That case is not unrealistic; a merchant might generate addresses that
> are beyond their gap limit, and orders get filled at a later date.
> 
> In that case you will not get the same result when restoring your wallet
> in a block-scanning wallet and in Electrum.
> 
> The lack of consideration for these cases is another issue with BIP44.

The development paradigm of "maybe detect funds" is not something we
should *not* encourage for Bitcoin IMO.

Users might sweep their existing bip32/bip44 seed (which could miss
funds according to the problem above) to a new wallet and discard the
previous seed.

But I agree with Luke-Jr.
This Thread is not about specification, it's about what's used and what
should be marked as standard.

New BIPs could cover "overhauled" proposals for BIP39 and BIP44.
Otherwise  very likely  nothing will happen.

</jonas>


-------------------------------------
> > To enable block pruning set prune=<N> on the command line or in
> > bitcoin.conf, where N is the number of MiB to allot for raw block & undo
> > data.
> 

> From having read the Bitcoin whitepaper quite a few months ago ago, I have the 
> very very basic understanding that pruning is meant to:
> - delete old transaction data which merely "moves coins around"
> - instead only store the "origin" (= block where coins were mined) and 
> "current location" of the coins, i.e. the unspent transactions. Notably, I 
> understood it as "this is as secure as storing everything, since we know where 
> the coins were created, and where they are".
> 
> So from that point of view, I would assume that there is a "natural" amount of 
> megabytes which a fully pruned blockchain consists of: It would be defined by 
> the final amount of unspent coins.
> I thereby am confused why it is possible to configure a number of megabytes 
> "to allot for raw block & undo data". I would rather expect pruning just to be 
> a boolean on/off flag, and the number of megabytes to be an automatically 
> computed result from the natural size of the dataset.
> And especially, I fear that I could set N too low, and as a result, it would 
> delete "too much". I mean could this result in even security relevant 
> transaction data being deleted?

The term 'pruning', unfortunately is very much overused and overloaded in the
bitcoin ecosystem. Satoshi's paper refers to UTXO pruning. This is Pieter Wuille's "ultraprune",
which has been part of Bitcoin Core for more than three years.

Block pruning is not mentioned in that paper because it is just administrative,
the only reason that nodes store historical blocks at all is to serve it to other nodes,
as well as to catch up the wallet status and for -reindexes.

> Thus, it would be nice if you could yet once more edit the release notes to:

I don't have time to work on the release notes right now, but if someone else
wants to contribute that'd be awesome.

> - explain why a N must be given

To give a quotum. The point is that the user can choose how much harddisk space they want to
dedicate to block storage.

Block data that is stored can be used by other software, or potentially be
served to other nodes. The latter is not implemented at the moment - it would require
a change to the P2P protocol, thus right now pruning nodes don't serve block
data at all.

> - what a "safe" value of N is. I.e. how large must N be at least to not delete 
> security-relevant stuff?

There is no security compromise with pruning. Any value of N is intended to be safe.

Very low values would delete undo data that may be necessary in a reorganization,
but this is prohibited by argument checks.

Release notes are not meant as a replacement or supplement for documentation.
The documentation for -prune is this:

  -prune=<n>
       Reduce storage requirements by pruning (deleting) old blocks. This mode
       is incompatible with -txindex and -rescan. Warning: Reverting this
       setting requires re-downloading the entire blockchain. (default: 0 =
       disable pruning blocks, >550 = target size in MiB to use for block
       files)

> - maybe mention if there is a "auto" setting for N to ensure that it choses a 
> safe value on its own?

As said, there is no safe or unsafe value. The lowest acceptable value is just as safe
as storing everything.

Wladimir



-------------------------------------
> Again, my comments above about issues with using bitcoin: URI for
everything. Also, why do you want to bloat the blockchain with unnecessary
refund transaction data?

I don't, sorry -  I was just kind of thinking out loud and explaining what
happens when you stuff that into a URL.

My conclusion at the bottom of that post was to keep BIP 75 the same, don't
change a bit, and stick any subscription information (future payment
schedule) in the PaymentACK.   Then the wallet then re-initiates an invoice
(unattended or attended.. up to the user), after the subscription interval
is passed.  Subscriptions are pretty important for Bitcoin to be used as a
real payment system.


​

-------------------------------------
I believe i've seen Luke say this several times before, but there are
several more things that the majority of the devs agree should be in
bitcoin.
I would suggest to compile that list for your stage 3, so that you can have
an hardfork that fixes most of those things, and there should be some
repository with those changes deployed.

2016-02-09 14:16 GMT+00:00 jl2012--- via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

> I would like to present a 2-3 year roadmap to a better header format and
> bigger block size
>
> Objectives:
>
> 1. Multistage rule changes to make sure everyone will have enough time to
> upgrade
> 2. Make mining easier, without breaking existing mining hardware and the
> Stratum protocol
> 3. Make future hardfork less disruptive (with Luke-Jr's proposal)
>
> Stage 1 is Segregated Witness (BIP141), which will not break any existing
> full or light nodes. This may happen in Q2-Q3 2016
>
> Stage 2 is fixes that will break existing full nodes, but not light nodes:
> a. Increase the MAX_BLOCK_SIZE (the exact value is not suggested in this
> roadmap), potentially change the witness discount
> b. Anti-DoS rules for the O(n^2) validation of non-segwit scripts
> c. (optional) Move segwit's commitments to the header Merkle tree. This is
> optional at this stage as it will be fixed in Stage 3 anyway
> This may happen in Q1-Q2 2017
>
> Stage 3 is fixes that will break all existing full nodes and light nodes:
> a. Full nodes upgraded to Stage 2 will not need to upgrade again, as the
> rules and activation logic should be included already
> b. Change the header format to Luke-Jr's proposal, and move all commitments
> (tx, witness, etc) to the new structure. All existing mining hardware with
> Stratum protocol should work.
> c. Reclaiming unused bits in header for mining. All existing mining chips
> should still work. Newly designed chips should be ready for the new rule.
> d. Fix the time warp attack
> This may happen in 2018 to 2019
>
> Pros:
> a. Light nodes (usually less tech-savvy users) will have longer time to
> upgrade
> b. The stage 2 is opt-in for full nodes.
> c. The stage 3 is opt-in for light nodes.
>
> Cons:
> a. The stage 2 is not opt-in for light nodes. They will blindly follow the
> longest chain which they might actually don't want to
> b. Non-upgraded full nodes will follow the old chain at Stage 2, which is
> likely to have lower value.
> c. Non-upgraded light nodes will follow the old chain at Stage 3, which is
> likely to have lower value. (However, this is not a concern as no one
> should
> be mining on the old chain at that time)
>
> -------------------------------
> An alternative roadmap would be:
>
> Stage 2 is fixes that will break existing full nodes and light nodes.
> However, they will not follow the minority chain
> a. Increase the MAX_BLOCK_SIZE, potentially change the witness discount
> b. Anti-DoS rules for the O(n^2) validation of non-segwit scripts
> c. Change the header format to Luke-Jr's proposal, and move all commitments
> (tx, witness, etc) to the new structure.
> This may happen in mid 2017 or later
>
> Stage 3 is fixes that will break all existing full nodes and light nodes.
> a. Full nodes and light nodes upgraded to Stage 2 will not need to upgrade
> again, as the rules and activation logic should be included already
> b. Reclaiming unused bits in header for mining. All existing mining chips
> should still work.
> c. Fix the time warp attack
> This may happen in 2018 to 2019
>
> Pros:
> a. The stage 2 and 3 are opt-in for everyone
> b. Even failing to upgrade, full nodes and light nodes won't follow the
> minority chain at stage 2
>
> Cons:
> a. Non-upgraded full/light nodes will follow the old chain at Stage 3,
> which
> is likely to have lower value. (However, this is not a concern as no one
> should be mining on the old chain at that time)
> b. It takes longer to implement stage 2 to give enough time for light node
> users to upgrade
>
> -------------------------------
>
> In terms of safety, the second proposal is better. In terms of disruption,
> the first proposal is less disruptive
>
> I would also like to emphasize that it is miners' responsibility, not the
> devs', to confirm that the supermajority of the community accept changes in
> Stage 2 and 3.
>
> Reference:
> Matt Corallo's proposal:
>
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012403.
> html
> Luke-Jr's proposal:
>
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012377.
> html
>
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
BIP: TBD
Title: Best Practices for Heterogeneous Input Script Transactions
Author: Kristov Atlas  <kristov@openbitcoinprivacyproject.org>
Status: Draft
Type: Informational
Created: 2016-02-10

# Abstract

The privacy of Bitcoin users with respect to graph analysis is reduced when
a transaction is created that merges inputs composed from different
scripts. However, creating such transactions is often unavoidable.

This document proposes a set of best practice guidelines which minimize the
adverse privacy consequences of such unavoidable transaction situations
while simultaneously maximising the effectiveness of privacy-improving
techniques such as CoinJoin.

# Copyright

This BIP is in the public domain.

# Definitions

Heterogenous input script transaction (HIT): A transaction containing
multiple inputs where not all inputs have identical scripts (e.g. a
transaction spending from more than one Bitcoin address)
Unavoidable heterogenous input script transaction: An HIT created as a
result of a user’s desire to create a new output with a value larger than
the value of his wallet's largest unspent output
Intentional heterogenous input script transaction: An HIT created as part
of a protocol for improving user privacy against graph analysis, such as
CoinJoin

# Motivations

The recommendations in this document are designed to accomplish three goals:

1. Maximise the effectiveness of privacy-enhancing transactions:
Privacy-sensitive users may find that techniques such as CoinJoin are
counterproductive if such transactions have a distinctive fingerprint which
enables them to be censored or subjected to additional scrutiny.
2. Minimise the adverse privacy consequences of unavoidable heterogenous
input transactions: If unavoidable HITs are indistinguishable from
intentional HITs, a user creating an unavoidable HIT benefits from
ambiguity with respect to graph analysis.
3. Limiting the effect on UTXO set growth: To date, non-standardized
intentional HITs tend to increase the network's UTXO set with each
transaction; this standard attempts to minimize this effect by
standardizing unavoidable and intentional HITs to limit UTXO set growth.
In order to achieve these goals, this specification proposes a set of best
practices for heterogenous input script transaction creation. These
practices accommodate all applicable requirements of both intentional and
unavoidable HITs and render the two types indistinguishable to the maximum
extent possible.
In order to achieve this, two forms of HIT are proposed: Standard form and
alternate form.

# Standard form heterogenous input script transaction

## Rules

An HIT is Standard form if it adheres to all of the following rules:

1. The number of unique output scripts must be equal to the number of
unique inputs scripts (irrespective of the number of inputs and outputs).
2. All output scripts must be unique.
3. At least one pair of outputs must be of equal value.
4. The largest output in the transaction is a member of a set containing at
least two identically-sized outputs.

## Rationale

The requirement for equal numbers of unique input/output scripts instead of
equal number of inputs/outputs accommodates privacy-enhancing UTXO
selection behavior. Wallets may contain spendable outputs with identical
scripts due to intentional or accidental address reuse, or due to dusting
attacks. In order to minimise the adverse privacy consequences of address
reuse, any time a UTXO is included in a transaction as an input, all UTXOs
with the same spending script should also be included in the transaction.

The requirement that all output scripts are unique prevents address reuse.
Restricting the number of outputs to the number of unique input scripts
prevents this policy from growing the network’s UTXO set. A standard form
HIT transaction will always have a number of inputs greater than or equal
to the number of outputs.

The requirement for at least one pair of outputs to be of equal value
results in optimal join transactions, and causes intentional HITs to
resemble unavoidable HITs.

Outside controlled laboratory conditions, join transactions will not
involve identically-sized inputs due to a lack of accommodating volume.
Without the ability to cryptographically blind output values on the
blockchain, every join transaction leaks information in the form of output
sizes. Requiring a pair of identically sized outputs creates the desired
ambiguity for spend outputs, but in most cases makes change outputs
linkable to inputs.

# Alternate form heterogenous input script transactions

The formation of a standard form HIT is not possible in the following cases:

The HIT is unavoidable, and the user’s wallet contains an insufficient
number or size of UTXOs to create a standard form HIT.
The user wishes to reduce the number of utxos in their wallet, and does not
have any sets of utxos with identical scripts.
When one of the following cases exist, a compliant implementation may
create an alternate form HIT by constructing a transaction as follows:

## Procedure

1. Find the smallest combination of inputs whose value is at least the
value of the desired spend.
  i. Add these inputs to the transaction.
  ii. Add a spend output to the transaction.
  iii. Add a change output to the transaction containing the difference
between the current set of inputs and the desired spend.
2. Repeat step 1 to create a second spend output and change output.
3. Adjust the change outputs as necessary to pay the desired transaction
fee.

Clients which create intentional HITs must have the capability to form
alternate form HITs, and must do so for a non-zero fraction of the
transactions they create.

# Non-compliant heterogenous input script transactions

If a user wishes to create an output that is larger than half the total
size of their spendable outputs, or if their inputs are not distributed in
a manner in which the alternate form procedure can be completed, then the
user can not create a transaction which is compliant with this procedure.

----

A working draft of this document is here:
https://github.com/OpenBitcoinPrivacyProject/rfc/blob/master/bips/obpp-03.mediawiki

A few points of anticipated discussion:

1. It's possible for a CoinJoin transaction to decrease privacy by adhering
to the Standard Form in this BIP, depending on the utxos available for
creating it. For example, a typical two-person CoinJoin might look like:
input_A, input_B => spend_A, change_A, spend_B, change_B

In order to comply with the standard form, one or more parties would have
to add inputs to make this something like:

input_A_1, input_A_2, input_B_1, input_B_2 => spend_A, change_A, spend_B,
change_B.

If spend_A and spend_B are the same value, then input_A_1 and input_A_2 may
be linked based on the value of change_A and input_B_1 and input_B_2 may be
linked based on the value of change_B via sudoku analysis.

In that situation, wallets can opt to use the alternate form instead, or
stick with the standard form but enjoy a non-utxo set increase and a
significantly increased inter-transactional privacy set from other BIP
adherents.

2. In a naive simulation of a wallet randomly containing 1 to 10 utxos of
random value 1 to 10 and a desired spend of random value between 1 and the
sum of the utxos, the simulated wallet is able to create an alternate form
HIT 40% of the time. If we assume that half of all desire spends are a
value half or less of the total wallet balance, this improves to around
60%. Unfortunately, I don't have any good data on what values are found in
average wallets and what desired spends look like on average.

3. In the long-run it's better for all clients to participant in
CoinJoin-like operations, but this should significantly increase the cost
and decrease the signal of passive blockchain analysis attacks until that
becomes feasible.

Thanks in advance for your feedback.

-------------------------------------
Hi Peter,

I didn't entirely understand the process of transaction linearization.

What I see is a potential process where when the miner assembles the block,
he strips all but one sigscript per tx. The selection of which  sigscript
is retained is determined by the random oracle.  Is this is primary benefit
you are suggesting?

It appears to me that blocks still need to contain a list of full TX Input
and Tx Outputs with your approach. Some of the description seems to
indicate that there are opportunities to elide further data but it's
unclear to me how.

On Mon, Jun 20, 2016 at 7:14 AM Police Terror via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Bitcoin could embed a lisp interpreter such as Scheme, reverse engineer
> the current protocol into lisp (inside C++), run this alternative engine
> alongside the current one as an option for some years (only for fine
> tuning) then eventually fade this lisp written validation code instead
> of the current one.
>
> Scheme is small and minimal, and embeds easily in C++. This could be a
> better option than the libconsensus library - validation in a functional
> scripting language.
>
> That doesn't mean people can't program the validation code in other
> languages (maybe they'd want to optimize), but this code would be the
> standard.
>
> It's really good how you are thinking deeply how Bitcoin can be used,
> and the implications of everything. Also there's a lot of magical utopic
> thinking in Ethereum, which is transhumanist nonsense that is life
> denying. Bitcoin really speaks to me because it is real and a great tool
> following the UNIX principle.
>
> I wouldn't be so quick to deride good engineering over systematic
> provable systems for all domains. Bitcoin being written in C++ is not a
> defect. It's actually a strong language for what it does. Especially
> when used correctly (which is not often and takes years to master).
>
> With the seals idea- am I understand this correctly?: Every transaction
> has a number (essentially the index starting from 0 upwards) depending
> on where it is in the blockchain.
>
> Then there is an array (probably an on disk array mapping transaction
> indexes to hashes). Each hash entry in the array must be unique (the
> hashes) otherwise the transaction will be denied. This is a great idea
> to solve transaction hash collisions and simple to implement.
>
> Probabilistic validation is a good idea, although the real difficulty
> now seems to be writing and indexing all the blockchain data for
> lookups. And validation is disabled for most of the blocks. Pruning is
> only a stop gap measure (which loses data) that doesn't solve the issue
> of continually growing resource consumption. Hardware and implementation
> can only mitigate this so much. If only there was a way to simplify the
> underlying protocol to make it more resource efficient...
>
> Peter Todd via bitcoin-dev:
> > In light of Ethereum's recent problems with its imperative,
> account-based,
> > programming model, I thought I'd do a quick writeup outlining the
> building
> > blocks of the state-machine approach to so-called "smart contract"
> systems, an
> > extension of Bitcoin's own design that I personally have been developing
> for a
> > number of years now as my Proofchains/Dex research work.
> >
> >
> > # Deterministic Code / Deterministic Expressions
> >
> > We need to be able to run code on different computers and get identical
> > results; without this consensus is impossible and we might as well just
> use a
> > central authoritative database. Traditional languages and surrounding
> > frameworks make determinism difficult to achieve, as they tend to be
> filled
> > with undefined and underspecified behavior, ranging from signed integer
> > overflow in C/C++ to non-deterministic behavior in databases. While some
> > successful systems like Bitcoin are based on such languages, their
> success is
> > attributable to heroic efforts by their developers.
> >
> > Deterministic expression systems such as Bitcoin's scripting system and
> the
> > author's Dex project improve on this by allowing expressions to be
> precisely
> > specified by hash digest, and executed against an environment with
> > deterministic results. In the case of Bitcoin's script, the expression
> is a
> > Forth-like stack-based program; in Dex the expression takes the form of a
> > lambda calculus expression.
> >
> >
> > ## Proofs
> >
> > So far the most common use for deterministic expressions is to specify
> > conditions upon which funds can be spent, as seen in Bitcoin
> (particularly
> > P2SH, and the upcoming Segwit). But we can generalize their use to
> precisely
> > defining consensus protocols in terms of state machines, with each state
> > defined in terms of a deterministic expression that must return true for
> the
> > state to have been reached. The data that causes a given expression to
> return
> > true is then a "proof", and that proof can be passed from one party to
> another
> > to prove desired states in the system have been reached.
> >
> > An important implication of this model is that we need deterministic, and
> > efficient, serialization of proof data.
> >
> >
> > ## Pruning
> >
> > Often the evaluation of an expression against a proof doesn't require
> all all
> > data in the proof. For example, to prove to a lite client that a given
> block
> > contains a transaction, we only need the merkle path from the
> transaction to
> > the block header. Systems like Proofchains and Dex generalize this
> process -
> > called "pruning" - with built-in support to both keep track of what data
> is
> > accessed by what operations, as well as support in their underlying
> > serialization schemes for unneeded data to be elided and replaced by the
> hash
> > digest of the pruned data.
> >
> >
> > # Transactions
> >
> > A common type of state machine is the transaction. A transaction history
> is a
> > directed acyclic graph of transactions, with one or more genesis
> transactions
> > having no inputs (ancestors), and one or more outputs, and zero or more
> > non-genesis transactions with one or more inputs, and zero or more
> outputs. The
> > edges of the graph connect inputs to outputs, with every input connected
> to
> > exactly one output. Outputs with an associated input are known as spent
> > outputs; outputs with out an associated input are unspent.
> >
> > Outputs have conditions attached to them (e.g. a pubkey for which a valid
> > signature must be produced), and may also be associated with other
> values such
> > as "# of coins". We consider a transaction valid if we have a set of
> proofs,
> > one per input, that satisfy the conditions associated with each output.
> > Secondly, validity may also require additional constraints to be true,
> such as
> > requiring the coins spent to be >= the coins created on the outputs.
> Input
> > proofs also must uniquely commit to the transaction itself to be secure
> - if
> > they don't the proofs can be reused in a replay attack.
> >
> > A non-genesis transaction is valid if:
> >
> > 1. Any protocol-specific rules such as coins spent >= coins output are
> >    followed.
> >
> > 2. For every input a valid proof exists.
> >
> > 3. Every input transaction is itself valid.
> >
> > A practical implementation of the above for value-transfer systems like
> Bitcoin
> > could use two merkle-sum trees, one for the inputs, and one for the
> outputs,
> > with inputs simply committing to the previous transaction's txid and
> output #
> > (outpoint), and outputs committing to a scriptPubKey and output amount.
> > Witnesses can be provided separately, and would sign a signature
> committing to
> > the transaction or optionally, a subset of of inputs and/or outputs (with
> > merkle trees we can easily avoid the exponential signature validation
> problems
> > bitcoin currently has).
> >
> > As so long as all genesis transactions are unique, and our hash function
> is
> > secure, all transaction outputs can be uniquely identified (prior to
> BIP34 the
> > Bitcoin protocol actually failed at this!).
> >
> >
> > ## Proof Distribution
> >
> > How does Alice convince Bob that she has done a transaction that puts the
> > system into the state that Bob wanted? The obvious answer is she gives
> Bob data
> > proving that the system is now in the desired state; in a transactional
> system
> > that proof is some or all of the transaction history. Systems like
> Bitcoin
> > provide a generic flood-fill messaging layer where all participants have
> the
> > opportunity to get a copy of all proofs in the system, however we can
> also
> > implement more fine grained solutions based on peer-to-peer message
> passing -
> > one could imagine Alice proving to Bob that she transferred title to her
> house
> > to him by giving him a series of proofs, not unlike the same way that
> property
> > title transfer can be demonstrated by providing the buyer with a series
> of deed
> > documents (though note the double-spend problem!).
> >
> >
> > # Uniqueness and Single-Use Seals
> >
> > In addition to knowing that a given transaction history is valid, we
> also want
> > to know if it's unique. By that we mean that every spent output in the
> > transaction history is associated with exactly one input, and no other
> valid
> > spends exist; we want to ensure no output has been double-spent.
> >
> > Bitcoin (and pretty much every other cryptocurrency like it) achieves
> this goal
> > by defining a method of achieving consensus over the set of all (valid)
> > transactions, and then defining that consensus as valid if and only if no
> > output is spent more than once.
> >
> > A more general approach is to introduce the idea of a cryptographic
> Single-Use
> > Seal, analogous to the tamper-evidence single-use seals commonly used for
> > protecting goods during shipment and storage. Each individual seals is
> > associated with a globally unique identifier, and has two states, open
> and
> > closed. A secure seal can be closed exactly once, producing a proof that
> the
> > seal was closed.
> >
> > All practical single-use seals will be associated with some kind of
> condition,
> > such as a pubkey, or deterministic expression, that needs to be
> satisfied for
> > the seal to be closed. Secondly, the contents of the proof will be able
> to
> > commit to new data, such as the transaction spending the output
> associated with
> > the seal.
> >
> > Additionally some implementations of single-use seals may be able to also
> > generate a proof that a seal was _not_ closed as of a certain
> > time/block-height/etc.
> >
> >
> > ## Implementations
> >
> > ### Transactional Blockchains
> >
> > A transaction output on a system like Bitcoin can be used as a
> single-use seal.
> > In this implementation, the outpoint (txid:vout #) is the seal's
> identifier,
> > the authorization mechanism is the scriptPubKey of the output, and the
> proof
> > is the transaction spending the output. The proof can commit to
> additional
> > data as needed in a variety of ways, such as an OP_RETURN output, or
> > unspendable output.
> >
> > This implementation approach is resistant to miner censorship if the
> seal's
> > identifier isn't made public, and the protocol (optionally) allows for
> the
> > proof transaction to commit to the sealed contents with unspendable
> outputs;
> > unspendable outputs can't be distinguished from transactions that move
> funds.
> >
> >
> > ### Unbounded Oracles
> >
> > A trusted oracle P can maintain a set of closed seals, and produce signed
> > messages attesting to the fact that a seal was closed. Specifically, the
> seal
> > is identified by the tuple (P, q), with q being the per-seal
> authorization
> > expression that must be satisfied for the seal to be closed. The first
> time the
> > oracle is given a valid signature for the seal, it adds that signature
> and seal
> > ID to its closed seal set, and makes available a signed message
> attesting to
> > the fact that the seal has been closed. The proof is that message (and
> > possibly the signature, or a second message signed by it).
> >
> > The oracle can publish the set of all closed seals for
> transparency/auditing
> > purposes. A good way to do this is to make a merkelized key:value set,
> with the
> > seal identifiers as keys, and the value being the proofs, and in turn
> create a
> > signed certificate transparency log of that set over time. Merkle-paths
> from
> > this log can also serve as the closed seal proof, and for that matter, as
> > proof of the fact that a seal has not been closed.
> >
> >
> > ### Bounded Oracles
> >
> > The above has the problem of unbounded storage requirements as the
> closed seal
> > set grows without bound. We can fix that problem by requiring users of
> the
> > oracle to allocate seals in advance, analogous to the UTXO set in
> Bitcoin.
> >
> > To allocate a seal the user provides the oracle P with the authorization
> > expression q. The oracle then generates a nonce n and adds (q,n) to the
> set of
> > unclosed seals, and tells the user that nonce. The seal is then uniquely
> > identified by (P, q, n)
> >
> > To close a seal, the user provides the oracle with a valid signature
> over (P,
> > q, n). If the open seal set contains that seal, the seal is removed from
> the
> > set and the oracle provides the user with a signed message attesting to
> the
> > valid close.
> >
> > A practical implementation would be to have the oracle publish a
> transparency
> > log, with each entry in the log committing to the set of all open seals
> with a
> > merkle set, as well as any seals closed during that entry. Again, merkle
> paths
> > for this log can serve as proofs to the open or closed state of a seal.
> >
> > Note how with (U)TXO commitments, Bitcoin itself is a bounded oracle
> > implementation that can produce compact proofs.
> >
> >
> > ### Group Seals
> >
> > Multiple seals can be combined into one, by having the open seal commit
> to a
> > set of sub-seals, and then closing the seal over a second set of closed
> seal
> > proofs. Seals that didn't need to be closed can be closed over a special
> > re-delegation message, re-delegating the seal to a new open seal.
> >
> > Since the closed sub-seal proof can additionally include a proof of
> > authorization, we have a protcol where the entity with authorization to
> close
> > the master seal has the ability to DoS attack sub-seals owners, but not
> the
> > ability to fraudulently close the seals over contents of their choosing.
> This
> > may be useful in cases where actions on the master seal is expensive -
> such as
> > seals implemented on top of decentralized blockchains - by amortising
> the cost
> > over all sub-seals.
> >
> >
> > ## Atomicity
> >
> > Often protocols will require multiple seals to be closed for a
> transaction to
> > be valid. If a single entity controls all seals, this is no problem: the
> > transaction simply isn't valid until the last seal is closed.
> >
> > However if multiple parties control the seals, a party could attack
> another
> > party by failing to go through with the transaction, after another party
> has
> > closed their seal, leaving the victim with an invalid transaction that
> they
> > can't reverse.
> >
> > We have a few options to resolve this problem:
> >
> > ### Use a single oracle
> >
> > The oracle can additionally guarantee that a seal will be closed iff
> some other
> > set of seals are also closed; seals implemented with Bitcoin can provide
> this
> > guarantee. If the parties to a transaction aren't already all on the same
> > oracle, they can add an additional transaction reassigning their outputs
> to a
> > common oracle.
> >
> > Equally, a temporary consensus between multiple mutually trusting
> oracles can
> > be created with a consensus protocol they share; this option doesn't
> need to
> > change the proof verification implementation.
> >
> >
> > ### Two-phase Timeouts
> >
> > If a proof to the fact that a seal is open can be generated, even under
> > adversarial conditions, we can make the seal protocol allow a close to be
> > undone after a timeout if evidence can be provided that the other
> seal(s) were
> > not also closed (in the specified way).
> >
> > Depending on the implementation - especially in decentralized systems -
> the
> > next time the seal is closed, the proof it has been closed may in turn
> provide
> > proof that a previous close was in fact invalid.
> >
> >
> > # Proof-of-Publication and Proof-of-Non-Publication
> >
> > Often we need to be able to prove that a specified audience was able to
> receive
> > a specific message. For example, the author's PayPub protocol[^paypub],
> > Todd/Taaki's timelock encryption protocol[^timelock], Zero-Knowledge
> Contingent
> > Payments[^zkcp], and Lightning, among others work by requiring a secret
> key to
> > be published publicly in the Bitcoin blockchain as a condition of
> collecting a
> > payment. At a much smaller scale - in terms of audience - in certain
> FinTech
> > applications for regulated environments a transaction may be considered
> invalid
> > unless it was provably published to a regulatory agency.  Another
> example is
> > Certificate Transparency, where we consider a SSL certificate to be
> invalid
> > unless it has been provably published to a transparency log maintained
> by a
> > third-party.
> >
> > Secondly, many proof-of-publication schemes also can prove that a
> message was
> > _not_ published to a specific audience. With this type of proof
> single-use
> > seals can be implemented, by having the proof consist of proof that a
> specified
> > message was not published between the time the seal was created, and the
> time
> > it was closed (a proof-of-publication of the message).
> >
> > ## Implementations
> >
> > ### Decentralized Blockchains
> >
> > Here the audience is all participants in the system. However miner
> censorship
> > can be a problem, and compact proofs of non-publication aren't yet
> available
> > (requires (U)TXO commitments).
> >
> > The authors treechains proposal is a particularly generic and scalable
> > implementation, with the ability to make trade offs between the size of
> > audience (security) and publication cost.
> >
> > ### Centralized Public Logs
> >
> > Certificate Transparency works this way, with trusted (but auditable)
> logs run
> > by well known parties acting as the publication medium, who promise to
> allow
> > anyone to obtain copies of the logs.
> >
> > The logs themselves may be indexed in a variety of ways; CT simply
> indexes logs
> > by time, however more efficient schemes are possible by having the
> operator
> > commit to a key:value mapping of "topics", to allow publication (and
> > non-publication) proofs to be created for specified topics or topic
> prefixes.
> >
> > Auditing the logs is done by verifying that queries to the state of the
> log
> > return the same state at the same time for different requesters.
> >
> > ### Receipt Oracles
> >
> > Finally publication can be proven by a receipt proof by the oracle,
> attesting
> > to the fact that the oracle has successfully received the message. This
> is
> > particularly appropriate in cases where the required audience is the
> oracle
> > itself, as in the FinTech regulator case.
> >
> >
> > # Validity Oracles
> >
> > As transaction histories grow longer, they may become impractical to
> move from
> > one party to another. Validity oracles can solve this problem by
> attesting to
> > the validity of transactions, allowing history prior to the attested
> > transactions to be discarded.
> >
> > A particularly generic validity oracle can be created using deterministic
> > expressions systems. The user gives the oracle an expression, and the
> oracle
> > returns a signed message attesting to the validity of the expression.
> > Optionally, the expression may be incomplete, with parts of the
> expression
> > replaced by previously generated attestations. For example, an
> expression that
> > returns true if a transaction is valid could in turn depend on the
> previous
> > transaction also being valid - a recursive call of itself - and that
> recursive
> > call can be proven with a prior attestation.
> >
> > ## Implementations
> >
> > ### Proof-of-Work Decentralized Consensus
> >
> > Miners in decentralized consensus systems act as a type of validity
> oracle, in
> > that the economic incentives in the system are (supposed to be) designed
> to
> > encourage only the mining of valid blocks; a user who trusts the
> majority of
> > hashing power can trust that any transaction with a valid merkle path to
> a
> > block header in the most-work chain is valid. Existing decentralized
> consensus
> > systems like Bitcoin and Ethereum conflate the roles of validity oracle
> and
> > single-use seal/anti-replay oracle, however in principle that need not
> be true.
> >
> >
> > ### Trusted Oracles
> >
> > As the name suggests. Remote-attestation-capable trusted hardware is a
> > particularly powerful implementation - a conspiracy theory is that the
> reason
> > why essentially zero secure true remote attestation implementations
> exist is
> > because they'd immediately make untraceable digital currency systems
> easy to
> > implement (Finney's RPOW[^rpow] is a rare counter-example).
> >
> > Note how a single-use seal oracle that supports a generic deterministic
> > expressions scheme for seal authorization can be easily extended to
> provide a
> > validity oracle service as well. The auditing mechanisms for a
> single-use seal
> > oracle can also be applied to validity oracles.
> >
> >
> > # Fraud Proofs
> >
> > Protocols specified with deterministic expressions can easily generate
> "fraud
> > proofs", showing that claimed states/proof in the system are actually
> invalid.
> > Additionally many protocols can be specified with expressions of
> k*log2(n)
> > depth, allowing these fraud proofs to be compact.
> >
> > A simple example is proving fraud in merkle-sum tree, where the validity
> > expression would be something like:
> >
> >     (defun valid? (node)
> >         (or (== node.type leaf)
> >             (and (== node.sum (+ node.left.sum node.right.sum))
> >                  (and (valid? node.left)
> >                       (valid? node.right)))))
> >
> > To prove the above expression evaluates to true, we'll need the entire
> contents
> > of the tree. However, to prove that it evaluates to false, we only need a
> > subset of the tree as proving an and expression evaluates to false only
> > requires one side, and requires log2(n) data. Secondly, with pruning, the
> > deterministic expressions evaluator can automatically keep track of
> exactly
> > what data was needed to prove that result, and prune all other data when
> > serializing the proof.
> >
> >
> > ## Validity Challenges
> >
> > However how do you guarantee it will be possible to prove fraud in the
> first
> > place? If pruning is allowed, you may simply not have access to the data
> > proving fraud - an especially severe problem in transactional systems
> where a
> > single fraudulent transaction can counterfeit arbitrary amounts of value
> out of
> > thin air.
> >
> > A possible approach is the validity challenge: a subset of proof data,
> with
> > part of the data marked as "potentially fraudulent". The challenge can be
> > satisfied by providing the marked data and showing that the proof in
> question
> > is in fact valid; if the challenge is unmet participants in the system
> can
> > choose to take action, such as refusing to accept additional
> transactions.
> >
> > Of course, this raises a whole host of so-far unsolved issues, such as
> DoS
> > attacks and lost data.
> >
> >
> > # Probabilistic Validation
> >
> > Protocols that can tolerate some fraud can make use of probabilistic
> > verification techniques to prove that the percentage of undetected fraud
> within
> > the system is less than a certain amount, with a specified probability.
> >
> > A common way to do this is the Fiat-Shamir transform, which repeatedly
> samples
> > a data structure deterministically, using the data's own hash digest as
> a seed
> > for a PRNG. Let's apply this technique to our merkle-sum tree example.
> We'll
> > first need a recursive function to check a sample, weighted by value:
> >
> >     (defun prefix-valid? (node nonce)
> >         (or (== node.type leaf)
> >             (and (and (== node.sum (+ node.left.sum node.right.sum))
> >                       (> 0 node.sum)) ; mod by 0 is invalid, just like
> division by zero
> >                                       ; also could guarantee this with a
> type system
> >                  (and (if (< node.left.sum (mod nonce node.sum))
> >                           (prefix-valid? node.right (hash nonce))
> >                           (prefix-valid? node.left (hash nonce)))))))
> >
> > Now we can combine multiple invocations of the above, in this case 256
> > invocations:
> >
> >     (defun prob-valid? (node)
> >         (and (and (and .... (prefix-valid? node (digest (cons (digest
> node) 0)))
> >              (and (and ....
> >                             (prefix-valid? node (digest (cons (digest
> node) 255)))
> >
> > As an exercise for a reader: generalize the above with a macro, or a
> suitable
> > types/generics system.
> >
> > If we assume our attacker can grind up to 128 bits, that leaves us with
> 128
> > random samples that they can't control. If the (value weighted)
> probability of
> > a given node is fraudulent q, then the chance of the attacker getting
> away with
> > fraud is (1-q)^128 - for q=5% that works out to 0.1%
> >
> > (Note that the above analysis isn't particularly well done - do a better
> > analysis before implementing this in production!)
> >
> >
> > ## Random Beacons and Transaction History Linearization
> >
> > The Fiat-Shamir transform requires a significant number of samples to
> defeat
> > grinding attacks; if we have a random beacon available we can
> significantly
> > reduce the size of our probabilistic proofs. PoW blockchains can
> themselves act
> > as random beacons, as it is provably expensive for miners to manipulate
> the
> > hash digests of blocks they produce - to do so requires discarding
> otherwise
> > valid blocks.
> >
> > An example where this capability is essential is the author's transaction
> > history linearization technique. In value transfer systems such as
> Bitcoin, the
> > history of any given coin grows quasi-exponentially as coins are mixed
> across
> > the entire economy. We can linearize the growth of history proofs by
> redefining
> > coin validity to be probabilistic.
> >
> > Suppose we have a transaction with n inputs. Of those inputs, the total
> value
> > of real inputs is p, and the total claimed value of fake inputs is q. The
> > transaction commits to all inputs in a merkle sum tree, and we define the
> > transaction as valid if a randomly chosen input - weighted by value - can
> > itself be proven valid. Finally, we assume that creating a genuine input
> is a
> > irrevocable action which irrevocable commits to the set of all inputs,
> real and
> > fake.
> >
> > If all inputs are real, 100% of the time the transaction will be valid;
> if all
> > inputs are fake, 100% of the time the transaction will be invalid. In
> the case
> > where some inputs are real and some are fake the probability that the
> fraud
> > will be detected is:
> >
> >     q / (q + p)
> >
> > The expected value of the fake inputs is then the sum of the potential
> upside -
> > the fraud goes detected - and the potential downside - the fraud is
> detected
> > and the real inputs are destroyed:
> >
> >     E = q(1 - q/(q + p)) - p(q/(q + p)
> >       = q(p/(q + p)) - p(q/(q + p)
> >       = (q - q)(p/(q + p))
> >       = 0
> >
> > Thus so long as the random beacon is truly unpredictable, there's no
> economic
> > advantage to creating fake inputs, and it is sufficient for validity to
> only
> > require one input to be proven, giving us O(n) scaling for transaction
> history
> > proofs.
> >
> >
> > ### Inflationary O(1) History Proofs
> >
> > We can further improve our transaction history proof scalability by
> taking
> > advantage of inflation. We do this by occasionally allowing a
> transaction proof
> > to be considered valid without validating _any_ of the inputs; every
> time a
> > transaction is allowed without proving any inputs the size of the
> transaction
> > history proof is reset. Of course, this can be a source of inflation, but
> > provided the probability of this happening can be limited we can limit
> the
> > maximum rate of inflation to the chosen value.
> >
> > For example, in Bitcoin as of writing every block inflates the currency
> supply
> > by 25BTC, and contains a maximum of 1MB of transaction data,
> 0.025BTC/KB. If we
> > check the prior input proof with probability p, then the expected value
> of a
> > transaction claiming to spend x BTC is:
> >
> >     E = x(1-p)
> >
> > We can rewrite that in terms of the block reward per-byte R, and the
> transaction size l:
> >
> >     lR = x(1-p)
> >
> > And solving for p:
> >
> >     p = 1 - lR/x
> >
> > For example, for a 1KB transaction proof claiming to spending 10BTC we
> can omit
> > checking the input 0.25% of the time without allowing more monetary
> inflation
> > than the block reward already does. Secondly, this means that after n
> > transactions, the probability that proof shortening will _not_ happen is
> p^n,
> > which reaches 1% after 1840 transactions.
> >
> > In a system like Bitcoin where miners are expected to validate, a
> transaction
> > proof could consist of just a single merkle path showing that a
> single-use seal
> > was closed in some kind of TXO commitment - probably under 10KB of data.
> That
> > gives us a history proof less than 18.4MB in size, 99% of the time, and
> less
> > than 9.2MB in size 90% of the time.
> >
> > An interesting outcome of thing kind of design is that we can
> institutionalize
> > inflation fraud: the entire block reward can be replaced by miners
> rolling the
> > dice, attempting to create valid "fake" transactions. However, such a
> pure
> > implementation would put a floor on the lowest transaction fee possible,
> so
> > better to allow both transaction fee and subsidy collection at the same
> time.
> >
> >
> > # References
> >
> > [^paypub] https://github.com/unsystem/paypub
> > [^timelock] https://github.com/petertodd/timelock
> > [^zkcp]
> https://bitcoincore.org/en/2016/02/26/zero-knowledge-contingent-payments-announcement/
> > [^rpow] https://cryptome.org/rpow.htm
> >
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On 13/05/16 18:03, Aaron Voisine wrote:
> I like the idea of specifying the type of address as a bit field flag.
> 0x80000000 is already used to specify hardened derivation, so 0x40000000
> would be the next available to specify witness addresses. This is
> compatible with existing accounts and wallet layouts.

I think this is over-optimization. What is the advantage of

m/0'/0x40000000 instead of m/whatever'/0 ?

But this is off-topic anyway, as we are discussing multiple-accounts per
wallet layout here, not one-account-per-wallet design.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
On Thursday, 22 September 2016 14:26:18 CEST Peter Todd wrote:
> > The way towards that flexibility is to use a generic concept made
> > popular various decades ago with the XML format. The idea is that we
> > give each field a name and this means that new fields can be added or
> > optional fields can be omitted from individual transactions
> 
> That argument is not applicable to required fields: 

The argument that optional fields can be omitted is not applicable to 
required fields, you are correct. That should be rather obvious because 
required fields are not optional fields.

> the code to get the
> fields from the extensible format is every bit as complex as the very
> simple code required to deserialize/serialize objects in the current
> system.

Probably a tiny bit more complex as the current format assumes a lot more.

You may have misread my email because there was no argument made towards 
complexity. The argument was towards flexibility.

> In any case your BIP needs to give some explicit examples of hypothetical
> upgrades in the future, how they'd take advantage of this, and what the
> code to do so would look like.

Why?

> > > Also, if you're going to break compatibility with all existing
> > > software, it makes sense to use a format that extends the merkle
> > > tree down into the transaction inputs and outputs.
> > 
> > Please argue your case.
> 
> See my arguments re: segwit a few months ago, e.g. the hardware wallet
> txin proof use-case.

Please consider that I'm not going to search for something based on a vague 
reference like that, if you want to convince me you could you at least 
provide a URL?
You want me to see the value of your idea, I think you should at least 
provide the argument. Isn't that fair?

Thanks for your email Peter, would love you to put a bit more time into 
understanding flexible transactions and we can have a proper discussion 
about it.


-------------------------------------
Hi all,

We've made some significant changes to BIP75 which we think simplify things
greatly:

Instead of introducing encrypted versions of all BIP70 messages
(EncryptedPaymentRequest, EncryptedPayment, etc), we have defined a generic
EncryptedProtocolMessage type which is essentially a wrapper that enables
encryption for all existing BIP70 messages. This reduces the number of new
messages we are defining and makes it easier to add new message types in
the future.

We've also decided to use AES-GCM instead of AES-CBC, which eliminates the
need for the verification hash.

A pull request has been submitted, which can be seen here:
https://github.com/bitcoin/bips/pull/385

All comments are welcome. Thank you!

James

-------------------------------------
Ehm, I though those discussions about "ASICs are bad, because X" ended
years ago by starting "ASIC unfriendly" altcoins. ASIC industry is twisted
even without AsicBoost. I don't see any particular reason why to change
rules just because of 10% edge.

This is opening Pandora box and it is potentially extremely dangerous for
the health of the network. You cannot know in advance what you'll break by
changing the rules.

Disclaimer: I don't have any stake in any ASIC company/facility.

slush

On Wed, May 11, 2016 at 2:20 PM, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
>
> On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <
> sergio.d.lerner@gmail.com> wrote:
>
>>
>>
>> You can find it here:
>> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/
>>
>> Basically, the idea is to put in the first 64 bytes a 4 byte hash of the
>> second 64-byte chunk. That design also allows increased nonce space in the
>> first 64 bytes.
>>
>> My mistake here. I didn't recalled correctly my own idea. The idea is to
> include in the second 64-byte chunk a 4-byte hash of the first chunk, not
> the opposite.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Hi

Is there a worked out scriptPubKey for doing multisig with just hashes
of the participants? I think it is doable and it is more secure to a
compromised ECDSA. I'm thinking something like this for the
scriptPubKey:
 2 OP_SWAP OP_SWAP OP_SWAP OP_DUP OP_HASH160 <pubKeyHash1>
OP_EQUALVERIFY OP_DUP OP_HASH160 <pubKeyHash2> OP_EQUALVERIFY OP_DUP
OP_HASH160 <pubKeyHash3> OP_EQUALVERIFY 3 OP_CHECKMULTISIG

and <sigs><pubkeys> for the scriptSig

Can anyone confirm or send me a link to the worked out script?

Thanks

-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647


-------------------------------------
Hi List,

Following up to the discussion last month ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012695.html ), ive prepared a proposal for a BIP here:
	
	https://github.com/DanielWeigl/bips/blob/master/bip-p2sh-accounts.mediawiki


Any comments on it? Does anyone working on a BIP44 compliant wallet implement something different?
If there are no objection, id also like to request a number for it.

Thx,
Daniel


-------------------------------------
On Sat, Feb 6, 2016 at 9:37 AM, Gavin Andresen wrote:

> Responding to "28 days is not long enough" :
>

Gavin,

Thank you for the emails. Bitcoin Core has been working with the Bitcoin
ecosystem on developing and now testing a new capacity increasing feature
called segregated witness (segwit). Segregated witness is a voluntary,
mutually backwards-compatible capacity upgrade for the Bitcoin system.
Many, many hundreds of millions of dollars of Bitcoin value have flowed
through soft-forked upgrades to the Bitcoin system, representing upgrades
from across the entire ecosystem and the entire Bitcoin network, over
multiple years including BIP 12, BIP 16, BIP 17, BIP 30, BIP 34, BIP 42,
BIP 62, BIP 65, BIP 66, etc. So that’s the context from which I have been
approaching your hard-fork ideas for the past year.

Benefits of segregated witness
https://bitcoincore.org/en/2016/01/26/segwit-benefits/

Ecosystem buy-in and support for segregated witness continues to grow:
https://bitcoincore.org/en/segwit_adoption/

There is also a segwit testnet which everyone is encouraged to investigate
and develop against-- companies love them some testing, after all:
https://bitcoincore.org/en/2016/01/21/launch_segwit_testnet/

A plan for Bitcoin Core capacity increases was put forward and can be found
here:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011865.html
https://bitcoincore.org/en/2015/12/23/capacity-increases-faq/

With respect, the question should not be "is 28 days enough time for anyone
to roll out new binaries", it's instead a question of "how long does it
take someone to agree to upgrade to these new incompatible rules".

If Bitcoin users don't want to upgrade to incompatible rules right now, why
would they agree when 10% of the hashpower is setting some flag in a block?
Why would they change their minds at 20%? 90%? I am not saying here that
hard-forks should never be attempted, although we need as an ecosystem to
develop much more rigor and a more data-driven approach, and while that
might be hard to define exactly, as was once said by regulators, “I know it
when I see it”. Companies in the financial sector give a year or more
before deprecating old APIs even after the new one has been up and running
concurrently and well proven, and would not shut off their old one in order
to get adoption of the new one.

Are we OK with some percent of the Bitcoin ecosystem not agreeing with the
existing rules? What would that mean? Are you willing to maintain two
separate networks, and if not, would you please document this in your BIP?
Deprecation timeline and emergency procedures?? Should we include
rationalizations for not using a new address prefix? In the event of a
partial hard-fork where two chains exist, wouldn't it make more sense to
have the new chain use a new address prefix? Using a new address prefix
could conceivably serve to minimize the impact of what almost looks like an
intentionally constructed y2k-bug type of event for the ecosystem.

I suspect that soft-fork upgrades have in the past tolerated _less_ rigor
around planning because voluntary soft-fork upgrading does not
intentionally break backwards-compatibility. Over time I expect that even
soft-fork upgrades will have much more planning, but again, it seems that
incompatible changes require much more rigor. If the sky is truly falling
according to your pronouncements, then there are millions if not billions
of dollars of value on the line which are being risked from lack of
engineering rigor without a well documented procedure, and suggesting that
we agree on that "next time" is not going to create the results that meet
your or anyone else’s desire. Much more, we need to signal to the broader
ecosystem and world that we are serious, mature and ready for business.

Regarding your request for definitions about soft-hard forks and
generalized soft-forks, you can find some definitions over here:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012173.html
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012172.html

About hard-forks you may be interested in reading and internalizing,
https://github.com/bitcoin/bips/blob/master/bip-0099.mediawiki

This was an interesting exploration of soft-forks and hard-forks:
https://petertodd.org/2016/soft-forks-are-safer-than-hard-forks

On the security of soft-forks
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012014.html

Are soft-forks misnamed?
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-September/011266.html

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
In the section https://github.com/luke-jr/bips/blob/bip-biprevised/bip-biprevised.mediawiki#formally-defining-consensus

Can we please find another term for the "consensus" here (which is
often confused with "consensus rules", "consensus code" etc)?
In BIP99 I used the term "uncontroversial", but I'm happy to change it
to something else if that helps us moving away from consistently using
the same term for two related but very different concepts.
"nearly universal acceptance", "ecosystem-harmonious"...seriously,
almost anything would be better than keep overloading "consensus"...


-------------------------------------
On Tue, Sep 20, 2016 at 07:15:45PM +0200, Tom via bitcoin-dev wrote:
> === Serialization order===
> 
> The tokens defined above have to be serialized in a certain order for the
> transaction to be well-formatted.  Not serializing transactions in the
> order specified would allow multiple interpretations of the data which
> can't be allowed.

If the order of the tokens is fixed, the tokens themselves are redundant
information when tokens are required; when tokens may be omitted, a simple
"Some/None" flag to mark whether or not the optional data has been omitted is
appropriate.


Also, if you're going to break compatibility with all existing software, it
makes sense to use a format that extends the merkle tree down into the
transaction inputs and outputs.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Hi all,

For those who missed it, the deadline for submissions has been extended
to Sept 9th so be sure to submit before then! We will be doing rolling
acceptance this time around to try to get most responses out before the
23rd.

Because a few folks seemed to have some confusion, the definition of
"scaling" here is pretty broad - while we definitely will have a lot of
talks on the usual tx-volume-throughput things, the topics of interest
also include things like fungibility. The full list from the site (for
inspiration purposes, this is by no means exhaustive) is:

Improving Bitcoin throughput
Layer 2 ideas (i.e. payment channels, etc.)
Security and privacy
Incentives and fee structures
Testing, simulation, and modeling
Network resilience and latency
Fungibility
Anti-spam measures
Block size proposals
Mining concerns

Thanks,
Matt

On 08/02/16 01:49, Pindar Wong via bitcoin-dev wrote:
> Dear All,
> 
> The Call for Proposals (CFP) for 'Scaling Bitcoin 2016: Retarget' is now
> open. 
> 
> Please see https://scalingbitcoin.org for details.
> 
> *Important Dates*
> 
> Sept 2nd - Deadline for submissions to the CFP
> Sept 23rd - Deadline for applicant acceptance notification
> 
> See you in Milan! (October 8th and 9th)
> 
> Ciao! :)
> 
> p.
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
Maybe I'm asking this question on the wrong mailing list:

Matt/Adam: do you have some reason to think that RIPEMD160 will be broken
before SHA256?
And do you have some reason to think that they will be so broken that the
nested hash construction RIPEMD160(SHA256()) will be vulnerable?

Adam: re: "where to stop"  :  I'm suggesting we stop exactly at the current
status quo, where we use RIPEMD160 for P2SH and P2PKH.

Ethan:  your algorithm will find two arbitrary values that collide. That
isn't useful as an attack in the context we're talking about here (both of
those values will be useless as coin destinations with overwhelming
probability).

Dave: you described a first preimage attack, which is 2**160 cpu time and
no storage.


-- 
--
Gavin Andresen

-------------------------------------
BIP draft: https://github.com/jl2012/bips/blob/mast/bip-mast.mediawiki
Reference implementation:
https://github.com/jl2012/bitcoin/commit/f335cab76eb95d4f7754a718df201216a49
75d8c

This BIP defines a new witness program type that uses a Merkle tree to
encode mutually exclusive branches in a script. This enables complicated
redemption conditions that are currently not possible, improves privacy by
hiding unexecuted scripts, and allows inclusion of non-consensus enforced
data with very low or no additional cost.

The reference implementation is a small and simple patch on top of BIP141
(segwit), however, I have no intention to push this before segwit is
enforced. Instead, I hope the MAST will come with many new op codes,
particularly Schnorr signature.



-------------------------------------
Hello everyone.

Pieter Wuille has pushed code for a new segwit testnet that features activation via BIP9 as well as support for BIP68, BIP112, and BIP113. In particular, it now supports Lightning Network app development and collaboration.
I encourage everyone to spin up a node and try it out.

For source code, please go to Pieter's github repo:
https://github.com/sipa/bitcoin/tree/segwit4 <https://github.com/sipa/bitcoin/tree/segwit4>

Feedback is welcome here or on the #segwit-dev channel on Freenode.


- Eric
-------------------------------------
The BIP147 reads:

*Sigop cost* is defined. The cost of a sigop in traditional script is 4,
while the cost of a sigop in witness program is 1.

The new rule is total *sigop cost* ≤ 80,000.

But the code implements:
if (nSigOps + (nWitSigOps + 3) / 4 > MAX_BLOCK_SIGOPS)
 ... error....

Which is not the same.

For example:
nSigOps = 1
nWitSigOps =79999

Is not an error by BIP definition but it's an error by the implemented code.

Regards, Sergio.

-------------------------------------
That is very interesting.

There has been some recent discussion about atomic cross chain transfers
between Bitcoin and legacy altcoins.  For this purpose a legacy altcoin is
one that has strict IsStandard() rules and none of the advanced script
opcodes.

It has a requirement that Bob sends Alice a pair [hash_of_bob_private_key,
bob_public_key].  Bob has to prove that the hash is actually the result of
hashing the private key that matches bob_public_key.

This can be achieved with a cut-and-choose scheme.  It uses a fee so that
an attacker loses money on average.  It is vulnerable to an attacker who
doesn't mind losing money as long as the target loses money too.

Bob would have to prove that he has an x such that

xG = <bob_public_key>
hash(x) = hash_of_bob_private_key

Is the scheme fast enough such that an elliptic curve multiply would be
feasible?  You mention 20 seconds for 5 SHA256 operations, so I am guessing
no?



On Fri, Feb 26, 2016 at 11:06 PM, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Congratulations!
>
> It a property of the SKCP system that the person who performed the trusted
> setup cannot extract any information from a proof?
>
> In other words, is it proven hard to obtain information from a proof by
> the buyer?
>
> On Fri, Feb 26, 2016 at 6:42 PM, Gregory Maxwell via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> I am happy to announce the first successful Zero-Knowledge Contingent
>> Payment (ZKCP) on the Bitcoin network.
>>
>> ZKCP is a transaction protocol that allows a buyer to purchase
>> information from a seller using Bitcoin in a manner which is private,
>> scalable, secure, and which doesn’t require trusting anyone: the
>> expected information is transferred if and only if the payment is
>> made. The buyer and seller do not need to trust each other or depend
>> on arbitration by a third party.
>>
>> Imagine a movie-style “briefcase swap” (one party with a briefcase
>> full of cash, another containing secret documents), but without the
>> potential scenario of one of the cases being filled with shredded
>> newspaper and the resulting exciting chase scene.
>>
>> An example application would be the owners of a particular make of
>> e-book reader cooperating to purchase the DRM master keys from a
>> failing manufacturer, so that they could load their own documents on
>> their readers after the vendor’s servers go offline. This type of sale
>> is inherently irreversible, potentially crosses multiple
>> jurisdictions, and involves parties whose financial stability is
>> uncertain–meaning that both parties either take a great deal of risk
>> or have to make difficult arrangement. Using a ZKCP avoids the
>> significant transactional costs involved in a sale which can otherwise
>> easily go wrong.
>>
>> In today’s transaction I purchased a solution to a 16x16 Sudoku puzzle
>> for 0.10 BTC from Sean Bowe, a member of the Zcash team, as part of a
>> demonstration performed live at Financial Cryptography 2016 in
>> Barbados. I played my part in the transaction remotely from
>> California.
>>
>> The transfer involved two transactions:
>>
>> 8e5df5f792ac4e98cca87f10aba7947337684a5a0a7333ab897fb9c9d616ba9e
>> 200554139d1e3fe6e499f6ffb0b6e01e706eb8c897293a7f6a26d25e39623fae
>>
>> Almost all of the engineering work behind this ZKCP implementation was
>> done by Sean Bowe, with support from Pieter Wuille, myself, and Madars
>> Virza.
>>
>>
>> Read more, including technical details at
>>
>> https://bitcoincore.org/en/2016/02/26/zero-knowledge-contingent-payments-announcement/
>>
>> [I hope to have a ZKCP sudoku buying faucet up shortly. :) ]
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Sorry about the last email, I deleted the repository to get rid of the BIP
number to prevent confusion. The correct address is
https://github.com/Dako300/BIP

This is my BIP idea: a fast, robust, and standardized way for representing
Bitcoin addresses over audio. It takes the binary representation of the
Bitcoin address (little endian), chops that up into 4 or 2 bit chunks
(depending on type, 2 bit only for low quality audio like american
telephone lines), and generates a tone based upon that value. This started
because I wanted an easy way to donate to podcasts that I listen to, and
having a Shazam-esque app (or a media player with this capability) that
gives me an address automatically would be wonderful for both the consumer
and producer. Comes with error correction built into the protocol

-------------------------------------
On Mar 10, 2016 02:04, "Mustafa Al-Bassam via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> >A hard-fork BIP requires adoption from the entire Bitcoin economy,
> particularly including those selling desirable goods and services in
> exchange for bitcoin payments, as well as Bitcoin holders who wish to
> spend or would spend their bitcoins (including selling for other
> currencies) differently in the event of such a hard-fork.
> What if one shop owner, for example, out of thousands, doesn't adapt the
> hard-fork? It is expected, and should perhaps be encouraged, for a small
> minority to not accept a hard fork, but by the wording of the BIP
> ("entire Bitcoin economy"), one shop owner can veto a hard-fork.

No, the hardfork can still happen, but if a small group remains using the
old chain (a single person will likely abandon it very soon), then it
cannot be said that deployment was universal and thus the hardfork BIP
doesn't move to the final state. As long as there's users using the old
chain, a hardfork BIP shouldn't become final if I understood BIP2
correctly.

In other words,  uncontroversial hardfork bips can make it to the final
state once deployed, controversial hardforks may never become universally
deployed.

-------------------------------------
Hi Pieter,

> I tried to derive what length of short ids is actually necessary (some
> write-up is on
> https://gist.github.com/sipa/b2eb2e486156b5509ac711edd16153ed but it's
> incomplete).
> 
> For any reasonable numbers I can come up with (in a very wide range),
> the number of bits needed is very well approximated by:
> 
>  log2(#receiver_mempool_txn * #block_txn_not_in_receiver_mempool /
> acceptable_per_block_failure_rate)
> 
> For example, with 20000 mempool transactions, 2500 transactions in a
> block, 95% hitrate, and a chance of 1 in 10000 blocks to fail to
> reconstruct, needed_bits = log2(20000 * 2500 * (1 - 0.95) / 0.0001) =
> 34.54, or 5 byte txids would suffice.
> 
> Note that 1 in 10000 failures may sound like a lot, but this is for each
> individual connection, and since every transmission uses separately
> salted identifiers, occasional failures should not affect global
> propagation. Given that transmission failures due to timeouts, network
> connectivity, ... already occur much more frequently than once every few
> gigabytes (what 10000 blocks corresponds to), that's probably already
> more than enough.
> 
> In short: I believe 5 or 6 byte txids should be enough, but perhaps it
> makes sense to allow the sender to choose (so he can weigh trying
> multiple nonces against increasing the short txid length).

[9 May 16 @ 11am PDT]  

We worked on this with respect to “Xthin" for Bitcoin Unlimited, and came to a similar conclusion.  

But we (I think it was theZerg) also noticed another trick: if the node receiving the thin blocks has a small number of collisions with transactions in its mempool (e.g., 1 or 2), then it can test each possible block against the Merkle root in the block header to determine the correct one.  Using this technique, it should be possible to further reduce the number of bytes used for the txids.  That being said, even thin blocks built from 64-bit short IDs represent a tremendous savings compared to standard block propagation.  So we (Bitcoin Unlimited) decided not to pursue this optimization any further at that time.

***

It’s also interesting to ask what the information-theoretic minimum amount of information necessary for a node to re-construct a block is. The way I’m thinking about this currently[1] is that the node needs all of the transactions in the block that were not initially part of its mempool, plus enough information to select and ordered subset from that mempool that represents the block.  If m is the number of transactions in mempool and n is the number of transactions in the block, then the number of possible subsets (C') is given by the binomial coefficient:

  C' =  m! / [n! (m - n)!]

Since there are n! possible orderings for each subset, the total number of possible blocks (C) of size n from a mempool of size m is

  C = n! C’ = m! / (m-n)!

Assuming that all possible blocks are equally likely, the Shannon entropy (the information that must be communicated) is the base-2 logarithm of the number of possible blocks.  After making some approximations, this works out very close to

   minimum information ~= n * log2(m),

which for your case of 20,000 transactions in mempool (m = 20,000) and a 2500-transaction block (n = 2500), yields

   minimum information = 2500 * log2(20,000) ~ 2500 * 15 bits.

In other words, a lower bound on the information required is about 2 bytes per transactions for every transaction in the block that the node is already aware of, as well as all the missing transactions in full. 

Of course, this assumes an unlimited number of round trips, and it is probably complicated by other factors that I haven’t considered (queue the “spherical cow” jokes :), but I thought it was interesting that a technique like Xthin or compact blocks is already pretty close to this limit.  

Cheers,
Peter 

[1] There are still some things that I can’t wrap my mind around that I’d love to discuss with another math geek :)




-------------------------------------
And anyone who would have discovered it independently would have been free
to implement it.  That's the issue, not that there's an optimization.

On Wed, May 11, 2016 at 9:27 PM, Tom Harding via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On 5/10/2016 2:43 PM, Sergio Demian Lerner via bitcoin-dev wrote:
> >
> > If we change the protocol then the message to the ecosystem is that
> > ASIC optimizations should be kept secret.
>
> Further to that point, if THIS optimization had been kept secret, nobody
> would be talking about doing anything, as with countless other
> optimizations.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
> I haven't been able to find the beginning of this thread, so apologies
> if I've misunderstood what this is for, but it _sounds_ like we're
> re-inventing HKDF.

> I'd recommend reading the paper about HKDF. It stands out among crypto
> papers for having a nice clear justification for each of its design
> decisions, so you can see why they did it (very slightly) differently
> than the various constructions proposed up-thread.

Thanks Zooko

I think HKDF instead of a single HMAC_SHA512 seams reasonable and
something we should consider.

I'll try to evaluate the implications of using HKDF over HMAC_SHA512 and
will update the BIP if there are no concerns about it.

</jonas>


-------------------------------------
Ups, I forgot that you take the midstate which of course depends on the
version number. So forget everything I said about the version bits. You are
right. But why take the midstate? It can be any hash of the first chunk. So
you probably want to take a hash function that's available in standard
software libraries. And I suppose midstate() is not.


On Wed, May 11, 2016 at 11:28 AM, Timo Hanke <timo.hanke@web.de> wrote:

> Sorry, you must have meant all 12 bytes. That makes finding a collision
> substantially harder. However, you may have to restrict yourself to 10
> bytes because you don't know if any hardware does timestamp rolling
> on-chip. Also you create an incentive to mess around with the version bits
> instead, so you would have to fix that as well. So it basically means a new
> mining header with the real blockheader as a child header.
>
> On Wed, May 11, 2016 at 9:24 AM, Timo Hanke <timo.hanke@web.de> wrote:
>
>> Luke, do you mean to replace the first 4 bytes of the second chunk (bytes
>> 64..67 in 0-based counting) by the XOR of those 4 bytes with the first 4
>> bytes of the midstate? (I assume you don't care about 12 bytes but rather
>> those 4 bytes.)
>>
>> This does not work. All it does is adding another computational step
>> before you can check for a collision in those 4 bytes. It makes finding a
>> collision only marginally harder.
>>
>> On Wed, May 11, 2016 at 7:28 AM, Luke Dashjr via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> On Wednesday, May 11, 2016 12:20:55 PM Sergio Demian Lerner via
>>> bitcoin-dev
>>> wrote:
>>> > On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <
>>> > sergio.d.lerner@gmail.com> wrote:
>>> > > You can find it here:
>>> > >
>>> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-blo
>>> > > ck-header/
>>> > >
>>> > > Basically, the idea is to put in the first 64 bytes a 4 byte hash of
>>> the
>>> > > second 64-byte chunk. That design also allows increased nonce space
>>> in
>>> > > the first 64 bytes.
>>> >
>>> > My mistake here. I didn't recalled correctly my own idea. The idea is
>>> to
>>> > include in the second 64-byte chunk a 4-byte hash of the first chunk,
>>> not
>>> > the opposite.
>>>
>>> What if we XOR bytes 64..76 with the first 12 bytes of the SHA2 midstate?
>>> Would that work?
>>>
>>> Luke
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
>>
>

-------------------------------------
Pieter Wuille via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
writes:
> Yes, this is what I worry about. We're constructing a 2-of-2 multisig
> escrow in a contract. I reveal my public key A, you do a 80-bit search for
> B and C such that H(A and B) = H(B and C). You tell me your keys B, and I
> happily send to H(A and B), which you steal with H(B and C).

FWIW, this attack would effect the current lightning-network "deployable
lightning" design at channel establishment; we reveal our pubkey in the
opening packet (which is used to redeem a P2SH using normal 2of2).

At least you need to grind before replying (which will presumably time
out), rather than being able to do it once the channel is open.

We could pre-commit by exchanging hashes of pubkeys first, but contracts
on bitcoin are hard enough to get right that I'm reluctant to add more
hoops.

Cheers,
Rusty.


-------------------------------------
On Friday, 14 October 2016 04:51:01 CEST Daniel Robinson via bitcoin-dev 
wrote:
> > Because if not, the DPL is still better than the status quo.
> 
> Agreed. Also worth noting that it has a potential advantage over
> unilateral patent disarmament, analogous to the advantage of copyleft
> licenses over MIT/BSD: it provides an incentive (at least a theoretical
> one) for other companies to adopt it too.

This is a very important point and a huge step forward in my opinion.

The downside of MIT/BSD licenses is that companies can take and not give 
back. It doesn't build a community and commonly-shared property. Copyleft 
allows people to take and embrace, but if they extend they have to give 
back. Which is fair, you build it on their stuff...

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Tuesday, February 02, 2016 3:58:21 PM Gavin Andresen wrote:
> I don't like the definition of "consensus".  I think the definition
> described gives too much centralized control to whoever controls the
> mailing list and the wiki.

How can I improve this? Inevitably, every medium of communications will be 
controlled by someone (even if unmoderated, it becomes effectively controlled 
by trolls who spam it with garbage).

I think it's important to note that this is also only for updating the status 
of BIPs, and is not in any way relevant to such proposals *actually* being 
accepted. So if the BIP process were to breakdown on this or any other point, 
it isn't somehow controlling the actual reality. To explicitly clarify this 
point, I have added to the end of the section:
    "These criteria are considered objective ways to observe the de facto
     adoption of the BIP, and are not to be used as reasons to oppose or
     reject a BIP. Should a BIP become actually and unambiguously adopted
     despite not meeting the criteria outlined here, it should still be
     updated to Final status."
Does that help?

Thanks,

Luke


-------------------------------------
My BIP was ultimately accepted, it's number 74

https://github.com/bitcoin/bips/blob/master/bip-0074.mediawiki

The editor did not agree with it, and I suspect would comment against it
with his new proposed BIP :)

I really appreciated that despite his vehement disagreement, he assigned
the BIP. It seems like the process worked great. There was deep vetting,
lots of back and forth and the editor put aside his personal opinions to
accept the BIP.

That being said...

The mailing list is a problem. I'm still on moderation only. I have no idea
if this message will go through and when it will go through. I totally
understand the desire to keep the conversation level high, but when people
who *are* whitelisted can quickly post multiple heated arguments against
you (publicly) and you can't respond, then that starts to look very
centralized and discouraging.

I would agree with Gavin on the other thread about the proposed BIP
commenting BIP. Putting more decision power behind a moderated mailing list
and wiki doesn't seem like a good idea.

On Tue, Feb 2, 2016 at 9:16 AM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

> On Feb 2, 2016 18:04, "Peter Todd via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> >
> > On Tue, Jan 26, 2016 at 09:44:48AM -0800, Toby Padilla via bitcoin-dev
> wrote:
> > > I really don't like the idea of policing other people's use of the
> > > protocol. If a transaction pays its fee and has a greater than dust
> value,
> > > it makes no sense to object to it.
> >
> > I'll point out that getting a BIP for a feature is *not* a hard
> > requirement for deployment. I'd encourage you to go write up your BIP
> > document, give it a non-numerical name for ease of reference, and lobby
> > wallet vendors to implement it.
> >
> > While I'll refrain from commenting on whether or not I think the feature
> > itself is a good idea, I really don't want people to get the impression
> > that we're gatekeepers for how people choose use Bitcoin.
>
> I'll go further: whatever people have commented here and elsewhere about
> this feature (myself included) are personal opinions on the feature itself,
> in the hope you take the concerns into account.
>
> These comments are not a judgement on whether this should be accepted as a
> BIP. Specifically, the BIP editor should accept a BIP even if he personally
> dislikes the ideas in it, when the criteria are satisfied.
>
> Beyond that, having a BIP accepted does not mean wallets have to implement
> it. That's up to the individual wallet authors/maintainers.
>
> --
> Pieter
>

-------------------------------------
On Thu, Aug 25, 2016 at 2:27 PM, Christian Decker via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> If however
> he is planning to use it as a foothold to further compromise your
> company, send spam or similar, he will likely try to avoid these
> tripwires. [...]

Depends on the value of their activity compared to the value of the coins.
Spamming doesn't pay much.

Covert tripwires would obviously be better, but if shared tripwires
allow you to have 100x the funds available it could be a good
trade-off.


-------------------------------------
Folks:

I don't fully understand this thread, but it sounds like to me it
might be omitting consideration of multi-target attacks. For example,
Tier Nolan's attack
(http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012230.html),
which seems to be the best attack on this thread, seems to start with
one specific public key of an intended victim, but if the attacker is
happy to find a collision with *any* one out of a large number of
potential victims, he gets an advantage proportional to the number of
potential victims.

So it would be wise, in addition to the kind of analysis already done
on this thread (which appears to have already settled at "Yes, we need
> 80-bit security."), to make a nice optimistic estimate of how many
public keys we could eventually have in use. 2⁴⁰? 2⁵⁰? Or maybe be
*very* optimistic, with some added IoT [*] goodness, and budget for
2⁶⁰?

Then we need to budget that many more bits of security to keep the
future attacker's chances of success low enough that the attacker will
never succeed. (Assuming that's our requirement.)

You might enjoy this recent blog post by DJB, legendary cryptographer
who works in this niche of cryptography as well as several other
niches:

http://blog.cr.yp.to/20151120-batchattacks.html

It has some interesting philosophical musings about the "Attacker
Economist" approach. (N.B. My respect for DJB's accomplishments is
tremendous, but that doesn't mean I automatically agree with
everything he says. I haven't made up my mind what I think about this
particular philosophical argument.)

Sincerely,

Zooko

[*] The Internet of Targets


-------------------------------------
Hi

As already mentioned in the recent BIP151 thread
(https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-June/012826.html),
I propose the following authentication scheme to basically allow MITM
detection and rejection in conjunction with BIP151.

The proposed authentication BIP does require BIP151.

The propose BIP does assume, node operators want to build trusted
connections for various reasons.

BIPs mediawiki github page available here:
https://github.com/bitcoin/bips/compare/master...jonasschnelli:2016/07/auth_bip?expand=1

===================================================

  BIP: ???
  Title: Peer Authentication
  Author: Jonas Schnelli <dev@jonasschnelli.ch>
  Status: Draft
  Type: Standards Track
  Created: 2016-03-23

== Abstract ==

This BIP describes a way how peers can authenticate – without opening
fingerprinting possibilities – to other peers to guarantee ownership
and/or allowing to access additional or limited services.

== Motivation ==

We assume peer operators want to limit the access of different services
or increase datastream priorities to a selective subset of peers. Also
we assume peers want to connect to specific peers to broadcast or filter
transactions (or similar action that reveals sensitive informations) and
therefore they want to authenticate the remote peer and make sure that
they have not connected to a MITM.

Benefits with peer authentication:
* Peers could detect MITM attacks when connecting to known peers
* Peers could allow resource hungry transaction filtering only to
specific peers
* Peers could allow access to sensitive information that can lead to
node fingerprinting (fee estimation)
* Peers could allow custom message types (private extensions) to
authenticated peers

A simple authentication scheme based on elliptic cryptography will allow
peers to identify each other and selective allow access to restricted
services or reject the connection if the identity could not be verified.

== Specification ==

The authentication scheme proposed in this BIP uses ECDSA, ___secrets
will never be transmitted___.

___Authentication initialization must only happen if encrypted channels
have been established (according to BIP-151 [1]).___

The ___encryption-session-ID___ is available once channels are encrypted
(according to BIP-151 [1]).

The identity-public-keys used for the authentication must be pre-shared
over a different channel (Mail/PGP, physical paper exchange, etc.). This
BIP does not cover a "trust on first use" (TOFU) concept.

The authentication state must be kept until the encryption/connection
terminates.

Only one authentication process is allowed per connection.
Re-authenticate require re-establishing the connection.

=== Known-peers and authorized-peers database ===
Each peer that supports p2p authentication must provide two users
editable "databases"

# ___known-peers___ contains known identity-public-keys together with a
network identifier (IP & port), similar to the "known-host" file
supported by openssh.
# ___authorized-peers___ contains authorized identity-public-keys

=== Local identity key management ===
Each peer can configure one identity-key (ECC, 32 bytes) per listening
network interface (IPv4, IPv6, tor).
The according identity-public-key can be shared over a different channel
with other node-operators (or non-validating clients) to grant
authorized access.

=== Authentication procedure ===
Authentication after this BIP will require both sides to authenticate.
Signatures/public-keys will only be revealed if the remote peer could
prove that they already know the remote identity-public-key.

# -> Requesting peer sends `AUTHCHALLENGE` (hash)
# <- Responding peer sends `AUTHREPLY` (signature)
# -> Requesting peer sends `AUTHPROPOSE` (hash)
# <- Responding peer sends `AUTHCHALLENGE` (hash)
# -> Requesting peer sends `AUTHREPLY` (signature)

For privacy reasons, dropping the connection or aborting during the
authentication process must not be possible.

=== `AUTHCHALLENGE` message ===
A peer can send an authentication challenge to see if the responding
peer can produce a valid signature with the expected responding peers
identity-public-key by sending an `AUTHCHALLENGE`-message to the remote
peer.

The responding peer needs to check if the hash matches the hash
calculated with his own local identity-public-key. Fingerprinting the
requesting peer is not possible.

32bytes challenge-hash `hash(encryption-session-ID || challenge_type ||
remote-peers-expected-identity-public-key)`

`challenge_type` is a single character. `i` if the
`AUTHCHALLENGE`-message is the first, requesting challenge or `r` if
it's the second, remote peers challenge message.

=== `AUTHREPLY` message ===
A peer must reply an `AUTHCHALLENGE`-message with an `AUTHREPLY`-message.


| 64bytes || signature || normalized comp.-signature || A signature of
the encryption-session-ID done with the identity-key


If the challenge-hash from the `AUTHCHALLENGE`-message did not match the
local authentication public-key, the signature must contain 64bytes of
zeros.

The requesting peer can check the responding peers identity by checking
the validity of the sent signature against with the pre-shared remote
peers identity-public-key.

If the signature was invalid, the requesting peer must still proceed
with the authentication by sending an `AUTHPROPOSE`-message with 32
random bytes.

=== `AUTHPROPOSE` message ===
A peer can propose authentication of the channel by sending an
`AUTHPROPOSE`-message to the remote peer.

If the signature sent in `AUTHREPLY` was invalid, the peer must still
send an `AUTHPROPOSE`-message containing 32 random bytes.

The `AUTHPROPOSE` message must be answered with an
`AUTHCHALLENGE`-message – even if the proposed requesting-peers
identity-public-key has not been found in the authorized_peers database.
In case of no match, the responding `AUTHCHALLENGE`-message must
contains 32 bytes of zeros.


| 32bytes || auth-propose-hash || hash || `hash(encryption-session-ID


== Post-Authentication Re-Keying ==

After the second `AUTHREPLY` message (requesting peers signature ->
responding peer), both clients must re-key the symmetric encryption
according to BIP151 while using ___a slightly  different re-key key
derivation hash___.

They both re-key with `hash(encryption-session-ID ||
old_symmetric_cipher_key || requesting-peer-identity-public-key ||
responding-peer-identity-public-key)`

== Identity-Addresses ==
The peers should display/log the identity-public-key as an
identity-address to the users, which is a base58-check encoded
ripemd160(sha256) hash. The purpose of this is for better visual
comparison (logs, accept-dialogs).
The base58check identity byte is `0x0F` followed by an identity-address
version number (=`0xFF01`).

An identity address would look like
`TfG4ScDgysrSpodWD4Re5UtXmcLbY5CiUHA` and can be interpreted as a remote
peers fingerprint.

== Compatibility ==

This proposal is backward compatible. Non-supporting peers will ignore
the new `AUTH*` messages.

== Example of an auth interaction ==

Before authentication (once during peer setup or upgrade)
# Requesting peer and responding peer create each an identity-keypair
(standard ECC priv/pubkey)
# Requesting and responding peer share the identity-public-key over a
different channel (PGP mail, physical exchange, etc.)
# Responding peer stores requesting peers identity-public-key in its
authorized-peers database (A)
# Requesting peer stores responding peers identity-public-key in its
known-peers database together with its IP and port (B)

Encryption
# Encrypted channels must be established (according to BIP-151 [1])

Authentication
# Requesting peer sends an `AUTHCHALLENGE` message
  AUTHCHALLENGE:
    [32 bytes, hash(encryption-session-ID || "i" ||
<remote-peers-expected-identity-public-key>)]

# Responding peer does create the same hash `(encryption-session-ID ||
"i" || <remote-peers-expected-identity-public-key>)` with its local
identity-public-key
# If the hash does not match, response with an `AUTHREPLY` message
containing 64bytes of zeros.
# In case of a match, response with an `AUTHREPLY` message
  AUTHREPLY:
    [64 bytes normalized compact ECDSA signature (H)] (sig of the
encryption-session-ID done with the identity-key)

# Requesting peer does verify the signature with the
`remote-peers-identity-public-key`
# If the signature is invalid, requesting peer answers with an
`AUTHREPLY` message containing 32 random bytes
# In case of a valid signature, requesting peer sends an `AUTHPROPOSE`
message
  AUTHPROPOSE:
    [32 bytes, hash(encryption-session-ID || "p" ||
<client-identity-public-key>)]

# Responding peer iterates over authorized-peers database (A), hashes
the identical data and looks for a match.
# If the hash does not match, responding peer answer with an
`AUTHCHALLENGE` message containing 32 bytes of zeros.
# In case of a match, responding peer sends an `AUTHCHALLENGE` message
with the hashed client public-key
  AUTHCHALLENGE:
    [32 bytes, hash(encryption-session-ID || "r" ||
<client-identity-public-key>)]
# Requesting peer sends an `AUTHREPLY` message containing 64 bytes of
zeros if server failed to authenticate
# Otherwise, response with signature in the `AUTHREPLY` message
  AUTHREPLY:
    [64 bytes normalized compact ECDSA signature (H)] (sig of the
encryption-session-ID done with the identity-key)
# Responding peer must verify the signature and can grant access to
restricted services.
# Both peers re-key the encryption after BIP151 including the
requesting-peer-identity-public-key and responding-peer-identity-public-key

== Disadvantages ==

The protocol may be slow if a peer has a large authorized-peers database
due to the requirement of iterating and hashing over all available
authorized peers identity-public-keys.

== Reference implementation ==

== References ==

* [1] [[bip-0151.mediawiki|BIP 151: Peer-to-Peer Communication Encryption]]

== Acknowledgements ==
* Gregory Maxwell and Pieter Wuille for most of the ideas in this BIP.

== Copyright ==
This work is placed in the public domain.







-------------------------------------
Pledging to not use patents offensively defeats the point of owning patents.
The point of owning a patent is so that you can use it offensively, either to
prevent competition, or get licensing fees.

Obtaining a patent for defense doesn't make sense. The litigants you need to
worry about do not produce or make anything. Their 'product' is patent lawsuits.

Unless you have a patent on using a mail-merge program to sue people, your
defensive patents are useless in that situation.

On Fri, Oct 14, 2016 at 4:57 AM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Fri, Oct 14, 2016 at 07:38:07AM -0300, Sergio Demian Lerner via bitcoin-dev wrote:
>> I read the DPL v1.1 and I find it dangerous for Bitcoin users. Current
>> users may be confident they are protected but in fact they are not, as the
>> future generations of users can be attacked, making Bitcoin technology
>> fully proprietary and less valuable.
>
> Glad to hear you're taking a conservative approach.
>
> So I assume Rootstock is going to do something stronger then, like
> Blockstream's DPL + binding patent pledge to only use patents defensively?
>
>     https://www.blockstream.com/about/patent_pledge/
>
> Because if not, the DPL is still better than the status quo.
>
>> If you read the DPL v1.1 you will see that companies that join DPL can
>> enforce their patents against anyone who has chosen not to join the DPL.
>> (http://defensivepatentlicense.org/content/defensive-patent-license)
>>
>> So basically most users of Bitcoin could be currently under threat of being
>> sued by Bitcoin companies and individuals that joined DPL in the same way
>> they might be under threat by the remaining companies. And even if they
>> joined DPL, they may be asked to pay royalties for the use of the
>> inventions prior joining DPL.
>>
>> DPL changes nothing for most individuals that cannot and will not hire
>> patent attorneys to advise them on what the DPL benefits are and what
>> rights they are resigning. Remember that patten attorneys fees may be
>> prohibitive for individuals in under-developed countries.
>>
>> Also DPL is revocable by the signers (with only a 180-day notice), so if
>> Bitcoin Core ends up using ANY DPL covered patent, the company owning the
>> patent can later force all new Bitcoin users to pay royalties.
>
> Indeed. However, you're also free to adopt the DPL irrevocably by additionally
> stating that you will never invoke that 180-day notice provision (or more
> humorously, make it a 100 year notice period to ensure any patents expire!).
>
> If you're concerned about this problem, I'd suggest that Rootstock do exactly
> that.
>
>> Because Bitcoin user base grows all the time with new individuals, the sole
>> existence of DPL licensed patents in Bitcoin represents a danger to Bitcoin
>> future almost the same as the existence of non-DPL license patents.
>
> To be clear, modulo the revocability provision, it's a danger mainly to those
> who are unwilling to adopt the DPL themselves, perhaps because they support
> software patents.
>
>> If you're publishing all your ideas and code (public disclosure), you
>> cannot later go and file a patent in most of the world except the US, where
>> you have a 1 year grace period. So we need to do something specific to
>> prevent the publishers filing a US patent.
>
> Again, lets remember that you personally proposed a BIP[1] that had the effect
> of aiding your ASICBOOST patent[2] without disclosing that fact in your BIP nor
> your pull-req[3]. The simple fact is we can't rely solely on voluntary
> disclosure - your own behavior is a perfect example of why not.
>
> [1]: BIP: https://github.com/BlockheaderNonce2/bitcoin/wiki
> [2]: ASICBOOST PATENT https://www.google.com/patents/WO2015077378A1?cl=en
> [3]: Extra nonce pull request: https://github.com/bitcoin/bitcoin/pull/5102
>
>> What we need much more than DPL, we need that every BIP and proposal to the
>> Bitcoin mailing list contains a note that grants all Bitcoin users a
>> worldwide, royalty-free, no-charge, non-exclusive, irrevocable license for
>> the content of the e-mail or BIP.
>
> A serious problem here is the definition of "Bitcoin users". Does Bitcoin
> Classic count? Bitcoin Unlimited? What if Bitcoin forks?
>
> Better to grant _everyone_ a irrevocable license.
>
>
> Along those lines, it'd be reasonable to consider changing the Bitcoin Core
> license to something like an Apache2/LGPL3 dual license to ensure the copyright
> license also has anti-patent protections.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
Based on current GH/s count of 775,464,121 Bitcoin tests 2^80 every 19 days.
log2(775464121*(1000*1000*1000*60*60*24*19)) = ~80.07

I don't fully understand the security model of segwit, so my analysis
will assume that any collision is bad.

>But it also requires O(2^80) storage, which is utterly infeasible

You don't store all 2^80 previous hashes, instead you just hash a seed
value 2^80 times, then look for a cycle.

seed = {0,1}^160
x = hash(seed)

for i in 2^80:
....x = hash(x)
x_final = x

y = hash(x_final)

for j in 2^80:
....if y == x_final:
........print "cycle len: "+j
........break
....y = hash(y)

If at any point x collides with a prior value of x it will form a
cycle. Thus y will also cycle and collide with x_final. j gives you
the cycle length, which allows you find the collision:
hash^(2^80-j)(seed) == hash^(j)(hash^(2^80-j)(seed)).

Worst case:
First loop costs 2**80, second loop costs 2**80=j, finding the
colliding value is 2**80. Total cost 2**80+2**80+2**80 = 2**81.5 and
requires storing less than a kilobyte.

This is a toy example, does not exploit parallelism, time memory trade
offs, can be easily made better, etc...

On Thu, Jan 7, 2016 at 2:02 PM, Gavin Andresen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I'm hoisting this from some private feedback I sent on the segregated
> witness BIP:
>
> I said:
>
> "I'd also use RIPEMD160(SHA256()) as the hash function and save the 12
> bytes-- a successful preimage attack against that ain't gonna happen before
> we're all dead. I'm probably being dense, but I just don't see how a
> collision attack is relevant here."
>
> Pieter responded:
>
> "The problem case is where someone in a contract setup shows you a script,
> which you accept as being a payment to yourself. An attacker could use a
> collision attack to construct scripts with identical hashes, only one of
> which does have the property you want, and steal coins.
>
> So you really want collision security, and I don't think 80 bits is
> something we should encourage for that. Normal pubkey hashes don't have that
> problem, as they can't be constructed to pay to you."
>
> ... but I'm unconvinced:
>
> "But it is trivial for contract wallets to protect against collision
> attacks-- if you give me a script that is "gavin_pubkey CHECKSIG
> arbitrary_data OP_DROP" with "I promise I'm not trying to rip you off, just
> ignore that arbitrary data" a wallet can just refuse. Even more likely, a
> contract wallet won't even recognize that as a pay-to-gavin transaction.
>
> I suppose it could be looking for some form of "gavin_pubkey
> somebody_else_pubkey CHECKMULTISIG ... with the attacker using
> somebody_else_pubkey to force the collision, but, again, trivial contract
> protocol tweaks ("send along a proof you have the private key corresponding
> to the public key" or "everybody pre-commits pubkeys they'll use at protocol
> start") would protect against that.
>
> Adding an extra 12 bytes to every segwit to prevent an attack that takes
> 2^80 computation and 2^80 storage, is unlikely to be a problem in practice,
> and is trivial to protect against is the wrong tradeoff to make."
>
> 20 bytes instead of 32 bytes is a savings of almost 40%, which is
> significant.
>
> The general question I'd like to raise on this list is:
>
> Should we be worried, today, about collision attacks against RIPEMD160 (our
> 160-bit hash)?
>
> Mounting a successful brute-force collision attack would require at least
> O(2^80) CPU, which is kinda-sorta feasible (Pieter pointed out that Bitcoin
> POW has computed more SHA256 hashes than that). But it also requires O(2^80)
> storage, which is utterly infeasible (there is something on the order of
> 2^35 bytes of storage in the entire world).  Even assuming doubling every
> single year (faster than Moore's Law), we're four decades away from an
> attacker with THE ENTIRE WORLD's storage capacity being able to mount a
> collision attack.
>
>
> References:
>
> https://en.wikipedia.org/wiki/Collision_attack
>
> https://vsatglobalseriesblog.wordpress.com/2013/06/21/in-2013-the-amount-of-data-generated-worldwide-will-reach-four-zettabytes/
>
>
> --
> --
> Gavin Andresen
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
The INV scheme used by Bitcoin is not very efficient at all. Once you take into account Bitcoin, TCP (including ACKs), IP, and ethernet overheads, each INV takes 193 bytes, according to wireshark. That's 127 bytes for the INV message and 66 bytes for the ACK. All of this is for 32 bytes of payload, for an "efficiency" of 16.5% (i.e. 83.5% overhead). For a 400 byte transaction with 20 peers, this can result in 3860 bytes sent in INVs for only 400 bytes of actual data.

An improvement that I've been thinking about implementing (after Blocktorrent) is an option for batched INVs. Including the hashes for two txes per IP packet instead of one would increase the INV size to 229 bytes for 64 bytes of payload -- that is, you add 36 bytes to the packet for every 32 bytes of actual payload. This is a marginal efficiency of 88.8% for each hash after the first. This is *much* better.

Waiting a short period of time to accumulate several hashes together and send them as a batched INV could easily reduce the traffic of running bitcoin nodes by a factor of 2, and possibly even more than that. However, if too many people used it, such a technique would slow down the propagation of transactions across the bitcoin network slightly, which might make some people unhappy. The ill effects could likely be mitigated by choosing a different batch size for each peer based on each peer's preferences. Each node could choose one or two peers to which they send INVs in batches of one or two, four more peers in which they send batches of two to four, and the rest in batches of four to eight, for example.

(This is a continuation of a conversation started on https://bitcointalk.org/index.php?topic=1377345 <https://bitcointalk.org/index.php?topic=1377345.new#new>.)

Jonathan

-------------------------------------
On Sat, Feb 06, 2016 at 10:37:30AM -0500, Gavin Andresen via bitcoin-dev wrote:
> 2) People are committing to spinning up thousands of supports-2mb-nodes
> during the grace period.

Why wouldn't an attacker be able to counter-sybil-attack that effort?

Who are these people?


On Sat, Feb 06, 2016 at 12:45:14PM -0500, Gavin Andresen via bitcoin-dev wrote:
> Would Blockstream be willing to help out by running a dozen or two extra
> full nodes?

I'll remind everyone that Bitcoin Core does not condone participation in
network attacks to push controversial protcol changes through. I also
checked with Adam Back, who confirmed Blockstream as a company shares
those views.


For those readers unfamiliar with Sybil attacks, basically what the
above does is prevents nodes from being able to finding peers with
accurate information about what blockchains exist - the above can be
used to prevent nodes from learning about the longest chain for
instance, or the existance of substantial support for a minority chain.
This is why we've advocated giving users sufficient time to actively
opt-in to protocol changes.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698

-------------------------------------


Le 25/08/2016  09:39, Jonas Schnelli via bitcoin-dev a crit :
> (I think this case if not completely unrealistic):
> 
> What would happen, if a user gave out 21 addresses, then address0 had
> receive funds in +180 days after generation where address21 had receive
> funds immediately (all other addresses never received a tx).
> 
> In a scan, address0 would be detected at <address-birthday>+180 days
> which would trigger the resize+20 of the address-lookup-window, but, we
> would require to go back 180day in order to detect received transaction
> of address21 (new lookup-window) in that case.
> 
> Or do I misunderstand something?
> 
> 

That case is not unrealistic; a merchant might generate addresses that
are beyond their gap limit, and orders get filled at a later date.

In that case you will not get the same result when restoring your wallet
in a block-scanning wallet and in Electrum.

The lack of consideration for these cases is another issue with BIP44.


-------------------------------------
> Clearly the primary purpose of BIP0075 is to enshrine a DNSSEC protocol
> for giving wallet addresses memorable names.
>
>
I can't tell if you're being sarcastic or not, but if you aren't, I don't
think this is an accurate description at all. BIP75 is, at its most
simplest, nothing more than an encrypted/encapsulated version of BIP70. All
we did was make it safe for people to exchange BIP70 messages through an
intermediary.

The only identity information included in BIP75 is the pki_data field,
which wasn't even introduced in BIP75--it was already in BIP70. I'm
guessing Peter would also have us remove BIP70 altogether?

-------------------------------------
If clients were designed to warn their users when a soft fork happens, then
it could be done reasonably safely.  The reference client does this (or is
it just for high POW softforks?), but many SPV clients don't.

If there was a delay between version number changing and the rule
activation, at least nodes would get a warning recommending that they
update.

* At each difficulty interval, if 950 of the last 1000 blocks have the new
version number, reject the old version blocks from then on.

* Start new target at 255, the least significant byte must be less than or
equal to the target

* Update target at each difficulty re-targetting

T = ((T << 3) - T) >> 3

This increases the difficulty by around 12.5% per fortnight.   After 64
weeks, the target would reach 0 and stay there meaning that the difficulty
would be 256 times higher than what is given in the header.

An attacker with 2% of the network power could create 5 blocks for every
block produced by the rest of the network.

-------------------------------------
On Thu, Feb 4, 2016 at 12:15 AM, Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Various changes have been made based on initial input.
> Further review and re-review is of course welcome.

These recent edits definitely guide us towards less hard feelings when
comments are offered, without excessive policy structure.

[BIP 2:]
> A process BIP may change status from Draft to Active when it
> achieves rough consensus on the mailing list.

Is this mix of wiki and mailing list intentional?  If so, the wiki
talk page is meant to be a self-curated permanent record of support
and dissent, but second-order reply commentary might fall either on
the wiki or the mailing list?

Mediawiki offers watchlists on a polling model, and there is some
email support [1], but it would be nice of a BIP author to at least
gather new/edited comment titles and report them to bitcoin-dev once a
week, during review.  Someone has to stare at the diffs.

  [1] https://www.mediawiki.org/wiki/Manual:Page_change_notification

BIP 2 should ask that all current and future forums that BIP authors
might choose for review have indisputable records of moderation and
user edits.

Is dump.bitcoin.it a sufficient public record of contentious
moderation or user cross-comment editing?  It seems like as long as
the wiki as a whole is verifiable, it would suffice.


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.13.0rc2 are available from:

    https://bitcoin.org/bin/bitcoin-core-0.13.0/test.rc2/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.13.0rc2

This is a release candidate for a new major version release, bringing new
features, bug fixes, as well as other improvements.

Preliminary release notes for the release can be found at

    https://github.com/bitcoin/bitcoin/blob/0.13/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.13.0.

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues

Notable changes since rc1:

### Build system
- - #8373 `1fe7f40` Fix OSX non-deterministic dmg (theuni)
- - #8358 `cfd1280` Gbuild: Set memory explicitly (default is too low) (MarcoFalke)

### GUI
- - #8407 `45eba4b` Add dbcache migration path (jonasschnelli)

### Wallet
- - #8378 `ebea651` Move SetMinVersion for FEATURE_HD to SetHDMasterKey (pstratem)
- - #8390 `73adfe3` Correct hdmasterkeyid/masterkeyid name confusion (jonasschnelli)
- - #8206 `18b8ee1` Add HD xpriv to dumpwallet (jonasschnelli)
- - #8389 `c3c82c4` Create a new HD seed after encrypting the wallet (jonasschnelli)

### P2P protocol and network code
- - #8408 `b7e2011` Prevent fingerprinting, disk-DoS with compact blocks (sdaftuar)

### Consensus
- - #8412 `8360d5b` libconsensus: Expose a flag for BIP112 (jtimon)

### Mining
- - #8362 `86edc20` Scale legacy sigop count in CreateNewBlock (sdaftuar)

### Block and transaction handling
- - #8381 `f84ee3d` Make witness v0 outputs non-standard (jl2012)

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJXngBiAAoJEHSBCwEjRsmmS5kIAMFiXFua9ruR8Vwu1fNgnWTb
X4tsNOdPScm7jwsFavcwygqZQlDNDURjcocQFcehHgEickBrk6eaplTuB4VJidPG
Aqw+nLrd6M//Ohy+7eke7aCg5/QV7poplM3glwow4gQfoSBvL0ywMEhWEzGL7EPH
FH5pyY9o4QZw5wGdvMWxvYVTLPZkm0W2cSWCHZ0WgzWvTkZ7aMzSQ5F5TXPfjzED
DNuQQRMm9H1H3LJkmWAwjCXLzKNMzjmefLujyEII388s6UoWnA1ufosqb1kMqL+h
kuEelzef4cMBZEvHgfzsvlLmba2DLr7xhwudd3HK2NHSmO/wAUdhbQOQSts9NoY=
=rN68
-----END PGP SIGNATURE-----


-------------------------------------
On Wed, Sep 21, 2016 at 11:32:33AM +0200, Tom wrote:
> Thanks for your email Peter!
> 
> On Tuesday 20 Sep 2016 17:56:44 Peter Todd wrote:
> > On Tue, Sep 20, 2016 at 07:15:45PM +0200, Tom via bitcoin-dev wrote:
> > > === Serialization order===
> > > 
> > > The tokens defined above have to be serialized in a certain order for the
> > > transaction to be well-formatted.  Not serializing transactions in the
> > > order specified would allow multiple interpretations of the data which
> > > can't be allowed.
> > 
> > If the order of the tokens is fixed, the tokens themselves are redundant
> > information when tokens are required; when tokens may be omitted, a simple
> > "Some/None" flag to mark whether or not the optional data has been omitted
> > is appropriate.
> 
> This is addressed in the spec; 
> https://github.com/bitcoinclassic/documentation/blob/master/spec/transactionv4.md
> 
> The way towards that flexibility is to use a generic concept made popular
> various decades ago with the XML format. The idea is that we give each
> field a name and this means that new fields can be added or optional fields
> can be omitted from individual transactions

That argument is not applicable to required fields: the code to get the fields
from the extensible format is every bit as complex as the very simple code
required to deserialize/serialize objects in the current system.

In any case your BIP needs to give some explicit examples of hypothetical
upgrades in the future, how they'd take advantage of this, and what the code to
do so would look like.

> > Also, if you're going to break compatibility with all existing software, it
> > makes sense to use a format that extends the merkle tree down into the
> > transaction inputs and outputs.
> 
> Please argue your case.

See my arguments re: segwit a few months ago, e.g. the hardware wallet txin
proof use-case.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Mon, Aug 08, 2016 at 09:41:27PM +0000, James MacWhyte via bitcoin-dev wrote:
> Wouldn't you lose the ability to assume transactions in the blockchain are
> verified as valid, since miners can't see the details of what is being
> spent and how? I feel like this ability is bitcoin's greatest asset, and by
> removing it you're creating an altcoin different enough to not be connected
> to/supported by the main bitcoin project.

The fact that miners verify transactions is just an optimisation:

    https://petertodd.org/2013/disentangling-crypto-coin-mining

Preventing double-spending however is a fundemental requirement of Bitcoin, and
this proposal does prevent double-spending perfectly well (although there may
be better ways to do it).

The OP's proposal sounds quite similar to my earlier one along similar lines:

    https://petertodd.org/2016/closed-seal-sets-and-truth-lists-for-privacy

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
I can see how it looks but actually most of the underlying libraries have
already been adapted or are almost finished being adapted for segwit. Since
segwit is not live on mainnet, most are not released (either still in PR
form or merged to a development branch). As a software developer, I think
you can appreciate that libraries cant actually release a segwit supported
versions until segwit is released as final in 0.13.1. Obviously consumers
of the libraries cant update for segwit until the libraries are released -
you get the idea. I wouldn't be too concerned about the adoption chart,
it's just very difficult to reflect the actual state of affairs. For
example Trezor is marked as wip, but they have had an updated, but
unreleased firmware version for many months[1]. We are in the process of
planning a migration guide and updating the existing development notes[2].
Additionally, many companies are already planning to update their services
for segwit.

Regarding BIP9, it's purpose is to co-ordinate *miner upgrade* and
activation. The starttime delay is simply designed to avoid miners
signalling before the soft fork has been made available for general
release; so the starttime prevents unreleased software from setting the
version bit prematurely. The whole BIP9 state machine allows predictable
activation. Non mining full nodes are under no constraints to upgrade on a
specific schedule, which is by design of soft forks. Signalling will not
begin until the first diff retarget period after the starttime of 15th Nov.
Practically it means there will be a minimum of 4-6 weeks at the very least
before activation can happen.

[1] https://github.com/bitcoin-core/bitcoincore.org/pull/30#issu
ecomment-217329474
[2] https://bitcoincore.org/en/segwit_wallet_dev/

On Sun, Oct 16, 2016 at 7:20 PM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Sunday, 16 October 2016 09:47:40 CEST Douglas Roark via bitcoin-dev
> wrote:
> > Would I want anyone to lose money due to faulty wallets? Of course not.
> > By the same token, devs have had almost a year to tinker with SegWit and
> > make sure the wallet isn't so poorly written that it'll flame out when
> > SegWit comes along. It's not like this is some untested, mostly unknown
> > feature that's being slipped out at the last minute
>
> There have been objections to the way that SegWit has been implemented for
> a
> long time, some wallets are taking a "wait and see" approach.  If you look
> at the page you linked[1], that is a very very sad state of affairs. The
> vast majority is not ready.  Would be interesting to get a more up-to-date
> view.
> Wallets probably won't want to invest resources adding support for a
> feature
> that will never be activated. The fact that we have a much safer
> alternative
> in the form of Flexible Transactions may mean it will not get activated. We
> won't know until its actually locked in.
> Wallets may not act until its actually locked in either. And I think we
> should respect that.
>
> Even if all wallets support it (and thats a big if), they need to be rolled
> out and people need to actually download those updates.
> This takes time, 2 months after the lock-in of SegWit would be the minimum
> safe time for people to actually upgrade.
>
> 1) https://bitcoincore.org/en/segwit_adoption/
> --
> Tom Zander
> Blog: https://zander.github.io
> Vlog: https://vimeo.com/channels/tomscryptochannel
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
This is actually very useful for LN too, see relevant discussion here

http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011827.html

2016-02-12 11:05 GMT+01:00 Tier Nolan via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org>:
> On Fri, Feb 12, 2016 at 5:02 AM, <jl2012@xbt.hk> wrote:
>>
>> Seems it could be done without any new opcode:
>
>
> The assumption was that the altcoin would only accept standard output
> scripts.  Alice's payment in step 2 pays to a non-standard script.
>
> This is an improvement over the cut and choose, but it will only work for
> coins which allow non-standard scripts (type 2 in the BIP).
>
> I guess I was to focused on maintaining standard scripts on the altcoin.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
On Mon, Jan 18, 2016 at 10:02:51PM +1000, Anthony Towns via bitcoin-dev wrote:
> I think these numbers are slightly mistaken -- I was only aware of version
> 1 segwit scripts at the time, and assumed 256 bit hashes would be used
> for all segwit transaction, however version 0 segwit txns would be more
> efficient for p2pkh, with the same security as bitcoin currently has
> (which seems fine).

Latest segwit code just has version 0 witness format, and treats a 32
byte push as the sha256 of a script, and a 20 byte push as the hash of
the pub key. Also, the witness scriptPubKey format uses "OP_0 [hash]" to
push the version and hash to the script separately, rather than "[0x00
script]" or "[0x01 hash]" (this changes allows segwit transactions to
be encoded backwards compatibly as a p2sh payment).

> p2pkh:
>   segwit: 10+41i+36o + 0.25*105*i [1]

> [0] 10 bytes for version (4), input count (1), output count (1) and
>     locktime (4); 146 bytes per input consisting of tx hash (32), txout
>     index (4), script length (1), scriptsig (signature and pubkey =
>     105), CHECKSIG = 25), and sequence number (4); 34 bytes per output
>     consisting of value (8), script length (1) and scriptpubkey (DUP
>     HASH160 PUSH20 EQVERIFY CHECKSIG = 25).

> [1] Same as [0], except two extra bytes per output script (segwit push
>     and segwit version byte), and moving the 105 bytes of signature
>     script directly into the segregated witness

So this change means segwit p2pkh needs 31 bytes per output not 36 bytes (value,
length stay the same, scriptpubkey becomes "OP_0 PUSH20" for 22 bytes
instead of 25+2 bytes). This gives another couple of percent gain, so:

    segwit: 10+41i+31o + 0.25*105*i [1]

Setting i=o makes:

>         p2pkh           2-of-2 msig
> now     10+180i         10+286i
> segwit  10+104i         10+140i

become:

segwit    10+99i          10+140i

and therefore,

>         p2pkh           2-of-2 msig
> now     100%            100%
> segwit  166%-173%       197%-204%

becomes:

segwit    174%-181%       197%-204%

Constantly creeping up! Pretty nice.

Also, p2pkh with segwit-via-p2sh is probably interesting, those numbers
work out as:

segwit:   10+41i+31o + 0.25*105*i (for comparison)
segp2sh:  10+60i+32o + 0.25*105*i [0]
  ->      10+119i
  ->      147%-151%

So that still looks like a reasonable improvement even if (eg) in the
short term merchants are the only ones that upgrade, and customers just
use non-segwit-aware wallets with a p2sh address that's only redeemable
by a segwit-aware wallet.

Cheers,
aj

[0] 10 bytes standard. For each input, tx hash (32) plus index (4),
    script length (1) and scriptsig which is a push of the standard segwit
    pubscript (22+1) totaling to 60, and witness data is the same as for
    normal segwit (105). Each output is standard p2sh, which is value
    (8), length (1) and script (23) for a total of 32.

Cheers,
aj



-------------------------------------

> On Jun 28, 2016, at 10:36 PM, Peter Todd <pete@petertodd.org> wrote:
> 
>> On Tue, Jun 28, 2016 at 10:29:54PM +0200, Eric Voskuil wrote:
>> 
>> 
>>>> On Jun 28, 2016, at 10:14 PM, Peter Todd <pete@petertodd.org> wrote:
>>>> 
>>>> On Tue, Jun 28, 2016 at 08:35:26PM +0200, Eric Voskuil wrote:
>>>> Hi Peter,
>>>> 
>>>> What in this BIP makes a MITM attack easier (or easy) to detect, or increases the probability of one being detected?
>>> 
>>> BIP151 gives users the tools to detect a MITM attack.
>>> 
>>> It's kinda like PGP in that way: lots of PGP users don't properly check keys,
>> 
>> PGP requires a secure side channel for transmission of public keys. How does one "check" a key of an anonymous peer? I know you well enough to know you wouldn't trust a PGP key received over an insecure channel.
>> 
>> All you can prove is that you are talking to a peer and that communications in the session remain with that peer. The peer can be the attacker. As Jonas has acknowledged, authentication is required to actually guard against MITM attacks.
> 
> Easy: anonymous peers aren't always actually anonymous.
> 
> A MITM attacker can't easily distinguish communications between two nodes that
> randomly picked their peers, and nodes that are connected because their operators manually used -addnode to peer; in the latter case the operators can
> check whether or not they're being attacked with an out-of-band key check.

An "out of band key check" is not part of BIP151. It requires a secure channel and is authentication. So BIP151 doesn't provide the tools to detect an attack, that requires authentication. A general requirement for authentication is the issue I have raised.

e

-------------------------------------



 ---- On Mon, 17 Oct 2016 04:08:29 +0800 Tom Zander via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote ---- 
 > On Monday, 17 October 2016 03:11:23 CEST Johnson Lau wrote: 
 > > > Honestly, if the reason for the too-short-for-safety timespan is that 
 > > > you 
 > > > want to use BIP9, then please take a step back and realize that SegWit 
 > > > is a contriversial soft-fork that needs to be deployed in a way that is 
 > > > extra safe because you can't roll the feature back a week after 
 > > > deployment. All transactions that were made in the mean time turn into 
 > > > everyone-can- spent transactions. 
 > >  
 > > No one should use, nor anyone is advised to use, segwit transactions 
 > > before it is fully activated.  
 >  
 > Naturally, I fully agree. 
 >  
 > It seems I choose the wrong words, let me rephrase; 
 >  
 > You can't roll the SegWit back a week after people are allowed to send  
 > segwit transactions (lock-in + fallow period). All transactions that were  
 > made in the mean time turn into everyone-can- spent transactions. 
 > 
 > Because the network as a whole and any implementation is unable to roll back  
 > in an environment where SegWit is a contriversial soft-fork, it is super  
 > important to make sure that it is properly supported by all miners. This  
 > takes time and the risk you take by pushing this is that actual real people  
 > loose actual real money because of the issue I outlined inthe previous  
 > paragraph. 
 
It would only happen if a large proportion of miners are false-signalling, like how BU did with BIP109 and forked your Classic away on testnet

But this is a egg-and-chicken problem and extending the grace period would not have any improvement. Until the rules are fully activated, it is totally impossible to tell if some miners are false signalling. The only method to prevent it, as usual, is the majority of miners will orphan the blocks of malicious miners. Like in the last year, some miners did not correctly implement BIP66 and got punished by losing many blocks.

If your are suggesting >51% of miners may false-signal (like in the BIP109 case), we already have a much bigger problem.

If people are really worrying about that, I would advise them not to use segwit extensively at the beginning, and wait for at least a week to see any sign of false signalling (which will be shown as invalid orphaned blocks). If the grace period was 2 weeks, they need to wait for 3 weeks; if the grace period was 2 months, they need to wait for 2 months and a week. Pre-activation consensus-imposed grace period could never replace post-activation self-imposed observation period



-------------------------------------

> The development paradigm of "maybe detect funds" is not something we
> should *not* encourage for Bitcoin IMO.

Sorry. That was one "not" to many.

</jonas>


-------------------------------------
Folks,

I think the current situation with forks could have been avoided with a better process that can distinguish between different layers for bitcoin modification proposals.

For instance, BIP64 was proposed by Mike Hearn, which does not affect the consensus layer at all. Many Core devs disliked the proposal and Mike had lots of pushback. Regardless of whether or not you agree with the merits of Mike’s ideas here, fact is having nodes that support BIP64 would not fundamentally break the Bitcoin network.

This issue prompted Mike to break off from Core and create XT as the applications he was developing required BIP64 to work. With this split, Gavin found a new home for his big block ideas…and the two teamed up.

We need to have a process that clearly distinguishes these different layers and allows much more freedom in the upper layers while requiring agreement at the consensus layer. Many of these fork proposals are actually conflating different features, only some of which would actually be consensus layer changes. When people proposing nonconsensus features get pushback from Core developers they feel rejected and are likely to team up with others trying to push for hard forks and the like.

A while back I had submitted a BIP -  BIP123 - that addresses this issue. I have updated it to include all the currently proposed and accepted BIPs and have submitted a PR: https://github.com/bitcoin/bips/pull/311 <https://github.com/bitcoin/bips/pull/311>

I urge everyone to seriously consider getting this BIP accepted as a top priority before we get more projects all trying their hand at stuff and not understanding these critical distinctions.


- Eric

-------------------------------------
On Tuesday, February 09, 2016 10:00:44 PM Matt Corallo via bitcoin-dev wrote:
> Indeed, we could push for more place by just always having one 0-byte,
> but I'm not sure the added complexity helps anything? ASICs can never be
> designed which use more extra-nonce-space than what they can reasonably
> assume will always be available, so we might as well just set the
> maximum number of bytes and let ASIC designers know exactly what they
> have available. Currently blocks start with at least 8 0-bytes. We could
> just say minimum difficulty is now 6 0-bytes (2**16x harder) and reserve
> those?

The extranonce rolling doesn't necessarily need to happen in the ASIC itself. 
With the current extranonce-in-gentx, an old RasPi 1 can only handle creating 
work for up to 5 Gh/s with a 500k gentx.

Furthermore, there is a direct correlation between ASIC speeds and difficulty, 
so increasing the extranonce space dynamically makes a lot of sense.

I don't see any reason *not* to increase the minimum difficulty at the same 
time, though.

Luke


-------------------------------------
I do like that the volume of emails has been reduced substantially. I used
to delete hordes of dev emails because I couldn't keep up. At least now I
feel like I'm able to skim most things that look interesting and I get to
assume that if the subject seems relevant to me the content is worthwhile.

My life has improved because of the changes.
On Jan 23, 2016 8:08 PM, "Dave Scotese via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> +1
> The distinction we are making importantly requires that contributors
> provide readers with another thing to say in favor of something - another
> thing which is different than "X people support this instead of only X-1
> people."  Evidence trumps votes.
>
> On Sat, Jan 23, 2016 at 1:38 PM, Gavin via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>>
>> > On Jan 23, 2016, at 3:59 PM, Peter Todd via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>> >
>> > I would extend this to say that the technical explanation also should
>> > contribute uniquely to the conversation; a +1 with an explanation
>> > the last +1 gave isn't useful.
>>
>> Yes, comments should contribute to the discussion, with either technical
>> discussion or additional relevant data. I think a +1 like the following
>> should be encouraged:
>>
>> "+1: we had eleven customer support tickets in just the last week that
>> would have been prevented if XYZ.
>>
>> Jane Doe, CTO CoinBitChainBasely.com"
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
>
> --
> I like to provide some work at no charge to prove my value. Do you need a
> techie?
> I own Litmocracy <http://www.litmocracy.com> and Meme Racing
> <http://www.memeracing.net> (in alpha).
> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>
> which now accepts Bitcoin.
> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
> "He ought to find it more profitable to play by the rules" - Satoshi
> Nakamoto
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
There's  an absurd fee (non-consensus) check already. Maybe that check can
be improved, but probably the wallet layer is more appropriate for this.
On Mar 3, 2016 16:23, "Henning Kopp via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi,
> I think there is no need to do a hardfork for this. Rather it should
> be implemented as a safety-mechanism in the client. Perhaps a warning
> can pop up, if one of your conditions A) or B) is met.
>
> All the best
> Henning Kopp
>
>
> On Thu, Mar 03, 2016 at 05:02:11AM -0800, Alice Wonder via bitcoin-dev
> wrote:
> > I think the next hard fork should require a safety rule for TX fees.
> >
> >
> https://blockchain.info/tx/6fe69404e6c12b25b60fcd56cc6dc9fb169b24608943def6dbe1eb0a9388ed08
> >
> > 15 BTC TX fee for < 7 BTC of outputs.
> >
> > Probably either a typo or client bug.
> >
> > My guess is the user was using a client that does not adjust TX fee, and
> > needed to manually set it in order to get the TX in the block sooner, and
> > meant 15 mBTC or something.
> >
> > I suggest that either :
> >
> > A) TX fee may not be larger than sum of outputs
> > B) TX fee per byte may not be larger than 4X largest fee per byte in
> > previous block
> >
> > Either of those would have prevented this TX from going into a block.
> >
> > Many people I know are scared of bitcoin, that they will make a TX and
> make
> > a mistake they can't undo.
> >
> > Adding protections may help give confidence and there is precedence to
> doing
> > things to prevent typo blunders - a public address has a four byte
> checksum
> > to reduce the odds of a typo.
> >
> > This kind of mistake is rare, so a fix could be included in the coming HF
> > for the possible July 2017 block increase.
> >
> > Thank you for your time.
> >
> > Alice Wonder
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
>
> --
> Henning Kopp
> Institute of Distributed Systems
> Ulm University, Germany
>
> Office: O27 - 3402
> Phone: +49 731 50-24138
> Web: http://www.uni-ulm.de/in/vs/~kopp
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Thanks for starting this discussion, Erik.


> Should this be a new BIP?  I know netki's BIP75 is out there - but I think
> it's too specific and too reliant on the domain name system.
>

This is not quite accurate. BIP75 is designed to be independent of any name
resolution system. You could use it with a static URL that you share, for
example, or even use it to implement a mesh-network payment system over
bluetooth. Netki's wallet names do use DNS, but that isn't related to this
discussion.

What BIP75 *does* do is provide a way for a client to get a new payment
address for every payment. I personally think it is better than BIP47 for
the uses you mentioned (subscriptions, etc).

I'm glad you brought up identity methods other than x509. At breadwallet we
are thinking about how to establish the most universal system, and letting
users identify themselves with any of a selection of identity systems is
ideal. I think the pki_data slot should be constantly expanded to allow new
identity types, but they should be explained/standardized in the BIPs that
add them and use universal names. "netki://" wouldn't be appropriate, for
example, if their method is open sourced and possibly used by others--it
should instead be given a product name like "dnswallet://" or something
more clever.

James

-------------------------------------
On 08/05/16 15:48, Marek Palatinus via bitcoin-dev wrote:
> unambiguously be used to refer to an idea. My suggestion would be to write
> a new BIP that overrides parts of BIP32, and then put a note in BIP32 that
> a better mechanism is available that is unlikely to change things in
> reality for the secp256k1 curve.

I guess, we'll write that down to SLIP-0032 then.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
On Thu, Aug 11, 2016 at 2:55 PM, Erik Aronesty via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Sorr, I thought there was some BIP for a public seed such that someone can
> generate new random addresses, but cannot trivially verify whether an
> address was derived from the seed.
>

If you take a public key and multiply it by k, then the recipient can work
out the private key by multiplying their master private key by k.

If k is random, then the recipient wouldn't be able to work it out, but if
it is non-random, then everyone else can work it out.  You need some way to
get k to the recipient without others figuring it out.

This means either the system is interactive or you use a shared secret.

The info about the shared secret is included in the scriptPubKey (or the
more socially conscientious option, an OP_RETURN).

The address would indicate the master public key.

master_public = master_private * G

The transaction contains k*G.

Both sides can compute the shared secret.

secret = k*master_private*G = master_private*k*G

<encode(k*G)> DROP DUP HASH160 <hash160(encode(secret + pub key))>
EQUALVERIFY CHECKSIG

This adds 34 bytes to the scriptPubKey.

This is pretty heavy for scanning for transactions sent to you.  You have
to check every transaction output to see if it is the given template.  Then
you have to do an ECC multiply to compute the shared secret.  Once you have
the shared secret, you need to do an ECC addition and a hash to figure out
if it matches the public key hash in the output.

This is approx one ECC multiply per output and is similar CPU load to what
you would need to do to actually verify a block.

-------------------------------------
On 2016/10/16 09:35, Gavin Andresen via bitcoin-dev wrote:
> I asked a lot of businesses and individuals how long it would take them
> to upgrade to a new release over the last year or two.
> 
> Nobody said it would take them more than two weeks.
> 
> If somebody is running their own validation code... then we should
> assume they're sophisticated enough to figure out how to mitigate any
> risks associated with segwit activation on their own.

In addition, there has been a page up for several months
(https://bitcoincore.org/en/segwit_adoption/) that gauges whether or not
wallets are ready for SegWit. Unfortunately, it appears that the page
hasn't been updated since May. I do know that several wallets have
finished or are close to finishing their support, though.

Would I want anyone to lose money due to faulty wallets? Of course not.
By the same token, devs have had almost a year to tinker with SegWit and
make sure the wallet isn't so poorly written that it'll flame out when
SegWit comes along. It's not like this is some untested, mostly unknown
feature that's being slipped out at the last minute, unlike other
features I could name but won't. :)

-- 
---
Douglas Roark
Cryptocurrency, network security, travel, and art.
https://onename.com/droark
joroark@vt.edu
PGP key ID: 26623924


-------------------------------------
> I understand the use, when coupled with a yet-to-be-devised identity system, with Bloom filter features. Yet these features

This is a bit of a strawman, you've selected a single narrow usecase
which isn't proposed by the BIP and then argue it is worthless. I
agree that example doesn't have much value (and I believe that
eventually the BIP37 bloom filters should be removed from the
protocol).

Without something like BIP151 network participants cannot have privacy
for the transactions they originate within the protocol against
network observers. Even if, through some extraordinary effort, their
own first hop is encrypted, unencrypted later hops would rapidly
expose significant information about transaction origins in the
network.

Without something like BIP151 authenticated links are not possible, so
manually curated links (addnode/connect) cannot be counted on to
provide protection against partitioning sybils.

Along the way BIP151 appears that it will actually make the protocol faster.

> Given that the BIP relies on identity

This is untrue. The proposal is an ephemerally keyed opportunistic
encryption system. The privacy against a network observer does not
depend on authentication, much less "identity".  And when used with
authentication at all it makes interception strongly detectable after
the fact.

> The BIP does not [...] contemplate the significant problems associated with key distribution in any identity system

Because it does not propose any "identity system" or authorization
(also, I object to your apparent characterization of authentication as
as an 'identity system'-- do you also call Bitcoin addresses an
identity system?).

That said, manually maintaining adds nodes to your own and somewhat
trusted nodes is a recommend best practice for miners and other high
value systems which is rendered much less effective due to a lack of
authentication, there is no significant key distribution problem in
that case, and I expect the future auth BIP (Jonas had one before, but
it was put aside for now to first focus on the link layer encryption)
to address that case quite well.


-------------------------------------
I am happy to announce the first successful Zero-Knowledge Contingent
Payment (ZKCP) on the Bitcoin network.

ZKCP is a transaction protocol that allows a buyer to purchase
information from a seller using Bitcoin in a manner which is private,
scalable, secure, and which doesn’t require trusting anyone: the
expected information is transferred if and only if the payment is
made. The buyer and seller do not need to trust each other or depend
on arbitration by a third party.

Imagine a movie-style “briefcase swap” (one party with a briefcase
full of cash, another containing secret documents), but without the
potential scenario of one of the cases being filled with shredded
newspaper and the resulting exciting chase scene.

An example application would be the owners of a particular make of
e-book reader cooperating to purchase the DRM master keys from a
failing manufacturer, so that they could load their own documents on
their readers after the vendor’s servers go offline. This type of sale
is inherently irreversible, potentially crosses multiple
jurisdictions, and involves parties whose financial stability is
uncertain–meaning that both parties either take a great deal of risk
or have to make difficult arrangement. Using a ZKCP avoids the
significant transactional costs involved in a sale which can otherwise
easily go wrong.

In today’s transaction I purchased a solution to a 16x16 Sudoku puzzle
for 0.10 BTC from Sean Bowe, a member of the Zcash team, as part of a
demonstration performed live at Financial Cryptography 2016 in
Barbados. I played my part in the transaction remotely from
California.

The transfer involved two transactions:

8e5df5f792ac4e98cca87f10aba7947337684a5a0a7333ab897fb9c9d616ba9e
200554139d1e3fe6e499f6ffb0b6e01e706eb8c897293a7f6a26d25e39623fae

Almost all of the engineering work behind this ZKCP implementation was
done by Sean Bowe, with support from Pieter Wuille, myself, and Madars
Virza.


Read more, including technical details at
https://bitcoincore.org/en/2016/02/26/zero-knowledge-contingent-payments-announcement/

[I hope to have a ZKCP sudoku buying faucet up shortly. :) ]


-------------------------------------
On 11/17/2016 12:44 AM, Peter Todd wrote:
> On Wed, Nov 16, 2016 at 04:43:08PM -0800, Eric Voskuil via bitcoin-dev wrote:
>>> This means that all future transactions will have different txids...
>> rules do guarantee it.
>>
>> No, it means that the chance is small, there is a difference.
>>
>> If there is an address collision, someone may lose some money. If there
>> is a tx hash collision, and implementations handle this differently, it
>> will produce a chain split. As such this is not something that a node
>> can just dismiss. If they do they are implementing a hard fork.
> 
> If there is a tx hash collision it is almost certainly going to be because
> SHA256 has become weak..

Almost certainly is not certainly. Hash collisions happen because of chance.

BIP30 is quite explicit:

> "Fully-spent transactions are allowed to be duplicated in order not to
hinder pruning at some point in the future. Not allowing any transaction
to be duplicated would require evidence to be kept for each transaction
ever made."

BIP34 motivations:

> "2. Enforce block and transaction uniqueness, and assist unconnected
block validation."

But it only specifies making collisions harder, not impossible (i.e.
explicitly rejected by consensus).

Are you proposing that we draft a new BIP that allows us all to not have
to code for this? If we do so it will be impossible to guard against a
chain split due to pruning, but you seem to think that's unimportant.

e


-------------------------------------
On Wed, Mar 2, 2016 at 8:56 AM, Luke Dashjr wrote:

> We are coming up on the subsidy halving this July, and there have been some
>

Luke,

One reason "hard-fork to fix difficulty drop algorithm" could be
controversial is that the proposal involves a hard-fork (perhaps
necessarily so, at my first and second glance). There are a number of
concerns with hard-forks including security, deployment, participation,
readiness measurement, backwards incompatibility, etc. In fact, some
Bitcoin Core developers believe that hard-forks are not a good idea and
should not be used.

# Hard-forks

An interesting (unspoken?) idea I’ve heard from a few people has been “we
should try to avoid all hard-forks because they are backwards
incompatible”, another thought has been "there should only be one more
hard-fork if any" and/or "there should only be one hard-fork every 30
years". I also recognize feedback from others who have mentioned "probably
unrealistic to expect that the consensus layer can be solidified this early
in Bitcoin's history". At the same time there are concerns about “slippery
slopes”....

Also, if you are going to participate in a hard-fork then I think you
should make up some proposals for ensure minimal monetary loss on the old
(non-hard-forked) chain, especially since your proposed timeline is so
short seems reasonable to expect even more safety-related due diligence to
minimize money loss (such as using a new address prefix on the hard-forked
upgrade). Anyway, it should be clear that hard-forks are an unsettled issue
and are controversial in ways that I believe you are already aware about.

# Have miners gradually reduce their hashrate instead of using a step
function cliff

adam3us recently proposed that miners who are thinking of turning off
equipment should consider gradually ramping down their hashrate, as a show
of goodwill (and substantial loss to themselves, similar to how they would
incur losses from no longer mining after the halving). This is not
something the consensus algorithm can enforce at the moment, and this
suggestion does not help under adversarial conditions. Since this
suggestion does not require a hard-fork, perhaps some effort should be made
to query miners and figure out if they need assistance with implementing
this (if they happen to be interested).

# Contingency planning

Having said all of the negative things above about hard-forks, I will add
that I do actually like the idea of having backup plans available and
tested and gitian-built many weeks ahead of expected network event dates.
Unfortunately this might encourage partial consensus layer hard-forks in
times of extreme uncertainty such as "emergencies".... creating an even
further emergency.

# "Indefinite backlog growth"

You write "the backlog would grow indefinitely until the adjustment
occurs". This seems to be expected behavior regardless of difficulty
adjustment (in fact, a backlog could continue to grow even once difficulty
adjusts downward), and the consensus protocol does not commit to
information regarding that backlog anyway...

# Difficulty adjustment taking time is expected

This is an expected part of the protocol, it's been mentioned since
forever, it's well known and accounted for. Instead, we should be providing
advice to users about which alternative payment systems they should be
using if they expect instantaneous transaction confirmations. This has been
a long-standing issue, and rolling out a hard-fork is not going to fix
mistaken assumptions from users. They will still think that confirmations
were meant to be instantaneous regardless of how many hard-forks you choose
to deploy.

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
Yes, it makes sense. A BIP is something people refer to, either just by
its number or by URL, and with multiple orthogonal "sub-BIPs" it's
difficult to refer to. We have this problem with BIP32 already -- all HD
wallets implement the derivation part of BIP32 but almost none do
implement the hierarchy part (and use BIP43/44 instead). I tried to
split up BIP32 into two BIPs later (without any content changes), but it
was declined because of its final state.

There is no harm in using a BIP only for a small thing, BIP numbers are
infinite.


On 03/11/2016 08:32 PM, James MacWhyte via bitcoin-dev wrote:
> That's a valid point, and one we had thought of, which is why I wanted
> to get everyone's opinion. I agree the proposed field extensions have
> nothing to do with encryption, but does it make sense to propose a
> completely separate BIP for such a small thing? If that is the accepted
> way to go, we can split it into two and make a separate proposal.
> 
> On Fri, Mar 11, 2016 at 5:48 AM Andreas Schildbach via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>     I think it's a bad idea to pollute the original idea of this BIP with
>     other extensions. Other extensions should go to separate BIPs,
>     especially since methods to clarify the fee have nothing to do with
>     secure and authenticated bi-directional BIP70 communication.
> 
> 
>     On 03/10/2016 10:43 PM, James MacWhyte via bitcoin-dev wrote:
>     > Hi everyone,
>     >
>     > Our BIP (officially proposed on March 1) has tentatively been assigned
>     > number 75. Also, the title has been changed to "Out of Band Address
>     > Exchange using Payment Protocol Encryption" to be more accurate.
>     >
>     > We thought it would be good to take this opportunity to add some
>     > optional fields to the BIP70 paymentDetails message. The new
>     fields are:
>     > subtractable fee (give permission to the sender to use some of the
>     > requested amount towards the transaction fee), fee per kb (the minimum
>     > fee required to be accepted as zeroconf), and replace by fee
>     (whether or
>     > not a transaction with the RBF flag will be accepted with zeroconf). I
>     > know it doesn't make much sense for merchants to accept RBF with
>     > zeroconf, so that last one might be used more to explicitly refuse RBF
>     > transactions (and allow the automation of choosing a setting based on
>     > who you are transacting with).
>     >
>     > I see BIP75 as a general modernization of BIP70, so I think it
>     should be
>     > fine to include these extensions in the new BIP, even though these
>     > fields are not specific to the features we are proposing. Please
>     take a
>     > look at the relevant section and let me know if anyone has any
>     concerns:
>     >
>     https://github.com/techguy613/bips/blob/master/bip-0075.mediawiki#Extending_BIP70_PaymentDetails
>     >
>     > The BIP70 extensions page in our fork has also been updated.
>     >
>     > Thanks!
>     >
>     > James
>     >
>     >
>     > _______________________________________________
>     > bitcoin-dev mailing list
>     > bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>     >
> 
> 
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 




-------------------------------------
In a previous thread ("New BIP: Dealing with OP_IF and OP_NOTIF
malleability in P2WSH") it was briefly discussed what happens if someone
modifies segwit data during transmission. I think the discussion should
continue.

What worries me is what happens with non-segwit transactions after segwit
is activated. I've followed the code from transaction arrival to
transaction relay and it seems that a malicious node could receive a
non-segwit tx, and re-format it into a segwit tx having as high as 400
Kbytes of segwit witness program data, and then relay it. Both transaction
would have the same hash.

The MAX_SCRIPT_ELEMENT_SIZE limit is only enforced on segwit execution, not
in old non-segwit execution, so witness program stack elements could be as
large as 400 Kbytes (MAX_STANDARD_TX_WEIGHT prevents increasing more).
Such large modified transaction will probably not be properly relayed by
the network due too low fee/byte, so the honest miner will probably win and
forward the original transaction through the network.
But if the attacker has better connectivity with the network and he
modifies the original transaction adding segwit witness program data only
up to the point where the transaction is relayed but miners are discouraged
to include it in blocks due to low fees/byte, then the attacker has
successfully prevented a transaction from being mined (or at least it will
take much more).

Also an attacker can encode arbitrary data (such as virus signatures or
illegal content) into passing non-segwit transactions.

One solution would be to increase the transaction version to 3 for segwit
transactions, so a non-segwit transaction cannot be converted into a segwit
transaction without changing the transaction hash. But this seems not to be
a good solution, because it does not solve all the problems. Transactions
having a mixture of segwit and non-segwit inputs could suffer the same
attack (even if they are version 3).

I proposed that a rule is added to IsStandardTX() that prevents witness
programs of having a stack elements of length greater than
MAX_SCRIPT_ELEMENT_SIZE. (currently this is not a rule)

That's a simple check that prevents most of the problems.

A long term solution would be to add the maximum size of the witness stack
in bytes (maxWitnessSize) as a field for each input, or as a field of the
whole transaction.

Regards

-------------------------------------
On Saturday, February 06, 2016 3:37:30 PM Gavin Andresen wrote:
> I suspect there ARE a significant percentage of un-maintained full nodes--

Do you have evidence these are intentionally unmaintained, and not users who 
have simply not had time to review and decide on upgrading?

> There is broad agreement that a capacity increase is needed NOW.

If so, it is only based on misinformation. I am concerned you are implying 
this conclusion is true. When I spoke with you maybe a year ago with my 
concerns that block size might grow too fast, you suggested that the miners 
could be trusted to not increase the block size until necessary (which is not 
likely to be any time soon, despite the massive misinformation campaigns out 
there).

> On Sat, Feb 6, 2016 at 1:12 AM, Luke Dashjr via bitcoin-dev
> > > > Miners express their support for this BIP by ...
> > > 
> > > But miners don't get to decide hardforks. How does the economy
> > > express their support for it? What happens if miners trigger it
> > > without consent from the economy?
> 
> "The economy" does support this.

I have seen evidence which suggests the contrary. For example:
    https://twitter.com/barrysilbert/status/694911989701861376


Where is yours?

> > If you are intent on using the version bits to trigger the
> > hardfork, I suggest rephrasing this such that miners should
> > only enable the bit when they have independently confirmed
> > economic support (this means implementations need a config
> > option that defaults to off).
> 
> Happy to add words about economic majority.
> 
> Classic will not implement a command-line option (the act of running
> Classic is "I opt in"), but happy to add one for a pull request to Core,
> assuming Core would not see such a pull request as having any hostile
> intent.

But this isn't about the miner opting in, it is about the miner *observing 
economic support* for the change. I have successfully downloaded Bitcoin 
Classic's beta binaries without ANY warning that by running it, I am 
expressing that I believe the economy has approved of a hardfork.

> > > SPV (simple payment validation) wallets are compatible with this
> > > change.
> > 
> > Would prefer if this is corrected to "Light clients" or something.
> > Actual SPV wallets do not exist at this time, and would not be
> > compatible with a hardfork.
> 
> Is there an explanation of SPV versus "Light Client" written somewhere more
> permanent than a reddit comment or forum post that I can point to?

Not that I am aware of. (But both reddit comments and forum posts have  
outlived many other posts, such as blogs, so I'm not sure why to exclude them 
specifically...)

In any case, since SPV nodes don't exist, there is probably no real need to 
address them. Everyone will know what "light client" means.
 
> > I would also prefer to see any hardfork:
> > 2. Be deployed as a soft-hardfork so as not to leave old nodes entirely
> > insecure.
> 
> I haven't been paying attention to all of the
> "soft-hardfork/hard-softfork/etc" terminology so have no idea what you
> mean. Is THAT written up somewhere?

Working on a BIP draft for it, but it's not ready for publication yet. The 
basic idea is to turn the merkle root in the block header into simply a hash 
of a second block header, which is constructed to parse as a valid empty 
generation transaction under the old rules. Thus, old nodes see the forked 
blockchain as valid with continually growing work on it, but as if the blocks 
were all empty. This protects them from attackers mining a short blockchain 
they perceive as valid.

Luke


-------------------------------------
Hi

> That's a valid concern, but I don't see the conflict here. In order to
> recover funds from a wallet conforming to BIPXX, you must have wallet
> software that handles BIPXX. Simply making BIPXX backwards compatible
> with previously created BIP44 or BIP43 purpose 0 wallets doesn't change
> this at all.

Maybe I'm going a bit offtopic. Sorry for that.

Importing a bip32 wallet (bip44 or not) is still an expert job IMO.
Also importing can lead to bad security practice (especially without a
sweep).

Users will send around xpriv or import an seed over a compromised
computer to a cold storage, etc.

I don't think users want to import private keys.
They probably want to import the transaction history and send all funds
covered by that seed to a new wallet.

I often though that task is better covered by a little GUI tool or
cli-app/script:
-> Accept different bip32 schematics (bip32 native, bip44, etc.)
-> Accept different bip39 (like) implementation
-> Create large lookup windows
-> Create a sweep transaction to a new address/wallet and sign/broadcast it.
-> Export transaction history (CSV)

But maybe I'm over-complicating things.

--
</jonas>


-------------------------------------
Here is my thinking.

The BIP process is about changes to a living project which is the bitcoin 
prptocol.
This specific BIP got accepted and we know in the blockchain that
this event (the acceptance) is recorded.
Before a certain block the rules were one way, after they were changed.

I have no problem with changing the *code* to be less complex because it 
already knows the past. A checkpoint is the same, it is the registeration of 
a past event.
This makes software less complex and still capable of checking the entire 
blockchain from genesis.

I don’t see any harm in this change. I see prudent software engineering 
practices.


On Monday, 14 November 2016 10:47:35 CET Eric Voskuil via bitcoin-dev wrote:
> NACK
> 
> Horrible precedent (hardcoding rule changes based on the assumption that
> large forks indicate a catastrophic failure), extremely poor process
> (already shipped, now the discussion), and not even a material
> performance optimization (the checks are avoidable once activated until a
> sufficiently deep reorg deactivates them).


-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
Hi,
I think there is no need to do a hardfork for this. Rather it should
be implemented as a safety-mechanism in the client. Perhaps a warning
can pop up, if one of your conditions A) or B) is met.

All the best
Henning Kopp


On Thu, Mar 03, 2016 at 05:02:11AM -0800, Alice Wonder via bitcoin-dev wrote:
> I think the next hard fork should require a safety rule for TX fees.
> 
> https://blockchain.info/tx/6fe69404e6c12b25b60fcd56cc6dc9fb169b24608943def6dbe1eb0a9388ed08
> 
> 15 BTC TX fee for < 7 BTC of outputs.
> 
> Probably either a typo or client bug.
> 
> My guess is the user was using a client that does not adjust TX fee, and
> needed to manually set it in order to get the TX in the block sooner, and
> meant 15 mBTC or something.
> 
> I suggest that either :
> 
> A) TX fee may not be larger than sum of outputs
> B) TX fee per byte may not be larger than 4X largest fee per byte in
> previous block
> 
> Either of those would have prevented this TX from going into a block.
> 
> Many people I know are scared of bitcoin, that they will make a TX and make
> a mistake they can't undo.
> 
> Adding protections may help give confidence and there is precedence to doing
> things to prevent typo blunders - a public address has a four byte checksum
> to reduce the odds of a typo.
> 
> This kind of mistake is rare, so a fix could be included in the coming HF
> for the possible July 2017 block increase.
> 
> Thank you for your time.
> 
> Alice Wonder
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 

-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version 0.13.1 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.13.1/>

Or through bittorrent:

  magnet:?xt=urn:btih:dbe48c446b1113890644bbef03e361269f69c49a&dn=bitcoin-core-0.13.1&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2F

This is a new minor version release, including activation parameters for the
segwit softfork, various bugfixes and performance improvements, as well as
updated translations.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

To receive security and update notifications, please subscribe to:

  <https://bitcoincore.org/en/list/announcements/join/>

Compatibility
==============

Microsoft ended support for Windows XP on [April 8th, 2014](https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support),
an OS initially released in 2001. This means that not even critical security
updates will be released anymore. Without security updates, using a bitcoin
wallet on a XP machine is irresponsible at least.

In addition to that, with 0.12.x there have been varied reports of Bitcoin Core
randomly crashing on Windows XP. It is [not clear](https://github.com/bitcoin/bitcoin/issues/7681#issuecomment-217439891)
what the source of these crashes is, but it is likely that upstream
libraries such as Qt are no longer being tested on XP.

We do not have time nor resources to provide support for an OS that is
end-of-life. From 0.13.0 on, Windows XP is no longer supported. Users are
suggested to upgrade to a newer version of Windows, or install an alternative OS
that is supported.

No attempt is made to prevent installing or running the software on Windows XP,
you can still do so at your own risk, but do not expect it to work: do not
report issues about Windows XP to the issue tracker.

- From 0.13.1 onwards OS X 10.7 is no longer supported. 0.13.0 was intended to work on 10.7+, 
but severe issues with the libc++ version on 10.7.x keep it from running reliably. 
0.13.1 now requires 10.8+, and will communicate that to 10.7 users, rather than crashing unexpectedly.

Notable changes
===============

Segregated witness soft fork
- ----------------------------

Segregated witness (segwit) is a soft fork that, if activated, will
allow transaction-producing software to separate (segregate) transaction
signatures (witnesses) from the part of the data in a transaction that is
covered by the txid. This provides several immediate benefits:

- - **Elimination of unwanted transaction malleability:** Segregating the witness
  allows both existing and upgraded software to calculate the transaction
  identifier (txid) of transactions without referencing the witness, which can
  sometimes be changed by third-parties (such as miners) or by co-signers in a
  multisig spend. This solves all known cases of unwanted transaction
  malleability, which is a problem that makes programming Bitcoin wallet
  software more difficult and which seriously complicates the design of smart
  contracts for Bitcoin.

- - **Capacity increase:** Segwit transactions contain new fields that are not
  part of the data currently used to calculate the size of a block, which
  allows a block containing segwit transactions to hold more data than allowed
  by the current maximum block size. Estimates based on the transactions
  currently found in blocks indicate that if all wallets switch to using
  segwit, the network will be able to support about 70% more transactions. The
  network will also be able to support more of the advanced-style payments
  (such as multisig) than it can support now because of the different weighting
  given to different parts of a transaction after segwit activates (see the
  following section for details).

- - **Weighting data based on how it affects node performance:** Some parts of
  each Bitcoin block need to be stored by nodes in order to validate future
  blocks; other parts of a block can be immediately forgotten (pruned) or used
  only for helping other nodes sync their copy of the block chain.  One large
  part of the immediately prunable data are transaction signatures (witnesses),
  and segwit makes it possible to give a different "weight" to segregated
  witnesses to correspond with the lower demands they place on node resources.
  Specifically, each byte of a segregated witness is given a weight of 1, each
  other byte in a block is given a weight of 4, and the maximum allowed weight
  of a block is 4 million.  Weighting the data this way better aligns the most
  profitable strategy for creating blocks with the long-term costs of block
  validation.

- - **Signature covers value:** A simple improvement in the way signatures are
  generated in segwit simplifies the design of secure signature generators
  (such as hardware wallets), reduces the amount of data the signature
  generator needs to download, and allows the signature generator to operate
  more quickly.  This is made possible by having the generator sign the amount
  of bitcoins they think they are spending, and by having full nodes refuse to
  accept those signatures unless the amount of bitcoins being spent is exactly
  the same as was signed.  For non-segwit transactions, wallets instead had to
  download the complete previous transactions being spent for every payment
  they made, which could be a slow operation on hardware wallets and in other
  situations where bandwidth or computation speed was constrained.

- - **Linear scaling of sighash operations:** In 2015 a block was produced that
  required about 25 seconds to validate on modern hardware because of the way
  transaction signature hashes are performed.  Other similar blocks, or blocks
  that could take even longer to validate, can still be produced today.  The
  problem that caused this can't be fixed in a soft fork without unwanted
  side-effects, but transactions that opt-in to using segwit will now use a
  different signature method that doesn't suffer from this problem and doesn't
  have any unwanted side-effects.

- - **Increased security for multisig:** Bitcoin addresses (both P2PKH addresses
  that start with a '1' and P2SH addresses that start with a '3') use a hash
  function known as RIPEMD-160.  For P2PKH addresses, this provides about 160
  bits of security---which is beyond what cryptographers believe can be broken
  today.  But because P2SH is more flexible, only about 80 bits of security is
  provided per address. Although 80 bits is very strong security, it is within
  the realm of possibility that it can be broken by a powerful adversary.
  Segwit allows advanced transactions to use the SHA256 hash function instead,
  which provides about 128 bits of security  (that is 281 trillion times as
  much security as 80 bits and is equivalent to the maximum bits of security
  believed to be provided by Bitcoin's choice of parameters for its Elliptic
  Curve Digital Security Algorithm [ECDSA].)

- - **More efficient almost-full-node security** Satoshi Nakamoto's original
  Bitcoin paper describes a method for allowing newly-started full nodes to
  skip downloading and validating some data from historic blocks that are
  protected by large amounts of proof of work.  Unfortunately, Nakamoto's
  method can't guarantee that a newly-started node using this method will
  produce an accurate copy of Bitcoin's current ledger (called the UTXO set),
  making the node vulnerable to falling out of consensus with other nodes.
  Although the problems with Nakamoto's method can't be fixed in a soft fork,
  Segwit accomplishes something similar to his original proposal: it makes it
  possible for a node to optionally skip downloading some blockchain data
  (specifically, the segregated witnesses) while still ensuring that the node
  can build an accurate copy of the UTXO set for the block chain with the most
  proof of work.  Segwit enables this capability at the consensus layer, but
  note that Bitcoin Core does not provide an option to use this capability as
  of this 0.13.1 release.

- - **Script versioning:** Segwit makes it easy for future soft forks to allow
  Bitcoin users to individually opt-in to almost any change in the Bitcoin
  Script language when those users receive new transactions.  Features
  currently being researched by Bitcoin Core contributors that may use this
  capability include support for Schnorr signatures, which can improve the
  privacy and efficiency of multisig transactions (or transactions with
  multiple inputs), and Merklized Abstract Syntax Trees (MAST), which can
  improve the privacy and efficiency of scripts with two or more conditions.
  Other Bitcoin community members are studying several other improvements
  that can be made using script versioning.

Activation for the segwit soft fork is being managed using BIP9
versionbits.  Segwit's version bit is bit 1, and nodes will begin
tracking which blocks signal support for segwit at the beginning of the
first retarget period after segwit's start date of 15 November 2016.  If
95% of blocks within a 2,016-block retarget period (about two weeks)
signal support for segwit, the soft fork will be locked in.  After
another 2,016 blocks, segwit will activate.

For more information about segwit, please see the [segwit FAQ][], the
[segwit wallet developers guide][] or BIPs [141][BIP141], [143][BIP143],
[144][BIP144], and [145][BIP145].  If you're a miner or mining pool
operator, please see the [versionbits FAQ][] for information about
signaling support for a soft fork.

[Segwit FAQ]: https://bitcoincore.org/en/2016/01/26/segwit-benefits/
[segwit wallet developers guide]: https://bitcoincore.org/en/segwit_wallet_dev/
[BIP141]: https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki
[BIP143]: https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki
[BIP144]: https://github.com/bitcoin/bips/blob/master/bip-0144.mediawiki
[BIP145]: https://github.com/bitcoin/bips/blob/master/bip-0145.mediawiki
[versionbits FAQ]: https://bitcoincore.org/en/2016/06/08/version-bits-miners-faq/


Null dummy soft fork
- -------------------

Combined with the segwit soft fork is an additional change that turns a
long-existing network relay policy into a consensus rule. The
`OP_CHECKMULTISIG` and `OP_CHECKMULTISIGVERIFY` opcodes consume an extra
stack element ("dummy element") after signature validation. The dummy
element is not inspected in any manner, and could be replaced by any
value without invalidating the script.

Because any value can be used for this dummy element, it's possible for
a third-party to insert data into other people's transactions, changing
the transaction's txid (called transaction malleability) and possibly
causing other problems.

Since Bitcoin Core 0.10.0, nodes have defaulted to only relaying and
mining transactions whose dummy element was a null value (0x00, also
called OP_0).  The null dummy soft fork turns this relay rule into a
consensus rule both for non-segwit transactions and segwit transactions,
so that this method of mutating transactions is permanently eliminated
from the network.

Signaling for the null dummy soft fork is done by signaling support
for segwit, and the null dummy soft fork will activate at the same time
as segwit.

For more information, please see [BIP147][].

[BIP147]: https://github.com/bitcoin/bips/blob/master/bip-0147.mediawiki

Low-level RPC changes
- ---------------------

- - `importprunedfunds` only accepts two required arguments. Some versions accept
  an optional third arg, which was always ignored. Make sure to never pass more
  than two arguments.


Linux ARM builds
- ----------------

With the 0.13.0 release, pre-built Linux ARM binaries were added to the set of
uploaded executables. Additional detail on the ARM architecture targeted by each
is provided below.

The following extra files can be found in the download directory or torrent:

- - `bitcoin-${VERSION}-arm-linux-gnueabihf.tar.gz`: Linux binaries targeting
  the 32-bit ARMv7-A architecture.
- - `bitcoin-${VERSION}-aarch64-linux-gnu.tar.gz`: Linux binaries targeting
  the 64-bit ARMv8-A architecture.

ARM builds are still experimental. If you have problems on a certain device or
Linux distribution combination please report them on the bug tracker, it may be
possible to resolve them. Note that the device you use must be (backward)
compatible with the architecture targeted by the binary that you use.
For example, a Raspberry Pi 2 Model B or Raspberry Pi 3 Model B (in its 32-bit
execution state) device, can run the 32-bit ARMv7-A targeted binary. However,
no model of Raspberry Pi 1 device can run either binary because they are all
ARMv6 architecture devices that are not compatible with ARMv7-A or ARMv8-A.

Note that Android is not considered ARM Linux in this context. The executables
are not expected to work out of the box on Android.


0.13.1 Change log
=================

Detailed release notes follow. This overview includes changes that affect
behavior, not code moves, refactors and string updates. For convenience in locating
the code changes and accompanying discussion, both the pull request and
git merge commit are mentioned.

### Consensus
- - #8636 `9dfa0c8` Implement NULLDUMMY softfork (BIP147) (jl2012)
- - #8848 `7a34a46` Add NULLDUMMY verify flag in bitcoinconsensus.h (jl2012)
- - #8937 `8b66659` Define start and end time for segwit deployment (sipa)

### RPC and other APIs
- - #8581 `526d2b0` Drop misleading option in importprunedfunds (MarcoFalke)
- - #8699 `a5ec248` Remove createwitnessaddress RPC command (jl2012)
- - #8780 `794b007` Deprecate getinfo (MarcoFalke)
- - #8832 `83ad563` Throw JSONRPCError when utxo set can not be read (MarcoFalke)
- - #8884 `b987348` getblockchaininfo help: pruneheight is the lowest, not highest, block (luke-jr)
- - #8858 `3f508ed` rpc: Generate auth cookie in hex instead of base64 (laanwj)
- - #8951 `7c2bf4b` RPC/Mining: getblocktemplate: Update and fix formatting of help (luke-jr)

### Block and transaction handling
- - #8611 `a9429ca` Reduce default number of blocks to check at startup (sipa)
- - #8634 `3e80ab7` Add policy: null signature for failed CHECK(MULTI)SIG (jl2012)
- - #8525 `1672225` Do not store witness txn in rejection cache (sipa)
- - #8499 `9777fe1` Add several policy limits and disable uncompressed keys for segwit scripts (jl2012)
- - #8526 `0027672` Make non-minimal OP_IF/NOTIF argument non-standard for P2WSH (jl2012)
- - #8524 `b8c79a0` Precompute sighashes (sipa)
- - #8651 `b8c79a0` Predeclare PrecomputedTransactionData as struct (sipa)

### P2P protocol and network code
- - #8740 `42ea51a` No longer send local address in addrMe (laanwj)
- - #8427 `69d1cd2` Ignore `notfound` P2P messages (laanwj)
- - #8573 `4f84082` Set jonasschnellis dns-seeder filter flag (jonasschnelli)
- - #8712 `23feab1` Remove maxuploadtargets recommended minimum (jonasschnelli)
- - #8862 `7ae6242` Fix a few cases where messages were sent after requested disconnect (theuni)
- - #8393 `fe1975a` Support for compact blocks together with segwit (sipa)
- - #8282 `2611ad7` Feeler connections to increase online addrs in the tried table (EthanHeilman)
- - #8612 `2215c22` Check for compatibility with download in FindNextBlocksToDownload (sipa)
- - #8606 `bbf379b` Fix some locks (sipa)
- - #8594 `ab295bb` Do not add random inbound peers to addrman (gmaxwell)
- - #8940 `5b4192b` Add x9 service bit support to dnsseed.bluematt.me, seed.bitcoinstats.com (TheBlueMatt, cdecker)
- - #8944 `685e4c7` Remove bogus assert on number of oubound connections. (TheBlueMatt)
- - #8949 `0dbc48a` Be more agressive in getting connections to peers with relevant services (gmaxwell)

### Build system
- - #8293 `fa5b249` Allow building libbitcoinconsensus without any univalue (luke-jr)
- - #8492 `8b0bdd3` Allow building bench_bitcoin by itself (luke-jr)
- - #8563 `147003c` Add configure check for -latomic (ajtowns)
- - #8626 `ea51b0f` Berkeley DB v6 compatibility fix (netsafe)
- - #8520 `75f2065` Remove check for `openssl/ec.h` (laanwj)

### GUI
- - #8481 `d9f0d4e` Fix minimize and close bugs (adlawren)
- - #8487 `a37cec5` Persist the datadir after option reset (achow101)
- - #8697 `41fd852` Fix op order to append first alert (rodasmith)
- - #8678 `8e03382` Fix UI bug that could result in paying unexpected fee (jonasschnelli)
- - #8911 `7634d8e` Translate all files, even if wallet disabled (laanwj)
- - #8540 `1db3352` Fix random segfault when closing "Choose data directory" dialog (laanwj)
- - #7579 `f1c0d78` Show network/chain errors in the GUI (jonasschnelli)

### Wallet
- - #8443 `464dedd` Trivial cleanup of HD wallet changes (jonasschnelli)
- - #8539 `cb07f19` CDB: fix debug output (crowning-)
- - #8664 `091cdeb` Fix segwit-related wallet bug (sdaftuar)
- - #8693 `c6a6291` Add witness address to address book (instagibbs)
- - #8765 `6288659` Remove "unused" ThreadFlushWalletDB from removeprunedfunds (jonasschnelli)

### Tests and QA
- - #8713 `ae8c7df` create_cache: Delete temp dir when done (MarcoFalke)
- - #8716 `e34374e` Check legacy wallet as well (MarcoFalke)
- - #8750 `d6ebe13` Refactor RPCTestHandler to prevent TimeoutExpired (MarcoFalke)
- - #8652 `63462c2` remove root test directory for RPC tests (yurizhykin)
- - #8724 `da94272` walletbackup: Sync blocks inside the loop (MarcoFalke)
- - #8400 `bea02dc` enable rpcbind_test (yurizhykin)
- - #8417 `f70be14` Add walletdump RPC test (including HD- & encryption-tests) (jonasschnelli)
- - #8419 `a7aa3cc` Enable size accounting in mining unit tests (sdaftuar)
- - #8442 `8bb1efd` Rework hd wallet dump test (MarcoFalke)
- - #8528 `3606b6b` Update p2p-segwit.py to reflect correct behavior (instagibbs)
- - #8531 `a27cdd8` abandonconflict: Use assert_equal (MarcoFalke)
- - #8667 `6b07362` Fix SIGHASH_SINGLE bug in test_framework SignatureHash (jl2012)
- - #8673 `03b0196` Fix obvious assignment/equality error in test (JeremyRubin)
- - #8739 `cef633c` Fix broken sendcmpct test in p2p-compactblocks.py (sdaftuar)
- - #8418 `ff893aa` Add tests for compact blocks (sdaftuar)
- - #8803 `375437c` Ping regularly in p2p-segwit.py to keep connection alive (jl2012)
- - #8827 `9bbe66e` Split up slow RPC calls to avoid pruning test timeouts (sdaftuar)
- - #8829 `2a8bca4` Add bitcoin-tx JSON tests (jnewbery)
- - #8834 `1dd1783` blockstore: Switch to dumb dbm (MarcoFalke)
- - #8835 `d87227d` nulldummy.py: Don't run unused code (MarcoFalke)
- - #8836 `eb18cc1` bitcoin-util-test.py should fail if the output file is empty (jnewbery)
- - #8839 `31ab2f8` Avoid ConnectionResetErrors during RPC tests (laanwj)
- - #8840 `cbc3fe5` Explicitly set encoding to utf8 when opening text files (laanwj)
- - #8841 `3e4abb5` Fix nulldummy test (jl2012)
- - #8854 `624a007` Fix race condition in p2p-compactblocks test (sdaftuar)
- - #8857 `1f60d45` mininode: Only allow named args in wait_until (MarcoFalke)
- - #8860 `0bee740` util: Move wait_bitcoinds() into stop_nodes() (MarcoFalke)
- - #8882 `b73f065` Fix race conditions in p2p-compactblocks.py and sendheaders.py (sdaftuar)
- - #8904 `cc6f551` Fix compact block shortids for a test case (dagurval)

### Documentation
- - #8754 `0e2c6bd` Target protobuf 2.6 in OS X build notes. (fanquake)
- - #8461 `b17a3f9` Document return value of networkhashps for getmininginfo RPC endpoint (jlopp)
- - #8512 `156e305` Corrected JSON typo on setban of net.cpp (sevastos)
- - #8683 `8a7d7ff` Fix incorrect file name bitcoin.qrc  (bitcoinsSG)
- - #8891 `5e0dd9e` Update bips.md for Segregated Witness (fanquake)
- - #8545 `863ae74` Update git-subtree-check.sh README (MarcoFalke)
- - #8607 `486650a` Fix doxygen off-by-one comments, fix typos (MarcoFalke)
- - #8560 `c493f43` Fix two VarInt examples in serialize.h (cbarcenas)
- - #8737 `084cae9` UndoReadFromDisk works on undo files (rev), not on block files (paveljanik)
- - #8625 `0a35573` Clarify statement about parallel jobs in rpc-tests.py (isle2983)
- - #8624 `0e6d753` build: Mention curl (MarcoFalke)
- - #8604 `b09e13c` build,doc: Update for 0.13.0+ and OpenBSD 5.9 (laanwj)
- - #8939 `06d15fb` Update implemented bips for 0.13.1 (sipa)

### Miscellaneous
- - #8742 `d31ac72` Specify Protobuf version 2 in paymentrequest.proto (fanquake)
- - #8414,#8558,#8676,#8700,#8701,#8702 Add missing copyright headers (isle2983, kazcw)
- - #8899 `4ed2627` Fix wake from sleep issue with Boost 1.59.0 (fanquake)
- - #8817 `bcf3806` update bitcoin-tx to output witness data (jnewbery)
- - #8513 `4e5fc31` Fix a type error that would not compile on OSX. (JeremyRubin)
- - #8392 `30eac2d` Fix several node initialization issues (sipa)
- - #8548 `305d8ac` Use `__func__` to get function name for output printing (MarcoFalke)
- - #8291 `a987431` [util] CopyrightHolders: Check for untranslated substitution (MarcoFalke)

Credits
=======

Thanks to everyone who directly contributed to this release:

- - adlawren
- - Alexey Vesnin
- - Anders Øyvind Urke-Sætre
- - Andrew Chow
- - Anthony Towns
- - BtcDrak
- - Chris Stewart
- - Christian Barcenas
- - Christian Decker
- - Cory Fields
- - crowning-
- - Dagur Valberg Johannsson
- - David A. Harding
- - Eric Lombrozo
- - Ethan Heilman
- - fanquake
- - Gaurav Rana
- - Gregory Maxwell
- - instagibbs
- - isle2983
- - Jameson Lopp
- - Jeremy Rubin
- - jnewbery
- - Johnson Lau
- - Jonas Schnelli
- - jonnynewbs
- - Justin Camarena
- - Kaz Wesley
- - leijurv
- - Luke Dashjr
- - MarcoFalke
- - Marty Jones
- - Matt Corallo
- - Micha
- - Michael Ford
- - mruddy
- - Pavel Janík
- - Pieter Wuille
- - rodasmith
- - Sev
- - Suhas Daftuar
- - whythat
- - Wladimir J. van der Laan

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJYEkpHAAoJEHSBCwEjRsmmgEAH+gNUut+ywWNWp0B13+gHch2s
/d/4OzTQNKAERRCniL9BMh/MEFAK7xDYXcDkQyj9uNS/+G9EhXNysjWEt8aftd/p
HBhzWkGHO5WPSvYtBr6/0k/z3KCNDPrXybMf5x0e5P241S8ufKcqYVEspPUzezMb
aoco+Vr0XePAvCXTo3MPNTjcEFOQ7gHu/NOWC1glgPQp+krNkOjYGbtgwDwbKVIx
kLbi+40+wRQUTLR/fSqs0F/5vjbQ2JCuEREWasxQiTuHDW+6HlcgGnx0AXQyPLo5
p7BkSFAJpHG5ipqKNcymEGcFhcfWXhx/7DfYaBS1ugWQEJlQVk0jn7+cxSzyYAQ=
=vk0C
-----END PGP SIGNATURE-----


-------------------------------------
On 13/05/16 18:59, Aaron Voisine wrote:
> This scheme is independent of the number of accounts. It works with BIP44
> as well as BIP43 purpose 0, or any other BIP43 purpose/layout. Instead of
> overloading the account index to indicate the type of address, you use the
> chain index, which is already being used to indicate what the specific
> address chain is to be used for, i.e. receive vs change addresses.

I see the advantage here. But there is a major problem here.

We came up with BIP44 so a wallet can claim it is BIP44 compatible and
you can be 100% sure that you can migrate accounts from one wallet
implementation to another. This was not previously possible when a
wallet claimed it is BIP32 compatible.

Now we have a similar problem. When there is a BIP44 wallet, does it
mean it supports segwit or not? For this reason I would like to see
another BIPXX for segwit, so a wallet can claim it is BIP44, BIP44+BIPXX
or BIPXX compatible and you'll know what other wallets are compatible
with it.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
Thanks for your comments Luke.

> Are you saying your proposal is intentionally not intended to reflect the
reality?

That's right. I want to be able to include more voices and be able to get a
clearer idea of acceptance then the process currently has available.

This process should work alongside the current one; not replace it.

> conditions under which a proposal is *known to be* accepted by the
community

*known to be* Is what I'm working towards; yes; but I think we need
additional tools/processes to determine that then what we currently have
available.

> As mentioned, IMO a committee shouldn't be indicating acceptance, as much
as
it should be *determining* acceptance.

The committee determine acceptance when taking their opinions in aggregate.
The source of their indication might be similar to what we currently have
(esp for Core Devs, etc.)

> That sounds very time consuming

Ok

> And what happens if these committees don't represent the community?

The committee structures are fluid-- that is users are able to re-organize
at any time.

> What about when only part of the community - let's say 10% - decides to
adopt a BIP that doesn't require consensus

This might happen, but is not a flaw in my process. My process makes it
clear they are going against the acceptance of the rest of the community. I
don't intend to "force" anyone to implement or use an accepted BIP. If that
is desired that's outside the scope of this BIP.

> But the Bitcoin user base is completely unknown, and tracking software
user base is a privacy violation.

I made a suggestion for this here:
https://gist.github.com/andychase/dddb83c294295879308b

If there are other ways for honest but anonymous voting mechanisms (that
aren't proof-of-stake since that's proof-of-most-money) I'd be all ears.

> Bitcoin economic activity is also unknown
> This needs a proper specification. How do miners express their positions?

I agree these are flaws in the proposal. I'm not sure that one way of
indicating should be considered valid forever, but may have to change over
time.

> Chosen how, and by whom?

I think the process could be used to determine this.

> but I don't think we can just let the author set any conditions they like
either

I'm not sure what you mean here but would love more information.

On Mon, Jan 18, 2016 at 6:12 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Saturday, September 05, 2015 9:19:51 PM Andy Chase wrote:
> > Okay for sure yeah writing another proposal that reflects the current
> state
> > of affairs as people see it might provide some interesting perspective on
> > this proposal. I would welcome that.
>
> Are you saying your proposal is intentionally not intended to reflect the
> reality? I wasn't talking about a "current state of affairs" for BIPs as
> much
> as that that the acceptance of BIPs is *defined by* the state of affairs.
>
> Overall, I think something *similar to* this proposal is a good idea, but I
> disagree with how this proposal currently approaches the problem. Instead,
> what I would recommend is a specification based on BIP 123 that specifies
> the
> conditions under which a proposal is *known to be* accepted by the
> community
> (ie, discerning, not deciding), and establishes a way for a committee to
> review the BIP and *determine* if these conditions have been met. This
> would
> avoid a "disconnect" between the "official status" and reality, making the
> BIP
> process more useful to everyone.
>
> Reviewing your current proposal:
>
> > * It sets up '''committees''' for reviewing comments and indicating
> > acceptance under precise conditions.
>
> As mentioned, IMO a committee shouldn't be indicating acceptance, as much
> as
> it should be *determining* acceptance.
>
> > ** Committees are authorized groups that represent client authors,
> miners,
> > merchants, and users (each as a segment). Each one must represent at
> least
> > 1% stake in the Bitcoin ecosystem.
>
> 1% seems like an awful lot to dedicate to BIP status changes.
>
> > A committee system is used to organize the essential concerns of each
> > segment of the Bitcoin ecosystem. Although each segment may have many
> > different viewpoints on each BIP, in order to seek a decisive yes/no on a
> > BIP, a representational authoritative structure is sought. This structure
> > should be fluid, allowing people to move away from committees that do not
> > reflect their views and should be re-validated on each BIP evaluation.
>
> That sounds very time consuming. And what happens if these committees don't
> represent the community? What about when only part of the community - let's
> say 10% - decides to adopt a BIP that doesn't require consensus? Logically
> that BIP should still proceed...
>
> > ** Proof of claim and minimum 1% stake via:
> > *** Software: proof of ownership and user base (Min 1% of Bitcoin
> userbase)
>
> But the Bitcoin user base is completely unknown, and tracking software user
> base is a privacy violation.
>
> > ** Merchant: proof of economic activity (Min 1% of Bitcoin economic
> > activity)
>
> Bitcoin economic activity is also unknown, and it seems likely that
> merchants
> consider their own activity confidential.
>
> > Mining: proof of work (Min 1% of Hashpower)
>
> This needs a proper specification. How do miners express their positions?
>
> > A BIP Process Manager should be chosen who is in charge of:
>
> Chosen how, and by whom?
>
> > == Conditions for activation ==
> >
> > In order for this process BIP to become active, it must succeed by its
> own
> > rules. At least a 4% sample of the Bitcoin community must be represented,
> > with at least one committee in each segment included. Once at least one
> > committee has submitted a declaration, a request for comments will be
> called
> > and the process should be completed from there.
>
> Until this BIP is active, its rules do not apply, so this would be a form
> of
> circular reasoning. I like the idea of putting conditions for activation in
> the BIP text, but I don't think we can just let the author set any
> conditions
> they like either...
>
> Luke
>

-------------------------------------
On Sat, Sep 10, 2016 at 01:48:40AM +0000, Gregory Maxwell wrote:
> On Sat, Sep 10, 2016 at 12:58 AM, Peter Todd <pete@petertodd.org> wrote:
> > Good to do this sooner rather than later, as alert propagation on the P2P
> > network is going to continue to get less reliable as nodes upgrade to software
> 
> Yes, this was one of my motivations for doing this soon.
> 
> It would only require about 2 LOC to have Bitcoin Core vomit out a
> blob containing the final alert to any old protocol version peers that
> connect.  I don't know how other people would feel about that, but I
> wouldn't mind implementing it, and it would greatly improve the
> likelihood that they continue to to get once propagation of it is
> gone. This could be left in the codebase for a couple years or until
> other changes made those old versions p2p incompatible for other
> reasons.

I think that's a good idea, and it's a simple way to document that final alert
as well.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On 6 Feb 2016 4:41 p.m., "Gavin Andresen via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> Responding to "28 days is not long enough" :
>
> I keep seeing this claim made with no evidence to back it up.  As I said,
I surveyed several of the biggest infrastructure providers and the btcd
lead developer and they all agree "28 days is plenty of time."

28 days doesn't sound like enough for exchanges and others holding 3rd
party coins. They will have to start untangling the Bitcoins from
classiccoins immediately, while pausing all withdrawals. They *must* be
able to send their customers both coins as separate withdrawals. If not,
that amounts to theft of their customers funds.

(Note that the above describes the honest exchanges. Imagine the dishonest
ones that simply steal the classiccoins from their customers and sell them
for their own profit.)

The only other option is guaranteeing customers both coins in one
transaction, which they can't.

Surely you can't expect small entities to start putting in massive man
hours into this even before the hard fork has been triggered? Or even big
entities to have all that implemented and tested within *20* working days?

--
Jannes

-------------------------------------
On Tue, May 10, 2016 at 5:28 AM, Rusty Russell via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I used variable-length bit encodings, and used the shortest encoding
> which is unique to you (including mempool).  It's a little more work,
> but for an average node transmitting a block with 1300 txs and another
> ~3000 in the mempool, you expect about 12 bits per transaction.  IOW,
> about 1/5 of your current size.  Critically, we might be able to fit in
> two or three TCP packets.

Hm. 12 bits sounds very small even giving those figures. Why failure
rate were you targeting?

I've mostly been thing in terms of 3000 txn, and 20k mempools, and
blocks which are 90% consistent with the remote mempool, targeting
1/100000 failure rates (which is roughly where it should be to put it
well below link failure levels).

If going down the path of more complexity, set reconciliation is
enormously more efficient (e.g. 90% reduction), which no amount of
packing/twiddling can achieve.

But the savings of going from 20kb to 3kb is not interesting enough to
justify it*.  My expectation is that later we'll deploy set
reconciliation to fix relay efficiency, where the savings is _much_
larger,  and then with the infrastructure in place we could define
another compactblock mode that used it.

(*Not interesting because it mostly reduces exposure to loss and the
gods of TCP, but since those are the long poles in the latency tent,
it's best to escape them entirely, see Matt's udp_wip branch.)

> I would also avoid the nonce to save recalculating for each node, and
> instead define an id as:

Doing this would greatly increase the cost of a collision though, as
it would happen in many places in the network at once over the on the
network at once, rather than just happening on a single link, thus
hardly impacting overall propagation.

(The downside of the nonce is that you get an exponential increase in
the rate that a collision happens "somewhere", but links fail
"somewhere" all the time-- propagation overall doesn't care about
that.)

Using the same nonce means you also would not get a recovery gain from
jointly decoding using compact blocks sent from multiple peers (which
you'll have anyways in high bandwidth mode).

With a nonce a sender does have the option of reusing what they got--
but the actual encoding cost is negligible, for a 2500 transaction
block its 27 microseconds (once per block, shared across all peers)
using Pieter's suggestion of siphash 1-3 instead of the cheaper
construct in the current draft.

Of course, if you're going to check your whole mempool to reroll the
nonce, thats another matter-- but that seems wasteful compared to just
using a table driven size with a known negligible failure rate.

64-bits as a maximum length is high enough that the collision rate
would be negligible even under fairly unrealistic assumptions-- so
long as it's salted. :)

> As Peter R points out, we could later enhance receiver to brute force
> collisions (you could speed that by sending a XOR of all the txids, but
> really if there are more than a few collisions, give up).

The band between "no collisions" and "infeasible many" is fairly
narrow.  You can add a small amount more space to the ids and
immediately be in the no collision zone.

Some earlier work we had would send small amount of erasure coding
data of the next couple bytes of the IDs.  E.g. the receiver in all
the IDs you know, mark totally unknown IDs as erased and the let the
error correction fix the rest. This let you algebraically resolve
collisions _far_ beyond what could be feasibly bruteforced. Pieter
went and implemented... but the added cost of encoding and software
complexity seem not worth it.


-------------------------------------
Patrick,

I would say that a company's terms of service should include their position
on this issue. It does not seem reasonable that they all are required to
provide access to coins on every single fork. Are custodial wallet users
also entitled to Clam, Zcash, and Decred, and others?

Regardless, I think this thread should be about the technical merits of the
BIP. Discussion of hard forks would be better held elsewhere.

On Sun, Feb 7, 2016 at 1:03 PM, Patrick Strateman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I would expect that custodians who fail to produce coins on both sides
> of a fork in response to depositor requests will find themselves in
> serious legal trouble.
>
> Especially if the price moves against either fork.
>
> On 02/07/2016 10:55 AM, Jonathan Toomim via bitcoin-dev wrote:
> >
> > On Feb 6, 2016, at 9:21 PM, Jannes Faber via bitcoin-dev
> > <bitcoin-dev@lists.linuxfoundation.org
> > <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> >
> >> They *must* be able to send their customers both coins as separate
> >> withdrawals.
> >>
> > Supporting the obsolete chain is unnecessary. Such support has not
> > been offered in any cryptocurrency hard fork before, as far as I know.
> > I do not see why it should start now.
> >>
> >> If not, that amounts to theft of their customers funds.
> >>
> > If they announce their planned behavior before the fork, I do not see
> > any ethical or legal issues.
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Monday 09 May 2016 10:43:02 Gregory Maxwell wrote:
> On Mon, May 9, 2016 at 9:35 AM, Tom Zander via bitcoin-dev
> 
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > You misunderstand the networking effects.
> > The fact that your node is required to choose which one to set the
> > announce
> > bit on implies that it needs to predict which node will have the best data
> > in the future.
> 
> Not required. It may. 

It is required, in the reference of wanting to actually use compact block 
relay.


> Testing on actual nodes in the actual network (not a "lab") shows

Apologies, I thought that the term was wider known.  "Laboratory situations" 
is used where I am from as the opposite of real-world messy and unpredictable 
situations.

So, your measurements may be true, but are not useful to decide how well it 
behaves under less optimal situations. aka "the real world".

> This also _increases_ robustness. Right now a single peer failing at
> the wrong time will delay blocks with a long time out.

If your peers that were supposed to send you a compact block fail, then you'll 
end up in exactly that same situation again.  Only with various timeouts in 
between before you get your block making it a magnitude slower.

In networking this is solved by reacting instead of predicting. The network is 
not stable. Your protocol design assumes it to be.


> > Another problem with your solution is that nodes send a much larger amount
> > of unsolicited data to peers in the form of the thin-block compared to
> > the normal inv or header-first data.
> 
> "High bandwidth" mode 

Another place where I may have explained better.
This is not about the difference about the two modes of your design.
This is about the design as a whole. As compared to current.


> > Am I to understand that you choose the solution based on the fact that
> > service bits are too expensive to extend? (if not, please respond to my
> > previous question actually answering the suggestion)
> > 
> > That sounds like a rather bad way of doing design. Maybe you can add a
> > second service bits field of message instead and then do the compact
> > blocks correctly.
> Service bits are not generally a good mechanism for negating optional
> peer-local parameters.

Service bits are exactly the right solution to indicate additional p2p 
feature-support.


> [It's a little disconcerting that you appear to be maintaining a fork
> and are unaware of this.]

ehm...


> > Wait, you didn't steal the variable length encoding from an existing
> > standard and you programmed a new one?
> 
> This is one of the two variable length encodings used for years in
> Bitcoin Core. This is just the first time it's shown up in a BIP.
>
> > Look at UTF-8 on wikipedia, you may have "invented" the same encoding that
> > IBM published in 1992.
> 
> The similarity with UTF-8 is that both are variable length and some
> control information is in the high bits. The similarity ends there.

That's all fine and well, it doesn't at any point take away from my point that 
any specification should NOT invent something new that has for decades had a 
great specification already.

If you make a spec to be used by all nodes, on the wire, don't base it on your 
proprietary implementation. Please.


> > Just the first (highest) 8 bytes of a sha256 hash.
> > 
> > The amount of collisions will not be less if you start xoring the rest.
> > The whole reason for doing this extra work is also irrelevant as a spam
> > protection.
> 
> Then you expose it to a trivial collision attack:  To find two 64 bit
> hashes that collide I need perform only roughly 2^32 computation. Then
> I can send them to the network.

No, you still need to have done a POW.

Next to that, your scheme is 2^32 computations *and* some XORs. The XORs are 
percentage wise a rounding error on the total time. So your argument also 
destroys your own addition.

> This issue is eliminated by salting the hash. 

The issue is better eliminated by not allowing nodes to send uninvited large 
messages.

I don't think we're getting anywhere.

I'm not sold on your design and I explained why. I tried explaining in this 
email some misconceptions that may have appeared after my initial emails. I 
hope things are more clear.




-------------------------------------
>
> I would love to see an RFC-style standard "multiple-colored-coin-protocol"
> written by reps from all of the major protocols and that meta-merges the
> features of these implementations
>

We actually tried to do that in 2014-2015, but that effort have failed...
Nobody was really interested in collaboration, each company only cared
about it's own product.
Especially Colu, they asked everyone for requirements, and then developed a
new protocol completely on their own without taking anyone's input.

I'm not sure that merging the protocols makes sense, as some protocols
value simplicity, and a combined protocol cannot have this feature.

I don't think there is much interest in a merged colored coin protocol now.
Colu is moving away from colored coins, as far as I can tell.
CoinSpark is now doing MultiChain closed-source private blockchain.
CoinPrism also seems to be largely disinterested in colored coins.

We (ChromaWay) won't mind replacing EPOBC with something better, our
software could always support multiple different kernels so adding a new
protocol isn't a big deal for us.

So if somebody is interested in a new protocol please ping me.

One of ideas I have is to decouple input-output mapping/multiplexing from
coloring.
So one layer will describe a mapping, e.g. "Inputs 0 and 1 should go into
outputs 0, 1 and 2".
In this case it will be possible to create more advanced protocols (e.g.
with support for 'smart contracts' and whatnot) while also keeping them
compatible with old ones to some extent, e.g. a wallet can safely engage in
p2ptrade or CoinJoin transactions without understanding all protocols used
in a transaction.


> - in collaboration with feedback from core developers that understand the
> direction the protocol will be taking and the issues to avoid.
>

Core developers generally dislike things like colored coins, so I doubt
they are going to help.

Blockstream is developing a sidechain with user-defined assets, so I guess
they see it as the preferred way of doing things:
https://elementsproject.org/elements/asset-issuance/


> As it stands, investors have to install multiple wallets to deal with
> these varying implementations.
>

Actually this can be solved without making a new "merged protocol": one can
just implement a wallet which supports multiple protocols.

-------------------------------------


> On August 16, 2016 at 6:20 AM Luke Dashjr <luke@dashjr.org> wrote:
>
>
> On Tuesday, August 16, 2016 10:10:01 AM Johnson Lau via bitcoin-dev wrote:
> > Specification
> >
> > Every signature passed to OP_CHECKSIG, OP_CHECKSIGVERIFY, OP_CHECKMULTISIG,
> > or OP_CHECKMULTISIGVERIFY, to which ECDSA verification is applied,
>
> Not 20-byte witness v0 programs?

That's an implicit CHECKSIG. Will clarify.

>
> > These operators all perform ECDSA verifications on pubkey/signature pairs,
> > iterating from the top of the stack backwards. For each such verification,
> > if the signature does not pass the IsLowDERSignature check,
>
> "the IsLowDERSignature check" is not well-defined. Probably intend to
> reference the previous paragraph?

IsLowDERSignature is the function in Bitcoin Core. That's a bit complicated as the real checking function is not directly called. I'll clarify.


>
> Luke

-------------------------------------
On Sun, Jul 03, 2016 at 03:20:42AM +0800, Johnson Lau wrote:
> >>> the fact that we do this has a rather odd result: a transaction spending a witness output with an unknown version is valid even if the transaction doesn’t have any witnesses!
> >> 
> >> I don’t see any reason to have such check. We simply leave unknown witness program as any-one-can-spend without looking at the witness, as described in BIP141.
> > 
> > It will lead to a special case in code that does things with witness
> > transactions, as we can spend a witness output without a witness.
> 
> It is trivial to softfork a new rule to require the witness must not be empty for a witness output. However, does it really make the code simpler?

The Bitcoin Core codebase, no, but it does reduce the number of special cases
other codebases have to contend with.

Probably not worth changing now, but it was I think a weird design choice to
make.

> > Thus you could summarize the argument for the P2PKH special case as "We don't
> > want to make it possible to use 160-bit commitments for multisig, which _might_
> > need 256-bit security. But we do want to special-case pubkeys to save a few
> > bytes.”
> 
> Actually I would like to see even shorter hash and pubkey to be used. Storing 1 BTC for a few months does not really require the security level of P2PKH.

How short? 128 bits? 80 bits? 64 bits?

It's hard to know what's the point where you're going to risk massive losses
due to theft... and as we've seen with the DAO, those kinds of losses can lead
to very undesirable pressure for devs to act as a central authority and
intervene.

> >>> we haven’t explicitly ensured that signatures for the new signature hash can’t be reused for the old signature hash
> >> 
> >> How could that be? That’d be a hash collision.
> > 
> > Nope. The problem is it might not be a hash collission, if the actual bytes
> > signed can be interpreted in two different ways, by different types of
> > signature hashes.
> > 
> > This is the same reason the signmessage functionality prepends the message
> > being signed with the "Bitcoin Signed Message:\n" magic string.
> > 
> 
> In BIP143 sig, first 4 bytes is nVersion, and the next 32 bytes (hashPrevouts) is a hash of all prevouts. (in the case of ANYONECANPAY, the bytes following would be zero, as you proposed)
> 
> In the original sig, first 4 bytes in nVersion, next 4 bytes is number of inputs, and the next 32 bytes is a txid.
> 
> For a signature to be valid for both schemes, the last 28 bytes of the hashPrevouts must also be the first 28 bytes of a valid txid. This is already impossible. And this is just one of the many collisions required. In such case SHA256 would be insecure and adding a zero after the nVersion as you suggest would not be helpful at all.

Why isn't this carefully documented in the BIPs then?

Again, as I said in my summary:

    In a number of places we either have a belt, or suspenders, when given the
    importance of this code we’d rather have both a belt and suspenders.

Tagged hashing is an excellent way to absolutely sure that signatures can't be
reused in different contexts; if it happens to be overkill in a specific
context, the overhead of hashing another few bytes is trivial; the gain of
being absolutely sure you haven't missed a vulnerability can't be easily
dismissed.

Equally, I think in most cases simply XORing the digest obtained by hashing
with a magic tag prior to using it (e.g. by signing it) should be sufficient
for signature applications, and the overhead of doing that is ~zero.
Essentially you can think of the magic tag that's XORed with the raw digest as
making clear the intent of the signature: "this is why I think I'm signing this
digest"

However, the XOR option does have one potentially big downside in other
contexts, like general use in committed data structures: it's incompatible with
timestamping schemes like OpenTimestamps that rely on all operations being
cryptographically secure.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Wed, Feb 10, 2016 at 6:14 AM, David Vorick via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I'm not clear on the utility of more nodes. Perhaps there is significant
> concern about SPV nodes getting enough bandwidth or the network struggling
> from the load?
>

It is unfortunate that when pruning is activated, the NODE_NETWORK bit is
cleared.  This means that supporting SPV clients means running full nodes
without pruning.  OTOH, a pruning node could support SPV clients that sync
more often than once every few days, especially if it stores a few GB of
block data.

-------------------------------------
On Thursday, January 21, 2016 03:14:47 PM Rusty Russell wrote:
> +1s here means simpling say "+1" or "me too" that carries no additional
> information.  ie. if you like an idea, that's great, but it's not worth
> interruping the entire list for.
> 
> If you say "I prefer proposal X over Y because <reasons>" that's
> different.  As is "I dislike X because <reasons>" or "I need X because
> <reasons>".

So "+1"ing is OK as long as I provide a technical explanation of why I agree?
While I still think that this is too much of a restriction because it prevents 
peer-review, I would say that I could live with it as a last resort if you 
don't plan to abolish this rule altogether.

So in that case, to foster peer review, I would recommend you amend the rules 
to clarify this.
Example: "+1s are not allowed unless you provide an explanation of why you 
agree with something".
-------------------------------------
On Thu, Nov 17, 2016 at 12:43 AM, Eric Voskuil <eric@voskuil.org> wrote:

> > This means that all future transactions will have different txids...
> rules do guarantee it.
>
> No, it means that the chance is small, there is a difference.
>

I think we are mostly in agreement then?  It is just terminology.

In terms of discussing the BIP, barring a hash collision, it does make
duplicate txids impossible.

Given that a hash collision is so unlikely, the qualifier should be added
to those making claims that require hash collisions rather than those who
assume that they aren't possible.

You could have said "However nothing precludes different txs from having
the same hash, but it requires a hash collision".

Thinking about it, a re-org to before the enforcement height could allow
it.  The checkpoints protect against that though.


> As such this is not something that a node
> can just dismiss.


The security of many parts of the system is based on hash collisions not
being possible.

-------------------------------------
On Tue, Feb 9, 2016 at 8:59 AM, Yifu Guo <yifu@coinapex.com> wrote:

>
> There are 406 nodes total that falls under the un-maintained category,
> which is below 10% of the network.
> Luke also have some data here that shows similar results.
> http://luke.dashjr.org/programs/bitcoin/files/charts/versions.txt
>

I love seeing data!  I was considering 0.10 nodes as 'unmaintained' because
it has been a long time since the 0.11 release.


>
> > The network could shrink by 60% and it would still have plenty of open
>> connection slots
>
>
> I'm afraid we have to agree to disagree if you think dropping support for
> 60% of the nodes on the network when rolling out an upgrade is the sane
> default.
>

That is my estimate of the worst-case-- not 'sane default.'

My point is that even if the number of nodes shrank by 60%, we would not
see any issues (SPV nodes would still have no problem finding a full node
to connect to, full nodes would not have any problem connecting to each
other, and we would not be significantly more vulnerable to Sybil attacks
or "governments get together and try to ban running a full node" attacks).



>
>> > People are committing to spinning up thousands of supports-2mb-nodes
>> during the grace period.
>
>
> thousands of nodes?! where did you get this figure? who are these people?
> *Please* elaborate.
>

There are over a thousand people subscribed to the Classic slack channel,
many of whom have privately told me they are willing and able to run an
extra node or three (or a hundred-and-eleven) once there is a final release.

I'm not going to name names, because
 a) these were private communications, and
 b) risk of death threats, extortion, doxxing, DoS attacks, etc.  Those
risks aren't theoretical, they are very real.

To be clear: I will discourage and publicly condemn anybody who runs
'pseudo nodes' or plans to spin up lots of nodes to try to influence the
debate. The only legitimate reason to run extra nodes is to fill in a
possible gap in total node count that might be caused by old, unmaintained
nodes that stop serving blocks because the rest of the network has upgraded.


> We could wait a year and pick up maybe 10 or 20% more.
>
>
> I don't understand this statement at all, please explicate.
>

The adoption curve for a new major release is exponential: lots of adoption
in the first 30 days or so, then it rapidly tapers off.  Given that
people's nodes will be alerting them that they must upgrade, and given that
every source of Bitcoin news will probably be covering the miner adoption
vote like it was a presidential election, I expect the adoption curve for
the 2mb bump to be steeper than we've ever seen.  So my best guess is
70-80% of nodes will upgrade within 30 days of the miner voting hitting 50%
of blocks and triggering the automatic 'version obsolete; upgrade required'
warning.

Wait a year, and my guess is you might reach another 10-20% (80 to
90-something percent).

-------------------------------------
On Feb 7, 2016, at 9:24 AM, jl2012@xbt.hk wrote:

> You are making a very nave assumption that miners are just looking for profit for the next second. Instead, they would try to optimize their short term and long term ROI. It is also well known that some miners would mine at a loss, even not for ideological reasons, if they believe that their action is beneficial to the network and will provide long term ROI. It happened after the last halving in 2012. Without any immediate price appreciation, the hashing rate decreased by only less than 10%
> 


In 2012, revenue dropped by about 50% instantaneously. That does not mean that profitability became negative.

The difficulty at the time of the halving was about 3M. The exchange rate was about $12. A common miner at the time was the Radeon 6970, which performed about 350 Mh/s on 200 W for about 1.75 Mh/J. A computer with 4 6970s would use about 1 kW of power, once AC/DC losses and CPU overhead are taken into account. This 1 kW rig would have earned about $0.22/kWh before the halving, and $0.11/kWh after the halving. Since it's not hard to find electricity cheaper than $0.11/kWh, the hashrate didn't drop much.

It's a common misconception that the mining hashrate increases until an equilibrium is reached, and nobody is making a profit any longer. However, this is not true. The hashrate stops increasing when the expected operating profit over a reasonable time frame is no longer greater than the hardware cost, not when the operating profit approaches zero. For example, an S7 right now costs a little over $1000. If I don't expect to earn more than $1000 in operating profit over the next year or two with an S7, then I won't buy one.

Right now, an S7 earns about $190/month and costs about $60/month to operate, for a profit of $120/month. After the halving, revenue would drop to $95/month (or less, depending on difficulty and exchange rate), leaving profit at about $35/month. The $120/month profit is good enough motivation to buy hardware now, and the $35/month would be good enough motivation to keep running hardware after the halving.

I know in advance when the halvings are coming. There's going to be one in about 5 months, for example. I'm going to stop buying miners before the halving even if they're very profitable for a month because I don't want to be stuck with hardware that won't reach 100% return on investment (ROI).



-------------------------------------
On Mon, Aug 8, 2016 at 5:09 PM, Andy Schroder via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I have mixed feelings about strictly tying the identity-public-keys with a
[...]
> guaranteed static IP address. The second reason is because the DNS PTR

I don't see any reason that it couldn't also accept a DNS name there.

The purpose of that table is so the client knows which server ID to expect.

> I consider it a good thing from a privacy perspective if my IP address
> changes every once and a while.

And the design seeks to preserve that privacy.

> Maybe a strict check option where the identity-public-keys must optionally
> match a specific network identifier would be a compromise? Maybe this is up

The client must know the identity of the server it is expecting. The
server does not announce itself. If it did then your changing of IPs
would provide you with no privacy at all.

If the design is to provide any protection against MITM you need to
know who you expected to connect to in any case.

> I think the purpose of this is to detect if someone has physically stolen and compromised my bitcoin node and placed it on another network under control of an attacker.

Huh. No. Almost the opposite. The system is designed to inhibit
fingerprinting. You can't tell what identity key(s) a node has unless
you already know them. This means that if you don't publish your node
pubkey, no one can use it to track your node around the network.

> Is there an option for a wildcard here? Couldn't there be a case where the
> client wants to authenticate, but the bitcoin node does not care who it's
> clients are? This would be similar to many of the http based bitcoin block
> explorer API services that are out there. The API operators have built up
> some reputation, so people use them, but they don't necessarily care about
> who their users are.

Then they're just not listed in the file. The client can ask the server to
authenticate without authenticating itself.

> Does openssh have this same problem?

No. OpenSSH doesn't make an effort to protect the privacy of its users.

> I'm assuming this could be parallelized very easily, so it is not a huge
> problem?

It's not a issue because we're not aware of any usecase where a node
would have a large list of authenticated peers.

> Each peer can configure one identity-key (ECC, 32 bytes) per listening
network interface (IPv4, IPv6, tor).

I'm not aware of any reason for this limitation to exist. A node
should be able to have as many listening identities as it wants, with
a similar cost to having a large authorized keys list.


-------------------------------------
On 03/02/2016 12:08 PM, Paul Sztorc wrote:
> On 3/2/2016 2:01 PM, Eric Voskuil via bitcoin-dev wrote:
>>> A 6 month investment with 3 months on the high subsidy and 3 months on
>> low subsidy would not be made
>>
>> Yes, this is the essential point. All capital investments are made based
>> on expectations of future returns. To the extent that futures are
>> perfectly knowable, they can be perfectly factored in. This is why
>> inflation in Bitcoin is not a tax, its a cost. These step functions are
>> made continuous by their predictability, removing that predictability
>> will make them -- unpredictable.
> 
> The Ministry of Truth is taking job applications in the doublespeak
> department...

Not sure how you interpret a tautology as doublespeak.

>> Changing these futures punishes those who have planned properly and
>> favors those who have not. Sort of like a Bitcoin bail-in; are some
>> miners are too big to fail? It also creates the expectation that it may
>> happen again. This infects the money with the sort of uncertainty that
>> Bitcoin is designed to prevent.
> 
> Coinbase-smoothing can be done via soft fork (soft forks typically only
> move "one way" toward stability).

I'm addressing the hard fork proposal (see subject line).

> Moreover, the effect *costs* miners,
> it does not benefit them. Finally, it can be done so that the economic
> impact on miners is minimized.

Changes to consensus rules change the value of coins, which are property
of their owners. Nobody owes a miner a promise of consistent revenue for
future work. Cost or benefit to miners is relevant only to the extent
that those who hold money believe it will affect their value and
therefore consider it in their decision to consent.

> You'll just have to weigh the risks -- some vague, tiny effect on
> expectations today, vs the need for a small group of experts to
> emergency hard fork once every four years.

How is the small group of experts today different from the small group
of experts tomorrow?

> I'm sure those experts are completely reliable, and won't get threatened
> or assassinated!

This is precisely the issue. The precedent of hard-forking to "fix" the
money is a precedent for establishing authority over the money.

>> A 6 month investment with 3 months on the high subsidy and 3 months on
>> low subsidy would not be made if it only generated a small profit for
>> the first 3 and then massive losses for the 2nd period of 3 months.  For
>> it to be made, there needs to be large profit during the first period to
>> compensate for the losses in the 2nd period.
> 
> The word "loss" is of utmost importance here...if they are operational
> losses, it should be obvious to everyone that the best "compensation for
> losses in the 2nd period" is to just shut them off (thus reducing losses
> to zero).

But of course the losses would not be entirely operational, since
hardware (at a minimum) does not depreciate to zero because of a
halving. The ability to plan does not change this fact. There are
certainly similar considerations for labor, bandwidth, space and even
electrical/cooling costs (contracts). To the extent that these costs are
sunk (as Tier said) *any* earnings are better than none.

> So you must be arguing that miners have made an investment 3 months
> prior, knowing that it would pay for itself despite the reward halving.

Of course, how could they not?

> That's nice, but it ignores the fact that, if that investment is made
> everyone, by all miners, the *difficulty* will have increased 2 weeks
> afterward...such that operating profits are tending *immediately* toward
> zero, and will be zero by the time the first set of 3 months is over.

... which also ignores fees.

e


-------------------------------------
On Wed, Mar 02, 2016 at 02:56:14PM +0000, Luke Dashjr via bitcoin-dev wrote:
> To alleviate this risk, it seems reasonable to propose a hardfork to the 
> difficulty adjustment algorithm so it can adapt quicker to such a significant 
> drop in mining rate. BtcDrak tells me he has well-tested code for this in his 
> altcoin, which has seen some roller-coaster hashrates, so it may even be 
> possible to have such a proposal ready in time to be deployed alongside SegWit 
> to take effect in time for the upcoming subsidy halving. If this slips, I 
> think it may be reasonable to push for at least code-readiness before July, 
> and possibly roll it into any other hardfork proposed before or around that 
> time.
> 
> I am unaware of any reason this would be controversial, so if anyone has a 
> problem with such a change, please speak up sooner rather than later. Other 
> ideas or concerns are of course welcome as well.

Changing the difficulty adjustment algorithm significantly changes the
security of the whole system, as it lets attackers create fake chains
with a lot less hashing power.

Given as tx fees rise this problem will hopefully be a one-time issue, a
simple fixed difficulty adjustment probably makes sense. No need to
bring in new algorithms here with controversial new security tradeoffs.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
0000000000000000045a03e0e551c4e674f301e0a8eeb217a31ad13580446626

-------------------------------------
On Wed, Jan 27, 2016 at 2:12 AM, Luzius Meisser <luzius.meisser@gmail.com>
wrote:

> I agree that flex cap is promising. However, for it to be a viable
> long-term solution, it must not depend on significant block subsidies
> to work as the block subsidy will become less and less relevant over
> time


There is another variant of the Flex Cap approach that allows miners to pay
with a slightly higher difficulty target instead of deferring a portion of
subsidy to later blocks.  I think the HK presentation was about the subsidy
deferral variant because of miner feedback that they preferred that
approach.

Myself and a few other developers think proposals like BIP100 where the
block size is subject to a vote by the miners is suboptimal because this
type of vote is costless.  You were astute in recognizing in your post it's
a good thing to somehow align the global marginal cost with the miner's
incentive.  I feel a costless vote is not great because it aligns only to
the miner's marginal cost, and not the marginal cost to the entire flood
network.  Flex Cap is superior as "vote" mechanism as there is an actual
cost associated, allowing block size to grow with actual demand.

Warren

-------------------------------------
Since "buried deployments" are specifically in reference to historical
consensus changes, I think the question is more one of human consensus than
machine consensus. Is there any disagreement amongst Bitcoin users that
BIP34 activated at block 227931, BIP65 activated at block 388381, and BIP66
activated at block 363725? Somehow I doubt it.

It seems to me that this change is merely cementing into place a few
attributes of the blockchain's history that are not in dispute.

- Jameson

On Tue, Nov 15, 2016 at 5:42 PM, Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Actually this does nothing to provide justification for this consensus
> rule change. It is just an attempt to deflect criticism from the fact that
> it is such a change.
>
> e
>
> > On Nov 15, 2016, at 9:45 AM, Btc Drak <btcdrak@gmail.com> wrote:
> >
> > I think this is already covered in the BIP text:-
> >
> > "As of November 2016, the most recent of these changes (BIP 65,
> > enforced since December 2015) has nearly 50,000 blocks built on top of
> > it. The occurrence of such a reorg that would cause the activating
> > block to be disconnected would raise fundamental concerns about the
> > security assumptions of Bitcoin, a far bigger issue than any
> > non-backwards compatible change.
> >
> > So while this proposal could theoretically result in a consensus
> > split, it is extremely unlikely, and in particular any such
> > circumstances would be sufficiently damaging to the Bitcoin network to
> > dwarf any concerns about the effects of this proposed change."
> >
> >
> > On Mon, Nov 14, 2016 at 6:47 PM, Eric Voskuil via bitcoin-dev
> > <bitcoin-dev@lists.linuxfoundation.org> wrote:
> >> NACK
> >>
> >> Horrible precedent (hardcoding rule changes based on the assumption that
> >> large forks indicate a catastrophic failure), extremely poor process
> >> (already shipped, now the discussion), and not even a material
> performance
> >> optimization (the checks are avoidable once activated until a
> sufficiently
> >> deep reorg deactivates them).
> >>
> >> e
> >>
> >> On Nov 14, 2016, at 10:17 AM, Suhas Daftuar via bitcoin-dev
> >> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> >>
> >> Hi,
> >>
> >> Recently Bitcoin Core merged a simplification to the consensus rules
> >> surrounding deployment of BIPs 34, 66, and 65
> >> (https://github.com/bitcoin/bitcoin/pull/8391), and though the change
> is a
> >> minor one, I thought it was worth documenting the rationale in a BIP for
> >> posterity.
> >>
> >> Here's the abstract:
> >>
> >> Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated via miner
> >> signaling in block version numbers. Now that the chain has long since
> passed
> >> the blocks at which those consensus rules have triggered, we can (as a
> >> simplification and optimization) replace the trigger mechanism by
> caching
> >> the block heights at which those consensus rules became enforced.
> >>
> >> The full draft can be found here:
> >>
> >> https://github.com/sdaftuar/bips/blob/buried-deployments/
> bip-buried-deployments.mediawiki
> >>
> >> _______________________________________________
> >> bitcoin-dev mailing list
> >> bitcoin-dev@lists.linuxfoundation.org
> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >>
> >>
> >> _______________________________________________
> >> bitcoin-dev mailing list
> >> bitcoin-dev@lists.linuxfoundation.org
> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Protobuf vs. JSON was a deliberate decision. Afaik Protobuf was chosen
because of its strong types, less vulnerability to malleability and very
good platform support. Having coded both, I can say Protobuf is not more
difficult than JSON. (Actually the entire Bitcoin P2P protocol should be
based on Protobuf, but that's another story.)

Yes, all extensions to BIP70 should go into new BIPs. Note the plural
here: if you have orthogonal ideas I strongly suggest one BIP per idea
so they can be discussed and implemented (or rejected) separately.


On 06/20/2016 07:33 PM, Erik Aronesty via bitcoin-dev wrote:
> BIP 0070 has been a a moderate success, however, IMO:
> 
> - protocol buffers are inappropriate since ease of use and extensibility
> is desired over the minor gains of efficiency in this protocol.  Not too
> late to support JSON messages as the standard going forward
> 
> - problematic reliance on merchant-supplied https (X509) as the sole
> form of mechant identification.   alternate schemes (dnssec/netki), pgp
> and possibly keybase seem like good ideas.   personally, i like keybase,
> since there is no reliance on the existing domain-name system (you can
> sell with a github id, for example)
> 
> - missing an optional client supplied identification
> 
> - lack of basic subscription support
> 
> /Proposed for subscriptions:/
> 
> - BIP0047 payment codes are recommended instead of wallet addresses when
> establishing subscriptions.  Or, merchants can specify replacement
> addresses in ACK/NACK responses.   UI confirms are /required /when there
> are no replacement addresses or payment codes used.
> 
> - Wallets must confirm and store subscriptions, and are responsible for
> initiating them at the specified interval.  
> 
> - Intervals can /only /be from a preset list: weekly, biweekly, or 1,
> 2,3,4,6 or 12 months.   Intervals missed by more than 3 days cause
> suspension until the user re-verifies.
> 
> - Wallets /may /optionally ask the user whether they want to be notified
> and confirm every interval - or not.   Wallets that do not ask /must
> /notify before initiating each payment.   Interval confirmations should
> begin at /least /1 day in advance of the next payment.
> 
> /Proposed in general:
> /
> - JSON should be used instead of protocol buffers going forward.  Easier
> to use, explain extend.
> 
> - "Extendible" URI-like scheme to support multi-mode identity mechanisms
> on both payment and subscription requests.   Support for keybase://,
> netki:// and others as alternates to https://. 
> 
> - Support for client as well as merchant multi-mode verification
> 
> - Ideally, the identity verification URI scheme is somewhat
> orthogonal/independent of the payment request itself
> 
> Question:
> 
> Should this be a new BIP?  I know netki's BIP75 is out there - but I
> think it's too specific and too reliant on the domain name system.
> 
> Maybe an identity-protocol-agnostic BIP + solid implementation of a
> couple major protocols without any mention of payment URI's ... just a
> way of sending and receiving identity verified messages in general?
> 
> I would be happy to implement plugins for identity protocols, if anyone
> thinks this is a good idea.
> 
> Does anyone think https:// or keybase, or PGP or netki all by
> themselves, is enough - or is it always better to have an extensible
> protocol?
> 
> - Erik Aronesty
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 




-------------------------------------
Hi,

> 0x40000000 would be the next available to specify witness addresses.
> This is compatible with existing accounts and wallet layouts.

my main concern here is that
 -) every Bip<this-bip>-compatible wallet in the future will have to implement all (then probably) legacy derivation and tx schemes.
 -) it does not fail in a deterministic way, if I import a seed or xPriv/xPub across different capable wallets.
	It is more visible if one account has [no funds/does not show up] at all after an import than if something shows up but you need to make sure that the balance is what you might expect.


Daniel/Mycelium


On 2016-05-13 18:03, Aaron Voisine wrote:
> We use the default BIP32 wallet layout, mentioned in BIP43 as purpose
> "0". We were thinking of of having 4 chains below the "account"
> level, the original 0 and 1 for receive and change addresses, and
> then 0x40000000 and 0x40000001 for P2WPKH-in-P2SH versions of receive
> and change addresses.
> 
> I like the idea of specifying the type of address as a bit field
> flag. 0x80000000 is already used to specify hardened derivation, so
> 0x40000000 would be the next available to specify witness addresses.
> This is compatible with existing accounts and wallet layouts.
> 
> As Daniel mentioned, the downside is that trying to recover on
> non-segwit software will miss segwit receives, however it does avoid
> the problem of having to check multiple address types for each key.
> 
> Aaron Voisine co-founder and CEO breadwallet
> <http://breadwallet.com>
> 
> On Fri, May 13, 2016 at 8:00 AM, Pavol Rusnak via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
> On 13/05/16 15:16, Daniel Weigl via bitcoin-dev wrote:
>> 2) Define a new derivation path, parallel to Bip44, but a different
>> 'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the
>> user choose which account he want to add ("Normal account",
>> "Witness account").
> 
> We had quite a long discussion in our team some time ago and we
> agreed on that option #2 is much better and we'd like to implement
> this way in myTREZOR.
> 
>> +) Wallet needs only to take care of 1 address per public key
> 
> True, if this BIP only supports P2WPKH.
> 
> P2WSH should probably be handled by another account type and another 
> BIP, anyway.
> 
>> Has any Bip44 compliant wallet already done any integration at this
>> point?
> 
> We have something in the pipeline, but no visible results yet.
> 
> -- Best Regards / S pozdravom,
> 
> Pavol "stick" Rusnak SatoshiLabs.com 
> _______________________________________________ bitcoin-dev mailing
> list bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org> 
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 


-------------------------------------
There seems to be a perception out there that Bitcoin is eventually
consistent. I wrote this post to describe why this perception is completely
false.

Bitcoin Guarantees Strong, not Eventual, Consistency
http://hackingdistributed.com/2016/03/01/bitcoin-guarantees-strong-not-eventual-consistency/

I hope we can lay this bad meme to rest. Bitcoin provides a strong
guarantee.
- egs

-------------------------------------
Hi

I have just PRed a draft version of two BIPs I recently wrote.
https://github.com/bitcoin/bips/pull/362

Two BIPs that addresses the problem of decoupling wallets/clients from
nodes while assuming a user (or a group) know the remote peer.

Authentication would be necessary to selective allow bloom filtering of
transactions, encryption or any other node service that might lead to
fingerprinting or resource attacks. Authentication would also be a
pre-requirement for certificate free encryption-handshakes that is
(enough?) resistant to MITM attacks.

Encryption is highly recommended if you connect a SPV node to a trusted
node.

Authentication would allow accessing private p2p extensions from a
remote SPV peer (example: fee estimation).

I'm aware of other methods to increase privacy and integrity (tor, VPN,
stunnel, etc.), however I think authentication and a basic communication
encryption should be part of the protocol and its setup should be
complete hassle-free.

Thanks for your feeback.

/jonas


-------------------------------------
Hi Tony,

I see some issues in your protocol.

1. How are mining fees handled?

2. Assume Alice sends Bob some Coins together with their history and
Bob checks that the history is correct. How does the hash of the txout
find its way into the blockchain?

Regarding the blinding factor, I think you could just use HMAC.

All the best
Henning


On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via bitcoin-dev wrote:
> This is a proposal about hiding the entire content of bitcoin
> transactions.  It goes farther than CoinJoin and ring signatures, which
> only obfuscate the transaction graph, and Confidential Transactions, which
> only hide the amounts.
> 
> The central idea of the proposed design is to hide the entire inputs and
> outputs, and publish only the hash of inputs and outputs in the
> blockchain.  The hash can be published as OP_RETURN.  The plaintext of
> inputs and outputs is sent directly to the payee via a private message, and
> never goes into the blockchain.  The payee then calculates the hash and
> looks it up in the blockchain to verify that the hash was indeed published
> by the payer.
> 
> Since the plaintext of the transaction is not published to the public
> blockchain, all validation work has to be done only by the user who
> receives the payment.
> 
> To protect against double-spends, the payer also has to publish another
> hash, which is the hash of the output being spent.  We’ll call this hash *spend
> proof*.  Since the spend proof depends solely on the output being spent,
> any attempt to spend the same output again will produce exactly the same
> spend proof, and the payee will be able to see that, and will reject the
> payment.  If there are several outputs consumed by the same transaction,
> the payer has to publish several spend proofs.
> 
> To prove that the outputs being spent are valid, the payer also has to send
> the plaintexts of the earlier transaction(s) that produced them, then the
> plaintexts of even earlier transactions that produced the outputs spent in
> those transactions, and so on, up until the issue (similar to coinbase)
> transactions that created the initial private coins.  Each new owner of the
> coin will have to store its entire history, and when he spends the coin, he
> forwards the entire history to the next owner and extends it with his own
> transaction.
> 
> If we apply the existing bitcoin design that allows multiple inputs and
> multiple outputs per transaction, the history of ownership transfers would
> grow exponentially.  Indeed, if we take any regular bitcoin output and try
> to track its history back to coinbase, our history will branch every time
> we see a transaction that has more than one input (which is not uncommon).
> After such a transaction (remember, we are traveling back in time), we’ll
> have to track two or more histories, for each respective input.  Those
> histories will branch again, and the total number of history entries grows
> exponentially.  For example, if every transaction had exactly two inputs,
> the size of history would grow as 2^N where N is the number of steps back
> in history.
> 
> To avoid such rapid growth of ownership history (which is not only
> inconvenient to move, but also exposes too much private information about
> previous owners of all the contributing coins), we will require each
> private transaction to have exactly one input (i.e. to consume exactly one
> previous output).  This means that when we track a coin’s history back in
> time, it will no longer branch.  It will grow linearly with the number of
> transfers of ownership.  If a user wants to combine several inputs, he will
> have to send them as separate private transactions (technically, several
> OP_RETURNs, which can be included in a single regular bitcoin transaction).
> 
> Thus, we are now forbidding any coin merges but still allowing coin
> splits.  To avoid ultimate splitting into the dust, we will also require
> that all private coins be issued in one of a small number of
> denominations.  Only integer number of “banknotes” can be transferred, the
> input and output amounts must therefore be divisible by the denomination.
> For example, an input of amount 700, denomination 100, can be split into
> outputs 400 and 300, but not into 450 and 250.  To send a payment, the
> payer has to pick the unspent outputs of the highest denomination first,
> then the second highest, and so on, like we already do when we pay in cash.
> 
> With fixed denominations and one input per transaction, coin histories
> still grow, but only linearly, which should not be a concern in regard to
> scalability given that all relevant computing resources still grow
> exponentially.  The histories need to be stored only by the current owner
> of the coin, not every bitcoin node.  This is a fairer allocation of
> costs.  Regarding privacy, coin histories do expose private transactions
> (or rather parts thereof, since a typical payment will likely consist of
> several transactions due to one-input-per-transaction rule) of past coin
> owners to the future ones, and that exposure grows linearly with time, but
> it is still much much better than having every transaction immediately on
> the public blockchain.  Also, the value of this information for potential
> adversaries arguably decreases with time.
> 
> There is one technical nuance that I omitted above to avoid distraction.
>  Unlike regular bitcoin transactions, every output in a private payment
> must also include a blinding factor, which is just a random string.  When
> the output is spent, the corresponding spend proof will therefore depend on
> this blinding factor (remember that spend proof is just a hash of the
> output).  Without a blinding factor, it would be feasible to pre-image the
> spend proof and reveal the output being spent as the search space of all
> possible outputs is rather small.
> 
> To issue the new private coin, one can burn regular BTC by sending it to
> one of several unspendable bitcoin addresses, one address per denomination.
>  Burning BTC would entitle one to an equal amount of the new private coin,
> let’s call it *black bitcoin*, or *BBC*.
> 
> Then BBC would be transferred from user to user by:
> 1. creating a private transaction, which consists of one input and several
> outputs;
> 2. storing the hash of the transaction and the spend proof of the consumed
> output into the blockchain in an OP_RETURN (the sender pays the
> corresponding fees in regular BTC)
> 3. sending the transaction, together with the history leading to its input,
> directly to the payee over a private communication channel.  The first
> entry of the history must be a bitcoin transaction that burned BTC to issue
> an equal amount of BCC.
> 
> To verify the payment, the payee:
> 1. makes sure that the amount of the input matches the sum of outputs, and
> all are divisible by the denomination
> 2. calculates the hash of the private transaction
> 3. looks up an OP_RETURN that includes this hash and is signed by the
> payee.  If there is more than one, the one that comes in the earlier block
> prevails.
> 4. calculates the spend proof and makes sure that it is included in the
> same OP_RETURN
> 5. makes sure the same spend proof is not included anywhere in the same or
> earlier blocks (that is, the coin was not spent before).  Only transactions
> by the same author are searched.
> 6. repeats the same steps for every entry in the history, except the first
> entry, which should be a valid burning transaction.
> 
> To facilitate exchange of private transaction data, the bitcoin network
> protocol can be extended with a new message type.  Unfortunately, it lacks
> encryption, hence private payments are really private only when bitcoin is
> used over tor.
> 
> There are a few limitations that ought to be mentioned:
> 1. After user A sends a private payment to user B, user A will know what
> the spend proof is going to be when B decides to spend the coin.
>  Therefore, A will know when the coin was spent by B, but nothing more.
>  Neither the new owner of the coin, nor its future movements will be known
> to A.
> 2. Over time, larger outputs will likely be split into many smaller
> outputs, whose amounts are not much greater than their denominations.
> You’ll have to combine more inputs to send the same amount.  When you want
> to send a very large amount that is much greater than the highest available
> denomination, you’ll have to send a lot of private transactions, your
> bitcoin transaction with so many OP_RETURNs will stand out, and their
> number will roughly indicate the total amount.  This kind of privacy
> leakage, however it applies to a small number of users, is easy to avoid by
> using multiple addresses and storing a relatively small amount on each
> address.
> 3. Exchanges and large merchants will likely accumulate large coin
> histories.  Although fragmented, far from complete, and likely outdated, it
> is still something to bear in mind.
> 
> No hard or soft fork is required, BBC is just a separate privacy preserving
> currency on top of bitcoin blockchain, and the same private keys and
> addresses are used for both BBC and the base currency BTC.  Every BCC
> transaction must be enclosed into by a small BTC transaction that stores
> the OP_RETURNs and pays for the fees.
> 
> Are there any flaws in this design?
> 
> Originally posted to BCT https://bitcointalk.org/index.php?topic=1574508.0,
> but got no feedback so far, apparently everybody was consumed with bitfinex
> drama and now mimblewimble.
> 
> Tony

> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------

> * What if the "old" wallet has used more then 1000 addresses? I guess
> some wallets do not even create a lookup window up to 1000 addresses.
> There is a high chance of loosing funds when doing sweep (move all funds
> to a new wallet) operation.

If that is the case, the wallet is not following the standard. The wallet hierarchy standards like BIP44 specify how to walk an address chain. They all specify that you should keep going until you dont find any more used keys within the lookup window. If a wallet leaves gaps that are too big, that is also not compliant.

In any case, if the sweeping wallet understands how the old wallet uses the hierarchy, it can be safely swept without a potential loss of funds.

> * I guess most or maybe all wallets will keep all keys (the
> "lookup-window" keys) in the wallet database which could affect
> performance [4]

Yes, wallets with more addresses take more time to process.

> * I guess most wallets do not offer "moving the funds to a new seed" [5]
> which results in not solving the problem of a "lost" or "compromised"
> wallet and implies wrong security to the enduser.

Some wallets do and for those that dont, implementing it is straight forward if it already implements BIP32. Its just a matter of knowing how the old wallet uses the hierarchy and prioritizing the work.

> * If I import a bip39 mnemonic into a hardware wallet (assume Trezor or
> Keepkey) I have to type in the words into my computer which bypasses
> some of the security my hardware wallet provides me (MITM seed attack).
> Together with the point above this reduces the security of a wallet (in
> particular cold storage significant).

Both TREZOR and KeepKey have developed strategies to prevent MITM attacks during seed recovery. TREZOR asks for the words in a random order and in some cases, adds noise words. KeepKey uses a rotating substitution cipher.

> I just wanted to point out that importing a wallet is a tricky step
> especially cross-wallet imports (I think cross wallet imports is an
> experts job without further improvements).

I dont think it is as hard as you think. If a wallet uses BIP32 HD, all of the hard code is already implemented. It is just a matter of stringing the correct sequence of steps together.

Also, if the new hierarchy is under a separate purpose code as specified in BIP43, there is no need to create new seed. The BIP44 hierarchy and the new hierarchy can be extended from the same seed.


Ken Heutmaker, KeepKey

-------------------------------------
On Thu, Jan 07, 2016 at 08:54:00PM -0500, Gavin Andresen via bitcoin-dev wrote:
> ---
> 
> I'm really disappointed with the "Here's the spec, take it or leave it"
> attitude. What's the point of having a BIP process if the discussion just
> comes down to "We think more is better. We don't care what you think."

I'll point out that I personally raised an issue with segpregated
witnesses quite recently - my concern that it could make validationless
mining easier and more profitable(1). Neither Pieter Wuille nor Gregory
Maxwell believed my concern to be important at first in private
communication. However, it was still discussed on IRC, with Pieter,
Greg, and others contributing valuable input on the problem and my
proposed fix. Right now I think the next step for me is to write the
code to implement my fix and submit a pull-req against the segwit
branch.

I certainly wouldn't describe that experience as "Here's the spec, take
it or leave it; We don't what what you think."

1) "Segregated witnesses and validationless mining",
    Peter Todd, Dec 23 2015, Bitcoin-dev mailing list,
    http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html

-- 
'peter'[:-1]@petertodd.org
000000000000000004aea2cfdb89c4816b7a42208dca1f3cfd66a1c9b5df4506

-------------------------------------
BIP 2 is currently believed to be a final draft of what will replace BIP 1 in 
specifying how the entire BIP process works. This Process BIP will require 
rough consensus from the Bitcoin-dev mailing list to become Active (see BIP 2 
for the procedure, which I intend to use for its own activation due to absence 
of a clear process defined in BIP 1).

Therefore, if you have any objections to the new BIP process as specified in 
BIP 2, please voice your concerns ASAP.

https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki

Thanks for your review,

Luke


-------------------------------------
Hi Cameron, good to hear from you!

> On Jun 28, 2016, at 11:40 PM, Cameron Garnham <da2ce7@gmail.com> wrote:
> 
> Unauthenticated link level encryption is wonderful! MITM attacks are overrated; as they require an active attacker.

This is not really the case with Bitcoin. A MITM attack does not require that the attacker find a way to inject traffic into the communication between nodes. Peers will connect to the attacker directly, or accept connections directly from it. Such attacks can be easier than even passive attacks.

> Stopping passive attacks is the low hanging fruit. This should be taken first.
> 
> Automated and secure peer authentication in a mesh network is a huge topic. One of the unsolved problems in computer science.
> 
> A simple 'who is that' by asking for the fingerprint of your peers from your other peers is a very simple way to get 'some' authentication.  Semi-trusted index nodes also is a low hanging fruit for authentication.

It is the implication of widespread authentication that is at issue. Clearly there are ways to implement it using a secure side channels.

> However, let's first get unauthenticated encryption. Force the attackers to use active attacks. (That are thousands times more costly to couduct).
> 
> Sent from my iPhone
> 
>> On 29 Jun 2016, at 00:36, Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> 
>> On Tue, Jun 28, 2016 at 9:22 PM, Eric Voskuil via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> An "out of band key check" is not part of BIP151.
>> 
>> It has a session ID for this purpose.
>> 
>>> It requires a secure channel and is authentication. So BIP151 doesn't provide the tools to detect an attack, that requires authentication. A general requirement for authentication is the issue I have raised.
>> 
>> One might wonder how you ever use a Bitcoin address, or even why we
>> might guess these emails from "you" aren't actually coming from the
>> NSA.
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
There was some confusion over the following email which was posted to the list
which appears to have been cancelled before a decision could be reached.

Please note the email seems inflammatory in the "acknowledgement" section and
really should have been rewritten to contain specific details of the objection
and corrections expected.

To be clear posts to the mailing list are either approved, or rejected for not
meeting the posting standards. This allows the author to make a quick correction
and resubmit. All rejections are cc'd to
https://lists.ozlabs.org/pipermail/bitcoin-dev-moderation/
for transparency. Sometimes moderators get delayed - this week has been a busy
with lots of distractions one for everyone :)

I'm copying the entire message below:

---------- Forwarded message ----------
From: Tom <tomz@freedommail.ch>
To: bitcoin-dev@lists.linuxfoundation.org, Matt Corallo
<lf-lists@mattcorallo.com>
Cc:
Date: Fri, 06 May 2016 13:31:15 +0100
Subject: Re: [bitcoin-dev] Compact Block Relay BIP
On Monday 02 May 2016 22:13:22 Matt Corallo via bitcoin-dev wrote:

Thanks for putting in the time to make a spec!

It looks good already, but I do think some more improvements can be made.


> ===Intended Protocol Flow===
I'm not a fan of the solution that a CNode should keep state and talk to
its remote nodes differently while announcing new blocks.
Its too complicated and ultimately counter-productive.

The problem is that an individual node needs to predict network behaviour in
advance. With the downside that if it guesses wrong that both nodes end up
paying for the wrong guess.
This is not a good way to design a p2p layer.



I would suggest that a new block is announced to all nodes equally and then
individual nodes can respond with a request of either a 'compact' or a
normal block.
This is much more in line with the current design as well.

Detection if remote nodes support compact blocks, for the purpose of
requesting a compact-block, can be done either via a network-bit or just a
protocol version. Or something else entirely, if you have better
suggestions.



> Variable-length integers: bytes are a MSB base-128 encoding of the
> number.
> The high bit in each byte signifies whether another digit follows.
> [snip bitwise spec]

I suggest just referring to UTF-8 which describes this just fine.
it is good practice to refer to existing specs when possible and not copy
the details.

> ====Short transaction IDs====
> Short transaction IDs are used to represent a transaction without
> sending a full 256-bit hash. They are calculated by:
> # single-SHA256 hashing the block header with the nonce appended (in
> little-endian)
> # XORing each 8-byte chunk of the double-SHA256 transaction hash with
> each corresponding 8-byte chunk of the hash from the previous step
> # Adding each of the XORed 8-byte chunks together (in little-endian)
> iteratively to find the short transaction ID

I don't think this is needed. Just use the first 8 bytes.
The reason to do xor-ing doesn't hold up and extra complexity is unneeded.
Especially since you mention some lines down;

> The short transaction ID calculation is designed to take absolutely
> minimal processing time during block compaction to avoid introducing
> serious DoS vulnerabilities


==Acknowledgements==

I think you need to acknowledge some more people, or just remove this
paragraph.

Cheers


---------- Forwarded message ----------
From: bitcoin-dev-request@lists.linuxfoundation.org
To:
Cc:
Date: Fri, 06 May 2016 12:31:23 +0000
Subject: confirm 37d25406a07ab77823fba5f9b450438c410ccd75
If you reply to this message, keeping the Subject: header intact,
Mailman will discard the held message.  Do this if the message is
spam.  If you reply to this message and include an Approved: header
with the list password in it, the message will be approved for posting
to the list.  The Approved: header can also appear in the first line
of the body of the reply.


On Mon, May 2, 2016 at 3:13 PM, Matt Corallo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi all,
>
> The following is a BIP-formatted design spec for compact block relay
> designed to limit on wire bytes during block relay. You can find the
> latest version of this document at
> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.
>
> There are several TODO items left on the document as indicated.
> Additionally, the implementation linked at the bottom of the document
> has a few remaining TODO items as well:
>
>  * Only request compact-block-announcement from one or two peers at a
> time, as the spec requires.
>  * Request new blocks using MSG_CMPCT_BLOCK where appropriate.
>  * Fill prefilledtxn with more than just the coinbase, as noted by the
> spec, up to 10K in transactions.
>
> Luke (CC'd): Can you assign a BIP number?
>
> Thanks,
> Matt
>
> <pre>
>   BIP: TODO
>   Title: Compact block relay
>   Author: Matt Corallo <bip@bluematt.me>
>   Status: Draft
>   Type: Standards Track
>   Created: 2016-04-27
> </pre>
>
> ==Abstract==
>
> Compact blocks on the wire as a way to save bandwidth for nodes on the
> P2P network.
>
> The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
> "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
> document are to be interpreted as described in RFC 2119.
>
> ==Motivation==
>
> Historically, the Bitcoin P2P protocol has not been very bandwidth
> efficient for block relay. Every transaction in a block is included when
> relayed, even though a large number of the transactions in a given block
> are already available to nodes before the block is relayed. This causes
> moderate inbound bandwidth spikes for nodes when receiving blocks, but
> can cause very significant outbound bandwidth spikes for some nodes
> which receive a block before their peers. When such spikes occur, buffer
> bloat can make consumer-grade internet connections temporarily unusable,
> and can delay the relay of blocks to remote peers who may choose to wait
> instead of redundantly requesting the same block from other, less
> congested, peers.
>
> Thus, decreasing the bandwidth used during block relay is very useful
> for many individuals running nodes.
>
> While the goal of this work is explicitly not to reduce block transfer
> latency, it does, as a side effect reduce block transfer latencies in
> some rather significant ways. Additionally, this work forms a foundation
> for future work explicitly targeting low-latency block transfer.
>
> ==Specification==
>
> ===Intended Protocol Flow===
> TODO: Diagrams
>
> The protocol is intended to be used in two ways, depending on the peers
> and bandwidth available, as discussed [[#Implementation_Details|later]].
> The "high-bandwidth" mode, which nodes may only enable for a few of
> their peers, is enabled by setting the first boolean to 1 in a
> "sendcmpct" message. In this mode, peers send new block announcements
> with the short transaction IDs already, possibly even before fully
> validating the block. In some cases no further round-trip is needed, and
> the receiver can reconstruct the block and process it as usual
> immediately. When some transactions were not available from local
> sources (ie mempool), a getblocktxn/blocktxn roundtrip is neccessary,
> bringing the best-case latency to the same 1.5*RTT minimum time that
> nodes take today, though with significantly less bandwidth usage.
>
> The "low-bandwidth" mode is enabled by setting the first boolean to 0 in
> a "sendcmpct" message. In this mode, peers send new block announcements
> with the usual inv/headers announcements (as per BIP130, and after fully
> validating the block). The receiving peer may then request the block
> using a MSG_CMPCT_BLOCK getdata reqeuest, which will receive a response
> of the header and short transaction IDs. In some cases no further
> round-trip is needed, and the receiver can reconstruct the block and
> process it as usual, taking the same 1.5*RTT minimum time that nodes
> take today, though with significantly less bandwidth usage. When some
> transactions were not available from local sources (ie mempool), a
> getblocktxn/blocktxn roundtrip is neccessary, bringing the best-case
> latency to 2.5*RTT, again with significantly less bandwidth usage than
> today. Because TCP often exhibits worse transfer latency for larger data
> sizes (as a multiple of RTT), total latency is expected to be reduced
> even when full the 2.5*RTT transfer mechanism is used.
>
> ===New data structures===
> Several new data structures are added to the P2P network to relay
> compact blocks: PrefilledTransaction, HeaderAndShortIDs,
> BlockTransactionsRequest, and BlockTransactions. Additionally, we
> introduce a new variable-length integer encoding for use in these data
> structures.
>
> For the purposes of this section, CompactSize refers to the
> variable-length integer encoding used across the existing P2P protocol
> to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.
>
> ====New VarInt====
> TODO: I just copied this out of the src...Something that is
> wiki-formatted and more descriptive should be used here isntead.
>
> Variable-length integers: bytes are a MSB base-128 encoding of the number.
> The high bit in each byte signifies whether another digit follows. To make
> sure the encoding is one-to-one, one is subtracted from all but the last
> digit.
> Thus, the byte sequence a[] with length len, where all but the last byte
> has bit 128 set, encodes the number:
>
> (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))
>
> Properties:
> * Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)
> * Every integer has exactly one encoding
> * Encoding does not depend on size of original integer type
> * No redundancy: every (infinite) byte sequence corresponds to a list
>   of encoded integers.
>
> 0:         [0x00]  256:        [0x81 0x00]
> 1:         [0x01]  16383:      [0xFE 0x7F]
> 127:       [0x7F]  16384:      [0xFF 0x00]
> 128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]
> 255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]
> 2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]
>
> Several uses of New VarInts below are "differentially encoded". For
> these, instead of using raw indexes, the number encoded is the
> difference between the current index and the previous index, minus one.
> For example, a first index of 0 implies a real index of 0, a second
> index of 0 thereafter refers to a real index of 1, etc.
>
> ====PrefilledTransaction====
> A PrefilledTransaction structure is used in HeaderAndShortIDs to provide
> a list of a few transactions explicitly.
>
> {|
> |Field Name||Type||Size||Encoding||Purpose
> |-
> |index||New VarInt||1-3 bytes||[[#New_VarInt|New VarInt]],
> differentially encoded since the last PrefilledTransaction in a
> list||The index into the block at which this transaction is
> |-
> |tx||Transaction||variable||As encoded in "tx" messages||The transaction
> which is in the block at index index.
> |}
>
> ====HeaderAndShortIDs====
> A HeaderAndShortIDs structure is used to relay a block header, the short
> transactions IDs used for matching already-available transactions, and a
> select few transactions which we expect a peer may be missing.
>
> {|
> |Field Name||Type||Size||Encoding||Purpose
> |-
> |header||Block header||80 bytes||First 80 bytes of the block as defined
> by the encoding used by "block" messages||The header of the block being
> provided
> |-
> |nonce||uint64_t||8 bytes||Little Endian||A nonce for use in short
> transaction ID calculations
> |-
> |shortids_length||CompactSize||1, 3, 5, or 9 bytes||As used elsewhere to
> encode array lengths||The number of short transaction IDs in shortids
> |-
> |shortids||List of uint64_ts||8*shortids_length bytes||Little
> Endian||The short transaction IDs calculated from the transactions which
> were not provided explicitly in prefilledtxn
> |-
> |prefilledtxn_length||CompactSize||1, 3, 5, or 9 bytes||As used
> elsewhere to encode array lengths||The number of prefilled transactions
> in prefilledtxn
> |-
> |prefilledtxn||List of PrefilledTransactions||variable
> size*prefilledtxn_length||As defined by PrefilledTransaction definition,
> above||Used to provide the coinbase transaction and a select few which
> we expect a peer may be missing
> |}
>
> ====BlockTransactionsRequest====
> A BlockTransactionsRequest structure is used to list transaction indexes
> in a block being requested.
>
> {|
> |Field Name||Type||Size||Encoding||Purpose
> |-
> |blockhash||Binary blob||32 bytes||The output from a double-SHA256 of
> the block header, as used elsewhere||The blockhash of the block which
> the transactions being requested are in
> |-
> |indexes_length||New VarInt||1-3 bytes||As defined in [[#New_VarInt|New
> VarInt]]||The number of transactions being requested
> |-
> |indexes||List of New VarInts||1-3 bytes*indexes_length||As defined in
> [[#New_VarInt|New VarInt]], differentially encoded||The indexes of the
> transactions being requested in the block
> |}
>
> ====BlockTransactions====
> A BlockTransactions structure is used to provide some of the
> transactions in a block, as requested.
>
> {|
> |Field Name||Type||Size||Encoding||Purpose
> |-
> |blockhash||Binary blob||32 bytes||The output from a double-SHA256 of
> the block header, as used elsewhere||The blockhash of the block which
> the transactions being provided are in
> |-
> |transactions_length||New VarInt||1-3 bytes||As defined in
> [[#New_VarInt|New VarInt]]||The number of transactions provided
> |-
> |transactions||List of Transactions||variable||As encoded in "tx"
> messages||The transactions provided
> |}
>
> ====Short transaction IDs====
> Short transaction IDs are used to represent a transaction without
> sending a full 256-bit hash. They are calculated by:
> # single-SHA256 hashing the block header with the nonce appended (in
> little-endian)
> # XORing each 8-byte chunk of the double-SHA256 transaction hash with
> each corresponding 8-byte chunk of the hash from the previous step
> # Adding each of the XORed 8-byte chunks together (in little-endian)
> iteratively to find the short transaction ID
>
> ===New messages===
> A new inv type (MSG_CMPCT_BLOCK == 4) and several new protocol messages
> are added: sendcmpct, cmpctblock, getblocktxn, and blocktxn.
>
> ====sendcmpct====
> # The sendcmpct message is defined as a message containing a 1-byte
> integer followed by a 8-byte integer where pchCommand == "sendcmpct".
> # The first integer SHALL be interpreted as a boolean (and MUST have a
> value of either 1 or 0)
> # The second integer SHALL be interpreted as a little-endian version
> number. Nodes sending a sendcmpct message MUST currently set this value
> to 1.
> # Upon receipt of a "sendcmpct" message with the first and second
> integers set to 1, the node SHOULD announce new blocks by sending a
> cmpctblock message.
> # Upon receipt of a "sendcmpct" message with the first integer set to 0,
> the node SHOULD NOT announce new blocks by sending a cmpctblock message,
> but SHOULD announce new blocks by sending invs or headers, as defined by
> BIP130.
> # Upon receipt of a "sendcmpct" message with the second integer set to
> something other than 1, nodes SHOULD treat the peer as if they had not
> received the message (as it indicates the peer will provide an
> unexpected encoding in cmpctblock, and/or other, messages)
> # Nodes SHOULD check for a protocol version of >= 70014 before sending
> sendcmpct messages.
> # Nodes MUST NOT send a request for a MSG_CMPCT_BLOCK object to a peer
> before having received a sendcmpct message from that peer.
>
> ====MSG_CMPCT_BLOCK====
> # getdata messages may now contain requests for MSG_CMPCT_BLOCK objects.
> # Upon receipt of a getdata containing a request for a MSG_CMPCT_BLOCK
> object with the hash of a block which was recently announced and after
> having sent the requesting peer a sendcmpct message, nodes MUST respond
> with a cmpctblock message containing appropriate data representing the
> block being requested.
> # MSG_CMPCT_BLOCK inv objects MUST NOT appear anywhere except for in
> getdata messages.
>
> ====cmpctblock====
> # The cmpctblock message is defined as as a message containing a
> serialized HeaderAndShortIDs message and pchCommand == "cmpctblock".
> # Upon receipt of a cmpctblock message after sending a sendcmpct
> message, nodes SHOULD calculate the short transaction ID for each
> unconfirmed transaction they have available (ie in their mempool) and
> compare each to each short transaction ID in the cmpctblock message.
> # After finding already-available transactions, nodes which do not have
> all transactions available to reconstruct the full block SHOULD request
> the missing transactions using a getblocktxn message.
> # A node MUST NOT send a cmpctblock message unless they are able to
> respond to a getblocktxn message which requests every transaction in the
> block.
> # A node MUST NOT send a cmpctblock message without having validated
> that the header properly commits to each transaction in the block, and
> properly builds on top of the existing chain with a valid proof-of-work.
> A node MAY send a cmpctblock before validating that each transaction in
> the block validly spends existing UTXO set entries.
>
> ====getblocktxn====
> # The getblocktxn message is defined as as a message containing a
> serialized BlockTransactionsRequest message and pchCommand ==
> "getblocktxn".
> # Upon receipt of a properly-formatted getblocktxnmessage, nodes which
> recently provided the sender of such a message a cmpctblock for the
> block hash identified in this message MUST respond with an appropriate
> blocktxn message. Such a blocktxn message MUST contain exactly and only
> each transaction which is present in the appropriate block at the index
> specified in the getblocktxn indexes list, in the order requested.
>
> ====blocktxn====
> # The blocktxn message is defined as as a message containing a
> serialized BlockTransactions message and pchCommand == "blocktxn".
> # Upon receipt of a properly-formatted requested blocktxn message, nodes
> SHOULD attempt to reconstruct the full block by:
> ## Taking the prefilledtxn transactions from the original cmpctblock and
> placing them in the marked positions.
> ## For each short transaction ID from the original cmpctblock, in order,
> find the corresponding transaction either from the blocktxn message or
> from other sources and place it in the first available position in the
> block.
> # Once the block has been reconstructed, it shall be processed as
> normal, keeping in mind that short transaction IDs are expected to
> occasionally collide, and that nodes MUST NOT be penalized for such
> collisions, wherever they appear.
>
> ===Implementation Notes===
> # For nodes which have sufficient inbound bandwidth, sending a sendcmpct
> message with the first integer set to 1 to up to three peers is
> RECOMMENDED. If possible, it is RECOMMENDED that those peers be selected
> based on their past performance in providing blocks quickly. This will
> allow them to receive some blocks in only 0.5*RTT between them and the
> sending peer. It will also reduce their block transfer latency in other
> cases due to the smaller amount of data transmitted. Nodes MUST NOT send
> such sendcmpct messages to all peers, as it encourages wasting outbound
> bandwidth across the network.
>
> # All nodes SHOULD send a sendcmpct message to all appropriate peers.
> This will reduce their outbound bandwidth usage by allowing their peers
> to request compact blocks instead of full blocks.
>
> # Nodes with limited inbound bandwidth SHOULD request blocks using
> MSG_CMPCT_BLOCK/getblocktxn requests, when possible. While this
> increases worst-case message round-trips, it is expected to reduce
> overall transfer latency as TCP is more likely to exhibit poor
> throughput on low-bandwidth nodes.
>
> # Nodes sending cmpctblock messages SHOULD make an attempt to not place
> too many transactions into prefilledtxn (ie should limit prefilledtxn to
> only around 10KB of transactions). When in doubt, nodes SHOULD only
> include the coinbase transaction in prefilledtxn.
>
> # Nodes MAY pick one nonce per block they wish to send, and only build a
> cmpctblock message once for all peers which they wish to send a given
> block to. Nodes SHOULD NOT use the same nonce across multiple different
> blocks.
>
> # Nodes MAY impose additional requirements on when they announce new
> blocks by sending cmpctblock messages. For example, nodes with limited
> outbound bandwidth MAY choose to announce new blocks using inv/header
> messages (as per BIP130) to conserve outbound bandwidth.
>
> # Note that the MSG_CMPCT_BLOCK section does not require that nodes
> respond to MSG_CMPCT_BLOCK getdata requests for blocks which they did
> not recently announce. This allows nodes to calculate cmpctblock
> messages at announce-time instead of at request-time. Thus, nodes MUST
> NOT request blocks using MSG_CMPCT_BLOCK getdatas unless it is in
> response to an inv/headers block announcement (as per BIP130), and MUST
> NOT request blocks using MSG_CMPCT_BLOCK getdatas in response to headers
> messages which were, themselves, responses to getheaders requests.
>
> # While the current version sends transactions with the same encodings
> as is used in tx messages and elsewhere in the protocol, the version
> field in sendcmpct is intended to allow this to change in the future.
> For this reason, it is recommended that the code used to decode
> PrefilledTransaction and BlockTransactions messages be prepared to take
> a different transaction encoding, if and when the version field in
> sendcmpct changes in a future BIP.
>
> ==Justification==
>
> ====Protocol design====
> There have been many proposals to save wire bytes when relaying blocks.
> Many of them have a two-fold goal of reducing block relay time and thus
> rely on the use of significant processing power in order to avoid
> introducing additional worst-case RTTs. Because this work is not focused
> primarily on reducing block relay time, its design is much simpler (ie
> does not rely on set reconciliation protocols). Still, in testing at the
> time of writing, nodes are able to relay blocks without the extra
> getblocktxn/blocktxn RTT around 90% of the time. With a smart
> compact-block-announcement policy, it is thus expected that this work
> might allow blocks to be relayed between nodes in 0.5*RTT instead of
> 1.5*RTT at least 75% of the time.
>
> ====Use of New VarInts====
> Bitcoin has long had a variable-length integer implementation (referred
> to as CompactSize in this document), making a second a strange protocol
> quirk. However, in this protocol most of our variable-length integers
> are between 0 and 2000. For both encodings, small numbers (<100) are
> encoded as 1-byte. For numbers over 250, the CompactSize encoding begins
> to use 3 bytes instead of 1, whereas the New VarInt encoding uses 2.
> Because the primary motivation for this work is to save bytes during
> block relay, the extra byte of saving per transaction-difference is
> considered worth the extra design complexity.
>
> ====Short transaction ID calculation====
> The short transaction ID calculation is designed to take absolutely
> minimal processing time during block compaction to avoid introducing
> serious DoS vulnerabilities such as those introduced by the
> bloom-filtering in BIP 37. As such, it is possible for a node to
> construct one compact-block representation of a block for relay to
> multiple peers. Additionally, only one cryptographic hash (2 SHA rounds)
> is used when calculating the short transaction IDs for an entire block.
>
> The XOR-and-add method is used for calculating short transaction IDs
> primarily because it is fast and is reasonably able to limit the ability
> of an attacker who does not know the block hash or nonce to cause
> collisions in short transaction IDs. If an attacker were able to cause
> such collisions, filling mempools (and, thus, blocks) with them would
> cause poor network propagation of new (or non-attacker, in the case of a
> miner) blocks.
>
> The 8-byte nonce in short transaction ID calculation is used to
> introduce additional entropy on a per-node level. While the use of 8
> bytes is sufficient for an attacker to maliciously cause short
> transaction ID collisions in their own block relay, this would have less
> of an effect than if such an attacker were relaying headers/invs and not
> responding to requests for the full block.
>
> ==Backward compatibility==
>
> Older clients remain fully compatible and interoperable after this change.
>
> ==Implementation==
>
> https://github.com/TheBlueMatt/bitcoin/tree/udp
>
> ==Acknowledgements==
>
> Thanks to Gregory Maxwell for the initial suggestion as well as a lot of
> back-and-forth design and significant testing.
>
> ==Copyright==
>
> This document is placed in the public domain.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
Johnathan Corgan
Corgan Labs - SDR Training and Development Services
http://corganlabs.com

-------------------------------------
xor--- via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:
> On Thursday, January 21, 2016 11:20:46 AM Rusty Russell via bitcoin-dev wrote:
>> So, what should moderation look like from now on?
>
> The original mail which announced moderation contains this rule:
>> - Generally discouraged: [...], +1s, [...]
>
> I assume "+1s" means statements such as "I agree with doing X".
>
> Any sane procedure of deciding something includes asking the involved people 
> whether they're for or against it.
> If there are dozens of proposals on how to solve a particular technical 
> problem, how else do you want to decide it than having a vote?

+1s here means simpling say "+1" or "me too" that carries no additional
information.  ie. if you like an idea, that's great, but it's not worth
interruping the entire list for.

If you say "I prefer proposal X over Y because <reasons>" that's
different.  As is "I dislike X because <reasons>" or "I need X because
<reasons>".

Hope that clarifies!
Rusty.


-------------------------------------
On Mon, Nov 21, 2016 at 3:28 PM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Thanks for your email, Russell.
>
> Unfortunately you waited 6 weeks with writing this and the problem you are
> seeing has been fixed quite some time ago.
>

Oh, that is good news!  I look forward to seeing BIP 134 updated with your
solution.


> Thanks again for reviewing, though!

-------------------------------------
Dear list,

This message concerns pegged "sidechains", namely the Two Way Peg [1].
Specifically, it is to introduce a new OP Code (perhaps called
"OP_CheckVotesVerify"). This OP code can be deployed by soft fork, and
has (as we all probably know) many benefits, including:

1. ("Optional hard forks") Sidechains allow 'opt in' adoption of new
features. As a result, Bitcoin (the bearer asset, not the software) will
never need to worry about competing with an alternate system. This
includes competitors such as Ripple or Ethereum (supposedly
"innovative"), as well as BitcoinXT and Bitcoin Classic (supposedly
"popular").

2. ("Staging Upgrades") SCs allow complex updates to Bitcoin to be
tested, in a realistic environment (where actual BTC are at risk, and
utilizing actual network mining resources). If these updates fail, they
can be revised; if they succeed, they can be incorporated into the
mainchain.

3. Directing "blockchain resources" to Bitcoin. This includes money,
developer talent, public attention, etc.

4. Less time spent debating controversial features. Instead, we return
to a culture of "permissionless innovation".

Again, as we all know, the concept has generally received high interest
and favorable appraisal.

--

However, this feature has highly complex effects on the Bitcoin
ecosystem, and so the details should command our full attention.

First, the deployment of this OP Code involves new block validation
rules ("Drivechain") which are described on my blog [2].

In addition to that post, I intend to release short presentations:

1. On the overall design justification.
2. On "Enforcing Limits on Shared Resources". This explores the
potential for SCs to have a detrimental effect on users of vanilla BTC,
and how this proposal confronts these problems.
3. On the governance of SCs-- aka the degree of 'coupling',
inter-relatedness, and/or hierarchy --- and how Drivechain's design acts
to maximize the total value of the "chain portfolio".

My purpose, in emailing today, is to begin the conversation. The scope
of the concept is simply too large, to draft a readable BIP without
knowing what the actual points of interest are. Please express your
reactions!

Thank you for reading,
Paul

P.S. In assessing the proposal, you may find a recent technical paper
[3] by Sergio Demian Lerner to be of interest.

--

[1]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-March/004724.html
[2] http://www.truthcoin.info/blog/drivechain/
[3] http://www.rootstock.io/#resources   (
https://uploads.strikinglycdn.com/files/27311e59-0832-49b5-ab0e-2b0a73899561/Drivechains_Sidechains_and_Hybrid_2-way_peg_Designs_R9.pdf
)




-------------------------------------
On Fri, Jan 29, 2016 at 5:39 PM, Gavin Andresen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Thu, Jan 28, 2016 at 9:31 PM, Jannes Faber via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> It doesn't matter much where in the difficulty period the fork happens; if
> it happens in the middle, the lower-power fork's difficulty will adjust a
> little quicker.

The reason why BIP9 (versionbits) only checks for new activations
during difficulty retargettings is a simple optimization to only check
1/2016 of the blocks.
I suspect the check itself is not that costly for Bitcoin Core, which
has all the block headers in memory anyway, but I don't think we
should assume that will be the case for all implementations.

<BIP99 aside comment>
As an aside, BIP99 never recommends a 75% mining signaling activation
threshold: it recommends 95% for uncontroversial rule changes and no
miner signaling at all for controversial hardforks.
I still have to update BIP99 with some later changes I commented at
Scaling Bitcoin HK like signaling hardfork activation with the
"negative int32_t bit" so that old clients are forced to
upgrade/decide. We could start deploying better ways to inform users
about a hardfork event, but of course those changes cannot be applied
to older software that is already deployed (but hopefully they will
still notice something is weird is happening if the longest chain that
keeps growing is invalid because it contained a block with a negative
version in it).
But I'm yet to see a single hardfork proposal that follows BIP99's
recommendations besides the hardfork proposed in BIP99 itself, which
should consist on a manageable list of very simple to deploy fixes
like the timewarp fix forward-ported from Freicoin 0.8 for the BIP. I
haven't seen much interest in growing that little list of "a few fixes
nobody disagrees are bugs or sub-optimal design decisions, plus the
changes are easy to implement both separately and as a whole" either.
I cannot say I have seen any opposition at all to BIP99 as a hardfork
either, but I naively expected people would ask me to implement more
things for BIP99 besides
https://github.com/bitcoin/bitcoin/compare/0.11...jtimon:hardfork-timewarp-0.11
or even contribute the patches themselves. For all that, I don't
consider BIP99 a priority to work on and I plan to complete it at some
point later, unless there's a time limit for a BIP to be in the
"draft" state or something.
If someone else considers completing BIP99 a priority, I'm happy to
review and integrate things, though. Thanks again to all the reviewers
and contributors to the BIP at this time and I'm sorry that it has
been stuck for some time. Maybe the classification/recommendations
should have been a BIP without code and the hardfork proposal itself
should have been another one and that would have been clearer. I just
wanted to have some code on my first BIP (and as said the plan is
still to put more code at some point).
</BIP99 aside comment>


-------------------------------------
I would like to get community feedback on whether the following idea would
be reasonable to write as an informational BIP proposal:


Boolean Addresses: Standardized p2sh addresses combining public keys,
multisigs and time locks with arbitrary and/or-operations


Abstract
========
It is currently straightforward to create Bitcoin addresses which can be
redeemed by a single key or an m-of-n multi signature. It is not as
straight forward to create addresses that can be redeemed by, for example,
key A or (key B and key C).

This proposal describes a consistent way to create s type of p2sh addresses
(“Boolean addresses”) which can be redeemed by an arbitrary set of keys and
multi signatures combined with logical and/or operations.


Examples

========
In the examples below, Alice has key A, Bob key B, Charles key C, etc).

Example 1:

A corporation has an account that can be spent by the CEO Alice or two
board members (of Bob, Charles, David or Eric) in union. The account should
allow signatures by "A or (2 of 4 of B, C, D, E)"


Example 2:


Alice wants a bitcoin address that she normally signs herself. However, if
she has a fatal accident, she sets up a key "B" to be automatically mailed
from a cloud service after a given time of inactivity to close relatives
Charles, David and Eric. These relatives are also given keys written on
paper.


Alice's address can be redeemed by "A or (B and 1-of-3 of C, D, E)". This
way, if the cloud wallet key B is compromised or paper wallets C, D or E
are stolen, it is not sufficient to redeem the address. If Alice’s key is
lost, she can ask C, D, or E for their key and use key B to spend the
address to a new one with a new key for Alice.


Motivation

==========
Standardisation of these addresses would allow interoperability for wallet
software to create, sign and share signature requests for such addresses.


Implementation
==============
A Boolean address is described as a tree starting at a root node, where a
node can be:

* An “and” operation, with a list of sub-nodes
* An “or” operation, with a list of sub-nodes
* A public key
* A Multisig operation n-of-m with a list of public keys
* A CHECKLOCKTIMEVERIFY operation

The implementation will describe a single well-defined way to generate a
P2SH script from a given boolean address tree.

It will also define the ordering of sub-nodes for and and or operations.

The implementation will further detail how spending transactions are to be
signed. A signature will consist of keys required for a given path through
the tree. Signing an “or”- branch of the tree, will consist of a value
specifying which or-subnode is signed, followed by the signatures for that
node. That way, only one or-case has to be evaluated in the script.

For example, in the case of an account that can be redeemed by the example
"A or (B and 1-of-3 of C, D, E)" from above, could be signed by something
like:

0 (meaning evaluate the first sub-node of the or condition)
A

or

1 (evaluate the second sub-node of the top level or condition)
B
1 (One key for the multisig)
D (one of the 1-of-3 signatures)
0 (padding required for multisig opcode)

-------------------------------------
Hello Daniel,

Am 14.06.2016 um 17:41 schrieb Daniel Weigl via bitcoin-dev:
> Hi List,
> 
> Following up to the discussion last month ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012695.html ), ive prepared a proposal for a BIP here:
> 	
> 	https://github.com/DanielWeigl/bips/blob/master/bip-p2sh-accounts.mediawiki
> 
> 
> Any comments on it? Does anyone working on a BIP44 compliant wallet implement something different?
> If there are no objection, id also like to request a number for it.

thank you for going forward with this.  Should we keep the discussion on
the list, or should we make it on github?

I think we should already consider not only P2WPKH over P2SH addresses
but also "native" P2WPKH addresses.  Instead of having one BIP for these
two kinds of segwit addresses and forcing the user to have several
different accounts for each BIP, the idea would be that every fully
BIP?? compatible wallet must support both of them.  Since P2WPKH is
simpler than P2WPKH over P2SH, this is IMHO reasonable to require.

I would go with the suggestion from Aaron Voisine to use different chain
id's to distinguish between different address types.   E.g., 0,1 for
P2WPKH over P2SH and 2,3 for native P2WPKH.  I see no reason why a
wallet would want to use P2WPKH over P2SH for change addresses instead
of native P2WPKH, though.

  Jochen



-------------------------------------
On Friday 25 Mar 2016 19:43:00 Jonas Schnelli via bitcoin-dev wrote:
> An encrypted channel together with a trusted full node would finally
> allow to have a secure and save SPV communication.

I guess my question didn't get across. 

Why would you want to make your usecase do connections over the peer2peer 
(net.cpp) connection at all?

Mixing messages that are being sent to everyone and encrypted messages is 
asking for trouble.
Making your private connection out-of-band would work much better.

> > Also, you didn't actually address the attack-vector.
> 
> Which attack-vector?

The statistical attack I mentioned earlier.  Which comes from knowing which 
plain text messages are being sent over the encrypted channel, So as long as 
you keep saying you want to encrypt data that identical copies of are being 
sent to other nodes at practically the same time, you will keep being 
vulnerable to that.




-------------------------------------
On Saturday, July 30, 2016 11:18:36 PM Paul Sztorc via bitcoin-dev wrote:
> In my view, "alerts" are relatively straightforward: a new OP CODE (details
> below) st. the txn only succeeds if it references invalid block content on
> a "pretender block".
> 
> However, my background reading seems to reveal that "fraud proofs" (as they
> are now called) require some kind of tremendous engineering overhaul. Can
> anyone point me to these large problem(s)?

Essentially this comes down to attackers being able to construct a block for 
which invalidity cannot be proven. While you could always show a proof for an 
invalid transaction within a well-formed block, you cannot show a proof that a 
block is not well-formed. For example, the merkle tree that ought to represent 
a set of transactions may be corrupted in such a manner that the transaction 
paying Alice can have a SPV proof made, but the links in the merkle path have 
no known data (transactions) behind them. This could even be a perfectly valid 
block, but with some of the transactions withheld until it is stale - full 
nodes and miners cannot accept it without knowing the entire block's 
transactions. The only solution to this I am aware of, is for Alice to be told 
"hey, block XYZHASH is incomplete and cannot be checked", and then Alice 
demands the full block from the attacker. But of course this makes it trivial 
to DoS Alice by giving her bogus incomplete-block claims and forcing her to 
use the same bandwidth as a full node - which is a major problem if she lacks 
the bandwidth to run a full node (presumably her reason for using SPV in the 
first place).

Luke


-------------------------------------
On Fri, Feb 12, 2016 at 5:02 AM, <jl2012@xbt.hk> wrote:

> Seems it could be done without any new opcode:
>

The assumption was that the altcoin would only accept standard output
scripts.  Alice's payment in step 2 pays to a non-standard script.

This is an improvement over the cut and choose, but it will only work for
coins which allow non-standard scripts (type 2 in the BIP).

I guess I was to focused on maintaining standard scripts on the altcoin.

-------------------------------------

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
 
On 9/17/2016 9:20 PM, Peter Todd via bitcoin-dev wrote:
> The probability that all N blocks are found by dishonest miners is q^N,

That's the probability that dishonest miners find N blocks in a row
immediately.  What you want is the probability that they can build a
chain N blocks long, taking the random-walk into account.

So use Satoshi's formula from bitcoin.pdf, section 11.  The results are
remarkably different.  In particular, q=.5 is totally insecure, since
for any N, both factions are guaranteed to eventually possess a chain of
length N anchored at x at some point during the wild reorg melee.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2
 
iQIcBAEBAgAGBQJX4A60AAoJEG/AI00/Ca/qLmEP/0fg+XJQNexTTrS/aPqcIY00
KStPNIruJD4QA9zgyx4K3fCst85/L9rsmv/9Xo6tyn8oneAMjjVY57mTG3smhiXA
Qfu9/tG0AHneRxEpRNDA/x4IwCrr1xACOaO26gEqs9zVIszIVQq4z3Vc54gj39VD
9Jpc0653RVqHhJFT4ozZkAzg2CcPMHOxi45ufBtScaJO2AwtcLvtVYaC1BE9itDM
wDdAS175jq+LlV20Igaf/s4Cc9G3LWnrNqzVCPBr/ua4U60ZO+r3nLr9gYtYNR0H
37xgktNZA8D/YI8gjYZ5p11bIqCs4lRyI5LP3Rvh/+5zQu4hdi25HMoUMys/lw4c
ABuUVLaCa2r7pH7QczUx4jWJslaHlZ4M6tMUJ7bGZpVcPmA8FOk0j+DLTfUmYVYi
Eqc5cf2Z+PEc9kBmvsxQ351WjT7fq3OtZCcMH5dhpGv4NMuVBwQ38Dh3Pz8rhBPe
pIXMUPkmdWdczjoACpjOHbhYffCI7zCvsypydnImF7FReohWPFSKdaeoSHxotHzb
cy2EWZS2IM009qY3+jF1j3uj4bJfPSlgLgfUE23Bmvsp9PJi9W+FARbKJKxr6HaB
vvMg6rMfU8uWElQqz19ixI55PUDmtugwXccyWvhcr0ueN1P6fpNxF36Q9zwS6/+D
4orUC+rp1yNTeddvaYDv
=HS6r
-----END PGP SIGNATURE-----



-------------------------------------
On Wed, Jun 15, 2016 at 5:10 PM, Peter Todd <pete@petertodd.org> wrote:

> On Tue, Jun 14, 2016 at 05:14:23PM -0700, Bram Cohen via bitcoin-dev wrote:
> >
> > Peter proposes that there should be both UTXO and STXO commitments, and
>
> No, that's incorrect - I'm only proposing TXO commitments, not UTXO nor
> STXO
> commitments.
>

What do you mean by TXO commitments? If you mean that it only records
insertions rather than deletions, then that can do many of the same proofs
but has no way of proving that something is currently in the UTXO set,
which is functionality I'd like to provide.

When I say 'merkle tree' what I mean is a patricia trie. What I assume is
meant by a merkle mountain range is a series of patricia tries of
decreasing size each of which is an addition to the previous one, and
they're periodically consolidated into larger tries so the old ones can go
away. This data structure has the nice property that it's both in sorted
order and has less than one cache miss per operation because the
consolidation operations can be batched and done linearly. There are a
number of different things you could be describing if I misunderstood.


> I'm not proposing STXO commitments precisely because the set of _spent_
> transactions grows without bound.


I'm worried that once there's real transaction fees everyone might stop
consolidating dust and the set of unspent transactions might grow without
bound as well, but that's a topic for another day.


> > Now I'm going to go out on a limb. My thesis is that usage of a mountain
> > range is unnecessary, and that a merkle tree in the raw can be made
> > serviceable by sprinkling magic pixie dust on the performance problem.
>
> It'd help if you specified exactly what type of merkle tree you're talking
> about here; remember that the certificate transparency RFC appears to have
> reinvented merkle mountain ranges, and they call them "merkle trees".
> Bitcoin
> meanwhile uses a so-called "merkle tree" that's broken, and Zcash uses a
> partially filled fixed-sized perfect tree.
>

What I'm making is a patricia trie. Its byte level definition is very
similar to the one in your MMR codebase.

Each level of the tree has a single metadata byte and followed by two
hashes. The hashes are truncated by one byte and the hash function is a
non-padding variant of sha256 (right now it's just using regular sha256,
but that's a nice optimization which allows everything to fit in a single
block).

The possible metadata values are: TERM0, TERM1, TERMBOTH, ONLY0, ONLY1,
MIDDLE. They mean:

TERM0, TERM1: There is a single thing in the tree on the specified side.
The thing hashed on that side is that thing verbatim. The other side has
more than one thing and the hash of it is the root of everything below.

TERMBOTH: There are exactly two things below and they're included inline.
Note that two things is a special case, if there are more you sometimes
have ONLY0 or ONLY1.

ONLY0, ONLY1: There are more than two things below and they're all on the
same side. This makes proofs of inclusion and exclusion simpler, and makes
some implementation details easier, for example there's always something at
every level with perfect memory positioning. It doesn't cause much extra
memory usage because of the TERMBOTH exception for exactly two things.

MIDDLE: There two or more things on both sides.

There's also a SINGLETON special case for a tree which contains only one
thing, and an EMPTY special value for tree which doesn't contain anything.

The main differences to your patricia trie are the non-padding sha256 and
that each level doesn't hash in a record of its depth and the usage of
ONLY0 and ONLY1.


>
> > There are two causes of performance problems for merkle trees: hashing
> > operations and memory cache misses. For hashing functions, the difference
> > between a mountain range and a straight merkle tree is roughly that in a
> > mountain range there's one operation for each new update times the number
> > of times that thing will get merged into larger hills. If there are fewer
> > levels of hills the number of operations is less but the expense of proof
> > of inclusion will be larger. For raw merkle trees the number of
> operations
> > per thing added is the log base 2 of the number of levels in the tree,
> > minus the log base 2 of the number of things added at once since you can
> do
> > lazy evaluation. For practical Bitcoin there are (very roughly) a million
> > things stored, or 20 levels, and there are (even more roughly) about a
> > thousand things stored per block, so each thing forces about 20 - 10 = 10
> > operations. If you follow the fairly reasonable guideline of mountain
> range
> > hills go up by factors of four, you instead have 20/2 = 10 operations per
> > thing added amortized. Depending on details this comparison can go either
> > way but it's roughly a wash and the complexity of a mountain range is
> > clearly not worth it at least from the point of view of CPU costs.
>
> I'm having a hard time understanding this paragraph; could you explain
> what you
> think is happening when things are "merged into larger hills"?
>

I'm talking about the recalculation of mountain tips, assuming we're on the
same page about what 'MMR' means.


> As UTXO/STXO/TXO sets are all enormously larger than L1/L2 cache, it's
> impossible to get CPU cache misses below one for update operations. The
> closest
> thing to an exception is MMR's, which due to their insertion-ordering could
> have good cache locality for appends, in the sense that the mountain tips
> required to recalculate the MMR tip digest will already be in cache from
> the
> previous append. But that's not sufficient, as you also need to modify old
> TXO's further back in the tree to mark them as spent - that data is going
> to be
> far larger than L1/L2 cache.
>

This makes me think we're talking about subtly different things for MMRs.
The ones I described above have sub-1 cache miss per update due to the
amortized merging together of old mountains.

Technically even a patricia trie utxo commitment can have sub-1 cache
misses per update if some of the updates in a single block are close to
each other in memory. I think I can get practical Bitcoin updates down to a
little bit less than one l2 cache miss per update, but not a lot less.

-------------------------------------
Hello Jochen,

> I think we should already consider not only P2WPKH over P2SH addresses
> but also "native" P2WPKH addresses.  Instead of having one BIP for these
[...]
> BIP?? compatible wallet must support both of them.  Since P2WPKH is
> simpler than P2WPKH over P2SH, this is IMHO reasonable to require.
[...]
> E.g., 0,1 for
> P2WPKH over P2SH and 2,3 for native P2WPKH.  I see no reason why a

Thats a good point and should be simple to maintain. Yes, ill extend on that part.

The problem is, we dont have a final decision how the address encoding for P2WPKH 
public keys should look like. Or do we? Bip141 is "Status: Deferred"

But for now, I can at least include the public key derivation path.

> I see no reason why a
> wallet would want to use P2WPKH over P2SH for change addresses instead
> of native P2WPKH, though.

That would be a big privacy leak, imo. As soon as both outputs are spent, its visible 
which one was the P2WPKH-in-P2SH and which one the pure P2WPKH and as a consequence
you leak which output was the change and which one the actual sent output

So, i'd suggest to even make it a requirement for "normal" send-to-single-address transactions
to always use the same output type for the change output (if the wallet is able to recognize it)

Daniel

On 2016-06-15 12:26, Jochen Hoenicke wrote:
> Hello Daniel,
> 
> Am 14.06.2016 um 17:41 schrieb Daniel Weigl via bitcoin-dev:
>> Hi List,
>>
>> Following up to the discussion last month ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012695.html ), ive prepared a proposal for a BIP here:
>> 	
>> 	https://github.com/DanielWeigl/bips/blob/master/bip-p2sh-accounts.mediawiki
>>
>>
>> Any comments on it? Does anyone working on a BIP44 compliant wallet implement something different?
>> If there are no objection, id also like to request a number for it.
> 
> thank you for going forward with this.  Should we keep the discussion on
> the list, or should we make it on github?
> 
> I think we should already consider not only P2WPKH over P2SH addresses
> but also "native" P2WPKH addresses.  Instead of having one BIP for these
> two kinds of segwit addresses and forcing the user to have several
> different accounts for each BIP, the idea would be that every fully
> BIP?? compatible wallet must support both of them.  Since P2WPKH is
> simpler than P2WPKH over P2SH, this is IMHO reasonable to require.
> 
> I would go with the suggestion from Aaron Voisine to use different chain
> id's to distinguish between different address types.   E.g., 0,1 for
> P2WPKH over P2SH and 2,3 for native P2WPKH.  I see no reason why a
> wallet would want to use P2WPKH over P2SH for change addresses instead
> of native P2WPKH, though.
> 
>   Jochen
> 


-------------------------------------
> While disk space requirements might not be a big problem, block
propagation time is

Is block propagation time really still a problem? Compact blocks and FIBRE
should help here.

> Bitcoin, because its fundamental design, can scale by using offchain
solutions.

I agree.
However, I believe that on-chain scaling will be needed regardless of which
off-chain solution gains popularity.

2016-12-10 11:44 GMT+01:00 s7r via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

> t. khan via bitcoin-dev wrote:
> > BIP Proposal - Managing Bitcoin’s block size the same way we do
> > difficulty (aka Block75)
> >
> > The every two-week adjustment of difficulty has proven to be a
> > reasonably effective and predictable way of managing how quickly blocks
> > are mined. Bitcoin needs a reasonably effective and predictable way of
> > managing the maximum block size.
> >
> > It’s clear at this point that human beings should not be involved in the
> > determination of max block size, just as they’re not involved in
> > deciding the difficulty.
> >
> > Instead of setting an arbitrary max block size (1MB, 2MB, 8MB, etc.) or
> > passing the decision to miners/pool operators, the max block size should
> > be adjusted every two weeks (2016 blocks) using a system similar to how
> > difficulty is calculated.
> >
> > Put another way: let’s stop thinking about what the max block size
> > should be and start thinking about how full we want the average block to
> > be regardless of size. Over the last year, we’ve had averages of 75% or
> > higher, so aiming for 75% full seems reasonable, hence naming this
> > concept ‘Block75’.
> >
> > The target capacity over 2016 blocks would be 75%. If the last 2016
> > blocks are more than 75% full, add the difference to the max block size.
> > Like this:
> >
> > MAX_BLOCK_BASE_SIZE = 1000000
> > TARGET_CAPACITY = 750000
> > AVERAGE_OVER_CAP = average block size of last 2016 blocks minus
> > TARGET_CAPACITY
> >
> > To check if a block is valid, ≤ (MAX_BLOCK_BASE_SIZE + AVERAGE_OVER_CAP)
> >
> > For example, if the last 2016 blocks are 85% full (average block is 850
> > KB), add 10% to the max block size. The new max block size would be
> > 1,100 KB until the next 2016 blocks are mined, then reset and
> > recalculate. The 1,000,000 byte limit that exists currently would
> > remain, but would effectively be the minimum max block size.
> >
> > Another two weeks goes by, the last 2016 blocks are again 85% full, but
> > now that means they average 935 KB out of the 1,100 KB max block size.
> > This is 93.5% of the 1,000,000 byte limit, so 18.5% would be added to
> > that to make the new max block size of 1,185 KB.
> >
> > Another two weeks passes. This time, the average block is 1,050 KB. The
> > new max block size is calculated to 1,300 KB (as blocks were 105% full,
> > minus the 75% capacity target, so 30% added to max block size).
> >
> > Repeat every 2016 blocks, forever.
> >
> > If Block75 had been applied at the difficulty adjustment on November
> > 18th, the max block size would have been 1,080KB, as the average block
> > during that period was 83% full, so 8% is added to the 1,000KB limit.
> > The current size, after the December 2nd adjustment would be 1,150K.
> >
> > Block75 would allow the max block size to grow (or shrink) in response
> > to transaction volume, and does so predictably, reasonably quickly, and
> > in a method that prevents wild swings in block size or transaction fees.
> > It attempts to keep blocks at 75% total capacity over each two week
> > period, the same way difficulty tries to keep blocks mined every ten
> > minutes. It also keeps blocks as small as possible.
> >
> > Thoughts?
> >
> > -t.k.
> >
>
> I like the idea. It is good wrt growing the max. block size
> automatically without human action, but the main problem (or question)
> is not how to grow this number, it is what number can the network
> handle, considering both miners and users. While disk space requirements
> might not be a big problem, block propagation time is. The time required
> for a block to propagate in the network (or at least to all the miners)
> is directly dependent of its size.  If blocks take too much time to
> propagate in the network, the orphan rate will increase in unpredictable
> ways. For example if the internet speed in China is worse than in
> Europe, and miners in China have more than 50% of the hashing power,
> blocks mined by European miners might get orphaned.
>
> The system as described can also be gamed, by filling the network with
> transactions. Miners have the monetary interest to include as many
> transactions as possible in a block in order to collect the fees.
> Regardless how you think about it, there has to be a maximum block size
> that the network will allow as a consensus rule. Increasing it
> dynamically based on transaction volume will reach a point where the
> number got big enough that it broke things. Bitcoin, because its
> fundamental design, can scale by using offchain solutions.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Friday, 23 September 2016 13:55:50 CEST Christian Decker via bitcoin-dev 
wrote: 
> Not sure if the comparison to XML and HTML holds: the lack of closing
> tags makes the meaning of individual tokens ambiguous, like I pointed
> out before. The use of segments gives at most two levels of nesting,
> so any relationship among tokens in the same segment has to rely on
> their relative position, which could result in ambiguities, like
> whether a tag refers to a single input or the transaction as a whole.


Practically all tagged formats make ordering a requirement, so indeed this 
is relevant, and not unique.

For instance if you write;
  <div> Some line </br>Another line</br>3rd line</div>
you can get a good idea of how ordering is relevant. You can reuse any item 
many times.

Whenever there is a possible confusion the specification specifically 
explains which order to use.

I'm not sure what you mean with the idea this;

>  The use of segments gives at most two levels of nesting

It looks like you assume there is some opening and closing tags, since 
otherwise there would be no nesting.
Such tags are not intended, nor documented.

> so any relationship among tokens in the same segment has to rely on
> their relative position, which could result in ambiguities, like
> whether a tag refers to a single input or the transaction as a whole.

I quoted parts of the spec in your previous email stating the same thing, 
but I'll repeat here.
Any place that has any sort of possibility to be ambiguous is specified 
specifically to have an order.  This makes writing and parsing easier.

Since you wrote two emails now with the same issue, and I addressed it 
twice, I would urge you to write out some examples which may be confusing 
and if you find that the spec is indeed missing requirements then please 
share it with us.  I did this some time ago and it helps understanding the 
ideas by having actual explicit examples.  I am not aware of any sort of 
ambiguities that the spec allows.

Cheers!


-------------------------------------
On Fri, Feb 26, 2016 at 11:45 PM, Gregory Maxwell <greg@xiph.org> wrote:

> Why not use the single-show-signature scheme I came up with a while
> back on the Bitcoin side to force the bitcoin side to reveal a private
> key?
>
>
> http://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000344.html
>

Thanks for the info, I will give it a look.

-------------------------------------
On Friday, 25 November 2016 20:45:20 CET Sergio Demian Lerner wrote:
> I now think my reasoning and conclusions are based on a false premise:
> that BU block size policies for miners can be heterogeneous.

Agreed.
 
> There can't be short forks because forks are not in the best interest of
> the honest miner majority. All miners need to announce and follow the same
> block size policy to prevent short forks.

What you appear to want to say is that it is in everyones best interest to 
avoid short forks.
Its impossible to guarentee they can't happen, but very possible to minimize 
them.
 
> If block size negotiations are meant to be open and carried on on-chain,
> then it's much better to let miners increase or decrease the block size
> limit by 1% per block (such as what Ethereum does with the gas limit).

No, there are no block-size-negotiations on chain.

The blockchain is used here for one purpose, to state the position of 
individual miners. But what may not be clear is that you can use this as a 
time-stamped way to hold them to it. Which means that if they lie (by 
rejecting a block), everyone in the world will be able to individually 
verify that fact and their credibility will be affected.

Which will not help their case next time any block size negotiations will 
happen.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Mon, Aug 8, 2016 at 1:48 AM, Matthew Roberts <matthew@roberts.pm> wrote:

> Not everyone who uses centralized exchanges are there to obtain the
> currency though. A large portion are speculators who need to be able to
> enter and exit complex positions in milliseconds and don't care about
> decentralization, security, and often even the asset that they're buying.
>

Centralized exchanges also allow for things like limit orders.  You don't
even have to be logged in and they can execute trades.  This couldn't be
done with channels.

> Try telling everyone who currently uses Btc-e to go do their margin
> trading over lightning channels, for example.
>

Using channels and a centralized exchange gets many of the benefits of a
distributed exchange.

The channel allows instant funding while allowing the customer to have full
control over the funds.  The customer could fund the channel and then move
money to the exchange when needed.

Even margin account holders might like the fact that it is clear which
funds are under their direct control and which funds are held by the
exchange.

If they are using bitcoin funds as collateral for a margin trade, then
inherently the exchange has to have control over those funds.  A 2 of 3
system where the customer, exchange and a 3rd party arbitration agency
holds keys might be acceptable to the exchange.

-------------------------------------
On Sunday, October 02, 2016 5:18:08 PM Andrew Johnson via bitcoin-dev wrote:
> Is this particular proposal encumbered by a licensing type, patent, or
> pending patent which would preclude it from being used in the bitcoin
> project?  If not, you're wildly off topic.

I think that's the concern: we don't - and *can't* - know. Pending patents are 
not publicly visible, as far as I am aware, and the BIP process does not 
(presently) require any patent disclosure.

Of course, it is entirely possible to voluntarily provide a disclosure of 
patents in the BIP (and ideally a free license to such patents, at least those 
for the BIP). This is an alternative possibility to resolve patent concerns if 
Rootstock is not prepared to adopt a defensive patent strategy in general 
(yet?).

On Sunday, October 02, 2016 6:17:12 PM Russell O'Connor via bitcoin-dev wrote:
> If I understand this BIP correctly, the values pushed onto the stack by the
> OP_COUNT_ACKS operation depends on the ack and nack counts relative to the
> block that this happens to be included in.
> 
> This isn't going to be acceptable.  The validity of a transaction should
> always be monotone in the sense that if a transaction was acceptable in a
> given block, it must always be acceptable in any subsequent block, with the
> only exception being if one of the transaction's inputs get double spent.

I don't know if it's possible to implement decentralised sidechains without 
"breaking" this rule. But I would argue that in this scenario, the only way it 
would become invalid is the equivalent of a double-spend... and therefore it 
may be acceptable in relation to this argument.

Luke


-------------------------------------
On Wed, Mar 2, 2016 at 5:14 PM, David A. Harding via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Wed, Mar 02, 2016 at 02:56:14PM +0000, Luke Dashjr via bitcoin-dev wrote:
>> To alleviate this risk, it seems reasonable to propose a hardfork to the
>> difficulty adjustment algorithm so it can adapt quicker to such a significant
>> drop in mining rate.
>
> Having a well-reviewed hard fork patch for rapid difficulty adjustment
> would seem to be a useful reserve for all sorts of possible problems.
> That said, couldn't this specific potential situation be dealt with by a
> relatively simple soft fork?
[...]


What you are proposing makes sense only if it was believed that a very
large difficulty drop would be very likely.

This appears to be almost certainly untrue-- consider-- look how long
ago since hashrate was 50% of what it is now, or 25% of what it is
now-- this is strong evidence that supermajority of the hashrate is
equipment with state of the art power efficiency. (I've also heard
more directly-- but I think the this evidence is more compelling
because it can't be tainted by boasting). If a pre-programmed ramp and
drop is set then it has the risk of massively under-setting
difficulty; which is also strongly undesirable (e.g. advanced
inflation and exacerbating existing unintentional selfish mining)...
and that is before suggesting that miners voluntarily take a loss of
inflation now.

So while I think this concern is generally implausible; I think it's
prudent to have a difficulty step patch (e.g. a one time single point
where a particular block is required to lower bits a set amount) ready
to go in the unlikely case the network is stalled. Of course, if the
alternative is "stuck" from a large hashrate drop the deployment would
be both safe and relatively uncontroversial. I think the
unfavorability of that approach is well matched to the implausibility
of the situation, and likely the right coarse of action compared to
risky interventions that would likely cause harm. The cost of
developing and testing such a patch is low, and justified purely on
the basis of increasing confidence that an issue would be handled (a
fact _I_ am perfectly confident in; but apparently some are not).

With respect what Luke was suggesting; without specifics its hard to
comment, but most altcoin "tolerate difficulty drop" changes have made
them much more vulnerable to partitioning attacks and other issues
(e.g. strategic behavior by miners to increase inflation), and have
actually been exploited in practice several times (solidcoin's being
the oldest I'm aware of). Many survived a fairly long time before
being shown to be pretty broken, simply because they were deployed in
cases where no one cared to attack. I'm currently doubtful that
particular path would be fruitful.


-------------------------------------
Good point, to be honest. Maybe there's a better way to combine the block
hashes like taking the first N bits from each block hash to produce a
single number but the direction that this is going in doesn't seem ideal.

I just asked a friend about this problem and he mentioned using the hash of
the proof of work hash as part of the number so you have to throw away a
valid POW if it doesn't give you the hash you want. I suppose its possible
to make it infinitely expensive to manipulate the number but I can't think
of anything better than that for now.

I need to sleep on this for now but let me know if anyone has any better
ideas.



On Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012@xbt.hk> wrote:

> Using the hash of multiple blocks does not make it any safer. The miner of
> the last block always determines the results, by knowing the hashes of all
> previous blocks.
>
>
> == Security
>
> Pay-to-script-hash can be used to protect the details of contracts that
> use OP_PRANDOM from the prying eyes of miners. However, since there is also
> a non-zero risk that a participant in a contract may attempt to bribe a
> miner the inclusion of multiple block hashes as a source of randomness is a
> must. Every miner would effectively need to be bribed to ensure control
> over the results of the random numbers, which is already very unlikely. The
> risk approaches zero as N goes up.
>
>
>

-------------------------------------
>
> >I've always assumed honeypots were meant to look like regular, yet
> >poorly-secured, assets.
>
> Not at all. Most servers have zero reason to have any Bitcoin's accessible
> via them, so the presence of BTC privkeys is a gigantic red flag that they
> are part of a honeypot.
>

I was talking about the traditional concept. From Wikipedia: "Generally, a
honeypot consists of data (for example, in a network site) that appears to
be a legitimate part of the site but is actually isolated and monitored,
and that seems to contain information or a resource of value to attackers,
which are then blocked."

I would argue there are ways to make it look like it is not a honeypot
(plenty of bitcoin services have had their hot wallets hacked before, and
if the intruder only gains access to one server they wouldn't know that all
the servers have the same honeypot on them). But I was just confirming that
the proposal is for an obvious honeypot.


> Re-read my last section on the "scorched earth" disincentive to
> doublespend the intruder.
>
> The first time I read it I didn't realize that the second transaction the
intruder has is designed to waste the honeypot AND additional funds
belonging to the honeypot creator. That's pretty good, from a game theory
perspective.

-------------------------------------
On Jun 28, 2016, at 9:55 PM, Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

>> I understand the use, when coupled with a yet-to-be-devised identity system, with Bloom filter features. Yet these features
> 
> This is a bit of a strawman, you've selected a single narrow usecase which isn't proposed by the BIP and then argue it is worthless. I agree that example doesn't have much value (and I believe that
> eventually the BIP37 bloom filters should be removed from the protocol).

I don't follow this comment. The BIP aims quite clearly at "SPV" wallets as its justifying scenario.

> Without something like BIP151 network participants cannot have privacy for the transactions they originate within the protocol against network observers.

And they won't get it with BIP151 either. Being a peer is easier than observing the network. If one can observe the encrypted traffic one can certainly use a timing attack to determine what the node has sent.

> Even if, through some extraordinary effort, their own first hop is encrypted, unencrypted later hops would rapidly
> expose significant information about transaction origins in the network.

As will remain the case until all connections are encrypted and authenticated, and all participants are known to be good guys. Starting to sound like PKI?

> Without something like BIP151 authenticated links are not possible, so
> manually curated links (addnode/connect) cannot be counted on to provide protection against partitioning sybils.

If we trust the manual links we don't need/want the other links. In fact retaining the other links enables the attack you described above. Of course there is no need to worry about Sybil attacks when all of your peers are authenticated. But again, let us not ignore the problems of requiring all peers on the network be authenticated.

> Along the way BIP151 appears that it will actually make the protocol faster.
> 
>> Given that the BIP relies on identity
> 
> This is untrue. The proposal is an ephemerally keyed opportunistic
> encryption system. The privacy against a network observer does not depend on authentication, much less "identity".  And when used with authentication at all it makes interception strongly detectable after the fact.

Maybe I was insufficiently explicit. By "relies on identity" I meant that the BIP is not effective without it. I did not mean to imply that the BIP itself implements an identity scheme. I thought this was clear from the context.

>> The BIP does not [...] contemplate the significant problems associated with key distribution in any identity system
> 
> Because it does not propose any "identity system" or authorization (also, I object to your apparent characterization of authentication as as an 'identity system'-- do you also call Bitcoin addresses an identity system?).

Please read more carefully what I wrote. I did not characterize authentication as an identity system. I proposed that key distribution has significant problems, and used identity systems as an example of systems with such problems. I could just have easily written "authentication systems", (and probably should have).

> That said, manually maintaining adds nodes to your own and somewhat trusted nodes is a recommend best practice for miners and other high value systems which is rendered much less effective due to a lack of
> authentication, there is no significant key distribution problem in that case

This is the only legitimate scenario that I am aware of. Doing this by IP address (as we do) is weak if there is no VPN.

Yet this scenario is very different than general authentication. This scenario is a set of nodes that is essentially a single logical node from the perspective of the Bitcoin security model. One entity controls the validation rules, or is collaborating with another entity to do so.

My concern is that a general authentication requirement expands this single logical node and gives control over if to the entity that controls key distribution - the hard problem that hasn't been addressed.

If there is no such entity restricting access to the network (which hopefully we can assume) then there is no reason to expect any effective improvement, since nodes will necessarily have to connect with anonymous peers. Anyone with a node and the ability to monitor traffic should remain very effective.

> and I expect the future auth BIP (Jonas had one before, but it was put aside for now to first focus on the link layer encryption)
> to address that case quite well.

Defining an auth implementation is not a hard problem, nor is it the concern I have raised.

e

-------------------------------------
On Mon, Mar 14, 2016 at 12:18:33PM +0100, Wladimir J. van der Laan wrote:
> Proposed release schedule for 0.13.0:
> 
> 2015-05-01
> 2015-05-15

Obviously these are 2016, not 2015.

More active tracking here:
https://github.com/bitcoin/bitcoin/issues/7679
https://github.com/bitcoin/bitcoin/milestones/0.13.0



-------------------------------------
On Wed, Nov 16, 2016 at 6:16 PM, Eric Voskuil <eric@voskuil.org> wrote:

> On 11/16/2016 05:50 PM, Pieter Wuille wrote:
>


> > So are checkpoints good now?
> > I believe we should get rid of checkpoints because they seem to be
> misunderstood as a security feature rather than as an optimization.
>
> Or maybe because they place control of the "true chain" in the hands of
> those selecting the checkpoints? It's not a great leap for the parties
> distributing the checkpoints to become the central authority.
>

Yes, they can be used to control the "true chain", and this has happened
with various forks. But developers inevitably have this possibility, if you
ignore review. If review is good enough to catch unintended consensus
changes, it is certainly enough to catch the introduction of an invalid
checkpoint. The risk you point out is real, but the way to deal with it is
good review and release practices.

I wish we had never used checkpoints the way we did, but here we are.
Because of this, I want to get rid of them. However, It's not because I
think they offer an excessive power to developers - but because they're
often perceived this way (partially as a result of how they've been abused
in other systems).


> I recommend users of our node validate the full chain without
> checkpoints and from that chain select their own checkpoints and place
> them into config. From that point forward they can apply the
> optimization. Checkpoints should never be hardcoded into the source.
>

Having users with the discipline you suggest would be wonderful to have. I
don't think it's very realistic, though, and I fear that the result would
be that everyone copies their config from one or a few websites "because
that's what everyone uses".

> I don't think buried softforks have that problem.
>
> I find "buried softfork" a curious name as you are using it. You seem to
> be implying that this type of change is itself a softfork as opposed to
> a hardfork that changes the activation of a softfork. It was my
> understanding that the term referred to the 3 softforks that were being
> "buried", or the proposal, but not the burial itself.
>

I do not consider the practice of "buried softforks" to be a fork at all.
It is a change that modifies the validity of a theoretically construable
chain from invalid to valid. However, a reorganization to that theoretical
chain itself is likely already impossible due to the vast number of blocks
to rewind, and economic damage that is far greater than chain divergence
itself.


> Nevertheless, this proposal shouldn't have "that problem" because it is
> clearly neither a security feature nor an optimization. That is the
> first issue that needs to be addressed.


It is clearly not a security feature, agreed. But how would you propose to
avoid the ISM checks for BIP34 and BIP66 all the time? I feel this approach
is a perfectly reasonable choice for code that likely won't ever affect the
valid chain again.

Cheers,

-- 
Pieter

-------------------------------------
As Luke pointed, BIP44 is already used by many wallets and to my knowledge
people don't have any real world issues with that, including loading funds
in another BIP44 wallet. I'm not saying that BIP44 is perfect from all
points of view, but IMO it just works for most use cases. Let's set it as
final, and propose competing standards which cover all your concerns.

slush

On Thu, Aug 25, 2016 at 10:12 AM, Jonas Schnelli via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> > The development paradigm of "maybe detect funds" is not something we
> > should *not* encourage for Bitcoin IMO.
>
> Sorry. That was one "not" to many.
>
> </jonas>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On May 17, 2016 15:23, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
> # TXO Commitments
>

> Specifically TXO commitments proposes a Merkle Mountain Range¹ (MMR), a
> type of deterministic, indexable, insertion ordered merkle tree, which
allows
> new items to be cheaply appended to the tree with minimal storage
requirements,
> just log2(n) "mountain tips". Once an output is added to the TXO MMR it is
> never removed; if an output is spent its status is updated in place. Both
the
> state of a specific item in the MMR, as well the validity of changes to
items
> in the MMR, can be proven with log2(n) sized proofs consisting of a
merkle path
> to the tip of the tree.

How expensive it is to update a leaf from this tree from unspent to spent?

Wouldn't it be better to have both an append-only TXO and an append-only
STXO (with all spent outputs, not only the latest ones like in your "STXO")?

-------------------------------------
Here's a method of fixing block withholding attacks with a soft fork:

We require blocks to choose an arbitrary target, e.g. two bytes. We
redefine the block PoW target to be "less than the difficulty, with the
last two bytes being the target".

We require blocks to include a blinded hash of the target plus some junk
(which blinds it) in the coinbase. The miner cannot arbitrarily switch
targets.

The miner can now send the block header to hashers. Hashers do not know the
target, and hence must submit all shares that matches the first PoW
criteria (below difficulty) and delegate secondary verification to the
miner. With two bytes as the target, there are 65335 false positives for
every valid block.

Finally, we require miners to communicate a proof of their target hash (ie,
the junk they generated) in a non-hashed area of the block. This can be a
protocol extension. The target is already included in the hash as the last
two bytes.

This can be deployed as a soft fork with miner opt in, triggering across
many difficulty cycles. Initially, we soft fork to require the last bit of
the block hash to be zero. The next difficulty cycle, we require the last
two bits to be zero. We do this 16 times to get 2 bytes, and then we
actually activate targets.

By then, nominal difficulty would have been cut by 2^16 (65536), but the
block target makes mining each block 65536 times as hard. We do the
progression over 16 difficulty cycles to minimise increases in block
timings. We can be more specific and progress over even more difficulty
cycles through more clever soft fork rules.

For example, Vitalik detailed "timewalking" attacks that allow effective
granular lowering of the nominal difficulty.

Critique welcome.

-------------------------------------
On Fri, Feb 26, 2016 at 11:33 PM, Tier Nolan via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> That is very interesting.
>
> There has been some recent discussion about atomic cross chain transfers
> between Bitcoin and legacy altcoins.  For this purpose a legacy altcoin is
> one that has strict IsStandard() rules and none of the advanced script
> opcodes.

One might wonder why anyone would want to own coins that couldn't keep
up technologically, but to each his own. (especially one defunct
enough that it can't even update IsStandard rules...)

I don't think it's infeasible to do the EC multiply in a snark, but an
efficient implementation would be a lot of work. You'd probably want
to build a circuit for the field operations using 128 bit operations.
Fortunately the overall operation is pretty easy to directly convert
into a circuit (e.g. no branching).

Why not use the single-show-signature scheme I came up with a while
back on the Bitcoin side to force the bitcoin side to reveal a private
key?

http://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000344.html


-------------------------------------
No, anyone with the bip32 public seed can do the same as the receiver as
"watch only". The only difference is rhat the receiver can actually spend
the coins. As gmaxwell explained, if it's expensive for everyone, it will
be also expensive for the receiver (assuming no interaction after the bip32
public seed is transfered).

Something different would be to give a different bip32 public seed to each
payer.  That way they can simply start with zero an increment with each new
payment. With those assumptions, the receiver could start listening to new
addresses only after they receive something in the previous address.

Probably not useful for this case, just thinking out loud about using bip32
public seeds instead of one use addresses when there's going to be several
payments from the same payer to the payee.

On Aug 12, 2016 2:37 PM, "Erik Aronesty via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> I'm imagining a "publishable seed" such that:
>
>  - someone can derive a random bitcoin address from it -  and send funds
to it.
>  - the possible derived address space is large enough that generating all
possible addresses would be a barrier
>  - the receiver, however, knowing the private key, can easily scan the
blockchain fairly efficiently and determine which addresses he has the keys
to
>  - another interested party cannot easily do so
>
> Perhaps homomorphic encryption may need to be involved?
>
>
> On Thu, Aug 11, 2016 at 8:36 PM, Gregory Maxwell <greg@xiph.org> wrote:
>>
>> On Thu, Aug 11, 2016 at 8:37 PM, Erik Aronesty via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > Still not sure how you can take a BIP32 public seed and figure out if
an
>> > address was derived from it though.   I mean, wouldn't I have to
compute all
>> > 2^31 possible public child addresses?
>>
>> Which would take a quad core laptop about 8 hours with competent software
>>
>> And presumably you're not using the whole 2^31 space else the receiver
>> also has to do that computation...
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Oh God... here we go again..

>
> Again, lets remember that you personally proposed a BIP[1] that had the
> effect
> of aiding your ASICBOOST patent[2] without disclosing that fact in your
> BIP nor
> your pull-req[3].
>
> This is false. The first sentence of the BIP states: "There are incentives
for miners to find cheap, non-standard ways to generate new work which are
not in the best interest of the protocol".

The BIP actually PROTECTS the network from stealth Shared-Nonce mining and
the fact you rejected it made the Bitcoin network LESS secure because now
we just don't know at what extent it is in use.

Shared-nonce mining can be done with or without that BIP/pull-req.

We didn't disclose more in the BIP because it was not clear if shared-nonce
mining (the fact that Bitcoin had a design flaw) would have a negative
affect on Bitcoin price.

ASICBoost patent may be a patent that protects Bitcoiners from mining
centralization: ASICBoost is the only company that at this point showed
interest in licensing the technology. But I do not control ASICBoost nor
the patent so I cannot do anything about it.

I propose we as a community do a crowdfund to try to license it from that
company (or any other that wants to put theirs in the deal) and put all in
public domain.

-------------------------------------

> On Jan 23, 2016, at 3:59 PM, Peter Todd via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> I would extend this to say that the technical explanation also should
> contribute uniquely to the conversation; a +1 with an explanation
> the last +1 gave isn't useful.

Yes, comments should contribute to the discussion, with either technical discussion or additional relevant data. I think a +1 like the following should be encouraged:

"+1: we had eleven customer support tickets in just the last week that would have been prevented if XYZ.

Jane Doe, CTO CoinBitChainBasely.com"



-------------------------------------
This is a proposal about hiding the entire content of bitcoin
transactions.  It goes farther than CoinJoin and ring signatures, which
only obfuscate the transaction graph, and Confidential Transactions, which
only hide the amounts.

The central idea of the proposed design is to hide the entire inputs and
outputs, and publish only the hash of inputs and outputs in the
blockchain.  The hash can be published as OP_RETURN.  The plaintext of
inputs and outputs is sent directly to the payee via a private message, and
never goes into the blockchain.  The payee then calculates the hash and
looks it up in the blockchain to verify that the hash was indeed published
by the payer.

Since the plaintext of the transaction is not published to the public
blockchain, all validation work has to be done only by the user who
receives the payment.

To protect against double-spends, the payer also has to publish another
hash, which is the hash of the output being spent.  We’ll call this hash *spend
proof*.  Since the spend proof depends solely on the output being spent,
any attempt to spend the same output again will produce exactly the same
spend proof, and the payee will be able to see that, and will reject the
payment.  If there are several outputs consumed by the same transaction,
the payer has to publish several spend proofs.

To prove that the outputs being spent are valid, the payer also has to send
the plaintexts of the earlier transaction(s) that produced them, then the
plaintexts of even earlier transactions that produced the outputs spent in
those transactions, and so on, up until the issue (similar to coinbase)
transactions that created the initial private coins.  Each new owner of the
coin will have to store its entire history, and when he spends the coin, he
forwards the entire history to the next owner and extends it with his own
transaction.

If we apply the existing bitcoin design that allows multiple inputs and
multiple outputs per transaction, the history of ownership transfers would
grow exponentially.  Indeed, if we take any regular bitcoin output and try
to track its history back to coinbase, our history will branch every time
we see a transaction that has more than one input (which is not uncommon).
After such a transaction (remember, we are traveling back in time), we’ll
have to track two or more histories, for each respective input.  Those
histories will branch again, and the total number of history entries grows
exponentially.  For example, if every transaction had exactly two inputs,
the size of history would grow as 2^N where N is the number of steps back
in history.

To avoid such rapid growth of ownership history (which is not only
inconvenient to move, but also exposes too much private information about
previous owners of all the contributing coins), we will require each
private transaction to have exactly one input (i.e. to consume exactly one
previous output).  This means that when we track a coin’s history back in
time, it will no longer branch.  It will grow linearly with the number of
transfers of ownership.  If a user wants to combine several inputs, he will
have to send them as separate private transactions (technically, several
OP_RETURNs, which can be included in a single regular bitcoin transaction).

Thus, we are now forbidding any coin merges but still allowing coin
splits.  To avoid ultimate splitting into the dust, we will also require
that all private coins be issued in one of a small number of
denominations.  Only integer number of “banknotes” can be transferred, the
input and output amounts must therefore be divisible by the denomination.
For example, an input of amount 700, denomination 100, can be split into
outputs 400 and 300, but not into 450 and 250.  To send a payment, the
payer has to pick the unspent outputs of the highest denomination first,
then the second highest, and so on, like we already do when we pay in cash.

With fixed denominations and one input per transaction, coin histories
still grow, but only linearly, which should not be a concern in regard to
scalability given that all relevant computing resources still grow
exponentially.  The histories need to be stored only by the current owner
of the coin, not every bitcoin node.  This is a fairer allocation of
costs.  Regarding privacy, coin histories do expose private transactions
(or rather parts thereof, since a typical payment will likely consist of
several transactions due to one-input-per-transaction rule) of past coin
owners to the future ones, and that exposure grows linearly with time, but
it is still much much better than having every transaction immediately on
the public blockchain.  Also, the value of this information for potential
adversaries arguably decreases with time.

There is one technical nuance that I omitted above to avoid distraction.
 Unlike regular bitcoin transactions, every output in a private payment
must also include a blinding factor, which is just a random string.  When
the output is spent, the corresponding spend proof will therefore depend on
this blinding factor (remember that spend proof is just a hash of the
output).  Without a blinding factor, it would be feasible to pre-image the
spend proof and reveal the output being spent as the search space of all
possible outputs is rather small.

To issue the new private coin, one can burn regular BTC by sending it to
one of several unspendable bitcoin addresses, one address per denomination.
 Burning BTC would entitle one to an equal amount of the new private coin,
let’s call it *black bitcoin*, or *BBC*.

Then BBC would be transferred from user to user by:
1. creating a private transaction, which consists of one input and several
outputs;
2. storing the hash of the transaction and the spend proof of the consumed
output into the blockchain in an OP_RETURN (the sender pays the
corresponding fees in regular BTC)
3. sending the transaction, together with the history leading to its input,
directly to the payee over a private communication channel.  The first
entry of the history must be a bitcoin transaction that burned BTC to issue
an equal amount of BCC.

To verify the payment, the payee:
1. makes sure that the amount of the input matches the sum of outputs, and
all are divisible by the denomination
2. calculates the hash of the private transaction
3. looks up an OP_RETURN that includes this hash and is signed by the
payee.  If there is more than one, the one that comes in the earlier block
prevails.
4. calculates the spend proof and makes sure that it is included in the
same OP_RETURN
5. makes sure the same spend proof is not included anywhere in the same or
earlier blocks (that is, the coin was not spent before).  Only transactions
by the same author are searched.
6. repeats the same steps for every entry in the history, except the first
entry, which should be a valid burning transaction.

To facilitate exchange of private transaction data, the bitcoin network
protocol can be extended with a new message type.  Unfortunately, it lacks
encryption, hence private payments are really private only when bitcoin is
used over tor.

There are a few limitations that ought to be mentioned:
1. After user A sends a private payment to user B, user A will know what
the spend proof is going to be when B decides to spend the coin.
 Therefore, A will know when the coin was spent by B, but nothing more.
 Neither the new owner of the coin, nor its future movements will be known
to A.
2. Over time, larger outputs will likely be split into many smaller
outputs, whose amounts are not much greater than their denominations.
You’ll have to combine more inputs to send the same amount.  When you want
to send a very large amount that is much greater than the highest available
denomination, you’ll have to send a lot of private transactions, your
bitcoin transaction with so many OP_RETURNs will stand out, and their
number will roughly indicate the total amount.  This kind of privacy
leakage, however it applies to a small number of users, is easy to avoid by
using multiple addresses and storing a relatively small amount on each
address.
3. Exchanges and large merchants will likely accumulate large coin
histories.  Although fragmented, far from complete, and likely outdated, it
is still something to bear in mind.

No hard or soft fork is required, BBC is just a separate privacy preserving
currency on top of bitcoin blockchain, and the same private keys and
addresses are used for both BBC and the base currency BTC.  Every BCC
transaction must be enclosed into by a small BTC transaction that stores
the OP_RETURNs and pays for the fees.

Are there any flaws in this design?

Originally posted to BCT https://bitcointalk.org/index.php?topic=1574508.0,
but got no feedback so far, apparently everybody was consumed with bitfinex
drama and now mimblewimble.

Tony

-------------------------------------
I just realize that if we have OP_CAT, OP_CHECKPRIVATEKEYVERIFY (aka OP_CHECKPRIVPUBPAIR) is not needed (and is probably better for privacy)

 

Bob has the prikey-x for pubkey-x. Alice and Bob will agree to a random secret nonce, k. They calculate r, in the same way as signing a transaction.

 

The script is:

 

SIZE <r-length + 1> ADD <0x30> SWAP CAT <0x02|r-length|r> CAT SWAP CAT <pubkey-x> CECHKSIGVERIFY <Bob pubkey hash> CHECKSIG

 

To redeem, Bob has to provide:

 

<Bob sig> <0x02|s-length|s|sighashtype>

 

With k, s and sighash, Alice (and only Alice) can recover the prikey-x with the well-known k-reuse exploit

( https://en.wikipedia.org/wiki/Elliptic_Curve_Digital_Signature_Algorithm )

 

The script will be much cleaner if we remove the DER encoding in the next generation of CHECKSIG

 

The benefit is prikey-x remains a secret among Alice and Bob. If they don’t mind exposing the prikey-x, they could use r = x coordinate of pubkey-x, which means k = prikey-x (https://bitcointalk.org/index.php?topic=291092.0) This would reduce the witness size a little bit as a DUP may be used

 

From: bitcoin-dev-bounces@lists.linuxfoundation.org [mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Tier Nolan via bitcoin-dev
Sent: Monday, 29 February, 2016 19:53
Cc: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] BIP CPRKV: Check private key verify

 

On Mon, Feb 29, 2016 at 10:58 AM, Mats Jerratsch <matsjj@gmail.com <mailto:matsjj@gmail.com> > wrote:

This is actually very useful for LN too, see relevant discussion here

http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011827.html

 

Is there much demand for trying to code up a patch to the reference client?  I did a basic one, but it would need tests etc. added.

I think that segregated witness is going to be using up any potential soft-fork slot for the time being anyway.


-------------------------------------
I would suggest that, before discussing how best to fork the chain to meet this objective, we consider the objective.

The implementers have acknowledged that this does not represent a performance improvement. Especially given that this was apparently not initially understood, that alone is good reason for them to reconsider.

The remaining stated objective is reduction of code complexity. Let us be very clear, a proposal to change the protocol must be considered independently of any particular implementation of the protocol. While the implementation of BIP34 style activation may be hugely complex in the satoshi code, it is definitely not complex as a matter of necessity.

Activation constitutes maybe a dozen lines of additional code in libbitcoin. The need to hit the chain (or cache) to obtain historical header info will remain for proof of work, so this change doesn't even accomplish some sort of beneficial isolation from blockchain history.

So, at best, we are talking about various ways to introduce a consensus fork so that a well designed implementation  can remove a tiny amount of already-written code and associated tests. In my opinion this is embarrassingly poor reasoning. It would be much more productive to reduce satoshi code complexity in ways that do not impact the protocol. There are a *huge* number of such opportunities, and in fact activation is one of them. Once that is done, we can talk about forking to reduce source code complexity.

These fork suggestions actually increase *necessary* complexity for any implantation that takes a rational approach to forks. By rational I mean *additive*. Deleting rules from Bitcoin code is simply bad design. Rules are never removed, they are added. A new rule to modify an old rule is simply a new rule. This is new and additional code. So please don't assume in this "proposal" that this makes development simpler for other implementations, that is not a necessary conclusion.

e

> On Nov 16, 2016, at 1:01 PM, Peter Todd via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
>> On Wed, Nov 16, 2016 at 09:32:24AM -0500, Alex Morcos via bitcoin-dev wrote:
>> I think we are misunderstanding the effect of this change.
>> It's still "OK" for a 50k re-org to happen.
>> We're just saying that if it does, we will now have potentially introduced
>> a hard fork between new client and old clients if the reorg contains
>> earlier signaling for the most recent ISM soft fork and then blocks which
>> do not conform to that soft fork before the block height encoded activation.
>> 
>> I think the argument is this doesn't substantially add to the confusion or
>> usability of the system as its likely that old software won't even handle
>> 50k block reorgs cleanly anyway and there will clearly have to be human
>> coordination at the time of the event.  In the unlikely event that the new
>> chain does cause such a hard fork, that coordination can result in everyone
>> upgrading to software that supports the new rules anyway.
>> 
>> So no, I don't think we should add a checkpoint.  I think we should all
>> just agree to a hard fork that only has a very very slim chance of any
>> practical effect.
> 
> So, conceptually, another way to deal with this is to hardcode a blockhash
> where we allow blocks in a chain ending with that blockhash to _not_ follow
> BIP65, up until that blockhash, and any blockchain without that blockhash must
> respect BIP65 for all blocks in the chain.
> 
> This is a softfork: we've only added rules that made otherwise valid chains
> invalid, and at the same time we are still accepting large reorgs (albeit under
> stricter rules than before).
> 
> I'd suggest we call this a exemption hash - we've exempted a particular
> blockchains from a soft-forked rule that we would otherwise enforce.
> 
> -- 
> https://petertodd.org 'peter'[:-1]@petertodd.org
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
On Mon, May 2, 2016 at 10:13 PM, Matt Corallo via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Hi all,
>
> The following is a BIP-formatted design spec for compact block relay
> designed to limit on wire bytes during block relay. You can find the
> latest version of this document at
> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.

Thanks Matt!

I've been testing this for a couple weeks (in various forms).  I've
been getting over 96% reduction in block-bytes sent. I don't have a
good metric for it, but bandwidth spikes are greatly reduced. The
largest blocktxn message I've seen on a node that has been up for at
least a day is 475736 bytes. 94% of the blocks less than 100kb must be
sent in total.

In the opportunistic mode my measurements are showing 73% of blocks
transferred with 0.5 RTT even without prediction, 87% if up to 4
additional transactions are predicted, and 91% for 30 transactions (my
rough estimate for the 10k maximum prediction suggested in the BIP.


-------------------------------------
It would be nice to decouple the venue, but even BIP 1 gives that
control to whoever controls the mailing list: "Following a discussion,
the proposal should be sent to the bitcoin-dev list and the BIP editor
with the draft BIP." (BIP 1)

A neater way to do it might be to replace references to the mailing list
with "public discussion medium" where "medium" can be defined as
something like any discussion forum frequented by the wider development
community, like the pull requests section of the BIP repo, conferences, etc.

On 02/02/16 15:58, Gavin Andresen via bitcoin-dev wrote:
> On Mon, Feb 1, 2016 at 5:53 PM, Luke Dashjr via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>
>     I've completed an initial draft of a BIP that provides
>     clarifications on the
>     Status field for BIPs, as well as adding the ability for public
>     comments on
>     them, and expanding the list of allowable BIP licenses.
>
>     https://github.com/luke-jr/bips/blob/bip-biprevised/bip-biprevised.mediawiki
>
>     I plan to open discussion of making this BIP an Active status
>     (along with BIP
>     123) a month after initial revisions have completed. Please
>     provide any
>     objections now, so I can try to address them now and enable
>     consensus to be
>     reached.
>
>  
>
> I like the more concrete definitions of the various statuses.
>
> I don't like the definition of "consensus".  I think the definition
> described gives too much centralized control to whoever controls the
> mailing list and the wiki.
>
> -- 
> --
> Gavin Andresen
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
We have models for estimating the probability that a block is orphaned
given average network bandwidth and block size.

The question is, do we have objective measures of these two quantities?
Couldn't we target an orphan_rate < max_rate?



On Dec 10, 2016 1:01 PM, <bitcoin-dev-request@lists.linuxfoundation.org>
wrote:

Send bitcoin-dev mailing list submissions to
        bitcoin-dev@lists.linuxfoundation.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
or, via email, send a message with subject or body 'help' to
        bitcoin-dev-request@lists.linuxfoundation.org

You can reach the person managing the list at
        bitcoin-dev-owner@lists.linuxfoundation.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of bitcoin-dev digest..."


Today's Topics:

   1. Managing block size the same way we do difficulty (aka
      Block75) (t. khan)
   2. Re: Managing block size the same way we do difficulty (aka
      Block75) (s7r)


----------------------------------------------------------------------

Message: 1
Date: Mon, 5 Dec 2016 10:27:32 -0500
From: "t. khan" <teekhan42@gmail.com>
To: bitcoin-dev@lists.linuxfoundation.org
Subject: [bitcoin-dev] Managing block size the same way we do
        difficulty      (aka Block75)
Message-ID:
        <CAGCNRJqdu7DMC+AMR4mYKRAYStRMKVGqbnjtEfmzcoeMij5u=A@mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

BIP Proposal - Managing Bitcoin?s block size the same way we do difficulty
(aka Block75)

The every two-week adjustment of difficulty has proven to be a reasonably
effective and predictable way of managing how quickly blocks are mined.
Bitcoin needs a reasonably effective and predictable way of managing the
maximum block size.

It?s clear at this point that human beings should not be involved in the
determination of max block size, just as they?re not involved in deciding
the difficulty.

Instead of setting an arbitrary max block size (1MB, 2MB, 8MB, etc.) or
passing the decision to miners/pool operators, the max block size should be
adjusted every two weeks (2016 blocks) using a system similar to how
difficulty is calculated.

Put another way: let?s stop thinking about what the max block size should
be and start thinking about how full we want the average block to be
regardless of size. Over the last year, we?ve had averages of 75% or
higher, so aiming for 75% full seems reasonable, hence naming this concept
?Block75?.

The target capacity over 2016 blocks would be 75%. If the last 2016 blocks
are more than 75% full, add the difference to the max block size. Like this:

MAX_BLOCK_BASE_SIZE = 1000000
TARGET_CAPACITY = 750000
AVERAGE_OVER_CAP = average block size of last 2016 blocks minus
TARGET_CAPACITY

To check if a block is valid, ? (MAX_BLOCK_BASE_SIZE + AVERAGE_OVER_CAP)

For example, if the last 2016 blocks are 85% full (average block is 850
KB), add 10% to the max block size. The new max block size would be 1,100
KB until the next 2016 blocks are mined, then reset and recalculate. The
1,000,000 byte limit that exists currently would remain, but would
effectively be the minimum max block size.

Another two weeks goes by, the last 2016 blocks are again 85% full, but now
that means they average 935 KB out of the 1,100 KB max block size. This is
93.5% of the 1,000,000 byte limit, so 18.5% would be added to that to make
the new max block size of 1,185 KB.

Another two weeks passes. This time, the average block is 1,050 KB. The new
max block size is calculated to 1,300 KB (as blocks were 105% full, minus
the 75% capacity target, so 30% added to max block size).

Repeat every 2016 blocks, forever.

If Block75 had been applied at the difficulty adjustment on November 18th,
the max block size would have been 1,080KB, as the average block during
that period was 83% full, so 8% is added to the 1,000KB limit. The current
size, after the December 2nd adjustment would be 1,150K.

Block75 would allow the max block size to grow (or shrink) in response to
transaction volume, and does so predictably, reasonably quickly, and in a
method that prevents wild swings in block size or transaction fees. It
attempts to keep blocks at 75% total capacity over each two week period,
the same way difficulty tries to keep blocks mined every ten minutes. It
also keeps blocks as small as possible.

Thoughts?

-t.k.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/
attachments/20161205/c24d6c6d/attachment-0001.html>

------------------------------

Message: 2
Date: Sat, 10 Dec 2016 12:44:31 +0200
From: s7r <s7r@sky-ip.org>
To: bitcoin-dev@lists.linuxfoundation.org
Subject: Re: [bitcoin-dev] Managing block size the same way we do
        difficulty (aka Block75)
Message-ID: <c318f76d-0904-2e1b-453b-60179f8209bb@sky-ip.org>
Content-Type: text/plain; charset="utf-8"

t. khan via bitcoin-dev wrote:
> BIP Proposal - Managing Bitcoin?s block size the same way we do
> difficulty (aka Block75)
>
> The every two-week adjustment of difficulty has proven to be a
> reasonably effective and predictable way of managing how quickly blocks
> are mined. Bitcoin needs a reasonably effective and predictable way of
> managing the maximum block size.
>
> It?s clear at this point that human beings should not be involved in the
> determination of max block size, just as they?re not involved in
> deciding the difficulty.
>
> Instead of setting an arbitrary max block size (1MB, 2MB, 8MB, etc.) or
> passing the decision to miners/pool operators, the max block size should
> be adjusted every two weeks (2016 blocks) using a system similar to how
> difficulty is calculated.
>
> Put another way: let?s stop thinking about what the max block size
> should be and start thinking about how full we want the average block to
> be regardless of size. Over the last year, we?ve had averages of 75% or
> higher, so aiming for 75% full seems reasonable, hence naming this
> concept ?Block75?.
>
> The target capacity over 2016 blocks would be 75%. If the last 2016
> blocks are more than 75% full, add the difference to the max block size.
> Like this:
>
> MAX_BLOCK_BASE_SIZE = 1000000
> TARGET_CAPACITY = 750000
> AVERAGE_OVER_CAP = average block size of last 2016 blocks minus
> TARGET_CAPACITY
>
> To check if a block is valid, ? (MAX_BLOCK_BASE_SIZE + AVERAGE_OVER_CAP)
>
> For example, if the last 2016 blocks are 85% full (average block is 850
> KB), add 10% to the max block size. The new max block size would be
> 1,100 KB until the next 2016 blocks are mined, then reset and
> recalculate. The 1,000,000 byte limit that exists currently would
> remain, but would effectively be the minimum max block size.
>
> Another two weeks goes by, the last 2016 blocks are again 85% full, but
> now that means they average 935 KB out of the 1,100 KB max block size.
> This is 93.5% of the 1,000,000 byte limit, so 18.5% would be added to
> that to make the new max block size of 1,185 KB.
>
> Another two weeks passes. This time, the average block is 1,050 KB. The
> new max block size is calculated to 1,300 KB (as blocks were 105% full,
> minus the 75% capacity target, so 30% added to max block size).
>
> Repeat every 2016 blocks, forever.
>
> If Block75 had been applied at the difficulty adjustment on November
> 18th, the max block size would have been 1,080KB, as the average block
> during that period was 83% full, so 8% is added to the 1,000KB limit.
> The current size, after the December 2nd adjustment would be 1,150K.
>
> Block75 would allow the max block size to grow (or shrink) in response
> to transaction volume, and does so predictably, reasonably quickly, and
> in a method that prevents wild swings in block size or transaction fees.
> It attempts to keep blocks at 75% total capacity over each two week
> period, the same way difficulty tries to keep blocks mined every ten
> minutes. It also keeps blocks as small as possible.
>
> Thoughts?
>
> -t.k.
>

I like the idea. It is good wrt growing the max. block size
automatically without human action, but the main problem (or question)
is not how to grow this number, it is what number can the network
handle, considering both miners and users. While disk space requirements
might not be a big problem, block propagation time is. The time required
for a block to propagate in the network (or at least to all the miners)
is directly dependent of its size.  If blocks take too much time to
propagate in the network, the orphan rate will increase in unpredictable
ways. For example if the internet speed in China is worse than in
Europe, and miners in China have more than 50% of the hashing power,
blocks mined by European miners might get orphaned.

The system as described can also be gamed, by filling the network with
transactions. Miners have the monetary interest to include as many
transactions as possible in a block in order to collect the fees.
Regardless how you think about it, there has to be a maximum block size
that the network will allow as a consensus rule. Increasing it
dynamically based on transaction volume will reach a point where the
number got big enough that it broke things. Bitcoin, because its
fundamental design, can scale by using offchain solutions.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/
attachments/20161210/c231038d/attachment-0001.sig>

------------------------------

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


End of bitcoin-dev Digest, Vol 19, Issue 4
******************************************

-------------------------------------
Agreed, the clear goal of 10 minutes per block is why the difficulty
adjustment works well. Blocks averaging 75% full is the clear goal of the
described method. That's the target to attempt.

Under Block75, there will still be full blocks. There will still be
transaction fees and a fee market. The fees will be lower than they are now
of course.

Hardcoding a cap will inevitably become a roadblock (again), and we'll be
back in the same position as we are now. Permanent solutions are preferred.

On Sat, Dec 10, 2016 at 6:12 PM, Bram Cohen <bram@bittorrent.com> wrote:

> On Mon, Dec 5, 2016 at 7:27 AM, t. khan via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>>
>> Put another way: let’s stop thinking about what the max block size should
>> be and start thinking about how full we want the average block to be
>> regardless of size. Over the last year, we’ve had averages of 75% or
>> higher, so aiming for 75% full seems reasonable, hence naming this concept
>> ‘Block75’.
>>
>
> That's effectively making the blocksize limit completely uncapped and only
> preventing spikes, and even in the case of spikes it doesn't differentiate
> between 'real' traffic and low value spam attacks. It suffers from the same
> fundamental problems as bitcoin unlimited: There are in the end no
> transaction fees, and inevitably some miners will want to impose some cap
> on block size for practical purposes, resulting in a fork.
>
> Difficulty adjustment works because there's a clear goal of having a
> certain rate of making new blocks. Without a target to attempt automatic
> adjustment makes no sense.
>

-------------------------------------
As part of my recent work(1) on OpenTimestamps I've been putting some thought
towards how to interpret the nTime fields in block headers, for the purpose of
timestamping. I'd like to get some peer review on the following scheme I've
come up with.


# Motivation

We want to use the Bitcoin blockchain to provide evidence (the "attestation")
that a message M existed prior to some point in time T. Exactly how we do this
is beyond the scope of this post, but suffice to say we show that some block
header b cryptographically commits to the message, e.g. via a commitment
operation path proof, as implemented by OpenTimestamps.

A valid timestamp is simply one where T is a point in time where the message
did in fact exist. Of course, if a timestamp for time T is valid, all
subsequent T+d are also valid; such timestamps are simply more conservative
versions of the same statement.

A naively approach - as is implemented by most (all?) existing Bitcoin
timestamping schemes - is to assume that the block header's nTime field was
perfectly accurate, and thus M exists prior to the block's nTime. But that
doesn't take into account malicious miners, who may backdate their blocks.


# Threat Model

We assume miners are divided into two categories:

1) Dishonest Miners --- These miners are actively conspiring to create invalid
timestamps for time's prior to when the message existed. A dishonest miner will
set the nTime field in blocks they create to the minimum possible value.

2) Honest Miners --- These miners set nTime in blocks they create to
approximately the current true time. An honest miner may use techniques such as
nTime-rolling. Additionally, all honest miners may be simultaneously affected
by systematic misconfigurations.


## nTime Rolling

Prior to BIP113, reducing a block's nTime from the work given by a pool by even
a second could easily render it invalid, as the pool may have included
nLockTime'd transactions in the block. Thus hashing software was designed to
only roll nTime in the forward direction, not reverse, even though rolling
could be done in the reverse direction, up to the limit of the median-time-past
+ 1.

The Stratum mining protocol doesn't even have a way to tell clients what the
minimum allowed time is, just a single field, nTime, which is defined as "the
current time". Thus Stratum hashers will only ever increase nTime, which can
never result in an invalid timestamp if the original, unrolled, nTime would
have been a valid timestamp.

The getblocktemplate protocol does support telling hashers the minimum time via
the mintime field, which Bitcoin Core sets to the median-time-past. Regardless,
it appears that the pools supporting GBT (Eligius) return a much tighter limit
on mintime than the median-time-past, just 180 seconds, and as of writing,
don't actually declare that the ntime field is mutable anyway.

From an implementation point of view, relying on being able to roll nTime
backwards is unwise anyway, as the amount you can roll it back may be minimal
(e.g. if multiple blocks were recently found).

Since all miners have an incentive for time to move forward to keep difficulty
down it's reasonable to assume that the above observed behavior will continue,
and nTime rolling in the reverse direction will be a minimal effect; we'll
assume no miner rolls nTime backwards more than 1 hour.


## Systematic Errors

1) Botched daylight savings time changes --- While internal clocks should be
unaffected by timezone changes, it's plausible that some kind of mistake
related to daylight savings could result in the time being set incorrectly +- 1
hour. For example, multiple large miners might manually set their clocks, based
on an incorrect understanding of what time it was.

2) Broken NTP servers --- It's reasonable to assume that many miners are using
NTP to set their clocks, and it's plausible that they're using the same NTP
servers. Of course, a broken NTP server could return any time at all! The
Bitcoin protocol considers blocks to be valid if nTime is set up to 2 hours in
the future (from the perspective of the local node) so we'll say instead that
we expect systematic NTP errors to be corrected with high probability if
they're more than 2 hours in magnitude - more than that and the Bitcoin network
is broken in a very visible way anyway.

Thus, we'll assume honest miners always create blocks with nTime greater than
the true time minus two hours, which accounts for both likely daylight savings
time misconfigurations, and likely NTP server misconfigurations. Additionally,
two hours is greater than any expected effects from nTime rolling.


# Proposed Algorithm

For a timestamp anchored at a block of height x we'll define the time T it
represents as:

    T = max(block[i].nTime for i in {x, ..., x + N-1}) + max_offset

In short, T is the maximum nTime out of the N blocks that confirmed the
timestamp, including first block that actually committed the timestamp;
max_offset is the maximum nTime offset we expect from a block created by an
honest miner, discussed above.

The dishonest miners can successfully create an invalid timestamp iff all N
blocks are found by them; if any block is found by an honest miner, the nTime
field will be set correctly. Of course T may not be the minimum possible value,
but the timestamp will be at least valid.

So how big should N be? Let q be the ratio of dishonest miners to total hashing
power. The probability that all N blocks are found by dishonest miners is q^N,
and thus the probability P that at least one block is found by an honest miner
is:

    P = 1 - q^N  =>  N = log(1 - P)/log(q)

If the dishonest miners have q>0.5, the situation is hopeless, as they can
reject blocks from honest miners entirely; the only limit on them setting nTime
is the median-time-past rule, which only requires blocks timestamps to
increment by one second per block (steady state). Thus we'll assume q=0.5, the
worst possible case where a Bitcoin timestamp can still be meaningful evidence:

    P = 97%      => N = 5
    P = 99%      => N = 7
    P = 99.9%    => N = 10
    P = 99.99%   => N = 14
    P = 99.999%  => N = 17
    P = 99.9999% => N = 20

The reliability for the higher N is higher than the actual reliability of
Bitcoin itself. On the other hand, there's no known instance where miners have
ever created blocks with nTime's significantly less than true time on a wide
scale; even in the well-known cases where the Bitcoin network has temporarily
failed due to forks, timestamps produced during those times would be valid, if
delayed by a few hours.

Similarly, if we assume a lower q, say a single "rogue" 20% mining pool, we
get:

    q = 0.20, P = 99.99% => N = 6

Another way to think about the choice of N is to compare its contribution to
how conservative the timestamp is - T as compared to the true time - to the
effect of the max-honest-miner-offset we choose earlier. For example, 98% of
the time at least 6 blocks will be found within 2 hours, which means that if we
pick N=7, 98% of the time the conservatism added by N will be less than the
contribution of the max offset.


# UI Considerations

One problem with the above algorithm is that it will frequently return
timestamps in the future, from the perspective of the user. A user who sees a
message like the following at 2:00 pm, immediately after their timestamp
confirms, is understandably going to be confused:

   Bitcoin: 99% chance that <f> existed prior to 4:00 pm, Jan 1st 2016

A reasonable approach to this problem might just to refrain from displaying
timestamps at all until the local time is after the timestamp; the UI could
describe the timestamp as "Not yet confirmed"

It may also be reasonable to round the timestamp up to the nearest day when
displaying it. However what timezone to use is a tricky issue; people rarely
expect to see timezones specified alongside dates.

Of course, in some cases a less conservative approach to interpreting the
timestamp is reasonable; those users however should be reading and
understanding the calculations in this post!


# References

1) https://petertodd.org/2016/opentimestamps-announcement

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Dave Hudson [dave@hashingit.com] wrote:
> A damping-based design would seem like the obvious choice (I can think of a
> few variations on a theme here, but most are found in the realms of control
> theory somewhere).  The problem, though, is working working out a timeframe
> over which to run the derivative calculations.

>From a measurement theory perspective this is straightforward.  Each block is a
measurement, and error propagation can be performed to derive an error on the
derivatives.

The statistical theory of Bitcoin's block timing is known as a Poisson Point
Process: https://en.wikipedia.org/wiki/Poisson_point_process or temporal point
process.  If you google those plus "estimation" you'll find a metric shit-ton of
literature on how to handle this.

> The problem is the measurement of the hashrate, which is pretty inaccurate at
> best because even 2016 events isn't really enough (with a completely constant
> hash rate running indefinitely we'd see difficulty swings of up to +/- 5% even
> with the current algorithm).  In order to meaningfully react to a major loss
> of hashing we'd still need to be considering a window of probably 2 weeks.

You don't want to assume it's constant in order to get a better measurement.
The assumption is clearly false.  But, errors can be calculated, and retargeting
can take errors into account, because no matter what we'll always be dealing
with a finite sample.

Personally I don't think difficulty target variations are such a big deal, if
the algorithm targets that over any long time interval, the average block time
is 10 min.  Bitcoin's current algorithm fails here, with increasing hashrate (as
we have), it issues coins faster than its assumed schedule.

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 



-------------------------------------
On Thu, Jun 23, 2016 at 02:16:48PM +0200, Pieter Wuille wrote:
> On Jun 23, 2016 14:10, "Peter Todd" <pete@petertodd.org> wrote:
> 
> > Right, so you accept that we'll exert some degree of editorial control;
> the
> > question now is what editorial policies should we exert?
> 
> No, I do not. I am saying that some degree of editorial control will
> inevitably exist, simply because there is some human making the choice of
> assigning a BIP number and merging. My opinion is that we should try to
> restrict that editorial control to only be subject to objective process,
> and not be dependent on personal opinions.
>
> > My argument is that rejecting BIP75 is something we should do on
> > ethical/strategic grounds. You may disagree with that, but please don't
> troll
> > and call that "advocating censorship"
> 
> I think that you are free to express dislike of BIP75. Suggesting to remove
> it for that reason is utterly ridiculous to me, whatever you want to call
> it.

In the future we're likely to see a lot of BIPs around AML/KYC support, e.g.
adding personal identity information to transactions, blacklist standards, etc.
Should we accept those BIPs into the bips repo?

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
I've completed an initial draft of a BIP for updating getblocktemplate for 
segregated witness here:
    https://github.com/luke-jr/bips/blob/segwit_gbt/bip-segwit-gbt.mediawiki

Please review and comment (especially with regard to the changes in the 
sigoplimits handling).

(Note: libblkmaker's reference implementation is at this time incompatible 
with the "last output" rule in this BIP.)

Thanks,

Luke


-------------------------------------
On Wednesday 21 Sep 2016 14:00:23 Andreas Schildbach via bitcoin-dev wrote:
> Just glancing over your BIP, I wonder if we should use Protobuf. It uses
> this "flexible" format already and is quite compact/binary. We use
> Protobuf already for the payment protocol, and there is very good tool
> support.

There is a lot of overlap between different binary formats. Looking through 
the on-the-wire protocol you'll see that my spec is very similar. Practically 
all the advantages of protobuf are present in CMF. I can write you a java 
parser if you want, it should be easy to port from Qt/C++ code :)
https://github.com/bitcoinclassic/transactions

CMF: 
https://github.com/bitcoinclassic/documentation/blob/master/spec/compactmessageformat.md

There is no tool support needed, just one or two classes. Which personally I 
think is an advantage.


Some advantages of CMF over protobuf from the top of my head;

* It reuses the var-int parsing that Bitcoin uses (which is itself slightly 
different from others).

* zero-copy support (not relevant for this bip, though).

* Additional values addition (i.e. adding new data) is .. tricky in protobuf.
https://developers.google.com/protocol-buffers/docs/proto#updating

* In my experience parsing a message manually (like a SOX parser) is much 
better in reporting errors and detecting wrong usages than auto-generated code 
(but personally I'm not much a fan of auto-generated APIs) at all...

* Generated parsing/writing code will not be as fast as we can make it.

* CMF is more compact (uses less bytes) for its messages.


Protobuf is something I've used before and I think we can do better. I think 
that CMF together with some support classes can do this better.


-------------------------------------

> I guess my question didn't get across. 
> 
> Why would you want to make your usecase do connections over the peer2peer 
> (net.cpp) connection at all?

First, because there _are_ a hight amount of SPV wallets in the field.
SPV wallets are "dumb-clients" with only a tiny value for the bitcoin
network (they don't validate, they don't relay). They already are
decoupled wallets. We need solution that offers higher privacy and
higher traffic analysis resistance.

Using the p2p channel for communication between full validation peers
and wallet-only-peers makes sense IMO because wallet-only-peers can
slowly validate the chain and create a UTXO set in the background (could
take a couple of weeks) or solve other purposes that increases the
security and/or serving something back to the bitcoin network.

Sure, you can always use client/server wallets (Coinbase / Copay, etc.)
that offers SSL.
But I strongly recommend to improve the communication and interface
possibilities between wallet-nodes (SPV) and full-validation-nodes.

Otherwise we will very likely see centralization regarding end-user
wallets (with all the large risks of disrupting the community in case of
attacks/thefts, etc.).

_If we think Bitcoin should scale, we also need to scale and improve at
the point where users enter the network and start using Bitcoin._

> Mixing messages that are being sent to everyone and encrypted messages is 
> asking for trouble.
> Making your private connection out-of-band would work much better.

The current encryption BIP requires to encrypt the complete traffic.
Having an option to do analysis resistant communication with a remote
peer within the protocol itself is something that is very valuable IMO.


>>> Also, you didn't actually address the attack-vector.
>>
>> Which attack-vector?
> 
> The statistical attack I mentioned earlier.  Which comes from knowing which 
> plain text messages are being sent over the encrypted channel, So as long as 
> you keep saying you want to encrypt data that identical copies of are being 
> sent to other nodes at practically the same time, you will keep being 
> vulnerable to that.

The encryption BIP recommends Chacha20-Poly1305 as encryption AEAD. This
is a very broad used encryption scheme (Google uses it to connect
Android phones with their cloud services).

Completely avoiding side channel on data analysis would probably require
extremely inefficient constant time encrypted datastreams.

Also, the BIP allows combining of multiple plaintext message in one
encrypted message.

Additionally we could extend the enc. BIP by allowing random padding of
encrypted messages or other techniques to reduce side channel analysis.

</jonas>


-------------------------------------
On Fri, Sep 23, 2016 at 10:20 PM, Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> In the innocent use case of this opcode, a double-spend has already occurred,
> and this should be a strict improvement. In the non-innocent abuse of this
> opcode, I don't see that it's any worse than simply double-spending.

There is a fungibility hit... right now, absent double spends (and
privacy issues), every coin you might get paid is equal.

With this script feature as described, you could get paid a coin which
has one of these in its recent past, pinning the block immediately
before it. A reorg long enough to remove that block-- due to an
attack, or an ordinary block race, or some kind of consensus glitch
(like we had in March 2013 or around the activation of BIP65)-- is
_guaranteed_ to invalidate those coins, even without any double spend.

If the scheme doesn't do as I suggest and prevent over-eager usage
(perhaps 100 is too much, I just decided to match coinbases); then it
should probably have a consensus enforced explicit "maximum survivable
reorg" that is traced along with the outputs, so that someone who
received exposed coins could handle it sensibly.

Just for plain engineering reasons, I still think it is important to
now allow overly short back references. If the reference has to be a
few blocks back we don't need to worry about short forks breaking
propagation, and simple mempool handling like purging all CBAH
transactions on a large reorg would work fine.  It need not be so long
as to implicate Petertodd's concern that you could only use it where
it wouldn't matter.  (Though I also disagree that a depth of 100
achieves that, consider persistent chain forks).


-------------------------------------
Please do report bugs to https://github.com/bitcoin/bitcoin . If you never report them of course they won't get fixed. I'm not aware of test suite failures and know a bunch of folks who use CentOS, though not sure how many develop on it.

On December 18, 2016 12:07:36 PM PST, Alice Wonder via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>On 12/14/2016 07:38 PM, Juan Garavaglia via bitcoin-dev wrote:
>
>>
>> For reasons I am unable to determine a significant number of node
>> operators do not upgrade their clients.
>
>I almost did not update to 0.13.0 because the test suite was failing
>due 
>to python errors. How to fix them was posted on bitcointalk.
>
>0.13.1 came with new python errors in the test suite. So I just said 
>fuck it.
>
>When the test suite actually works in my fairly standard environment 
>(CentOS) in the distributed release, I will upgrade.
>
>Until then, I'm not jumping through hoops to make the test suite work 
>and I'm not running clients that haven't passed the test suite so
>that's 
>why I almost didn't update to 0.13.0 and haven't updated since.
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev@lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
OP_RETURN was not part of isStandard? from day one. Once it was supported
by Core it became necessary to actually support it, not try to support it
in one part of the software and not in others. The whole reason it was
supported is because without it people will use more heinous methods to
encode data on the blockchain. There's no way to stop people from doing
that, so this compromise seemed best for everyone.

I think we should actually define "spam". To me a valid transaction someone
willing pays for is never spam. Also PaymentRequests would be a very
inefficient way to spam. It would be much easier to write a script to
automatically create and submit transactions yourself. With PaymentRequests
 customers have to initiate the transaction and submit/pay for it one by
one.

What is actually the worst case scenario that those opposed to this are
concerned about? That this takes off like wild fire and all of the sudden
millions of people are using PaymentRequests and creating small
transactions? That seems like a win for Bitcoin. It will help spread
support for the Payment protocol and IF it becomes a problem it's because
so many people are using it. In which case there's a very valid use case
for Bitcoin that people are obviously excited about.

I really don't like the idea of policing other people's use of the
protocol. If a transaction pays its fee and has a greater than dust value,
it makes no sense to object to it.

On Tue, Jan 26, 2016 at 8:19 AM, Thomas Kerin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> On 26/01/16 03:30, Toby Padilla via bitcoin-dev wrote:
> > There are already valid use cases for OP_RETURN, it only makes sense
> > to fully support the feature. The only reason it's not supported now
> > is because the Payments protocol came before OP_RETURN.
> >
> You keep saying OP_RETURN is new, but it has been there from day one.
> It's purpose is causing script execution to end if encountered.
>
> Since then, we have tolerated putting pushdata's after it, and even
> raised the limit for the size of this data. It still doesn't mean every
> proposal has to be rewritten to cater for a new allowance we give
> OP_RETURN.
>
>
> > I've also been exploring this area with key.run
> > (https://git.playgrub.com/toby/keyrun) and want the functionality for
> > voting based on aggregate OP_RETURN value. *Not* to store data on the
> > blockchain, but to associate content pointers with transactions.
> >
> > I think that since OP_RETURN has already been approved and supported
> > it doesn't make much sense for me to have to re-defend it from scratch
> > here.
>
> I'd generally agree with Luke. Removing the cost of this hurts bitcoin,
> and ironically, your application to a certain degree. Just because you
> can do a thing one way, it doesn't mean you should. Especially if your
> applications success depends on people spamming OP_RETURN hashes of
> every torrent they like.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
By sending a public seed,  there's no way for someone to use the
transmitted address and trace the total amount of payments to it.

On Aug 10, 2016 12:02 PM, "Daniel Hoffman via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Erik
> What would be the advantages of transmitting a BIP32 public seed, instead
> of a plain address?
>
> Theo
> I didn't really think of that, but that's genius.
>
> On Wed, Aug 10, 2016 at 6:49 AM, Theo Chino via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Another use for the audio would be for watches that can listen but can't
>> use a camera (ie: Samsung S2), so sound would be great.
>>
>> On Wed, Aug 10, 2016 at 7:42 AM, Erik Aronesty via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> NOTE:
>>>
>>> Addresses aren't really meant to be broadcast - you should probably be
>>> encoding BIP32 public seeds, not addresses.
>>>
>>> OR simply:
>>>
>>> - Send btc to rick@q32.com
>>> - TXT record _btc.rick.q32.com is queried (_<coin-code>.<name>.<domain>)
>>> - DNS-SEC validation is *required*
>>> - TXT record contains addr:[<bip32-pub-seed>]
>>>
>>> Then you can just say, in the podcast, "Send your bitcoin donations to
>>> rick@q32.com".   And you can link it to your email address, if your
>>> provider lets you set up a TXT record.   (By structuring the TXT record
>>> that way, many existing email providers will support the standard without
>>> having to change anything.)
>>>
>>> This works with audio, video, web and other publishing formats... and
>>> very little infrastructure change is needed.
>>>
>>>
>>> On Wed, Aug 10, 2016 at 6:41 AM, Tier Nolan via bitcoin-dev <
>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>> Have you considered CDMA?  This has the nice property that it just
>>>> sounds like noise.  The codes would take longer to send, but you could send
>>>> multiple bits at once and have the codes orthogonal.
>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev@lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Dear list

I've spent the past couple of months developing a simple protocol for
working with payment channels. I've written up a specification of how
it operates, in an attempt to standardize the operations of opening,
paying and closing.

This specification was derived from a mostly-working implementation
which will be open sourced in a short while (after a clean-up). It's
written in Haskell.

I'm sharing the specification now because I think it has value in
itself to have everything documented, especially since I've chosen to
write the implementation in Haskell, and for people to point out any
errors or basically anything I haven't thought of.

Link:

https://raw.githubusercontent.com/runeksvendsen/simple-bitcoin-payment-channel-protocol/master/simple-bitcoin-payment-channel-protocol-v0.1.txt




Regards
Rune


-------------------------------------
I apologize if this discussion should be moved to -discuss, I'll let the
moderators decide, I've copied both.

And Gavin, I apologize for picking on you here, because certainly this
carelessness in how people represent "facts" applies to both sides, but
much of this discussion really infuriates me.
I believe it is completely irresponsible for you to state:
"There will be approximately zero percentage of hash power left on the
weaker branch of the fork, based on past soft-fork adoption by miners"
Sure, the rest of the technical community is capable of evaluating that for
themselves, but your statements are considered authoritative by much larger
audience.  In truth, no one has any idea what would happen if the proposed
Classic hard fork activated with 75% right now.  There is some chance you
are right, but there is a very legitimate possibility that a concerted
effort would arise to maintain a minority fork or perhaps if miners don't
see nearly a complete switch over, many of them might themselves reverse
the fork if they think it would be easier to achieve consensus that way.
We as a community have never been in such a situation before and it
behooves us to speak honestly and directly about the uncertainty of the
situation.

And the back and forth discussion over your BIP has been in large part a
charade.  People asking why you aren't picking 95% know very well why you
aren't, but lets have an honest discussion of what the risks and in your
mind potential benefits of 75% are.   Important debate about parameters of
your BIP get lost because we're sniping at each other about known
disagreements.  For instance, I strongly believe 28 days is far too short.
I think its extremely unlikely that those who are opposed to a contentious
hard fork will do the development work to prepare for it as that may only
make it more likely to happen.  Thus if you did achieve activation with
75%, its almost impossible to imagine that if Bitcoin Core decided to come
along (as opposed to pursuing a minority fork) that they'd have the time to
develop and test the patch and roll it out to wide adoption.   If the goal
of your attempt is that any minority that disagreed will "choose" to follow
the majority branch, then you'd be much more likely to achieve that by
giving them time to decide that's what they wanted and roll out the
software to do so.




On Sun, Feb 7, 2016 at 9:16 AM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Sat, Feb 6, 2016 at 3:46 PM, Luke Dashjr via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Saturday, February 06, 2016 5:25:21 PM Tom Zander via bitcoin-dev
>> wrote:
>> > On Saturday, February 06, 2016 06:09:21 PM Jorge Timón via bitcoin-dev
>> wrote:
>> > > None of the reasons you list say anything about the fact that "being
>> > > lost" (kicked out of the network) is a problem for those node's users.
>> >
>> > That's because its not.
>> >
>> > If you have a node that is "old" your node will stop getting new blocks.
>> > The node will essentially just say "x-hours behind" with "x" getting
>> larger
>> > every hour. Funds don't get confirmed. etc.
>>
>> Until someone decides to attack you. Then you'll get 6, 10, maybe more
>> blocks
>> confirming a large 10000 BTC payment. If you're just a normal end user (or
>> perhaps an automated system), you'll figure that payment is good and
>> irreversibly hand over the title to the house.
>>
>
> There will be approximately zero percentage of hash power left on the
> weaker branch of the fork, based on past soft-fork adoption by miners (they
> upgrade VERY quickly from 75% to over 95%).
>
> So it will take a week to get 6 confirmations.
>
> If you are a full node, you are warned that your software is obsolete and
> you must upgrade.
>
> If you are a lightweight node, it SHOULD tell you something is wrong, but
> even if it doesn't, given that people running lightweight nodes run them so
> they don't have to be connected to the network 24/7, it is very likely
> during that week you disconnect and reconnect to the network several times.
> And every time you do that you increase your chances that you will connect
> to full nodes on the majority branch of the chain, where you will be told
> about the double-spend.
>
> All of that is assuming that there is no OTHER mitigation done. DNS seeds
> should avoid reporting nodes that look like they are in the middle of
> initial block download (that are at a block height significantly behind the
> rest of the network), for example.
>
> --
> --
> Gavin Andresen
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version 0.12.0 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.12.0/>

Or through bittorrent:

   magnet:?xt=urn:btih:e6c0cd47cce75e53b04c1c575a39d2022612d1d6&dn=bitcoin-core-0.12.0&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2F

This is a new major version release, bringing new features and other improvements.

See the full release announcement here:

  <https://bitcoincore.org/en/2016/02/23/release-0.12.0/>

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

Upgrading and downgrading
=========================

How to Upgrade
- --------------

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes for older versions), then run the
installer (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or
bitcoind/bitcoin-qt (on Linux).

Downgrade warning
- -----------------

### Downgrade to a version < 0.10.0

Because release 0.10.0 and later makes use of headers-first synchronization and
parallel block download (see further), the block files and databases are not
backwards-compatible with pre-0.10 versions of Bitcoin Core or other software:

* Blocks will be stored on disk out of order (in the order they are
received, really), which makes it incompatible with some tools or
other programs. Reindexing using earlier versions will also not work
anymore as a result of this.

* The block index database will now hold headers for which no block is
stored on disk, which earlier versions won't support.

If you want to be able to downgrade smoothly, make a backup of your entire data
directory. Without this your node will need start syncing (or importing from
bootstrap.dat) anew afterwards. It is possible that the data from a completely
synchronised 0.10 node may be usable in older versions as-is, but this is not
supported and may break as soon as the older version attempts to reindex.

This does not affect wallet forward or backward compatibility.

### Downgrade to a version < 0.12.0

Because release 0.12.0 and later will obfuscate the chainstate on every
fresh sync or reindex, the chainstate is not backwards-compatible with
pre-0.12 versions of Bitcoin Core or other software.

If you want to downgrade after you have done a reindex with 0.12.0 or later,
you will need to reindex when you first start Bitcoin Core version 0.11 or
earlier.

Notable changes
===============

Signature validation using libsecp256k1
- ---------------------------------------

ECDSA signatures inside Bitcoin transactions now use validation using
[https://github.com/bitcoin/secp256k1](libsecp256k1) instead of OpenSSL.

Depending on the platform, this means a significant speedup for raw signature
validation speed. The advantage is largest on x86_64, where validation is over
five times faster. In practice, this translates to a raw reindexing and new
block validation times that are less than half of what it was before.

Libsecp256k1 has undergone very extensive testing and validation.

A side effect of this change is that libconsensus no longer depends on OpenSSL.

Reduce upload traffic
- ---------------------

A major part of the outbound traffic is caused by serving historic blocks to
other nodes in initial block download state.

It is now possible to reduce the total upload traffic via the `-maxuploadtarget`
parameter. This is *not* a hard limit but a threshold to minimize the outbound
traffic. When the limit is about to be reached, the uploaded data is cut by not
serving historic blocks (blocks older than one week).
Moreover, any SPV peer is disconnected when they request a filtered block.

This option can be specified in MiB per day and is turned off by default
(`-maxuploadtarget=0`).
The recommended minimum is 144 * MAX_BLOCK_SIZE (currently 144MB) per day.

Whitelisted peers will never be disconnected, although their traffic counts for
calculating the target.

A more detailed documentation about keeping traffic low can be found in
[/doc/reduce-traffic.md](/doc/reduce-traffic.md).

Direct headers announcement (BIP 130)
- -------------------------------------

Between compatible peers, [BIP 130]
(https://github.com/bitcoin/bips/blob/master/bip-0130.mediawiki)
direct headers announcement is used. This means that blocks are advertized by
announcing their headers directly, instead of just announcing the hash. In a
reorganization, all new headers are sent, instead of just the new tip. This
can often prevent an extra roundtrip before the actual block is downloaded.

With this change, pruning nodes are now able to relay new blocks to compatible
peers.

Memory pool limiting
- --------------------

Previous versions of Bitcoin Core had their mempool limited by checking
a transaction's fees against the node's minimum relay fee. There was no
upper bound on the size of the mempool and attackers could send a large
number of transactions paying just slighly more than the default minimum
relay fee to crash nodes with relatively low RAM. A temporary workaround
for previous versions of Bitcoin Core was to raise the default minimum
relay fee.

Bitcoin Core 0.12 will have a strict maximum size on the mempool. The
default value is 300 MB and can be configured with the `-maxmempool`
parameter. Whenever a transaction would cause the mempool to exceed
its maximum size, the transaction that (along with in-mempool descendants) has
the lowest total feerate (as a package) will be evicted and the node's effective
minimum relay feerate will be increased to match this feerate plus the initial
minimum relay feerate. The initial minimum relay feerate is set to
1000 satoshis per kB.

Bitcoin Core 0.12 also introduces new default policy limits on the length and
size of unconfirmed transaction chains that are allowed in the mempool
(generally limiting the length of unconfirmed chains to 25 transactions, with a
total size of 101 KB).  These limits can be overriden using command line
arguments; see the extended help (`--help -help-debug`) for more information.

Opt-in Replace-by-fee transactions
- ----------------------------------

It is now possible to replace transactions in the transaction memory pool of
Bitcoin Core 0.12 nodes. Bitcoin Core will only allow replacement of
transactions which have any of their inputs' `nSequence` number set to less
than `0xffffffff - 1`.  Moreover, a replacement transaction may only be
accepted when it pays sufficient fee, as described in [BIP 125]
(https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki).

Transaction replacement can be disabled with a new command line option,
`-mempoolreplacement=0`.  Transactions signaling replacement under BIP125 will
still be allowed into the mempool in this configuration, but replacements will
be rejected.  This option is intended for miners who want to continue the
transaction selection behavior of previous releases.

The `-mempoolreplacement` option is *not recommended* for wallet users seeking
to avoid receipt of unconfirmed opt-in transactions, because this option does
not prevent transactions which are replaceable under BIP 125 from being accepted
(only subsequent replacements, which other nodes on the network that implement
BIP 125 are likely to relay and mine).  Wallet users wishing to detect whether
a transaction is subject to replacement under BIP 125 should instead use the
updated RPC calls `gettransaction` and `listtransactions`, which now have an
additional field in the output indicating if a transaction is replaceable under
BIP125 ("bip125-replaceable").

Note that the wallet in Bitcoin Core 0.12 does not yet have support for
creating transactions that would be replaceable under BIP 125.


RPC: Random-cookie RPC authentication
- -------------------------------------

When no `-rpcpassword` is specified, the daemon now uses a special 'cookie'
file for authentication. This file is generated with random content when the
daemon starts, and deleted when it exits. Its contents are used as
authentication token. Read access to this file controls who can access through
RPC. By default it is stored in the data directory but its location can be
overridden with the option `-rpccookiefile`.

This is similar to Tor's CookieAuthentication: see
https://www.torproject.org/docs/tor-manual.html.en

This allows running bitcoind without having to do any manual configuration.

Relay: Any sequence of pushdatas in OP_RETURN outputs now allowed
- -----------------------------------------------------------------

Previously OP_RETURN outputs with a payload were only relayed and mined if they
had a single pushdata. This restriction has been lifted to allow any
combination of data pushes and numeric constant opcodes (OP_1 to OP_16) after
the OP_RETURN. The limit on OP_RETURN output size is now applied to the entire
serialized scriptPubKey, 83 bytes by default. (the previous 80 byte default plus
three bytes overhead)

Relay and Mining: Priority transactions
- ---------------------------------------

Bitcoin Core has a heuristic 'priority' based on coin value and age. This
calculation is used for relaying of transactions which do not pay the
minimum relay fee, and can be used as an alternative way of sorting
transactions for mined blocks. Bitcoin Core will relay transactions with
insufficient fees depending on the setting of `-limitfreerelay=<r>` (default:
`r=15` kB per minute) and `-blockprioritysize=<s>`.

In Bitcoin Core 0.12, when mempool limit has been reached a higher minimum
relay fee takes effect to limit memory usage. Transactions which do not meet
this higher effective minimum relay fee will not be relayed or mined even if
they rank highly according to the priority heuristic.

The mining of transactions based on their priority is also now disabled by
default. To re-enable it, simply set `-blockprioritysize=<n>` where is the size
in bytes of your blocks to reserve for these transactions. The old default was
50k, so to retain approximately the same policy, you would set
`-blockprioritysize=50000`.

Additionally, as a result of computational simplifications, the priority value
used for transactions received with unconfirmed inputs is lower than in prior
versions due to avoiding recomputing the amounts as input transactions confirm.

External miner policy set via the `prioritisetransaction` RPC to rank
transactions already in the mempool continues to work as it has previously.
Note, however, that if mining priority transactions is left disabled, the
priority delta will be ignored and only the fee metric will be effective.

This internal automatic prioritization handling is being considered for removal
entirely in Bitcoin Core 0.13, and it is at this time undecided whether the
more accurate priority calculation for chained unconfirmed transactions will be
restored. Community direction on this topic is particularly requested to help
set project priorities.

Automatically use Tor hidden services
- -------------------------------------

Starting with Tor version 0.2.7.1 it is possible, through Tor's control socket
API, to create and destroy 'ephemeral' hidden services programmatically.
Bitcoin Core has been updated to make use of this.

This means that if Tor is running (and proper authorization is available),
Bitcoin Core automatically creates a hidden service to listen on, without
manual configuration. Bitcoin Core will also use Tor automatically to connect
to other .onion nodes if the control socket can be successfully opened. This
will positively affect the number of available .onion nodes and their usage.

This new feature is enabled by default if Bitcoin Core is listening, and
a connection to Tor can be made. It can be configured with the `-listenonion`,
`-torcontrol` and `-torpassword` settings. To show verbose debugging
information, pass `-debug=tor`.

Notifications through ZMQ
- -------------------------

Bitcoind can now (optionally) asynchronously notify clients through a
ZMQ-based PUB socket of the arrival of new transactions and blocks.
This feature requires installation of the ZMQ C API library 4.x and
configuring its use through the command line or configuration file.
Please see [docs/zmq.md](/doc/zmq.md) for details of operation.

Wallet: Transaction fees
- ------------------------

Various improvements have been made to how the wallet calculates
transaction fees.

Users can decide to pay a predefined fee rate by setting `-paytxfee=<n>`
(or `settxfee <n>` rpc during runtime). A value of `n=0` signals Bitcoin
Core to use floating fees. By default, Bitcoin Core will use floating
fees.

Based on past transaction data, floating fees approximate the fees
required to get into the `m`th block from now. This is configurable
with `-txconfirmtarget=<m>` (default: `2`).

Sometimes, it is not possible to give good estimates, or an estimate
at all. Therefore, a fallback value can be set with `-fallbackfee=<f>`
(default: `0.0002` BTC/kB).

At all times, Bitcoin Core will cap fees at `-maxtxfee=<x>` (default:
0.10) BTC.
Furthermore, Bitcoin Core will never create transactions smaller than
the current minimum relay fee.
Finally, a user can set the minimum fee rate for all transactions with
`-mintxfee=<i>`, which defaults to 1000 satoshis per kB.

Wallet: Negative confirmations and conflict detection
- -----------------------------------------------------

The wallet will now report a negative number for confirmations that indicates
how deep in the block chain the conflict is found. For example, if a transaction
A has 5 confirmations and spends the same input as a wallet transaction B, B
will be reported as having -5 confirmations. If another wallet transaction C
spends an output from B, it will also be reported as having -5 confirmations.
To detect conflicts with historical transactions in the chain a one-time
`-rescan` may be needed.

Unlike earlier versions, unconfirmed but non-conflicting transactions will never
get a negative confirmation count. They are not treated as spendable unless
they're coming from ourself (change) and accepted into our local mempool,
however. The new "trusted" field in the `listtransactions` RPC output
indicates whether outputs of an unconfirmed transaction are considered
spendable.

Wallet: Merkle branches removed
- -------------------------------

Previously, every wallet transaction stored a Merkle branch to prove its
presence in blocks. This wasn't being used for more than an expensive
sanity check. Since 0.12, these are no longer stored. When loading a
0.12 wallet into an older version, it will automatically rescan to avoid
failed checks.

Wallet: Pruning
- ---------------

With 0.12 it is possible to use wallet functionality in pruned mode.
This can reduce the disk usage from currently around 60 GB to
around 2 GB.

However, rescans as well as the RPCs `importwallet`, `importaddress`,
`importprivkey` are disabled.

To enable block pruning set `prune=<N>` on the command line or in
`bitcoin.conf`, where `N` is the number of MiB to allot for
raw block & undo data.

A value of 0 disables pruning. The minimal value above 0 is 550. Your
wallet is as secure with high values as it is with low ones. Higher
values merely ensure that your node will not shut down upon blockchain
reorganizations of more than 2 days - which are unlikely to happen in
practice. In future releases, a higher value may also help the network
as a whole: stored blocks could be served to other nodes.

For further information about pruning, you may also consult the [release
notes of v0.11.0](https://github.com/bitcoin/bitcoin/blob/v0.11.0/doc/release-notes.md#block-file-pruning).

`NODE_BLOOM` service bit
- ------------------------

Support for the `NODE_BLOOM` service bit, as described in [BIP
111](https://github.com/bitcoin/bips/blob/master/bip-0111.mediawiki), has been
added to the P2P protocol code.

BIP 111 defines a service bit to allow peers to advertise that they support
bloom filters (such as used by SPV clients) explicitly. It also bumps the protocol
version to allow peers to identify old nodes which allow bloom filtering of the
connection despite lacking the new service bit.

In this version, it is only enforced for peers that send protocol versions
`>=70011`. For the next major version it is planned that this restriction will be
removed. It is recommended to update SPV clients to check for the `NODE_BLOOM`
service bit for nodes that report versions newer than 70011.

Option parsing behavior
- -----------------------

Command line options are now parsed strictly in the order in which they are
specified. It used to be the case that `-X -noX` ends up, unintuitively, with X
set, as `-X` had precedence over `-noX`. This is no longer the case. Like for
other software, the last specified value for an option will hold.

RPC: Low-level API changes
- --------------------------

- - Monetary amounts can be provided as strings. This means that for example the
  argument to sendtoaddress can be "0.0001" instead of 0.0001. This can be an
  advantage if a JSON library insists on using a lossy floating point type for
  numbers, which would be dangerous for monetary amounts.

* The `asm` property of each scriptSig now contains the decoded signature hash
  type for each signature that provides a valid defined hash type.

* OP_NOP2 has been renamed to OP_CHECKLOCKTIMEVERIFY by [BIP 65](https://github.com/bitcoin/bips/blob/master/bip-0065.mediawiki)

The following items contain assembly representations of scriptSig signatures
and are affected by this change:

- - RPC `getrawtransaction`
- - RPC `decoderawtransaction`
- - RPC `decodescript`
- - REST `/rest/tx/` (JSON format)
- - REST `/rest/block/` (JSON format when including extended tx details)
- - `bitcoin-tx -json`

For example, the `scriptSig.asm` property of a transaction input that
previously showed an assembly representation of:

    304502207fa7a6d1e0ee81132a269ad84e68d695483745cde8b541e3bf630749894e342a022100c1f7ab20e13e22fb95281a870f3dcf38d782e53023ee313d741ad0cfbc0c509001 400000 OP_NOP2

now shows as:

    304502207fa7a6d1e0ee81132a269ad84e68d695483745cde8b541e3bf630749894e342a022100c1f7ab20e13e22fb95281a870f3dcf38d782e53023ee313d741ad0cfbc0c5090[ALL] 400000 OP_CHECKLOCKTIMEVERIFY

Note that the output of the RPC `decodescript` did not change because it is
configured specifically to process scriptPubKey and not scriptSig scripts.

RPC: SSL support dropped
- ------------------------

SSL support for RPC, previously enabled by the option `rpcssl` has been dropped
from both the client and the server. This was done in preparation for removing
the dependency on OpenSSL for the daemon completely.

Trying to use `rpcssl` will result in an error:

    Error: SSL mode for RPC (-rpcssl) is no longer supported.

If you are one of the few people that relies on this feature, a flexible
migration path is to use `stunnel`. This is an utility that can tunnel
arbitrary TCP connections inside SSL. On e.g. Ubuntu it can be installed with:

    sudo apt-get install stunnel4

Then, to tunnel a SSL connection on 28332 to a RPC server bound on localhost on port 18332 do:

    stunnel -d 28332 -r 127.0.0.1:18332 -p stunnel.pem -P ''

It can also be set up system-wide in inetd style.

Another way to re-attain SSL would be to setup a httpd reverse proxy. This solution
would allow the use of different authentication, loadbalancing, on-the-fly compression and
caching. A sample config for apache2 could look like:

    Listen 443

    NameVirtualHost *:443
    <VirtualHost *:443>

    SSLEngine On
    SSLCertificateFile /etc/apache2/ssl/server.crt
    SSLCertificateKeyFile /etc/apache2/ssl/server.key

    <Location /bitcoinrpc>
        ProxyPass http://127.0.0.1:8332/
        ProxyPassReverse http://127.0.0.1:8332/
        # optional enable digest auth
        # AuthType Digest
        # ...

        # optional bypass bitcoind rpc basic auth
        # RequestHeader set Authorization "Basic <hash>"
        # get the <hash> from the shell with: base64 <<< bitcoinrpc:<password>
    </Location>

    # Or, balance the load:
    # ProxyPass / balancer://balancer_cluster_name

    </VirtualHost>

Mining Code Changes
- -------------------

The mining code in 0.12 has been optimized to be significantly faster and use less
memory. As part of these changes, consensus critical calculations are cached on a
transaction's acceptance into the mempool and the mining code now relies on the
consistency of the mempool to assemble blocks. However all blocks are still tested
for validity after assembly.

Other P2P Changes
- -----------------

The list of banned peers is now stored on disk rather than in memory.
Restarting bitcoind will no longer clear out the list of banned peers; instead
a new RPC call (`clearbanned`) can be used to manually clear the list.  The new
`setban` RPC call can also be used to manually ban or unban a peer.

0.12.0 Change log
=================

Detailed release notes follow. This overview includes changes that affect
behavior, not code moves, refactors and string updates. For convenience in locating
the code changes and accompanying discussion, both the pull request and
git merge commit are mentioned.

### RPC and REST

- - #6121 `466f0ea` Convert entire source tree from json_spirit to UniValue (Jonas Schnelli)
- - #6234 `d38cd47` fix rpcmining/getblocktemplate univalue transition logic error (Jonas Schnelli)
- - #6239 `643114f` Don't go through double in AmountFromValue and ValueFromAmount (Wladimir J. van der Laan)
- - #6266 `ebab5d3` Fix univalue handling of \u0000 characters. (Daniel Kraft)
- - #6276 `f3d4dbb` Fix getbalance * 0 (Tom Harding)
- - #6257 `5ebe7db` Add `paytxfee` and `errors` JSON fields where appropriate (Stephen)
- - #6271 `754aae5` New RPC command disconnectnode (Alex van der Peet)
- - #6158 `0abfa8a` Add setban/listbanned RPC commands (Jonas Schnelli)
- - #6307 `7ecdcd9` rpcban fixes (Jonas Schnelli)
- - #6290 `5753988` rpc: make `gettxoutsettinfo` run lock-free (Wladimir J. van der Laan)
- - #6262 `247b914` Return all available information via RPC call "validateaddress" (dexX7)
- - #6339 `c3f0490` UniValue: don't escape solidus, keep espacing of reverse solidus (Jonas Schnelli)
- - #6353 `6bcb0a2` Show softfork status in getblockchaininfo (Wladimir J. van der Laan)
- - #6247 `726e286` Add getblockheader RPC call (Peter Todd)
- - #6362 `d6db115` Fix null id in RPC response during startup (Forrest Voight)
- - #5486 `943b322` [REST] JSON support for /rest/headers (Jonas Schnelli)
- - #6379 `c52e8b3` rpc: Accept scientific notation for monetary amounts in JSON (Wladimir J. van der Laan)
- - #6388 `fd5dfda` rpc: Implement random-cookie based authentication (Wladimir J. van der Laan)
- - #6457 `3c923e8` Include pruned state in chaininfo.json (Simon Males)
- - #6456 `bfd807f` rpc: Avoid unnecessary parsing roundtrip in number formatting, fix locale issue (Wladimir J. van der Laan)
- - #6380 `240b30e` rpc: Accept strings in AmountFromValue (Wladimir J. van der Laan)
- - #6346 `6bb2805` Add OP_RETURN support in createrawtransaction RPC call, add tests. (paveljanik)
- - #6013 `6feeec1` [REST] Add memory pool API (paveljanik)
- - #6576 `da9beb2` Stop parsing JSON after first finished construct. (Daniel Kraft)
- - #5677 `9aa9099` libevent-based http server (Wladimir J. van der Laan)
- - #6633 `bbc2b39` Report minimum ping time in getpeerinfo (Matt Corallo)
- - #6648 `cd381d7` Simplify logic of REST request suffix parsing. (Daniel Kraft)
- - #6695 `5e21388` libevent http fixes (Wladimir J. van der Laan)
- - #5264 `48efbdb` show scriptSig signature hash types in transaction decodes. fixes #3166 (mruddy)
- - #6719 `1a9f19a` Make HTTP server shutdown more graceful (Wladimir J. van der Laan)
- - #6859 `0fbfc51` http: Restrict maximum size of http + headers (Wladimir J. van der Laan)
- - #5936 `bf7c195` [RPC] Add optional locktime to createrawtransaction (Tom Harding)
- - #6877 `26f5b34` rpc: Add maxmempool and effective min fee to getmempoolinfo (Wladimir J. van der Laan)
- - #6970 `92701b3` Fix crash in validateaddress with -disablewallet (Wladimir J. van der Laan)
- - #5574 `755b4ba` Expose GUI labels in RPC as comments (Luke-Jr)
- - #6990 `dbd2c13` http: speed up shutdown (Wladimir J. van der Laan)
- - #7013 `36baa9f` Remove LOCK(cs_main) from decodescript (Peter Todd)
- - #6999 `972bf9c` add (max)uploadtarget infos to getnettotals RPC help (Jonas Schnelli)
- - #7011 `31de241` Add mediantime to getblockchaininfo (Peter Todd)
- - #7065 `f91e29f` http: add Boost 1.49 compatibility (Wladimir J. van der Laan)
- - #7087 `be281d8` [Net]Add -enforcenodebloom option (Patrick Strateman)
- - #7044 `438ee59` RPC: Added additional config option for multiple RPC users. (Gregory Sanders)
- - #7072 `c143c49` [RPC] Add transaction size to JSON output (Nikita Zhavoronkov)
- - #7022 `9afbd96` Change default block priority size to 0 (Alex Morcos)
- - #7141 `c0c08c7` rpc: Don't translate warning messages (Wladimir J. van der Laan)
- - #7312 `fd4bd50` Add RPC call abandontransaction (Alex Morcos)
- - #7222 `e25b158` RPC: indicate which transactions are replaceable (Suhas Daftuar)
- - #7472 `b2f2b85` rpc: Add WWW-Authenticate header to 401 response (Wladimir J. van der Laan)
- - #7469 `9cb31e6` net.h fix spelling: misbeha{b,v}ing (Matt)

### Configuration and command-line options

- - #6164 `8d05ec7` Allow user to use -debug=1 to enable all debugging (lpescher)
- - #5288 `4452205` Added -whiteconnections=<n> option (Josh Lehan)
- - #6284 `10ac38e` Fix argument parsing oddity with -noX (Wladimir J. van der Laan)
- - #6489 `c9c017a` Give a better error message if system clock is bad (Casey Rodarmor)
- - #6462 `c384800` implement uacomment config parameter which can add comments to user agent as per BIP-0014 (Pavol Rusnak)
- - #6647 `a3babc8` Sanitize uacomment (MarcoFalke)
- - #6742 `3b2d37c` Changed logging to make -logtimestamps to work also for -printtoconsole (arnuschky)
- - #6846 `2cd020d` alias -h for -help (Daniel Cousens)
- - #6622 `7939164` Introduce -maxuploadtarget (Jonas Schnelli)
- - #6881 `2b62551` Debug: Add option for microsecond precision in debug.log (Suhas Daftuar)
- - #6776 `e06c14f` Support -checkmempool=N, which runs checks once every N transactions (Pieter Wuille)
- - #6896 `d482c0a` Make -checkmempool=1 not fail through int32 overflow (Pieter Wuille)
- - #6993 `b632145` Add -blocksonly option (Patrick Strateman)
- - #7323 `a344880` 0.12: Backport -bytespersigop option (Luke-Jr)
- - #7386 `da83ecd` Add option `-permitrbf` to set transaction replacement policy (Wladimir J. van der Laan)
- - #7290 `b16b5bc` Add missing options help (MarcoFalke)
- - #7440 `c76bfff` Rename permitrbf to mempoolreplacement and provide minimal string-list forward compatibility (Luke-Jr)

### Block and transaction handling

- - #6203 `f00b623` Remove P2SH coinbase flag, no longer interesting (Luke-Jr)
- - #6222 `9c93ee5` Explicitly set tx.nVersion for the genesis block and mining tests (Mark Friedenbach)
- - #5985 `3a1d3e8` Fix removing of orphan transactions (Alex Morcos)
- - #6221 `dd8fe82` Prune: Support noncontiguous block files (Adam Weiss)
- - #6124 `41076aa` Mempool only CHECKLOCKTIMEVERIFY (BIP65) verification, unparameterized version (Peter Todd)
- - #6329 `d0a10c1` acceptnonstdtxn option to skip (most) "non-standard transaction" checks, for testnet/regtest only (Luke-Jr)
- - #6410 `7cdefb9` Implement accurate memory accounting for mempool (Pieter Wuille)
- - #6444 `24ce77d` Exempt unspendable transaction outputs from dust checks (dexX7)
- - #5913 `a0625b8` Add absurdly high fee message to validation state (Shaul Kfir)
- - #6177 `2f746c6` Prevent block.nTime from decreasing (Mark Friedenbach)
- - #6377 `e545371` Handle no chain tip available in InvalidChainFound() (Ross Nicoll)
- - #6551 `39ddaeb` Handle leveldb::DestroyDB() errors on wipe failure (Adam Weiss)
- - #6654 `b0ce450` Mempool package tracking (Suhas Daftuar)
- - #6715 `82d2aef` Fix mempool packages (Suhas Daftuar)
- - #6680 `4f44530` use CBlockIndex instead of uint256 for UpdatedBlockTip signal (Jonas Schnelli)
- - #6650 `4fac576` Obfuscate chainstate (James O'Beirne)
- - #6777 `9caaf6e` Unobfuscate chainstate data in CCoinsViewDB::GetStats (James O'Beirne)
- - #6722 `3b20e23` Limit mempool by throwing away the cheapest txn and setting min relay fee to it (Matt Corallo)
- - #6889 `38369dd` fix locking issue with new mempool limiting (Jonas Schnelli)
- - #6464 `8f3b3cd` Always clean up manual transaction prioritization (Casey Rodarmor)
- - #6865 `d0badb9` Fix chainstate serialized_size computation (Pieter Wuille)
- - #6566 `ff057f4` BIP-113: Mempool-only median time-past as endpoint for lock-time calculations (Mark Friedenbach)
- - #6934 `3038eb6` Restores mempool only BIP113 enforcement (Gregory Maxwell)
- - #6965 `de7d459` Benchmark sanity checks and fork checks in ConnectBlock (Matt Corallo)
- - #6918 `eb6172a` Make sigcache faster, more efficient, larger (Pieter Wuille)
- - #6771 `38ed190` Policy: Lower default limits for tx chains (Alex Morcos)
- - #6932 `73fa5e6` ModifyNewCoins saves database lookups (Alex Morcos)
- - #5967 `05d5918` Alter assumptions in CCoinsViewCache::BatchWrite (Alex Morcos)
- - #6871 `0e93586` nSequence-based Full-RBF opt-in (Peter Todd)
- - #7008 `eb77416` Lower bound priority (Alex Morcos)
- - #6915 `2ef5ffa` [Mempool] Improve removal of invalid transactions after reorgs (Suhas Daftuar)
- - #6898 `4077ad2` Rewrite CreateNewBlock (Alex Morcos)
- - #6872 `bdda4d5` Remove UTXO cache entries when the tx they were added for is removed/does not enter mempool (Matt Corallo)
- - #7062 `12c469b` [Mempool] Fix mempool limiting and replace-by-fee for PrioritiseTransaction (Suhas Daftuar)
- - #7276 `76de36f` Report non-mandatory script failures correctly (Pieter Wuille)
- - #7217 `e08b7cb` Mark blocks with too many sigops as failed (Suhas Daftuar)
- - #7387 `f4b2ce8` Get rid of inaccurate ScriptSigArgsExpected (Pieter Wuille)

### P2P protocol and network code

- - #6172 `88a7ead` Ignore getheaders requests when not synced (Suhas Daftuar)
- - #5875 `9d60602` Be stricter in processing unrequested blocks (Suhas Daftuar)
- - #6256 `8ccc07c` Use best header chain timestamps to detect partitioning (Gavin Andresen)
- - #6283 `a903ad7` make CAddrMan::size() return the correct type of size_t (Diapolo)
- - #6272 `40400d5` Improve proxy initialization (continues #4871) (Wladimir J. van der Laan, Diapolo)
- - #6310 `66e5465` banlist.dat: store banlist on disk (Jonas Schnelli)
- - #6412 `1a2de32` Test whether created sockets are select()able (Pieter Wuille)
- - #6498 `219b916` Keep track of recently rejected transactions with a rolling bloom filter (cont'd) (Peter Todd)
- - #6556 `70ec975` Fix masking of irrelevant bits in address groups. (Alex Morcos)
- - #6530 `ea19c2b` Improve addrman Select() performance when buckets are nearly empty (Pieter Wuille)
- - #6583 `af9305a` add support for miniupnpc api version 14 (Pavel Vasin)
- - #6374 `69dc5b5` Connection slot exhaustion DoS mitigation (Patrick Strateman)
- - #6636 `536207f` net: correctly initialize nMinPingUsecTime (Wladimir J. van der Laan)
- - #6579 `0c27795` Add NODE_BLOOM service bit and bump protocol version (Matt Corallo)
- - #6148 `999c8be` Relay blocks when pruning (Suhas Daftuar)
- - #6588 `cf9bb11` In (strCommand == "tx"), return if AlreadyHave() (Tom Harding)
- - #6974 `2f71b07` Always allow getheaders from whitelisted peers (Wladimir J. van der Laan)
- - #6639 `bd629d7` net: Automatically create hidden service, listen on Tor (Wladimir J. van der Laan)
- - #6984 `9ffc687` don't enforce maxuploadtarget's disconnect for whitelisted peers (Jonas Schnelli)
- - #7046 `c322652` Net: Improve blocks only mode. (Patrick Strateman)
- - #7090 `d6454f6` Connect to Tor hidden services by default (when listening on Tor) (Peter Todd)
- - #7106 `c894fbb` Fix and improve relay from whitelisted peers (Pieter Wuille)
- - #7129 `5d5ef3a` Direct headers announcement (rebase of #6494) (Pieter Wuille)
- - #7079 `1b5118b` Prevent peer flooding inv request queue (redux) (redux) (Gregory Maxwell)
- - #7166 `6ba25d2` Disconnect on mempool requests from peers when over the upload limit. (Gregory Maxwell)
- - #7133 `f31955d` Replace setInventoryKnown with a rolling bloom filter (rebase of #7100) (Pieter Wuille)
- - #7174 `82aff88` Don't do mempool lookups for "mempool" command without a filter (Matt Corallo)
- - #7179 `44fef99` net: Fix sent reject messages for blocks and transactions (Wladimir J. van der Laan)
- - #7181 `8fc174a` net: Add and document network messages in protocol.h (Wladimir J. van der Laan)
- - #7125 `10b88be` Replace global trickle node with random delays (Pieter Wuille)
- - #7415 `cb83beb` net: Hardcoded seeds update January 2016 (Wladimir J. van der Laan)
- - #7438 `e2d9a58` Do not absolutely protect local peers; decide group ties based on time (Gregory Maxwell)
- - #7439 `86755bc` Add whitelistforcerelay to control forced relaying. [#7099 redux] (Gregory Maxwell)
- - #7482 `e16f5b4` Ensure headers count is correct (Suhas Daftuar)

### Validation

- - #5927 `8d9f0a6` Reduce checkpoints' effect on consensus. (Pieter Wuille)
- - #6299 `24f2489` Bugfix: Don't check the genesis block header before accepting it (Jorge Timón)
- - #6361 `d7ada03` Use real number of cores for default -par, ignore virtual cores (Wladimir J. van der Laan)
- - #6519 `87f37e2` Make logging for validation optional (Wladimir J. van der Laan)
- - #6351 `2a1090d` CHECKLOCKTIMEVERIFY (BIP65) IsSuperMajority() soft-fork (Peter Todd)
- - #6931 `54e8bfe` Skip BIP 30 verification where not necessary (Alex Morcos)
- - #6954 `e54ebbf` Switch to libsecp256k1-based ECDSA validation (Pieter Wuille)
- - #6508 `61457c2` Switch to a constant-space Merkle root/branch algorithm. (Pieter Wuille)
- - #6914 `327291a` Add pre-allocated vector type and use it for CScript (Pieter Wuille)
- - #7500 `889e5b3` Correctly report high-S violations (Pieter Wuille)


### Build system

- - #6210 `0e4f2a0` build: disable optional use of gmp in internal secp256k1 build (Wladimir J. van der Laan)
- - #6214 `87406aa` [OSX] revert renaming of Bitcoin-Qt.app and use CFBundleDisplayName (partial revert of #6116) (Jonas Schnelli)
- - #6218 `9d67b10` build/gitian misc updates (Cory Fields)
- - #6269 `d4565b6` gitian: Use the new bitcoin-detached-sigs git repo for OSX signatures (Cory Fields)
- - #6418 `d4a910c` Add autogen.sh to source tarball. (randy-waterhouse)
- - #6373 `1ae3196` depends: non-qt bumps for 0.12 (Cory Fields)
- - #6434 `059b352` Preserve user-passed CXXFLAGS with --enable-debug (Gavin Andresen)
- - #6501 `fee6554` Misc build fixes (Cory Fields)
- - #6600 `ef4945f` Include bitcoin-tx binary on Debian/Ubuntu (Zak Wilcox)
- - #6619 `4862708` depends: bump miniupnpc and ccache (Michael Ford)
- - #6801 `ae69a75` [depends] Latest config.guess and config.sub (Michael Ford)
- - #6938 `193f7b5` build: If both Qt4 and Qt5 are installed, use Qt5 (Wladimir J. van der Laan)
- - #7092 `348b281` build: Set osx permissions in the dmg to make Gatekeeper happy (Cory Fields)
- - #6980 `eccd671` [Depends] Bump Boost, miniupnpc, ccache & zeromq (Michael Ford)
- - #7424 `aa26ee0` Add security/export checks to gitian and fix current failures (Cory Fields)

### Wallet

- - #6183 `87550ee` Fix off-by-one error w/ nLockTime in the wallet (Peter Todd)
- - #6057 `ac5476e` re-enable wallet in autoprune (Jonas Schnelli)
- - #6356 `9e6c33b` Delay initial pruning until after wallet init (Adam Weiss)
- - #6088 `91389e5` fundrawtransaction (Matt Corallo)
- - #6415 `ddd8d80` Implement watchonly support in fundrawtransaction (Matt Corallo)
- - #6567 `0f0f323` Fix crash when mining with empty keypool. (Daniel Kraft)
- - #6688 `4939eab` Fix locking in GetTransaction. (Alex Morcos)
- - #6645 `4dbd43e` Enable wallet key imports without rescan in pruned mode. (Gregory Maxwell)
- - #6550 `5b77244` Do not store Merkle branches in the wallet. (Pieter Wuille)
- - #5924 `12a7712` Clean up change computation in CreateTransaction. (Daniel Kraft)
- - #6906 `48b5b84` Reject invalid pubkeys when reading ckey items from the wallet. (Gregory Maxwell)
- - #7010 `e0a5ef8` Fix fundrawtransaction handling of includeWatching (Peter Todd)
- - #6851 `616d61b` Optimisation: Store transaction list order in memory rather than compute it every need (Luke-Jr)
- - #6134 `e92377f` Improve usage of fee estimation code (Alex Morcos)
- - #7103 `a775182` [wallet, rpc tests] Fix settxfee, paytxfee (MarcoFalke)
- - #7105 `30c2d8c` Keep track of explicit wallet conflicts instead of using mempool (Pieter Wuille)
- - #7096 `9490bd7` [Wallet] Improve minimum absolute fee GUI options (Jonas Schnelli)
- - #6216 `83f06ca` Take the training wheels off anti-fee-sniping (Peter Todd)
- - #4906 `96e8d12` Issue#1643: Coinselection prunes extraneous inputs from ApproximateBestSubset (Murch)
- - #7200 `06c6a58` Checks for null data transaction before issuing error to debug.log (Andy Craze)
- - #7296 `a36d79b` Add sane fallback for fee estimation (Alex Morcos)
- - #7293 `ff9b610` Add regression test for vValue sort order (MarcoFalke)
- - #7306 `4707797` Make sure conflicted wallet tx's update balances (Alex Morcos)
- - #7381 `621bbd8` [walletdb] Fix syntax error in key parser (MarcoFalke)
- - #7491 `00ec73e` wallet: Ignore MarkConflict if block hash is not known (Wladimir J. van der Laan)
- - #7502 `1329963` Update the wallet best block marker before pruning (Pieter Wuille)

### GUI

- - #6217 `c57e12a` disconnect peers from peers tab via context menu (Diapolo)
- - #6209 `ab0ec67` extend rpc console peers tab (Diapolo)
- - #6484 `1369d69` use CHashWriter also in SignVerifyMessageDialog (Pavel Vasin)
- - #6487 `9848d42` Introduce PlatformStyle (Wladimir J. van der Laan)
- - #6505 `100c9d3` cleanup icons (MarcoFalke)
- - #4587 `0c465f5` allow users to set -onion via GUI (Diapolo)
- - #6529 `c0f66ce` show client user agent in debug window (Diapolo)
- - #6594 `878ea69` Disallow duplicate windows. (Casey Rodarmor)
- - #5665 `6f55cdd` add verifySize() function to PaymentServer (Diapolo)
- - #6317 `ca5e2a1` minor optimisations in peertablemodel (Diapolo)
- - #6315 `e59d2a8` allow banning and unbanning over UI->peers table (Jonas Schnelli)
- - #6653 `e04b2fa` Pop debug window in foreground when opened twice (MarcoFalke)
- - #6864 `c702521` Use monospace font (MarcoFalke)
- - #6887 `3694b74` Update coin control and smartfee labels (MarcoFalke)
- - #7000 `814697c` add shortcurts for debug-/console-window (Jonas Schnelli)
- - #6951 `03403d8` Use maxTxFee instead of 10000000 (MarcoFalke)
- - #7051 `a190777` ui: Add "Copy raw transaction data" to transaction list context menu (Wladimir J. van der Laan)
- - #6979 `776848a` simple mempool info in debug window (Jonas Schnelli)
- - #7006 `26af1ac` add startup option to reset Qt settings (Jonas Schnelli)
- - #6780 `2a94cd6` Call init's parameter interaction before we create the UI options model (Jonas Schnelli)
- - #7112 `96b8025` reduce cs_main locks during tip update, more fluently update UI (Jonas Schnelli)
- - #7206 `f43c2f9` Add "NODE_BLOOM" to guiutil so that peers don't get UNKNOWN[4] (Matt Corallo)
- - #7282 `5cadf3e` fix coincontrol update issue when deleting a send coins entry (Jonas Schnelli)
- - #7319 `1320300` Intro: Display required space (Jonas Schnelli)
- - #7318 `9265e89` quickfix for RPC timer interface problem (Jonas Schnelli)
- - #7327 `b16b5bc` [Wallet] Transaction View: LastMonth calculation fixed (crowning-)
- - #7364 `7726c48` [qt] Windows: Make rpcconsole monospace font larger (MarcoFalke)
- - #7384 `294f432` [qt] Peertable: Increase SUBVERSION_COLUMN_WIDTH (MarcoFalke)

### Tests and QA

- - #6305 `9005c91` build: comparison tool swap (Cory Fields)
- - #6318 `e307e13` build: comparison tool NPE fix (Cory Fields)
- - #6337 `0564c5b` Testing infrastructure: mocktime fixes (Gavin Andresen)
- - #6350 `60abba1` add unit tests for the decodescript rpc (mruddy)
- - #5881 `3203a08` Fix and improve txn_doublespend.py test (Tom Harding)
- - #6390 `6a73d66` tests: Fix bitcoin-tx signing test case (Wladimir J. van der Laan)
- - #6368 `7fc25c2` CLTV: Add more tests to improve coverage (Esteban Ordano)
- - #6414 `5121c68` Fix intermittent test failure, reduce test time (Tom Harding)
- - #6417 `44fa82d` [QA] fix possible reorg issue in (fund)rawtransaction(s).py RPC test (Jonas Schnelli)
- - #6398 `3d9362d` rpc: Remove chain-specific RequireRPCPassword (Wladimir J. van der Laan)
- - #6428 `bb59e78` tests: Remove old sh-based test framework (Wladimir J. van der Laan)
- - #5515 `d946e9a` RFC: Assert on probable deadlocks if the second lock isnt try_lock (Matt Corallo)
- - #6287 `d2464df` Clang lock debug (Cory Fields)
- - #6465 `410fd74` Don't share objects between TestInstances (Casey Rodarmor)
- - #6534 `6c1c7fd` Fix test locking issues and un-revert the probable-deadlines assertions commit (Cory Fields)
- - #6509 `bb4faee` Fix race condition on test node shutdown (Casey Rodarmor)
- - #6523 `561f8af` Add p2p-fullblocktest.py (Casey Rodarmor)
- - #6590 `981fd92` Fix stale socket rebinding and re-enable python tests for Windows (Cory Fields)
- - #6730 `cb4d6d0` build: Remove dependency of bitcoin-cli on secp256k1 (Wladimir J. van der Laan)
- - #6616 `5ab5dca` Regression Tests: Migrated rpc-tests.sh to all Python rpc-tests.py (Peter Tschipper)
- - #6720 `d479311` Creates unittests for addrman, makes addrman more testable. (Ethan Heilman)
- - #6853 `c834f56` Added fPowNoRetargeting field to Consensus::Params (Eric Lombrozo)
- - #6827 `87e5539` [rpc-tests] Check return code (MarcoFalke)
- - #6848 `f2c869a` Add DERSIG transaction test cases (Ross Nicoll)
- - #6813 `5242bb3` Support gathering code coverage data for RPC tests with lcov (dexX7)
- - #6888 `c8322ff` Clear strMiscWarning before running PartitionAlert (Eric Lombrozo)
- - #6894 `2675276` [Tests] Fix BIP65 p2p test (Suhas Daftuar)
- - #6863 `725539e` [Test Suite] Fix test for null tx input (Daniel Kraft)
- - #6926 `a6d0d62` tests: Initialize networking on windows (Wladimir J. van der Laan)
- - #6822 `9fa54a1` [tests] Be more strict checking dust (MarcoFalke)
- - #6804 `5fcc14e` [tests] Add basic coverage reporting for RPC tests (James O'Beirne)
- - #7045 `72dccfc` Bugfix: Use unique autostart filenames on Linux for testnet/regtest (Luke-Jr)
- - #7095 `d8368a0` Replace scriptnum_test's normative ScriptNum implementation (Wladimir J. van der Laan)
- - #7063 `6abf6eb` [Tests] Add prioritisetransaction RPC test (Suhas Daftuar)
- - #7137 `16f4a6e` Tests: Explicitly set chain limits in replace-by-fee test (Suhas Daftuar)
- - #7216 `9572e49` Removed offline testnet DNSSeed 'alexykot.me'. (tnull)
- - #7209 `f3ad812` test: don't override BITCOIND and BITCOINCLI if they're set (Wladimir J. van der Laan)
- - #7226 `301f16a` Tests: Add more tests to p2p-fullblocktest (Suhas Daftuar)
- - #7153 `9ef7c54` [Tests] Add mempool_limit.py test (Jonas Schnelli)
- - #7170 `453c567` tests: Disable Tor interaction (Wladimir J. van der Laan)
- - #7229 `1ed938b` [qa] wallet: Check if maintenance changes the balance (MarcoFalke)
- - #7308 `d513405` [Tests] Eliminate intermittent failures in sendheaders.py (Suhas Daftuar)
- - #7468 `947c4ff` [rpc-tests] Change solve() to use rehash (Brad Andrews)

### Miscellaneous

- - #6213 `e54ff2f` [init] add -blockversion help and extend -upnp help (Diapolo)
- - #5975 `1fea667` Consensus: Decouple ContextualCheckBlockHeader from checkpoints (Jorge Timón)
- - #6061 `eba2f06` Separate Consensus::CheckTxInputs and GetSpendHeight in CheckInputs (Jorge Timón)
- - #5994 `786ed11` detach wallet from miner (Jonas Schnelli)
- - #6387 `11576a5` [bitcoin-cli] improve error output (Jonas Schnelli)
- - #6401 `6db53b4` Add BITCOIND_SIGTERM_TIMEOUT to OpenRC init scripts (Florian Schmaus)
- - #6430 `b01981e` doc: add documentation for shared library libbitcoinconsensus (Braydon Fuller)
- - #6372 `dcc495e` Update Linearize tool to support Windows paths; fix variable scope; update README and example configuration (Paul Georgiou)
- - #6453 `8fe5cce` Separate core memory usage computation in core_memusage.h (Pieter Wuille)
- - #6149 `633fe10` Buffer log messages and explicitly open logs (Adam Weiss)
- - #6488 `7cbed7f` Avoid leaking file descriptors in RegisterLoad (Casey Rodarmor)
- - #6497 `a2bf40d` Make sure LogPrintf strings are line-terminated (Wladimir J. van der Laan)
- - #6504 `b6fee6b` Rationalize currency unit to "BTC" (Ross Nicoll)
- - #6507 `9bb4dd8` Removed contrib/bitrpc (Casey Rodarmor)
- - #6527 `41d650f` Use unique name for AlertNotify tempfile (Casey Rodarmor)
- - #6561 `e08a7d9` limitedmap fixes and tests (Casey Rodarmor)
- - #6565 `a6f2aff` Make sure we re-acquire lock if a task throws (Casey Rodarmor)
- - #6599 `f4d88c4` Make sure LogPrint strings are line-terminated (Ross Nicoll)
- - #6630 `195942d` Replace boost::reverse_lock with our own (Casey Rodarmor)
- - #6103 `13b8282` Add ZeroMQ notifications (João Barbosa)
- - #6692 `d5d1d2e` devtools: don't push if signing fails in github-merge (Wladimir J. van der Laan)
- - #6728 `2b0567b` timedata: Prevent warning overkill (Wladimir J. van der Laan)
- - #6713 `f6ce59c` SanitizeString: Allow hypen char (MarcoFalke)
- - #5987 `4899a04` Bugfix: Fix testnet-in-a-box use case (Luke-Jr)
- - #6733 `b7d78fd` Simple benchmarking framework (Gavin Andresen)
- - #6854 `a092970` devtools: Add security-check.py (Wladimir J. van der Laan)
- - #6790 `fa1d252` devtools: add clang-format.py (MarcoFalke)
- - #7114 `f3d0fdd` util: Don't set strMiscWarning on every exception (Wladimir J. van der Laan)
- - #7078 `93e0514` uint256::GetCheapHash bigendian compatibility (arowser)
- - #7094 `34e02e0` Assert now > 0 in GetTime GetTimeMillis GetTimeMicros (Patrick Strateman)

Credits
=======

Thanks to everyone who directly contributed to this release:

- - accraze
- - Adam Weiss
- - Alex Morcos
- - Alex van der Peet
- - AlSzacrel
- - Altoidnerd
- - Andriy Voskoboinyk
- - antonio-fr
- - Arne Brutschy
- - Ashley Holman
- - Bob McElrath
- - Braydon Fuller
- - BtcDrak
- - Casey Rodarmor
- - centaur1
- - Chris Kleeschulte
- - Christian Decker
- - Cory Fields
- - daniel
- - Daniel Cousens
- - Daniel Kraft
- - David Hill
- - dexX7
- - Diego Viola
- - Elias Rohrer
- - Eric Lombrozo
- - Erik Mossberg
- - Esteban Ordano
- - EthanHeilman
- - Florian Schmaus
- - Forrest Voight
- - Gavin Andresen
- - Gregory Maxwell
- - Gregory Sanders / instagibbs
- - Ian T
- - Irving Ruan
- - Jacob Welsh
- - James O'Beirne
- - Jeff Garzik
- - Johnathan Corgan
- - Jonas Schnelli
- - Jonathan Cross
- - João Barbosa
- - Jorge Timón
- - Josh Lehan
- - J Ross Nicoll
- - kazcw
- - Kevin Cooper
- - lpescher
- - Luke Dashjr
- - Marco
- - MarcoFalke
- - Mark Friedenbach
- - Matt
- - Matt Bogosian
- - Matt Corallo
- - Matt Quinn
- - Micha
- - Michael
- - Michael Ford / fanquake
- - Midnight Magic
- - Mitchell Cash
- - mrbandrews
- - mruddy
- - Nick
- - Patrick Strateman
- - Paul Georgiou
- - Paul Rabahy
- - Pavel Janík / paveljanik
- - Pavel Vasin
- - Pavol Rusnak
- - Peter Josling
- - Peter Todd
- - Philip Kaufmann
- - Pieter Wuille
- - ptschip
- - randy-waterhouse
- - rion
- - Ross Nicoll
- - Ryan Havar
- - Shaul Kfir
- - Simon Males
- - Stephen
- - Suhas Daftuar
- - tailsjoin
- - Thomas Kerin
- - Tom Harding
- - tulip
- - unsystemizer
- - Veres Lajos
- - Wladimir J. van der Laan
- - xor-freenet
- - Zak Wilcox
- - zathras-crypto

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWzD2rAAoJEHSBCwEjRsmmg/wIAMdVQie2KQWASn+lDAxE/njW
zOeWunnyiWLOEJYSHhPzb+1kDfubsHkEr8tvkfhBKI25NMg0yLBzB1QSfBmbXGZK
XNWuaqkda9424iAcAuajtNHLYa9oolKI6ECYikYmsAFR2q0IlpV8c3BwGWJ7+/MV
yD79f1PfmFakgApe53/dz1USm/y9afcZAiEsfhs5wc8Q8IJxOFv+7F05hRa2g4IJ
ZJk+Zotb9kQh39fGv4YGyo91NOr5ZzOhEYQAezJ+mCFflkjTwynz8ocjuqYg3nzq
viKYTMi7zX56aDIw2OTX+gzWigIExObYxvre1oNCtXANTyzEMoLRYaSwjcrtTgE=
=JwtP
-----END PGP SIGNATURE-----


-------------------------------------
On 11/16/2016 06:47 PM, Pieter Wuille wrote:
> On Wed, Nov 16, 2016 at 6:16 PM, Eric Voskuil <eric@voskuil.org
> <mailto:eric@voskuil.org>> wrote:
> 
>     On 11/16/2016 05:50 PM, Pieter Wuille wrote:
> 
>     > So are checkpoints good now?
>     > I believe we should get rid of checkpoints because they seem to be
>     misunderstood as a security feature rather than as an optimization.
> 
>     Or maybe because they place control of the "true chain" in the hands of
>     those selecting the checkpoints? It's not a great leap for the parties
>     distributing the checkpoints to become the central authority.
> 
> Yes, they can be used to control the "true chain", and this has happened
> with various forks. But developers inevitably have this possibility, if
> you ignore review. If review is good enough to catch unintended
> consensus changes, it is certainly enough to catch the introduction of
> an invalid checkpoint. The risk you point out is real, but the way to
> deal with it is good review and release practices.
> 
> I wish we had never used checkpoints the way we did, but here we are.
> Because of this, I want to get rid of them. However, It's not because I
> think they offer an excessive power to developers - but because they're
> often perceived this way (partially as a result of how they've been
> abused in other systems).
>  
>     I recommend users of our node validate the full chain without
>     checkpoints and from that chain select their own checkpoints and place
>     them into config. From that point forward they can apply the
>     optimization. Checkpoints should never be hardcoded into the source.
> 
> Having users with the discipline you suggest would be wonderful to have.
> I don't think it's very realistic, though, and I fear that the result
> would be that everyone copies their config from one or a few websites
> "because that's what everyone uses".

Certainly, but embedding them in the code makes that a practical
certainty. People cannot be prevented from doing dumb things, but let's
not make it hard for them to be smart.

>     > I don't think buried softforks have that problem.
> 
>     I find "buried softfork" a curious name as you are using it. You seem to
>     be implying that this type of change is itself a softfork as opposed to
>     a hardfork that changes the activation of a softfork. It was my
>     understanding that the term referred to the 3 softforks that were being
>     "buried", or the proposal, but not the burial itself.


> I do not consider the practice of "buried softforks" to be a fork at
> all. It is a change that modifies the validity of a theoretically
> construable chain from invalid to valid.

I was out at a Bitcoin meetup when I read this and I think beer actually
came out of my nose.

> However, a reorganization to
> that theoretical chain itself is likely already impossible due to the
> vast number of blocks to rewind, and economic damage that is far greater
> than chain divergence itself.

It's either possible or it is not. If it is not there is no reason for a
proposal - just make the change and don't bother to tell anyone. The
reason we are having this discussion is because it is not impossible.

>     Nevertheless, this proposal shouldn't have "that problem" because it is
>     clearly neither a security feature nor an optimization. That is the
>     first issue that needs to be addressed.
> 
> It is clearly not a security feature, agreed. But how would you propose
> to avoid the ISM checks for BIP34 and BIP66 all the time?

I'll call straw man on the question. It is not important to avoid the
activation checks. The question is whether there is a material
performance optimization in eliminating them. This would have to be
significant enough to rise to the level of a change to the protocol.
Having said that there are a few options:

1. The naive approach to activation is, for each new block, to query the
store for the previous 1000 block headers (to the extent there are that
many), and just do so forever, summing up after the query. This is the
most straightforward but also the most costly approach.

2. A slightly less costly approach is, for each new block, to reverse
iterate over the store until all decisions can be made. This would be an
improvement below activation in that it would take it takes as little as
251 vs. 1000 queries to make the determinations.

3. A further improvement is available by caching the height of full
activation of all three soft forks. Unless there is a subsequent reorg
with a fork point prior that height, there is never a need to make
another query. Once fully activated the activation height is cached to
the store (otherwise just query the last 1000 versions at startup to
determine the state), eliminating any ongoing material cost.

4. We may also be interested in optimizing initial block download. A
cache of the last 1000 block versions can be maintained by adding each
to a circular buffer as they are committed. This eliminates *all*
querying for block versions unless:

(1) there is a restart prior to full activation - in which case there is
a query of up to 1000 versions to prime the cache.

(2) there is a potential reorg after full activation, and the fork point
precedes the saved full activation height - in which case the cache must
be reprimed.

(3) there is a potential reorg. before reaching full activation - in
which case the cache must be backfilled with a query for a number of
versions equal to the depth of the fork point.

During initial block download potential reorgs are exceedingly rare
(reorgs don't have potential unless they have sufficient work to
overcome the long chain) and the cost of handling them as described
above is trivial. The cost of priming the cache is immaterial in the
context of a restart.

So even with a full chain validation one is not likely to *ever* need to
query the store. The memory cost of the cache is strictly 3 bits per
block (375 bytes total). A simpler less memory-sensitive approach is to
use one byte (1,000 bytes total). The computational cost is trivial.

This should already be implemented. A protocol fork (or "change that
modifies the validity of a theoretically construable chain from invalid
to valid") to avoid doing so is not a performance optimization.

> I feel this
> approach is a perfectly reasonable choice for code that likely won't
> ever affect the valid chain again.

I find it to be completely unsupportable as there is no security,
performance, or feature benefit in it.

e





-------------------------------------
Repost by request from my blog, apologies for the somewhat screwy formatting!


---
layout: post
title:  "Progress On Hardfork Proposals Following The Segwit Blocksize Increase"
date:   2016-08-05
tags:
- bitcoin
- hardfork
- segwit
---

With segwit getting close to its initial testnet release in Bitcoin Core
v0.13.0 - expected to be followed soon by a mainnet release in Bitcoin Core
v0.13.1 - I thought it'd be a good idea to go over work being done on a
potential hard-fork to follow it, should the Bitcoin community decide to accept
the segwit proposal.

First of all, to recap, in addition to many other improvements such as fixing
transaction malleability, fixing the large transaction signature verification
DoS attack, providing a better way to upgrade the scripting system in the
future, etc. segwit increases the maximum blocksize to 4MB. However, because
it's a soft-fork - a backwards compatible change to the protocol - only witness
(signature) data can take advantage of this blocksize increase; non-witness
data is still limited to 1MB total per block. With current transaction patterns
it's expected that blocks post-segwit won't use all 4MB of serialized data
allowed by the post-segwit maximum blocksize limit.

Secondly, there's two potential upgrades to the Bitcoin protocol that will
further reduce the amount of witness data most transactions need: [Schnorr
signatures](https://bitcoinmagazine.com/articles/the-power-of-schnorr-the-signature-algorithm-to-increase-bitcoin-s-scale-and-privacy-1460642496) and [BLS aggregate signatures](http://diyhpl.us/wiki/transcripts/2016-july-bitcoin-developers-miners-meeting/dan-boneh/).
Basically, both these improvements allow multiple signatures to be combined,
the former on a per-transaction level, and the latter on a per-block level.

[Last February](https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff)
some of the mining community and some of the developer community got together to discuss potential
hard-forks, with the aim of coming up with a reasonable proposal to take to the
wider community for further discussion and consensus building. Let's look at
where that effort has lead.

## Ethereum: Lessons to be learned

But first, Ethereum. Or as some have quipped, the Etherea:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">The Battle for Etherea. <a href="https://t.co/2ATQRQRXnH">https://t.co/2ATQRQRXnH</a></p>&mdash; Samson Mow (@Excellion) <a href="https://twitter.com/Excellion/status/759677608753627136">July 31, 2016</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

If you've been following the crypto-currency space at all recently, you
probably know that the Ethereum community has split in two following a very
controversial hard-fork to the Ethereum protocol, To make a long story short, a
unintended feature in a smart-contract called "The DAO" was exploited by a
as-yet-unknown individual to drain around $50 million worth of the Ethereum
currency from the contract. While "white-hat attackers" did manage to recover a
majority of the funds in the DAO, a hard-fork was proposed to rewrite the
Ethereum ledger to recover all funds - an action that many, [including myself](/2016/ethereum-dao-bailout-vote),
have described as a bailout.

The result has been a big mess. This isn't the place to talk about all the
drama that's followed in depth, but I think it's fair to say that the Ethereum
community found out the hard way that just because you give a new protocol the
same name as an existing protocol, that doesn't force everyone to use it. As of
writing, what a month ago was called "Ethereum" - Ethereum Classic - has 20% of
the hashing power as the bailout chain, and peaked only two or three days ago
at around 30%. As for market cap, while the combined total for the two chains
is similar to the one chain pre-fork, this is likely misleading: there's
probably a lot of coins on both chains that aren't actually accessible and
don't represent liquid assets on the market. Instead, there's a good chance a
significant amount of value has been lost.

In particular, both chains have suffered significantly from transaction replay
issues. Basically, due to the way the Ethereum protocol is designed - in
particular the fact that Ethereum isn't based on a UTXO model - when the
Ethereum chain split transactions on one chain were very often valid on another
chain. Both attacks and accidents can lead to transactions from one chain
ending up broadcast to others, leading to unintentional spends. This wasn't an
unexpected problem:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">.<a href="https://twitter.com/petertoddbtc">@petertoddbtc</a> we knew it would happen weeks before launch, we didn&#39;t want to implement replay-protection b.c. of implementation complexity</p>&mdash; Vlad Zamfir (@VladZamfir) <a href="https://twitter.com/VladZamfir/status/759552287157133313">July 31, 2016</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

...and it's lead to costly losses. Among others Coinbase has lost [an unknown amount of
funds](https://twitter.com/eiaine/status/758560296017416194) that they may [have to buy back](https://twitter.com/brian_armstrong/status/760991445352386560). Even worse, BTC-e [lost pretty much their entire balance](https://www.reddit.com/r/EthereumClassic/comments/4v2d6j/btce_dear_clients_btces_official_standpoint_on/d5v2f3t)
of original Ethereum coins - apparently becoming insolvent - and instead of
returning customer funds, they decided to [declare the original Ethereum chain a scam](https://btc-e.com/news/230) instead.

A particularly scary thing about this kind of problem is that it can lead to
artificial demand for a chain that would otherwise die: for all we know
Coinbase has been scrambling behind the scenes to buy replacement ether to make
up for the ether that it lost due to replay issues.

More generally, the fact that the community split shows the difficulty - and
unpredictability - of achieving consensus, maintaining consensus, and
measuring consensus. For instance, while the Ethereum community did do a coin
vote [as I suggested](/2016/ethereum-dao-bailout-vote), turnout was extremely
low - around 5% - with a significant minority in opposition (and note that
exchanges' coins were blacklisted from the vote due to technical reasons).
Additionally, the miner vote also had low turnout, and again, significant
minority opposition.

With regard to [drama](https://twitter.com/petertoddbtc/status/761506592827125760) resulting
from a coin split, something I think not many in the technical community had
considered, is that exchanges can have perverse incentives to encourage it. The
split resulted in significant trading volume on the pre-fork, status quo,
Ethereum chain, which of course is very profitable for exchanges. The second
exchange to list the status-quo chain was Poloniex, who have over 100
Bitcoin-denominated markets for a very wide variety of niche currencies - their
business is normally niche currencies that don't necessarily have wide appeal.

Finally, keep in mind that while this has been bad for Ethereum, it'd be even
worse for Bitcoin: unlike Ethereum, Bitcoin actually has non-trivial usage in
commerce, by users who aren't necessarily keeping up to date with the latest
drama^H^H^H^H^H news. We need to proceed carefully with any
non-backwards-compatible changes if we're to keep those users informed, and
protect them from sending and receiving coins on chains that they didn't mean
too.


## Splitting Safely

So how can we split safely? Luke Dashjr has written both a
[BIP](https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki), and
[preliminary code](https://github.com/luke-jr/bitcoin/compare/bc94b87…luke-jr:hardfork2016)
to do a combination of a hard-fork, and a soft-fork.

This isn't a new idea, in fact Luke [posted it](https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012377.html)
to the bitcoin-dev mailing list last February, and it's been known as an option
for years prior; I personally mentioned it [on this blog](/2016/forced-soft-forks) last January.

The idea is basically that we do a hard-fork - an incompatible rule change - by
"wrapping" it in a soft-fork so that all nodes are forced to choose one chain
or the other. The new soft-forked rule-set is simple: no transactions are
allowed at all. Assuming that a majority of hashing power chooses to adopt the
fork, nodes that haven't made a decision are essentially 51% attacked and will
follow an empty chain, unable to make any transactions at all.

For those who choose not to adopt the hard-fork, they need to themselves do a
hard-fork to continue transacting. This can be as simple as blacklisting the
block where the two sides diverged, or something more complex like a
proof-of-work change.

On the plus side, Luke's proposal maximizes safety in many respects: so long as
a majority of hashing power adopts the fork no-one will accidentally accept
funds from a chain that they didn't intend too.


### Giving Everyone A Voice

It's notable that what Luke calls a "soft-hardfork" has also been called a
"forced soft-fork" by myself, as well as an "evil fork" by many others - what
name you give it is a matter of perspective. From a technical point of view,
the idea is a 51% attack against those who choose not to support the new
protocol; it's notable that when I pointed this out to some miners they were
very concerned about the precedent this could set if done badly.

Interestingly, due to implementation details Ethereum hard-fork was similar to
Luke's suggestion: pre-fork Ethereum clients would generally fail to start due
to an implementation flaw - in most cases - so everyone was forced to get new
software. Yet, Ethereum still split into two economically distinct coins.

This shows that attempting to kill an unwanted side of a split chain via 51%
attack isn't a panacea: people can and will choose to use the chain they want
to if there's controversy. So we'd be wise to try to achieve broad community
consensus first.

Interestingly, Tom Harding's [Hard fork opt-out bits](https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-July/012917.html)
proposal can also be used to measure consent. Basically, as an anti-replay
mechanism, wallets could begin (un)setting a nSequence bit in transaction
inputs that a hard-fork would make _invalid_, while simultaneously a soft-fork
would make (un)setting a different bit invalid already; the hard-fork would
make that second bit _valid_ (users of nLockTime would (un)set neither bit,
making their transactions valid on both chains). This allows us to easily
measure readiness for a fork by looking at what percentage of transactions are
setting the anti-replay bit correctly - a sign that their running software that
is ready for a future hard-fork.

Secondly, I've been working on coming up with more concrete mechanisms based on
signaling/voting proportional to coins held, in particular, out-of-band
mechanisms based on signed messages and probabilistic sampling that could
potentially offer better privacy and censorship resistance, and give "hodlers"
who aren't necessarily doing frequent transactions a voice as well. My recent
work on [making TXO commitments more practical](/2016/delayed-txo-commitments)
is part of that effort.


### UTXO Size

Segwit's witness-data-discount has the important effect of discouraging the
creation of new UTXOs, in favor of spending existing ones, hopefully resulting
in [reduced UTXO set growth](https://bitcoincore.org/en/2016/01/26/segwit-benefits/#reducing-utxo-growth).
As a full copy of the UTXO set is (currently) a mandatory requirement for
running a full node, even with pruning, it's important that we keep UTXO growth
rates sustainable.

Matt Corallo has been doing work on finding better ways to properly account for
the cost to the network as a whole of UTXO creation, and he has told me he'll
be publishing that work soon. In addition, I've been working on a longer-term
solution in the form of [TXO commitments](/2016/delayed-txo-commitments), which
hopefully can eliminate the problem entirely, by allowing UTXO's to be
archived, shifting the burden of storing them to wallets rather than all
Bitcoin nodes. Additionally, Bram Cohen has been [working on](https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-June/012758.html)
making the necessary data structures for TXO commitments faster in of
themselves by optimizing cache access patterns.


### Block Propagation Latency

A significant concern with any blocksize increase - including segwit - is that
the higher bandwidth requirements will encourage centralization of hashing
power due to the disproportionate effect higher latency has on stale rates
between large and small miners. Matt Corallo has done a significant amount of
work recently on mitigating this effect with his [compact blocks](https://bitcoincore.org/en/2016/06/07/compact-blocks-faq/) - to be
released in v0.13.0 - and his next-gen block relay network
[FIBRE](http://bluematt.bitcoin.ninja/2016/07/07/relay-networks/).

Additionally, I've been [doing research](http://bluematt.bitcoin.ninja/2016/07/07/relay-networks/) to better
understanding the limitations of these approaches in adversarial,
semi-adversarial, and "uncaring" scenarios.


### Anti-Replay

I mentioned Tom Harding's work, above; I'll also mention that Gregory Maxwell
proposed a generic - and very robust - solution to anti-replay: have
transactions commit to a recent but not too recent (older than ~100 blocks or so) block hash.
While this has some potential downsides in a large reorg - transactions may
become permanently invalid due to differing block hashes - so long as the block
hashes committed too aren't too recently the idea does very robustly fix replay
attacks across chains, in a way that's completely generic no matter how many
forks happen. For example, a reasonable way to deploy would be to have wallet's
refuse to make transactions for the first day or two after a hard-fork, and
then use a post-fork blockhash in all transactions to ensure they can't be
replayed on an unwanted chain.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------

> On Aug 17, 2016, at 15:24, Jonas Schnelli via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> URI scheme instead of stdio/pipe
> --------------------------------
> The URI scheme is not ugly. Its a modern way  implemented in almost all
> platforms  how applications can interact with each other while not
> directly knowing each other. Registering a URI scheme like "bitcoin://"
> has some concrete advantages over just piping through stdio.
> 
> Also, the stdio/piping approach does not work for mobile platforms
> (where the URI scheme works).
> 
> The URI scheme does not require any sorts of wallet app level
> configuration (where the stdio/pipe approach would require to configure
> some details about the used hardware wallet).

Hi everybody, just thought Id throw my opinion in here.

The URI scheme is a nice idea, but this ignores the fact that hardware wallet vendors do most of the work on talking between the computer/mobile and the wallet on a lower level of communication. In the case of BitLox, the base protocol is Googles ProtoBuf. The commands and transaction data is in a schema which is then encoded in different methods accessible via ProtoBuf (depending on the data being sent). The advantages of this protocol is that it can be implemented on a wide variety of platforms. (but thats a whole 'nother discussion)

The URI would be handled waaaaay up in the specific application (such as the mytrezor wallet software or the various standalone wallets) - nowhere near the actual hardware communications layer.

Best regards,
Dana
BitLox

-------------------------------------
I now think my reasoning and conclusions are based on a false premise: that
BU block size policies for miners can be heterogeneous.

There can't be short forks because forks are not in the best interest of
the honest miner majority. All miners need to announce and follow the same
block size policy to prevent short forks.

The incentives are established so that all block size negotiations will be
carried between miners in a off-chain manner, not by modifying the policy
nor by announcing anything in the coinbase,

If block size negotiations are meant to be open and carried on on-chain,
then it's much better to let miners increase or decrease the block size
limit by 1% per block (such as what Ethereum does with the gas limit).





On Fri, Nov 25, 2016 at 7:31 PM, Sergio Demian Lerner <
sergio.d.lerner@gmail.com> wrote:

>
>
> On Fri, Nov 25, 2016 at 12:25 PM, Tom Zander via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Thursday, 24 November 2016 22:39:05 CET Sergio Demian Lerner via
>> bitcoin-
>> dev wrote:
>> > Without a detailed analysis, unlimited block size seems a risky change
>> to
>> > Bitcoin, to me.
>>
>> What exactly do you think is a ‘change’ in bitcoin here?
>>
>> A change is anything that modifies with a HF the current state of the
> Bitcoin Core implementation of the consensus protocol. Sadly (or happily,
> for some) there is no "abstract" definition of Bitcoin.
>
>
>
>> The concept of proof-of-work is that the longer a chain, the higher
>> probability that that one will be extended for the simple reason that
>> another chain will have to show a higher amount of proof of work to ‘win’.
>>
>> We know what Bitcoin the protocol dictates, but if what the protocol
> dictates is not in the best interest of miners or full-nodes? then they
> will simply choose a rule that maximizes their revenue (or any other
> measure of performance, such as lower latency, or less transaction reversal
> probability).
>
>
> As far as I understand the document from Peter, there is no change there at
>> all. Only chains with more POW will win.
>>
>
> I haven't gone to the code to check, but the video Peter sent does not say
> that. It says that miners will mine on top of a block ONLY if the "gate"
> has been opened for that block (e.g. there is additional blocks to push a
> big block). So a miner having a preferring low block sizes will choose to
> mine on top of the A1,A2,A3 chain (3 units of work), while miners
> supporting bigger sizes will mine on top of the chain B1,S2,S3,S4 (4 units
> of work).
>
> Saying that the chain starting with B1 is not considered by a node X does
> not mean that the node X is blind to the information that can be extracted
> from the fact that there is a chain of 4 blocks starting from B1.
> If there is more information, there may be a better local choice. If there
> are better local choices, there is probably a better global equilibrium (or
> not equilibrium at all).
>
>
>> Or, to answer your example, miners will prefer to extend the chain with
>> the
>> most POW.
>>
>
> Clearly this is not universal: some miners will, and some other miners
> won't, because some miners have postponed adding some blocks.
>
>
>
>>
>> The other fact stays the same as well, if you protect from reorgs by
>> expecting more confirmations. Nothing changes here either. The
>> common-sense 6
>> confirmations for things like exchange-deposits keep having the same
>> security.
>>
>
> Suppose that I provide a service that accepts payments with 2
> confirmations, and in certain time I have the information that the network
> is at the same time considering the forks B1 S2 and A1 A2. Then the best I
> can do is NOT to accept the 2-confirmation and wait for a resolution of the
> fork. Choosing either fork may put me at the risk of immediate reversal.
>
> The existence of fork information changes equilibrium decision to choose
> the longest-chain.  This is the same that happens with the GHOST protocol:
> the information on the existence of uncles changes the local incentives to
> choose the longest chain to some different strategy, and when all nodes
> change their strategy, then the supposedly last equilibrium state is that
> all follow the GHOST strategy for choosing the heaviest chain.
>
>
>>
>> The basic idea that we have a 3 or 4 deep fork is a huge problem in
>> Bitcoin.
>> It hasn’t happened for ages, and we like it that way. The miners like it
>> that way too. Its disruptive.
>> The is a problem that is not created by the ‘excessive block’ concept. It
>> does, however, provide a possible solution to this very far-fetched
>> problem.
>>
>> You should also realize that the policy of a miner is stored in the
>> coinbase.
>>
>> This is important, but yet the full node does not use this information
> automatically. The amount of confirmations that a node accepts is not
> affected by the miner's policies or the size of the blocks mined, but it
> should.
>
>
>> That said, I’m sure there are improvements to be made to the policy that
>> BU
>> uses.
>
>
> Probably a simple wise addition would be to estimate the accepted block
> size for the majority of the miners (S), and only count block confirmations
> for wallet transactions taking into account only blocks whose size is lower
> or equal than S. So for example, if Alice receives a transaction T in block
> B1 and it is confirmed by block B2, but size(B1)>S and size(B2)>S, then the
> wallet should tell Alice that transaction T has 0 confirmations. This local
> strategy reduces the chances that Alice accept T but is then easily
> reversed for the opposite fork growing one block ahead.
>
> Regards,
>  Sergio
>
>

-------------------------------------
Based on Luke Dashjr’s code and BIP: https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki , I created an experimental network to show how a new header format may be implemented.

Basically, the header hash is calculated in a way that non-upgrading nodes would see it as a block with only the coinbase tx and zero output value. They are effectively broken as they won’t see any transactions confirmed. This allows rewriting most of the rules related to block and transaction validity. Such technique has different names like soft-hardfork, firmfork, evil softfork, and could be itself a controversial topic. However, I’d rather not to focus on its soft-hardfork property, as that would be trivial to turn this into a true hardfork (e.g. setting the sign bit in block nVersion, or setting the most significant bit in the dummy coinbase nLockTime)

Instead of its soft-HF property, I think the more interesting thing is the new header format. The current bitcoin header has only 80 bytes. It provides only 32bits of nonce space and is far not enough for ASICs. It also provides no room for committing to additional data. Therefore, people are forced to put many different data in the coinbase transaction, such as merge-mining commitments, and the segwit commitment. It is not a ideal solution, especially for light wallets.

Following the practice of segwit development of making a experimental network (segnet), I made something similar and call it the Forcenet (as it forces legacy nodes to follow the post-fork chain)

The header of forcenet is mostly described in Luke’s BIP, but I have made some amendments as I implemented it. The format is (size in parentheses; little endian):

Height (4), BIP9 signalling field (4), hardfork signalling field (3), merge-mining hard fork signalling field (1), prev hash (32), timestamp (4), nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), Hash TMR (32), Hash WMR (32), total tx size (8) , total tx weight (8), total sigops (8), number of tx (4), merkle branches leading to header C (compactSize + 32 bit hashes)

In addition to increasing the max block size, I also showed how the calculation and validation of witness commitment may be changed with a new header. For example, since the commitment is no longer in the coinbase tx, we don’t need to use a 0000….0000 hash for the coinbase tx like in BIP141.

Something not yet done:
1. The new merkle root algorithm described in the MMHF BIP
2. The nTxsSigops has no meaning currently
3. Communication with legacy nodes. This version can’t talk to legacy nodes through the P2P network, but theoretically they could be linked up with a bridge node
4. A new block weight definition to provide incentives for slowing down UTXO growth
5. Many other interesting hardfork ideas, and softfork ideas that works better with a header redesign

For easier testing, forcenet has the following parameters:

Hardfork at block 200
Segwit is always activated
1 minutes block with 40000 (prefork) and 80000 (postfork) weight limit
50 blocks coinbase maturity
21000 blocks halving
144 blocks retarget

How to join: codes at https://github.com/jl2012/bitcoin/tree/forcenet1 , start with "bitcoind —forcenet" .
Connection: I’m running a node at 8333.info with default port (38901)
Mining: there is only basic internal mining support. Limited GBT support is theoretically possible but needs more hacking. To use the internal miner, writeup a shell script to repeatedly call “bitcoin-cli —forcenet generate 1”
New RPC commands: getlegacyblock and getlegacyblockheader, which generates blocks and headers that are compatible with legacy nodes.

This is largely work-in-progress so expect a reset every couple weeks

jl2012



-------------------------------------
Slush,

You can actually detect the use of this improvement by looking at the I/O
of the chip, the I/O of an on-board micro-controller or even at the system
I/O because all the communication including the mining pool protocol is
different.

Timo

On Wed, Apr 6, 2016 at 1:57 AM, Marek Palatinus <marek@palatinus.cz> wrote:

> To my understanding it is purely software thing. It cannot be detected
> from outside if miner uses this improvement or not. So patenting it is
> worthless.
>
> slush
>
> On Tue, Apr 5, 2016 at 1:01 AM, Mustafa Al-Bassam via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Alternatively scenario: it will cause a sudden increase of Bitcoin mines
>> in countries where the algorithm is not patented, possibly causing a
>> geographical decentralization of miners from countries that already have a
>> lot of miners like China (if it is patented in China).
>>
>> On 01/04/16 10:00, Peter Todd via bitcoin-dev wrote:
>>
>> On Thu, Mar 31, 2016 at 09:41:40PM -0700, Timo Hanke via bitcoin-dev wrote:
>>
>> Hi.
>>
>> I'd like to announce a white paper that describes a very new and
>> significant algorithmic improvement to the Bitcoin mining process which has
>> never been discussed in public before. The white paper can be found here:
>> http://www.math.rwth-aachen.de/~Timo.Hanke/AsicBoostWhitepaperrev5.pdf
>>
>> What steps are you going to take to make sure that this improvement is
>> available to all ASIC designers/mfgs on a equal opportunity basis?
>>
>> The fact that you've chosen to patent this improvement could be a
>> centralization concern depending on the licensing model used. For example, one
>> could imagine a licensing model that gave one manufacture exclusive rights.
>>
>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing listbitcoin-dev@lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>

-------------------------------------
On Fri, Jan 8, 2016 at 2:54 AM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:
> I'm saying we can eliminate one somewhat unlikely attack (that there is a
> bug in the code or test cases, today or some future version, that has to
> decide what to do with "version 0" versus "version 1" witness programs) by
> accepting the risk of another insanely, extremely unlikely attack.

Ok, just having one witness program version now is a somewhat different
proposal. It would be simpler for sure. The reasoning was that you'd need
this to not add significant overhead to small scripts, but that may not be
the case anymore. I wouldn't mind seeing numbers.

> My proposal would be to just do a version 0 witness program now, that is
> RIPEMD160(SHA256(script)).

I don't think that is wise. Bitcoin has a 128-bit security target for
everything else. We did not know that P2SH and similar constructs were
vulnerable to a collision attack at the time, but now we do, so the obvious
choice is to pick a size that is sufficiently large to maintain the 128-bit
security target. This is a no brainer to me; we're not proposing switching
to a 160-bit EC curve either, right?

> I'm really disappointed with the "Here's the spec, take it or leave it"
> attitude. What's the point of having a BIP process if the discussion just
> comes down to "We think more is better. We don't care what you think."

It is a proposal and we are discussing it. You first brought up some
criticisms in private, and I agreed with several things you said.

But it remains the proposal of a few people including me, and I do not
agree with the specific suggestion of reducing the security target for
witness scripts to 80 bits.

We are not deciding what the system will be. We're making a proposal, and
hope that due to its technical merit, the ecosystem will adopt it. You're
free to participate in that discussion.

-- 
Pieter

-------------------------------------
On Fri, Feb 5, 2016 at 8:51 PM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> This has been reviewed by merchants, miners and exchanges for a couple of
> weeks, and has been implemented and tested as part of the Bitcoin Classic
> and Bitcoin XT implementations.
>
> Constructive feedback welcome; argument about whether or not it is a good
> idea to roll out a hard fork now will be unproductive, so I vote we don't
> go there.
>
> Draft BIP:
>   https://github.com/gavinandresen/bips/blob/bump2mb/bip-bump2mb.mediawiki
>
> Summary:
>   Increase block size limit to 2,000,000 bytes.
>   After 75% hashpower support then 28-day grace period.
>   With accurate sigop counting, but existing sigop limit (20,000)
>   And a new, high limit on signature hashing
>
> Blog post walking through the code:
>   http://gavinandresen.ninja/a-guided-tour-of-the-2mb-fork
>
> Blog post on a couple of the constants chosen:
>   http://gavinandresen.ninja/seventyfive-twentyeight
>

It's great to finally see a BIP, although seems strange to ask for feedback
after releasing binaries.

In any case, the issue isn't about "whether or not it is a good idea to
roll out a hard fork", the question has always been about how to do safe
hard fork deployment and what the technological requirements are for doing
so. Your BIP/blogs do not actually address any of this. 75% miner
signalling with a 28 day flag day thereafter gives virtually no time for
the entire ecosystem to migrate and is widely considered unsafe. It's
plainly obvious that an entire ecosystem of 5000 full nodes cannot be
prepared in a month.

-------------------------------------
It has occurred to me that some folks may not have seen the link floating
around the other day on IRC.

Transcript:
https://bitcoincore.org/logs/2016-05-zurich-meeting-notes.html
https://bitcoincore.org/logs/2016-05-zurich-meeting-notes.txt

Meeting notes summary:
https://bitcoincore.org/en/meetings/2016/05/20/

Topics discussed and documented include mostly obscure details about
segwit, segwit code review, error correcting codes for future address
types, encryption for the p2p network protocol, compact block relay,
Schnorr signatures and signature aggregation, networking library, encrypted
transactions, UTXO commitments, MAST stuff, and many other topics. I think
this is highly useful reading material.

Any errors in transcription are very likely my own as it is difficult to
capture everything with high accuracy in real-time. Another thing to keep
in mind is that there are many different parallel conversations and I only
do linear serialization at best... and finally, I also want to mention that
this is the result of collaboration with many colleagues and this should
not be considered merely the work of just myself.

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
On Monday, February 01, 2016 9:43:33 PM Cory Fields wrote:
> On Mon, Feb 1, 2016 at 2:46 PM, Luke Dashjr <luke@dashjr.org> wrote:
> > Allowing for simpler cases both encourages the lazy case, and enables
> > pools to require miners use it. It also complicates the server-side
> > implementation somewhat, and could in some cases make it more vulnerable
> > to DoS attacks. Keep in mind that GBT is not merely a bitcoind protocol,
> > but is used between pool<->miner as well... For now, it makes sense to
> > leave
> > "default_witness_commitment" as a bitcoind-specific extension to
> > encourage adoption, but it seems better to leave it out of the standard
> > protocol. Let me know if this makes sense or if I'm overlooking
> > something.
> 
> I think that's a bit of a loaded answer. What's to keep a pool from
> building its own commitment and requiring miners to use that? I don't
> see how providing the known-working commitment for the
> passed-in-hashes allows the pool/miner to do anything they couldn't
> already, with the exception of skipping some complexity. Please don't
> confuse encouraging with enabling.

Making it simpler to do a centralised implementation than a decentralised one, 
is both enabling and encouraging. GBT has always been designed to make it 
difficult to do in a centralised manner.

> What's the DoS vector here?

It's more work for the pool to provide it, similar to the "midstate" field was 
with getwork. Someone performing a DoS needs to do less work to force the pool 
to do complex calculations (unless the same transaction tree / commitment is 
used for all miners, which would be an unfortunate limitation).

> >> The issue in particular here is that a non-trivial burden is thrust
> >> upon mining software, increasing the odds of bugs in the process.
> > 
> > It can always use libblkmaker to handle the "heavy lifting"... In any
> > case, the calculation for the commitment isn't significantly more than
> > what it must already do for the stripped merkle tree.
> 
> Agreed. However for the sake of initial adoption, it's much easier to
> have a known-correct value to use. Even if it's just for the sake of
> checking against.

Sure, I'm not suggesting we remove this from bitcoind (probably the only place 
that makes initial adoption easier).

> >> [4]:
> >> https://github.com/theuni/ckpool/commit/7d84b1d76b39591cc1c1ef495ebec513
> >> cb 19a08e
> > 
> > I'm pretty sure this commit is actually /introducing/ a bug in working
> > (albeit ugly) code. The height, while always positive, is serialised as
> > a signed number, so 0x80 needs to be two bytes: 80 00.
> 
> You're right, thanks. The current code breaks on heights of (for ex)
> 16513. I'll fix up my changes to take the sign bit into account.

I'm curious what bug it was fixing? Was it overwriting data beyond the number?

Luke


-------------------------------------
Hard forks should always come in response to some major crisis that all participants can agree is an actual crisis, as per the excellent rational here:

http://bitledger.info/why-a-hard-fork-should-be-fought-and-its-not-evil-to-discuss/

And here:

http://bitledger.info/hard-fork-risks-and-why-95-should-be-the-standard/

Also, if you’re going to do a hard fork, you’d better make the most of it as hard forks must be a *rare* world-is-ending-if-we-don’t-do-it thing (otherwise Bitcoin cannot be considered decentralized in any sense of the word).

So for any sort of hard fork, be sure to address the real threats and challenges that are facing Bitcoin today:

1. Mining centralization.
2. Privacy.

Best regards,
Greg Slepak

> On Feb 8, 2016, at 12:37 PM, jl2012--- via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> Thanks for this proposal. Just some quick response:
> 
> 1. The segwit hardfork (BIP HF) could be deployed with BIP141 (segwit
> softfork). BIP141 doesn't need grace period. BIP HF will have around 1 year
> of grace period.
> 
> 2. Threshold is 95%. Using 4 versoin bits: a) BIP 141; b) BIP HF; c) BIP 141
> if BIP HF has already got 95%; d) BIP HF if BIP141 has already got 95%.
> Voting a and c (or b and d) at the same time is invalid. BIP 141 is
> activated if a>95% or (a+c>95% and b+d>95%). BIP HF is activated if b>95% or
> (a+c>95% and b+d>95%).
> 
> 3. Fix time warp attack: this may break some SPV implementation
> 
> 4. Limiting non-segwit inputs may make some existing signed tx invalid. My
> proposal is: a) count the number of non-segwit sigop in a tx, including
> those in unexecuted branch (sigop); b) measure the tx size without scripgSig
> (size); c) a new rule is SUM(sigop*size) < some_value . This allows
> calculation without actually running the script.
> 
> 
> -----Original Message-----
> From: bitcoin-dev-bounces@lists.linuxfoundation.org
> [mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Matt
> Corallo via bitcoin-dev
> Sent: Tuesday, 9 February, 2016 03:27
> To: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
> Subject: [bitcoin-dev] On Hardforks in the Context of SegWit
> 
> Hi all,
> 
> I believe we, today, have a unique opportunity to begin to close the book on
> the short-term scaling debate.
> 
> First a little background. The scaling debate that has been gripping the
> Bitcoin community for the past half year has taken an interesting turn in
> 2016. Until recently, there have been two distinct camps - one proposing a
> significant change to the consensus-enforced block size limit to allow for
> more on-blockchain transactions and the other opposing such a change,
> suggesting instead that scaling be obtained by adding more flexible systems
> on top of the blockchain. At this point, however, the entire Bitcoin
> community seems to have unified around a single vision - roughly 2MB of
> transactions per block, whether via Segregated Witness or via a hard fork,
> is something that can be both technically supported and which adds more
> headroom before second-layer technologies must be in place. Additionally, it
> seems that the vast majority of the community agrees that segregated witness
> should be implemented in the near future and that hard forks will be a
> necessity at some point, and I don't believe it should be controversial
> that, as we have never done a hard fork before, gaining experience by
> working towards a hard fork now is a good idea.
> 
> With the apparent agreement in the community, it is incredibly disheartening
> that there is still so much strife, creating a toxic environment in which
> developers are not able to work, companies are worried about their future
> ability to easily move Bitcoins, and investors are losing confidence. The
> way I see it, this broad unification of visions across all parts of the
> community places the burden of selecting the most technically-sound way to
> achieve that vision squarely on the development community.
> 
> Sadly, the strife is furthered by the huge risks involved in a hard fork in
> the presence of strife, creating a toxic cycle which prevents a safe hard
> fork. While there has been talk of doing an "emergency hardfork" as an
> option, and while I do believe this is possible, it is not something that
> will be easy, especially for something as controversial as rising fees.
> Given that we have never done a hard fork before, being very careful and
> deliberate in doing so is critical, and the technical community working
> together to plan for all of the things that might go wrong is key to not
> destroying significant value.
> 
> As such, I'd like to ask everyone involved to take this opportunity to
> "reset", forgive past aggressions, and return the technical debates to
> technical forums (ie here, IRC, etc).
> 
> As what a hard fork should look like in the context of segwit has never
> (!) been discussed in any serious sense, I'd like to kick off such a
> discussion with a (somewhat) specific proposal.
> 
> First some design notes:
> * I think a key design feature should be taking this opportunity to add
> small increases in decentralization pressure, where possible.
> * Due to the several non-linear validation time issues in transaction
> validation which are fixed by SegWit's signature-hashing changes, I strongly
> believe any hard fork proposal which changes the block size should rely on
> SegWit's existence.
> * As with any hard fork proposal, its easy to end up pulling in hundreds of
> small fixes for any number of protocol annoyances. In order to avoid doing
> this, we should try hard to stick with a few simple changes.
> 
> Here is a proposed outline (to activate only after SegWit and with the
> currently-proposed version of SegWit):
> 
> 1) The segregated witness discount is changed from 75% to 50%. The block
> size limit (ie transactions + witness/2) is set to 1.5MB. This gives a
> maximum block size of 3MB and a "network-upgraded" block size of roughly
> 2.1MB. This still significantly discounts script data which is kept out of
> the UTXO set, while keeping the maximum-sized block limited.
> 
> 2) In order to prevent significant blowups in the cost to validate
> pessimistic blocks, we must place additional limits on the size of many
> non-segwit transactions. scriptPubKeys are now limited to 100 bytes in size
> and may not contain OP_CODESEPARATOR, scriptSigs must be push-only (ie no
> non-push opcodes), and transactions are only allowed to contain up to 20
> non-segwit inputs. Together these limits limit total-bytes-hashed in block
> validation to under 200MB without any possibility of making existing outputs
> unspendable and without adding additional per-block limits which make
> transaction-selection-for-mining difficult in the face of attacks or
> non-standard transactions. Though 200MB of hashing (roughly 2 seconds of
> hash-time on my high-end
> workstation) is pretty strongly centralizing, limiting transactions to fewer
> than 20 inputs seems arbitrarily low.
> 
> Along similar lines, we may wish to switch MAX_BLOCK_SIGOPS from
> 1-per-50-bytes across the entire block to a per-transaction limit which is
> slightly looser (though not too much looser - even with libsecp256k1
> 1-per-50-bytes represents 2 seconds of single-threaded validation in just
> sigops on my high-end workstation).
> 
> 3) Move SegWit's generic commitments from an OP_RETURN output to a second
> branch in the merkle tree. Depending on the timeline this may be something
> to skip - once there is tooling for dealing with the extra OP_RETURN output
> as a generic commitment, the small efficiency gain for applications checking
> the witness of only one transaction or checking a non-segwit commitment may
> not be worth it.
> 
> 4) Instead of requiring the first four bytes of the previous block hash
> field be 0s, we allow them to contain any value. This allows Bitcoin mining
> hardware to reduce the required logic, making it easier to produce
> competitive hardware [1].
> 
> I'll deliberately leave discussion of activation method out of this
> proposal. Both jl2012 and Luke-Jr recently begun some discussions about
> methods for activation on this list, and I'd love to see those continue.
> If folks think a hard fork should go ahead without SPV clients having a say,
> we could table #4, or activate #4 a year or two after 1-3 activate.
> 
> 
> [1] Simpler here may not be entirely true. There is potential for
> optimization if you brute force the SHA256 midstate, but if nothing else,
> this will prevent there being a strong incentive to use the version field as
> nonce space. This may need more investigation, as we may wish to just set
> the minimum difficulty higher so that we can add more than 4 nonce-bytes.
> 
> 
> 
> 
> Obviously we cannot reasonably move forward with a hard fork as long as the
> contention in the community continues. Still, I'm confident continuing to
> work towards SegWit as a 2MB-ish soft-fork in the short term with some plans
> on what a hard fork should look like if we can form broad consensus can go a
> long way to resolving much of the contention we've seen.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
I accidentally replied to Luke off-list, and this was his reply to my last
message:

"But wouldn't the server be a trusted third-party in this case?
I'm thinking it's very close to being possible for an untrusted server to do
this..."

If you are okay with anyone being able to view your PaymentRequest
messages, then you wouldn't need to encrypt them. Just upload them to the
server and let it give them away--no trust needed as long as you include a
signature. If you want only certain people to be able to see your messages,
then you need to denote those people in some way. In this situation, you
would do that by trading public keys and uploading encryptedPaymentRequest
messages to the server that only those people could read.

Using the encrypted method doesn't require the devices to be online, but it
does require at least one of the parties to know the other party's public
key. Do you have a specific use case in mind?

James

On Tue, Mar 8, 2016 at 3:07 PM James MacWhyte <macwhyte@gmail.com> wrote:

> Our BIP just defines protocol definitions, and doesn't really dictate how
> people use them (we're coming up with a new title for the BIP, by the way,
> to more accurately convey that). Using our definitions as building blocks,
> someone could definitely accomplish what you described. For example, Joe
> Mobile Wallet User's wallet could upload a slew of generic PaymentRequest
> messages with signatures to prove his identity, and the server could then
> create encryptedPaymentRequest messages using the server's key for
> encryption and communication with the other party. In this case the server
> would essentially be a proxy for the user without having actual access to
> the user's private keys.
>
> My personal goal with the protocol was to keep it extremely flexible so
> developers could use it to build all different types of schemes while
> keeping standard messages that could be forwarded between services if
> needed. Does the above make sense?
>
> James
>
> On Tue, Mar 8, 2016 at 2:55 PM Luke Dashjr via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Is there a way for Joe Mobile Wallet User to upload a set of N
>> PaymentRequests
>> authenticated by his key to an untrusted server, which encrypts and passes
>> them on in response to InvoiceRequests? Or does this necessarily require
>> the
>> recipient to be online?
>>
>> On Tuesday, March 01, 2016 6:58:16 PM Justin Newton via bitcoin-dev wrote:
>> > The following draft BIP proposes an update to the Payment Protocol.
>> >
>> > Motivation:
>> >
>> > The motivation for defining this extension to the BIP70 Payment
>> Protocol is
>> > to allow 2 parties to exchange payment information in a permissioned and
>> > encrypted way such that wallet address communication can become a more
>> > automated process. Additionally, this extension allows for the
>> requestor of
>> > a PaymentRequest to supply a certificate and signature in order to
>> > facilitate identification for address release. This also allows
>> > for automated creation of off blockchain transaction logs that are human
>> > readable, containing who you transacted with, in addition to the
>> > information that it contains today.
>> >
>> > The motivation for this extension to BIP70 is threefold:
>> >
>> > 1. Ensure that the payment details can only be seen by the participants
>> in
>> > the transaction, and not by any third party.
>> > 2. Enhance the Payment Protocol to allow for store and forward servers
>> in
>> > order to allow, for example, mobile wallets to sign and serve
>> > Payment Requests.
>> > 3. Allow a sender of funds the option of sharing their identity with the
>> > receiver. This information could then be used to:
>> >
>> >         * Make bitcoin logs more human readable
>> >         * Give the user the ability to decide who to release payment
>> > details to
>> >         * Allow an entity such as a political campaign to ensure donors
>> > match regulatory and legal requirements
>> >         * Allow for an open standards based way for regulated financial
>> > entities to meet regulatory requirements
>> >         * Automate the active exchange of payment addresses, so static
>> > addresses and BIP32 X-Pubs can be avoided to maintain privacy
>> > and convenience
>> >
>> > In short we wanted to make bitcoin more human, while at the same time
>> > improving transaction privacy.
>> >
>> > Full proposal here:
>> >
>> >
>> https://github.com/techguy613/bips/blob/master/bip-invoicerequest-extension
>> > .mediawiki
>> >
>> > We look forward to your thoughts and feedback on this proposal!
>> >
>> > Justin
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>

-------------------------------------
On Sat, Aug 6, 2016 at 11:39 AM, s7r via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> * reversal of transactions is impossible
>

I think it would be more accurate to say that the requirement is that
reversal doesn't happen unexpectedly.

If it is clear in the script that reversal is possible, then obviously the
recipient can take that into consideration.


> * keep private keys private and safe. Lose them, it's like losing cash,
> you can just forget about it.
>

Key management is a thing.  Managing risk by keeping some keys offline is
an important part of that.


> * while we try hard to make 0-conf as safe as possible (if there's no
> RBF flag on the transaction), we make it almost impossible or very very
> expensive to reverse a confirmed transaction.
>

BitGo has an "instant" system where they promise to only sign one
transaction for a given output.  If you trust BitGo, then this is safe from
double spending, since a double spender can't sign two transactions.

If BitGo had actually implemented a daily withdrawal limit, then their
system ends up similar to cold storage.  Only 10% of the funds at Bitfinex
could have been withdrawn before manual intervention was required (with
offline keys).

Who will accept
> such an input and treat it as a payment if it can be reversed during the
> settlement layer?


Obviously, if a payment is reversible, then you treat it as a reversible
payment.  The protection here relates to moving coins from the equivalent
of cold storage to hot storage.

It is OK if it takes longer, since security is more important than
convenience for coins in cold storage.


> The linked page describes that merchants will never accept payments from
> 'vaults', and it will take 24 hours for coins to be irreversible moved
> outside the 'vault'.


This relates to the reserves held by the exchange.  A portion of the funds
are in hot storage with live keys.  These funds can be stolen by anyone who
gets access to the servers.  The remaining funds are held in cold storage
and they cannot be accessed unless you have the offline keys.  These funds
are supposed to be hard to reach and require manual intervention.

I think this is a wrong approach. hacks and big losses are sad, but all
> the time users / exchanges are to blame for wrong implementations or
> terrible security practices.
>

Setting up offline keys to act as firebreaks is part of good security
practices.

-------------------------------------
On Mon, Jun 20, 2016 at 04:21:39PM +0000, zaki--- via bitcoin-dev wrote:
> Hi Peter,
> 
> I didn't entirely understand the process of transaction linearization.
> 
> What I see is a potential process where when the miner assembles the block,
> he strips all but one sigscript per tx. The selection of which  sigscript
> is retained is determined by the random oracle.  Is this is primary benefit
> you are suggesting?
> 
> It appears to me that blocks still need to contain a list of full TX Input
> and Tx Outputs with your approach. Some of the description seems to
> indicate that there are opportunities to elide further data but it's
> unclear to me how.

I think you've misunderstood what I'm proposing. The state machine approach I
described doesn't necessarily require blocks or even miners to exist at all.
Rather, it assumes that a single-use seal primitive is available, and a random
beacon primitive for tx linearization, and then builds a system on top of those
primitives. Transaction data - the proofs that certain states have been reached
in the system - does not need to be broadcast publicly; if Alice wants to
convince Bob that she has given him money, the only person who needs that
transaction (and transactions prior to it in the tx history) is Bob.

So as to your question about miners assembling blocks, and what blocks contain:
there doesn't need to be blocks at all! Transaction history linearization is
something your wallet would do for you.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Erik
What would be the advantages of transmitting a BIP32 public seed, instead
of a plain address?

Theo
I didn't really think of that, but that's genius.

On Wed, Aug 10, 2016 at 6:49 AM, Theo Chino via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Another use for the audio would be for watches that can listen but can't
> use a camera (ie: Samsung S2), so sound would be great.
>
> On Wed, Aug 10, 2016 at 7:42 AM, Erik Aronesty via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> NOTE:
>>
>> Addresses aren't really meant to be broadcast - you should probably be
>> encoding BIP32 public seeds, not addresses.
>>
>> OR simply:
>>
>> - Send btc to rick@q32.com
>> - TXT record _btc.rick.q32.com is queried (_<coin-code>.<name>.<domain>)
>> - DNS-SEC validation is *required*
>> - TXT record contains addr:[<bip32-pub-seed>]
>>
>> Then you can just say, in the podcast, "Send your bitcoin donations to
>> rick@q32.com".   And you can link it to your email address, if your
>> provider lets you set up a TXT record.   (By structuring the TXT record
>> that way, many existing email providers will support the standard without
>> having to change anything.)
>>
>> This works with audio, video, web and other publishing formats... and
>> very little infrastructure change is needed.
>>
>>
>> On Wed, Aug 10, 2016 at 6:41 AM, Tier Nolan via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> Have you considered CDMA?  This has the nice property that it just
>>> sounds like noise.  The codes would take longer to send, but you could send
>>> multiple bits at once and have the codes orthogonal.
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Wow. No value judgement, but 1980 called, they want their radio broadcast
for analogue modems back. Both very cool and very cringe worthy.

It sounds quite horrible tbh. Imagine this being as pervasive as bar and qr
codes. And it's as meaningful and unpleasant to the human ear as a qr code
is to the eye.

Please think of something like using a Mozart symphony as the carrier wave
onto which you modulate your signal. Let the notes last a little longer to
represent a 1 bit. Or change the tempo. Or add an echo. Make it so the
listener can interpret it as a generic not too annoying tune and not even
realise it's different every time without being an audiophile.

Maybe have a 100 different base tunes from mozart to hiphop so the user can
pick one suitable to their audience and context. Maybe have some that don't
interfere with human speech frequencies so narrator can keep talking right
over it.

I guess it may be tricky because you want your signal to survive
re-encoding as increased playback speeds.

Another consideration: you want a preamble that is very easy to detect, so
it doesn't cost a lot of CPU (battery) to have your podcast player
continuously scanning for these things.

Not sure all these wishes are possible at the same time, but surely there's
research around on some?.

On 10 Aug 2016 1:28 a.m., "Daniel Hoffman via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I have updated the GitHub a lot (changed tones to be less chirpy, fixed
> some smalls) and made a couple of samples (see attachment for MP3 and FLAC
> of both tone tables, first 16 then 4). Is this good enough to warrant an
> official BIP number? I haven't built a decoder yet, but it seems like the
> encoder is working properly (looked at Audacity, seems like it is working),
> and some people on reddit want to "allow for decoding experiments"
> <https://www.reddit.com/r/btc/comments/4wsn7v/bip_proposal_addresses_over_audio_thoughts/d69m3st>
>
> What suggestions do you all have for it?
>
> On Mon, Aug 8, 2016 at 8:50 PM, Daniel Hoffman <danielhoffman699@gmail.com
> > wrote:
>
>> It wouldn't be feasible in the vast majority of cases, but I can't think
>> of a reason why it can't be built into the standard.
>>
>> On Mon, Aug 8, 2016 at 5:59 PM, Trevin Hofmann via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> Would it be feasible to transmit an entire BIP21 URI as audio? If you
>>> were to encode any extra information (such as amount), it would be useful
>>> to include a checksum for the entire message. This checksum could possibly
>>> be used instead of the checksum in the address.
>>>
>>> Trevin
>>>
>>> On Aug 8, 2016 3:06 PM, "Justin Newton via bitcoin-dev" <
>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>> Daniel,
>>>>    Thanks for proposing this.  I think this could have some useful use
>>>> cases as you state.  I was wondering what you would think to adding some
>>>> additional tones to optionally denote an amount (in satoshis?).
>>>>
>>>> (FYI, actual link is here:  https://github.com/Dako300/BIP )
>>>>
>>>> Justin
>>>>
>>>> On Mon, Aug 8, 2016 at 2:22 PM, Daniel Hoffman via bitcoin-dev <
>>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>>
>>>>> This is my BIP idea: a fast, robust, and standardized for representing
>>>>> Bitcoin addresses over audio. It takes the binary representation of the
>>>>> Bitcoin address (little endian), chops that up into 4 or 2 bit chunks
>>>>> (depending on type, 2 bit only for low quality audio like american
>>>>> telephone lines), and generates a tone based upon that value. This started
>>>>> because I wanted an easy way to donate to podcasts that I listen to, and
>>>>> having a Shazam-esque app (or a media player with this capability) that
>>>>> gives me an address automatically would be wonderful for both the consumer
>>>>> and producer. Comes with error correction built into the protocol
>>>>>
>>>>> You can see the full specification of the BIP on my GitHub page (
>>>>> https://github.com/Dako300/BIP-0153).
>>>>>
>>>>> _______________________________________________
>>>>> bitcoin-dev mailing list
>>>>> bitcoin-dev@lists.linuxfoundation.org
>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>>
>>>> Justin W. Newton
>>>> Founder/CEO
>>>> Netki, Inc.
>>>>
>>>> justin@netki.com
>>>> +1.818.261.4248
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev@lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Seems it could be done without any new opcode:

 

Bob is trading b Bitcoins for a altcoins.

 

1. Bob Pays D Bitcoins to

 

IF

<now+2days> CLTV DROP <Alice PK> CHECKSIG

ELSE

HASH160 <hash secret B> EQUALVERIFY <Bob PK> CHECKSIG

ENDIF

 

2. Alice pays a altcoins to

 

IF

HASH160 <hash secret B> EQUALVERIFY <Alice PK> CHECKSIG

ELSE

HASH160 <hash secret A> EQUALVERIFY <Bob PK> CHECKSIG

ENDIF

 

3. Bob pays b Bitcoins to

 

IF

<now+1days> CLTV DROP <Bob PK> CHECKSIG

ELSE

HASH160 <hash secret A> EQUALVERIFY <Alice PK> CHECKSIG

ENDIF

 

4. Alice claims output from step 3 and reveals secret A

 

5. Bob claims output from step 2

 

6. Bob claims output from step 1 and reveals secret B

 

From: bitcoin-dev-bounces@lists.linuxfoundation.org [mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Tier Nolan via bitcoin-dev
Sent: Friday, 12 February, 2016 04:05
To: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
Subject: [bitcoin-dev] BIP CPRKV: Check private key verify

 

There was some discussion on the bitcointalk forums about using CLTV for cross chain transfers.

Many altcoins don't support CLTV, so transfers to those coins cannot be made secure.  

I created a protocol.  It uses on cut and choose to allow commitments to publish private keys, but it is clunky and not entirely secure.

I created a BIP draft for an opcode which would allow outputs to be locked unless a private key was published that matches a given public key.


https://github.com/TierNolan/bips/blob/cpkv/bip-cprkv.mediawiki


 <https://www.avast.com/sig-email> 

This email has been sent from a virus-free computer protected by Avast. 
 <https://www.avast.com/sig-email> www.avast.com 

 


-------------------------------------
> The entire point of the definition of eventually consistency is that your
> computer system is running continously and DO NOT have a final state, and
> therefore you must be able to describe the behavior when your system either
> may give responses to queries across time that are either perfectly
> consistent *or not* perfectly consistent.
>
This is not the definition of eventual consistency. From
https://en.wikipedia.org/wiki/Eventual_consistency:
Eventual consistency is a consistency model used in distributed computing
to achieve high availability that informally guarantees that, if no new
updates are made to a given data item, eventually all accesses to that item
will return the last updated value.

The actual definition makes it quite clear that a system need not have a
final state to be evaluated for its consistency properties. Almost all
practical database systems execute continuously without a final state.

> And Bitcoin by default *does not* ignore the contents of the last X
> blocks. A Bitcoin node being queried about the current blockchain state
> WILL give inconsistent answers when there's block rearrangements = no
> strong consistency.


One could split hairs here by pedantically defining "Bitcoin by default" --
you could refer to just the reference client code and ignore the shim code
in the app that interfaces with the client -- but that'd drag us into a
fruitless email-list-style discussion from which no one would emerge any
wiser. I'll avoid that, and will instead dryly note that the reference
client's listreceivedbyaddress will return the number of confirmations by
default, and every application will then check the confirmations value to
confirm that it exceeds that application's own omega, while
getbalance,getreceivedbyaddress will take a number of confirmations as an
argument, shielding the app from reorgs of the suffix. That is precisely
the point made in the post.

> Not to mention that your definition ignores the nonzero probability of a
> block rearrangement extending beyond your constant omega.
>
The post covers this case. Technically, there is a difference between 0
probability and epsilon probability -- this is the reason why Nakamoto
Consensus was an exciting breakthrough result; the same reason why
Lamport's results regarding a 3f+1 bound on the Byzantine Generals Problem
do not apply to Nakamoto Consensus; and the same reason it took our paper
(Majority is Not Enough) to show that Nakamoto consensus has a similar 33%
bound as Lamport-style consensus when it comes to tolerating Byzantine
actors.

Practically, however, there is little difference between 0 and a value that
exponentially approximates 0, given that we operate on hardware subject to
random errors. The post makes the case that one can pick an omega such that
the probability of your processor mis-executing your code is larger than
the probability of observing a reorganization.

Bitcoin provides a probabilistic, accumulative probability. Not a perfect
> one.
>
Sometimes, non-technical people get confused about the difference between
very very very small probabilities that approximate 0 and 0. For instance,
some people get very worried about hash collisions, on which Bitcoin relies
for its correctness, whose probability also drops exponentially but is not
exactly 0. Your overall point seems to be an analogous concern that
Bitcoin's exponentially dropping probability of reorganization isn't quite
a "perfect" 0. If so, I agree and the original post made this quite clear.
Though I hope we can avoid that kind of discussion on this particular list.

- egs

-------------------------------------
On Friday, 23 September 2016 13:42:36 CEST Christian Decker via bitcoin-dev wrote:
> > I have to disagree. That is not malleability. Creating a new document
> > and re- signing it is not changing anything. Its re-creating.
> > Something that the owner of the coin has every right to do.
> Same thing I was arguing back then, however Luke pointed out that
> malleability just refers to the possibility of modifying a transaction
> after the fact.

I am not a fan of redefining dictionary words. I'll stick to the 
universally excepted one, thanks.

> Nope, that is exactly the kind of dependency I was talking
> about. Instead of nesting a construct like the current transactions
> do, you rely on the order of tokens to imply that they belong
> together.


> if we
> add new fields that a non-upgraded node doesn't know about and it
> rejects transactions containing it, we'll have a hard-fork. It should
> probably not reject transactions with unknown fields if the
> transaction is included in a block.

This is addressed here;
https://github.com/bitcoin/bips/blob/master/bip-0134.mediawiki#future-extensibility



-------------------------------------
Hi Tom

>> The encryption should be optional.
>> The proposed authentication scheme does take care of the
>> identity-management and therefor prevent MITM attacks.
>> Without the identity management, you might not detect sending/receiving
>> encrypted data from/to a MITM.
> 
> If you want to extend the Bitcoin protocol itself, you will have to resolve 
> that. Which many other solutions do (ssh for instance).

Please check the newest auth BIP (it solves MITM).

The encryption BIP itself does not cover peer authentication.
Encryption without authentication of peers can also be valuable.


>>> * What is the reason for using the p2p code to connect a wallet to a node?
>>> I suggest using one of the other connection methods to connect to the
>>> node.
>>> This avoids a change in the bitcoin protocol for a very specific usecase.
>>
>> Most known use-case: SPV.
> 
> You didn't answer the question.

I hope you see the today's problem with SPV.
You fully reveal to your ISP / WiFi provider most of your wallet
controlled addresses (when using BF). The ISP/WiFi provider can link
your bitcoin usage to other inet traffic and/or they could sell
information to statistics company like google.

Also, an attacker controlling a WiFi router or any other network peer
between your SPV node and the remote full node could censorship
transactions.

Etc. etc.

An encrypted channel together with a trusted full node would finally
allow to have a secure and save SPV communication.


>>> * Why do you want to do a per-message encryption (wrapping the original)?
>>> Smaller messages that contain predictable content and are able to be
>>> matched to the unencrypted versions on the wire send to other nodes will
>>> open this scheme up to various old statistical attacks.
>>
>> It's probably extremely inefficient to create a constant time stream.
> 
> Your use of "probably" makes me wonder if you already have an implementation. 
> Doing any encryption and handshaking design *without* actually having it coded 
> and gone though testing yet makes no sense.
> I do not belief Bitcoin will benefit from "design by committee" where a 
> specification is drawn before an implementation is written.
> 
> Also, you didn't actually address the attack-vector.

Which attack-vector? MITM? Is conceptual solved with the auth BIP (that
requires encryption).

There is no implementation done yet.
It would be a waste of time to start writing a such implementation
_before_ having this discusses and improved by the community.

But the encryption BIP now recommends Chacha20-Polay1305 as AEAD which
is widely used.

I'm ready to write an implementation as soon as I have some signs that
the BIP does make sense.

Also, auth and enc is not something we will have in the next couple of
weeks. This might require a couple of months until its stable and ready
for production.

> 
>  
>>>> Responding peers must ignore (banning would lead to fingerprinting) the
>>>
>>> requesting peer after 5 unsuccessfully authentication tries to avoid
>>> resource attacks.
>>>
>>> Any implementation of that kind would itself again be open to resource
>>> attacks.
>>> Why 5? Do you want to allow a node to make a typo?
>>
>> Good point. Maybe one false try should lead to ignoring the peer.
> 
> That doesn't take away the resource attack at all.
> 
>  
>>>> To ensure that no message was dropped or blocked, the complete
>>>> communication> 
>>> must be hashed (sha256). Both peers keep the SHA256 context of the
>>> encryption session. The complete <code>enc</code> message (leaving out
>>> the hash itself) must be added to the hash-context by both parties.
>>> Before sending a <code>enc</code> command, the sha256 context will be
>>> copied and finalized.
>>>
>>> You write "the complete communication must be hashed" and every message
>>> has a hash of the state until it is at that point.
>>> I think you need to explain how that works specifically.
>>
>> This is a relative simple concept and does not require rehashing the
>> whole communication. 
> 
> Apologies, I should have been more clear; the BIP should specify the actual 
> algorithm, otherwise you can't create an implementation from just reading the 
> BIP.

The sha256 context is gone now and replaced by a proper MAC.

> 
> Also, this may be a good time to ask why you want to have a per-message 
> encryption?
> Practically every single popular end-to-end encryption uses one approach or 
> another were it just encrypts as another layer. (the  L in ssl). You are 
> mixing layers, and unless you do that for a very good reason, or have a very 
> good reason why everyone else is doing it wrong, I suggest using a layered 
> encryption approach.

Like most other encryption layers, we would still use messages. But we
call them "encrypted messages", the have a tiny header of plaintext data
(message length, AEAD-tag) and they will contain <n> plaintext p2p
messages _after_ decrypting. The plaintext messages have a much simpler
header (removed the 4 bytes sha256 checksum, removed the 4byte network)

</jonas>


-------------------------------------
On Tue, Sep 27, 2016 at 11:51:40AM +0200, Tom via bitcoin-dev wrote:
> On Monday 26 Sep 2016 14:41:36 Peter Todd via bitcoin-dev wrote:
> > Note how the OPL is significantly more restrictive than the Bitcoin Core
> > license; not good if we can't ship documentation with the code.
> 
> Documentation and code can have different licenses, the sole existence of 
> various documentation licenses attests to that point.
> Shipping your docs under a separate licence has never been a problem before, 
> so you don't have to worry that you can't ship documentation with code.

The issue isn't that the licenses are different, it's that the OPL is
significantly more restrictive (with the additional clauses that you opted
into).

Indeed, using a different license for documentation is common advise, although
if the documentation also includes example code you may want to dual-license
the documentation with a code-oriented license as well if the documentation
license isn't maximally permissive.

> That said, I wrote my suggestion in reply to Luke's BIP2 revival which is a 
> more formal suggestion of a solution. Maybe you can ACK that one instead?
>
> Last, in preparation of acceptance of BIP2 I changed the licence of my BIP to 
> be dual-licensed.  Now its also available under a Creative Commons license.

Thanks, CC-BY-SA is a perfectly good license for that purpose.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Hi-

One concern is that this doesn't seem compatible with Lightning as
currently written.  Most relevant is that non-cooperative channel close
transactions in Lightning use OP_CHECKSEQUENCEVERIFY, which references the
sequence field of the txin; if the txin doesn't have a sequence number,
OP_CHECKSEQUENCEVERIFY can't work.

LockByBlock and LockByTime aren't described and there doesn't seem to be
code for them in the PR (186).  If there's a way to make OP_CLTV and OP_CSV
work with this new format, please let us know, thanks!

-Tadge

-------------------------------------
Hi,

Recently Bitcoin Core merged a simplification to the consensus rules
surrounding deployment of BIPs 34, 66, and 65 (
https://github.com/bitcoin/bitcoin/pull/8391), and though the change is a
minor one, I thought it was worth documenting the rationale in a BIP for
posterity.

Here's the abstract:

Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated via miner
signaling in block version numbers. Now that the chain has long since
passed the blocks at which those consensus rules have triggered, we can (as
a simplification and optimization) replace the trigger mechanism by caching
the block heights at which those consensus rules became enforced.

The full draft can be found here:

https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki

-------------------------------------
Hi Peter,

What in this BIP makes a MITM attack easier (or easy) to detect, or increases the probability of one being detected?

e

> On Jun 28, 2016, at 8:22 PM, Peter Todd <pete@petertodd.org> wrote:
> 
> On Tue, Jun 28, 2016 at 06:45:58PM +0200, Eric Voskuil via bitcoin-dev wrote:
>>> 1) Transaction censorship
>>> ISPs, WIFI provider or any other MITM, can holdback/censor unconfirmed
>>> transactions. Regardless if you are a miner or a validation/wallet node.
>>> 
>>> 2) Peer censorship
>>> MITM can remove or add entries from a "addr" message.
>>> 
>>> 3) Fingerprinting
>>> ISPs or any other MITM can intercept/inject fingerprinting relevant
>>> messages like "mempool" to analyze the bitcoin network.
>> 
>> Encryption alone cannot protect against a MITM attack in an anonymous and permissionless network. This is accepted in the BIP (and your follow-up reply).
> 
> Being able to easily detect MITM attacks is a _huge_ step forward that
> shouldn't be underestimated; even if 99% of users aren't in a position to
> detect the MITM you only need a small subset of users that do the necessary
> checks to alert the wider community, who can then respond with stronger
> security measures. Those measures are likely to be more costly - authenticated
> systems are significantly harder than not - so better to save your efforts
> until the need for them is more obvious.
> 
> Also the fact that an attack has a reasonable probability of detection is a big
> disincentive for many types of attackers - note how one of the things revealed
> in the Snowden leaks was the fact that the NSA generally tries quite hard to
> avoid tipping off targets to the fact that they're being surveilled, with a
> myriad of carefully scripted policies to control when and how exploits are used
> against targets.
> 
> -- 
> https://petertodd.org 'peter'[:-1]@petertodd.org


-------------------------------------
On Mon, Feb 1, 2016 at 2:46 PM, Luke Dashjr <luke@dashjr.org> wrote:
> On Monday, February 01, 2016 6:41:06 PM Cory Fields wrote:
>> Noticeably absent here is the "default_witness_commitment" key, as
>> added by the current reference implementation[0].
>>
>> I assume (please correct me if I'm wrong) that this has been omitted
>> for the sake of having clients create the commitment themselves as
>> opposed to having it provided to them.
>>
>> I don't think that the two approaches (providing the default
>> commitment for the complete tx set as well as the ability to create it
>> from chosen transactions) are at odds with each-other, rather it
>> merely allows for a simpler approach for those who are taking tx's
>> as-is from bitcoind. It's obviously important for the clients to be
>> able to chose tx's and create commitments as they desire, but it's
>> equally important to allow for simpler use-cases.
>
> Allowing for simpler cases both encourages the lazy case, and enables pools to
> require miners use it. It also complicates the server-side implementation
> somewhat, and could in some cases make it more vulnerable to DoS attacks. Keep
> in mind that GBT is not merely a bitcoind protocol, but is used between
> pool<->miner as well... For now, it makes sense to leave
> "default_witness_commitment" as a bitcoind-specific extension to encourage
> adoption, but it seems better to leave it out of the standard protocol. Let me
> know if this makes sense or if I'm overlooking something.
>

I think that's a bit of a loaded answer. What's to keep a pool from
building its own commitment and requiring miners to use that? I don't
see how providing the known-working commitment for the
passed-in-hashes allows the pool/miner to do anything they couldn't
already, with the exception of skipping some complexity. Please don't
confuse encouraging with enabling.

What's the DoS vector here?

>> The issue in particular here is that a non-trivial burden is thrust
>> upon mining software, increasing the odds of bugs in the process.
>
> It can always use libblkmaker to handle the "heavy lifting"... In any case,
> the calculation for the commitment isn't significantly more than what it must
> already do for the stripped merkle tree.

Agreed. However for the sake of initial adoption, it's much easier to
have a known-correct value to use. Even if it's just for the sake of
checking against.

>
>> I'd like to point out that this is not a theoretical argument. I've
>> already fixed a handful of bugs relating to serialization or
>> commitment creation in the mining/pool software that I've worked on
>> for segwit [1][2][3][4].
>
> That's not really fair IMO. I wrote the libblkmaker branch prior to even
> reading the SegWit BIPs or code, and without a way to test it. It's only to be
> expected there are bugs that get fixed in first-try testing.

I didn't mean this as an insult/attack, quite the opposite actually.
Thanks for doing the integration :)

I was merely pointing out how easy it is to introduce subtle bugs here.

>
>> [4]:
>> https://github.com/theuni/ckpool/commit/7d84b1d76b39591cc1c1ef495ebec513cb
>> 19a08e
>
> I'm pretty sure this commit is actually /introducing/ a bug in working (albeit
> ugly) code. The height, while always positive, is serialised as a signed
> number, so 0x80 needs to be two bytes: 80 00.

You're right, thanks. The current code breaks on heights of (for ex)
16513. I'll fix up my changes to take the sign bit into account.

Heh, that only reinforces my point above about introducing bugs :p

>
> Luke


-------------------------------------
On Mon, Feb 29, 2016 at 10:58 AM, Mats Jerratsch <matsjj@gmail.com> wrote:

> This is actually very useful for LN too, see relevant discussion here
>
>
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011827.html
>

Is there much demand for trying to code up a patch to the reference
client?  I did a basic one, but it would need tests etc. added.

I think that segregated witness is going to be using up any potential
soft-fork slot for the time being anyway.

-------------------------------------
> Yes. I think this limitation could be removed.
> A responding node can have  in theory  multiple identity-keys per
> network interface (network interfaces is also confusing, because you
> could run multiple bitcoind instances on the same interface with
> different ports).
> 
> The BIP should just make clear, that it is probably wise, to use
> different identity-keys for each network interface (ipv4, v6, tor).
> 

I have updated that part of the BIP

-----------
Each peer can configure multiple identity-keys (ECC, 32 bytes). Peers
should make sure, each network interface (IPv4, IPv6, tor) has its own
identity-key (otherwise it would be possible to link a tor address to a
IPvX address).
The identity-public-key(s) can be shared over a different channel with
other node-operators (or non-validating clients) to grant authorized access.
-----------

https://github.com/bitcoin/bips/compare/master...jonasschnelli:2016/07/auth_bip?expand=1

</jonas>


-------------------------------------
This start time seems reasonable to me. It is mostly in line with BIP 9's proposed defaults, which seems like an appropriate choice.

On October 16, 2016 10:31:55 AM EDT, Pieter Wuille via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>Hello all,
>
>We're getting ready for Bitcoin Core's 0.13.1 release - the first one
>to include segregated witness (BIP 141, 143, 144, 145) for Bitcoin
>mainnet, after being extensively tested on testnet and in other
>software. Following the BIP9 recommendation [1] to set the versionbits
>start time a month in the future and discussion in the last IRC
>meeting [2], I propose we set BIP 141's start time to November 15,
>2016, 0:00 UTC (unix time 1479168000).
>
>Note that this is just a lower bound on the time when the versionbits
>signalling can begin. Activation on the network requires:
>(1) This date to pass
>(2) A full retarget window of 2016 blocks with 95% signalling the
>version bit (bit 1 for BIP141)
>(3) A fallow period consisting of another 2016 blocks.
>
>  [1] https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki
>[2]
>http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.2016-10-13-19.04.log.html
>
>Cheers,



-------------------------------------
On Sun, May 8, 2016 at 10:07 AM, Pavol Rusnak via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On 21/04/16 14:08, Marek Palatinus via bitcoin-dev wrote:
>> Sipa, you are probably the most competent to answer this.
>> Could you please tell us your opinion? For me, this is
>> straightforward, backward compatible fix and I like it a lot.
>> Not sure about the process of changing "Final" BIP though.
>
> Sipa: Marek told me you posted your answer and he received it, but it
> never reached the list. Could you please resend after figuring out what
> went wrong?

AFAIK Sipa has not been on this list for some time.


-------------------------------------
Also published at https://petertodd.org/2016/defensive-patent-license-offer,
and Bitcoin txid b4bf94f5c457d080924aa163106d423670373cfe3b10f8ec00742c2234b01b72

    -----BEGIN PGP SIGNED MESSAGE-----
    Hash: SHA256

    I, Peter Todd, hereby declare myself and all technology companies that I
    control as "Defensive" by committing to offer a Defensive Patent License,
    version 1.1, for any of my patents either existing or future, to any DPL User.
    Neither I nor any companies that I control have any patents at this time.

    My contact address is pete@petertodd.org

    -----BEGIN PGP SIGNATURE-----

    iQEcBAEBCAAGBQJX/t11AAoJEGOZARBE6K+yR00H/0xp3oO7FiMvM4pjfoHZPPOa
    m3KjT4RSbFQLa9uniz0u/9bkc5I70CggkY3jtNLtDMbMBTwcMP61ABsvx+5y2gGD
    zE6VZ9DPcHVg/Eup6WSBlQO3HQKuFVz7vXSMuaidG7A+fpkU71SjDpB4M6hdvWnS
    +L9XBQ1GtQe0lSM73s4mld/IvB1giwPN1bOheQ9koYcQjj+B8PWyt2gIUwctxyvA
    7bC+KtCQT4RJPsQHbHx569CDkyIi3dNt0rTjCo5bOeUKrJF7eA3YktYdTJefZ+Rf
    00dbRZMslrg3dW9VWECfC0xC/kn+heStJ7WqJJKqYWo4apm6IiKPZxlwIcVscF0=
    =xrPk
    -----END PGP SIGNATURE-----

# Notes

* On the advice of my lawyer, I'm currently offering only a specific version of
  the DPL. I probably will offer licenses under subsequent versions in the
  future, but I'd prefer to see how the DPL evolves and whether or not the
  license stewards behind it prove trustworthy before committing to doing so.

* The language "all technology companies I control" is there to avoid any
  complications with non-technology companies that I may control in the future,
  e.g. family real-estate holding companies, and the non-profit caving group
  I'm a part of. To my knowledge, I only control one company as of writing, the
  numbered company I do all my consulting through; I consider that company a
  "technology company", and thus the above offer applies to it.

* Equally, if by some stroke of luck I do end up in control of any other
  technology companies - maybe Bill Gate's blockchain smart-contract will
  mysteriously gives me control of Microsoft - then the above offer will apply.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Aside from patents related to the silicon manufacturing process itself and patents not yet published, yes, the process is unencumbered, and setting the correct precedent (that the community will fight large centralization risks) is important in the first case.

Matt

On May 11, 2016 9:23:21 PM EDT, Russell O'Connor via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>Is the design and manufacturing processes for the most power efficient
>ASICs otherwise patent unencumbered?  If not, why do we care so much
>about
>this one patent over all the others that stand on the road between pen
>and
>paper computation and thermodynamically ideal computation?
>
>On Wed, May 11, 2016 at 8:02 PM, Gregory Maxwell via bitcoin-dev <
>bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Wed, May 11, 2016 at 11:01 PM, Peter Todd via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > Secondly, we can probably make the consensus PoW allow blocks to be
>> mined using
>> > both the existing PoW algorithm, and a very slightly tweaked
>version
>> where
>> > implementing AsicBoost gives no advantage. That removes any
>incentive to
>> > implement AsicBoost, without making any hardware obsolete
>>
>> Taking that a step further, the old POW could continue to be accepted
>> but with a 20% target penalty. (or vice versa, with the new POW
>having
>> a 20% target boost.)
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev@lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
Hi all,

Thanks again Jonas for starting this!

I worked on a similar proposal a while back (never posted), approaching
the same problem as if a merchant's website accepted xpubs/public keys,
created multi-signature addresses, and wanted the user to easily sign
offline instead of using some javascript code / using Core's debug
console / coinb.in

Happily the procedure is largely the same, though I would echo Jochen's
point that there needs to be a way to request an xpub/public key.

The redeemScript and witnessScript are also required fields for full
validation & signing a transaction input if it's P2SH, or just the
witnessScript if it's bare V0_P2WSH

Since the output amounts are required, so maybe instead provide
serialized TxOut's? or Utxo's i.e: [txid, vout, amount, scriptPubKey].

The protocol ought to be as stateless as possible - it can't be assumed
whether the redeemScript and other details will ever be saved on the
device - so perhaps provide the redeemScript + witnessScript as the
final fields on the Utxo structure above.

I do think it enables two important choices for bitcoin users:

* it might be preferable to provide your own xpub vs generating a brand
new HD key to potentially lose.

* you could leverage the services provided by [random example]
GreenAddress without necessarily having to rely on signing code provided
by them, and so end up only having to trust only one ECDSA
implementation when interacting with a wide number of services

All the best

Thomas

On 08/16/2016 06:48 PM, Jochen Hoenicke via bitcoin-dev wrote:
> Hello Jonas,
>
> thanks for your efforts of writing the draft for the standard.
>
> First, this only describes detached signing.  A wallet also needs to
> connect with a hardware wallet at some time to learn the xpubs
> controlled by the hardware.  Do you plan to have this in a separate
> standard or should this also be included here?  Basically one needs one
> operation: get xpub for an HD path.
>
> From a first read over the specification I found the following points
> missing, that a fully checking hardware wallet needs to know:
>
> - the amount spent by each input (necessary for segwit).
> - the full serialized input transactions (without witness informations)
> to prove that the amount really matches (this is not necessary for segwit)
> - the position of the change output and its HD Path (to verify that it
> really is a change output).
> - For multisig change addresses, there are more extensive checks
> necessary:  All inputs must be multisig addresses signed with public
> keys derived from the same set of xpubs as the change address and use
> the same "m of n" scheme.  So for multisig inputs and multisig change
> address the standard should allow to give the parent xpubs of the other
> public keys and their derivation paths.
>
> It is also a bit ambiguous what the "inputscript" is especially for p2sh
> transactions.  Is this always the scriptPubKey of the transaction output
> that is spent by this input? For p2wsh nested in BIP16 p2sh transactions
> there are three scripts
>
>     witness:      0 <signature1> <1 <pubkey1> <pubkey2> 2 CHECKMULTISIG>
>     scriptSig:    <0 <32-byte-hash>>
>                   (0x220020{32-byte-hash})
>     scriptPubKey: HASH160 <20-byte-hash> EQUAL
>                   (0xA914{20-byte-hash}87)
>  (quoted from BIP-141).
>
> In principle one could put witness and scriptSig (with "OP_FALSE" in
> places of the signatures) in the raw transaction and make inputscript
> always the scriptPubKey of the corresponding output.  Then one also
> doesn't need to distinguish between p2pkh or p2sh or p2wpkh or "p2wpkh
> nested in bip16 p2sh" transactions.
>
> Regards,
>   Jochen
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


On 08/16/2016 06:48 PM, Jochen Hoenicke via bitcoin-dev wrote:
> Hello Jonas,
>
> thanks for your efforts of writing the draft for the standard.
>
> First, this only describes detached signing.  A wallet also needs to
> connect with a hardware wallet at some time to learn the xpubs
> controlled by the hardware.  Do you plan to have this in a separate
> standard or should this also be included here?  Basically one needs one
> operation: get xpub for an HD path.
>
> From a first read over the specification I found the following points
> missing, that a fully checking hardware wallet needs to know:
>
> - the amount spent by each input (necessary for segwit).
> - the full serialized input transactions (without witness informations)
> to prove that the amount really matches (this is not necessary for segwit)
> - the position of the change output and its HD Path (to verify that it
> really is a change output).
> - For multisig change addresses, there are more extensive checks
> necessary:  All inputs must be multisig addresses signed with public
> keys derived from the same set of xpubs as the change address and use
> the same "m of n" scheme.  So for multisig inputs and multisig change
> address the standard should allow to give the parent xpubs of the other
> public keys and their derivation paths.
>
> It is also a bit ambiguous what the "inputscript" is especially for p2sh
> transactions.  Is this always the scriptPubKey of the transaction output
> that is spent by this input? For p2wsh nested in BIP16 p2sh transactions
> there are three scripts
>
>     witness:      0 <signature1> <1 <pubkey1> <pubkey2> 2 CHECKMULTISIG>
>     scriptSig:    <0 <32-byte-hash>>
>                   (0x220020{32-byte-hash})
>     scriptPubKey: HASH160 <20-byte-hash> EQUAL
>                   (0xA914{20-byte-hash}87)
>  (quoted from BIP-141).
>
> In principle one could put witness and scriptSig (with "OP_FALSE" in
> places of the signatures) in the raw transaction and make inputscript
> always the scriptPubKey of the corresponding output.  Then one also
> doesn't need to distinguish between p2pkh or p2sh or p2wpkh or "p2wpkh
> nested in bip16 p2sh" transactions.
>
> Regards,
>   Jochen
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
Bitcoin could embed a lisp interpreter such as Scheme, reverse engineer
the current protocol into lisp (inside C++), run this alternative engine
alongside the current one as an option for some years (only for fine
tuning) then eventually fade this lisp written validation code instead
of the current one.

Scheme is small and minimal, and embeds easily in C++. This could be a
better option than the libconsensus library - validation in a functional
scripting language.

That doesn't mean people can't program the validation code in other
languages (maybe they'd want to optimize), but this code would be the
standard.

It's really good how you are thinking deeply how Bitcoin can be used,
and the implications of everything. Also there's a lot of magical utopic
thinking in Ethereum, which is transhumanist nonsense that is life
denying. Bitcoin really speaks to me because it is real and a great tool
following the UNIX principle.

I wouldn't be so quick to deride good engineering over systematic
provable systems for all domains. Bitcoin being written in C++ is not a
defect. It's actually a strong language for what it does. Especially
when used correctly (which is not often and takes years to master).

With the seals idea- am I understand this correctly?: Every transaction
has a number (essentially the index starting from 0 upwards) depending
on where it is in the blockchain.

Then there is an array (probably an on disk array mapping transaction
indexes to hashes). Each hash entry in the array must be unique (the
hashes) otherwise the transaction will be denied. This is a great idea
to solve transaction hash collisions and simple to implement.

Probabilistic validation is a good idea, although the real difficulty
now seems to be writing and indexing all the blockchain data for
lookups. And validation is disabled for most of the blocks. Pruning is
only a stop gap measure (which loses data) that doesn't solve the issue
of continually growing resource consumption. Hardware and implementation
can only mitigate this so much. If only there was a way to simplify the
underlying protocol to make it more resource efficient...

Peter Todd via bitcoin-dev:
> In light of Ethereum's recent problems with its imperative, account-based,
> programming model, I thought I'd do a quick writeup outlining the building
> blocks of the state-machine approach to so-called "smart contract" systems, an
> extension of Bitcoin's own design that I personally have been developing for a
> number of years now as my Proofchains/Dex research work.
> 
> 
> # Deterministic Code / Deterministic Expressions
> 
> We need to be able to run code on different computers and get identical
> results; without this consensus is impossible and we might as well just use a
> central authoritative database. Traditional languages and surrounding
> frameworks make determinism difficult to achieve, as they tend to be filled
> with undefined and underspecified behavior, ranging from signed integer
> overflow in C/C++ to non-deterministic behavior in databases. While some
> successful systems like Bitcoin are based on such languages, their success is
> attributable to heroic efforts by their developers.
> 
> Deterministic expression systems such as Bitcoin's scripting system and the
> author's Dex project improve on this by allowing expressions to be precisely
> specified by hash digest, and executed against an environment with
> deterministic results. In the case of Bitcoin's script, the expression is a
> Forth-like stack-based program; in Dex the expression takes the form of a
> lambda calculus expression.
> 
> 
> ## Proofs
> 
> So far the most common use for deterministic expressions is to specify
> conditions upon which funds can be spent, as seen in Bitcoin (particularly
> P2SH, and the upcoming Segwit). But we can generalize their use to precisely
> defining consensus protocols in terms of state machines, with each state
> defined in terms of a deterministic expression that must return true for the
> state to have been reached. The data that causes a given expression to return
> true is then a "proof", and that proof can be passed from one party to another
> to prove desired states in the system have been reached.
> 
> An important implication of this model is that we need deterministic, and
> efficient, serialization of proof data.
> 
> 
> ## Pruning
> 
> Often the evaluation of an expression against a proof doesn't require all all
> data in the proof. For example, to prove to a lite client that a given block
> contains a transaction, we only need the merkle path from the transaction to
> the block header. Systems like Proofchains and Dex generalize this process -
> called "pruning" - with built-in support to both keep track of what data is
> accessed by what operations, as well as support in their underlying
> serialization schemes for unneeded data to be elided and replaced by the hash
> digest of the pruned data.
> 
> 
> # Transactions
> 
> A common type of state machine is the transaction. A transaction history is a
> directed acyclic graph of transactions, with one or more genesis transactions
> having no inputs (ancestors), and one or more outputs, and zero or more
> non-genesis transactions with one or more inputs, and zero or more outputs. The
> edges of the graph connect inputs to outputs, with every input connected to
> exactly one output. Outputs with an associated input are known as spent
> outputs; outputs with out an associated input are unspent.
> 
> Outputs have conditions attached to them (e.g. a pubkey for which a valid
> signature must be produced), and may also be associated with other values such
> as "# of coins". We consider a transaction valid if we have a set of proofs,
> one per input, that satisfy the conditions associated with each output.
> Secondly, validity may also require additional constraints to be true, such as
> requiring the coins spent to be >= the coins created on the outputs. Input
> proofs also must uniquely commit to the transaction itself to be secure - if
> they don't the proofs can be reused in a replay attack.
> 
> A non-genesis transaction is valid if:
> 
> 1. Any protocol-specific rules such as coins spent >= coins output are
>    followed.
> 
> 2. For every input a valid proof exists.
> 
> 3. Every input transaction is itself valid.
> 
> A practical implementation of the above for value-transfer systems like Bitcoin
> could use two merkle-sum trees, one for the inputs, and one for the outputs,
> with inputs simply committing to the previous transaction's txid and output #
> (outpoint), and outputs committing to a scriptPubKey and output amount.
> Witnesses can be provided separately, and would sign a signature committing to
> the transaction or optionally, a subset of of inputs and/or outputs (with
> merkle trees we can easily avoid the exponential signature validation problems
> bitcoin currently has).
> 
> As so long as all genesis transactions are unique, and our hash function is
> secure, all transaction outputs can be uniquely identified (prior to BIP34 the
> Bitcoin protocol actually failed at this!).
> 
> 
> ## Proof Distribution
> 
> How does Alice convince Bob that she has done a transaction that puts the
> system into the state that Bob wanted? The obvious answer is she gives Bob data
> proving that the system is now in the desired state; in a transactional system
> that proof is some or all of the transaction history. Systems like Bitcoin
> provide a generic flood-fill messaging layer where all participants have the
> opportunity to get a copy of all proofs in the system, however we can also
> implement more fine grained solutions based on peer-to-peer message passing -
> one could imagine Alice proving to Bob that she transferred title to her house
> to him by giving him a series of proofs, not unlike the same way that property
> title transfer can be demonstrated by providing the buyer with a series of deed
> documents (though note the double-spend problem!).
> 
> 
> # Uniqueness and Single-Use Seals
> 
> In addition to knowing that a given transaction history is valid, we also want
> to know if it's unique. By that we mean that every spent output in the
> transaction history is associated with exactly one input, and no other valid
> spends exist; we want to ensure no output has been double-spent.
> 
> Bitcoin (and pretty much every other cryptocurrency like it) achieves this goal
> by defining a method of achieving consensus over the set of all (valid)
> transactions, and then defining that consensus as valid if and only if no
> output is spent more than once.
> 
> A more general approach is to introduce the idea of a cryptographic Single-Use
> Seal, analogous to the tamper-evidence single-use seals commonly used for
> protecting goods during shipment and storage. Each individual seals is
> associated with a globally unique identifier, and has two states, open and
> closed. A secure seal can be closed exactly once, producing a proof that the
> seal was closed.
> 
> All practical single-use seals will be associated with some kind of condition,
> such as a pubkey, or deterministic expression, that needs to be satisfied for
> the seal to be closed. Secondly, the contents of the proof will be able to
> commit to new data, such as the transaction spending the output associated with
> the seal.
> 
> Additionally some implementations of single-use seals may be able to also
> generate a proof that a seal was _not_ closed as of a certain
> time/block-height/etc.
> 
> 
> ## Implementations
> 
> ### Transactional Blockchains
> 
> A transaction output on a system like Bitcoin can be used as a single-use seal.
> In this implementation, the outpoint (txid:vout #) is the seal's identifier,
> the authorization mechanism is the scriptPubKey of the output, and the proof
> is the transaction spending the output. The proof can commit to additional
> data as needed in a variety of ways, such as an OP_RETURN output, or
> unspendable output.
> 
> This implementation approach is resistant to miner censorship if the seal's
> identifier isn't made public, and the protocol (optionally) allows for the
> proof transaction to commit to the sealed contents with unspendable outputs;
> unspendable outputs can't be distinguished from transactions that move funds.
> 
> 
> ### Unbounded Oracles
> 
> A trusted oracle P can maintain a set of closed seals, and produce signed
> messages attesting to the fact that a seal was closed. Specifically, the seal
> is identified by the tuple (P, q), with q being the per-seal authorization
> expression that must be satisfied for the seal to be closed. The first time the
> oracle is given a valid signature for the seal, it adds that signature and seal
> ID to its closed seal set, and makes available a signed message attesting to
> the fact that the seal has been closed. The proof is that message (and
> possibly the signature, or a second message signed by it).
> 
> The oracle can publish the set of all closed seals for transparency/auditing
> purposes. A good way to do this is to make a merkelized key:value set, with the
> seal identifiers as keys, and the value being the proofs, and in turn create a
> signed certificate transparency log of that set over time. Merkle-paths from
> this log can also serve as the closed seal proof, and for that matter, as
> proof of the fact that a seal has not been closed.
> 
> 
> ### Bounded Oracles
> 
> The above has the problem of unbounded storage requirements as the closed seal
> set grows without bound. We can fix that problem by requiring users of the
> oracle to allocate seals in advance, analogous to the UTXO set in Bitcoin.
> 
> To allocate a seal the user provides the oracle P with the authorization
> expression q. The oracle then generates a nonce n and adds (q,n) to the set of
> unclosed seals, and tells the user that nonce. The seal is then uniquely
> identified by (P, q, n)
> 
> To close a seal, the user provides the oracle with a valid signature over (P,
> q, n). If the open seal set contains that seal, the seal is removed from the
> set and the oracle provides the user with a signed message attesting to the
> valid close.
> 
> A practical implementation would be to have the oracle publish a transparency
> log, with each entry in the log committing to the set of all open seals with a
> merkle set, as well as any seals closed during that entry. Again, merkle paths
> for this log can serve as proofs to the open or closed state of a seal.
> 
> Note how with (U)TXO commitments, Bitcoin itself is a bounded oracle
> implementation that can produce compact proofs.
> 
> 
> ### Group Seals
> 
> Multiple seals can be combined into one, by having the open seal commit to a
> set of sub-seals, and then closing the seal over a second set of closed seal
> proofs. Seals that didn't need to be closed can be closed over a special
> re-delegation message, re-delegating the seal to a new open seal.
> 
> Since the closed sub-seal proof can additionally include a proof of
> authorization, we have a protcol where the entity with authorization to close
> the master seal has the ability to DoS attack sub-seals owners, but not the
> ability to fraudulently close the seals over contents of their choosing. This
> may be useful in cases where actions on the master seal is expensive - such as
> seals implemented on top of decentralized blockchains - by amortising the cost
> over all sub-seals.
> 
> 
> ## Atomicity
> 
> Often protocols will require multiple seals to be closed for a transaction to
> be valid. If a single entity controls all seals, this is no problem: the
> transaction simply isn't valid until the last seal is closed.
> 
> However if multiple parties control the seals, a party could attack another
> party by failing to go through with the transaction, after another party has
> closed their seal, leaving the victim with an invalid transaction that they
> can't reverse.
> 
> We have a few options to resolve this problem:
> 
> ### Use a single oracle
> 
> The oracle can additionally guarantee that a seal will be closed iff some other
> set of seals are also closed; seals implemented with Bitcoin can provide this
> guarantee. If the parties to a transaction aren't already all on the same
> oracle, they can add an additional transaction reassigning their outputs to a
> common oracle.
> 
> Equally, a temporary consensus between multiple mutually trusting oracles can
> be created with a consensus protocol they share; this option doesn't need to
> change the proof verification implementation.
> 
> 
> ### Two-phase Timeouts
> 
> If a proof to the fact that a seal is open can be generated, even under
> adversarial conditions, we can make the seal protocol allow a close to be
> undone after a timeout if evidence can be provided that the other seal(s) were
> not also closed (in the specified way).
> 
> Depending on the implementation - especially in decentralized systems - the
> next time the seal is closed, the proof it has been closed may in turn provide
> proof that a previous close was in fact invalid.
> 
> 
> # Proof-of-Publication and Proof-of-Non-Publication
> 
> Often we need to be able to prove that a specified audience was able to receive
> a specific message. For example, the author's PayPub protocol[^paypub],
> Todd/Taaki's timelock encryption protocol[^timelock], Zero-Knowledge Contingent
> Payments[^zkcp], and Lightning, among others work by requiring a secret key to
> be published publicly in the Bitcoin blockchain as a condition of collecting a
> payment. At a much smaller scale - in terms of audience - in certain FinTech
> applications for regulated environments a transaction may be considered invalid
> unless it was provably published to a regulatory agency.  Another example is
> Certificate Transparency, where we consider a SSL certificate to be invalid
> unless it has been provably published to a transparency log maintained by a
> third-party.
> 
> Secondly, many proof-of-publication schemes also can prove that a message was
> _not_ published to a specific audience. With this type of proof single-use
> seals can be implemented, by having the proof consist of proof that a specified
> message was not published between the time the seal was created, and the time
> it was closed (a proof-of-publication of the message).
> 
> ## Implementations
> 
> ### Decentralized Blockchains
> 
> Here the audience is all participants in the system. However miner censorship
> can be a problem, and compact proofs of non-publication aren't yet available
> (requires (U)TXO commitments).
> 
> The authors treechains proposal is a particularly generic and scalable
> implementation, with the ability to make trade offs between the size of
> audience (security) and publication cost.
> 
> ### Centralized Public Logs
> 
> Certificate Transparency works this way, with trusted (but auditable) logs run
> by well known parties acting as the publication medium, who promise to allow
> anyone to obtain copies of the logs.
> 
> The logs themselves may be indexed in a variety of ways; CT simply indexes logs
> by time, however more efficient schemes are possible by having the operator
> commit to a key:value mapping of "topics", to allow publication (and
> non-publication) proofs to be created for specified topics or topic prefixes.
> 
> Auditing the logs is done by verifying that queries to the state of the log
> return the same state at the same time for different requesters.
> 
> ### Receipt Oracles
> 
> Finally publication can be proven by a receipt proof by the oracle, attesting
> to the fact that the oracle has successfully received the message. This is
> particularly appropriate in cases where the required audience is the oracle
> itself, as in the FinTech regulator case.
> 
> 
> # Validity Oracles
> 
> As transaction histories grow longer, they may become impractical to move from
> one party to another. Validity oracles can solve this problem by attesting to
> the validity of transactions, allowing history prior to the attested
> transactions to be discarded.
> 
> A particularly generic validity oracle can be created using deterministic
> expressions systems. The user gives the oracle an expression, and the oracle
> returns a signed message attesting to the validity of the expression.
> Optionally, the expression may be incomplete, with parts of the expression
> replaced by previously generated attestations. For example, an expression that
> returns true if a transaction is valid could in turn depend on the previous
> transaction also being valid - a recursive call of itself - and that recursive
> call can be proven with a prior attestation.
> 
> ## Implementations
> 
> ### Proof-of-Work Decentralized Consensus
> 
> Miners in decentralized consensus systems act as a type of validity oracle, in
> that the economic incentives in the system are (supposed to be) designed to
> encourage only the mining of valid blocks; a user who trusts the majority of
> hashing power can trust that any transaction with a valid merkle path to a
> block header in the most-work chain is valid. Existing decentralized consensus
> systems like Bitcoin and Ethereum conflate the roles of validity oracle and
> single-use seal/anti-replay oracle, however in principle that need not be true.
> 
> 
> ### Trusted Oracles
> 
> As the name suggests. Remote-attestation-capable trusted hardware is a
> particularly powerful implementation - a conspiracy theory is that the reason
> why essentially zero secure true remote attestation implementations exist is
> because they'd immediately make untraceable digital currency systems easy to
> implement (Finney's RPOW[^rpow] is a rare counter-example).
> 
> Note how a single-use seal oracle that supports a generic deterministic
> expressions scheme for seal authorization can be easily extended to provide a
> validity oracle service as well. The auditing mechanisms for a single-use seal
> oracle can also be applied to validity oracles.
> 
> 
> # Fraud Proofs
> 
> Protocols specified with deterministic expressions can easily generate "fraud
> proofs", showing that claimed states/proof in the system are actually invalid.
> Additionally many protocols can be specified with expressions of k*log2(n)
> depth, allowing these fraud proofs to be compact.
> 
> A simple example is proving fraud in merkle-sum tree, where the validity
> expression would be something like:
> 
>     (defun valid? (node)
>         (or (== node.type leaf)
>             (and (== node.sum (+ node.left.sum node.right.sum))
>                  (and (valid? node.left)
>                       (valid? node.right)))))
> 
> To prove the above expression evaluates to true, we'll need the entire contents
> of the tree. However, to prove that it evaluates to false, we only need a
> subset of the tree as proving an and expression evaluates to false only
> requires one side, and requires log2(n) data. Secondly, with pruning, the
> deterministic expressions evaluator can automatically keep track of exactly
> what data was needed to prove that result, and prune all other data when
> serializing the proof.
> 
> 
> ## Validity Challenges
> 
> However how do you guarantee it will be possible to prove fraud in the first
> place? If pruning is allowed, you may simply not have access to the data
> proving fraud - an especially severe problem in transactional systems where a
> single fraudulent transaction can counterfeit arbitrary amounts of value out of
> thin air.
> 
> A possible approach is the validity challenge: a subset of proof data, with
> part of the data marked as "potentially fraudulent". The challenge can be
> satisfied by providing the marked data and showing that the proof in question
> is in fact valid; if the challenge is unmet participants in the system can
> choose to take action, such as refusing to accept additional transactions.
> 
> Of course, this raises a whole host of so-far unsolved issues, such as DoS
> attacks and lost data.
> 
> 
> # Probabilistic Validation
> 
> Protocols that can tolerate some fraud can make use of probabilistic
> verification techniques to prove that the percentage of undetected fraud within
> the system is less than a certain amount, with a specified probability.
> 
> A common way to do this is the Fiat-Shamir transform, which repeatedly samples
> a data structure deterministically, using the data's own hash digest as a seed
> for a PRNG. Let's apply this technique to our merkle-sum tree example. We'll
> first need a recursive function to check a sample, weighted by value:
> 
>     (defun prefix-valid? (node nonce)
>         (or (== node.type leaf)
>             (and (and (== node.sum (+ node.left.sum node.right.sum))
>                       (> 0 node.sum)) ; mod by 0 is invalid, just like division by zero
>                                       ; also could guarantee this with a type system
>                  (and (if (< node.left.sum (mod nonce node.sum))
>                           (prefix-valid? node.right (hash nonce))
>                           (prefix-valid? node.left (hash nonce)))))))
> 
> Now we can combine multiple invocations of the above, in this case 256
> invocations:
> 
>     (defun prob-valid? (node)
>         (and (and (and .... (prefix-valid? node (digest (cons (digest node) 0)))
>              (and (and ....
>                             (prefix-valid? node (digest (cons (digest node) 255)))
> 
> As an exercise for a reader: generalize the above with a macro, or a suitable
> types/generics system.
> 
> If we assume our attacker can grind up to 128 bits, that leaves us with 128
> random samples that they can't control. If the (value weighted) probability of
> a given node is fraudulent q, then the chance of the attacker getting away with
> fraud is (1-q)^128 - for q=5% that works out to 0.1%
> 
> (Note that the above analysis isn't particularly well done - do a better
> analysis before implementing this in production!)
> 
> 
> ## Random Beacons and Transaction History Linearization
> 
> The Fiat-Shamir transform requires a significant number of samples to defeat
> grinding attacks; if we have a random beacon available we can significantly
> reduce the size of our probabilistic proofs. PoW blockchains can themselves act
> as random beacons, as it is provably expensive for miners to manipulate the
> hash digests of blocks they produce - to do so requires discarding otherwise
> valid blocks.
> 
> An example where this capability is essential is the author's transaction
> history linearization technique. In value transfer systems such as Bitcoin, the
> history of any given coin grows quasi-exponentially as coins are mixed across
> the entire economy. We can linearize the growth of history proofs by redefining
> coin validity to be probabilistic.
> 
> Suppose we have a transaction with n inputs. Of those inputs, the total value
> of real inputs is p, and the total claimed value of fake inputs is q. The
> transaction commits to all inputs in a merkle sum tree, and we define the
> transaction as valid if a randomly chosen input - weighted by value - can
> itself be proven valid. Finally, we assume that creating a genuine input is a
> irrevocable action which irrevocable commits to the set of all inputs, real and
> fake.
> 
> If all inputs are real, 100% of the time the transaction will be valid; if all
> inputs are fake, 100% of the time the transaction will be invalid. In the case
> where some inputs are real and some are fake the probability that the fraud
> will be detected is:
> 
>     q / (q + p)
> 
> The expected value of the fake inputs is then the sum of the potential upside -
> the fraud goes detected - and the potential downside - the fraud is detected
> and the real inputs are destroyed:
> 
>     E = q(1 - q/(q + p)) - p(q/(q + p)
>       = q(p/(q + p)) - p(q/(q + p)
>       = (q - q)(p/(q + p))
>       = 0
> 
> Thus so long as the random beacon is truly unpredictable, there's no economic
> advantage to creating fake inputs, and it is sufficient for validity to only
> require one input to be proven, giving us O(n) scaling for transaction history
> proofs.
> 
> 
> ### Inflationary O(1) History Proofs
> 
> We can further improve our transaction history proof scalability by taking
> advantage of inflation. We do this by occasionally allowing a transaction proof
> to be considered valid without validating _any_ of the inputs; every time a
> transaction is allowed without proving any inputs the size of the transaction
> history proof is reset. Of course, this can be a source of inflation, but
> provided the probability of this happening can be limited we can limit the
> maximum rate of inflation to the chosen value.
> 
> For example, in Bitcoin as of writing every block inflates the currency supply
> by 25BTC, and contains a maximum of 1MB of transaction data, 0.025BTC/KB. If we
> check the prior input proof with probability p, then the expected value of a
> transaction claiming to spend x BTC is:
> 
>     E = x(1-p)
> 
> We can rewrite that in terms of the block reward per-byte R, and the transaction size l:
> 
>     lR = x(1-p)
> 
> And solving for p:
> 
>     p = 1 - lR/x
> 
> For example, for a 1KB transaction proof claiming to spending 10BTC we can omit
> checking the input 0.25% of the time without allowing more monetary inflation
> than the block reward already does. Secondly, this means that after n
> transactions, the probability that proof shortening will _not_ happen is p^n,
> which reaches 1% after 1840 transactions.
> 
> In a system like Bitcoin where miners are expected to validate, a transaction
> proof could consist of just a single merkle path showing that a single-use seal
> was closed in some kind of TXO commitment - probably under 10KB of data. That
> gives us a history proof less than 18.4MB in size, 99% of the time, and less
> than 9.2MB in size 90% of the time.
> 
> An interesting outcome of thing kind of design is that we can institutionalize
> inflation fraud: the entire block reward can be replaced by miners rolling the
> dice, attempting to create valid "fake" transactions. However, such a pure
> implementation would put a floor on the lowest transaction fee possible, so
> better to allow both transaction fee and subsidy collection at the same time.
> 
> 
> # References
> 
> [^paypub] https://github.com/unsystem/paypub
> [^timelock] https://github.com/petertodd/timelock
> [^zkcp] https://bitcoincore.org/en/2016/02/26/zero-knowledge-contingent-payments-announcement/
> [^rpow] https://cryptome.org/rpow.htm
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
On Feb 6, 2016 16:37, "Gavin Andresen" <gavinandresen@gmail.com> wrote:
>
> Responding to "28 days is not long enough" :

Any thoughts on the "95% better than 75%" and "grace period before miner
coordination instead of after" comments ?

> I suspect there ARE a significant percentage of un-maintained full
nodes-- probably 30 to 40%. Losing those nodes will not be a problem, for
three reasons:

None of the reasons you list say anything about the fact that "being lost"
(kicked out of the network) is a problem for those node's users.

> I strongly disagree with the statement that there is no cost to a longer
grace period.

I didn't say that.

> To bring it back to bitcoin-dev territory:  are there any TECHNICAL
arguments why an upgrade would take a business or individual longer than 28
days?

Their own software stack may require more work to integrate the new rules
or their resources may not be immediately available to focus on this within
28 days they hadn't planned.

I believe it wold be less controversial to chose something that nobody can
deny is more than plenty of time for everyone  to implement the changes
like, say, 1 year. I wouldn't personally oppose to something shorter like 6
months for really simple changes, but I don't see how 28 can ever be
considered uncontroversial and safe for everyone. Just trying to help in
removing controversy from the PR, but if you still think 28 can be safe and
uncontroversial, feel free to ignore these comments on the concrete length
and please let me know what you think about the other points I raised.

-------------------------------------
You are making a very nave assumption that miners are just looking for
profit for the next second. Instead, they would try to optimize their short
term and long term ROI. It is also well known that some miners would mine at
a loss, even not for ideological reasons, if they believe that their action
is beneficial to the network and will provide long term ROI. It happened
after the last halving in 2012. Without any immediate price appreciation,
the hashing rate decreased by only less than 10%

 

http://bitcoin.sipa.be/speed-ever.png

 

 

From: bitcoin-dev-bounces@lists.linuxfoundation.org
[mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Jonathan
Toomim via bitcoin-dev
Sent: Monday, 8 February, 2016 01:11
To: Anthony Towns <aj@erisian.com.au>
Cc: bitcoin-dev@lists.linuxfoundation.org
Subject: Re: [bitcoin-dev] BIP proposal: Increase block size limit to 2
megabytes

 

 

On Feb 7, 2016, at 7:19 AM, Anthony Towns via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org
<mailto:bitcoin-dev@lists.linuxfoundation.org> > wrote:





The stated reasoning for 75% versus 95% is "because it gives "veto power"
to a single big solo miner or mining pool". But if a 20% miner wants to
"veto" the upgrade, with a 75% threshold, they could instead simply use
their hashpower to vote for an upgrade, but then not mine anything on
the new chain. At that point there'd be as little as 55% mining the new
2MB chain with 45% of hashpower remaining on the old chain. That'd be 18
minute blocks versus 22 minute blocks, which doesn't seem like much of
a difference in practice, and at that point hashpower could plausibly
end up switching almost entirely back to the original consensus rules
prior to the grace period ending.

 

Keep in mind that within a single difficulty adjustment period, the
difficulty of mining a block on either chain will be identical. Even if the
value of a 1MB branch coin is $100 and the hashrate on the 1 MB branch is
100 PH/s, and the value of a 2 MB branch coin is $101 and the hashrate on
the 2 MB branch is 1000 PH/s, the rational thing for a miner to do (for the
first adjustment period) is to mine on the 2 MB branch, because the miner
would earn 1% more on that branch.

 

So you're assuming that 25% of the hashrate chooses to remain on the
minority version during the grace period, and that 20% chooses to switch
back to the minority side. The fork happens. One branch has 1 MB blocks
every 22 minutes, and the other branch has 2 MB blocks every 18 minutes. The
first branch cannot handle the pre-fork transaction volume, as it only has
45% of the capacity that it had pre-fork. The second one can, as it has 111%
of the pre-fork capacity. This makes the 1 MB branch much less usable than
the 2 MB branch, which in turn causes the market value of newly minted coins
on that branch to fall, which in turn causes miners to switch to the more
profitable 2MB branch. This exacerbates the usability difference, which
exacerbates the price difference, etc. Having two competing chains with
equal hashrate using the same PoW function and nearly equal features is not
a stable state. Positive feedback loops exist to make the vast majority of
the users and the hashrate join one side.

 

Basically, any miners who stick to the minority branch are going to lose a
lot of money.

 

 


-------------------------------------
On 11/17/2016 02:22 AM, Tier Nolan via bitcoin-dev wrote:
> On Thu, Nov 17, 2016 at 12:43 AM, Eric Voskuil <eric@voskuil.org
> <mailto:eric@voskuil.org>> wrote:
> 
>     > This means that all future transactions will have different txids...
>     rules do guarantee it.
> 
>     No, it means that the chance is small, there is a difference.
> 
> I think we are mostly in agreement then?  It is just terminology.

Sure, if you accept that mostly is not fully - just as unlikely is not
impossible.

> In terms of discussing the BIP, barring a hash collision, it does make
> duplicate txids impossible.

That's like saying, as long as we exclude car accidents from
consideration, car accidents are impossible.

> Given that a hash collision is so unlikely, the qualifier should be
> added to those making claims that require hash collisions rather than
> those who assume that they aren't possible.
> 
> You could have said "However nothing precludes different txs from having
> the same hash, but it requires a hash collision".

I generally try to avoid speaking in tautologies :)

> Thinking about it, a re-org to before the enforcement height could allow
> it.  The checkpoints protect against that though.
>  
>     As such this is not something that a node
>     can just dismiss. 
> 
> The security of many parts of the system is based on hash collisions not
> being possible.

This is not the case.

Block hash duplicates within the same chain are invalid as a matter of
consensus, which is the opposite of assuming impossibility.

Tx hash collisions are explicitly allowed in the case that preceding tx
with the same hash is unspent. This is also not a reliance on the
impossibility of hash collision. Core certainly implements this distinction:

https://github.com/bitcoin/bitcoin/blob/master/src/main.cpp#L2419-L2426

Address hashes and script hashes can collide without harming the
security of Bitcoin (although address owner(s) may experience harm).
Rare in this case is sufficient because of this distinction.

Compact blocks contemplates hash collisions:

https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki#Random_collision_probabilty

Checkpoints aren't part of Bitcoin security, so even the remote
possibility of two different potential blocks, with the same hash, at
the same height in the same chain, does not indicate a problem.

There is no case where the security of Bitcoin assumes that hashes never
collide. Consensus rules have specific handling for both block hash
collisions and tx hash collisions.

e


-------------------------------------
"You miss something obvious that makes this attack actually free of cost.
Nothing will "cost them more in transaction fees". A miner can create
thousands of transactions paying to himself, and not broadcast them to
the network, but hold them and include them in the blocks he mines. The
fees are collected by him because transactions are included in a block
that he mined and the left amount is in another wallet of the same
person. Repeat this continuously to fill blocks."

This is easily detectable as long as the network isn't heavily
partitioned(which is an assumption we make today in order for transaction
propagation to work reliably as well as for xThin and CompactBlocks to work
effectively to reduce block transmission time).  Other miners would have an
incentive to intentionally orphan blocks that contained a large number of
transactions that their nodes were unaware of.

I don't think this sort of attack would last long.  Even later when
subsidies are drastically reduced, you would still lose out on significant
genuine fee revenue if your orphan rate increased even 10%(one out of ten
of your poison blocks intentionally orphaned by another miner).

On Dec 11, 2016 11:12 AM, "s7r via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

 t. khan wrote:
> Miners 'gaming' the Block75 system -
> There is no financial incentive for miners to attempt to game the
> Block75 system. Even if it were attempted and assuming the goal was to
> create bigger blocks, the maximum possible increase would be 25% over
> the previous block size. And, that size would only last for two weeks
> before readjusting down. It would cost them more in transaction fees to
> stuff the network than they could ever make up. To game the system,
> they'd have to game it forever with no possibility of profit.
>

This is an incentive, if few miners agree to create a large conglomerate
that will ultimately control the network.

You miss something obvious that makes this attack actually free of cost.
Nothing will "cost them more in transaction fees". A miner can create
thousands of transactions paying to himself, and not broadcast them to
the network, but hold them and include them in the blocks he mines. The
fees are collected by him because transactions are included in a block
that he mined and the left amount is in another wallet of the same
person. Repeat this continuously to fill blocks.


> Blocks would get too big -
> Eventually, blocks would get too big, but only if bandwidth stopped
> increasing and the cost of disk space stopped decreasing. Otherwise, the
> incremental adjustments made by Block75 (especially in combination with
> SegWit) wouldn't break anyone's connection or result in significantly
> more orphaned blocks.
>

Topology and bandwidth speed / hash rate of the network cannot be
controlled - if we make assumptions about these it might have terrible
consequences.

Even if we take in consideration that bandwidth will only grow and disk
space will only cost less (which is not something we can safely assume,
by the way) the hard limit max. block size cannot grow to unlimited
value (even if the growth happens over time). There is also a validation
cost in time for each block, for the health of the network any node
should be able to download _and_ validate a block, before next block
gets mined.

You said in another post that a permanent solution is preferred, rather
than kicking the can down the road. I fully agree, as well as many
others reading this list, but the permanent solution doesn't necessarily
have to be increasing the max block size dynamically.

If you think about it the other way around, dynamically growing the max
block size is also kicking the can down the road ... just without having
to touch it and get dust on the boot ;)


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
On Tuesday 20 Sep 2016 21:31:47 Luke Dashjr wrote:
> On Tuesday, September 20, 2016 5:15:45 PM Tom via bitcoin-dev wrote:
> > As the title suggests, I would like to formally request the assignment of
> > a
> > BIP number for my FT spec.
> 
> Please open a pull request on the bitcoin/bips repo after this has been
> discussed a bit on the ML.

> It seems from the later comments, that it is the end of the transaction as a
> whole. Yet a separator between the txid and non-txid data would probably be
> valuable, rather than hard-coding txid to skip signature types (which may
> be unknown to old nodes, when extended).
> 
> > The OP_CHECKSIG is the most well known and, as its name implies, it
> > validates a signature.
> > In the new version of 'script' (version 2) the data that is signed is
> > changed to be equivalent to the transaction-id. This is a massive
> > simplification and also the only change between version 1 and version 2 of
> > script.
> 
> This seems to be a major regression. What is the replacement for
> SIGHASH_SINGLE and SIGHASH_ANYONECANPAY?

How is this a regression? Can you explain what functionality is lost please?

> When revising OP_CHECKSIG, it would also be nice to add the ability to use
> *only* a hash of the prevout's scriptPubKey in the input, so that *when* the
> prevtx is malleated, the spending one remains valid. (This use case is
> currently not supported.)

Maybe for the next version of script :)
 
> > Notice that the token ScriptVersion is currently not allowed
> What happens if I put ScriptVersion=1 here?

The transaction is invalid...
 
> > === Block-malleability ===
> > 
> > For this reason the merkle tree is extended to include (append) the hash
> > of
> > the v4 transactions (and those alone) where the hash is taken over a
> 
> > data-blob that is build up from:
>
> How should nodes know where in the merkle-tree the txids end, and the
> v4hashes begin?

Because the txid based ones are not going away. So the number of transactions 
in the block can be used to determine when the pure tx-id segment stops and 
when the v4 hashes begin.  Then its up to the client to rebuild the tree from 
that list based on the larger input set to get the same root-node.

I clarified many little things on my clone of the bips, check there if you 
want to see the details.


-------------------------------------
Hello, since trying to encapsulate consensus code without exposing
anything else (see my post from january
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012235.html
) wasn't succesful in getting review, I decided to turn "phase 2" into
"expose verifyHeader" again. I was previously starting the document
with pictures but since things we're changing and the pictures were
already deprecated, I decided to wait after segwit was merged and
include those changes in the pictures too.
This time I created a repository so that people can look at it, even
if it's less advanced than previous versions have been:

https://github.com/jtimon/consensus-doc

Here's a branch with the resulting images, latex file and pdf:

https://github.com/jtimon/consensus-doc/tree/generated

And here's the pdf:

https://github.com/jtimon/consensus-doc/blob/generated/libconsensus.pdf

Any questions or comments are welcomed. If some of the images are
wanted for some other more general documentation or you want me to
create a specific diagram to document Bitcoin Core I'm happy to do so
as well.

Note that some phases can be done in different order or in parallel
(ie phase 3 and phase 4 could happen before phase 2, although I
strongly doubt it because phase 2 is the simplest to review and I've
been harassing different people to do it for a while with little
success [thanks to those who reviewed it and gave feedback] ).

An implementation of phase 2 (expose verifyHeader()) can be seen in
https://github.com/bitcoin/bitcoin/pull/8493


-------------------------------------
> 1) The segregated witness discount is changed from 75% to 50%. The block
> size limit (ie transactions + witness/2) is set to 1.5MB. This gives a
> maximum block size of 3MB and a "network-upgraded" block size of roughly
> 2.1MB. This still significantly discounts script data which is kept out
> of the UTXO set, while keeping the maximum-sized block limited.

What is the rationale for offering a discount?

Is there an economic basis for setting the original discount at 75%
instead of some other number?

If it's okay to arbitrarily reduce the discount by 1/3, what are the
actual boundary limits:  50% - 75% ?  40% - 80% ?

--Simon


-------------------------------------
The fact that some implementations ban an invalid block hash and some do not, suggests that it’s not a pure p2p protocol issue. A pure p2p split should be unified by a bridge node. However, a bridge node is not helpful in this case. Banning an invalid block hash is an implicit “first seen” consensus rule.

jl2012

> On 18 Nov 2016, at 01:49, Eric Voskuil <eric@voskuil.org> wrote:
> 
> Actually both possibilities were specifically covered in my description. Sorry if it wasn't clear.
> 
> If you create a new valid block out of an old one it's has potential to cause a reorg. The blocks that previously built on the original are still able to do so but presumably cannot build forever on the *new* block as it has a different tx. But other new blocks can. There is no chain split due to a different interpretation of valid, there are simply two valid competing chains.
> 
> Note that this scenario requires not only block and tx validity with a tx hash collision, but also that the tx be valid within the block. Pretty far to reach to not even get a chain split, but it could produce a deep reorg with a very low chance of success. As I keep telling people, deep reorgs can happen, they are just unlikely, as is this scenario.
> 
> If you create a new invalid block it is discarded by everyone. That does not invalidate the hash of that block. Permanent blocking as you describe it would be a p2p protocol design choice, having nothing to do with consensus. Libbitcoin for example does not ban invalidated hashes at all. It just discards the block and drops the peer.
> 
> e




-------------------------------------
That is a good point. As you said, it puts a lot more burden on the coin
holders. One big downside would be data management. Instead of simply
backing up a single HD private key, the user would have to back up entire
histories of every output that has been sent to them if they want to secure
their funds.

It also requires them to be online to receive payments, and I think finding
a method of sending the private message containing the coin's history is
going to be a bit of a challenge. If you connect directly to the recipient
to convey the information through traditional channels, anonymity is lost.
Sending messages through the bitcoin network is one option to protect
anonymity, but without active pathfinding there's no guarantee the payee
will even get the message. I'm assuming you'd have to essentially replace
tx messages with encrypted BBC histories, and mempools are quite full as it
is.

Tony, do you have any more thoughts on exactly how users would convey the
private messages to payees?

On Mon, Aug 8, 2016 at 4:42 PM Tony Churyumoff <tony991@gmail.com> wrote:

> The whole point is in preventing every third party, including miners, from
> seeing the details of what is being spent and how.  The burden of
> verification is shifted to the owners of the coin (which is fair).
>
> In fact we could have miners recognize spend proofs and check that the
> same spend proof is not entered into the blockchain more than once (which
> would be a sign of double spend), but it is not required.  The coin owners
> can already do that themselves.
>
> 2016-08-09 0:41 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
>
>> Wouldn't you lose the ability to assume transactions in the blockchain
>> are verified as valid, since miners can't see the details of what is being
>> spent and how? I feel like this ability is bitcoin's greatest asset, and by
>> removing it you're creating an altcoin different enough to not be connected
>> to/supported by the main bitcoin project.
>>
>> On Mon, Aug 8, 2016, 09:13 Tony Churyumoff via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> Hi Henning,
>>>
>>> 1. The fees are paid by the enclosing BTC transaction.
>>> 2. The hash is encoded into an OP_RETURN.
>>>
>>> > Regarding the blinding factor, I think you could just use HMAC.
>>> How exactly?
>>>
>>> Tony
>>>
>>>
>>> 2016-08-08 18:47 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:
>>>
>>>> Hi Tony,
>>>>
>>>> I see some issues in your protocol.
>>>>
>>>> 1. How are mining fees handled?
>>>>
>>>> 2. Assume Alice sends Bob some Coins together with their history and
>>>> Bob checks that the history is correct. How does the hash of the txout
>>>> find its way into the blockchain?
>>>>
>>>> Regarding the blinding factor, I think you could just use HMAC.
>>>>
>>>> All the best
>>>> Henning
>>>>
>>>>
>>>> On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via
>>>> bitcoin-dev wrote:
>>>> > This is a proposal about hiding the entire content of bitcoin
>>>> > transactions.  It goes farther than CoinJoin and ring signatures,
>>>> which
>>>> > only obfuscate the transaction graph, and Confidential Transactions,
>>>> which
>>>> > only hide the amounts.
>>>> >
>>>> > The central idea of the proposed design is to hide the entire inputs
>>>> and
>>>> > outputs, and publish only the hash of inputs and outputs in the
>>>> > blockchain.  The hash can be published as OP_RETURN.  The plaintext of
>>>> > inputs and outputs is sent directly to the payee via a private
>>>> message, and
>>>> > never goes into the blockchain.  The payee then calculates the hash
>>>> and
>>>> > looks it up in the blockchain to verify that the hash was indeed
>>>> published
>>>> > by the payer.
>>>> >
>>>> > Since the plaintext of the transaction is not published to the public
>>>> > blockchain, all validation work has to be done only by the user who
>>>> > receives the payment.
>>>> >
>>>> > To protect against double-spends, the payer also has to publish
>>>> another
>>>> > hash, which is the hash of the output being spent.  We’ll call this
>>>> hash *spend
>>>> > proof*.  Since the spend proof depends solely on the output being
>>>> spent,
>>>> > any attempt to spend the same output again will produce exactly the
>>>> same
>>>> > spend proof, and the payee will be able to see that, and will reject
>>>> the
>>>> > payment.  If there are several outputs consumed by the same
>>>> transaction,
>>>> > the payer has to publish several spend proofs.
>>>> >
>>>> > To prove that the outputs being spent are valid, the payer also has
>>>> to send
>>>> > the plaintexts of the earlier transaction(s) that produced them, then
>>>> the
>>>> > plaintexts of even earlier transactions that produced the outputs
>>>> spent in
>>>> > those transactions, and so on, up until the issue (similar to
>>>> coinbase)
>>>> > transactions that created the initial private coins.  Each new owner
>>>> of the
>>>> > coin will have to store its entire history, and when he spends the
>>>> coin, he
>>>> > forwards the entire history to the next owner and extends it with his
>>>> own
>>>> > transaction.
>>>> >
>>>> > If we apply the existing bitcoin design that allows multiple inputs
>>>> and
>>>> > multiple outputs per transaction, the history of ownership transfers
>>>> would
>>>> > grow exponentially.  Indeed, if we take any regular bitcoin output
>>>> and try
>>>> > to track its history back to coinbase, our history will branch every
>>>> time
>>>> > we see a transaction that has more than one input (which is not
>>>> uncommon).
>>>> > After such a transaction (remember, we are traveling back in time),
>>>> we’ll
>>>> > have to track two or more histories, for each respective input.  Those
>>>> > histories will branch again, and the total number of history entries
>>>> grows
>>>> > exponentially.  For example, if every transaction had exactly two
>>>> inputs,
>>>> > the size of history would grow as 2^N where N is the number of steps
>>>> back
>>>> > in history.
>>>> >
>>>> > To avoid such rapid growth of ownership history (which is not only
>>>> > inconvenient to move, but also exposes too much private information
>>>> about
>>>> > previous owners of all the contributing coins), we will require each
>>>> > private transaction to have exactly one input (i.e. to consume
>>>> exactly one
>>>> > previous output).  This means that when we track a coin’s history
>>>> back in
>>>> > time, it will no longer branch.  It will grow linearly with the
>>>> number of
>>>> > transfers of ownership.  If a user wants to combine several inputs,
>>>> he will
>>>> > have to send them as separate private transactions (technically,
>>>> several
>>>> > OP_RETURNs, which can be included in a single regular bitcoin
>>>> transaction).
>>>> >
>>>> > Thus, we are now forbidding any coin merges but still allowing coin
>>>> > splits.  To avoid ultimate splitting into the dust, we will also
>>>> require
>>>> > that all private coins be issued in one of a small number of
>>>> > denominations.  Only integer number of “banknotes” can be
>>>> transferred, the
>>>> > input and output amounts must therefore be divisible by the
>>>> denomination.
>>>> > For example, an input of amount 700, denomination 100, can be split
>>>> into
>>>> > outputs 400 and 300, but not into 450 and 250.  To send a payment, the
>>>> > payer has to pick the unspent outputs of the highest denomination
>>>> first,
>>>> > then the second highest, and so on, like we already do when we pay in
>>>> cash.
>>>> >
>>>> > With fixed denominations and one input per transaction, coin histories
>>>> > still grow, but only linearly, which should not be a concern in
>>>> regard to
>>>> > scalability given that all relevant computing resources still grow
>>>> > exponentially.  The histories need to be stored only by the current
>>>> owner
>>>> > of the coin, not every bitcoin node.  This is a fairer allocation of
>>>> > costs.  Regarding privacy, coin histories do expose private
>>>> transactions
>>>> > (or rather parts thereof, since a typical payment will likely consist
>>>> of
>>>> > several transactions due to one-input-per-transaction rule) of past
>>>> coin
>>>> > owners to the future ones, and that exposure grows linearly with
>>>> time, but
>>>> > it is still much much better than having every transaction
>>>> immediately on
>>>> > the public blockchain.  Also, the value of this information for
>>>> potential
>>>> > adversaries arguably decreases with time.
>>>> >
>>>> > There is one technical nuance that I omitted above to avoid
>>>> distraction.
>>>> >  Unlike regular bitcoin transactions, every output in a private
>>>> payment
>>>> > must also include a blinding factor, which is just a random string.
>>>> When
>>>> > the output is spent, the corresponding spend proof will therefore
>>>> depend on
>>>> > this blinding factor (remember that spend proof is just a hash of the
>>>> > output).  Without a blinding factor, it would be feasible to
>>>> pre-image the
>>>> > spend proof and reveal the output being spent as the search space of
>>>> all
>>>> > possible outputs is rather small.
>>>> >
>>>> > To issue the new private coin, one can burn regular BTC by sending it
>>>> to
>>>> > one of several unspendable bitcoin addresses, one address per
>>>> denomination.
>>>> >  Burning BTC would entitle one to an equal amount of the new private
>>>> coin,
>>>> > let’s call it *black bitcoin*, or *BBC*.
>>>> >
>>>> > Then BBC would be transferred from user to user by:
>>>> > 1. creating a private transaction, which consists of one input and
>>>> several
>>>> > outputs;
>>>> > 2. storing the hash of the transaction and the spend proof of the
>>>> consumed
>>>> > output into the blockchain in an OP_RETURN (the sender pays the
>>>> > corresponding fees in regular BTC)
>>>> > 3. sending the transaction, together with the history leading to its
>>>> input,
>>>> > directly to the payee over a private communication channel.  The first
>>>> > entry of the history must be a bitcoin transaction that burned BTC to
>>>> issue
>>>> > an equal amount of BCC.
>>>> >
>>>> > To verify the payment, the payee:
>>>> > 1. makes sure that the amount of the input matches the sum of
>>>> outputs, and
>>>> > all are divisible by the denomination
>>>> > 2. calculates the hash of the private transaction
>>>> > 3. looks up an OP_RETURN that includes this hash and is signed by the
>>>> > payee.  If there is more than one, the one that comes in the earlier
>>>> block
>>>> > prevails.
>>>> > 4. calculates the spend proof and makes sure that it is included in
>>>> the
>>>> > same OP_RETURN
>>>> > 5. makes sure the same spend proof is not included anywhere in the
>>>> same or
>>>> > earlier blocks (that is, the coin was not spent before).  Only
>>>> transactions
>>>> > by the same author are searched.
>>>> > 6. repeats the same steps for every entry in the history, except the
>>>> first
>>>> > entry, which should be a valid burning transaction.
>>>> >
>>>> > To facilitate exchange of private transaction data, the bitcoin
>>>> network
>>>> > protocol can be extended with a new message type.  Unfortunately, it
>>>> lacks
>>>> > encryption, hence private payments are really private only when
>>>> bitcoin is
>>>> > used over tor.
>>>> >
>>>> > There are a few limitations that ought to be mentioned:
>>>> > 1. After user A sends a private payment to user B, user A will know
>>>> what
>>>> > the spend proof is going to be when B decides to spend the coin.
>>>> >  Therefore, A will know when the coin was spent by B, but nothing
>>>> more.
>>>> >  Neither the new owner of the coin, nor its future movements will be
>>>> known
>>>> > to A.
>>>> > 2. Over time, larger outputs will likely be split into many smaller
>>>> > outputs, whose amounts are not much greater than their denominations.
>>>> > You’ll have to combine more inputs to send the same amount.  When you
>>>> want
>>>> > to send a very large amount that is much greater than the highest
>>>> available
>>>> > denomination, you’ll have to send a lot of private transactions, your
>>>> > bitcoin transaction with so many OP_RETURNs will stand out, and their
>>>> > number will roughly indicate the total amount.  This kind of privacy
>>>> > leakage, however it applies to a small number of users, is easy to
>>>> avoid by
>>>> > using multiple addresses and storing a relatively small amount on each
>>>> > address.
>>>> > 3. Exchanges and large merchants will likely accumulate large coin
>>>> > histories.  Although fragmented, far from complete, and likely
>>>> outdated, it
>>>> > is still something to bear in mind.
>>>> >
>>>> > No hard or soft fork is required, BBC is just a separate privacy
>>>> preserving
>>>> > currency on top of bitcoin blockchain, and the same private keys and
>>>> > addresses are used for both BBC and the base currency BTC.  Every BCC
>>>> > transaction must be enclosed into by a small BTC transaction that
>>>> stores
>>>> > the OP_RETURNs and pays for the fees.
>>>> >
>>>> > Are there any flaws in this design?
>>>> >
>>>> > Originally posted to BCT
>>>> https://bitcointalk.org/index.php?topic=1574508.0,
>>>> > but got no feedback so far, apparently everybody was consumed with
>>>> bitfinex
>>>> > drama and now mimblewimble.
>>>> >
>>>> > Tony
>>>>
>>>> > _______________________________________________
>>>> > bitcoin-dev mailing list
>>>> > bitcoin-dev@lists.linuxfoundation.org
>>>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>>> --
>>>> Henning Kopp
>>>> Institute of Distributed Systems
>>>> Ulm University, Germany
>>>>
>>>> Office: O27 - 3402
>>>> Phone: +49 731 50-24138
>>>> Web: http://www.uni-ulm.de/in/vs/~kopp
>>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
>

-------------------------------------
> I don't know if it's possible to implement decentralised sidechains without
> "breaking" this rule.
>

I haven't really been following the sidechain developements, but my
understanding was that redemption from a side chain would be two phase.
The person unpegging the funds provides a proof that they have locked the
funds on the side chain and are eligible to withdraw the funds, plus they
put up a bounty.  Then there is a time-locked period where others can
collect the bounty by providing a fraud proof, that the locked funds given
in the proof have actually been double spent.  This two phase system
doesn't violate this rule.

-------------------------------------
On Feb 4, 2016 19:29, "Luke Dashjr via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> On Thursday, February 04, 2016 5:14:49 PM jl2012 via bitcoin-dev wrote:
> > ABSTRACT
> >
> > This document specifies a proposed change to the semantics of the sign
> > bit of the "version" field in Bitcoin block headers, as a mechanism to
> > indicate a hardfork is deployed.
>
> Disagree with treating the "version" field as a number, in BIP 9 or this
BIP
> which reinterpret it as a bit vector.

I don't interpret this as "treating version bits as a number" it is just
being explained which bit we're talking about. Could you propose some
concrete rephrasing instead of leaving the task of somehow solving this
vague and subtle concern to the author?

> > FLAG BLOCK Any planned hardfork must have one and only one flag block
> > which is the "point of no return". To ensure monotonicity, flag block
> > should be determined by block height, or as the first block with
> > GetMedianTimePast() greater than a threshold. Other mechanisms could be
> > difficult for SPV nodes to follow. The height/time threshold could be a
> > predetermined value or relative to other events (e.g. 10000 blocks / 100
> > days after 95% of miner support). The exact mechanism is out of the
> > scope of this BIP. No matter what mechanism is used, the threshold is
> > consensus critical. It must be publicly verifiable with only blockchain
> > data, and preferably SPV-friendly (i.e. verifiable with block headers
> > only, without downloading any transaction).
>
> With the current codebase, it is significantly easier to trigger on the
block
> timestamp rather than its height or median-time-past. Using either of the
> latter would require refactoring of CBlockIndex. As a hard-fork, even if
the
> rules are ineffective for a few blocks following the forking point, using
the
> hardfork version bit in this BIP would still ensure a clean break. While I
> agree that median-time-past and height are superior methods that ought to
be
> used for hardforks, an emergency hardfork may need to avoid them for
> simplicity, and I don't think they need to be mandated as such in this
BIP.

I very much disagree with "significant" and in any case it depends on the
hardfork: the changes required can still be quite minimal in all cases and
it should never be a problem, even for emergency hardforks. In emergency,
we could for example just a new global (we have many already anyway),
although activeChain.tip () is already there and one can simply get the
last height or median time from there.

> > VERSION BITS This proposal is also compatible with the BIP9. The version
> > bits mechanism could be employed to measure miner support towards a
> > hardfork proposal, and to determine the height or time threshold of the
> > flag block. Also, miners of the flag block may still cast votes for
> > other concurrent softfork or hardfork proposals as normal.
>
> Rather not imply BIP 9 should be used for hardforks, or that miners have
any
> voice in the decision. This is already a serious misconception.

This is consistent with bip99, which recommends bip9 for deploying
uncontroversial hardforks.

> > POINT OF NO RETURN After the flag block is generated, a miner may
> > support either the original rules or the new rules, but not both. It is
> > not possible for miners in one fork to attack or overtake the other fork
> > without giving up the mining reward of their preferred fork.
>
> This is not actually desirable, and would suggest a possible reason *not*
to
> comply with this BIP. A legitimate hardfork would never have two continued
> sets of rules for miners to choose from.

Controversial hardforks (as defined bip9) always have the potential to
create two chains that survive for unbounded amounts of time (maybe
forever) as discussed in one of the few threads of the bitcoin discuss
mailing list.
Of course, BIP99 cannot say anything general about the "legitimacy" of all
controversial hardforks since ASIC-reset hardforks, for example, are
controversial hardforks by definition in the context of bip99 (and the
definitions in bip99 seem to apply to this bip). BIP99 can only warn about
the dangers and risks of controversial hardforks but at some point (let's
hope never) a controversial hardfork may be required to save the system
from some evil (say, evil miners blacklisting via softforking out the
miners that don't  blacklist or something) and that controversial hardfork
would be legitimate (at least to the eyes of some).

-------------------------------------

> On 9 Mar 2016, at 20:21, Bob McElrath <bob_bitcoin@mcelrath.org> wrote:
> 
> Dave Hudson [dave@hashingit.com] wrote:
>> A damping-based design would seem like the obvious choice (I can think of a
>> few variations on a theme here, but most are found in the realms of control
>> theory somewhere).  The problem, though, is working working out a timeframe
>> over which to run the derivative calculations.
> 
> From a measurement theory perspective this is straightforward.  Each block is a
> measurement, and error propagation can be performed to derive an error on the
> derivatives.

Sure, but I think there are 2 problems:

1) My guess is that errors over anything but a long period are probably too large to be very useful.

2) We don't have a strong notion of time that is part of the consensus.  Sure, blocks have timestamps but they're very loosely controlled (can't be more than 2 hours ahead of what any validating node thinks the time might be).  Difficulty can't be calculated based on anything that's not part of the consensus data.

> The statistical theory of Bitcoin's block timing is known as a Poisson Point
> Process: https://en.wikipedia.org/wiki/Poisson_point_process or temporal point
> process.  If you google those plus "estimation" you'll find a metric shit-ton of
> literature on how to handle this.

Strictly it's a non-homogeneous Poisson Process, but I'm pretty familiar with the concept (Google threw one of my own blog posts back at me: http://hashingit.com/analysis/27-hash-rate-headaches, but I actually prefer this one: http://hashingit.com/analysis/30-finding-2016-blocks because most people seem to find it easier to visualize).

>> The problem is the measurement of the hashrate, which is pretty inaccurate at
>> best because even 2016 events isn't really enough (with a completely constant
>> hash rate running indefinitely we'd see difficulty swings of up to +/- 5% even
>> with the current algorithm).  In order to meaningfully react to a major loss
>> of hashing we'd still need to be considering a window of probably 2 weeks.
> 
> You don't want to assume it's constant in order to get a better measurement.
> The assumption is clearly false.  But, errors can be calculated, and retargeting
> can take errors into account, because no matter what we'll always be dealing
> with a finite sample.

Agreed, it's a thought experiment I ran in May 2014 (http://hashingit.com/analysis/28-reach-for-the-ear-defenders).  I found that many people's intuition is that there would be little or no difficulty changes in such a scenario, but the intuition isn't reliable.  Given a static hash rate the NHPP behaviour introduces a surprisingly large amount of noise (often much larger than any signal over a period of even weeks).  Any measurements in the order of even a few days has so much noise that it's practically unusable.  I just realized that unlike some of my other sims this one didn't make it to github; I'll fix that later this week.


Cheers,
Dave

-------------------------------------
On Thu, Jun 23, 2016 at 1:46 PM, s7r via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
>
>
> Any kind of built-in AML/KYC tools in Bitcoin is bad, and might draw
> expectations from _all_ users from authorities. Companies or individuals
> who want and/or need AML/KYC can find ways and do it at their side
> isolated from the entire network, and the solutions shouldn't come from
> upstream. AML/KYC/<insert other regulation here> differ from country to
> country and will be hard to implement in a global consensus network even
> if it would be worth it.
>
>
This was precisely our thinking as well.

This is actually exactly why BIP 75 was designed the way that it was.  Any
(voluntary) identity exchange is done at the application level, on an
encrypted https (or other) connection between the sender and receiver.
Identity data is not passed through or stored on the blockchain, and there
is actually no mark left on the blockchain that identity was even exchanged
on that transaction.

The only people who know identity info was exchanged, or what the identity
was is the counterparties in the transaction, and depending on
implementation, their service provider.  (At a high level, many software
based wallet providers wouldn’t have any visibility into identity info,
where many hosted services would, for example)

We did this to protect user privacy as well as fungibility.

We are allowing the people who want or need to exchange identtity info
(either self signed or 3rd party validated) the option to exchange it, in a
standards based way, directly between peers, without touching the
blockchain or network itself.

Is this more clear?

-- 

Justin W. Newton
Founder/CEO
Netki, Inc.

justin@netki.com
+1.818.261.4248

-------------------------------------
I think the next hard fork should require a safety rule for TX fees.

https://blockchain.info/tx/6fe69404e6c12b25b60fcd56cc6dc9fb169b24608943def6dbe1eb0a9388ed08

15 BTC TX fee for < 7 BTC of outputs.

Probably either a typo or client bug.

My guess is the user was using a client that does not adjust TX fee, and 
needed to manually set it in order to get the TX in the block sooner, 
and meant 15 mBTC or something.

I suggest that either :

A) TX fee may not be larger than sum of outputs
B) TX fee per byte may not be larger than 4X largest fee per byte in 
previous block

Either of those would have prevented this TX from going into a block.

Many people I know are scared of bitcoin, that they will make a TX and 
make a mistake they can't undo.

Adding protections may help give confidence and there is precedence to 
doing things to prevent typo blunders - a public address has a four byte 
checksum to reduce the odds of a typo.

This kind of mistake is rare, so a fix could be included in the coming 
HF for the possible July 2017 block increase.

Thank you for your time.

Alice Wonder


-------------------------------------
On Wed, Aug 3, 2016 at 7:16 PM, Matthew Roberts via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> The reason why I bring this up is existing OP codes and TX types don't
> seem suitable for a secure clearing mechanism;
>

I think reversing transactions is not likely to be acceptable.  You could
add an opcode that requires that an output be set to something.

[target script] SPENDTO

This would require that [target script] is the script for the corresponding
output.  This is a purely local check.

For example, if SPENDTO executes as part of the script for input 3, then it
checks that output 3 uses the given script as its scriptPubKey.  The value
of input 3 and output 3 would have to be the same too.

This allows check sequence verify to be used to lock the spending script
for a while.  This doesn't allow reversal, but would give a 24 hour window
where the spenders can reverse the transaction.

[IF <1 day> CSV DROP <live public key> CHECKSIG ELSE <offline protected
key> CHECKSIG] SPENDTO <live public key2> CHECKSIG

Someone with the live public key can create a transaction that spends the
funds to the script in the square brackets.

Once that transaction hits the blockchain, then someone with the <offline
protected key> has 24 hours to spend the output before the person with the
live keys can send the funds onward.

-------------------------------------
The purpose of this list is highly technical discussion, not political
disagreements.

Is this particular proposal encumbered by a licensing type, patent, or
pending patent which would preclude it from being used in the bitcoin
project?  If not, you're wildly off topic.

On Oct 2, 2016 12:11 PM, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Sun, Oct 02, 2016 at 02:00:01PM -0300, Sergio Demian Lerner wrote:
> > Peter, are you really going to try to down vote a decent free and
> > open-source proposal that benefits all the Bitcoin community including
> > you and your future children because a personal attack to me without any
> > logic or basis?
>
> I've suggested a way that you can rectify this situation so we can
> continue to
> collaborate: Have Rootstock adopt a legally binding patent pledge/license.
> I'd
> suggest you do as Blockstream has done and at minimum adopt the Defensive
> Patent License (DPL); I personally will be doing so in the next week or
> two for
> my own consulting company (I'm discussing exactly how to do so with my
> lawyer
> right now).
>
> If Rootstock is not planning on getting any patents for offensive purposes,
> then there is no issue with doing so - the DPL in particular is designed
> in a
> minimally intrusive way.
>
> Please fix this issue so we can in fact continue to collaborate to improve
> Bitcoin.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Thu, Nov 3, 2016 at 3:37 AM, Daniel Robinson <danrobinson010@gmail.com>
wrote:

> Really cool!
>
> How about "poison transactions," the other covenants use case proposed by
> Möser, Eyal, and Sirer? (I think OP_CHECKSIGFROMSTACKVERIFY will also make
> it easier to check fraud proofs, the other prerequisite for poison
> transactions.)
>

I admit I didn't study their poison transactions very carefully.  It seemed
specific to Bitcoin-NG.


> Seems a little wasteful to do those two "unnecessary" signature checks,
> and to have to construct the entire transaction data structure, just to
> verify a single output in the transaction. Any plans to add more flexible
> introspection opcodes to Elements, such as OP_CHECKOUTPUTVERIFY?
>

I used to be hesitant to the idea of adding transaction introspection
operations, because the script design seemed to be deliberately avoiding
doing that.  One of the big takeaways from this work, for me at least, is
that since the transaction data is so easily recoverable anyways, adding
transaction introspection operations isn't really going to provide any more
power to script; it will just save everyone a bunch of work.  There are no
specific plans to put transaction introspection opcodes into Elements at
this moment, but I feel that the door for that possibility is wide open now.

Really minor nit: "Notice that we have appended 0x83 to the end of the
> transaction data"—should this say "to the end of the signature"?
>

Probably should reed "Notice that we have appended 0x83000000 to the end of
the transaction data".  I'll make an update.


>
> On Thu, Nov 3, 2016 at 12:28 AM Russell O'Connor via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> Right.  There are minor trade-offs to be made with regards to that design
> point of OP_CHECKSIGFROMSTACKVERIFY.  Fortunately this covenant
> construction isn't sensitive to that choice and can be made to work with
> either implementation of OP_CHECKSIGFROMSTACKVERIFY.
>
> On Wed, Nov 2, 2016 at 11:35 PM, Johnson Lau <jl2012@xbt.hk> wrote:
>
> Interesting. I have implemented OP_CHECKSIGFROMSTACKVERIFY in a different
> way from the Elements. Instead of hashing the data on stack, I directly put
> the 32 byte hash to the stack. This should be more flexible as not every
> system are using double-SHA256
>
> https://github.com/jl2012/bitcoin/commits/mast_v3_master
>
>
> On 3 Nov 2016, at 01:30, Russell O'Connor via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> Hi all,
>
> It is possible to implement covenants using two script extensions: OP_CAT
> and OP_CHECKSIGFROMSTACKVERIFY.  Both of these op codes are already
> available in the Elements Alpha sidechain, so it is possible to construct
> covenants in Elements Alpha today.  I have detailed how the construction
> works in a blog post at <https://blockstream.com/2016/
> 11/02/covenants-in-elements-alpha.html>.  As an example, I've constructed
> scripts for the Moeser-Eyal-Sirer vault.
>
> I'm interested in collecting and implementing other useful covenants, so
> if people have ideas, please post them.
>
> If there are any questions, I'd be happy to answer.
>
> --
> Russell O'Connor
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
I saw these two repositories through the -wizards IRC channel earlier
today. I have not reviewed any of the source code for quality, security or
functionality, so I don't have word to offer regarding status of these.

https://github.com/LightningNetwork/lnd
https://github.com/LightningNetwork/lightning-onion

Also other git repositories with related work:
https://github.com/ElementsProject/lightning
https://github.com/matsjj/thundernetwork

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
We don't have any evidence of how fast nodes will upgrade when faced with
an impending hard fork, but it seems like a very safe assumption that the
upgrade pace will be significantly faster.  The hard fork case it is:
"upgrade or be kicked off the network".  In the previous cases it has been,
"here's the latest and greatest, give it a go!".  Also, there will be
alerts sent out warning people of the situation, prompting them to take
action.

It is unclear if this will translate into more or less than 6x the adoption
speed of previous instances, but the idea that it would be faster is
solid.  28 days is aggressive, but again, it is only 28 days from when the
fork triggers.  Compatible software is already available for anyone who
wants to prepare.

It is also of significance that this proposed fork, and this debate, has
been going on for many, many months.  If someone proposed a forking concept
today, wrote the BIP tomorrow, deployed it next week, miners adopted it
instantly, and 28 days later it was the flag day, those 28 days would be in
a different context.  There is no surprise here.

On Sun, Feb 7, 2016 at 1:33 PM, Steven Pine via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Is it me or did Gavin ignore Yifu's direct questions? In case you missed
> it Gavin --
>
> ~
> "We can look at the adoption of the last major Bitcoin core release to
> guess how long it might take people to upgrade. 0.11.0 was released on 12
> July, 2015. Twenty eight days later, about 38% of full nodes were running
> that release. Three months later, about 50% of the network was running
> that release, and six months later about 66% of the network was running
> some flavor of 0.11."
>
> On what grounds do you think it is reasonable to assume that this update
> will roll out 6x faster than previous data suggested, as oppose to your own
> observation of 66% adoption in 6 month. or do you believe 38% node
> upgrade-coverage (in 28 days ) on the network for a hard fork is good
> enough?
>
> There are no harm in choosing a longer grace period but picking one short
> as 28 days you risk on alienating the nodes who do not upgrade with the
> aggressive upgrade timeline you proposed.
> ~~
>
> When Gavin writes "Responding to "28 days is not long enough" :
>
> I keep seeing this claim made with no evidence to back it up.  As I said,
> I surveyed several of the biggest infrastructure providers and the btcd
> lead developer and they all agree "28 days is plenty of time."
>
> For individuals... why would it take somebody longer than 28 days to
> either download and restart their bitcoind, or to patch and then re-run
> (the patch can be a one-line change MAX_BLOCK_SIZE from 1000000 to
> 2000000)?"
>
> ~~
>
> Isn't Yifu's comment, evidence, the very best sort of evidence, it isn't
> propositional a priori logic, but empirical evidence that. As for why
> people take longer, who knows, we simply know from passed experience that
> it in fact does take longer.
>
> It's extremely frustrating to read Gavin's comments, it's hard to believe
> he is engaging in earnest discussion.
>
> On Sun, Feb 7, 2016 at 4:01 PM, Luke Dashjr via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Sunday, February 07, 2016 2:16:02 PM Gavin Andresen wrote:
>> > On Sat, Feb 6, 2016 at 3:46 PM, Luke Dashjr via bitcoin-dev <
>> > bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > > On Saturday, February 06, 2016 5:25:21 PM Tom Zander via bitcoin-dev
>> wrote:
>> > > > If you have a node that is "old" your node will stop getting new
>> > > > blocks. The node will essentially just say "x-hours behind" with "x"
>> > > > getting larger every hour. Funds don't get confirmed. etc.
>> > >
>> > > Until someone decides to attack you. Then you'll get 6, 10, maybe more
>> > > blocks confirming a large 10000 BTC payment. If you're just a normal
>> end
>> > > user (or perhaps an automated system), you'll figure that payment is
>> good
>> > > and irreversibly hand over the title to the house.
>> >
>> > There will be approximately zero percentage of hash power left on the
>> > weaker branch of the fork, based on past soft-fork adoption by miners
>> (they
>> > upgrade VERY quickly from 75% to over 95%).
>>
>> I'm assuming there are literally ZERO miners left on the weaker branch.
>> The attacker in this scenario simply rents hashing for a few days in
>> advance
>> to build his fake chain, then broadcasts the blocks to the unsuspecting
>> merchant at ~10 block intervals so it looks like everything is working
>> normal
>> again. There are lots of mining rental services out there, and miners
>> quite
>> often do not care to avoid selling hashrate to the highest bidder
>> regardless
>> of what they're mining. 10 blocks worth costs a little more than 250 BTC -
>> soon, that will be 125 BTC.
>>
>> Luke
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
>
> --
> Steven Pine
> (510) 517-7075
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
I still feel like you're better off getting rid of "hot wallets" and use
lightning-esqe networks to route orders.  I don't think either speed or
flexibility is an issue there.

IMO, the point of Bitcoin is to avoid the centralization that seems to be
happening on the network now.   By making "hot wallets" more "secure", we
encourage things to keep heading downhill with massive centralized
crappy-security exchanges.

Because, ultimately, there's no security that will prevent an inside job.
And all of these thefts have, in my opinion, been at least partly inside
jobs.

And centralization is the actually demon that needs slaying here.

A client-side library with P2P order routing, tether.to + bitcoin ....  and
you've got a decentralized exchange... with orders matched to users
directly, and channel-trades executed instantly.   And "market makers"
running nodes to facilitate routing, etc.

No center... nothing to shut down or sue... and no one holds your funds.
That's a real Bitcoin exchange.



On Sun, Aug 7, 2016 at 1:35 AM, Matthew Roberts via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I'm wondering if we're fully on the same page here. What I was thinking
> was that this protection mechanism would be applied to the coins in the hot
> wallet (I wasn't talking about moving coins from the cold wallet to the hot
> wallet -- though such a mechanism is also needed.)
>
> With the hot wallet you would have an output script that only allowed
> coins to be sent to a new transaction whose output script was then only
> redeemable after N confirmations (the output is relative time-locked) but
> which can also be recovered to a fixed fail-safe address before the
> time-lock is reached (exactly like TierNolan already listed only the
> time-locked destination shouldn't be completely fixed.) So the private key
> for this hot wallet can still sign valid transactions to withdraw coins to
> any known destination and these transactions still reach the blockchain.
>
> The key difference from a regular transaction is that the destination only
> has access to the coins -after- the relative time-lock is reached (N blocks
> after first confirm) so everyone knows where withdrawals are suppose to be
> going and how many coins are being withdrawn at any given time. Deposits to
> the hot wallet would therefore need to be encumbered by the same protection
> so that from then on this time-lock to redeem coins can be applied to every
> new transaction trying to move coins (withdrawn by a user of the exchange
> or sent to the cold wallet.)
>
> Notice we don't care about the destination in the TX script for the hot
> wallet because to process user's withdrawals we can't know ahead of time
> where they need to be sent (so it isn't possible to use a fixed address
> here – though you might want to remove the clearing phase and set a fixed
> address for coins sent from the hot wallet to the cold wallet.) The benefit
> here comes from being able to see what withdrawals are being cleared,
> matching those up to our expectations, and being able to "cancel"
> withdrawals if they look suspicious, and you get the benefits for transfers
> made from the hot wallet to the cold wallet and visa-versa.
>
>
> This approach is good for a number of crucial services:
>
> 1. Wallets could be built that grouped coins into different "accounts"
> with different time-frames required for clearing / unlocking coins. Your
> savings or investment account would say -- take up to a week to clear --
> whereas your everyday account used for smaller purchases (with less money)
> would only take a few hours. This could all be linked up to services that
> notified you of your money being moved + made any phone calls needed to
> verify any larger transfers.
>
> The service could also be entrusted with the “cancellation” key which can
> only be used to move money to your offline fail-safe address. This would be
> quite an interesting way to mitigate fraud without the user having to be
> trusted to do anything (except I suppose – not storing their recovery keys
> online … but this could be partially solved with BIP 32-style “master”
> public keys + hardware wallets + multi-sig, N factor auth, etc ...)
>
> 2. Gambling websites that process a lot of Bitcoins also have a hot wallet
> which could be better protected by this.
>
> 3. Various other e-commerce websites also accept Bitcoins directly. (Deep
> web markets come to mind -- hey, people breaking the law need good security
> too.)
>
> 4. Provable dead man's switches on the protocol level is another idea --
> no need to keep special time-locked transactions around and rely on them to
> be broadcast = more reliable escrow services.
>
> 5. And obviously exchange hot (and cold) wallets - enemy number 1.
>
> I hope that makes sense. I think I initially managed to confuse a lot of
> people by talking about revoking transactions / “settlement layers”, etc.
> But IMO: all of this needs to take place on the blockchain with a new set
> of OP_CODES and other than the fixed address issue with OP_SPENDTO, I think
> the general idea would still work.
>
>
> tl; dr, A pseudo-reversal mechanism for transactions would mean that
> stolen private keys were no longer such an issue. This is desperately
> needed for exchanges, wallets, and other services that are forced to manage
> private keys, and whose users (I argue) already expect for this to be
> possible (or at least will when they're hacked.)
>
>
>
>
> On Sat, Aug 6, 2016 at 9:13 PM, Tier Nolan via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Sat, Aug 6, 2016 at 11:39 AM, s7r via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> * reversal of transactions is impossible
>>>
>>
>> I think it would be more accurate to say that the requirement is that
>> reversal doesn't happen unexpectedly.
>>
>> If it is clear in the script that reversal is possible, then obviously
>> the recipient can take that into consideration.
>>
>>
>>> * keep private keys private and safe. Lose them, it's like losing cash,
>>> you can just forget about it.
>>>
>>
>> Key management is a thing.  Managing risk by keeping some keys offline is
>> an important part of that.
>>
>>
>>> * while we try hard to make 0-conf as safe as possible (if there's no
>>> RBF flag on the transaction), we make it almost impossible or very very
>>> expensive to reverse a confirmed transaction.
>>>
>>
>> BitGo has an "instant" system where they promise to only sign one
>> transaction for a given output.  If you trust BitGo, then this is safe from
>> double spending, since a double spender can't sign two transactions.
>>
>> If BitGo had actually implemented a daily withdrawal limit, then their
>> system ends up similar to cold storage.  Only 10% of the funds at Bitfinex
>> could have been withdrawn before manual intervention was required (with
>> offline keys).
>>
>> Who will accept
>>> such an input and treat it as a payment if it can be reversed during the
>>> settlement layer?
>>
>>
>> Obviously, if a payment is reversible, then you treat it as a reversible
>> payment.  The protection here relates to moving coins from the equivalent
>> of cold storage to hot storage.
>>
>> It is OK if it takes longer, since security is more important than
>> convenience for coins in cold storage.
>>
>>
>>> The linked page describes that merchants will never accept payments from
>>> 'vaults', and it will take 24 hours for coins to be irreversible moved
>>> outside the 'vault'.
>>
>>
>> This relates to the reserves held by the exchange.  A portion of the
>> funds are in hot storage with live keys.  These funds can be stolen by
>> anyone who gets access to the servers.  The remaining funds are held in
>> cold storage and they cannot be accessed unless you have the offline keys.
>> These funds are supposed to be hard to reach and require manual
>> intervention.
>>
>> I think this is a wrong approach. hacks and big losses are sad, but all
>>> the time users / exchanges are to blame for wrong implementations or
>>> terrible security practices.
>>>
>>
>> Setting up offline keys to act as firebreaks is part of good security
>> practices.
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
> I'm curious to hear the answers to the questions Luke asked earlier. I
> also read through the documentation and wasn't convinced it was thought out
> well enough to actually build something on top of,
>
There are many colored coin protocols in use. OpenAssets is probably the
most popular one, but it has many critical flaws IMHO.
https://github.com/bitcoinx/colored-coin-tools/wiki/OpenAssets-deficiencies


> but there's no reason it can't get a number as a work-in-progress.
>
The protocol is not a work-in-progress, it is already in use, you cannot
change it without breaking stuff.
The doc can be improved, though. There is a lot of fluff but the actual
important stuff gets just few ambiguous sentences.

I hope it does continue to get worked on, though. The lack of response or
> discussion worries me that it might become an abandoned project.
>

The original author, Flavien, have abandoned it, he now does a private
blockchain thing, OpenChain.
There are others who still use OpenAssets, e.g. Nicolas, but the protocol
can't be changed.

There are other colored coin protocols in existence/in development, though.

-------------------------------------
Actually both possibilities were specifically covered in my description. Sorry if it wasn't clear.

If you create a new valid block out of an old one it's has potential to cause a reorg. The blocks that previously built on the original are still able to do so but presumably cannot build forever on the *new* block as it has a different tx. But other new blocks can. There is no chain split due to a different interpretation of valid, there are simply two valid competing chains.

Note that this scenario requires not only block and tx validity with a tx hash collision, but also that the tx be valid within the block. Pretty far to reach to not even get a chain split, but it could produce a deep reorg with a very low chance of success. As I keep telling people, deep reorgs can happen, they are just unlikely, as is this scenario.

If you create a new invalid block it is discarded by everyone. That does not invalidate the hash of that block. Permanent blocking as you describe it would be a p2p protocol design choice, having nothing to do with consensus. Libbitcoin for example does not ban invalidated hashes at all. It just discards the block and drops the peer.

e

> On Nov 17, 2016, at 9:22 AM, Johnson Lau <jl2012@xbt.hk> wrote:
> 
> I’m not sure if you really understand what you and I am talking. It has nothing to do with BIP30, 34, nor any other BIPs.
> 
> Say tx1 is confirmed 3 years ago in block X. An attacker finds a valid tx2 which (tx1 != tx2) and (SHA256(tx1) == SHA256(tx2)). Now he could replace tx1 with tx2 in block X and the block is still perfectly valid. Anyone trying to download the blockchain from the beginning may end up with a different ledger. The consensus is irrevocably broken as soon as tx1 or tx2 is spent.
> 
> Or, alternatively, an attacker finds an invalid tx3 which (tx1 != tx3) and (SHA256(tx1) == SHA256(tx3)). Now he could replace tx1 with tx3 in block X. Anyone trying to download the blockchain from the beginning will permanently reject the hash of block X. They will instead accept a fork built on top of block X-1. The chain will be permanently forked.
> 
> jl2012
> 
> 
>> On 18 Nov 2016, at 01:01, Eric Voskuil <eric@voskuil.org> wrote:
>> 
>> On 11/17/2016 07:40 AM, Johnson Lau wrote:
>>> 
>>>> On 17 Nov 2016, at 20:22, Eric Voskuil via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>> 
>>>> Given that hash collisions are unquestionably possible,
>>> 
>>> Everything you said after this point is irrelevant.
>> 
>> So... you think hash collisions are not possible, or that it's moot
>> because Core has broken its ability to handle them.
>> 
>> 
>>> Having hash collision is **by definition** a consensus failure,
>> 
>> I suppose if you take fairly recent un-BIPped consensus changes in Core
>> to be the definition of consensus, you would be right about that.
>> 
>> 
>>> or a hardfork.
>> 
>> And those changes could definitely result in a chain split. So right
>> about that too.
>> 
>> 
>>> You could replace the already-on-chain tx with the collision and
>> create 2 different versions of UTXOs (if the colliding tx is valid), or
>> make some nodes to accept a fork with less PoW (if the colliding tx is
>> invalid, or making the block invalid, such as being to big).
>> 
>> 
>> Not in accordance with BIP30 and not according to the implementation of
>> it that existed in Core until Nov 2015. A tx was only valid as a
>> "replacement" if it did not collide with the hash of an existing tx with
>> unspent outputs. The collision would have been rejected. And an invalid
>> colliding tx would not be accepted in any case (since nodes presumably
>> validate blocks and don't rely on checkpoints as a security measure).
>> 
>> A transaction duplicating the hash of another and taking its place in a
>> block would not only have to collide the hash, but it would have to be
>> fully valid in the context of the block you are suggesting it is
>> substituted into. In that case it's simply a fully valid block. This is
>> not just the case of a hash collision, this is the case of a hash
>> collision where both transactions are fully valid in the context of the
>> same block parent. Even if that unlikely event did occur, it's not a
>> hard fork, it's a reorg. The chain that builds on this block will be
>> valid to all nodes but necessarily deviates from the other block's valid
>> chain. This is true whether the magical block is assembled via compact
>> blocks or otherwise.
>> 
>> Transaction "replacement" is an implementation detail of Core. Once Core
>> accepted a replacement of a previously spent transaction it would be
>> unable to provide the previous block/spent-tx, but that would be a
>> wallet failure and an inability to provide valid historical blocks, not
>> a consensus/validation failure. The previously spent outputs no longer
>> contribute to validation, unless there is a reorg back to before the
>> original tx's block, and at that point it would be moot, since neither
>> transaction is on the chain.
>> 
>> You are referring to the *current* behavior ("replacement" without
>> concern for collision). That was an unpublished hard fork, and is the
>> very source of the problems you are describing.
>> 
>>> To put it simply, the Bitcoin protocol is broken. So with no doubt,
>> Bitcoin Core and any implementation of the Bitcoin protocol should
>> assume SHA256 collision is unquestionably **impossible**.
>> 
>> I'm not disagreeing with you that it is broken. I'm pointing out that it
>> was broken by code that was merged recently - an undocumented hard fork
>> that reverted the documented BIP30 behavior that was previously
>> implemented correctly, based on the assumption that hash collisions
>> cannot occur, for the modest performance boost of not having to check
>> for unspent duplicates (sounds sort of familiar).
>> 
>>> If some refuse to make such assumption, they should have introduced an
>> alternative hash algorithm and somehow run it in parallel with SHA256 to
>> prevent the consensus failure.
>> 
>> No hash algorithm can prevent hash collisions, including one that is
>> just two running in parallel. A better resolution would be to fix the
>> problem.
>> 
>> There is no need to replace the BIP30 rule. That resolves the TX hash
>> collision problem from a consensus standpoint. In order to serve up
>> whole blocks in the circumstance requires a more robust store than I
>> believe is exists in Core, but that has nothing to do with validity.
>> 
>> The block hash check and signature validation caching splits caused by
>> collision can easily be avoided, and doing so doesn't break with
>> consensus. I'm not aware of any other aspects of consensus that are
>> effected by an implementation assumption of non-colliding hashes. But in
>> any case I'm pretty sure there aren't any that are necessary to consensus.
>> 
>> e
> 
> 


-------------------------------------
"We can look at the adoption of the last major Bitcoin core release to
guess how long it might take people to upgrade. 0.11.0 was released on 12
July, 2015. Twenty eight days later, about 38% of full nodes were running
that release. Three months later, about 50% of the network was running that
release, and six months later about 66% of the network was running some
flavor of 0.11."

On what grounds do you think it is reasonable to assume that this update
will roll out 6x faster than previous data suggested, as oppose to your own
observation of 66% adoption in 6 month. or do you believe 38% node
upgrade-coverage ( in 28 days ) on the network for a hard fork is good
enough?

There are no harm in choosing a longer grace period but picking one short
as 28 days you risk on alienating the nodes who do not upgrade with the
aggressive upgrade timeline you proposed.



On Fri, Feb 5, 2016 at 3:51 PM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> This has been reviewed by merchants, miners and exchanges for a couple of
> weeks, and has been implemented and tested as part of the Bitcoin Classic
> and Bitcoin XT implementations.
>
> Constructive feedback welcome; argument about whether or not it is a good
> idea to roll out a hard fork now will be unproductive, so I vote we don't
> go there.
>
> Draft BIP:
>   https://github.com/gavinandresen/bips/blob/bump2mb/bip-bump2mb.mediawiki
>
> Summary:
>   Increase block size limit to 2,000,000 bytes.
>   After 75% hashpower support then 28-day grace period.
>   With accurate sigop counting, but existing sigop limit (20,000)
>   And a new, high limit on signature hashing
>
> Blog post walking through the code:
>   http://gavinandresen.ninja/a-guided-tour-of-the-2mb-fork
>
> Blog post on a couple of the constants chosen:
>   http://gavinandresen.ninja/seventyfive-twentyeight
>
> --
> --
> Gavin Andresen
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
*Yifu Guo*
*"Life is an everlasting self-improvement."*

-------------------------------------
On Saturday, 26 November 2016 15:35:49 CET Peter R via bitcoin-dev wrote:
> Therefore, it is in the best interest of miners to all set the same block
> size limit (and reliably signal in their coinbase TX what that limit is,
> as done by Bitcoin Unlimited miners).

As a point of interest, last week I merged into Classic the same concept. 
Classic will now respect the EB limit and put it in the coinbase.

>  (This actually surprised me because the only way they could lose money is
>  if some _other_ miner wasted even more money by purposely mining a
>  destined-to-be-orphaned block.)

Your surprise may come from the difference in cost vs. expected earnings of 
creating a block, which is quite significant.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Mon, Feb 1, 2016 at 6:08 PM, Luke Dashjr <luke@dashjr.org> wrote:
> On Monday, February 01, 2016 9:43:33 PM Cory Fields wrote:
>> On Mon, Feb 1, 2016 at 2:46 PM, Luke Dashjr <luke@dashjr.org> wrote:
>> > Allowing for simpler cases both encourages the lazy case, and enables
>> > pools to require miners use it. It also complicates the server-side
>> > implementation somewhat, and could in some cases make it more vulnerable
>> > to DoS attacks. Keep in mind that GBT is not merely a bitcoind protocol,
>> > but is used between pool<->miner as well... For now, it makes sense to
>> > leave
>> > "default_witness_commitment" as a bitcoind-specific extension to
>> > encourage adoption, but it seems better to leave it out of the standard
>> > protocol. Let me know if this makes sense or if I'm overlooking
>> > something.
>>
>> I think that's a bit of a loaded answer. What's to keep a pool from
>> building its own commitment and requiring miners to use that? I don't
>> see how providing the known-working commitment for the
>> passed-in-hashes allows the pool/miner to do anything they couldn't
>> already, with the exception of skipping some complexity. Please don't
>> confuse encouraging with enabling.
>
> Making it simpler to do a centralised implementation than a decentralised one,
> is both enabling and encouraging. GBT has always been designed to make it
> difficult to do in a centralised manner.
>

But your suggestion is "use libblkmaker" which will build the trees
for me. By that logic, isn't libblkmaker making a centralized
implementation easier? Shouldn't that usage be discouraged as well?
And along those lines, shouldn't the fact that it's used as a pool <->
miner protocol be discouraged rather than touted as a feature?

I don't wish to sound hostile, I'm just trying follow the logic. I
can't rationalize why GBT shouldn't expose the commitment that it
knows to be correct (when paired with the transactions it provides),
purely to make things difficult.

>> What's the DoS vector here?
>
> It's more work for the pool to provide it, similar to the "midstate" field was
> with getwork. Someone performing a DoS needs to do less work to force the pool
> to do complex calculations (unless the same transaction tree / commitment is
> used for all miners, which would be an unfortunate limitation).

It's being provided to them. And if they're using a modified set of
tx's, they'll need to re-calculate it in order to verify the result
anyway. I suspect I'm not understanding this argument.

>
>> >> The issue in particular here is that a non-trivial burden is thrust
>> >> upon mining software, increasing the odds of bugs in the process.
>> >
>> > It can always use libblkmaker to handle the "heavy lifting"... In any
>> > case, the calculation for the commitment isn't significantly more than
>> > what it must already do for the stripped merkle tree.
>>
>> Agreed. However for the sake of initial adoption, it's much easier to
>> have a known-correct value to use. Even if it's just for the sake of
>> checking against.
>
> Sure, I'm not suggesting we remove this from bitcoind (probably the only place
> that makes initial adoption easier).
>

How about exposing it as a feature/capability, then? That way pools
can expect it from bitcoind, but won't be required to expose it
downstream.

>> >> [4]:
>> >> https://github.com/theuni/ckpool/commit/7d84b1d76b39591cc1c1ef495ebec513
>> >> cb 19a08e
>> >
>> > I'm pretty sure this commit is actually /introducing/ a bug in working
>> > (albeit ugly) code. The height, while always positive, is serialised as
>> > a signed number, so 0x80 needs to be two bytes: 80 00.
>>
>> You're right, thanks. The current code breaks on heights of (for ex)
>> 16513. I'll fix up my changes to take the sign bit into account.
>
> I'm curious what bug it was fixing? Was it overwriting data beyond the number?

Using 16513 as an example:

serialized by bitcoind: 0x028140
serialized by ckpool: 0x03814000

ckpool works because blocks after 32768 end up looking the same, but
it will break again at 2113664.

>
> Luke


-------------------------------------
Thanks, Anthony, that works!

So...

How many years until we think a 2^84 attack where the work is an ECDSA
private->public key derivation will take a reasonable amount of time?

And Ethan or Anthony:  can you think of a similar attack scheme if you
assume we had switched to Schnorr 2-of-2 signatures by then?


And to everybody who might not be reading this closely:  All of the above
is discussing collision attacks; none of it is relevant in the normal case
where your wallet generates the scriptPubKey.



-- 
--
Gavin Andresen

-------------------------------------
The following won't be directly applicable to your question without some
kind of tremendous hacking on your part: but in cryptography there is
actually a way to sign a message using only hash functions.

If you're interested look up the definition for "Lamport Signatures." It's
an algorithm for masking the bits of a message by exchanging a table of
hashes prior to signing and then revealing the "secrets" behind said hashes
in such a way that you can selectively mask the bits of the message hash
that you're wishing to sign.

It's actually a really cool algorithm and probably the most elegant thing
I've ever seen in the way of digital signatures - and besides just being
something that's really cool: Lamport Signatures have the advantage of
being quantum safe too (if you care about that kind of thing.)

To actually answer your question indirectly: you would use a consensus
system that understands Lamport signature operations in the "scriptPubKey"
(Ethereum could probably do this now.) And note that as Nick ODell has
already said: using the hashes alone in this scheme won't work since the
moment you publish the transactions with said hash secrets anyone is then
free to pluck out those values and double spend the original transaction to
a new destination (and this is actually the reason why Peter Todd's
proof-of-hash collision scheme in Bitcoin is insecure but still allows us
to incentivize whether or not there may be a flaw with the recent SHA
algorithms.)

Regarding hash protected M of N multi-sig: there is already a similar smart
contract algorithm called "atomic cross-chain contracts" that relies on
hash values to be released as part of the protocol to swap coins across
blockchains but said algorithm also uses ECDSA public keys to prevent the
transactions from being double-spent. So in Bitcoin Multi-sig using hash
values will work - though you still need to include an ECDSA pub key to
protect them from attackers on the network.

(I hope this helps. You didn't say much about the intended use-case for
this.)

On Fri, Dec 23, 2016 at 4:29 AM, Andrew via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi
>
> Is there a worked out scriptPubKey for doing multisig with just hashes
> of the participants? I think it is doable and it is more secure to a
> compromised ECDSA. I'm thinking something like this for the
> scriptPubKey:
>  2 OP_SWAP OP_SWAP OP_SWAP OP_DUP OP_HASH160 <pubKeyHash1>
> OP_EQUALVERIFY OP_DUP OP_HASH160 <pubKeyHash2> OP_EQUALVERIFY OP_DUP
> OP_HASH160 <pubKeyHash3> OP_EQUALVERIFY 3 OP_CHECKMULTISIG
>
> and <sigs><pubkeys> for the scriptSig
>
> Can anyone confirm or send me a link to the worked out script?
>
> Thanks
>
> --
> PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
https://github.com/bitcoin/bips/pull/314 proposes updating the status of many 
Accepted BIPs to Final:

BIP 11: M-of-N Standard Transactions
BIP 14: Protocol Version and User Agent
BIP 21: URI Scheme
BIP 22: getblocktemplate - Fundamentals
BIP 23: getblocktemplate - Pooled Mining
BIP 31: Pong message
BIP 32: Hierarchical Deterministic Wallets
BIP 34: Block v2, Height in Coinbase
BIP 35: mempool message
BIP 37: Connection Bloom filtering
BIP 65: OP_CHECKLOCKTIMEVERIFY

This PR has been open for a week, and I plan to merge it within the next week 
unless there are objections.

Additionally, https://github.com/bitcoin/bips/pull/315 proposes to upgrade 
five additional from Draft to Final status, and preferably needs ACKs from the 
champions of the BIPs:

BIP 50: March 2013 Chain Fork Post-Mortem, by Gavin Andresen
BIP 60: Fixed Length "version" Message (Relay-Transactions Field), by Amir
        Taaki
BIP 64: getutxo message, by Mike Hearn
BIP 66: Strict DER signatures, by Pieter Wuille
BIP 73: Use "Accept" header for response type negotiation with Payment Request
        URLs, by Stephen Pair

Thanks,

Luke


-------------------------------------
On 2/10/2016 5:53 AM, Henning Kopp wrote:
> Hi Jeremy,
>
>> My understanding of the paper is that the blinding factor would be included
>> in the extra data which is incorporated into the ring signatures used in the
>> range proof.
> Yep, that is a possibility. The blinding factor could be encrypted
> with the public key of the receiver. Thus it is only visible for the
> receiver who can then check that the correct amount has been sent.
ECC doesn't work like RSA; you can't encrypt directly with a public 
key.  That's why you generate a shared secret between sender and 
receiver.  See also, ECDH. (Basically, if (m, M = m*G) is your 
private/public key pair, and (n, N = n*G) is your recipient's private 
public key pair, you can both generate shared secret S = m*N = n*M = 
m*n*G without revealing your private keys to each other, and without 
revealing the secret to anyone else as long as they don't know either 
private key. You then use S as the basis for the key to some symmetric 
algorithm.)
>> you'd transmit it then, though in any case, since using it will pretty much
>> require segwit, adding extraneous data isn't much of a problem.  In both
>> cases, I imagine the blinding factor would be protected from outside
>> examination via some form of shared secret generation... Although that would
>> require the sender to know the recipient's unhashed public key; I don't know
>> of any shared secret schemes that will work on hashed keys.
> Here you lost me.
> Why do we need to create a shared secret? Is this shared secret used
> as the blinding factor?
> Also I think the sender knows the unhashed public key of the receiver.
> The only reason not to include it in the transaction script is that an
> external observer is unable to see the receiver directly in the
> blockchain.
Normal Bitcoin transactions are made to the hash of a public key because 
once the public key is known, it becomes easier to break it if we ever 
develop quantum computers. That's why it's recommended that you only 
spend from a particular address once (if possible) since its only in 
spending that you are required to reveal your public key.   Since you 
can't do a shared secret with a public key hash, AFAIK, you'd have to 
know the public key of your recipient to be able to do ECDH.

Jeremy Papp


-------------------------------------
The assumption you're making is incorrect. There is not an infinite number
of low-fee transactions.

Yes, the average fee will go down compared to today with Block75, but this
will balance itself between demand and the minimum fee miners are willing
to accept (not zero).

For example, add 200kb to today's max block size. How does that affect fees?
(200kb would likely be the first increase if Block75 activated today)

-t.k.

On Sun, Dec 11, 2016 at 4:55 PM, James Hilliard <james.hilliard1@gmail.com>
wrote:

> I think the main thing you're missing is that there will always be
> transactions available to mine simply because demand for blockspace is
> effectively unbounded as fees approach 0. Nodes generally have a
> static mempool size and dynamic minrelaytxfee nowadays so as
> transactions get mined lower fee transactions get accepted into the
> mempool. An individual opting to not send a transaction would not make
> the blocks smaller simply because there will always be other
> transactions available(it would really only have an effect on the
> transaction fees needed to get mined).
>
> On Sun, Dec 11, 2016 at 3:40 PM, t. khan <teekhan42@gmail.com> wrote:
> >
> > On Sun, Dec 11, 2016 at 3:31 PM, James Hilliard <
> james.hilliard1@gmail.com>
> > wrote:
> >>
> >> What's most likely to happen is miners will max out the blocks they
> >> mine simply to try and get as many transaction fees as possible like
> >> they are doing right now(there will be a backlog of transactions at
> >> any block size). Having the block size double every year would likely
> >> cause major problems and this proposal allows over a 7x increase it
> >> seems.
> >
> >
> > Block75 is not exponential scaling. It's true the max theoretical
> increase
> > in the first year would be 7x, but the next year would be a max of 2x,
> and
> > the next could only increase by 50% and so on.
> >
> > However, to reach the max in the first year: 1) ALL blocks would have to
> be
> > 100% full and 2) transactions would have to increase at the same rate.
> We'd
> > have to be doing 2.1 million transactions a day within a year to make
> that
> > happen, and would therefore need blocks to be that big.
> >
> > Realistically, max block size will grow (and shrink) at a much slower
> rate
> > ... even more so with SegWit.
> >
> >>
> >>  The main problem with this proposal I think is that users effectively
> >>
> >> have no way to stop the miners from increasing block size
> >> continuously.
> >
> >
> > Yes they could, simply by not sending transactions. Users don't care at
> all
> > about block size. They just want their transactions to be fast and
> > relatively cheap.
> >
> > -t.k.
>

-------------------------------------
On Sun, Oct 16, 2016 at 04:31:55PM +0200, Pieter Wuille via bitcoin-dev wrote:
> Hello all,
> 
> We're getting ready for Bitcoin Core's 0.13.1 release - the first one
> to include segregated witness (BIP 141, 143, 144, 145) for Bitcoin
> mainnet, after being extensively tested on testnet and in other
> software. Following the BIP9 recommendation [1] to set the versionbits
> start time a month in the future and discussion in the last IRC
> meeting [2], I propose we set BIP 141's start time to November 15,
> 2016, 0:00 UTC (unix time 1479168000).

Speaking as maintainer of python-bitcoinlib, ACK.

Currently python-bitcoinlib doesn't have any support for segwit, although Bob
McElrath has had a pull-req open for it since July:

    https://github.com/petertodd/python-bitcoinlib/pull/112

I may or may not get time to finishing reviewing and merging that pull-req
before segwit activates - I've been a rather distracted maintainer. But either
way, as has been explained elsewhere ad nauseam, segwit is backwards compatible
with existing nodes and wallets so there's no rush to upgrade.

For example, another project of mine - OpenTimestamps - also makes use of
python-bitcoinlib for the relatively complex and hairy low-level code that
extracts timestamp proofs from blocks, among other things. In fact, in the
development of OpenTimestamps I had to fix a few minor bugs in
python-bitcoinlib, because it exercised parts of the codebase that few other
projects do.

Yet the impact on segwit for OpenTimestamps will be zero - since segwit is a
softfork it's 100% backwards compatible with existing software. Of course, at
some point in the future I'll probably get around to adding segwit support to
the software to reduce transaction fees, but there's no rush to do so. All I'll
be doing for segwit in the near future is upgrading the full nodes on the two
redundant OpenTimestamps calendar servers to v0.13.1, and even there I'll be
able to stagger the upgrades to protect against the unlikely occurance of
v0.13.1 having a bug that v0.13.0 doesn't. Again, staggering full-node upgrades
is only possible because segwit is a soft-fork.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Tuesday, January 26, 2016 3:17:12 AM Toby Padilla wrote:
> I don't think every application of OP_RETURN could be classified as "spam".

Perhaps not, but in this context I cannot think of any non-spam use cases.
Use cases should come before changes to support them.

> I also don't think burning the value is going to dissuade anyone from going
> down that route. I don't think lost value is better for anyone.

Lost value is better because it has a cost to the spammer, and deflates the 
rest of the bitcoins.

Luke


-------------------------------------
I'm not a lawyer, and my knowledge on patents is limited. I guess RSK WILL
endorse DPL or will make the required actions to make sure the things
developed by RSK remain free and open. This was not a priority until now,
but coding was. For me, coding always is the priority.

I will discuss prioritizing this with the team. Remember it took several
years to BlockStream to decide for DPL and not just publish everything as
they were doing. I suppose the decision it was not a simple one, involving
lawyers advise and all. I guess DPL needs to actually patent the things in
order to open them later, and patenting has a very high cost.

Give us time to decide which open source strategy is the best and cheaper
for RSK. At this point I can assert that RSK has not filed any patent not
is planing to.  This proposal is not encumbered by any patents, and
drivechains is actually not RSK's idea, but Paul Sztorc's.



On Sun, Oct 2, 2016 at 2:11 PM, Peter Todd <pete@petertodd.org> wrote:

> On Sun, Oct 02, 2016 at 02:00:01PM -0300, Sergio Demian Lerner wrote:
> > Peter, are you really going to try to down vote a decent free and
> > open-source proposal that benefits all the Bitcoin community including
> > you and your future children because a personal attack to me without any
> > logic or basis?
>
> I've suggested a way that you can rectify this situation so we can
> continue to
> collaborate: Have Rootstock adopt a legally binding patent pledge/license.
> I'd
> suggest you do as Blockstream has done and at minimum adopt the Defensive
> Patent License (DPL); I personally will be doing so in the next week or
> two for
> my own consulting company (I'm discussing exactly how to do so with my
> lawyer
> right now).
>
> If Rootstock is not planning on getting any patents for offensive purposes,
> then there is no issue with doing so - the DPL in particular is designed
> in a
> minimally intrusive way.
>
> Please fix this issue so we can in fact continue to collaborate to improve
> Bitcoin.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>

-------------------------------------
On Tuesday, February 02, 2016 11:28:40 PM Dave Scotese wrote:
> How about "defining" (rules, code, etc.) Such code and rules define what
> bitcoin is.  It does require consensus and it ends up being a concord, but
> all that can come after the fact (just as it did after bitcoin was first
> released to the public).

The difficulty is that this BIP needs to refer to three different context of 
consensus:

1. Consensus (stated) among developers for changes in the BIP Process.
2. Economic consensus (potential and stated) to veto a soft-fork by an
   intended "firing" of the set of miners if they choose to enforce it.
3. (Actual) consensus in economic adoption of changed rules, to determine the
   success of a hard-fork (after the fact).
4. The set of rules currently established as (defining) Bitcoin, enforced by
   an (actual) consensus of economically-relevant nodes.

Context 3 can be disambiguated with "adoption consensus", and context 4 with 
"consensus rules" and/or "consensus protocol", but I don't see a clear 
solution that covers all four contexts, and even sharing the word "consensus" 
for them may be confusing.

In addition, usage of the word "consensus" for context 4 has proven confusing 
to users. For example, recently users misinterpreted the "Consensus" label 
used in context 4 as implying that the idea itself had in fact achieved 
consensus among some group of decision-makers (similar to context 1, but not 
necessarily the group being "developers").

I don't know a good way to make this completely clear, so suggestions are more 
than welcome.

Luke


-------------------------------------
On Sunday, 16 October 2016 17:19:37 CEST Andrew C wrote:
> On 10/16/2016 4:58 PM, Tom Zander via bitcoin-dev wrote:
> > Lets get back to the topic. Having a longer fallow period is a simple
> > way to be safe.  Your comments make me even more scared that safety is
> > not taken into account the way it would.
> 
> Can you please explain how having a longer grace period makes it any
> safer? Once the fork reaches the LOCKED_IN status, the fork will become
> active after the period is over. How does having a longer grace period
> make this any safer besides just adding more waiting before it goes
> active? 

As Marek wrote just minutes before your email came in; companies will not 
roll out their updates until it locks in. Peter Todd says the same thing.
So your assumption that the lock-in time is the END of the upgrading is 
false. Thats only the case for miners.

The entire point here is that SegWit is much bigger than just full nodes 
(including miner).
An entire ecosystem of Bitconin will have a need to upgrade.

I understand people saying that non-miners don't *need* to upgrade in order 
to avoid being kicked of the network, but truely, thats a bogus argument.

People want to actually participate in Bitcoin and that means they need to 
understand all of it. Not just see anyone-can-spend transactions.
I think its rather odd to think that developers on this list can decide
the risk profile that Bitcoin using companies or individuals should have.


> You said something about rolling back the changes. There is no
> mechanism for roll backs, and the whole point of the soft fork
> signalling is such that there is no need to roll back anything because
> miners have signaled that they are supporting the fork.

There are a bunch of really large assumptions in there that I disagree with.
First of all, miners running the latest software doesn't mean the software 
is free from flaws. Everyone makes mistakes. People that see a way to abuse 
the system may have found things and are not reporting it because they want 
to abuse the flaw for their own gains. Which is exactly what happened with 
Etherium.

The amount of code and the amount of changes in SegWit makes this a very 
dangerous change in (of?) Bitcoin. I counted 10 core concepts in Bitcoin 
being changed with it. We don't yet know how they will interact. We can’t.

You are asking people to create everyone-can-spend transactions that would 
mean a loss of funds to everyone that used it if we do find a major flaw and 
need to rollback.

The gamble is rather uncomforable to many...
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Thursday, May 26, 2016 2:50:26 AM Nicolas Dorier via bitcoin-dev wrote:
>   Author: Flavien Charlon <flavien@charlon.net>

Is he the author of this BIP, or merely the protocol described in it?
Would it perhaps make sense to include yourself in the author list?

> The ID of an asset is the RIPEMD-160 hash of the SHA-256 hash of the
> output script referenced by the first input of the transaction that
> initially issued that asset (<code>script_hash =
> RIPEMD160(SHA256(script))</code>). An issuer can reissue more of an
> already existing asset as long as they retain the private key for that
> asset ID. Assets on two different outputs can only be mixed together
> if they have the same asset ID.

Quite a bit ugly, giving a meaning to an input's pubkey script like that.
But more problematically: how can this work with other pubkey scripts? 
Particularly relevant now that this old script format is being deprecated.

Another possible problem is that I don't see a way to provably guarantee an 
asset issuance is final.

> Transactions that are not recognized as an Open Assets transaction are
> considered as having all their outputs uncolored.

And the assets attached to its inputs are destroyed? Or?

> If multiple parsable PUSHDATA opcodes exist in the same output, the
> first one is used, and the other ones are ignored.
> 
> If multiple valid marker outputs exist in the same transaction, the
> first one is used and the other ones are considered as regular
> outputs.

Is it intentional that the first case is "parsable", and the second "valid"?
I think these need to be better specified; for example, it is not so clear how 
to reach if the OAP version number is something other than 1: is that 
parsable? valid?

> ! Asset quantity count || A
> [https://en.bitcoin.it/wiki/Protocol_specification#Variable_length_integer
> var-integer] representing the number of items in the <code>asset quantity
> list</code> field. || 1-9 bytes
> 
> |-
> 
> ! Asset quantity list  || A list of zero or more
> [http://en.wikipedia.org/wiki/LEB128 LEB128-encoded] unsigned integers
> representing the asset quantity of every output in order (excluding the
> marker output). || Variable

What determines the asset id? How would one issue and/or transfer multiple 
asset ids in the same transaction?

> The marker output is always uncolored.

What if I have a transaction with 5 outputs, the marker output at position 3, 
and all 4 other outputs are to receive assets? Does the marker output get 
skipped in the list (ie, the list is 4 elements long) or must it be set to 
zero quantity (ie, the list is 5 elements long)?

> Inputs are seen as a sequence of asset units, each having an asset ID.
> Similarly, outputs are seen as a sequence of asset units to be
> assigned an asset ID. These two sequences are built by taking each
> input or output in order, each of them adding a number of asset units
> equal to their asset quantity. The process starts with the first input
> of the transaction and the first output after the marker output.
> 
> After the sequences have been built, the asset ID of every asset unit
> in the input sequence is assigned to the asset unit at the same
> position in the output sequence until all the asset units in the
> output sequence have received an asset ID. If there are less asset
> units in the input sequence than in the output sequence, the marker
> output is considered invalid.
> 
> Finally, for each transfer output, if the asset units forming that
> output all have the same asset ID, the output gets assigned that asset
> ID. If any output is mixing units with more than one distinct asset
> ID, the marker output is considered invalid. Outputs with an asset
> quantity of zero are always considered uncolored.

I don't understand this.

> # This approach uses the recommended way to embed data in the Blockchain
> (OP_RETURN), and therefore does not pollute the UTXO.

Embedding data is not recommended at all. It seems a better way to have done 
this would be to put the info in an OP_DROP within a P2SH or witness script.

> # The whole cryptographic infrastructure that Bitcoin provides for
> securing the spending of outputs is reused for securing the ability to
> issue assets. There is a symmetry between ''an address + private key''
> as a way to spend Bitcoins, and ''an address + private key'' as a way
> to issue assets.

Addresses are not used for spending bitcoins, only for receiving them. The way 
this BIP uses inputs' pubkey script is extremely unusual and probably a bad 
idea.

> # Reissuing more of an existing asset is easy and can be done quickly
> and at no cost (except for the transaction fee) as long as the issuer
> retains the private key for the asset ID.

As I understand it, this would require address reuse to setup, which is not 
supported behaviour and insecure.

> For backward compatibility reasons, we consider than an older client
> is allowed to see a colored output as uncolored.

How is this compatible? Won't an older client then accidentally destroy 
assets?

Luke


-------------------------------------

> Yes, this is exactly what I meant. The complexity of the proposed construction is comparable to that of Bitcoin itself. This is not itself prohibitive, but it is clearly worthy of consideration.
> 
> A question we should ask is whether decentralized anonymous credentials is applicable to the authentication problem posed by BIP151. I propose that it is not.
> 
> The core problem posed by BIP151 is a MITM attack. The implied solution (BIP151 + authentication) requires that a peer trusts that another is not an attacker. 

BIP151 would increase the risks for MITM attackers.
What are the benefits for Mallory of he can't be sure Alice and Bob may
know that he is intercepting the channel?

MITM is possible today, it would still be possible (though under higher
costs) with BIP151.

With BIP151 we would have the basic tool-set to effectively reduce the
risks of being MITMled.

IMO we should focus on the risks and benefits of BIP151 and not drag
this discussion into the realm of authentication. This can and should be
done once we have proposals for authentication (and I'm sure this will
be a heated debate).

The only valid risk I have on my list from you, Eric, is the false sense
of security.

My countermeasure for that would be...
- deploy BIP151 together with the simplest form of authentication
(know_hosts / authorized_keys file, no TOFU only editable "by hand")
- make it more clear (in the BIP151 MOTIVATION text) that it won't solve
the privacy/MITM problem without additional authentication.

Or could you elaborate again  without stepping into the realm of
authentication/MITM (which is not part of the BIP or possible already
today)  why BIP151 would make things worse?

</jonas>


-------------------------------------
Hi Henning,

1. The fees are paid by the enclosing BTC transaction.
2. The hash is encoded into an OP_RETURN.

> Regarding the blinding factor, I think you could just use HMAC.
How exactly?

Tony


2016-08-08 18:47 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:

> Hi Tony,
>
> I see some issues in your protocol.
>
> 1. How are mining fees handled?
>
> 2. Assume Alice sends Bob some Coins together with their history and
> Bob checks that the history is correct. How does the hash of the txout
> find its way into the blockchain?
>
> Regarding the blinding factor, I think you could just use HMAC.
>
> All the best
> Henning
>
>
> On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via bitcoin-dev
> wrote:
> > This is a proposal about hiding the entire content of bitcoin
> > transactions.  It goes farther than CoinJoin and ring signatures, which
> > only obfuscate the transaction graph, and Confidential Transactions,
> which
> > only hide the amounts.
> >
> > The central idea of the proposed design is to hide the entire inputs and
> > outputs, and publish only the hash of inputs and outputs in the
> > blockchain.  The hash can be published as OP_RETURN.  The plaintext of
> > inputs and outputs is sent directly to the payee via a private message,
> and
> > never goes into the blockchain.  The payee then calculates the hash and
> > looks it up in the blockchain to verify that the hash was indeed
> published
> > by the payer.
> >
> > Since the plaintext of the transaction is not published to the public
> > blockchain, all validation work has to be done only by the user who
> > receives the payment.
> >
> > To protect against double-spends, the payer also has to publish another
> > hash, which is the hash of the output being spent.  We’ll call this hash
> *spend
> > proof*.  Since the spend proof depends solely on the output being spent,
> > any attempt to spend the same output again will produce exactly the same
> > spend proof, and the payee will be able to see that, and will reject the
> > payment.  If there are several outputs consumed by the same transaction,
> > the payer has to publish several spend proofs.
> >
> > To prove that the outputs being spent are valid, the payer also has to
> send
> > the plaintexts of the earlier transaction(s) that produced them, then the
> > plaintexts of even earlier transactions that produced the outputs spent
> in
> > those transactions, and so on, up until the issue (similar to coinbase)
> > transactions that created the initial private coins.  Each new owner of
> the
> > coin will have to store its entire history, and when he spends the coin,
> he
> > forwards the entire history to the next owner and extends it with his own
> > transaction.
> >
> > If we apply the existing bitcoin design that allows multiple inputs and
> > multiple outputs per transaction, the history of ownership transfers
> would
> > grow exponentially.  Indeed, if we take any regular bitcoin output and
> try
> > to track its history back to coinbase, our history will branch every time
> > we see a transaction that has more than one input (which is not
> uncommon).
> > After such a transaction (remember, we are traveling back in time), we’ll
> > have to track two or more histories, for each respective input.  Those
> > histories will branch again, and the total number of history entries
> grows
> > exponentially.  For example, if every transaction had exactly two inputs,
> > the size of history would grow as 2^N where N is the number of steps back
> > in history.
> >
> > To avoid such rapid growth of ownership history (which is not only
> > inconvenient to move, but also exposes too much private information about
> > previous owners of all the contributing coins), we will require each
> > private transaction to have exactly one input (i.e. to consume exactly
> one
> > previous output).  This means that when we track a coin’s history back in
> > time, it will no longer branch.  It will grow linearly with the number of
> > transfers of ownership.  If a user wants to combine several inputs, he
> will
> > have to send them as separate private transactions (technically, several
> > OP_RETURNs, which can be included in a single regular bitcoin
> transaction).
> >
> > Thus, we are now forbidding any coin merges but still allowing coin
> > splits.  To avoid ultimate splitting into the dust, we will also require
> > that all private coins be issued in one of a small number of
> > denominations.  Only integer number of “banknotes” can be transferred,
> the
> > input and output amounts must therefore be divisible by the denomination.
> > For example, an input of amount 700, denomination 100, can be split into
> > outputs 400 and 300, but not into 450 and 250.  To send a payment, the
> > payer has to pick the unspent outputs of the highest denomination first,
> > then the second highest, and so on, like we already do when we pay in
> cash.
> >
> > With fixed denominations and one input per transaction, coin histories
> > still grow, but only linearly, which should not be a concern in regard to
> > scalability given that all relevant computing resources still grow
> > exponentially.  The histories need to be stored only by the current owner
> > of the coin, not every bitcoin node.  This is a fairer allocation of
> > costs.  Regarding privacy, coin histories do expose private transactions
> > (or rather parts thereof, since a typical payment will likely consist of
> > several transactions due to one-input-per-transaction rule) of past coin
> > owners to the future ones, and that exposure grows linearly with time,
> but
> > it is still much much better than having every transaction immediately on
> > the public blockchain.  Also, the value of this information for potential
> > adversaries arguably decreases with time.
> >
> > There is one technical nuance that I omitted above to avoid distraction.
> >  Unlike regular bitcoin transactions, every output in a private payment
> > must also include a blinding factor, which is just a random string.  When
> > the output is spent, the corresponding spend proof will therefore depend
> on
> > this blinding factor (remember that spend proof is just a hash of the
> > output).  Without a blinding factor, it would be feasible to pre-image
> the
> > spend proof and reveal the output being spent as the search space of all
> > possible outputs is rather small.
> >
> > To issue the new private coin, one can burn regular BTC by sending it to
> > one of several unspendable bitcoin addresses, one address per
> denomination.
> >  Burning BTC would entitle one to an equal amount of the new private
> coin,
> > let’s call it *black bitcoin*, or *BBC*.
> >
> > Then BBC would be transferred from user to user by:
> > 1. creating a private transaction, which consists of one input and
> several
> > outputs;
> > 2. storing the hash of the transaction and the spend proof of the
> consumed
> > output into the blockchain in an OP_RETURN (the sender pays the
> > corresponding fees in regular BTC)
> > 3. sending the transaction, together with the history leading to its
> input,
> > directly to the payee over a private communication channel.  The first
> > entry of the history must be a bitcoin transaction that burned BTC to
> issue
> > an equal amount of BCC.
> >
> > To verify the payment, the payee:
> > 1. makes sure that the amount of the input matches the sum of outputs,
> and
> > all are divisible by the denomination
> > 2. calculates the hash of the private transaction
> > 3. looks up an OP_RETURN that includes this hash and is signed by the
> > payee.  If there is more than one, the one that comes in the earlier
> block
> > prevails.
> > 4. calculates the spend proof and makes sure that it is included in the
> > same OP_RETURN
> > 5. makes sure the same spend proof is not included anywhere in the same
> or
> > earlier blocks (that is, the coin was not spent before).  Only
> transactions
> > by the same author are searched.
> > 6. repeats the same steps for every entry in the history, except the
> first
> > entry, which should be a valid burning transaction.
> >
> > To facilitate exchange of private transaction data, the bitcoin network
> > protocol can be extended with a new message type.  Unfortunately, it
> lacks
> > encryption, hence private payments are really private only when bitcoin
> is
> > used over tor.
> >
> > There are a few limitations that ought to be mentioned:
> > 1. After user A sends a private payment to user B, user A will know what
> > the spend proof is going to be when B decides to spend the coin.
> >  Therefore, A will know when the coin was spent by B, but nothing more.
> >  Neither the new owner of the coin, nor its future movements will be
> known
> > to A.
> > 2. Over time, larger outputs will likely be split into many smaller
> > outputs, whose amounts are not much greater than their denominations.
> > You’ll have to combine more inputs to send the same amount.  When you
> want
> > to send a very large amount that is much greater than the highest
> available
> > denomination, you’ll have to send a lot of private transactions, your
> > bitcoin transaction with so many OP_RETURNs will stand out, and their
> > number will roughly indicate the total amount.  This kind of privacy
> > leakage, however it applies to a small number of users, is easy to avoid
> by
> > using multiple addresses and storing a relatively small amount on each
> > address.
> > 3. Exchanges and large merchants will likely accumulate large coin
> > histories.  Although fragmented, far from complete, and likely outdated,
> it
> > is still something to bear in mind.
> >
> > No hard or soft fork is required, BBC is just a separate privacy
> preserving
> > currency on top of bitcoin blockchain, and the same private keys and
> > addresses are used for both BBC and the base currency BTC.  Every BCC
> > transaction must be enclosed into by a small BTC transaction that stores
> > the OP_RETURNs and pays for the fees.
> >
> > Are there any flaws in this design?
> >
> > Originally posted to BCT https://bitcointalk.org/index.
> php?topic=1574508.0,
> > but got no feedback so far, apparently everybody was consumed with
> bitfinex
> > drama and now mimblewimble.
> >
> > Tony
>
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
> --
> Henning Kopp
> Institute of Distributed Systems
> Ulm University, Germany
>
> Office: O27 - 3402
> Phone: +49 731 50-24138
> Web: http://www.uni-ulm.de/in/vs/~kopp
>

-------------------------------------
On Wed, Jun 15, 2016 at 7:00 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Indeed, and you can go even further. When there are multiple "sending"
> outputs, pick one at random, and mimic it for the change output. This means
> that if you have a P2PKH and 3 P2SH sends, you'll have 25% chance for a
> P2PKH change output, and 75% chance for a P2SH output.
>

This isn't quite perfect because if there is only 1 P2PKH output and you
know the person is using the above algorithm then you know the P2PKH output
isn't the change.

I don't know what the perfect method is.  My guess is that it is to let p
be the probability that a P2PKH output is produced over the entire network
and to pick P2PKH for your change output with probability p (and similarly
for other output types).

On Wed, Jun 15, 2016 at 7:00 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> On Jun 15, 2016 12:53, "Daniel Weigl via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> >
> > That would be a big privacy leak, imo. As soon as both outputs are
> spent, its visible
> > which one was the P2WPKH-in-P2SH and which one the pure P2WPKH and as a
> consequence
> > you leak which output was the change and which one the actual sent output
> >
> > So, i'd suggest to even make it a requirement for "normal"
> send-to-single-address transactions
> > to always use the same output type for the change output (if the wallet
> is able to recognize it)
>
> Indeed, and you can go even further. When there are multiple "sending"
> outputs, pick one at random, and mimic it for the change output. This means
> that if you have a P2PKH and 3 P2SH sends, you'll have 25% chance for a
> P2PKH change output, and 75% chance for a P2SH output.
>
> You can go even further of course, if you want privacy that remains after
> those sends get spent. In that case, you also need to match the template of
> the redeemscript/witnessscript. For example, if the send you are mimicking
> is a 2-of-3, the change output should also use 2-of-3.
>
> --
> Pieter
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Sat, Sep 10, 2016 at 12:42:30AM +0000, Gregory Maxwell via bitcoin-dev wrote:
> The alert system was a centralized facility to allow trusted parties
> to send messages to be displayed in wallet software (and, very early
> on, actually remotely trigger the software to stop transacting).
> 
> It has been removed completely in Bitcoin Core after being disabled for a while.

As it has been disabled in relevant software I think it's mostly symbolic at
this point, but yes, it makes sense to 'officially' retire the key. Let's
pin the date and make it widely known.

Doing this in organized fashion is much better than the whodunit that would
undoubtly follow when the key would simply leak, which could happen at any
time, as no one can know who it has spread to over all those years.

Re: timing, I'd say leave three months grace time after this announcement for
altcoins and such that may have accidentally have copied it to remove it, then
at the beginning of 2017 broadcast the final alert.

After that it's neutered, it's up to each of us that has the key to reveal it
or not or when. It's a historical curiosity then.

Wladimir


-------------------------------------
Then the moderation is being unevenly applied. Luke commented against my
BIP multiple times right after it was published but it took hours for my
responses to go through and I had to track people down on IRC to ask about
it:

http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/thread.html

On Tue, Feb 2, 2016 at 9:38 AM, Peter Todd <pete@petertodd.org> wrote:

> On Tue, Feb 02, 2016 at 09:27:58AM -0800, Toby Padilla wrote:
> > The mailing list is a problem. I'm still on moderation only. I have no
> idea
> > if this message will go through and when it will go through. I totally
> > understand the desire to keep the conversation level high, but when
> people
> > who *are* whitelisted can quickly post multiple heated arguments against
> > you (publicly) and you can't respond, then that starts to look very
> > centralized and discouraging.
>
> Everyone is on moderation only in this mailing list, myself included.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
> 000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698
>

-------------------------------------
This BIP describes a new opcode (OP_CHECKBLOCKATHEIGHT) for the Bitcoin 
scripting system to address reissuing bitcoin transactions when the coins they 
spend have been conflicted/double-spent.

https://github.com/luke-jr/bips/blob/bip-cbah/bip-cbah.mediawiki

Does this seem like a good idea/approach?

Luke


-------------------------------------
On 06/22/2016 04:25 PM, Erik Aronesty via bitcoin-dev wrote:
> 
>     Only large merchants are able to maintain such an infrastructure; (even
>     Coinbase recently failed at it, they forgot to update their
>     certificate). For end users that is completely unpractical.
> 
> 
> Payment protocol is for when you buy stuff from purse.io
> <http://purse.io>, not really needed for face-to face transfers, end
> users, IMO.

What Andy said, plus there is an (unencrypted) version of BIP70 via
Bluetooth already in place. And its used in several thousand
face-to-face trades per day.




-------------------------------------
On Saturday, October 15, 2016 11:00:35 AM Tom Zander via bitcoin-dev wrote:
> My suggestion (sorry for not explaining it better) was that for BIPS to be
> a public domain (aka CC0) and a CC-BY option and nothing else.
> 
> I like you agree with that part, but I see you added two licenses.
> Do you have a good reason to add MIT/BSD to that list? Otherwise I think we
> agree.

BIPs often should include code.

> Well, it has this sentence;
> 
> > This BIP is dual-licensed under the Open Publication License and
> > BSD 2-clause license.
> 
> Which is a bit odd in light of the initial email from Luke that suggested
> we drop the Open Publication License and we use the CC ones instead in
> addition to the public domain one.

The "real" license in this case is the BSD 2-clause. However, BIP 1 only 
allows OPL and public domain, so BIP 2 is available under OPL as well so that 
it is acceptable before/until it activates also.

> Thats odd, you just stated you like the public domain (aka CC0) license,
> yet you encourage the BIP2 that states we can no longer use public domain
> for BIPs... Did you read it?

CC0 and public domain are two different things.

> This list has not seen a lot of traffic, if you want to make sure people
> keep using the BIP process, I think you need to reach out to the rest of
> the community and make sure this has been heard and discussed.
> Moving forward the way it is now will likely deminish the importance of the
> BIP process.

Yes, you're right. I'll post to Lightning-dev and libbitcoin's list about
BIP 2. If you're aware of any other Bitcoin development discussion groups, 
could you please bring BIP 2 to their attention so it gets wider review?

> 1) if you write as a rationale "In some jurisdictions, public domain is not
> recognised as a legitimate legal action" then you can at least name those
> jurisdictions and explain how they *do* support things like GPL. Burden of
> proof is on the man who wants to change things.

As I understand it, presently France and Germany do not recognise public 
domain as a possible status. GPL is merely a copyright license, so it should 
be valid anywhere copyright laws exist.

Luke


-------------------------------------
Please Peter Todd explain here all what you want to say about a patent of a
hardware design for an ASIC.

Remember that ASICBoost is not the only patent out there, there are at
least three similar patents, filed by major Bitcoin ASIC manufacturers in
three different countries, on similar technologies.

That suggest that the problem is not ASICBoot's: you cannot blame any
company from doing lawful commerce in a FREE MARKET.

It is a flaw in Bitcoin design that could be corrected if the guidelines I
posted in [1] had been followed.

[1]
https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/

-------------------------------------
On Wed, Nov 16, 2016 at 5:58 AM, Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> This sort of statement represents one consequence of the aforementioned
> bad precedent.
>
> Are checkpoints good now?
>

So far in this discussion, and in a thread that has forked off, I think 3
cases of implementation aspects have been mentioned that under certain
circumstances result in the validity of chains changing:
* Buried softforks (by simplifying the activation rules for certain rules)
* Not verifying BIP30 after BIP34 is active (since only under a SHA256^2
collision a duplicate txid can occur)
* The existence (and/or removal) of checkpoints (in one form or another).

None of these will influence the accepted main chain, however. If they ever
do, Bitcoin has far worse things to worry about (years-deep reorgs, or
SHA256 collisions).

If you were trying to point out that buried softforks are similar to
checkpoints in this regard, I agree. So are checkpoints good now? I believe
we should get rid of checkpoints because they seem to be misunderstood as a
security feature rather than as an optimization. I don't think buried
softforks have that problem.

-- 
Pieter

-------------------------------------
Congratulations!

It a property of the SKCP system that the person who performed the trusted
setup cannot extract any information from a proof?

In other words, is it proven hard to obtain information from a proof by the
buyer?

On Fri, Feb 26, 2016 at 6:42 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I am happy to announce the first successful Zero-Knowledge Contingent
> Payment (ZKCP) on the Bitcoin network.
>
> ZKCP is a transaction protocol that allows a buyer to purchase
> information from a seller using Bitcoin in a manner which is private,
> scalable, secure, and which doesn’t require trusting anyone: the
> expected information is transferred if and only if the payment is
> made. The buyer and seller do not need to trust each other or depend
> on arbitration by a third party.
>
> Imagine a movie-style “briefcase swap” (one party with a briefcase
> full of cash, another containing secret documents), but without the
> potential scenario of one of the cases being filled with shredded
> newspaper and the resulting exciting chase scene.
>
> An example application would be the owners of a particular make of
> e-book reader cooperating to purchase the DRM master keys from a
> failing manufacturer, so that they could load their own documents on
> their readers after the vendor’s servers go offline. This type of sale
> is inherently irreversible, potentially crosses multiple
> jurisdictions, and involves parties whose financial stability is
> uncertain–meaning that both parties either take a great deal of risk
> or have to make difficult arrangement. Using a ZKCP avoids the
> significant transactional costs involved in a sale which can otherwise
> easily go wrong.
>
> In today’s transaction I purchased a solution to a 16x16 Sudoku puzzle
> for 0.10 BTC from Sean Bowe, a member of the Zcash team, as part of a
> demonstration performed live at Financial Cryptography 2016 in
> Barbados. I played my part in the transaction remotely from
> California.
>
> The transfer involved two transactions:
>
> 8e5df5f792ac4e98cca87f10aba7947337684a5a0a7333ab897fb9c9d616ba9e
> 200554139d1e3fe6e499f6ffb0b6e01e706eb8c897293a7f6a26d25e39623fae
>
> Almost all of the engineering work behind this ZKCP implementation was
> done by Sean Bowe, with support from Pieter Wuille, myself, and Madars
> Virza.
>
>
> Read more, including technical details at
>
> https://bitcoincore.org/en/2016/02/26/zero-knowledge-contingent-payments-announcement/
>
> [I hope to have a ZKCP sudoku buying faucet up shortly. :) ]
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Wed, Dec 14, 2016 at 3:45 PM, Johnson Lau <jl2012@xbt.hk> wrote:

> I think that’s too much tech debt just for softforkability.
>
> The better way would be making the sum tree as an independent tree with a
> separate commitment, and define a special type of softfork (e.g. a special
> BIP9 bit).
>

One of the problems with fraud proofs is withholding by miners.  It is
important that proof of publication/archive nodes check that the miners are
actually publishing their blocks.

If you place the data in another tree, then care needs to be taken that the
merkle path information can be obtained for that tree.

If an SPV node asks for a run of transactions from an archive node, then
the archive node can give the merkle branch for all of those transactions.
The archive node inherently has to check that tree.

The question is if there is a way to show that data is not available, but
without opening up the network to DOS.  If enough people run full nodes
then this isn't a problem.

>
> When the softfork is activated, the legacy full node will stop validating
> the sum tree. This doesn’t really degrade the security by more than a
> normal softfork, as the legacy full node would still validate the total
> weight and nSigOp based on its own rules. The only purpose of the sum tree
> is to help SPV nodes to validate. This way we could even completely
> redefine the structure and data committed in the sum tree.
>

Seems reasonable.  I think the soft-fork would have to have a timeout
before actually activating.  That would give SPV clients time to switch
over.

That could happen before the vote though, so it isn't essential.  The SPV
clients would have to support both trees and then switch mode.  Ensuring
that SPV nodes actually bother would be helped by proving that the network
actually intends to soft fork.

The SPV client just has to check that every block has at least one of the
commitments that it accepts so that it can understand fraud proofs.


>
> I’d like to combine the size weight and sigOp weight, but not sure if we
> could. The current size weight limit is 4,000,000 and sigop limit is
> 80,000. It’s 50:1. If we maintain this ratio, and define
> weight = n * (total size +  3 * base size) + sigop , with n = 50
> a block may have millions of sigops which is totally unacceptable.
>

You multiplied by the wrong term.

weight = total size +  3 * base size + n * sigop , with n = 50

weight for max block = 8,000,000

That gives a maximum of 8,000,000 / 50 = 160,000 sigops.

To get that you would need zero transaction length.  You could get close if
you have transactions that just repeat OP_CHECKSIG over and over (or maybe
something with OP_CHECKMULTISIG).


>
> On the other hand, if we make n too low, we may allow either too few
> sigop, or a too big block size.
>
> Signature aggregation will make this a bigger problem as one signature may
> spend thousands of sigop
>
>
>
> On 14 Dec 2016, at 20:52, Tier Nolan <tier.nolan@gmail.com> wrote:
>
>
>
> On Wed, Dec 14, 2016 at 10:55 AM, Johnson Lau <jl2012@xbt.hk> wrote:
>
>> In a sum tree, however, since the nSigOp is implied, any redefinition
>> requires either a hardfork or a new sum tree (and the original sum tree
>> becomes a placebo for old nodes. So every softfork of this type creates a
>> new tree)
>>
>
> That's a good point.
>
>
>> The only way to fix this is to explicitly commit to the weight and
>> nSigOp, and the committed value must be equal to or larger than the real
>> value. Only in this way we could redefine it with softfork. However, that
>> means each tx will have an overhead of 16 bytes (if two int64 are used)
>>
>
> The weight and sigop count could be transmitted as variable length
> integers.  That would be around 2 bytes for the sigops and 3 bytes for the
> weight, per transaction.
>
> It would mean that the block format would have to include the raw
> transaction, "extra"/tree information and witness data for each transaction.
>
> On an unrelated note, the two costs could be combined into a unified
> cost.  For example, a sigop could have equal cost to 250 bytes.  This would
> make it easier for miners to decide what to charge.
>
> On the other hand, CPU cost and storage/network costs are not completely
> interchangeable.
>
> Is there anything that would need to be summed fees, raw tx size, weight
> and sigops that the greater or equal rule wouldn't cover?
>
> On 12 Dec 2016, at 00:40, Tier Nolan via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>
> On Sat, Dec 10, 2016 at 9:41 PM, Luke Dashjr <luke@dashjr.org> wrote:
>
>> On Saturday, December 10, 2016 9:29:09 PM Tier Nolan via bitcoin-dev
>> wrote:
>> > Any new merkle algorithm should use a sum tree for partial validation
>> and
>> > fraud proofs.
>>
>> PR welcome.
>>
>
> Fair enough.  It is pretty basic.
>
> https://github.com/luke-jr/bips/pull/2
>
> It sums up sigops, block size, block cost (that is "weight" right?) and
> fees.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
>
>
>

-------------------------------------
This would honestly work. It forces the attacker to go through with the
clearing phase which simultaneously makes it possible to "cancel" the TX
through another logic branch before the timeout occurs. I'd say that would
meet the needs of a clearing mechanism / fraud prevention system for an
exchange perfectly while requiring minimal changes to the software.

Very, very smart idea. A++, would read again.

On Thu, Aug 4, 2016 at 9:55 AM, Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Wed, Aug 3, 2016 at 7:16 PM, Matthew Roberts via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> The reason why I bring this up is existing OP codes and TX types don't
>> seem suitable for a secure clearing mechanism;
>>
>
> I think reversing transactions is not likely to be acceptable.  You could
> add an opcode that requires that an output be set to something.
>
> [target script] SPENDTO
>
> This would require that [target script] is the script for the
> corresponding output.  This is a purely local check.
>
> For example, if SPENDTO executes as part of the script for input 3, then
> it checks that output 3 uses the given script as its scriptPubKey.  The
> value of input 3 and output 3 would have to be the same too.
>
> This allows check sequence verify to be used to lock the spending script
> for a while.  This doesn't allow reversal, but would give a 24 hour window
> where the spenders can reverse the transaction.
>
> [IF <1 day> CSV DROP <live public key> CHECKSIG ELSE <offline protected
> key> CHECKSIG] SPENDTO <live public key2> CHECKSIG
>
> Someone with the live public key can create a transaction that spends the
> funds to the script in the square brackets.
>
> Once that transaction hits the blockchain, then someone with the <offline
> protected key> has 24 hours to spend the output before the person with the
> live keys can send the funds onward.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
It  is true that are many levels of consensus and that term itself is
not incorrect for any of the meanings.
Maybe we should try to start distinguishing between different types of
"consensus".
In BIP99 the only concepts that are needed are "consensus rules" and
"adoption consensus" (aka "community consensus", "full node runners
consenusus", "monetary users consenusus", "economic
super-ultra-majority", not sure if any of them or all of them, that's
still a placeholder in bip99 for
<everything_thats_needed_for_an_uncontroversial_hardfork_apart_from_the_hardfork_new_rules_being_uncontroversial>
[ie safe deployment requirements for an uncontroversial hardfork, just
like we have for uncontroversial softforks]).
Whatever term and defintion we chose for this concept, it has to be
neutral to whether the consensus rule changes are can be deployed as a
softfork or only as a hardfork [although we have had many
hardfork-to-softfork re-designs in the past and I agree that there
will be more, some people including @petertodd suspect that SF=HF, but
haven't been able to prove it yet], or otherwise we're implicitly
giving miners a power of unilaterally changing some consensus rules
that they don't have, for users can't never be denied from the right
to validate whatever rules they chose, just like an old radio receiver
machine owner cannot be forced to listen any channel in particular.
The "consensus rules" are in some sense the id of a theoretical
communication channel, and should not be confused with a real-life
process for how users should coordinate to "upgrade" to a new channel
(which is what BIP99 is about) or how we can objectively know whether
a proposed changed has had "adoption consensus" or not, which is what
this BIP is about.

But yeah, suggestions totally welcomed for a replacement for "adoption
consensus".

 On Tue, Feb 2, 2016 at 8:41 PM, Luke Dashjr <luke@dashjr.org> wrote:
> (Note Core currently has "consensus" only 249 times, most of which are simply
> identifier names, so it would be trivial to make this change.)

I'm afraid this would be horribly expensive in development hours for
not good enough reason and I must nack.

On Wed, Feb 3, 2016 at 1:03 AM, Luke Dashjr <luke@dashjr.org> wrote:
> On Tuesday, February 02, 2016 11:28:40 PM Dave Scotese wrote:
>> How about "defining" (rules, code, etc.) Such code and rules define what
>> bitcoin is.  It does require consensus and it ends up being a concord, but
>> all that can come after the fact (just as it did after bitcoin was first
>> released to the public).
>
> The difficulty is that this BIP needs to refer to three different context of
> consensus:
>
> 1. Consensus (stated) among developers for changes in the BIP Process.
> 2. Economic consensus (potential and stated) to veto a soft-fork by an
>    intended "firing" of the set of miners if they choose to enforce it.
> 3. (Actual) consensus in economic adoption of changed rules, to determine the
>    success of a hard-fork (after the fact).
> 4. The set of rules currently established as (defining) Bitcoin, enforced by
>    an (actual) consensus of economically-relevant nodes.
>
> Context 3 can be disambiguated with "adoption consensus", and context 4 with
> "consensus rules" and/or "consensus protocol", but I don't see a clear
> solution that covers all four contexts, and even sharing the word "consensus"
> for them may be confusing.
>
> In addition, usage of the word "consensus" for context 4 has proven confusing
> to users. For example, recently users misinterpreted the "Consensus" label
> used in context 4 as implying that the idea itself had in fact achieved
> consensus among some group of decision-makers (similar to context 1, but not
> necessarily the group being "developers").
>
> I don't know a good way to make this completely clear, so suggestions are more
> than welcome.
>
> Luke


-------------------------------------
For reasons others have pointed out, it's not really plausible.

Either way, this has nothing to do with transmitting data over audio.
Please start a new thread if you want to discuss your idea instead of
hijacking this one. Thanks ;)

On Fri, Aug 12, 2016, 05:36 Erik Aronesty via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I'm imagining a "publishable seed" such that:
>
>  - someone can derive a random bitcoin address from it -  and send funds
> to it.
>  - the possible derived address space is large enough that generating all
> possible addresses would be a barrier
>  - the receiver, however, knowing the private key, can easily scan the
> blockchain fairly efficiently and determine which addresses he has the keys
> to
>  - another interested party cannot easily do so
>
> Perhaps homomorphic encryption may need to be involved?
>
>
> On Thu, Aug 11, 2016 at 8:36 PM, Gregory Maxwell <greg@xiph.org> wrote:
>
>> On Thu, Aug 11, 2016 at 8:37 PM, Erik Aronesty via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > Still not sure how you can take a BIP32 public seed and figure out if an
>> > address was derived from it though.   I mean, wouldn't I have to
>> compute all
>> > 2^31 possible public child addresses?
>>
>> Which would take a quad core laptop about 8 hours with competent software
>>
>> And presumably you're not using the whole 2^31 space else the receiver
>> also has to do that computation...
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
I recently ran into an issue while importing a Mycelium HD wallet where it
was not finding all of my funds - upon further investigation with Mycelium
devs we realized that the wallet was following the BIP44 spec correctly,
but BIP44 may have a flaw.

The problem was a result of my creating 16 transactions in Mycelium in a
fairly short timeframe, but the first 15 transactions ended up never
confirming while the 16th was confirmed. As a result, when I later
reimported the account from the master seed, the chain derivation stopped
upon hitting this large gap of unused addresses on the internal / change
chain.

BIP44 recommends that there need not be a lookahead on internal chains
"because internal chains receive only coins that come from the associated
external chains."
https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki#Address_gap_limit

BIP32 also notes that "the look-ahead for internal chains can be very
small, as no gaps are to be expected here."
https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#Full_wallet_sharing_m

It seems to me that there /is/ an edge case that can result in significant
gaps in internal chain address usage and as such, the recommendation should
be to look ahead on both external and internal chains when performing
account discovery. On a related note, the recommended look-ahead of 20 may
not be safe enough - perhaps it should be raised to 100 if not higher.

In addition to recommending a larger look-ahead, it may also be advisable
for BIP44 to recommend that wallets "fill in" gaps of unused chain
addresses by "looking back" from the current tip of the internal chain's
index when the wallet decides to create a new change address. This could
help mitigate the size of gaps caused by failed transactions.

- Jameson

-------------------------------------
Just to clarify in BIP-0151 when it says:

>It is important to include the cipher-type into the symmetric cipher key to avoid weak-cipher-attacks.

the cipher-type here refers to the ECDH negotiation parameters?

On Wed, Jun 29, 2016 at 2:58 AM, Pieter Wuille <pieter.wuille@gmail.com> wrote:
> On Jun 29, 2016 07:05, "Ethan Heilman via bitcoin-dev"
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>> >It's also not clear to me why the HMAC, vs just
>> > SHA256(key|cipher-type|mesg).  But that's probably just my crypto
>> > ignorance...
>>
>> SHA256(key|cipher-type|mesg) is an extremely insecure MAC because of
>> the length extension property of SHA256.
>
> This property does technically not apply here, as the output of the hash is
> kept secret, and the possible messages are constants (which are presumably
> chosen in such a way that one is never an extension of another).
>
> However, this is a good example of why you can't generically use a hash
> function in places where you want a MAC (aka "a hash with a shared secret").
> Furthermore, if you already have a hash function anyway, HMAC is very easy
> construct on top of it.
>
> --
> Pieter


-------------------------------------
As part of the hard-fork proposed in the HK agreement(1) we'd like to make the
patented AsicBoost optimisation useless, and hopefully make further similar
optimizations useless as well.

What's the best way to do this? Ideally this would be SPV compatible, but if it
requires changes from SPV clients that's ok too. Also the fix this should be
compatible with existing mining hardware.


1) https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff

2) http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
I forget to send to bitcoin-dev.

> A related problem is that if this transaction is reorged out during an
innocent reorg, one that doesn't involve a double spend, the transaction
may never get back in unless it occurs at exactly  the same height, which
is not guaranteed.
>
> This affects fungabity of coins generated from these transactions.
>
>
> On Oct 2, 2016 18:37, "Sergio Demian Lerner" <sergio.d.lerner@gmail.com>
wrote:
>>
>>
>>
>> On Sun, Oct 2, 2016 at 6:46 PM, Russell O'Connor via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>
>>>> But I would argue that in this scenario, the only way it
>>>> would become invalid is the equivalent of a double-spend... and
therefore it
>>>> may be acceptable in relation to this argument.
>>>
>>>
>>> The values returned by OP_COUNT_ACKS vary in their exact value
depending on which block this transaction ends up in.  While the proposed
use of this operation is somewhat less objectionable (although still
objectionable to me), nothing stops users from using OP_EQUALVERIFY and and
causing their transaction fluctuate between acceptable and unacceptable,
with no party doing anything like a double spend.  This is a major problem
with the proposal.
>>
>>
>> Transactions that redeem an output containing (or referencing by means
of P2WSH) an OP_COUNT_ACKS are not broadcast by the network. That means
that the network cannot be DoS attacked by flooding with a transaction that
will not verify due to being too late.
>> The only parties that can include the redeem transaction are the miners
themselves.
>> Therefore I see no problem that an OP_COUNT_ACKS scriptSig transaction
is invalidated after the liveness times expires.
>> If there is no expiration, then polls can last forever and the system
fails to provide DoS protection for block validation since active polls can
accumulate forever.
>>
>>
>>

-------------------------------------
On Tuesday, February 02, 2016 5:50:29 AM Dave Scotese wrote:
> The section that starts "Should two software projects need to release"
> addresses issues that are difficult to ascertain from what is written
> there.  I'll take a stab at what it means:
> 
> Would bitcoin be better off if multiple applications provided their own
> implementations of API/RPC and corresponding application layer BIPs?
> 
>    - While there is only one such application, its UI will be the obvious
>    standard and confusion in usability will be avoided.
>    - Any more than a single such application will benefit from the
>    coordination encouraged and aided by this BIP and BIP 123.

The original question is intended to answer both: a) why only one 
implementation is insufficient for Final status, and b) why two is sufficient.

If every application had its own BIP (how I understand your version), none of 
them would be standards and it wouldn't make sense to have a BIP at all - just 
project documentation would be sufficient.

> "To avoid doubt: comments and status are unrelated metrics to judge a BIP,
> and neither should be directly influencing the other." makes more sense to
> me as "To avoid doubt: comments and status are intended to be unrelated
> metrics. Any influence of one over the other indicates a deviation from
> their intended use."  This can be expanded with a simple example: "In other
> words, a BIP having  the status 'Rejected' is no reason not to write
> additional comments about it.  Likewise, overwhelming support for a BIP in
> its comments section doesn't change the requirements for the 'Accepted' or
> 'Active' status."

Extending this to "influence" is probably too far - after all, comments may 
discourage implementations, which can very well result in the Status 
eventually becoming Rejected rather than Final. How about:

"To avoid doubt: comments and status are intended to be unrelated metrics. In 
other words, a BIP having the status 'Rejected' is no reason to write (or not 
write) additional comments about it, nor would a status of 'Final' preclude 
comments discouraging [further] implementation. Likewise, overwhelming support 
for a BIP in its comments section doesn't change the requirements for the 
'Final' or 'Active' status."

> Since the Bitcoin Wiki can be updated with comments from other places, I
> think the author of a BIP should be allowed to specify other Internet
> locations for comments.  So "link to a Bitcoin Wiki page" could instead be
> "link to a comments page (strongly recommended to be in the Bitcoin
> Wiki)". 

Hmm, I wonder if this could be too easily abuse to discourage comments 
(because the commenter does not wish to register with yet another forum), 
and/or censor negative comments (because the author has made his own forum 
specifically for the purpose).

On Tuesday, February 02, 2016 6:35:07 AM you wrote:
> For section "Formally defining consensus",
> 
> Where objections were not deemed substantiated by the community, clear
> reasoning must be offered.

I have integrated this into the draft.

> For section "BIP Comments",
> 
> Comments should be solicited on the bitcoin-dev mailing list, and
> summarized fairly in the wiki; with notice of summarization and time
> for suggesting edits on the mailing list.  Wiki registration and
> monitoring should not be a required hurdle to participation.

The intent is for the commenter to edit the wiki page himself. I have updated 
it to reflect this.

Luke


-------------------------------------
Our BIP just defines protocol definitions, and doesn't really dictate how
people use them (we're coming up with a new title for the BIP, by the way,
to more accurately convey that). Using our definitions as building blocks,
someone could definitely accomplish what you described. For example, Joe
Mobile Wallet User's wallet could upload a slew of generic PaymentRequest
messages with signatures to prove his identity, and the server could then
create encryptedPaymentRequest messages using the server's key for
encryption and communication with the other party. In this case the server
would essentially be a proxy for the user without having actual access to
the user's private keys.

My personal goal with the protocol was to keep it extremely flexible so
developers could use it to build all different types of schemes while
keeping standard messages that could be forwarded between services if
needed. Does the above make sense?

James

On Tue, Mar 8, 2016 at 2:55 PM Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Is there a way for Joe Mobile Wallet User to upload a set of N
> PaymentRequests
> authenticated by his key to an untrusted server, which encrypts and passes
> them on in response to InvoiceRequests? Or does this necessarily require
> the
> recipient to be online?
>
> On Tuesday, March 01, 2016 6:58:16 PM Justin Newton via bitcoin-dev wrote:
> > The following draft BIP proposes an update to the Payment Protocol.
> >
> > Motivation:
> >
> > The motivation for defining this extension to the BIP70 Payment Protocol
> is
> > to allow 2 parties to exchange payment information in a permissioned and
> > encrypted way such that wallet address communication can become a more
> > automated process. Additionally, this extension allows for the requestor
> of
> > a PaymentRequest to supply a certificate and signature in order to
> > facilitate identification for address release. This also allows
> > for automated creation of off blockchain transaction logs that are human
> > readable, containing who you transacted with, in addition to the
> > information that it contains today.
> >
> > The motivation for this extension to BIP70 is threefold:
> >
> > 1. Ensure that the payment details can only be seen by the participants
> in
> > the transaction, and not by any third party.
> > 2. Enhance the Payment Protocol to allow for store and forward servers in
> > order to allow, for example, mobile wallets to sign and serve
> > Payment Requests.
> > 3. Allow a sender of funds the option of sharing their identity with the
> > receiver. This information could then be used to:
> >
> >         * Make bitcoin logs more human readable
> >         * Give the user the ability to decide who to release payment
> > details to
> >         * Allow an entity such as a political campaign to ensure donors
> > match regulatory and legal requirements
> >         * Allow for an open standards based way for regulated financial
> > entities to meet regulatory requirements
> >         * Automate the active exchange of payment addresses, so static
> > addresses and BIP32 X-Pubs can be avoided to maintain privacy
> > and convenience
> >
> > In short we wanted to make bitcoin more human, while at the same time
> > improving transaction privacy.
> >
> > Full proposal here:
> >
> >
> https://github.com/techguy613/bips/blob/master/bip-invoicerequest-extension
> > .mediawiki
> >
> > We look forward to your thoughts and feedback on this proposal!
> >
> > Justin
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Hi

I’ve been thinking around a solution to reduce nodes bootstrap time
(IBD) as well as a way to reduce the amount of bandwidth/network usage
per node.
Not sure if this idea was/is already discussed, haven’t found anything
in a quick research.


==Title==
Fast bootstrapping with a pre-generated UTXO-set database.

==Abstract==
This documents describes a way how bitcoin nodes can bootstrap faster
by loading a pre-generated UTXO-set datafile with moderate reduction
of the security model.

==Specification==
Bitcoin-core or any other full node client will need to provide a
feature to "freeze" the UTXO-set at a specified height (will require a
reindex). The frozen UTXO-set – at a specific height – will be
deterministic linearized in a currently not specified
data-serializing-format.
Additionally, a serialized form of the current chain-index (chain
containing all block-headers) up to the specified height will be
appended to the pre-generated UTXO-set-datafile.
The datafile will be hashed with a double SHA256.

The corresponding hash will be produced/reproduced and signed (ECDSA)
by a group of developers, ideally the same group of developers who are
also signing deterministic builds (binary distribution).

Full node client implementations that supports bootstrapping from a
pre-generated UTXO-set, need to include...
1.) a set of pubkeys from trusted developers
2.) the hash (or hashes) of the pre-generated UTXO-set-datafile(s)
3.) n signatures of the hash(es) from 2) from a subset of developers
defined in 1)

To guarantee the integrity of developers pubkeys & signatures, methods
like the current gitian build, used in bitcoin-core, must be used.

New nodes could download a copy of the pre-generated UTXO-set, hash
it, verify the hash against the allowed UTXO-sets, verify the ECDSA
signatures from various developers, and continue bootstrapping from
the specified height if the users accepts the amount of valid signatures
.

Sharing of the pre-generated UTXO-set can be done over CDNs,
bit-torrent or any other file hosting solution. It would also be
possible to extend the bitcoin p2p layer with features to
distribute/share a such pre-generated UTXO-set, in chunks and with the
according hashes to detect invalidity before downloading the whole
content (but would probably end up in something very similar to
bit-torrent).


- ----------------------
</jonas>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJW1B1wAAoJECnUvLZBb1PsqzsP/iSdvyhUzy+BZVSZbKXNjk5P
2vrtirI6NvKQd8hHbrcFeLfyswzYc2JWRnX8sATlauIS0pYdr97JriwUGlvxvNrY
iVTDdf8MIVu8zScLQtJbMatpMvsewtqQEidn/yxWIhiCg4G2T5DZmlBU6O4XIKR6
5aPHElGOKZ15EWGHBG7z4owj3MiOaxhD9q5erBbfLPpcm08o6XAv5miqmGnnn3zh
gocg4Gxs6iDygh3b2dCJFwWIVPxF6UVJhyjv2kLZUmEHT2Y2QvdGcLIIewcWHDze
kgoZYmOEowujCbmeJ+LBwgOI0c1N6L/ciomPBne7ILmK4LyUEzyMLJKNYf/sZ8vI
sVlmwZwZZLfILC7mzMAM0pfj99IOW680WHch9v31lWFlxW/bLvLqAO7n3acQuD6s
xCZN2nAhmWC8FnMFxqB3EUz0lX8giV3qRJZjbQMS+ZrngYkAmVv2bAsoLndqf6MO
l9W8B+ICg1KZLGIOF2pUrInpkB6gUALDFnypV4CeIVdeqtk5l4LnCHK6c4++Hl5n
Bv5HQ/wTgKKNFtHBEJpWyYWvAjfFtgUZUKblR+Bh+D7/Gte1ehiYd02KYD4ds9Y4
3gfO8YbAz/I14Yuh2bIlvVKPWnLQBwL5BBioBfvmhV/r6rEpzWvB7H6Fmi1c759l
VlL0GiUV8ar2LlFhEmWk
=lZSy
-----END PGP SIGNATURE-----


-------------------------------------
On Wed, Mar 02, 2016 at 02:56:14PM +0000, Luke Dashjr via bitcoin-dev wrote:
> To alleviate this risk, it seems reasonable to propose a hardfork to the 
> difficulty adjustment algorithm so it can adapt quicker to such a significant 
> drop in mining rate.

Having a well-reviewed hard fork patch for rapid difficulty adjustment
would seem to be a useful reserve for all sorts of possible problems.
That said, couldn't this specific potential situation be dealt with by a
relatively simple soft fork?

Let's say that, starting soon, miners require that valid block header
hashes be X% below the target value indicated by nBits. The X% changes
with each block, starting at 0% and increasing to 50% just before block
420,000 (the halving). This means the before the halving, every two
hashes are being treated as one hash, on average.

For blocks 420,000 and higher the code is disabled, immediately doubling
the effective hash rate at the same time the subsidy is halved,
potentially roughly canceling each other out to make a pre-halving hash
equal in economic value to a post-halving hash.

Of course, some (perhaps many) miners will not be profitable at the
post-halving subsidy level, so the steady increase in X% will force them
off the network at some point before the halving, hopefully in small
numbers rather than all at once like the halving would be expected to do.

For example, if the soft fork begins enforcement at block 410,000, then
X% can be increased 0.01% per block. Alice is a miner whose costs are
24BTC per block and she never claims tx fees for some reason, so her
profits now are always 25BTC per block. During the first difficulty
period after the soft fork is deployed, the cost to produce a hash will
increase like this,

    0: 0%           500: 5%         1000: 10%       1500: 15%       2000: 20%
    100: 1%         600: 6%         1100: 11%       1600: 16%
    200: 2%         700: 7%         1200: 12%       1700: 17%
    300: 3%         800: 8%         1300: 13%       1800: 18%
    400: 4%         900: 9%         1400: 14%       1900: 19%

Somewhere around block 417, Alice will need to drop out because her
costs are now above 25BTC per block.  With the loss of her hash rate,
the average interblock time will increase and the capacity will decrease
(all other things being equal). However, Bob whose costs are 20BTC per
block can keep mining through the period.

At the retarget, the difficulty will go down (the target goes up) to
account for the loss of Alice's hashes. It may even go down enough
that Alice can mine profitably for a few more blocks early in the new
period, but the increasing X% factor will make her uneconomical again,
and this time it might even make Bob uneconomical too near the end of
the period. However, Charlie whose costs are 12BTC per block will
never be uneconomical as he can continue mining profitably even after
the halving. Alice and Bob mining less will increase the percentage of
blocks Charlie produces before the retarget, steadily shifting the
dynamics of the mining network to the state expected after the halving
and hopefully minimizing the magnitude of any shocks.

This does create the question about whether this soft fork would be
ethical, as Alice and Bob may have invested money and time on the
assumption that their marginal hardware would be usable up until the
halving and with this soft fork they would become uneconomical earlier
than block 420,000. A counterargument here is such an investment was
always speculative given the vagaries of exchange rate fluctuation, so
it could be permissible to change the economics slightly in order to
help ensure all other Bitcoin users experience minimal disruption during
the halving.

Unless I'm missing something (likely) I think this proposal has the
advantage of fast rollout (if the mechanism of an adjusted target is as
simple as I think it could be) in a non-emergency manner without a hard
fork that would require all full nodes upgrade (plus maybe some SPV
software that check nBits, which they probably all should be doing
given it's in the block headers that they download anyway).

-Dave

P.S. I see Tier Nolan proposed something similar while I was writing
     this. I think this proposal differs in its analysis to warrant a
     possible duplicate posting.


-------------------------------------
Please note that the BIP144 has just been revised to match the implementation in #7910. We will use the 1<<3 service bit (NODE_WITNESS) to signal the readiness for segwit.

https://github.com/bitcoin/bips/commit/dde47fc973b015c6cc91a0ed28fb3aca57add5e6 <https://github.com/bitcoin/bips/commit/dde47fc973b015c6cc91a0ed28fb3aca57add5e6>

-------------------------------------
Tricky choice. On the one hand I had spotted this too before and maybe
one or two more exceptions to bitcoin's 128-bit security target and
been vaguely tut-tutting about them in the background.  It's kind of a
violation of crypto rule of thumb that you want to balance things and
not have odd weak points as Watson was implying, it puts you closer to
the margin if there is a slip or other problem so you have an
imbalanced crypto format.

On the other hand it's not currently a problem as such and it's less
change and slightly more compact.

RIPEMD probably is less well reviewed than SHA2.  However SHA1 has
problems, and SHA2 is a bigger SHA1 basically so, hence the NIST
motivation for SHA3 designed to fix the design flaw in SHA1 (and SHA2
in principle).

So then if we agree with this rule of thumb (and not doing so would
fly against best practices which we probably shouldnt look to do in
such a security focussed domain) then what this discussion is more
about is when is a good time to write down tech debt.

I think that comes to segregated-witness itself which writes down a
tidily organised by lines of code robust fix to a bunch of long
standing problems.

Doing a 2MB hard-fork in comparison fixes nothing really.  Leaving
known issues to bake in for another N years eventually builds up on
you (not even in security just in software engineering) as another
rule of thumb.  I mean if we dont fix it now that we are making a
change that connects, when will we?

In software projects I ran we always disguised the cost of tech-debt
as non-negotiable baked into our estimates without a line item to
escape the PHB syndrome of haggling for features instead of tech debt
(which is _never_ a good idea:)

Pragmatism vs refactoring as you go.

But for scale I think segregated-witness does offer the intriguing
next step of being able to do 2 of 2, 3 of 3 and N of N which give
size of one sig multisig (indistinguishable even for privacy) as well
as K of N key tree sigs, which are also significantly more compact.

There was also the other thing I mentioned further up the thread that
if we want to take an approach of living with little bit of bloat from
getting back to a universal 128-bit target, there are still some
fixable bloat things going on:
a) sending pubKey in the signature vs recovery (modulo interference
with Schnorr batch verify compatibility*);
b) using the PubKey instead of PKH in the ScriptPubKey, though that
loses the nice property of of not having the key to do DL attacks on
until the signed transaction is broadcast;
c) I think there might be a way to combine hash & PubKey to keep the
delayed PubKey publication property and yet still save the bloat of
having both.

* I did suggest to Pieter that you could let the miner decide to forgo
Schnorr batch verifiability to get compaction from recovery - the pub
key could be optionally elided from the scriptSig serialisation by the
miner.

The other thing we could consider is variable sized hashes (& a few
pubkey size choices) that is software complexity however.  We might be
better of focussing on the bigger picture like IBLT/weak-blocks and
bigger wins like MAST, multiSig Schnorr & key tree sigs.

Didnt get time to muse on c) but a nice crypto question for someone :)

Another thing to note is combining has been known to be fragile to bad
interactions or unexpected behaviours.  This paper talks about things
tradeoffs and weaknesses in hash combiners.
http://tuprints.ulb.tu-darmstadt.de/2094/1/thesis.lehmann.pdf

Weak concept NACK I think for losing a cleanup opportunity to store it
up for the future when there is a reasonable opportunity to fix it?

Adam


On 8 January 2016 at 15:34, Watson Ladd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Fri, Jan 8, 2016 at 4:38 AM, Gavin Andresen via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> On Fri, Jan 8, 2016 at 7:02 AM, Rusty Russell <rusty@rustcorp.com.au> wrote:
>>>
>>> Matt Corallo <lf-lists@mattcorallo.com> writes:
>>> > Indeed, anything which uses P2SH is obviously vulnerable if there is
>>> > an attack on RIPEMD160 which reduces it's security only marginally.
>>>
>>> I don't think this is true?  Even if you can generate a collision in
>>> RIPEMD160, that doesn't help you since you need to create a specific
>>> SHA256 hash for the RIPEMD160 preimage.
>>>
>>> Even a preimage attack only helps if it leads to more than one preimage
>>> fairly cheaply; that would make grinding out the SHA256 preimage easier.
>>> AFAICT even MD4 isn't this broken.
>>
>>
>> It feels like we've gone over that before, but I can never remember where or
>> when. I believe consensus was that if we were using the broken MD5 in all
>> the places we use RIPEMD160 we'd still be secure today because of Satoshi's
>> use of nested hash functions everywhere.
>>
>>>
>>> But just with Moore's law (doubling every 18 months), we'll worry about
>>> economically viable attacks in 20 years.[1]
>>>
>>>
>>> That's far enough away that I would choose simplicity, and have all SW
>>> scriptPubKeys simply be "<0> RIPEMD(SHA256(WP))" for now, but it's
>>> not a no-brainer.
>>
>>
>> Lets see if I've followed the specifics of the collision attack correctly,
>> Ethan (or somebody) please let me know if I'm missing something:
>>
>> So attacker is in the middle of establishing a payment channel with
>> somebody. Victim gives their public key, attacker creates the innocent
>> fund-locking script  '2 V A 2 CHECKMULTISIG' (V is victim's public key, A is
>> attacker's) but doesn't give it to the victim yet.
>>
>> Instead they then generate about 2^81scripts that are some form of
>> pay-to-attacker ....
>> ... wait, no that doesn't work, because SHA256 is used as the inner hash
>> function.  They'd have to generate 2^129 to find a cycle in SHA256.
>
> For 2^80 they simply generate 2^80 scripts that look innocent, and
> 2^80 that are not. With high probability there is a collision. I agree
> that most cryptanalysis won't work because of the nesting, but 2^80 is
> not good.
>>
>> Instead, they .. what? I don't see a viable attack unless RIPEMD160 and
>> SHA256 (or the combination) suffers a cryptographic break.
>>
>>
>> --
>> --
>> Gavin Andresen
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
>
> --
> "Man is born free, but everywhere he is in chains".
> --Rousseau.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
On Sat, Sep 10, 2016 at 1:23 PM, Andrew C via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On 9/10/2016 5:41 AM, Johnson Lau via bitcoin-dev wrote:
>> 3. After a few months or so, publish the private key.
> Why wait a few months? Why not just publish the key a few days after the
> final alert?

Because if you were offline at the time of the final alert, the alert
you may see instead is "Urgent security problem! Upgrade to
UltraBitcoin NOW! http://scamsite.info/", among other similar reasons.


-------------------------------------
This is my BIP idea: a fast, robust, and standardized for representing
Bitcoin addresses over audio. It takes the binary representation of the
Bitcoin address (little endian), chops that up into 4 or 2 bit chunks
(depending on type, 2 bit only for low quality audio like american
telephone lines), and generates a tone based upon that value. This started
because I wanted an easy way to donate to podcasts that I listen to, and
having a Shazam-esque app (or a media player with this capability) that
gives me an address automatically would be wonderful for both the consumer
and producer. Comes with error correction built into the protocol

You can see the full specification of the BIP on my GitHub page (
https://github.com/Dako300/BIP-0153).

-------------------------------------
This thread has strayed extensively off topic from the OP which asked a
simple question about BIP141 signalling start params.

Please move to another thread, or take more general discussion to
bitcoin-discuss.

Thank you.

-------------------------------------
Hi,

I can clearly see some advantages for such a feature, but it's kind of
in conflict with Bitcoin's fundamental design. This design might be
problematic when it comes to hacks/thefts, but it's what gives Bitcoin
strength and make it differentiate from other currencies:

* reversal of transactions is impossible
* keep private keys private and safe. Lose them, it's like losing cash,
you can just forget about it.
* while we try hard to make 0-conf as safe as possible (if there's no
RBF flag on the transaction), we make it almost impossible or very very
expensive to reverse a confirmed transaction.

Also, we don't have a clear way to properly decide a good settlement
period length. It doesn't fix the problem any more than nLockTime fixes
it -- you can't know ahead of time when a withdraw needs to be made.
Fair enough, but even if the withdraw is made with a settlement layer,
will the user be able to spend it further immediately? Who will accept
such an input and treat it as a payment if it can be reversed during the
settlement layer? So, if you can't know ahead of time when a withdraw
needs to be made (nLockTime) how can you know ahead of time+settlement
period when a transaction needs to be declared irrevocable?

The linked page describes that merchants will never accept payments from
'vaults', and it will take 24 hours for coins to be irreversible moved
outside the 'vault'. This covers the part "is the user able to spend a
transaction with settlement layer" but it has security properties equal
to nLockTime = 24 hours - you can't benefit and use the coins
immediately and in 24 hours price might go up or down in an undesirable
way for a certain user. It however raises a lot of other questions: what
if the attacker manages to steal both the private key and vault key (we
have strong reasons to assume this can happen: if you can't keep a
private key safe, why would you be able to keep the vault key any
safer?) and starts a race with the actual user to unlock and lock back
the vault?

I think this is a wrong approach. hacks and big losses are sad, but all
the time users / exchanges are to blame for wrong implementations or
terrible security practices.

Thanks!

On 8/3/2016 9:16 PM, Matthew Roberts via bitcoin-dev wrote:
> In light of the recent hack: what does everyone think of the idea of
> creating a new address type that has a reversal key and settlement layer
> that can be used to revoke transactions?
> 
> You could specify so that transactions "sent" from these addresses must
> receive N confirmations before they can't be revoked, after which the
> transaction is "settled" and the coins become redeemable from their
> destination output. A settlement phase would also mean that a
> transaction's progress was publicly visible so transparent fraud
> prevention and auditing would become possible by anyone.
> 
> The reason why I bring this up is existing OP codes and TX types don't
> seem suitable for a secure clearing mechanism; Nlocktimed TXs won't work
> for this since you can't know ahead of time when and where a withdrawal
> needs to be made, plus there's still the potential for key
> mismanagement; Similar problems with OP_CHECKLOCKTIMEVERIFY apply too 
> unless you keep a private key around on the server which would defeat
> the purpose. The main use case here, would be specifically to improve
> centralized exchange security by making it impossible for a hot wallet
> to be raided all at once.
> 
> Thoughts?
> 
> Some existing background:
> 
> http://hackingdistributed.com/2016/08/03/how-bitfinex-heist-could-have-been-avoided/
> -- Proposed the basic idea for a time-based clearing house but using
> blockchains directly, this is a much better idea than my own.
> 
> roberts.pm/timechain <http://roberts.pm/timechain> -- My original paper
> written in 2015 which proposed a similar idea for secure wallet design
> but implemented using time-locked ECDSA keys. Obviously a blockchain
> would work better for this.
> 
> Other -- if the idea has already been brought up by other people, I
> apologize.
> 


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version 0.12.1 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.12.1/>

Or through bittorrent:

  magnet:?xt=urn:btih:25c4df2a822e840e972a50a31095632d87efadab&dn=bitcoin-core-0.12.1&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2F 

This is a new minor version release, including the BIP9, BIP68 and BIP112
softfork, various bugfixes and updated translations.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

To receive security and update notifications, please subscribe to
https://bitcoincore.org/en/list/announcements/join/.

Upgrading and downgrading
=========================

How to Upgrade
- --------------

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes for older versions), then run the
installer (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or
bitcoind/bitcoin-qt (on Linux).

Downgrade warning
- -----------------

### Downgrade to a version < 0.12.0

Because release 0.12.0 and later will obfuscate the chainstate on every
fresh sync or reindex, the chainstate is not backwards-compatible with
pre-0.12 versions of Bitcoin Core or other software.

If you want to downgrade after you have done a reindex with 0.12.0 or later,
you will need to reindex when you first start Bitcoin Core version 0.11 or
earlier.

Notable changes
===============

First version bits BIP9 softfork deployment
- -------------------------------------------

This release includes a soft fork deployment to enforce [BIP68][],
[BIP112][] and [BIP113][] using the [BIP9][] deployment mechanism.

The deployment sets the block version number to 0x20000001 between
midnight 1st May 2016 and midnight 1st May 2017 to signal readiness for 
deployment. The version number consists of 0x20000000 to indicate version
bits together with setting bit 0 to indicate support for this combined
deployment, shown as "csv" in the `getblockchaininfo` RPC call.

For more information about the soft forking change, please see
<https://github.com/bitcoin/bitcoin/pull/7648>

This specific backport pull-request can be viewed at
<https://github.com/bitcoin/bitcoin/pull/7543>

[BIP9]: https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki
[BIP68]: https://github.com/bitcoin/bips/blob/master/bip-0068.mediawiki
[BIP112]: https://github.com/bitcoin/bips/blob/master/bip-0112.mediawiki
[BIP113]: https://github.com/bitcoin/bips/blob/master/bip-0113.mediawiki

BIP68 soft fork to enforce sequence locks for relative locktime
- ---------------------------------------------------------------

[BIP68][] introduces relative lock-time consensus-enforced semantics of
the sequence number field to enable a signed transaction input to remain
invalid for a defined period of time after confirmation of its corresponding
outpoint.

For more information about the implementation, see
<https://github.com/bitcoin/bitcoin/pull/7184>

BIP112 soft fork to enforce OP_CHECKSEQUENCEVERIFY
- --------------------------------------------------

[BIP112][] redefines the existing OP_NOP3 as OP_CHECKSEQUENCEVERIFY (CSV)
for a new opcode in the Bitcoin scripting system that in combination with
[BIP68][] allows execution pathways of a script to be restricted based
on the age of the output being spent.

For more information about the implementation, see
<https://github.com/bitcoin/bitcoin/pull/7524>

BIP113 locktime enforcement soft fork
- -------------------------------------

Bitcoin Core 0.11.2 previously introduced mempool-only locktime
enforcement using GetMedianTimePast(). This release seeks to
consensus enforce the rule.

Bitcoin transactions currently may specify a locktime indicating when
they may be added to a valid block.  Current consensus rules require
that blocks have a block header time greater than the locktime specified
in any transaction in that block.

Miners get to choose what time they use for their header time, with the
consensus rule being that no node will accept a block whose time is more
than two hours in the future.  This creates a incentive for miners to
set their header times to future values in order to include locktimed
transactions which weren't supposed to be included for up to two more
hours.

The consensus rules also specify that valid blocks may have a header
time greater than that of the median of the 11 previous blocks.  This
GetMedianTimePast() time has a key feature we generally associate with
time: it can't go backwards.

[BIP113][] specifies a soft fork enforced in this release that
weakens this perverse incentive for individual miners to use a future
time by requiring that valid blocks have a computed GetMedianTimePast()
greater than the locktime specified in any transaction in that block.

Mempool inclusion rules currently require transactions to be valid for
immediate inclusion in a block in order to be accepted into the mempool.
This release begins applying the BIP113 rule to received transactions,
so transaction whose time is greater than the GetMedianTimePast() will
no longer be accepted into the mempool.

**Implication for miners:** you will begin rejecting transactions that
would not be valid under BIP113, which will prevent you from producing
invalid blocks when BIP113 is enforced on the network. Any
transactions which are valid under the current rules but not yet valid
under the BIP113 rules will either be mined by other miners or delayed
until they are valid under BIP113. Note, however, that time-based
locktime transactions are more or less unseen on the network currently.

**Implication for users:** GetMedianTimePast() always trails behind the
current time, so a transaction locktime set to the present time will be
rejected by nodes running this release until the median time moves
forward. To compensate, subtract one hour (3,600 seconds) from your
locktimes to allow those transactions to be included in mempools at
approximately the expected time.

For more information about the implementation, see
<https://github.com/bitcoin/bitcoin/pull/6566>

Miscellaneous
- -------------

The p2p alert system is off by default. To turn on, use `-alert` with
startup configuration.

0.12.1 Change log
=================

Detailed release notes follow. This overview includes changes that affect
behavior, not code moves, refactors and string updates. For convenience in locating
the code changes and accompanying discussion, both the pull request and
git merge commit are mentioned.

### RPC and other APIs
- - #7739 `7ffc2bd` Add abandoned status to listtransactions (jonasschnelli)

### Block and transaction handling
- - #7543 `834aaef` Backport BIP9, BIP68 and BIP112 with softfork (btcdrak)

### P2P protocol and network code
- - #7804 `90f1d24` Track block download times per individual block (sipa)
- - #7832 `4c3a00d` Reduce block timeout to 10 minutes (laanwj)

### Validation
- - #7821 `4226aac` init: allow shutdown during 'Activating best chain...' (laanwj)
- - #7835 `46898e7` Version 2 transactions remain non-standard until CSV activates (sdaftuar)

### Build system
- - #7487 `00d57b4` Workaround Travis-side CI issues (luke-jr)
- - #7606 `a10da9a` No need to set -L and --location for curl (MarcoFalke)
- - #7614 `ca8f160` Add curl to packages (now needed for depends) (luke-jr)
- - #7776 `a784675` Remove unnecessary executables from gitian release (laanwj)

### Wallet
- - #7715 `19866c1` Fix calculation of balances and available coins. (morcos)

### Miscellaneous
- - #7617 `f04f4fd` Fix markdown syntax and line terminate LogPrint (MarcoFalke)
- - #7747 `4d035bc` added depends cross compile info (accraze)
- - #7741 `a0cea89` Mark p2p alert system as deprecated (btcdrak)
- - #7780 `c5f94f6` Disable bad-chain alert (btcdrak)

Credits
=======

Thanks to everyone who directly contributed to this release:

- - accraze
- - Alex Morcos
- - BtcDrak
- - Jonas Schnelli
- - Luke Dashjr
- - MarcoFalke
- - Mark Friedenbach
- - NicolasDorier
- - Pieter Wuille
- - Suhas Daftuar
- - Wladimir J. van der Laan

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJXELrMAAoJEHSBCwEjRsmm75EH/0iyqFxXuJDbfzMmBbMTkXD2
/CXEeyMvs62F2ZeODE0SSqo9sXo4foiT9WI5Dq7BwAiF6jh/XE4QwBvc91BbPyGZ
1nOGEab+oe37xEOkn8MyGbHfCutsUldyKltVQjA3y685MxlSgTjl/nX6Pbpbxped
vZRog3KHRrpWAMrHdi6p/xgqX0ajxE6K1P16JMOx4W/gE9QgOPyy7+l/4WT6SyBj
k/pOLqJc+yQIOa9szS4pjLUqaSOirhsjXfro9FYjHqiTWQwAdvuK4xXgo1GrGIW1
PWs419uLmGl4bhg9jdY6v+PyPz4iUilRzoixVi8op1Rt9/AoNN1ViJ/LT15Hagw=
=h4Wp
-----END PGP SIGNATURE-----


-------------------------------------
Hello Eric,

I felt like I still owed you a response to the points below.

On Thu, Jun 30, 2016 at 5:10 PM, Eric Voskuil <eric@voskuil.org> wrote:
> Pieter, these are in my opinion very reasonable positions. I've made some observations inline.
>
>> On Jun 30, 2016, at 3:03 PM, Pieter Wuille <pieter.wuille@gmail.com> wrote:
>>
>> On Thu, Jun 30, 2016 at 11:57 AM, Eric Voskuil via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> The proliferation of node identity is my primary concern - this relates to privacy and the security of the network.
>>
>> I think this is a reasonable concern.
>>
>> However, node identity is already being used widely, and in a very
>> inadvisable way:
>> * Since forever there have been lists of 'good nodes' to pass in
>> addnode= configuration options.
>> * Various people run multiple nodes in different geographic locations,
>> peering with each other.
>> * Various pieces of infrastructure exist that relies on connecting to
>> well-behaving nodes (miner relay networks, large players peering
>> directly with each other, ...)
>
> Yes, libbitcoin also provides these options on an IP basis.
>
>> * Several lightweight clients support configuring a trusted host to connect to.
>
> I explicitly exclude client-server behavior as I believe the proper resolution is to isolate clients from the P2P protocol. Libbitcoin does this already.

I think that's a false dichotomy. There is no reason why the P2P
network consists of purely servers (full nodes) and clients
(lightweight nodes). Where does a client fit that is SPV at startup,
but upgrades in the background to a full node? It seems strange that
such a client would use a 'client protocol' for initial connections,
but the P2P protocol for syncing with history, when both come from the
same peers, and transmit the same kind of information.

What would make sense IMHO is a protocol split between the different
kinds of transmission:
1) Historical block download
2) Block synchronization at the tip
3) Transaction relay
...

(1) prefers high bandwidth, has no connectivity concerns, and does not
care about latency and has no privacy concerns. (2) needs
partition-resistance, low latency and has also no privacy concerns.
(3) needs moderate latency, reliability of propagation and privacy.

If there were to be separate protocols for these, I would argue that
(3) should use opportunistic encryption always to increase transaction
source privacy, and (2) and (3) need authentication when one of the
peers is not fully validating.

BIP 150/151 give the tools to construct these.

>> Perhaps you deplore that fact, but I believe it is inevitable that different pieces of the network will make different choices here. You can't prevent people from create connections along preexisting trust lines. That does not mean that the network as a whole relies on first establishing trust everywhere.
>
> Of course, the network operates just fine without universal trust. My concern is not that it is required, but that it may grow significantly and will have a tendency to gravitate towards more effective registration mechanisms for what is a "good" peer. Even an informal but pervasive web of trust may make it difficult for untrusted parties to connect.

Maybe, but I'm very unconvinced that that will happen more than how
today IP and DNS-based "authentication" is used already (in very
inadvisable ways).

>> And I do think there are advantages.
>>
>> BIP 151 on its own gives you opportunistic encryption. You're very right to point out that this does not give you protection from active attackers, and that active attacking is relatively easy through sybil attacks. I still prefer my attacker to actually do that over just listening in on my connection.

> I agree, and I doubt this proposal will have much impact on an advanced persistent threat, or even lesser threats. People should understand that there is both a risk and a limited benefit to this proposal.

I believe the risk is only in misunderstanding what it is good for,
and there significant benefits to a network that encrypts connections
by default, as it excludes purely passive attackers.

> I believe you have misinterpreted my comments on distributed anonymous credentials (and the like) as commentary on the construction of BIP151 (and a subsequent auth proposal). As such your observation that it is exaggerated would make sense, but it is not what I intended. Encryption and auth are straightforward. Preventing bad nodes from participating in an anonymous distributed system is not.

Preventing bad nodes from participating is a very hard problem, if not
impossible. That doesn't mean we can't improve the current situation:
people are relying on node identity already, and doing so in ways that
have unclear attack vectors (IP spoofing, DNS poisoning, BGP routing
attacks). Adding optional and non-discoverable cryptographic
identities can improve this.

-- 
Pieter


-------------------------------------
The fallow period sounds waaaay to short. I suggest 2 months at minimum 
since anyone that wants to be safe needs to upgrade.

Also, please comment on why you won't use the much more safe and much 
smaller Flexible Transactions.

On Sunday, 16 October 2016 16:31:55 CEST Pieter Wuille via bitcoin-dev 
wrote:
> Hello all,
> 
> We're getting ready for Bitcoin Core's 0.13.1 release - the first one
> to include segregated witness (BIP 141, 143, 144, 145) for Bitcoin
> mainnet, after being extensively tested on testnet and in other
> software. Following the BIP9 recommendation [1] to set the versionbits
> start time a month in the future and discussion in the last IRC
> meeting [2], I propose we set BIP 141's start time to November 15,
> 2016, 0:00 UTC (unix time 1479168000).
> 
> Note that this is just a lower bound on the time when the versionbits
> signalling can begin. Activation on the network requires:
> (1) This date to pass
> (2) A full retarget window of 2016 blocks with 95% signalling the
> version bit (bit 1 for BIP141)
> (3) A fallow period consisting of another 2016 blocks.
> 
>   [1] https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki
>   [2]
> http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.
> 2016-10-13-19.04.log.html
> 
> Cheers,


-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Tuesday, June 21, 2016 9:42:39 PM Erik Aronesty wrote:
> > What do you mean by "replacement addresses" and "UI confirms" here?
> 
> "Replacement addresses" would take the place of BIP 32/47 support, if
> someone thought maybe that was too difficult to deal with.   So each time i
> paid Alice, Alice could generate a new payment address for the next monthly
> payment.   If you support BIP 32 pub seed, then there's no need for this.

I suppose it makes sense that since every payment requires communication with 
the recipient, that the recipient could give you a new scriptPubKey each time. 
No need to save [potentially compromised] payment info in advance?

> I don't know any wallets that support a BIP 32 pub seed (and then what,
> some random number generator?) as a destination address yet.

The point, as I see it, of payment protocol(s) is to deprecate addresses.
ie, this new protocol *could be* the BIP 32 pub seed destination address. ;)

> > Disagree with hard-coding intervals, or mandating specific policies from
> > the service providers.
> 
> I think mandating is a harsh word here, but i I'm a strong believer in
> providing strict guidelines that if people break, others can call them
> on.   Giving someone a 12.3 +/- 5 day interval for payments using this
> protocol would suck.   You should use payment channels for that stuff.
> The idea is a lightweight protocol for getting monthly subscriptions
> working.

Maybe just a field specifying how far in advance payments should be sent, 
then?

Luke


-------------------------------------
On Mar 10, 2016 17:28, "Mustafa Al-Bassam" <mus@musalbas.com> wrote:
>
> The fact that it takes very little time and effort to prevent a BIP from
reaching final status, means that in an base of millions of users it's
guaranteed that some disgruntled or bored person out there will attack it,
even if it's for the lulz.

I still fail to see the harm caused by this attack. At some point the
attacker will get bored of laughing even if the attack has a small costs
(which I'm not that sure it is).

> To reasonably expect that any hark fork - including an uncontroversial
one - will be adapted by every single person in a ecosystem of millions of
people, is wishful thinking and the BIP may as well say "hard fork BIPs
shall never reach final status."

This is what seem to have happened with uncontroversial softforks in the
past. Why is wishful thinking to expect the same for uncontroversial
hardforks?

-------------------------------------
On 11/16/2016 05:24 PM, Alex Morcos wrote:
> huh?
> can you give an example of how a duplicate transaction hash (in the same
> chain) can happen given BIP34?

"The pigeonhole principle arises in computer science. For example,
collisions are inevitable in a hash table because the number of possible
keys exceeds the number of indices in the array. A hashing algorithm, no
matter how clever, cannot avoid these collisions."

https://en.wikipedia.org/wiki/Pigeonhole_principle

e

> On Wed, Nov 16, 2016 at 7:00 PM, Eric Voskuil via bitcoin-dev wrote:
> 
>     On 11/16/2016 03:58 PM, Jorge Timón via bitcoin-dev wrote:
>     > On Wed, Nov 16, 2016 at 3:18 PM, Thomas Kerin via bitcoin-dev
>     > <bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>     >> BIP30 actually was given similar treatment after a reasonable amount of time
>     >> had passed.
>     >> https://github.com/bitcoin/bitcoin/blob/master/src/main.cpp#L2392
>     <https://github.com/bitcoin/bitcoin/blob/master/src/main.cpp#L2392>
>     >
>     > This is not really the same. BIP30 is not validated after BIP34 is
>     > active because blocks complying with BIP34 will always necessarily
>     > comply with BIP30 (ie coinbases cannot be duplicated after they
>     > include the block height).
> 
>     This is a misinterpretation of BIP30. Duplicate transaction hashes can
>     and will happen and are perfectly valid in Bitcoin. BIP34 does not
>     prevent this.
> 
>     e


-------------------------------------
On Thu, Feb 4, 2016 at 5:14 PM, jl2012 via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> https://github.com/bitcoin/bips/pull/317

I think this is a good idea, and I've independently proposed it in the past.

I agree with most of luke's language nitpicks.

It could, however, be pointed out that the version number flag is not
sufficient in the deployed network, because many clients also do not
validate the version field, due to a disinterest in security great
enough to not implement anything around height-in-coinbase.

So to fully achieve the intended effect using the highest bit of prev
would currently be much more effective.


-------------------------------------
On Tuesday, August 09, 2016 11:06:20 PM Daniel Hoffman via bitcoin-dev wrote:
> Is this good enough to warrant an official BIP number?

Yeah, let's call it BIP 170.

Next step is to:
- Fix the BIP number in the file
- Format it in the usual BIP mediawiki format instead of markdown
- Add it to a fork of the bitcoin/bips git repository
- Open a pull request against bitcoin/bips

P.S. Why are telephones considered 4-tone? DTMF is 16-tone IIRC?


-------------------------------------
On Tue, Jun 28, 2016 at 11:33 PM, Eric Voskuil <eric@voskuil.org> wrote:
> I don't follow this comment. The BIP aims quite clearly at "SPV" wallets as its justifying scenario.

It cites SPV as an example, doesn't mention bloom filters.. and sure--
sounds like the bip text should make the

>> Without something like BIP151 network participants cannot have privacy for the transactions they originate within the protocol against network observers.
>
> And they won't get it with BIP151 either. Being a peer is easier than observing the network.

Not passively, undetectable, and against thousands of users at once at low cost.

> If one can observe the encrypted traffic one can certainly use a timing attack to determine what the node has sent.

Not against Bitcoin Core, transactions are batched and relayed in
sorted order.  (obviously there are limits at what this provides;
ironically, the lack of link encryption has been used to argue against
privacy preserving relay behavior)

>> Even if, through some extraordinary effort, their own first hop is encrypted, unencrypted later hops would rapidly
>> expose significant information about transaction origins in the network.
>
> As will remain the case until all connections are encrypted and authenticated, and all participants are known to be good guys. Starting to sound like PKI?

Huh? The first and subsequent hops obscures the origin and timing.

>> Without something like BIP151 authenticated links are not possible, so
>> manually curated links (addnode/connect) cannot be counted on to provide protection against partitioning sybils.
>
> If we trust the manual links we don't need/want the other links. In fact retaining the other links enables the attack you described above. Of course there is no need to worry about Sybil attacks when all of your peers are authenticated. But again, let us not ignore the problems of requiring all peers on the network be authenticated.

Don't need and want them for what?  For _partitioning_ resistance,
you are not partitioned if you have one honest connection to the
functional network. Additional peers purely reduce your partition
vulnerability-- so long as an active network attacker isn't
itercepting all your connections out.

For privacy, you have improve transaction privacy so long as your
transaction isn't initially relayed to a malicious peer-- but
malicious peers can lie further out because transit nodes obscure the
order of message creation.  Bitcoin Core currently relays transactions
first and more frequently to outbound and whitelisted peers.

> Maybe I was insufficiently explicit. By "relies on identity" I meant that the BIP is not effective without it. I did not mean to imply that the BIP itself implements an identity scheme. I thought this was clear from the context.

I understood that, but my point was that Bitcoin cannot be used at
all_unless users have secure communication channels to share
addresses.

> then there is no reason to expect any effective improvement, since nodes will necessarily have to connect with anonymous peers.

They're not required to _only_ connect with anonymous peers. And
partition resistance requires that you have any one good link.

> Anyone with a node and the ability to monitor traffic should remain very effective.

Not via passive observation.

> Defining an auth implementation is not a hard problem, nor is it the concern I have raised.

Glad you agree.

We seem to be looping now. Feel free to not implement this proposal,
no one suggests making it mandatory.


-------------------------------------
Thanks for keeping on-topic, replying to the proposal, and being civil!

Replies inline.

On 02/09/16 09:00, Anthony Towns via bitcoin-dev wrote:
> On Mon, Feb 08, 2016 at 07:26:48PM +0000, Matt Corallo via bitcoin-dev wrote:
>> As what a hard fork should look like in the context of segwit has never
>> (!) been discussed in any serious sense, I'd like to kick off such a
>> discussion with a (somewhat) specific proposal.
> 
>> Here is a proposed outline (to activate only after SegWit and with the
>> currently-proposed version of SegWit):
> 
> Is this intended to be activated soon (this year?) or a while away
> (2017, 2018?)?

It's intended to activate when we have clear and broad consensus around
a hard proposal across the community.

>> 1) The segregated witness discount is changed from 75% to 50%. The block
>> size limit (ie transactions + witness/2) is set to 1.5MB. This gives a
>> maximum block size of 3MB and a "network-upgraded" block size of roughly
>> 2.1MB. This still significantly discounts script data which is kept out
>> of the UTXO set, while keeping the maximum-sized block limited.
> 
> This would mean the limits go from:
> 
>    pre-segwit  segwit pkh  segwit 2/2 msig  worst case
>    1MB         -           -                1MB
>    1MB         1.7MB       2MB              4MB
>    1.5MB       2.1MB       2.2MB            3MB
> 
> That seems like a fairly small gain (20% for pubkeyhash, which would
> last for about 3 months if you're growth rate means doubling every 9
> months), so this probably makes the most sense as a "quick cleanup"
> change, that also safely demonstrates how easy/difficult doing a hard
> fork is in practice?
>
> On the other hand, if segwit wallet deployment takes longer than
> hoped, the 50% increase for pre-segwit transactions might be a useful
> release-valve.
> 
> Doing a "2x" hardfork with the same reduction to a 50% segwit discount
> would (I think) look like:
> 
>    pre-segwit  segwit pkh  segwit 2/2 msig  worst case
>    1MB         -           -                1MB
>    1MB         1.7MB       2MB              4MB
>    2MB         2.8MB       2.9MB            4MB
> 
> which seems somewhat more appealing, without making the worst-case any
> worse; but I guess there's concern about the relay networking scaling
> above around 2MB per block, at least prior to IBLT/weak-blocks/whatever?


The goal isnt really to get a "gain" here...its mostly to decrease the
worst-case (4MB is pretty crazy) and keep the total size in-line with
what the network could handle. Getting 1MB blocks through the network in
under a second is already incredibly difficult...2MB is pretty scary and
will take lots of work...3MB is over the bound of "yea, we can pretty
for sure get that to work pretty well".


>> 2) In order to prevent significant blowups in the cost to validate
>> [...] and transactions are only allowed to contain
>> up to 20 non-segwit inputs. [...]
> 
> This could potentially make old, signed, but time-locked transactions
> invalid. Is that a good idea?


Hmmmmmm...you make a valid point. I was trying to avoid breaking old
transactions, but didnt think too much about time-locked ones.
Hmmmmmm...we could apply the limits to txn that dont have at least one
"newer than the fork input", but I'm not sure I like that either.


>> Along similar lines, we may wish to switch MAX_BLOCK_SIGOPS from
>> 1-per-50-bytes across the entire block to a per-transaction limit which
>> is slightly looser (though not too much looser - even with libsecp256k1
>> 1-per-50-bytes represents 2 seconds of single-threaded validation in
>> just sigops on my high-end workstation).
> 
> I think turning MAX_BLOCK_SIGOPS and MAX_BLOCK_SIZE into a combined
> limit would be a good addition, ie:
> 
>   #define MAX_BLOCK_SIZE       1500000
>   #define MAX_BLOCK_DATA_SIZE  3000000
>   #define MAX_BLOCK_SIGOPS     50000
> 
>   #define MAX_COST             3000000
>   #define SIGOP_COST           (MAX_COST / MAX_BLOCK_SIGOPS)
>   #define BLOCK_COST           (MAX_COST / MAX_BLOCK_SIZE)
>   #define DATA_COST            (MAX_COST / MAX_BLOCK_DATA_SIZE)
> 
>   if (utxo_data * BLOCK_COST + bytes * DATA_COST + sigops * SIGOP_COST
>        > MAX_COST)
>   {
>       block_is_invalid();
>   }
> 
> Though I think you'd need to bump up the worst-case limits somewhat to
> make that work cleanly.


There is a clear goal here of NOT using block-based limits and switching
to transaction-based limits. By switching to transaction-based limits we
avoid nasty issues with mining code either getting complicated or
enforcing too-strict limits on individual transactions.


>> 4) Instead of requiring the first four bytes of the previous block hash
>> field be 0s, we allow them to contain any value. This allows Bitcoin
>> mining hardware to reduce the required logic, making it easier to
>> produce competitive hardware [1].
>> [1] Simpler here may not be entirely true. There is potential for
>> optimization if you brute force the SHA256 midstate, but if nothing
>> else, this will prevent there being a strong incentive to use the
>> version field as nonce space. This may need more investigation, as we
>> may wish to just set the minimum difficulty higher so that we can add
>> more than 4 nonce-bytes.
> 
> Could you just use leading non-zero bytes of the prevhash as additional
> nonce?
> 
> So to work out the actual prev hash, set leading bytes to zero until
> you hit a zero. Conversely, to add nonce info to a hash, if there are
> N leading zero bytes, fill up the first N-1 (or less) of them with
> non-zero values.
> 
> That would give a little more than 255**(N-1) possible values
> ((255**N-1)/254) to be exact). That would actually scale automatically
> with difficulty, and seems easy enough to make use of in an ASIC?



-------------------------------------
On 03/02/2016 03:02 PM, Peter Todd wrote:
> On Wed, Mar 02, 2016 at 11:01:36AM -0800, Eric Voskuil via bitcoin-dev wrote:
>>> A 6 month investment with 3 months on the high subsidy and 3 months on low subsidy would not be made…
>>
>> Yes, this is the essential point. All capital investments are made based on expectations of future returns. To the extent that futures are perfectly knowable, they can be perfectly factored in. This is why inflation in Bitcoin is not a tax, it’s a cost. These step functions are made continuous by their predictability, removing that predictability will make them -- unpredictable.
> 
> You know, I do agree with you.
> 
> But see, this is one of the reasons why we keep reminding people that
> strictly speaking a hardfork *is* an altcoin, and the altcoin can change
> any rule currently in Bitcoin.
> 
> It'd be perfectly reasonable to create an altcoin with a 22-million-coin
> limit and an inflation schedule that had smooth, rather than abrupt,
> drops. It'd also be reasonable to make that altcoin start with the same
> UTXO set as Bitcoin as a means of initial coin distribution.
> 
> If miners choose to start mining that altcoin en-mass on the halving,
> all the more power to them. It's our choice whether or not we buy those
> coins. We may choose not to, but if 95% of the hashing power decides to
> go mine something different we have to accept that under our current
> chosen rules confirmations might take a long time.
> 
> Of course, personally I agree with Gregory Maxwell: this is all fairly
> unlikely to happen, so the discussion is academic. But we'll see.
> 
I agree, this is a perfectly rational interpretation. I also agree that
this particular instance is academic. But I see more to this than
accepting what is possible.

In the case of Federal Reserve Notes the gold obligation was abrogated.
This was (at least) a contract default, implemented by force of arms.
This contentious hard fork was clearly an attack.

But in a system with no authority and in which nobody has formed a
contractual obligation with anyone else, what would constitute an attack
on the money? There is no difference between state attacks on (or
collusion with) miners and miners acting on self interest.

One answer is that nothing is an attack, it's up to the market to
decide. But to the extent that there can be an attack on the money, the
attempt to move the value of the coin to an altcoin (hard fork) is it.
Though the choice of the term "attack" isn't essential.

The importance of recognizing an attack is that it affords one the
opportunity to defend against it. People holding "dollars" in 1933 were
ill equipped to defend against a system level attack (monetary policy),
in part because many did not recognize it as such, and in part because
there was insufficient preparation by those who did.

I see us building the tools and awareness necessary for defense. As you
say, nobody has to buy into an altcoin forked from their coin. This much
is simple to achieve. The more difficult problem is preserving the
utility of the original coin. Clearly the purpose of a hard fork (as
opposed to a new coin) is to transfer this value.

We've all seen arguments for contentious hard fork deployment that
explicitly depend on the fear of monetary loss to drag people to
acceptance. While this may be the nature of the technology, it's
important that we develop effective defense against it.

Ultimately the only defense is individual validation. The collusion of
banks (web wallets) with miners in attacking consensus is obvious. But
even without active collusion, the surrender of validation leaves people
just as defenseless as *being* unarmed while retaining a right to
*become* armed.

Even if every person mines at the same level, the system amounts to
little more than majority rule if validation is not decentralized. There
are people perfectly willing to exploit this weakness.

e


-------------------------------------
Hi

> My guess is the user was using a client that does not adjust TX fee, and
> needed to manually set it in order to get the TX in the block sooner,
> and meant 15 mBTC or something.
> 
> I suggest that either :
> 
> A) TX fee may not be larger than sum of outputs
> B) TX fee per byte may not be larger than 4X largest fee per byte in
> previous block

I don't think a such "feature" or lets say protection should be part of
the consensus layer.

Such checks should be done by the tx creation clients (wallets)  or 
nodes could have an option to not accept transaction with insane fees
into their mempool (policy).

</jonas>


-------------------------------------
Implemented a few of your suggestions.

Also opened a formal pull request for the BIP at
https://github.com/bitcoin/bips/pull/389 and the code at
https://github.com/bitcoin/bitcoin/pull/8068.

On 05/09/16 17:06, Pieter Wuille via bitcoin-dev wrote:
> On 05/03/2016 12:13 AM, lf-lists at mattcorallo.com (Matt Corallo) wrote:
>> Hi all,
>>
>> The following is a BIP-formatted design spec for compact block relay
>> designed to limit on wire bytes during block relay. You can find the
>> latest version of this document at
>> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.
> 
> Hi Matt,
> 
> thank you for working on this!
> 
>> ===New data structures===
>> Several new data structures are added to the P2P network to relay
>> compact blocks: PrefilledTransaction, HeaderAndShortIDs,
>> BlockTransactionsRequest, and BlockTransactions. Additionally, we
>> introduce a new variable-length integer encoding for use in these data
>> structures.
>>
>> For the purposes of this section, CompactSize refers to the
>> variable-length integer encoding used across the existing P2P protocol
>> to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.
> 
> This is a not, but I think it's a bit strange to have two separate
> variable length integers in the same specification. I understand is one
> is already the default for variable-length integers currently, and there
> are reasons to use the other one for efficiency reasons in some places,
> but perhaps we should aim to get everything using the latter?

Fixed, the whole thing now uses New Varints.

>> ====New VarInt====
>> Variable-length integers: bytes are a MSB base-128 encoding of the number.
>> The high bit in each byte signifies whether another digit follows. To make
>> sure the encoding is one-to-one, one is subtracted from all but the last
>> digit.
> 
> Maybe it's worth mentioning that it is based on ASN.1 BER's compressed
> integer format (see
> https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf
> section 8.1.3.5), though with a small modification to make every integer
> have a single unique encoding.
> 
>> ====HeaderAndShortIDs====
>> A HeaderAndShortIDs structure is used to relay a block header, the short
>> transactions IDs used for matching already-available transactions, and a
>> select few transactions which we expect a peer may be missing.
>>
>> |shortids||List of uint64_ts||8*shortids_length bytes||Little
>> Endian||The short transaction IDs calculated from the transactions which
>> were not provided explicitly in prefilledtxn
> 
> I tried to derive what length of short ids is actually necessary (some
> write-up is on
> https://gist.github.com/sipa/b2eb2e486156b5509ac711edd16153ed but it's
> incomplete).
> 
> For any reasonable numbers I can come up with (in a very wide range),
> the number of bits needed is very well approximated by:
> 
>   log2(#receiver_mempool_txn * #block_txn_not_in_receiver_mempool /
> acceptable_per_block_failure_rate)
> 
> For example, with 20000 mempool transactions, 2500 transactions in a
> block, 95% hitrate, and a chance of 1 in 10000 blocks to fail to
> reconstruct, needed_bits = log2(20000 * 2500 * (1 - 0.95) / 0.0001) =
> 34.54, or 5 byte txids would suffice.
> 
> Note that 1 in 10000 failures may sound like a lot, but this is for each
> individual connection, and since every transmission uses separately
> salted identifiers, occasional failures should not affect global
> propagation. Given that transmission failures due to timeouts, network
> connectivity, ... already occur much more frequently than once every few
> gigabytes (what 10000 blocks corresponds to), that's probably already
> more than enough.
> 
> In short: I believe 5 or 6 byte txids should be enough, but perhaps it
> makes sense to allow the sender to choose (so he can weigh trying
> multiple nonces against increasing the short txid length).

I switched to 6-byte short txids.

>> ====Short transaction IDs====
>> Short transaction IDs are used to represent a transaction without
>> sending a full 256-bit hash. They are calculated by:
>> # single-SHA256 hashing the block header with the nonce appended (in
>> little-endian)
>> # XORing each 8-byte chunk of the double-SHA256 transaction hash with
>> each corresponding 8-byte chunk of the hash from the previous step
>> # Adding each of the XORed 8-byte chunks together (in little-endian)
>> iteratively to find the short transaction ID
> 
> An alternative would be using SipHash-1-3 (a form of SipHash with
> reduced iteration counts; the default is SipHash-2-4). SipHash was
> designed as a Message Authentication Code, where the security
> requirements are much stronger than in our case (in particular, we don't
> care about observers being able to finding the key, as the key is just
> public knowledge here). One of the designers of SipHash has commented
> that SipHash-1-3 for collision resistance in hash tables may be enough:
> https://github.com/rust-lang/rust/issues/29754#issuecomment-156073946
> 
> Using SipHash-1-3 on modern hardware would take ~32 CPU cycles per txid.

Switched to SipHash2-4.

>> ===Implementation Notes===
> 
> There are a few more heuristics that MAY be used to improve performance:
> 
> * Receivers should treat short txids in blocks that match multiple
> mempool transactions as non-matches, and request the transactions. This
> significantly reduces the failure to reconstruct.

Done.

> * When constructing a compact block to send, the sender can verify it
> against its own mempool to check for collisions, and if so, choose to
> either try another nonce, or increase the short txid length.

Additionally we should compare to the orphan pool (which apparently
helps a lot).


-------------------------------------
On Sunday, February 07, 2016 2:16:02 PM Gavin Andresen wrote:
> On Sat, Feb 6, 2016 at 3:46 PM, Luke Dashjr via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> > On Saturday, February 06, 2016 5:25:21 PM Tom Zander via bitcoin-dev 
wrote:
> > > If you have a node that is "old" your node will stop getting new
> > > blocks. The node will essentially just say "x-hours behind" with "x"
> > > getting larger every hour. Funds don't get confirmed. etc.
> > 
> > Until someone decides to attack you. Then you'll get 6, 10, maybe more
> > blocks confirming a large 10000 BTC payment. If you're just a normal end
> > user (or perhaps an automated system), you'll figure that payment is good
> > and irreversibly hand over the title to the house.
> 
> There will be approximately zero percentage of hash power left on the
> weaker branch of the fork, based on past soft-fork adoption by miners (they
> upgrade VERY quickly from 75% to over 95%).

I'm assuming there are literally ZERO miners left on the weaker branch.
The attacker in this scenario simply rents hashing for a few days in advance 
to build his fake chain, then broadcasts the blocks to the unsuspecting 
merchant at ~10 block intervals so it looks like everything is working normal 
again. There are lots of mining rental services out there, and miners quite 
often do not care to avoid selling hashrate to the highest bidder regardless 
of what they're mining. 10 blocks worth costs a little more than 250 BTC - 
soon, that will be 125 BTC.

Luke


-------------------------------------
Hello,

Excellent news that segregated witness is nearing release for the mainnet.

I know I don't only speak for myself in saying that this has been
eagerly awaited for some time.

For the timing, I'd support segwit being usable on the network as soon
as is technically and safely possible.

We at JoinMarket are very interested in eventually using schnorr which
would allow signature aggregation and so reduce the cost of coinjoins.

Chris Belcher

On 16/10/16 15:31, Pieter Wuille via bitcoin-dev wrote:
> Hello all,
> 
> We're getting ready for Bitcoin Core's 0.13.1 release - the first one
> to include segregated witness (BIP 141, 143, 144, 145) for Bitcoin
> mainnet, after being extensively tested on testnet and in other
> software. Following the BIP9 recommendation [1] to set the versionbits
> start time a month in the future and discussion in the last IRC
> meeting [2], I propose we set BIP 141's start time to November 15,
> 2016, 0:00 UTC (unix time 1479168000).
> 
> Note that this is just a lower bound on the time when the versionbits
> signalling can begin. Activation on the network requires:
> (1) This date to pass
> (2) A full retarget window of 2016 blocks with 95% signalling the
> version bit (bit 1 for BIP141)
> (3) A fallow period consisting of another 2016 blocks.
> 
>   [1] https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki
>   [2] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.2016-10-13-19.04.log.html
> 
> Cheers,
> 


-------------------------------------
On Fri, Jan 29, 2016 at 03:31:05AM +0100, Jannes Faber via bitcoin-dev wrote:
> On the other hand when a non-contentious hard fork is rolled out, one could
> argue that it's actually best for everyone if the remaining 1% chain
> doesn't stand a chance of ever reaching 2016 blocks anymore (not even by a
> decent sized attacker trying to double spend on stragglers). Also causing
> all alarm bells to go off in the non-updated clients.
> 
> Have people thought through all the different scenarios yet?

I wrote up some of those risks in my "Soft Forks Are Safer Than Hard
Forks" post the other week:

https://petertodd.org/2016/soft-forks-are-safer-than-hard-forks

I was writing mainly in terms of technical risks for deployment
non-controversial forks; for controversial forks there's many more
failure scenarios. In any case, on technical grounds alone it's obvious
that hard-forks without very high - 95% or so - activation thresholds
are quite dangerous.

In general, it should be remembered that high activation thresholds for
hard-forks can always be soft-forked down after the fact. For instance,
suppose we initially used 100% support over the past one month of blocks
as a hard-fork threshold, but can't get more than 96% support. A
soft-fork with the following rule can be implemented:

    If 95% of the past blocks vote yes, voting against the hard-fork is
    not allowed.

As soft-forks can be rolled out quite quickly, implementing this in the
event that a hard-fork isn't getting sufficient support won't add much
delay to the overall process; as it is a soft-fork, only miners need to
adopt it for it to take effect.

For this reason I'd suggest any hard fork use 99%+ activation
thresholds, measured over multi-week timespan. Hard-forks should not be
controversial for good social/political reasons anyway, so there's
little harm in most cases to at worst delaying the fork by two or three
months if stragglers won't upgrade (in very rare cases like security
issues there may be exceptions; blocksize is certainly not one of those
cases).

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000005822b77a904129795a3ff4167c57ed1044f5a93512c830f

-------------------------------------
Hi


> ==Generating the master mnemonic==
> 
> The master mnemonic is first derived as a standard mnemonic as described
> in BIP39.



> ==From master mnemonic to derived mnemonics==
> 
> From the master mnemonic a new string is created:
> 
> string = MasterMnemonic + " " + Count + " " + Strength;
> 
> Here, MasterMnemonic are the space separated words of the master
> mnemonic. Count = 0, 1, 2 denotes the different derived mnemonics of a
> given strength and Strength = numWords / 3 * 32, where numWords is the
> number of words desired for the derived mnemonic and only integer
> arithmetic is used in the calculation (e.g. for numWords = 14, Strength
> = 128). Both Count and Strength are converted to strings.
> 
> This string is then hashed using sha512:
> 
> hash = sha512(string);

1)
My humble cryptographic understanding tells me that you should probably
use sha512_hmac where you add an passphrase and a salt.

2)
Side-note: Bip39 does still use PBKDF2 with 2048 iterations which I
personally consider "not enough" to protect a serious amount of funds.

Also the checksum based on the predetermined wordlist has some security
downsides over using a plain 32byte entropy (64hex chars) or a
base58check encoded extended private master key.

3)
Another idea:
What would speak against deriving a child key after bip32, lets say at
m/88'/0'/n' and use the derived 256bits to encode your mnemonic?
This would at least require your master mnemonic passphrase to derive a
valid "child mnemonic".

4)
I'm still not convinced if we should encourage users to "only store and
backup" the bip39 mnemonic.
Reconstructing funds from a seed can be difficult especially if you
don't have access to a trusted TX-indexed full node (~150GB of data
required).
Novice users might also underestimate the risk of losing metadata
coupled with their transactions when they only store the wallet seed.

</jonas>


-------------------------------------
Really nice idea. So its like a smart contract that incentivizes
publication that a server has been hacked? I also really like how the
funding has been handled -- with all the coins stored in the same address
and then each server associated with a unique signature. That way, you
don't have to split up all the coins among every server and reduce the
incentive for an attacker yet you can still identify which server was
hacked.

It would be nice if after the attacker broke into the server that they were
also incentivized to act on the information as soon as possible (revealing
early on when the server was compromised.) I suppose you could split up the
coins into different outputs that could optimally be redeemed by the owner
at different points in the future -- so they're incentivzed to act lest
their reward decays even more (this is of course, assuming that the
monetary reward for this is greater than any possible legal consequences
for the attacker -- it might not be. Thinking about this some more: it
would also be somewhat hard to deny that this -wasn't- a honeypot with such
a complex and unique scheme required for transactions, and I for one
wouldn't like to reveal that I'd hacked a server if I knew it was all a
calculated ploy. Don't honeypots rely on subtly?)

What about also proving to an attacker that by breaking into a server they
would be guaranteed a reward? I know that the use-case for this is proof of
compromise so incentivizing a security audit would kind of fall more into
an active invitation to audit but couldn't you also make a cryptocurrency
that allowed coins to be moved based on a service banner existing at a
given IP address? Attackers could then break into the server, setup a
service that broadcasts their public key hash, and then spend coins locked
at this special contract address to that pub key hash which miners would
check on redemption (putting aside malicious use-cases for now.)


On Wed, Aug 24, 2016 at 11:46 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Bitcoin-based honeypots incentivise intruders into revealing the fact they
> have
> broken into a server by allowing them to claim a reward based on secret
> information obtained during the intrusion. Spending a bitcoin can only be
> done
> by publishing data to a public place - the Bitcoin blockchain - allowing
> detection of the intrusion.
>
> The simplest way to achieve this is with one private key per server, with
> each
> server associated with one transaction output spendable by that key.
> However
> this isn't capital efficient if you have multiple servers to protect: if we
> have N servers and P bitcoins that we can afford to lose in the
> compromise, one
> key per server gives the intruder only N/P incentive.
>
> Previously Piete Wuille proposed(1) tree signatures for honeypots, with a
> single txout protected by a 1-N tree of keys, with each server assigned a
> specific key. Unfortunately though, tree signatures aren't yet implemented
> in
> the Bitcoin protocol.
>
> However with a 2-of-2 multisig and the SIGHASH_SINGLE feature we can
> implement
> this functionality with the existing Bitcoin protocol using the following
> script:
>
>     2 <honeypot-pubkey> <distriminator-pubkey> 2 CHECKMULTISIG
>
> The honeypot secret key is shared among all N servers, and left on them.
> The
> distriminator secret key meanwhile is kept secret, however for each server
> a
> unique signature is created with SIGHASH_SINGLE, paying a token amount to a
> notification address. For each individual server a pre-signed signature
> created
> with the distriminator secret key is then left on the associated server
> along
> with the honeypot secret key.
>
> Recall the SIGHASH_SINGLE flag means that the signature only signs a single
> transaction input and transaction output; the transaction is allowed to
> have
> additional inputs and outputs added. This allows the thief to use the
> honeypot
> key to construct a claim transaction with an additional output added that
> pays
> an address that they own with the rest of the funds.
>
> Equally, we could also use SIGHASH_NONE, with the per-server discriminator
> being the K value used in the pre-signed transaction.
>
> Note that Jeff Coleman deserves credit as co-inventor of all the above.
>
>
> Censorship Resistance
> =====================
>
> A potential disadvantage of using non-standard SIGHASH flags is that the
> transactions involved are somewhat unusual, and may be flagged by
> risk analysis at exchanges and the like, a threat to the fungibility of the
> reward.
>
> We can improve on the above concept from Todd/Coleman by using a pre-signed
> standard transaction instead. The pre-signed transaction spends the
> honeypot
> txout to two addresses, a per-server canary address, and a change address.
> The
> private key associated with the change addres is also left on the server,
> and
> the intruder can then spend that change output to finally collect their
> reward.
>
> To any external observer the result looks like two normal transactions
> created
> in the process of someone with a standard wallet sending a small amount of
> funds to an address, followed by sending a larger amount.
>
>
> Doublespending
> ==============
>
> A subtlety in the the two transactions concept is that the intruder doesn't
> have the necessary private keys to modify the first transaction, which
> means
> that the honeypot owner can respond to the compromise by doublespending
> that
> transaction, potentially recovering the honeypot while still learning
> about the
> compromise. While this is possible with all honeypots, if the first
> transaction
> is signed with the opt-in RBF flags, and CPFP-aware transaction
> replacement is
> not implemented by miners, the mechanics are particularly disadvantageous
> to
> the intruder, as the honeypot owner only needs to increase the first
> transaction's fee slightly to have a high chance of recovering their funds.
> With CPFP-aware transaction replacement the intruder could in-turn respond
> with
> a high-fee CPFP second transaction, but currently no such implementation is
> known.
>
>
> Scorched Earth
> ==============
>
> We can use the "scorched earth" concept to improve the credibility of the
> honeypot reward by making it costly for the honeypot owner to doublespend.
> Here
> a second version of the honeypot pre-signed transaction would also be
> provided
> which sepnds the entirety of the honeypot output to fees, and additionally
> spends a second output to fees. An economically rational intruder will
> publish
> the first version, which maximizes the funds they get out of the honeypot.
> If
> the owner tries to dishonestly doublespend, they can respond by publishing
> the
> "scorched earth" transaction, encouraging the honeypot owner's honesty and
> making CPFP-aware transaction replacement irrelevant.
>
> Of course, miner centralization adds complexity to the above: in many
> instances
> honeypot owners and/or intruders will be able to recover funds from
> altruistic
> miners. Equally, the additional complexity may discourage intruders from
> making
> use of the honeypot entirely.
>
> Note that as an implementation consideration CHECKSEQUENCEVERIFY can be
> used to
> ensure the honeypot output can only be spent with transaction replacement
> enabled, as CSV requires nSequence to be set in specific ways in any
> transation
> spending the output.
>
>
> References
> ==========
>
> 1) https://blockstream.com/2015/08/24/treesignatures/
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------

> On 8 Jun 2016, at 15:29, Luke Dashjr <luke@dashjr.org> wrote:
> 
> On Wednesday, June 08, 2016 5:57:36 AM Johnson Lau via bitcoin-dev wrote:
>> Why not make it even bigger, e.g. 75 bytes?
> 
> I don't see a sufficient answer to this question. Pieter explained why >75
> would be annoying, but 75 seems like it should be fine.
> 
>> In any case, since scripts with a 1-byte push followed by a push of >40
>> bytes remain anyone-can-spend, we always have the option to redefine them
>> with a softfork.
> 
> It's not that simple, since this is preventing use of the witness field for
> such scripts. With this limit in place, any such a softfork would suddenly
> require either two different witness commitments, or disabling the previous
> witness transaction format.
> 
> Luke

This is exactly why I proposed to extend the definition. My initial proposal was extending it to 33 bytes to effectively allow 16*256 new script versions, assuming we will keep using 32 bytes program hash.

If someday 32 bytes hash is deemed to be unsafe, the txid would also be unsafe and a hard fork might be needed. Therefore, I don’t see how a witness program larger than 40 bytes would be useful in any case (as it is more expensive and takes more UTXO space). I think Pieter doesn’t want to make it unnecessarily lenient.

-------------------------------------
>
> Because if not, the DPL is still better than the status quo.


Agreed. Also worth noting that it has a potential advantage over unilateral
patent disarmament, analogous to the advantage of copyleft licenses over
MIT/BSD: it provides an incentive (at least a theoretical one) for other
companies to adopt it too.

As many people have proposed, the best option, though one that would
require a lot of work, might be a dedicated Bitcoin-related defensive
patent pool—similar to Linux's Open Invention Network—that could
strategically deploy patent licenses to incentivize cooperation and punish
aggressors.

Along those lines, it'd be reasonable to consider changing the Bitcoin
> Core license to something like an Apache2/LGPL3 dual license to ensure the
> copyright license also has anti-patent protections.


I think Apache 2.0 would be a great license for Bitcoin Core. It not only
contains an explicit patent license grant (rather than MIT's implicit one),
but terminates that license if the licensee asserts a claim alleging that
the covered work infringes a patent. That might be an effective deterrent
against bringing patent claims based on alleged infringement in Bitcoin
Core. (I'm not sure I see a good reason to dual-license under the LGPL3,
but am curious to hear more.)

It would probably be feasible to upgrade to the Apache license for new
releases and contributions (leaving already-existing code and previous
releases under the MIT license—so basically a copyright "soft-fork"). Has
this been discussed before? Are there any obstacles or objections?

(These are my personal opinions, do not necessarily reflect the views of
any company, and are definitely not legal advice.)


On Fri, Oct 14, 2016 at 3:58 AM Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Fri, Oct 14, 2016 at 07:38:07AM -0300, Sergio Demian Lerner via
> bitcoin-dev wrote:
> > I read the DPL v1.1 and I find it dangerous for Bitcoin users. Current
> > users may be confident they are protected but in fact they are not, as
> the
> > future generations of users can be attacked, making Bitcoin technology
> > fully proprietary and less valuable.
>
> Glad to hear you're taking a conservative approach.
>
> So I assume Rootstock is going to do something stronger then, like
> Blockstream's DPL + binding patent pledge to only use patents defensively?
>
>     https://www.blockstream.com/about/patent_pledge/
>
> Because if not, the DPL is still better than the status quo.
>
> > If you read the DPL v1.1 you will see that companies that join DPL can
> > enforce their patents against anyone who has chosen not to join the DPL.
> > (http://defensivepatentlicense.org/content/defensive-patent-license)
> >
> > So basically most users of Bitcoin could be currently under threat of
> being
> > sued by Bitcoin companies and individuals that joined DPL in the same way
> > they might be under threat by the remaining companies. And even if they
> > joined DPL, they may be asked to pay royalties for the use of the
> > inventions prior joining DPL.
> >
> > DPL changes nothing for most individuals that cannot and will not hire
> > patent attorneys to advise them on what the DPL benefits are and what
> > rights they are resigning. Remember that patten attorneys fees may be
> > prohibitive for individuals in under-developed countries.
> >
> > Also DPL is revocable by the signers (with only a 180-day notice), so if
> > Bitcoin Core ends up using ANY DPL covered patent, the company owning the
> > patent can later force all new Bitcoin users to pay royalties.
>
> Indeed. However, you're also free to adopt the DPL irrevocably by
> additionally
> stating that you will never invoke that 180-day notice provision (or more
> humorously, make it a 100 year notice period to ensure any patents
> expire!).
>
> If you're concerned about this problem, I'd suggest that Rootstock do
> exactly
> that.
>
> > Because Bitcoin user base grows all the time with new individuals, the
> sole
> > existence of DPL licensed patents in Bitcoin represents a danger to
> Bitcoin
> > future almost the same as the existence of non-DPL license patents.
>
> To be clear, modulo the revocability provision, it's a danger mainly to
> those
> who are unwilling to adopt the DPL themselves, perhaps because they support
> software patents.
>
> > If you're publishing all your ideas and code (public disclosure), you
> > cannot later go and file a patent in most of the world except the US,
> where
> > you have a 1 year grace period. So we need to do something specific to
> > prevent the publishers filing a US patent.
>
> Again, lets remember that you personally proposed a BIP[1] that had the
> effect
> of aiding your ASICBOOST patent[2] without disclosing that fact in your
> BIP nor
> your pull-req[3]. The simple fact is we can't rely solely on voluntary
> disclosure - your own behavior is a perfect example of why not.
>
> [1]: BIP: https://github.com/BlockheaderNonce2/bitcoin/wiki
> [2]: ASICBOOST PATENT https://www.google.com/patents/WO2015077378A1?cl=en
> [3]: Extra nonce pull request: https://github.com/bitcoin/bit
> coin/pull/5102
>
> > What we need much more than DPL, we need that every BIP and proposal to
> the
> > Bitcoin mailing list contains a note that grants all Bitcoin users a
> > worldwide, royalty-free, no-charge, non-exclusive, irrevocable license
> for
> > the content of the e-mail or BIP.
>
> A serious problem here is the definition of "Bitcoin users". Does Bitcoin
> Classic count? Bitcoin Unlimited? What if Bitcoin forks?
>
> Better to grant _everyone_ a irrevocable license.
>
>
> Along those lines, it'd be reasonable to consider changing the Bitcoin Core
> license to something like an Apache2/LGPL3 dual license to ensure the
> copyright
> license also has anti-patent protections.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
> On 14/05/16 10:16, Jonas Schnelli via bitcoin-dev wrote:
>> Importing a bip32 wallet (bip44 or not) is still an expert job IMO.
> 
> That's simply not true. All reasonable wallets (reasonable = user
> oriented) now use BIP39 mnemonic for doing exactly this.

AFAIK: Bip39 import (cross-wallet) is not supported by Schildbachs
android wallet [1] and Electrum [2] and Breadwallet [3].

But I think forming a BIP39 mnemonic into a extended master private key
is not the problem here.

The problems I see:
* What if the "old" wallet has used more then 1000 addresses? I guess
some wallets do not even create a lookup window up to 1000 addresses.
There is a high chance of loosing funds when doing sweep (move all funds
to a new wallet) operation.

* I guess most or maybe all wallets will keep all keys (the
"lookup-window" keys) in the wallet database which could affect
performance [4]

* I guess most wallets do not offer "moving the funds to a new seed" [5]
which results in not solving the problem of a "lost" or "compromised"
wallet and implies wrong security to the enduser.

* If I import a bip39 mnemonic into a hardware wallet (assume Trezor or
Keepkey) I have to type in the words into my computer which bypasses
some of the security my hardware wallet provides me (MITM seed attack).
Together with the point above this reduces the security of a wallet (in
particular cold storage significant).

Please correct me if I'm wrong.

I just wanted to point out that importing a wallet is a tricky step
especially cross-wallet imports (I think cross wallet imports is an
experts job without further improvements).

[1] https://github.com/bitcoin-wallet/bitcoin-wallet/issues/245
[2] http://docs.electrum.org/en/latest/seedphrase.html
[3] https://github.com/voisine/breadwallet/issues/360
[4] https://github.com/bitcoin-wallet/bitcoin-wallet/issues/158
[5]
https://github.com/voisine/breadwallet/blob/master/BreadWallet/BRRestoreViewController.m#L225

</jonas>


-------------------------------------
On Thu, Jun 23, 2016 at 3:56 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> In any case, I'd strongly argue that we remove BIP75 from the bips
> repository,
> and boycott wallets that implement it. It's bad strategy for Bitcoin
> developers
> to willingly participate in AML/KYC, just the same way as it's bad for Tor
> to
> add wiretapping functionality, and W3C to support DRM tech. The minor
> tactical
> wins you'll get our of this aren't worth it.
>
>
Peter, BIP75 gives the parties transacting complete control over who they
choose to share their identity information with. This was the entire point
of the proposal. You authorize who you choose to give your payment address
to, and the sender can verify who they are sending payment to. All
communication and payment info are encrypted against third party snooping,
while still allowing asynchronous communication to accommodate ephemeral
mobile connections.

The fact that some people will choose to use this identity information for
AML/KYC purposes doesn't detract at all from the fact that it gives bitcoin
users the tools they need to keep their payment information private, and
only communicate it with the parties they choose.

Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com/>

-------------------------------------
On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <
sergio.d.lerner@gmail.com> wrote:

>
>
> You can find it here:
> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/
>
> Basically, the idea is to put in the first 64 bytes a 4 byte hash of the
> second 64-byte chunk. That design also allows increased nonce space in the
> first 64 bytes.
>
> My mistake here. I didn't recalled correctly my own idea. The idea is to
include in the second 64-byte chunk a 4-byte hash of the first chunk, not
the opposite.

-------------------------------------
On Sunday, December 20, 2015 10:56:33 AM joe2015--- via bitcoin-dev wrote:
> "generalized" softfork.

FWIW, this is something I've been planning to proposed (in a nicer form) for a 
while, tentatively called a "soft hardfork" (or less-seriously a "softserve 
hardfork"). The big piece missing that I've been holding off on publishing it 
as a BIP until complete, is a planned-out defensive reaction for a community 
which wishes to reject the hardfork. I guess I should probably prioritise this 
a bit more now...

Luke


-------------------------------------
On Saturday, September 05, 2015 9:19:51 PM Andy Chase wrote:
> Okay for sure yeah writing another proposal that reflects the current state
> of affairs as people see it might provide some interesting perspective on
> this proposal. I would welcome that.

Are you saying your proposal is intentionally not intended to reflect the 
reality? I wasn't talking about a "current state of affairs" for BIPs as much 
as that that the acceptance of BIPs is *defined by* the state of affairs.

Overall, I think something *similar to* this proposal is a good idea, but I 
disagree with how this proposal currently approaches the problem. Instead, 
what I would recommend is a specification based on BIP 123 that specifies the 
conditions under which a proposal is *known to be* accepted by the community 
(ie, discerning, not deciding), and establishes a way for a committee to 
review the BIP and *determine* if these conditions have been met. This would 
avoid a "disconnect" between the "official status" and reality, making the BIP 
process more useful to everyone.

Reviewing your current proposal:

> * It sets up '''committees''' for reviewing comments and indicating
> acceptance under precise conditions.

As mentioned, IMO a committee shouldn't be indicating acceptance, as much as 
it should be *determining* acceptance.

> ** Committees are authorized groups that represent client authors, miners,
> merchants, and users (each as a segment). Each one must represent at least
> 1% stake in the Bitcoin ecosystem.

1% seems like an awful lot to dedicate to BIP status changes.

> A committee system is used to organize the essential concerns of each
> segment of the Bitcoin ecosystem. Although each segment may have many
> different viewpoints on each BIP, in order to seek a decisive yes/no on a
> BIP, a representational authoritative structure is sought. This structure
> should be fluid, allowing people to move away from committees that do not
> reflect their views and should be re-validated on each BIP evaluation.

That sounds very time consuming. And what happens if these committees don't 
represent the community? What about when only part of the community - let's 
say 10% - decides to adopt a BIP that doesn't require consensus? Logically 
that BIP should still proceed...

> ** Proof of claim and minimum 1% stake via:
> *** Software: proof of ownership and user base (Min 1% of Bitcoin userbase)

But the Bitcoin user base is completely unknown, and tracking software user 
base is a privacy violation.

> ** Merchant: proof of economic activity (Min 1% of Bitcoin economic
> activity)

Bitcoin economic activity is also unknown, and it seems likely that merchants 
consider their own activity confidential.

> Mining: proof of work (Min 1% of Hashpower)

This needs a proper specification. How do miners express their positions?

> A BIP Process Manager should be chosen who is in charge of:

Chosen how, and by whom?

> == Conditions for activation ==
>
> In order for this process BIP to become active, it must succeed by its own
> rules. At least a 4% sample of the Bitcoin community must be represented,
> with at least one committee in each segment included. Once at least one
> committee has submitted a declaration, a request for comments will be called
> and the process should be completed from there.

Until this BIP is active, its rules do not apply, so this would be a form of 
circular reasoning. I like the idea of putting conditions for activation in 
the BIP text, but I don't think we can just let the author set any conditions 
they like either...

Luke


-------------------------------------
On Thu, Jun 30, 2016 at 11:57 AM, Eric Voskuil via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> The proliferation of node identity is my primary concern - this relates to privacy and the security of the network.

I think this is a reasonable concern.

However, node identity is already being used widely, and in a very
inadvisable way:
* Since forever there have been lists of 'good nodes' to pass in
addnode= configuration options.
* Various people run multiple nodes in different geographic locations,
peering with each other.
* Various pieces of infrastructure exist that relies on connecting to
well-behaving nodes (miner relay networks, large players peering
directly with each other, ...)
* Several lightweight clients support configuring a trusted host to connect to.

Perhaps you deplore that fact, but I believe it is inevitable that
different pieces of the network will make different choices here. You
can'tg prevent people from create connections along preexisting trust
lines. That does not mean that the network as a whole relies on first
establishing trust everywhere.

And I do think there are advantages.

BIP 151 on its own gives you opportunistic encryption. You're very
right to point out that this does not give you protection from active
attackers, and that active attacking is relatively easy through sybil
attacks. I still prefer my attacker to actually do that over just
listening in on my connection. And yes, we should also work on
improving the privacy nodes and wallets have orthogonal to encryption,
but nothing will make everything perfectly private.

BIP 151 plus a simple optional pre-shared-secret authentication
extension can improve upon pure IP-based authentication, as well as
simplify things like SSL tunnels, and onion addresses purely used as
identity. This will still require explicit configuration, but not more
than now.

BIP 151 plus a non-leaking public key authentication scheme (where
peers can query "are you the peer with pubkey X?" but don't learn
anything if the answer is no) with keys specific to the IP addresses
can give a TOFU-like security. Nodes already remember IP addresses
they've succesfully interacted with in the past, and ban IP addresses
that misbehave. Being able to tell whether a node you connect to is
the same as one you've connected to before is a natural extension of
this, and does not require establishing any real-world identity beyond
what we're already implicitly relying on.

Perhaps these use cases and their security assumptions should be
spelled out more clearly in the BIP. If there is a misunderstanding,
it should be clearly stated that BIP 151 is only a building block for
further improvements

> Secondarily I am concerned about users operating under a false assumption about the strength of privacy.

This is a widespread problem, but it exists far outside the scope of
this proposal. The privacy properties of Bitcoin are often
misrepresented and even used as advertizements. The solution is
education, not avoiding improvements because they may be
misunderstood.

> The complexity of the proposed construction is comparable to that of Bitcoin itself.

I really think this is an exaggeration. It's a diffie-hellman
handshake and a stream cipher (both very common constructions), that
apply to individual connections. There are no consensus risks nor a
requirement for coordinated change through the network. The
cryptographic code can be directly reused from a well-known project
(OpenSSH), and is very small in size.

-- 
Pieter


-------------------------------------
"This is already possible. Just nLockTime your withdrawls for some future
block. Don't sign any transaction that isn't nLockTime'd at least N blocks
beyond the present tip."

This would have prevented the Bitfinex hack if BitGo did this, but it
wouldn't have helped if the Bitfinex offline key had been compromised
instead of BitGo doing the 2nd sig.  In the BFX hack the TXNs were signed
by Bitfinex's hot key and BitGo's key, they required 2 of 2.

If I'm understanding correctly, what Matthew is proposing is a new type of
UTXO that is only valid to be spent as an nLockTime transaction and can be
reversed by some sort of RBF-type transaction within that time period, I
believe.

But I don't think this will work. What do you do if the keys are
compromised?  What's to stop the attacker from locking the coins up
indefinitely by repeatedly broadcasting a refund transaction each time you
try to spend to an uncompromised address?

You'd need a third distinct key required for the refund TXN that's separate
from the keys used to sign the initial nLockTime TXN.  And the refund TXN
would need to be able to go to a new address entirely.

On Aug 3, 2016 11:28 PM, "Luke Dashjr via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Wednesday, August 03, 2016 6:16:20 PM Matthew Roberts via bitcoin-dev
> wrote:
> > In light of the recent hack: what does everyone think of the idea of
> > creating a new address type that has a reversal key and settlement layer
> > that can be used to revoke transactions?
>
> This isn't something that makes sense at the address, since it represents
> the
> recipient and not the sender. Transactions are not sent from addresses
> ever.
>
> > You could specify so that transactions "sent" from these addresses must
> > receive N confirmations before they can't be revoked, after which the
> > transaction is "settled" and the coins become redeemable from their
> > destination output. A settlement phase would also mean that a
> transaction's
> > progress was publicly visible so transparent fraud prevention and
> auditing
> > would become possible by anyone.
>
> This is already possible. Just nLockTime your withdrawls for some future
> block. Don't sign any transaction that isn't nLockTime'd at least N blocks
> beyond the present tip.
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
> I think it does not make sense to try to get this standardized for
> current Bitcoin transactions. They are just too complex.
> 
> What might be interesting is to have something similar for Segwit and
> Lightning transactions.
> 
> * TREZOR performs extended validation of the inputs, when all of
> prev-txs are streamed into the device and validated. Your standard does
> not tackle this at all and I don't think it's worthy to make this
> standard unnecessarily complicated.

I'm aware of this approach but I don't think this makes sense long term.
We need a better way on the protocol level to validate inputs amounts
(where segwit is a first step towards this).

IMO, not having a standard for hardware wallet interfaces/communication
will long term result in reducing the end user experience.

I think we should collaborate together and work out a standard.

My goal is to add hardware wallet support in Bitcoin-Core where adding
proprietary code (plugin-ish) is something we don't want to do for the
sake of security and compatibility.

As said, the "BIP" is very draft and I'm happy to include the input
streaming as part of it (or you could add it if you want because you
have more experience with it).

</jonas>


-------------------------------------
If there is concern about the
block-with-valid-header-but-invalid-transactions-spam-attack, I have a
strategy using sync flags that may drastically reduce the problem.

Sync flags documented here:

https://github.com/moral-agent/sync_flags/blob/master/README.md)

The strategy to defeat the above attack is illustrated here:

https://s32.postimg.org/e94tqdqat/sync_flag_invalid_block.png

The key is to relax the requirement that a flag commit to a completely
valid block. The flag is valid if it commits to a valid block header, even
if the block body is invalid.

>From the perspective of an individual miner, they can safely commence
mining a flag the moment they obtain (or discover) a valid block header.

As soon as the spam is discovered, miners can choose to either abandon the
flag and return to mining on the previous block, or they can continue
mining on the flag.

It's difficult for me to game out which of these strategies would be
preferable. My first thought is that the miners should have the incentive
to mine whichever option has the fewest miners, which should result in a
50/50 split.

However, the miners who continue mining the flag have a chance of ending up
in a situation where they mine the flag before anyone mines a valid block.
If this happens, it is sub-optimal for them. They can start mining for the
next valid block but if someone else broadcasts a valid block header they
will be in the same pickle that miners under the current protocol are: they
must either keep mining for a valid block, or SPV mine the newly arrived
block while they do validation. The third option, of mining a flag, is not
available to them, because the flag has already been mined for this cycle.

As a result of the above, it may be most rational for miners to (upon
learning that they are mining a flag on top of an invalid block) split
their hashpower unevenly between the flag and continuing to mine for a
valid block. The hashpower split reflects their estimates of the cost of
the above negative outcome. I think the split would be pretty close to
50/50, but deviations from 50/50 would not necessarily be bad. For example,
if they split 52/48, with more hashpower toward finding the valid block
instead of the flag, then that decreases the likelyhood that the flag will
be discovered before the next valid block, which is good for all of the
miners. So it's a nice positive feedback.

*****

This approach mostly neutralizes the harm done by the (currently very rare)
invalid block spam attack. As a kind of amazing side effect, the work done
to produce the spam is incorporated into the blockchain cumulative Proof of
Work, and the spammer is not paid for this contribution.

-------------------------------------
One thing which hasn't been addressed yet in this thread is developer centralization. Unlike other applications we want to ensure that it's not only possible for users to refuse an upgrade, but easy. While this by no means lessens the retirement that users run up to date software for security reasons, finding the right line to draw is difficult. 

Matt

On December 15, 2016 2:44:55 PM PST, Ethan Heilman via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>I assume this has been well discussed in at some point in the Bitcoin
>community, so I apologize if I'm repeating old ideas.
>
>Problem exploitable nodes:
>It is plausible that people running these versions of bitcoind may not
>be applying patches. Thus, these nodes may be vulnerable to known
>exploits. I would hope none of these nodes are gateway nodes for
>miners, web wallets or exchanges. How difficult would it be to crawl
>the network to find vulnerable nodes and exploit them? What percentage
>of the network is running vulnerable versions of bitcoind?
>
>Problem eclipsable nodes:
>Currently a bitcoind node disconnects from any node with a version
>below MIN_PEER_PROTO_VERSION. Such nodes become be ripe for an eclipse
>attack because they are partitioned from the newer nodes, especially
>when they are "freshly obsolete". I have not examined how protocol
>versioning works in detail so I could be missing something.
>
>One option could be that after a grace period:
>1. to still connect to obsolete nodes and even to transmit
>blockheaders,
>2. but to stop sending the full-blocks and transactions to these
>nodes, thereby alerting the operator that something is wrong and
>causing them to upgrade.
>It may make sense to create this as a rule, if your longest chain
>consists of only blockheaders and no one will tell you the
>transactions for over 1000 blocks you are obsolete, spit out an error
>message and shutdown.
>
>This would not address the issue of alt-coins which are forked from
>old vulnerable versions of bitcoind, but that is probably out of
>scope.
>
>On Thu, Dec 15, 2016 at 1:48 PM, Jorge Timón via bitcoin-dev
><bitcoin-dev@lists.linuxfoundation.org> wrote:
>> On Thu, Dec 15, 2016 at 4:38 AM, Juan Garavaglia via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> Older node versions may generate issues because some upgrades will
>make
>>> several of the nodes running older protocol versions obsolete and or
>>> incompatible. There may be other hard to predict behaviors on older
>versions
>>> of the client.
>>
>> Hard to predict or not, you can't force people to run newer software.
>>
>>> In order to avoid such wide fragmentation of "Bitcoin Core” node
>versions
>>> and to help there be a more predictable protocol improvement
>process, I
>>> consider it worth it to analyze introducing some planned
>obsolescence in
>>> each new version. In the last year we had 4 new versions so if each
>version
>>> is valid for about 1 year (52560 blocks) this may be a reasonable
>time frame
>>> for node operators to upgrade. If a node does not upgrade it will
>stop
>>> working instead of participating in the network with an outdated
>protocol
>>> version.
>>
>> When you introduce anti-features like this in free software they can
>> be trivially removed and they likely will.
>>
>>> These changes may also simplify the developer's jobs in some cases
>by
>>> avoiding them having to deal with ancient versions of the client.
>>
>> There's a simpler solution for this which is what is being done now:
>> stop maintaining and giving support for older versions.
>> There's limited resources and developers are rarely interested in
>> fixing bugs for very old versions. Users shouldn't expect things to
>be
>> backported to old versions (if developers do it and there's enough
>> testing, there's no reason not to do more releases of old versions,
>it
>> is just rarely the case).
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev@lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
On 08/08/2016 11:00 AM, Jonas Schnelli via bitcoin-dev wrote:
> # ___known-peers___ contains known identity-public-keys together with a
> network identifier (IP & port), similar to the "known-host" file
> supported by openssh.


I have mixed feelings about strictly tying the identity-public-keys with 
a network identifier. I think the purpose of this is to detect if 
someone has physically stolen and compromised my bitcoin node and placed 
it on another network under control of an attacker. This seems to be a 
bit of a benefit, however, an attacker could always spoof the original 
network identifier anyway.

I run my bitcoin node on an internet connection that does not guarantee 
a static IP address (although it usually stays the same for several 
weeks or months at a time). I'd like to be able to make secure 
connections back to my own node, even if I know the IP address may 
change from time to time. There are several reasons for wanting to this 
with a changing IP. The first is because the bandwidth on my internet 
connection with a guaranteed static IP address is considerably more 
expensive than my internet connection without a guaranteed static IP 
address. The second reason is because the DNS PTR record for my static 
IP address is personally identifiable based on other reasons/services. 
The internet connection that my bitcoin node is using without a 
guaranteed static IP address just has a PTR record that basically 
includes my IP address and ISP name. This isn't much use to the general 
public (although my ISP obviously knows who I am). The third reason is 
that I consider it a good thing from a privacy perspective if my IP 
address changes every once and a while.

Maybe a strict check option where the identity-public-keys must 
optionally match a specific network identifier would be a compromise? 
Maybe this is up to the client implementation to decide, so it should 
just be suggested in the BIP rather than required?




> # ___authorized-peers___ contains authorized identity-public-keys

Is there an option for a wildcard here? Couldn't there be a case where 
the client wants to authenticate, but the bitcoin node does not care who 
it's clients are? This would be similar to many of the http based 
bitcoin block explorer API services that are out there. The API 
operators have built up some reputation, so people use them, but they 
don't necessarily care about who their users are.







> === Local identity key management ===
> Each peer can configure one identity-key (ECC, 32 bytes) per listening
> network interface (IPv4, IPv6, tor).

What if I have bitcoind listening on multiple IPv4 interfaces? Can I 
have a different identity-key for each IPv4 interface?

Also, would it be possible to only allow this authentication on specific 
interfaces? In my example above where I have two internet connections, 
if you don't agree to loosening the tie between the network identifier 
and the identity-public-keys, maybe I would just connect my bitcoin node 
to both internet connections, but only allow a few authorized-peers on 
the static IP (which would be low bandwidth), and then not authenticate 
on the internet connection with the changing IP at all

If you don't want to increase complexity by adding these options, one 
could always accomplish the same thing by runing two instances of 
bitcoind and pairing the two over a local network, it would just be a 
waste of resources.




> == Disadvantages ==
>
> The protocol may be slow if a peer has a large authorized-peers database
> due to the requirement of iterating and hashing over all available
> authorized peers identity-public-keys.


Does openssh have this same problem?

I'm assuming this could be parallelized very easily, so it is not a huge 
problem?







-------------------------------------
On Sat, Jun 18, 2016 at 4:01 PM, Peter Todd <pete@petertodd.org> wrote:

>
>
Have you seen how BLAKE2 omits padding when the data to be hashed happens
> to be
> exactly one block in size? It's significantly faster than SHA256, and
> that's a
> standard part of the algorithm already.
>

That's very convenient! I didn't know it, but had 'look up how blake2 does
padding' in my list of stuff to do. I'm leaning heavily towards using
blake2b at this point, at least for internal hashing.


>
> > At the root there's a branch block. It consists of all nodes up to some
> > fixed depth - let's say 12 - with that depth set so that it roughly fits
> > within a single memory page. Branch blocks are arranged with the nodes in
> > fixed position defined by the prefix they correspond to, and the
> terminals
> > have outpointers to other blocks. Because they're all clustered
> together, a
> > lookup or update will only require a single
>
> A single....?
>

Okay, I've figured out the root cause of general confusion here. It's
mostly my fault.

There are a few different media on which data can be stored, with different
properties in terms of how long it takes to retrieve data from them, and
how much of a readahead they typically have. I was misreading the l2 cache
size as the main memory readahead amount, which is... probably wrong? The
readahead properties of memory aren't well documented and apparently vary a
lot. On SSDs it typically pulls down a kilobyte at once and they call them
pages, hence my use of that term above. But since my real point is that my
implementation should act as a silver bullet which can have acceptable
performance even on extremely bad devices, I'll give an analysis of how
well it works when everything is stored on a regular spinning hard drive.

Let's say you're storing 100 million items, which will fit within 10
gigabytes. If you set the block depths to about 10 bits they'll be about
32K, and if you set the size of leaf blocks to be about the same then
memory efficiency will be good because the leaf blocks will store twigs of
about 2^7 in size while having 2^10 space so they'll fit reasonably. Almost
everything will be three blocks from root, so updates will generally
require two disk seeks (plus one more for a write but those are generally
faster because they get batched.)

For latency numbers, I'm going off these:
https://gist.github.com/jboner/2841832

If the blockchain is very full of simple transactions and a disk seek takes
15 milliseconds, then going with the assumption that a block is about 600
seconds and the blockchain can handle 4 transactions per second and each of
them is 3 updates (one utxo spent plus two new ones created) that's 15 *
600 * 4 * 3 * 2 milliseconds per block, or about 200 seconds per block, so
if the uxto roots trail by a few blocks even a ludicrously underpowered
node could keep up.

On an SSD keeping up is completely trivial, the problem becomes one of how
quickly you can validate an entire blockchain history. There a read takes
about 0.15 milliseconds and you have to do 5 of them instead of 2 because
the natural memory block size is 4k, so it's about 1 millisecond per
update, or 600 * 4 * 3 total time for each block, which is about 7 seconds.
That's large enough that making the utxo root trail by two blocks is still
a good idea, but small enough that it isn't a big factor in the cost of
running a node. It's enough that validating a complete block history might
take a while though, and even validating just the last year would take a
few days. This is very conservative and it's assuming that *everything* is
kept on an SSD though. If the numbers work better and a few layers are kept
in regular memory validating a whole year of history might only take a few
hours.

Hopefully that all makes a fairly good case that raw merkle tree utxo root
trailing by a few blocks is a viable strategy. The data structures in the
MMR proposal are fairly complicated and the analysis of them talks in
somewhat vague terms about things being fast and slow. A similar analysis
of the MMR proposal specifying storage media and expectations of latency
numbers would clarify the reasoning a lot.

(By the way, sorry for the slow response - I got preempted by a bunch of
other work duties.)

-------------------------------------
On Wednesday, December 14, 2016 11:01:58 AM Johnson Lau via bitcoin-dev wrote:
> There is no reason to use a timestamp beyond 4 bytes.

Actually, there is: lock times... my overflow solution doesn't have a solution 
to that. :x


-------------------------------------
On Wednesday, July 20, 2016 5:46:54 AM Peter Todd via bitcoin-dev wrote:
> On Tue, Jul 19, 2016 at 10:35:39PM -0600, Sean Bowe via bitcoin-dev wrote:
> > I'm requesting feedback for Hash Time-Locked Contract (HTLC) transactions
> > in Bitcoin.
> > 
> > HTLC transactions allow you to pay for the preimage of a hash. CSV/CLTV
> > can be used to recover your funds if the other party is not cooperative.
> > These
> > 
> > scripts take the following general form:
> >     [HASHOP] <digest> OP_EQUAL
> >     OP_IF
> >     
> >         <seller pubkey>
> >     
> >     OP_ELSE
> >     
> >         <num> [TIMEOUTOP] OP_DROP <buyer pubkey>
> >     
> >     OP_ENDIF
> >     OP_CHECKSIG
> 
> Note that because you're hashing the top item on the stack regardless
> scriptSig's that satisfy HTLC's are malleable: that top stack item can be
> changed anything in the digest-not-provided case and the script still
> passes.

OP_SIZE
OP_IF
  [HASHOP] <digest> OP_EQUALVERIFY
  <seller pubkey>
OP_ELSE
  <num> [TIMEOUTOP]
  <buyer pubkey>
OP_ENDIF
OP_CHECKSIG



-------------------------------------
I understand the need for people to make repeated payments to 
individuals in real life that they know, without the payee every even 
taking the effort to make a formal payment request (say you're just 
paying a family member of friend back for picking something up for you 
at the store, and you've already payed them many times before).

For a subscription, wouldn't it be better to promote payment channels or 
just send another payment request? I've been brainstorming recently 
about a model where service providers could deliver invoices, receipts, 
and payment requests in a standardized and secure way. In addition to 
having a send, receive, and transaction history tab in your bitcoin 
wallet, you'd also have an open payment channels tab (which would 
include all applications on your computer that have an open real time 
payment channel, such as a wifi access point, web browser, voip 
provider, etc.), as well as a "bills to pay" tab. Since everything would 
be automated and consolidated locally, you wouldn't have to deal with 
logging into a million different websites to get the bills and then pay 
them. If it were this easy, why would you ever want to do a recurring 
payment from a single payment request? I understand why you may think 
you want to given current work flows, but I'm wondering if it may be 
better to just skip over to a completely better way of doing things.


Andy Schroder

On 06/22/2016 11:30 AM, Erik Aronesty wrote:
> My conclusion at the bottom of that post was to keep BIP 75 the same, 
> don't change a bit, and stick any subscription information (future 
> payment schedule) in the PaymentACK.   Then the wallet then 
> re-initiates an invoice (unattended or attended.. up to the user), 
> after the subscription interval is passed. Subscriptions are pretty 
> important for Bitcoin to be used as a real payment system. 



-------------------------------------
On 11/17/2016 07:40 AM, Johnson Lau wrote:
>
>> On 17 Nov 2016, at 20:22, Eric Voskuil via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>> Given that hash collisions are unquestionably possible,
>
> Everything you said after this point is irrelevant.

So... you think hash collisions are not possible, or that it's moot
because Core has broken its ability to handle them.


> Having hash collision is **by definition** a consensus failure,

I suppose if you take fairly recent un-BIPped consensus changes in Core
to be the definition of consensus, you would be right about that.


> or a hardfork.

And those changes could definitely result in a chain split. So right
about that too.


> You could replace the already-on-chain tx with the collision and
create 2 different versions of UTXOs (if the colliding tx is valid), or
make some nodes to accept a fork with less PoW (if the colliding tx is
invalid, or making the block invalid, such as being to big).


Not in accordance with BIP30 and not according to the implementation of
it that existed in Core until Nov 2015. A tx was only valid as a
"replacement" if it did not collide with the hash of an existing tx with
unspent outputs. The collision would have been rejected. And an invalid
colliding tx would not be accepted in any case (since nodes presumably
validate blocks and don't rely on checkpoints as a security measure).

A transaction duplicating the hash of another and taking its place in a
block would not only have to collide the hash, but it would have to be
fully valid in the context of the block you are suggesting it is
substituted into. In that case it's simply a fully valid block. This is
not just the case of a hash collision, this is the case of a hash
collision where both transactions are fully valid in the context of the
same block parent. Even if that unlikely event did occur, it's not a
hard fork, it's a reorg. The chain that builds on this block will be
valid to all nodes but necessarily deviates from the other block's valid
chain. This is true whether the magical block is assembled via compact
blocks or otherwise.

Transaction "replacement" is an implementation detail of Core. Once Core
accepted a replacement of a previously spent transaction it would be
unable to provide the previous block/spent-tx, but that would be a
wallet failure and an inability to provide valid historical blocks, not
a consensus/validation failure. The previously spent outputs no longer
contribute to validation, unless there is a reorg back to before the
original tx's block, and at that point it would be moot, since neither
transaction is on the chain.

You are referring to the *current* behavior ("replacement" without
concern for collision). That was an unpublished hard fork, and is the
very source of the problems you are describing.

> To put it simply, the Bitcoin protocol is broken. So with no doubt,
Bitcoin Core and any implementation of the Bitcoin protocol should
assume SHA256 collision is unquestionably **impossible**.

I'm not disagreeing with you that it is broken. I'm pointing out that it
was broken by code that was merged recently - an undocumented hard fork
that reverted the documented BIP30 behavior that was previously
implemented correctly, based on the assumption that hash collisions
cannot occur, for the modest performance boost of not having to check
for unspent duplicates (sounds sort of familiar).

> If some refuse to make such assumption, they should have introduced an
alternative hash algorithm and somehow run it in parallel with SHA256 to
prevent the consensus failure.

No hash algorithm can prevent hash collisions, including one that is
just two running in parallel. A better resolution would be to fix the
problem.

There is no need to replace the BIP30 rule. That resolves the TX hash
collision problem from a consensus standpoint. In order to serve up
whole blocks in the circumstance requires a more robust store than I
believe is exists in Core, but that has nothing to do with validity.

The block hash check and signature validation caching splits caused by
collision can easily be avoided, and doing so doesn't break with
consensus. I'm not aware of any other aspects of consensus that are
effected by an implementation assumption of non-colliding hashes. But in
any case I'm pretty sure there aren't any that are necessary to consensus.

e



-------------------------------------
On Wed, Aug 17, 2016 at 9:19 PM, Gregory Maxwell <gmaxwell@gmail.com> wrote:

> On Thu, Aug 18, 2016 at 12:11 AM, Sergio Demian Lerner via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > I think that we're not attacking the real source of the problem: that the
> > witness data size is not signed.
>
> It's not possible to do that for the general case, since you may not
> even know the witness size in advance (even for checksig's ECDSA, the
> encoding is variable sized).
>
> That's why scripts can check a maximum witness size, and not necessarily
an exact value.


I think that is overly focusing on "someone might change the feerate",
> yes that is an example of an undesirable witness tampering, but it's
> not the only one.
>
> I don't think fees are the problem. There is another problem. Let me
re-explain.
If I send a transaction to an IoT device (say to an OpenDime or to the old
Firmcoin), and the OpenDime must verify that the transaction has been mined
(SPV verification), then it may expect the witness program to be of certain
maximum size (an implementation-imposed  limit). If a Miner modifies the
witness size and makes it too large, then the device may not be able to
accept the transaction and the bitcoins may be lost. Lost because the
private key is in the device, and because the device cannot accept that
cloned transaction, never ever.

The same is true (although less strict) for side-chains and drive-chains:
they may have certain restrictions on the size of transactions they accept
to lock bitcoins.

That's why I'm proposing that a transaction becomes INVALID if the witness
size is higher than the expected size (by the sender).

-------------------------------------
Please note that the segregated witness (BIP141) consensus rule is updated. Originally, a witness program is a scriptPubKey or redeemScript that consists of a 1-byte push opcode (OP_0 to OP_16) followed by a data push between 2 and 32 bytes. The definition is now extended to 2 to 40 bytes:
https://github.com/bitcoin/bips/commit/d1b52cb198066d4e515e8a50fc3928c5397c3d9b https://github.com/bitcoin/bitcoin/pull/7910/commits/14d4d1d23a3cbaa8a3051d0da10ff7a536517ed0


Why?
----------
BIP141 defines only version 0 witness program: 20 bytes program for P2WPKH and 32 bytes program for P2WSH. Versions 1 to 16 are not defined, and are considered as anyone-can-spend scripts, reserved for future extension (e.g. the proposed BIP114). BIP141 also requires that only a witness program input may have witness data. Therefore, before this update, an 1-byte push opcode followed by a 33 bytes data push was not considered to be a witness program, and no witness data is allowed for that.

This may be over-restrictive for a future witness program softfork. When 32-byte program is used, this leaves only 16 versions for upgrade, and any “sub-version” metadata must be recorded in the witness field. This may not be compatible with some novel hashing functions we are exploring.

By extending the maximum length by 8 bytes, it allows up to 16 * 2 ^ 64 versions for future upgrades, which is enough for any foreseeable use.


Why not make it even bigger, e.g. 75 bytes?
----------
A 40 bytes witness program allows a 32-byte hash with 8-byte metadata. For any scripts that are larger than 32 bytes, they should be recorded in the witness field, like P2WSH in BIP141, to reduce the transaction cost and impact on UTXO set. Since SHA256 is already used everywhere, it is very unlikely that we would require a larger witness program (e.g. SHA512) without also a major revamp of the bitcoin protocol.

In any case, since scripts with a 1-byte push followed by a push of >40 bytes remain anyone-can-spend, we always have the option to redefine them with a softfork.


What are affected?
----------
As defined in BIP141, a version 0 witness program is valid only with 20 bytes (P2WPKH) or 32 bytes (P2WSH). Before this update, an OP_0 followed by a data push of 33-40 bytes was not a witness program and considered as anyone-can-spend. Now, such a script will fail due to incorrect witness program length.

Before this update, no witness data was allowed for a script with a 1-byte push followed by a data push of 33-40 bytes. This is now allowed.



Actions to take:
----------
If you are running a segnet node, or a testnet node with segwit code, please upgrade to the latest version at https://github.com/bitcoin/bitcoin/pull/7910

If you have an alternative implementation, please make sure your consensus code is updated accordingly, or your node may fork off the network.

-------------------------------------
On Sat, Feb 6, 2016 at 3:46 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Saturday, February 06, 2016 5:25:21 PM Tom Zander via bitcoin-dev wrote:
> > On Saturday, February 06, 2016 06:09:21 PM Jorge Timón via bitcoin-dev
> wrote:
> > > None of the reasons you list say anything about the fact that "being
> > > lost" (kicked out of the network) is a problem for those node's users.
> >
> > That's because its not.
> >
> > If you have a node that is "old" your node will stop getting new blocks.
> > The node will essentially just say "x-hours behind" with "x" getting
> larger
> > every hour. Funds don't get confirmed. etc.
>
> Until someone decides to attack you. Then you'll get 6, 10, maybe more
> blocks
> confirming a large 10000 BTC payment. If you're just a normal end user (or
> perhaps an automated system), you'll figure that payment is good and
> irreversibly hand over the title to the house.
>

There will be approximately zero percentage of hash power left on the
weaker branch of the fork, based on past soft-fork adoption by miners (they
upgrade VERY quickly from 75% to over 95%).

So it will take a week to get 6 confirmations.

If you are a full node, you are warned that your software is obsolete and
you must upgrade.

If you are a lightweight node, it SHOULD tell you something is wrong, but
even if it doesn't, given that people running lightweight nodes run them so
they don't have to be connected to the network 24/7, it is very likely
during that week you disconnect and reconnect to the network several times.
And every time you do that you increase your chances that you will connect
to full nodes on the majority branch of the chain, where you will be told
about the double-spend.

All of that is assuming that there is no OTHER mitigation done. DNS seeds
should avoid reporting nodes that look like they are in the middle of
initial block download (that are at a block height significantly behind the
rest of the network), for example.

-- 
--
Gavin Andresen

-------------------------------------
On Sun, Oct 2, 2016 at 11:19 PM, Matt Corallo <lf-lists@mattcorallo.com> wrote:
> Even if the Bitcoin Foundation decided to recklessly disregard Bitcoin's
> future centralization, I'm not sure going to them and asking them to pay
> a license fee in order to keep from holding the rest of the Bitcoin
> mining community hostage counts as "regard for centralization pressure".
> It also doesn't excuse the lack of transparent licensing being available
> today, or the lack of transparency when discussing it in public after
> the patent had been filed.

We can't change the past (besides, would you want BCF to have owned
that patent? I didn't)-- only the future.

To do so requires collaboration, so lets focus on that.


-------------------------------------

On Feb 7, 2016, at 7:19 AM, Anthony Towns via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

> The stated reasoning for 75% versus 95% is "because it gives "veto power"
> to a single big solo miner or mining pool". But if a 20% miner wants to
> "veto" the upgrade, with a 75% threshold, they could instead simply use
> their hashpower to vote for an upgrade, but then not mine anything on
> the new chain. At that point there'd be as little as 55% mining the new
> 2MB chain with 45% of hashpower remaining on the old chain. That'd be 18
> minute blocks versus 22 minute blocks, which doesn't seem like much of
> a difference in practice, and at that point hashpower could plausibly
> end up switching almost entirely back to the original consensus rules
> prior to the grace period ending.


Keep in mind that within a single difficulty adjustment period, the difficulty of mining a block on either chain will be identical. Even if the value of a 1MB branch coin is $100 and the hashrate on the 1 MB branch is 100 PH/s, and the value of a 2 MB branch coin is $101 and the hashrate on the 2 MB branch is 1000 PH/s, the rational thing for a miner to do (for the first adjustment period) is to mine on the 2 MB branch, because the miner would earn 1% more on that branch.

So you're assuming that 25% of the hashrate chooses to remain on the minority version during the grace period, and that 20% chooses to switch back to the minority side. The fork happens. One branch has 1 MB blocks every 22 minutes, and the other branch has 2 MB blocks every 18 minutes. The first branch cannot handle the pre-fork transaction volume, as it only has 45% of the capacity that it had pre-fork. The second one can, as it has 111% of the pre-fork capacity. This makes the 1 MB branch much less usable than the 2 MB branch, which in turn causes the market value of newly minted coins on that branch to fall, which in turn causes miners to switch to the more profitable 2MB branch. This exacerbates the usability difference, which exacerbates the price difference, etc. Having two competing chains with equal hashrate using the same PoW function and nearly equal features is not a stable state. Positive feedback loops exist to make the vast majority of the users and the hashrate join one side.

Basically, any miners who stick to the minority branch are going to lose a lot of money.

-------------------------------------
On Tuesday, August 16, 2016 2:10:04 PM Jonas Schnelli via bitcoin-dev wrote:
> The BIP describes two approaches how to communicate (pipe and
> URI-scheme) with the signing-devices app, although, in my opinion, all
> major platform do support the URI approach (maybe we could drop the pipe
> approach then).

IMO it's kindof ugly to abuse URIs for communication. Stdio pipes are pretty 
universally supported, why not just use those?

On the other hand, no matter how the plugin is implemented, it's still a 
security risk, and requires installation (which the user might not have access 
for). It would be best if the hardware protocol were standardised, so the user 
doesn't need a plugin of *any* sort... I notice some hardware wallets have 
begun to implement (or reuse) Trezor's interface, so that would seem a good 
place to start?

Luke


-------------------------------------
Thanks for this proposal. Just some quick response:

1. The segwit hardfork (BIP HF) could be deployed with BIP141 (segwit
softfork). BIP141 doesn't need grace period. BIP HF will have around 1 year
of grace period.

2. Threshold is 95%. Using 4 versoin bits: a) BIP 141; b) BIP HF; c) BIP 141
if BIP HF has already got 95%; d) BIP HF if BIP141 has already got 95%.
Voting a and c (or b and d) at the same time is invalid. BIP 141 is
activated if a>95% or (a+c>95% and b+d>95%). BIP HF is activated if b>95% or
(a+c>95% and b+d>95%).

3. Fix time warp attack: this may break some SPV implementation

4. Limiting non-segwit inputs may make some existing signed tx invalid. My
proposal is: a) count the number of non-segwit sigop in a tx, including
those in unexecuted branch (sigop); b) measure the tx size without scripgSig
(size); c) a new rule is SUM(sigop*size) < some_value . This allows
calculation without actually running the script.


-----Original Message-----
From: bitcoin-dev-bounces@lists.linuxfoundation.org
[mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Matt
Corallo via bitcoin-dev
Sent: Tuesday, 9 February, 2016 03:27
To: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
Subject: [bitcoin-dev] On Hardforks in the Context of SegWit

Hi all,

I believe we, today, have a unique opportunity to begin to close the book on
the short-term scaling debate.

First a little background. The scaling debate that has been gripping the
Bitcoin community for the past half year has taken an interesting turn in
2016. Until recently, there have been two distinct camps - one proposing a
significant change to the consensus-enforced block size limit to allow for
more on-blockchain transactions and the other opposing such a change,
suggesting instead that scaling be obtained by adding more flexible systems
on top of the blockchain. At this point, however, the entire Bitcoin
community seems to have unified around a single vision - roughly 2MB of
transactions per block, whether via Segregated Witness or via a hard fork,
is something that can be both technically supported and which adds more
headroom before second-layer technologies must be in place. Additionally, it
seems that the vast majority of the community agrees that segregated witness
should be implemented in the near future and that hard forks will be a
necessity at some point, and I don't believe it should be controversial
that, as we have never done a hard fork before, gaining experience by
working towards a hard fork now is a good idea.

With the apparent agreement in the community, it is incredibly disheartening
that there is still so much strife, creating a toxic environment in which
developers are not able to work, companies are worried about their future
ability to easily move Bitcoins, and investors are losing confidence. The
way I see it, this broad unification of visions across all parts of the
community places the burden of selecting the most technically-sound way to
achieve that vision squarely on the development community.

Sadly, the strife is furthered by the huge risks involved in a hard fork in
the presence of strife, creating a toxic cycle which prevents a safe hard
fork. While there has been talk of doing an "emergency hardfork" as an
option, and while I do believe this is possible, it is not something that
will be easy, especially for something as controversial as rising fees.
Given that we have never done a hard fork before, being very careful and
deliberate in doing so is critical, and the technical community working
together to plan for all of the things that might go wrong is key to not
destroying significant value.

As such, I'd like to ask everyone involved to take this opportunity to
"reset", forgive past aggressions, and return the technical debates to
technical forums (ie here, IRC, etc).

As what a hard fork should look like in the context of segwit has never
(!) been discussed in any serious sense, I'd like to kick off such a
discussion with a (somewhat) specific proposal.

First some design notes:
* I think a key design feature should be taking this opportunity to add
small increases in decentralization pressure, where possible.
* Due to the several non-linear validation time issues in transaction
validation which are fixed by SegWit's signature-hashing changes, I strongly
believe any hard fork proposal which changes the block size should rely on
SegWit's existence.
* As with any hard fork proposal, its easy to end up pulling in hundreds of
small fixes for any number of protocol annoyances. In order to avoid doing
this, we should try hard to stick with a few simple changes.

Here is a proposed outline (to activate only after SegWit and with the
currently-proposed version of SegWit):

1) The segregated witness discount is changed from 75% to 50%. The block
size limit (ie transactions + witness/2) is set to 1.5MB. This gives a
maximum block size of 3MB and a "network-upgraded" block size of roughly
2.1MB. This still significantly discounts script data which is kept out of
the UTXO set, while keeping the maximum-sized block limited.

2) In order to prevent significant blowups in the cost to validate
pessimistic blocks, we must place additional limits on the size of many
non-segwit transactions. scriptPubKeys are now limited to 100 bytes in size
and may not contain OP_CODESEPARATOR, scriptSigs must be push-only (ie no
non-push opcodes), and transactions are only allowed to contain up to 20
non-segwit inputs. Together these limits limit total-bytes-hashed in block
validation to under 200MB without any possibility of making existing outputs
unspendable and without adding additional per-block limits which make
transaction-selection-for-mining difficult in the face of attacks or
non-standard transactions. Though 200MB of hashing (roughly 2 seconds of
hash-time on my high-end
workstation) is pretty strongly centralizing, limiting transactions to fewer
than 20 inputs seems arbitrarily low.

Along similar lines, we may wish to switch MAX_BLOCK_SIGOPS from
1-per-50-bytes across the entire block to a per-transaction limit which is
slightly looser (though not too much looser - even with libsecp256k1
1-per-50-bytes represents 2 seconds of single-threaded validation in just
sigops on my high-end workstation).

3) Move SegWit's generic commitments from an OP_RETURN output to a second
branch in the merkle tree. Depending on the timeline this may be something
to skip - once there is tooling for dealing with the extra OP_RETURN output
as a generic commitment, the small efficiency gain for applications checking
the witness of only one transaction or checking a non-segwit commitment may
not be worth it.

4) Instead of requiring the first four bytes of the previous block hash
field be 0s, we allow them to contain any value. This allows Bitcoin mining
hardware to reduce the required logic, making it easier to produce
competitive hardware [1].

I'll deliberately leave discussion of activation method out of this
proposal. Both jl2012 and Luke-Jr recently begun some discussions about
methods for activation on this list, and I'd love to see those continue.
If folks think a hard fork should go ahead without SPV clients having a say,
we could table #4, or activate #4 a year or two after 1-3 activate.


[1] Simpler here may not be entirely true. There is potential for
optimization if you brute force the SHA256 midstate, but if nothing else,
this will prevent there being a strong incentive to use the version field as
nonce space. This may need more investigation, as we may wish to just set
the minimum difficulty higher so that we can add more than 4 nonce-bytes.




Obviously we cannot reasonably move forward with a hard fork as long as the
contention in the community continues. Still, I'm confident continuing to
work towards SegWit as a 2MB-ish soft-fork in the short term with some plans
on what a hard fork should look like if we can form broad consensus can go a
long way to resolving much of the contention we've seen.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------

> On August 17, 2016 at 12:40 AM Luke Dashjr <luke@dashjr.org> wrote:
>
>
> On Wednesday, August 17, 2016 3:02:53 AM Johnson Lau via bitcoin-dev wrote:
> > To completely replicate the original behaviour, one may use:
> > "DEPTH TOALTSTACK IFDUP DEPTH FROMALTSTACK NUMNOTEQUAL IF 2DROP {if script}
> > ELSE DROP {else script} ENDIF"
>
> This is much uglier than expected. IMO if that's the best workaround for the
> current behaviour, people should just use "OP_1 OP_EQUAL OP_IF" when/if they
> need to avoid malleability issues.

It is ugly only if you want to faithfully replicate the behaviour. I'd argue that in no real use case you need to do this. For example, "OP_SIZE OP_IF" could just become "OP_SIZE OP_0NOTEQUAL OP_IF", since OP_SIZE must return a valid MINIMALDATA number.

And your workaround does not fix malleability, since any non-0x01 values are valid FALSE

However, in some case, enforcing MINIMALIF does require 1 more witness byte: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-August/013036.html

I think the best strategy is to make it a relay policy first, and decide whether we want a softfork later.
-------------------------------------
Great post, Peter.

4) By fixing the problem (or possibly just "fixing" the problem) are
we encouraging/legitimising blockchain use-cases other than BTC value
transfer? Should we?

I don't think it would encourage non-value-transfer usage more
because, as you noted, many such use cases are valuable enough that
people are willing to pay much higher transaction fees in order to
have their data timestamped. I think it's more an issue of the block
space / transaction fee market since the cost of making a transaction
is directly borne by users, as opposed to the cost of the UTXO set
which may not be borne by them if they don't run a full node.

I'm of the opinion that if the world decides that Bitcoin is more
valuable as a trustworthy generalized timestamping mechanism than as a
value transfer system, protocol developers shouldn't try to steer the
ship against the wind. As more people and use cases enter the
ecosystem, the most valuable ones ought to survive - I hope that this
market will be fostered by the developers.

- Jameson


On Tue, May 17, 2016 at 9:23 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> # Motivation
>
> UTXO growth is a serious concern for Bitcoin's long-term decentralization.
> To
> run a competitive mining operation potentially the entire UTXO set must be
> in
> RAM to achieve competitive latency; your larger, more centralized,
> competitors
> will have the UTXO set in RAM. Mining is a zero-sum game, so the extra
> latency
> of not doing so if they do directly impacts your profit margin. Secondly,
> having possession of the UTXO set is one of the minimum requirements to
> run a
> full node; the larger the set the harder it is to run a full node.
>
> Currently the maximum size of the UTXO set is unbounded as there is no
> consensus rule that limits growth, other than the block-size limit itself;
> as
> of writing the UTXO set is 1.3GB in the on-disk, compressed serialization,
> which expands to significantly more in memory. UTXO growth is driven by a
> number of factors, including the fact that there is little incentive to
> merge
> inputs, lost coins, dust outputs that can't be economically spent, and
> non-btc-value-transfer "blockchain" use-cases such as anti-replay oracles
> and
> timestamping.
>
> We don't have good tools to combat UTXO growth. Segregated Witness
> proposes to
> give witness space a 75% discount, in part of make reducing the UTXO set
> size
> by spending txouts cheaper. While this may change wallets to more often
> spend
> dust, it's hard to imagine an incentive sufficiently strong to discourage
> most,
> let alone all, UTXO growing behavior.
>
> For example, timestamping applications often create unspendable outputs
> due to
> ease of implementation, and because doing so is an easy way to make sure
> that
> the data required to reconstruct the timestamp proof won't get lost - all
> Bitcoin full nodes are forced to keep a copy of it. Similarly anti-replay
> use-cases like using the UTXO set for key rotation piggyback on the
> uniquely
> strong security and decentralization guarantee that Bitcoin provides; it's
> very
> difficult - perhaps impossible - to provide these applications with
> alternatives that are equally secure. These non-btc-value-transfer
> use-cases
> can often afford to pay far higher fees per UTXO created than competing
> btc-value-transfer use-cases; many users could afford to spend $50 to
> register
> a new PGP key, yet would rather not spend $50 in fees to create a standard
> two
> output transaction. Effective techniques to resist miner censorship exist,
> so
> without resorting to whitelists blocking non-btc-value-transfer use-cases
> as
> "spam" is not a long-term, incentive compatible, solution.
>
> A hard upper limit on UTXO set size could create a more level playing
> field in
> the form of fixed minimum requirements to run a performant Bitcoin node,
> and
> make the issue of UTXO "spam" less important. However, making any coins
> unspendable, regardless of age or value, is a politically untenable
> economic
> change.
>
>
> # TXO Commitments
>
> A merkle tree committing to the state of all transaction outputs, both
> spent
> and unspent, we can provide a method of compactly proving the current
> state of
> an output. This lets us "archive" less frequently accessed parts of the
> UTXO
> set, allowing full nodes to discard the associated data, still providing a
> mechanism to spend those archived outputs by proving to those nodes that
> the
> outputs are in fact unspent.
>
> Specifically TXO commitments proposes a Merkle Mountain Range¹ (MMR), a
> type of deterministic, indexable, insertion ordered merkle tree, which
> allows
> new items to be cheaply appended to the tree with minimal storage
> requirements,
> just log2(n) "mountain tips". Once an output is added to the TXO MMR it is
> never removed; if an output is spent its status is updated in place. Both
> the
> state of a specific item in the MMR, as well the validity of changes to
> items
> in the MMR, can be proven with log2(n) sized proofs consisting of a merkle
> path
> to the tip of the tree.
>
> At an extreme, with TXO commitments we could even have no UTXO set at all,
> entirely eliminating the UTXO growth problem. Transactions would simply be
> accompanied by TXO commitment proofs showing that the outputs they wanted
> to
> spend were still unspent; nodes could update the state of the TXO MMR
> purely
> from TXO commitment proofs. However, the log2(n) bandwidth overhead per
> txin is
> substantial, so a more realistic implementation is be to have a UTXO cache
> for
> recent transactions, with TXO commitments acting as a alternate for the
> (rare)
> event that an old txout needs to be spent.
>
> Proofs can be generated and added to transactions without the involvement
> of
> the signers, even after the fact; there's no need for the proof itself to
> signed and the proof is not part of the transaction hash. Anyone with
> access to
> TXO MMR data can (re)generate missing proofs, so minimal, if any, changes
> are
> required to wallet software to make use of TXO commitments.
>
>
> ## Delayed Commitments
>
> TXO commitments aren't a new idea - the author proposed them years ago in
> response to UTXO commitments. However it's critical for small miners'
> orphan
> rates that block validation be fast, and so far it has proven difficult to
> create (U)TXO implementations with acceptable performance; updating and
> recalculating cryptographicly hashed merkelized datasets is inherently more
> work than not doing so. Fortunately if we maintain a UTXO set for recent
> outputs, TXO commitments are only needed when spending old, archived,
> outputs.
> We can take advantage of this by delaying the commitment, allowing it to be
> calculated well in advance of it actually being used, thus changing a
> latency-critical task into a much easier average throughput problem.
>
> Concretely each block B_i commits to the TXO set state as of block
> B_{i-n}, in
> other words what the TXO commitment would have been n blocks ago, if not
> for
> the n block delay. Since that commitment only depends on the contents of
> the
> blockchain up until block B_{i-n}, the contents of any block after are
> irrelevant to the calculation.
>
>
> ## Implementation
>
> Our proposed high-performance/low-latency delayed commitment full-node
> implementation needs to store the following data:
>
> 1) UTXO set
>
>     Low-latency K:V map of txouts definitely known to be unspent. Similar
> to
>     existing UTXO implementation, but with the key difference that old,
>     unspent, outputs may be pruned from the UTXO set.
>
>
> 2) STXO set
>
>     Low-latency set of transaction outputs known to have been spent by
>     transactions after the most recent TXO commitment, but created prior
> to the
>     TXO commitment.
>
>
> 3) TXO journal
>
>     FIFO of outputs that need to be marked as spent in the TXO MMR. Appends
>     must be low-latency; removals can be high-latency.
>
>
> 4) TXO MMR list
>
>     Prunable, ordered list of TXO MMR's, mainly the highest pending
> commitment,
>     backed by a reference counted, cryptographically hashed object store
>     indexed by digest (similar to how git repos work). High-latency ok.
> We'll
>     cover this in more in detail later.
>
>
> ### Fast-Path: Verifying a Txout Spend In a Block
>
> When a transaction output is spent by a transaction in a block we have two
> cases:
>
> 1) Recently created output
>
>     Output created after the most recent TXO commitment, so it should be
> in the
>     UTXO set; the transaction spending it does not need a TXO commitment
> proof.
>     Remove the output from the UTXO set and append it to the TXO journal.
>
> 2) Archived output
>
>     Output created prior to the most recent TXO commitment, so there's no
>     guarantee it's in the UTXO set; transaction will have a TXO commitment
>     proof for the most recent TXO commitment showing that it was unspent.
>     Check that the output isn't already in the STXO set (double-spent),
> and if
>     not add it. Append the output and TXO commitment proof to the TXO
> journal.
>
> In both cases recording an output as spent requires no more than two
> key:value
> updates, and one journal append. The existing UTXO set requires one
> key:value
> update per spend, so we can expect new block validation latency to be
> within 2x
> of the status quo even in the worst case of 100% archived output spends.
>
>
> ### Slow-Path: Calculating Pending TXO Commitments
>
> In a low-priority background task we flush the TXO journal, recording the
> outputs spent by each block in the TXO MMR, and hashing MMR data to obtain
> the
> TXO commitment digest. Additionally this background task removes STXO's
> that
> have been recorded in TXO commitments, and prunes TXO commitment data no
> longer
> needed.
>
> Throughput for the TXO commitment calculation will be worse than the
> existing
> UTXO only scheme. This impacts bulk verification, e.g. initial block
> download.
> That said, TXO commitments provides other possible tradeoffs that can
> mitigate
> impact of slower validation throughput, such as skipping validation of old
> history, as well as fraud proof approaches.
>
>
> ### TXO MMR Implementation Details
>
> Each TXO MMR state is a modification of the previous one with most
> information
> shared, so we an space-efficiently store a large number of TXO commitments
> states, where each state is a small delta of the previous state, by sharing
> unchanged data between each state; cycles are impossible in merkelized data
> structures, so simple reference counting is sufficient for garbage
> collection.
> Data no longer needed can be pruned by dropping it from the database, and
> unpruned by adding it again. Since everything is committed to via
> cryptographic
> hash, we're guaranteed that regardless of where we get the data, after
> unpruning we'll have the right data.
>
> Let's look at how the TXO MMR works in detail. Consider the following TXO
> MMR
> with two txouts, which we'll call state #0:
>
>       0
>      / \
>     a   b
>
> If we add another entry we get state #1:
>
>         1
>        / \
>       0   \
>      / \   \
>     a   b   c
>
> Note how it 100% of the state #0 data was reused in commitment #1. Let's
> add two more entries to get state #2:
>
>             2
>            / \
>           2   \
>          / \   \
>         /   \   \
>        /     \   \
>       0       2   \
>      / \     / \   \
>     a   b   c   d   e
>
> This time part of state #1 wasn't reused - it's wasn't a perfect binary
> tree - but we've still got a lot of re-use.
>
> Now suppose state #2 is committed into the blockchain by the most recent
> block.
> Future transactions attempting to spend outputs created as of state #2 are
> obliged to prove that they are unspent; essentially they're forced to
> provide
> part of the state #2 MMR data. This lets us prune that data, discarding it,
> leaving us with only the bare minimum data we need to append new txouts to
> the
> TXO MMR, the tips of the perfect binary trees ("mountains") within the MMR:
>
>             2
>            / \
>           2   \
>                \
>                 \
>                  \
>                   \
>                    \
>                     e
>
> Note that we're glossing over some nuance here about exactly what data
> needs to
> be kept; depending on the details of the implementation the only data we
> need
> for nodes "2" and "e" may be their hash digest.
>
> Adding another three more txouts results in state #3:
>
>                   3
>                  / \
>                 /   \
>                /     \
>               /       \
>              /         \
>             /           \
>            /             \
>           2               3
>                          / \
>                         /   \
>                        /     \
>                       3       3
>                      / \     / \
>                     e   f   g   h
>
> Suppose recently created txout f is spent. We have all the data required to
> update the MMR, giving us state #4. It modifies two inner nodes and one
> leaf
> node:
>
>                   4
>                  / \
>                 /   \
>                /     \
>               /       \
>              /         \
>             /           \
>            /             \
>           2               4
>                          / \
>                         /   \
>                        /     \
>                       4       3
>                      / \     / \
>                     e  (f)  g   h
>
> If an archived txout is spent requires the transaction to provide the
> merkle
> path to the most recently committed TXO, in our case state #2. If txout b
> is
> spent that means the transaction must provide the following data from
> state #2:
>
>             2
>            /
>           2
>          /
>         /
>        /
>       0
>        \
>         b
>
> We can add that data to our local knowledge of the TXO MMR, unpruning part
> of
> it:
>
>                   4
>                  / \
>                 /   \
>                /     \
>               /       \
>              /         \
>             /           \
>            /             \
>           2               4
>          /               / \
>         /               /   \
>        /               /     \
>       0               4       3
>        \             / \     / \
>         b           e  (f)  g   h
>
> Remember, we haven't _modified_ state #4 yet; we just have more data about
> it.
> When we mark txout b as spent we get state #5:
>
>                   5
>                  / \
>                 /   \
>                /     \
>               /       \
>              /         \
>             /           \
>            /             \
>           5               4
>          /               / \
>         /               /   \
>        /               /     \
>       5               4       3
>        \             / \     / \
>        (b)          e  (f)  g   h
>
> Secondly by now state #3 has been committed into the chain, and
> transactions
> that want to spend txouts created as of state #3 must provide a TXO proof
> consisting of state #3 data. The leaf nodes for outputs g and h, and the
> inner
> node above them, are part of state #3, so we prune them:
>
>                   5
>                  / \
>                 /   \
>                /     \
>               /       \
>              /         \
>             /           \
>            /             \
>           5               4
>          /               /
>         /               /
>        /               /
>       5               4
>        \             / \
>        (b)          e  (f)
>
> Finally, lets put this all together, by spending txouts a, c, and g, and
> creating three new txouts i, j, and k. State #3 was the most recently
> committed
> state, so the transactions spending a and g are providing merkle paths up
> to
> it. This includes part of the state #2 data:
>
>                   3
>                  / \
>                 /   \
>                /     \
>               /       \
>              /         \
>             /           \
>            /             \
>           2               3
>          / \               \
>         /   \               \
>        /     \               \
>       0       2               3
>      /       /               /
>     a       c               g
>
> After unpruning we have the following data for state #5:
>
>                   5
>                  / \
>                 /   \
>                /     \
>               /       \
>              /         \
>             /           \
>            /             \
>           5               4
>          / \             / \
>         /   \           /   \
>        /     \         /     \
>       5       2       4       3
>      / \     /       / \     /
>     a  (b)  c       e  (f)  g
>
> That's sufficient to mark the three outputs as spent and add the three new
> txouts, resulting in state #6:
>
>                         6
>                        / \
>                       /   \
>                      /     \
>                     /       \
>                    /         \
>                   6           \
>                  / \           \
>                 /   \           \
>                /     \           \
>               /       \           \
>              /         \           \
>             /           \           \
>            /             \           \
>           6               6           \
>          / \             / \           \
>         /   \           /   \           6
>        /     \         /     \         / \
>       6       6       4       6       6   \
>      / \     /       / \     /       / \   \
>    (a) (b) (c)      e  (f) (g)      i   j   k
>
> Again, state #4 related data can be pruned. In addition, depending on how
> the
> STXO set is implemented may also be able to prune data related to spent
> txouts
> after that state, including inner nodes where all txouts under them have
> been
> spent (more on pruning spent inner nodes later).
>
>
> ### Consensus and Pruning
>
> It's important to note that pruning behavior is consensus critical: a full
> node
> that is missing data due to pruning it too soon will fall out of
> consensus, and
> a miner that fails to include a merkle proof that is required by the
> consensus
> is creating an invalid block. At the same time many full nodes will have
> significantly more data on hand than the bare minimum so they can help
> wallets
> make transactions spending old coins; implementations should strongly
> consider
> separating the data that is, and isn't, strictly required for consensus.
>
> A reasonable approach for the low-level cryptography may be to actually
> treat
> the two cases differently, with the TXO commitments committing too what
> data
> does and does not need to be kept on hand by the UTXO expiration rules. On
> the
> other hand, leaving that uncommitted allows for certain types of soft-forks
> where the protocol is changed to require more data than it previously did.
>
>
> ### Consensus Critical Storage Overheads
>
> Only the UTXO and STXO sets need to be kept on fast random access storage.
> Since STXO set entries can only be created by spending a UTXO - and are
> smaller
> than a UTXO entry - we can guarantee that the peak size of the UTXO and
> STXO
> sets combined will always be less than the peak size of the UTXO set alone
> in
> the existing UTXO-only scheme (though the combined size can be temporarily
> higher than what the UTXO set size alone would be when large numbers of
> archived txouts are spent).
>
> TXO journal entries and unpruned entries in the TXO MMR have log2(n)
> maximum
> overhead per entry: a unique merkle path to a TXO commitment (by "unique"
> we
> mean that no other entry shares data with it). On a reasonably fast system
> the
> TXO journal will be flushed quickly, converting it into TXO MMR data; the
> TXO
> journal will never be more than a few blocks in size.
>
> Transactions spending non-archived txouts are not required to provide any
> TXO
> commitment data; we must have that data on hand in the form of one TXO MMR
> entry per UTXO. Once spent however the TXO MMR leaf node associated with
> that
> non-archived txout can be immediately pruned - it's no longer in the UTXO
> set
> so any attempt to spend it will fail; the data is now immutable and we'll
> never
> need it again. Inner nodes in the TXO MMR can also be pruned if all leafs
> under
> them are fully spent; detecting this is easy the TXO MMR is a merkle-sum
> tree,
> with each inner node committing to the sum of the unspent txouts under it.
>
> When a archived txout is spent the transaction is required to provide a
> merkle
> path to the most recent TXO commitment. As shown above that path is
> sufficient
> information to unprune the necessary nodes in the TXO MMR and apply the
> spend
> immediately, reducing this case to the TXO journal size question
> (non-consensus
> critical overhead is a different question, which we'll address in the next
> section).
>
> Taking all this into account the only significant storage overhead of our
> TXO
> commitments scheme when compared to the status quo is the log2(n) merkle
> path
> overhead; as long as less than 1/log2(n) of the UTXO set is active,
> non-archived, UTXO's we've come out ahead, even in the unrealistic case
> where
> all storage available is equally fast. In the real world that isn't yet the
> case - even SSD's significantly slower than RAM.
>
>
> ### Non-Consensus Critical Storage Overheads
>
> Transactions spending archived txouts pose two challenges:
>
> 1) Obtaining up-to-date TXO commitment proofs
>
> 2) Updating those proofs as blocks are mined
>
> The first challenge can be handled by specialized archival nodes, not
> unlike
> how some nodes make transaction data available to wallets via bloom
> filters or
> the Electrum protocol. There's a whole variety of options available, and
> the
> the data can be easily sharded to scale horizontally; the data is
> self-validating allowing horizontal scaling without trust.
>
> While miners and relay nodes don't need to be concerned about the initial
> commitment proof, updating that proof is another matter. If a node
> aggressively
> prunes old versions of the TXO MMR as it calculates pending TXO
> commitments, it
> won't have the data available to update the TXO commitment proof to be
> against
> the next block, when that block is found; the child nodes of the TXO MMR
> tip
> are guaranteed to have changed, yet aggressive pruning would have
> discarded that
> data.
>
> Relay nodes could ignore this problem if they simply accept the fact that
> they'll only be able to fully relay the transaction once, when it is
> initially
> broadcast, and won't be able to provide mempool functionality after the
> initial
> relay. Modulo high-latency mixnets, this is probably acceptable; the
> author has
> previously argued that relay nodes don't need a mempool² at all.
>
> For a miner though not having the data necessary to update the proofs as
> blocks
> are found means potentially losing out on transactions fees. So how much
> extra
> data is necessary to make this a non-issue?
>
> Since the TXO MMR is insertion ordered, spending a non-archived txout can
> only
> invalidate the upper nodes in of the archived txout's TXO MMR proof (if
> this
> isn't clear, imagine a two-level scheme, with a per-block TXO MMRs,
> committed
> by a master MMR for all blocks). The maximum number of relevant inner nodes
> changed is log2(n) per block, so if there are n non-archival blocks
> between the
> most recent TXO commitment and the pending TXO MMR tip, we have to store
> log2(n)*n inner nodes - on the order of a few dozen MB even when n is a
> (seemingly ridiculously high) year worth of blocks.
>
> Archived txout spends on the other hand can invalidate TXO MMR proofs at
> any
> level - consider the case of two adjacent txouts being spent. To guarantee
> success requires storing full proofs. However, they're limited by the
> blocksize
> limit, and additionally are expected to be relatively uncommon. For
> example, if
> 1% of 1MB blocks was archival spends, our hypothetical year long TXO
> commitment
> delay is only a few hundred MB of data with low-IO-performance
> requirements.
>
>
> ## Security Model
>
> Of course, a TXO commitment delay of a year sounds ridiculous. Even the
> slowest
> imaginable computer isn't going to need more than a few blocks of TXO
> commitment delay to keep up ~100% of the time, and there's no reason why we
> can't have the UTXO archive delay be significantly longer than the TXO
> commitment delay.
>
> However, as with UTXO commitments, TXO commitments raise issues with
> Bitcoin's
> security model by allowing relatively miners to profitably mine
> transactions
> without bothering to validate prior history. At the extreme, if there was
> no
> commitment delay at all at the cost of a bit of some extra network
> bandwidth
> "full" nodes could operate and even mine blocks completely statelessly by
> expecting all transactions to include "proof" that their inputs are
> unspent; a
> TXO commitment proof for a commitment you haven't verified isn't a proof
> that a
> transaction output is unspent, it's a proof that some miners claimed the
> txout
> was unspent.
>
> At one extreme, we could simply implement TXO commitments in a "virtual"
> fashion, without miners actually including the TXO commitment digest in
> their
> blocks at all. Full nodes would be forced to compute the commitment from
> scratch, in the same way they are forced to compute the UTXO state, or
> total
> work. Of course a full node operator who doesn't want to verify old
> history can
> get a copy of the TXO state from a trusted source - no different from how
> you
> could get a copy of the UTXO set from a trusted source.
>
> A more pragmatic approach is to accept that people will do that anyway, and
> instead assume that sufficiently old blocks are valid. But how old is
> "sufficiently old"? First of all, if your full node implementation comes
> "from
> the factory" with a reasonably up-to-date minimum accepted total-work
> thresholdⁱ - in other words it won't accept a chain with less than that
> amount
> of total work - it may be reasonable to assume any Sybil attacker with
> sufficient hashing power to make a forked chain meeting that threshold
> with,
> say, six months worth of blocks has enough hashing power to threaten the
> main
> chain as well.
>
> That leaves public attempts to falsify TXO commitments, done out in the
> open by
> the majority of hashing power. In this circumstance the "assumed valid"
> threshold determines how long the attack would have to go on before full
> nodes
> start accepting the invalid chain, or at least, newly installed/recently
> reset
> full nodes. The minimum age that we can "assume valid" is tradeoff between
> political/social/technical concerns; we probably want at least a few weeks
> to
> guarantee the defenders a chance to organise themselves.
>
> With this in mind, a longer-than-technically-necessary TXO commitment
> delayʲ
> may help ensure that full node software actually validates some minimum
> number
> of blocks out-of-the-box, without taking shortcuts. However this can be
> achieved in a wide variety of ways, such as the author's prev-block-proof
> proposal³, fraud proofs, or even a PoW with an inner loop dependent on
> blockchain data. Like UTXO commitments, TXO commitments are also
> potentially
> very useful in reducing the need for SPV wallet software to trust third
> parties
> providing them with transaction data.
>
> i) Checkpoints that reject any chain without a specific block are a more
>    common, if uglier, way of achieving this protection.
>
> j) A good homework problem is to figure out how the TXO commitment could be
>    designed such that the delay could be reduced in a soft-fork.
>
>
> ## Further Work
>
> While we've shown that TXO commitments certainly could be implemented
> without
> increasing peak IO bandwidth/block validation latency significantly with
> the
> delayed commitment approach, we're far from being certain that they should
> be
> implemented this way (or at all).
>
> 1) Can a TXO commitment scheme be optimized sufficiently to be used
> directly
> without a commitment delay? Obviously it'd be preferable to avoid all the
> above
> complexity entirely.
>
> 2) Is it possible to use a metric other than age, e.g. priority? While this
> complicates the pruning logic, it could use the UTXO set space more
> efficiently, especially if your goal is to prioritise bitcoin
> value-transfer
> over other uses (though if "normal" wallets nearly never need to use TXO
> commitments proofs to spend outputs, the infrastructure to actually do
> this may
> rot).
>
> 3) Should UTXO archiving be based on a fixed size UTXO set, rather than an
> age/priority/etc. threshold?
>
> 4) By fixing the problem (or possibly just "fixing" the problem) are we
> encouraging/legitimising blockchain use-cases other than BTC value
> transfer?
> Should we?
>
> 5) Instead of TXO commitment proofs counting towards the blocksize limit,
> can
> we use a different miner fairness/decentralization metric/incentive? For
> instance it might be reasonable for the TXO commitment proof size to be
> discounted, or ignored entirely, if a proof-of-propagation scheme (e.g.
> thinblocks) is used to ensure all miners have received the proof in
> advance.
>
> 6) How does this interact with fraud proofs? Obviously furthering
> dependency on
> non-cryptographically-committed STXO/UTXO databases is incompatible with
> the
> modularized validation approach to implementing fraud proofs.
>
>
> # References
>
> 1) "Merkle Mountain Ranges",
>    Peter Todd, OpenTimestamps, Mar 18 2013,
>
> https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md
>
> 2) "Do we really need a mempool? (for relay nodes)",
>    Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,
>
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009479.html
>
> 3) "Segregated witnesses and validationless mining",
>    Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,
>
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Hi Ethan


>> It is important to include the cipher-type into the symmetric cipher key to avoid weak-cipher-attacks.
> 
> the cipher-type here refers to the ECDH negotiation parameters?

No. Not to the ECDH negotiation.
BIP151 specifies a flexible symmetric key cipher type negotiation,
although, BIP151 only specifies chacha20-poly1305@openssh.com.

Lets assume someone adds another symmetric cipher type after BIP151 has
been deployed which has less strong security properties then
chacha20-poly1305.

If we don't include the ciphersuite-type in the key derivation HMAC, an
attacker/MITM could in theory force both nodes to use the weaker
symmetric cipher type.

</jonas>


-------------------------------------
continued from previous post...

> On Jun 28, 2016, at 2:13 PM, Jonas Schnelli via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> Hi Eric
> 
> Sorry for not directly addressing your points.

No problem. Thanks for the detailed replies.

> I try to be more precise in this follow up email:
> 
>> I understand the use, when coupled with a yet-to-be-devised identity system, with Bloom filter features. Yet these features are client-server in nature. Libbitcoin (for example) supports client-server features on an independent port (and implements a variant of CurveCP for encryption and identity). My concern arises with application of identity to the P2P protocol (excluding Bloom filter features).
> 
> I think the bloom filter SPV usecase is not "pure client-server". SPV
> clients could request from/broadcast to multiple "trusted nodes".

I have referred to the Bloom filters messages. These are clearly asymmetric in nature. Despite being possible it is not a valid use case for a full node to make BF requests to another node.

One client to multiple servers is still client-server for the sake of this discussion. The nature of the P2P protocol is synchronization of content between all nodes/peers. If the protocol is asymmetric the semantics, and therefore use cases, are different.

FWIW posting a transaction to the network can be done using the P2P protocol, connecting for a short period of time. But this is also a client-server scenario and is a hack when done (full disclosure, bx provides both P2P and client-server commands for tx posting). Broadcasting is naturally the behavior of a full node.

> Trusted nodes could be nodes where the operators have shared identities/keys in advance over a different channel.

Yes, this is necessarily the case in order to prevent a MITM attack. This is the basis of my concern.

> Further private p2p extensions (lets say a p2p form of the estimatefee
> command) are something which needs to be discussed first and not
> something that is encouraged or outlined in BIP151.

Sure, but then let us not make assumptions about it in the context of this discussion. Libbitcoin provides fee estimation by monitoring broadcast penetration using a client-server protocol with an optional subscription mechanism.

>> It seems to me that the desire to secure against the weaknesses of BF is being casually generalized to the P2P network. That generalization may actually weaken the security of the P2P protocol. One might consider the proper resolution is to move the BF features to a client-server protocol.
> 
> I don't see reasons why BIP151 could weaken the security of the P2P network. Can you point out some specific concerns?

TOFU cannot prevent MITM attacks (the goal of the encryption). Authentication requires a secure (trusted) side channel by which to distribute public keys. This presents what I consider a significant problem. If widespread, control over this distribution network would constitute control over who can use Bitcoin.

The effort to prevent censorship could actually enable it. I don't think it would get that far. Someone would point this out in the process of vetting the authentication BIP, and the result would be the scrapping of BIP151.

>> The BIP does not make a case for other scenarios, or contemplate the significant problems associated with key distribution in any identity system. Given that the BIP relies on identity, these considerations should be fully vetted before heading down another blind alley.
> 
> BIP151 does not rely on identities. BIP151 does not use persisted keys
> (only ephemeral keys).

BIP 151 is incomplete without authentication.

> The authentication/identity system needs to be described in a another BIP.
> But correct, BIP151 without a form of authentication/identity management
> is vulnerable to all sorts of MITM attacks and that's why I think BIP151
> must be deployed together with an p2p authentication scheme.

Agree, but my problem is that I do not believe we can assume this is a solvable problem.

> Scope creeping and the risks of overspecifying is the main reason to
> focus on the "pure encryption part" in BIP151.

Understood, yet this is the basis of my blind alley comment.

e

> Thanks
> ---
> </jonas>


-------------------------------------
On Fri, Jun 17, 2016 at 07:43:47PM -0700, Bram Cohen wrote:
> On Thu, Jun 16, 2016 at 9:34 PM, Peter Todd <pete@petertodd.org> wrote:
> 
> > So above you said that in merbinner trees each node "hash[es] in a record
> > of
> > its depth" That's actually incorrect: each node commits to the prefix that
> > all
> > keys below that level start with, not just the depth.
> 
> 
> I considered a similar trick at the implementation rather than the
> definition level: A node doesn't have to store the prefix which is implicit
> in its position. That would create a fair number of headaches though,
> because I'm using fixed size stuff in important ways, and it could at most
> save about 10% of memory, so it goes into the 'maybe later' bucket.

Wait, are you saying you think committing to the prefix is a "trick"? It's just
a very simple - and possibly not-optimal - way of committing to what data
should be accessible under a given node. An alternative would have been ensure
that in terms of _cryptographic_ tree position.

By "position", are you talking about position within RAM? That may or may not
be a viable optimization, but it's quite separate from the question of the
cryptographic structure of the data.

> > This means that in merbinner trees, cases where multiple keys share parts
> > of
> > the same prefix are handled efficiently, without introducing extra levels
> > unnecessarily; there's no need for the ONLY0/1 nodes as the children of an
> > inner node will always be on different sides.
> >
> > When keys are randomly distributed, this isn't a big deal; OTOH against
> > attackers who are choosing keys, e.g. by grinding hashes, merbinner trees
> > always have maximum depths in proportion to log2(n) of the actual number of
> > items in the tree. Grinding is particularly annoying to deal with due to
> > the
> > birthday attack: creating a ground prefix 64 bits long only takes 32 bits
> > worth
> > of work.
> >
> 
> Yes an attacker can force the tree to be deeper in places, but it's
> mitigated in several ways: (1) The way I'm using memory it won't cause a
> whole new block to be allocated, it will just force log(attack strength) -
> log(n) nodes to be used (2) logarithmic growth being what it is that isn't
> such a big amount (3) With the special casing of TERMBOTH an attacker needs
> three things with the same prefix to pull off an attack rather than two,
> which is quite a bit harder to pull off.



> That said, it wouldn't be all that hard to change how the hashing function
> works to do a single hash for a whole series of ONLY in a row instead of a
> new one at every level, which would make the attacker only able to force
> extra memory usage instead of extra CPU, but this is a slightly annoying
> thing to write to stop a fairly lame attack, so I'm at least not doing it
> for my initial implementation. I could likely be convinced that it's worth
> doing before an actual release though. There's another implementation trick
> to do the same thing for memory usage, which is much more in the 'do later'
> category because it doesn't involve changing the format and hence it can be
> put off.
> 
> 
> > In particular, case #2 handles your leaf node optimizations generically,
> > without special cases and additional complexity. It'd also be a better way
> > to
> > do the ONLY0/1 cases, as if the "nothing on this side" symbol is a single
> > byte,
> > each additional colliding level would simply extend the commitment without
> > hashing. In short, you'd have nearly the same level of optimization even
> > if at
> > the cryptography level your tree consists of only leaves, inner nodes, and
> > nil.
> >
> 
> I'm taking pains to make all the hashing be of fixed-size things, so that a
> non-padding variant of a secure hashing algorithm can be used. The chains
> of ONLY thing above would force a special exception to that, which can be
> done but is annoying. Making things smaller than a single block (64 bytes)
> won't speed up hashing time, and making things a single byte longer than
> that doubles it.

Have you seen how BLAKE2 omits padding when the data to be hashed happens to be
exactly one block in size? It's significantly faster than SHA256, and that's a
standard part of the algorithm already.

> > > > Technically even a patricia trie utxo commitment can have sub-1 cache
> > > > > misses per update if some of the updates in a single block are close
> > to
> > > > > each other in memory. I think I can get practical Bitcoin updates
> > down
> > > > to a
> > > > > little bit less than one l2 cache miss per update, but not a lot
> > less.
> > > >
> > > > I'm very confused as to why you think that's possible. When you say
> > > > "practical
> > > > Bitcoin updates", what exactly is the data structure you're proposing
> > to
> > > > update? How is it indexed?
> >
> 
> I'll re-answer this because I did a terrible job before. The entire data
> structure consists of nodes which contain a metadata byte (TERM0, ONLY1,
> etc.) followed by fixes size secure hashes, and (in some cases) pointers to
> where the children are. The secure hashes in parent nodes are found by
> hashing their children verbatim (or the stored string in the case of a
> TERM). This is very conventional. All of the cleverness is in where in
> memory these nodes are stored so that tracing down the tree causes very few
> cache misses.
> 
> (The alternate approach is to have each node store its own hash rather than
> that be stored by the parent. That approach means that when you're
> recalculating you have to look up siblings which doubles the number of
> cache misses. Not such a hot idea.)

Have you benchmarked the cost of a hash operation vs. the cost of a cache miss?
What are the actual numbers?

> At the root there's a branch block. It consists of all nodes up to some
> fixed depth - let's say 12 - with that depth set so that it roughly fits
> within a single memory page. Branch blocks are arranged with the nodes in
> fixed position defined by the prefix they correspond to, and the terminals
> have outpointers to other blocks. Because they're all clustered together, a
> lookup or update will only require a single

A single....?

> Below the root block are other branch blocks. Each of them has a fixed 12
> bit prefix it is responsible for. When doing a lookup a second cache miss
> will be hit for levels 13-24, because those are all clustered in the same
> branch block.

So, is this also how the data structure looks cryptographically, or is the way
it's hashed separate from the above description?

> Below the second level of root block (at Bitcoin utxo set scale - this
> varies based on how much is stored) there are leaf blocks. A leaf block
> consists of nodes with outpointers to its own children which must be within
> the same leaf block. All entry points into a leaf block are from the same
> branch block, and the leaf block has no out pointers to other blocks. When
> a leaf block overflows the entry point into it which overflowed is moved
> into the active leaf for that branch, and if that's full a new one is
> allocated. There's some subtlety to exactly how this is done, but I've
> gotten everything down to simple expedient tricks with straightforward
> implementations. The thing which matters for now is that there's only a
> single cache miss for each leaf node, because they also fit in a page.

Page as in 4096 bytes? But L1/L2/L3 cache is arranged in terms of 64 byte cache
lines - where do pages come in here?

At Bitcoin UTXO set scale, how large do you think these data structures are?

> So at Bitcoin scale there will probably only be 3 cache misses for a
> lookup, and that's a worst case scenario. The first one is probably always
> warm, bringing it down to 2, and if you do a bunch in sorted order they'll
> probably hit the same second level branches repeatedly bringing it down to
> 1, and might even average less than that if there are enough that the leaf
> block has multiple things being accessed.

"Sorted order" - what exact type of sorting do you mean here?

> > Anyway hashing is pretty slow. The very fast BLAKE2 is about 3 cycles/byte
> > (SHA256 is about 15 cycles/byte) so hashing that same data would take
> > around
> > 200 cycles, and probably quite a bit more in practice due to overheads
> > from our
> > short message lengths; fetching a cache line from DRAM only takes about
> > 1,000
> > cycles. I'd guess that once other overheads are taken into account, even
> > if you
> > could eliminate L2/L3 cache-misses it wouldn't be much of an improvement.
> >
> 
> Those numbers actually back up my claims about performance. If you're doing
> a single update and recalculating the root afterwards, then the amount of
> rehashing to be done is about 30 levels deep times 64 bytes per thing
> hashed times 15 cycles per byte then it's about 28,800 cycles of hashing.
> If you have a naive radix tree implementation which hits a cache miss at
> every level then that's 30,000 cycles, which is about half the performance
> time, certainly worth optimizing. If instead of sha256 you use blake2
> (Which sounds like a very good idea!) then hashing for an update will be
> about 5760 cycles and performance will be completely dominated by cache
> misses. If a more cache coherent implementation is used, then the cost of
> cache misses will be 3000 cycles, which will be a non-factor with sha256
> and a significant but not dominating one with blake2.

But that's assuming the dataset in question fits in cache; I don't see how it
does. Since it doesn't, I'm argung the total % improvement by _any_ cache
optimization on the subset that does fit in cache will be relatively small.

Again, how large a dataset do you think you're working with here?

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Interesting stuff! I have some comments, mostly about the header.

The header of forcenet is mostly described in Luke’s BIP, but I have made
> some amendments as I implemented it. The format is (size in parentheses;
> little endian):
>
> Height (4), BIP9 signalling field (4), hardfork signalling field (3),
> merge-mining hard fork signalling field (1), prev hash (32), timestamp (4),
> nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), Hash TMR (32),
> Hash WMR (32), total tx size (8) , total tx weight (8), total sigops (8),
> number of tx (4), merkle branches leading to header C (compactSize + 32 bit
> hashes)
>

First, I'd really rather not have variable length fields in the header.
It's so much nicer to just have a fixed size.

Is having both TMR and WMR really needed?  As segwit would be required with
this header type, and the WMR covers a superset of the data that the TMR
does, couldn't you get rid of the TMR?  The only disadvantage I can see is
that light clients may want a merkle proof of a transaction without having
to download the witnesses for that transaction.  This seems pretty minor,
especially as once they're convinced of block inclusion they can discard
the witness data, and also the tradeoff is that light clients will have to
download and store and extra 32 bytes per block, likely offsetting any
savings from omitting witness data.

The other question is that there's a bit that's redundant: height is also
committed to in the coinbase tx via bip 34 (speaking of which, if there's a
hard-fork, how about reverting bip 34 and committing to the height with
coinbase tx nlocktime instead?)

Total size / weight / number of txs also feels pretty redundant.  Not a lot
of space but it's hard to come up with a use for them.  Number of tx could
be useful if you want to send all the leaves of a merkle tree, but you
could also do that by committing to the depth of the merkle tree in the
header, which is 1 byte.

Also how about making timestamp 8 bytes?  2106 is coming up soon :)

Maybe this is too nit-picky; maybe it's better to put lots of stuff in for
testing the forcenet and then take out all the stuff that wasn't used or
had issues as it progresses.

Thanks and looking forward to trying out forcenet!

-Tadge

-------------------------------------
On Thu, Sep 22, 2016 at 02:09:38PM +0200, Tom via bitcoin-dev wrote:
> On Thursday 22 Sep 2016 13:10:49 Christian Decker via bitcoin-dev wrote:
> > 
> > I think BIPs should be self-contained, or rely on previous BIPs,
> > whenever possible. Referencing an external formatting document should
> > be avoided 
> 
> If luke-jr thinks I should submit CMF as a BIP, I can certainly do that.
> Luke, what do you think?
> 
> I don't have a preference either way.
> 
> > 
> > So the presence is signaled by encountering the tag, which contains
> > both token type and name-reference. The encoder and decoder operations
> > could be described better.
> 
> I'm sorry, I'm not following you here. Is there a question?

Nope, just clarifying how presence or absence is indicated :-)

> > 
> > Minor nit: that table is not well-formed.
> 
> I am not very well versed in mediawiki tables, and I found github has some 
> incompatibilities too.
> The markdown one looks better;
> https://github.com/bitcoinclassic/documentation/blob/master/spec/transactionv4.md

It's just some rows have 3 columns, others have 2. It's a minor nit
really.

> > As was pointed out in the
> > normalized transaction ID BIP, your proposal only addresses
> > third-party malleability, since signers can simply change the
> > transaction and re-sign it.
> 
> I have to disagree. That is not malleability. Creating a new document and re-
> signing it is not changing anything. Its re-creating. Something that the owner 
> of the coin has every right to do.

Same thing I was arguing back then, however Luke pointed out that
malleability just refers to the possibility of modifying a transaction
after the fact. Always referring to "third-party malleability" avoids
this ambiguity.

> > This is evident from the fact that inputs
> > and outputs do not have a canonical order and it would appear that
> > tokens can be re-ordered in segments. 
> 
> Sorry, what is evident? You seem to imply that it is uncommon that you can 
> create two transactions of similar intent but using different bytes.
> You would be wrong with this implication as this is very common. You can just 
> alter the order of the inputs, for instance.
> 
> I am unable to see what the point is you are trying to make. Is there a 
> question or a suggestion for improvement here?
> 
> > Dependencies of tokens inside a
> > segment are also rather alarming (TxInPrevHash <-> TxInPrevIndex,
> > TxOutScript <-> TxOutValue).
> 
> Maybe you missed this line; 
>   TxInPrevHash and TxInPrevIndex
>    Index can be skipped, but in any input the PrevHash always has
>    to come first

Nope, that is exactly the kind of dependency I was talking
about. Instead of nesting a construct like the current transactions
do, you rely on the order of tokens to imply that they belong
together.

> If you still see something alarming, let me know.
> You can look at the code in Bitcoin Classic and notice that it really isn't 
> anything complicated or worrying.
> 
> 
> > Finally, allowing miners to reject transactions with unknown fields
> > makes the OP_NOPs unusable 
> 
> Hmm, it looks like you are mixing terminology and abstraction-levels.  OP_NOP 
> is a field from script and there is no discussion about any rejection based on 
> script in this BIP at all.
> 
> Rejection of transactions is done on there being unrecognised tokens in the 
> transaction formatting itself.

Ah, thanks for clearing that up. However, the problem persists, if we
add new fields that a non-upgraded node doesn't know about and it
rejects transactions containing it, we'll have a hard-fork. It should
probably not reject transactions with unknown fields if the
transaction is included in a block.

> Thank you for your email to my BIP, I hope you got the answers you were 
> looking for :)

Cheers,
Christian


-------------------------------------
And to fend off the messag that I bet somebody is composing right now:

Yes, I know about a "security first" mindset.  But as I said earlier in the
thread, there is a tradeoff here between crypto strength and code
complexity, and "the strength of the crypto is all that matters" is NOT
security first.

-- 
--
Gavin Andresen

-------------------------------------
On Fri, Jan 8, 2016 at 4:38 AM, Gavin Andresen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Fri, Jan 8, 2016 at 7:02 AM, Rusty Russell <rusty@rustcorp.com.au> wrote:
>>
>> Matt Corallo <lf-lists@mattcorallo.com> writes:
>> > Indeed, anything which uses P2SH is obviously vulnerable if there is
>> > an attack on RIPEMD160 which reduces it's security only marginally.
>>
>> I don't think this is true?  Even if you can generate a collision in
>> RIPEMD160, that doesn't help you since you need to create a specific
>> SHA256 hash for the RIPEMD160 preimage.
>>
>> Even a preimage attack only helps if it leads to more than one preimage
>> fairly cheaply; that would make grinding out the SHA256 preimage easier.
>> AFAICT even MD4 isn't this broken.
>
>
> It feels like we've gone over that before, but I can never remember where or
> when. I believe consensus was that if we were using the broken MD5 in all
> the places we use RIPEMD160 we'd still be secure today because of Satoshi's
> use of nested hash functions everywhere.
>
>>
>> But just with Moore's law (doubling every 18 months), we'll worry about
>> economically viable attacks in 20 years.[1]
>>
>>
>> That's far enough away that I would choose simplicity, and have all SW
>> scriptPubKeys simply be "<0> RIPEMD(SHA256(WP))" for now, but it's
>> not a no-brainer.
>
>
> Lets see if I've followed the specifics of the collision attack correctly,
> Ethan (or somebody) please let me know if I'm missing something:
>
> So attacker is in the middle of establishing a payment channel with
> somebody. Victim gives their public key, attacker creates the innocent
> fund-locking script  '2 V A 2 CHECKMULTISIG' (V is victim's public key, A is
> attacker's) but doesn't give it to the victim yet.
>
> Instead they then generate about 2^81scripts that are some form of
> pay-to-attacker ....
> ... wait, no that doesn't work, because SHA256 is used as the inner hash
> function.  They'd have to generate 2^129 to find a cycle in SHA256.

For 2^80 they simply generate 2^80 scripts that look innocent, and
2^80 that are not. With high probability there is a collision. I agree
that most cryptanalysis won't work because of the nesting, but 2^80 is
not good.
>
> Instead, they .. what? I don't see a viable attack unless RIPEMD160 and
> SHA256 (or the combination) suffers a cryptographic break.
>
>
> --
> --
> Gavin Andresen
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
"Man is born free, but everywhere he is in chains".
--Rousseau.


-------------------------------------

> On 17 Nov 2016, at 20:22, Eric Voskuil via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> 
> Given that hash collisions are unquestionably possible, 

Everything you said after this point is irrelevant.

Having hash collision is **by definition** a consensus failure, or a hardfork. You could replace the already-on-chain tx with the collision and create 2 different versions of UTXOs (if the colliding tx is valid), or make some nodes to accept a fork with less PoW (if the colliding tx is invalid, or making the block invalid, such as being to big). To put it simply, the Bitcoin protocol is broken. So with no doubt, Bitcoin Core and any implementation of the Bitcoin protocol should assume SHA256 collision is unquestionably **impossible**. If some refuse to make such assumption, they should have introduced an alternative hash algorithm and somehow run it in parallel with SHA256 to prevent the consensus failure.

jl2012


-------------------------------------
Responding to "28 days is not long enough" :

I keep seeing this claim made with no evidence to back it up.  As I said, I
surveyed several of the biggest infrastructure providers and the btcd lead
developer and they all agree "28 days is plenty of time."

For individuals... why would it take somebody longer than 28 days to either
download and restart their bitcoind, or to patch and then re-run (the patch
can be a one-line change MAX_BLOCK_SIZE from 1000000 to 2000000)?

For the Bitcoin Core project:  I'm well aware of how long it takes to roll
out new binaries, and 28 days is plenty of time.

I suspect there ARE a significant percentage of un-maintained full nodes--
probably 30 to 40%. Losing those nodes will not be a problem, for three
reasons:
1) The network could shrink by 60% and it would still have plenty of open
connection slots
2) People are committing to spinning up thousands of supports-2mb-nodes
during the grace period.
3) We could wait a year and pick up maybe 10 or 20% more.

I strongly disagree with the statement that there is no cost to a longer
grace period. There is broad agreement that a capacity increase is needed
NOW.

To bring it back to bitcoin-dev territory:  are there any TECHNICAL
arguments why an upgrade would take a business or individual longer than 28
days?


Responding to Luke's message:

On Sat, Feb 6, 2016 at 1:12 AM, Luke Dashjr via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > On Friday, February 05, 2016 8:51:08 PM Gavin Andresen via bitcoin-dev
> wrote:
> >> Blog post on a couple of the constants chosen:
> >>   http://gavinandresen.ninja/seventyfive-twentyeight
> >
> > Can you put this in the BIP's Rationale section (which appears to be
> mis-named
> > "Discussion" in the current draft)?
>

I'll rename the section and expand it a little. I think standards documents
like BIPs should be concise, though (written for implementors), so I'm not
going to recreate the entire blog post there.


> >
> >> Signature operations in un-executed branches of a Script are not counted
> >> OP_CHECKMULTISIG evaluations are counted accurately; if the signature
> for a
> >> 1-of-20 OP_CHECKMULTISIG is satisified by the public key nearest the top
> >> of the execution stack, it is counted as one signature operation. If it
> is
> >> satisfied by the public key nearest the bottom of the execution stack,
> it
> >> is counted as twenty signature operations. Signature operations
> involving
> >> invalidly encoded signatures or public keys are not counted towards the
> >> limit
> >
> > These seem like they will break static analysis entirely. That was a
> noted
> > reason for creating BIP 16 to replace BIP 12. Is it no longer a concern?
> Would
> > it make sense to require scripts to commit to the total accurate-sigop
> count
> > to fix this?
>

After implementing static counting and accurate counting... I was wrong.
Accurate/dynamic counting/limiting is quick and simple and can be
completely safe (the counting code can be told the limit and can
"early-out" validation).

I think making scripts commit to a total accurate sigop count is a bad
idea-- it would make multisignature signing more complicated for zero
benefit.  E.g. if you're circulating a partially signed transaction to that
must be signed by 2 of 5 people, you can end up with a transaction that
requires 2, 3, 4, or 5 signature operations to validate (depending on which
public keys are used to do the signing).  The first signer might have no
idea who else would sign and wouldn't know the accurate sigop count.


> >
> >> The amount of data hashed to compute signature hashes is limited to
> >> 1,300,000,000 bytes per block.
> >
> > The rationale for this wasn't in your blog post. I assume it's based on
> the
> > current theoretical max at 1 MB blocks? Even a high-end PC would
> probably take
> > 40-80 seconds just for the hashing, however - maybe a lower limit would
> be
> > best?
>

It is slightly more hashing than was required to validate block number
364,422.

There are a couple of advantages to a very high limit:

1) When the fork is over, special-case code for dealing with old blocks can
be eliminated, because all old blocks satisfy the new limit.

2) More importantly, if the limit is small enough it might get hit by
standard transactions, then block creation code (CreateNewBlock() /
getblocktemplate / or some external transaction-assembling software) will
have to solve an even more complicated bin-packing problem to optimize for
fees paid.

In practice, the 20,000 sigop limit will always be reached before
MAX_BLOCK_SIGHASH.



> >
> >> Miners express their support for this BIP by ...
> >
> > But miners don't get to decide hardforks. How does the economy express
> their
> > support for it? What happens if miners trigger it without consent from
> the
> > economy?
>

"The economy" does support this.



> >
> > If you are intent on using the version bits to trigger the hardfork, I
> suggest
> > rephrasing this such that miners should only enable the bit when they
> have
> > independently confirmed economic support (this means implementations
> need a
> > config option that defaults to off).
>

Happy to add words about economic majority.

Classic will not implement a command-line option (the act of running
Classic is "I opt in"), but happy to add one for a pull request to Core,
assuming Core would not see such a pull request as having any hostile
intent.


>
> >> SPV (simple payment validation) wallets are compatible with this change.
> >
> > Would prefer if this is corrected to "Light clients" or something.
> Actual SPV
> > wallets do not exist at this time, and would not be compatible with a
> > hardfork.
>

Is there an explanation of SPV versus "Light Client" written somewhere more
permanent than a reddit comment or forum post that I can point to?


> >
> >> In the short term, an increase is needed to continue the current
> economic
> >> policies with regards to fees and block space, matching market
> expectations
> >> and preventing market disruption.
> >
> > IMO this sentence is the most controversial part of your draft, and it
> > wouldn't suffer a loss to remove it (or at least make it subjective).
>

Happy to remove.


> > I would also prefer to see any hardfork:
> >
> > 1. Address at least the simple tasks on the hardfork wishlist (eg,
> enable some
> >    disabled opcodes; fix P2SH for N-of->15 multisig; etc).
>

Those would be separate BIPs. (according to BIP 1, smaller is better)

After this 2MB bump, I agree we need to agree on a process for the next
hard fork to avoid all of the unnecessary drama.

> 2. Be deployed as a soft-hardfork so as not to leave old nodes entirely
> >    insecure.
>

I haven't been paying attention to all of the
"soft-hardfork/hard-softfork/etc" terminology so have no idea what you
mean. Is THAT written up somewhere?

-- 
--
Gavin Andresen

-------------------------------------
On 12/14/2016 07:38 PM, Juan Garavaglia via bitcoin-dev wrote:

>
> For reasons I am unable to determine a significant number of node
> operators do not upgrade their clients.

I almost did not update to 0.13.0 because the test suite was failing due 
to python errors. How to fix them was posted on bitcointalk.

0.13.1 came with new python errors in the test suite. So I just said 
fuck it.

When the test suite actually works in my fairly standard environment 
(CentOS) in the distributed release, I will upgrade.

Until then, I'm not jumping through hoops to make the test suite work 
and I'm not running clients that haven't passed the test suite so that's 
why I almost didn't update to 0.13.0 and haven't updated since.


-------------------------------------
Gavin,

I saw this in your blog post:

"Miners producing up-version blocks is a coordination mechanism. Other coordination mechanisms are possible– there could be a centrally determined “flag day” or “flag block” when everybody (or almost everybody) agrees that a change will happen."

Can you describe this a bit more? How would either a "flag day" or "flag block" work as an alternative and why did you decide against them?

More thoughts and questions inline, thanks!

On Feb 6, 2016, at 12:45 PM, Gavin Andresen via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

>> On Sat, Feb 6, 2016 at 12:01 PM, Adam Back <adam@cypherspace.org> wrote:
>> 
>> It would probably be a good idea to have a security considerations
>> section
> 
> Containing what?  I'm not aware of any security considerations that are any different from any other consensus rules change.

Can you explain and justify why that is the case? It would be nice to see that rationale laid out more fully as to why it's no different.

> 
> (I can write a blog post summarizing our slack discussion of SPV security immediately after the first greater-than-1mb-block if you like).

I'm not familiar with the context of your slack discussion, but why would you wait to summarize that only after the first-greater-than-1mb-block?

> 
>  
>> , also, is there a list of which exchange, library, wallet,
>> pool, stats server, hardware etc you have tested this change against?
> 
> That testing is happening by the exchange, library, wallet, etc providers themselves. There is a list on the Classic home page:
> 
> https://bitcoinclassic.com/

Is there a way to provide more transparency and visibility into that process and level of readiness? Is there an expectation of certain levels of readiness here before certain other things happen? I was thinking it would be really useful to have a visual timeline of events associated with all of this. Maybe you could add that to one of your web pages?

>  
>> 
>> Do you have a rollback plan in the event the hard-fork triggers via
>> false voting as seemed to be prevalent during XT?  (Or rollback just
>> as contingency if something unforseen goes wrong).
> 
> The only voting in this BIP is done by the miners, and that cannot be faked.
> 
> Are you talking about people spinning up pseudo-full-nodes that fake the user-agent?
> 
> As I said, there are people who have said they will spin up thousands of full nodes to help prevent possible Sybil attacks which would become marginally easier to accomplish immediately after the first >1mb block was produced and full nodes that hadn't upgraded were left behind.
> 
> Would Blockstream be willing to help out by running a dozen or two extra full nodes?
> 
> I can't imagine any even-remotely-likely sequence of events that would require a rollback, can you be more specific about what you are imagining?  Miners suddenly getting cold feet?

Well that, but also past performance isn't an indication of future performance, necessarily. They might have gone out of business, to give one example. There is surely assumed self-interest, but I have also seen rumors floating around of this being used as an arbitrage opportunity. Would suck to imagine that ever happening, but since this seems like it's being managed on more handshake type of deals (or conversations), are there any legal documents backing those commitments up? Or is that definitely overkill?

Maybe it's worth documenting the full range of possible series of events and then their presumed level of unlikelihood? "What can go wrong will go wrong", "Black Swan" events, type of considerations. :) Often when people discuss unlikely things like crypto being broken, it's like, "Assuming processing power of x, increasing at a rate of x, and a known age of the universe of x, it would take a billion times the known length of the universe for that to happen".

Certainly not everything fits so easily into that framing, but it would be really helpful to see the "what could possibly go wrong" things fully enumerated.

Thanks!!

Dave

>  
>> How do you plan to monitor and manage security through the hard-fork?
> 
> I don't plan to monitor or manage anything; the Bitcoin network is self-monitoring and self-managing. Services like statoshi.infowill do the monitoring, and miners and people and businesses will manage the network, as they do every day.
> 
> -- 
> --
> Gavin Andresen
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
A number of BIPs seem ready for updating to Final Status. If there are no 
objections, I will update these in 2 weeks:

BIP 39: Mnemonic code for generating deterministic keys
- Used by many wallets and hundreds of thousands of users.

BIP 44: Multi-Account Hierarchy for Deterministic Wallets
- Appears to be implemented by multiple wallets.

BIP 67: Deterministic Pay-to-script-hash multi-signature addresses through
        public key sorting
- Implementations in multiple wallet software exist.

BIP 125: Opt-in Full Replace-by-Fee Signaling
- Implemented in Bitcoin Core and derivatives; appears to be in regular use on 
the network.

BIP 130: sendheaders message
- Implemented in Bitcoin Core and derivatives.

Also, BIP 43 (Purpose Field for Deterministic Wallets) is an informational BIP 
which appears to be guiding to some extent the creation of new BIPs; therefore 
I propose its Status be upgraded to Active. I will make this update in 2 weeks 
also, if no objections.

Additionally, BIP 111 (NODE_BLOOM service bit) has been implemented in Bitcoin 
Core and derivatives; it is unclear if used by clients yet. Can developers of 
such clients please comment and let me know: 1) if their software supports 
this BIP already; 2) if not, do they intend to support it in the future?
If and only if there are any clients using this service bit already, I will 
update BIP 111 to Final Status in 2 weeks also.

Thanks,

Luke


-------------------------------------
The various chunks in the double SHA256 are

Chunk 1: 64 bytes
version
previous_block_digest
merkle_root[31:4]

Chunk 2: 64 bytes
merkle_root[3:0]
nonce
timestamp
target

Chunk 3: 64 bytes
digest from first sha pass

Their improvement requires that all data in Chunk 2 is identical except for
the nonce.  With 4 bytes, the birthday paradox means collisions can be
found reasonable easily.

If hard forks are allowed, then moving more of the merkle root into the 2nd
chunk would make things harder.  The timestamp and target could be moved
into chunk 1.  This increases the merkle root to 12 bytes in the 2nd
chunk.  Finding collisions would be made much more difficult.

If ASIC limitations mean that the nonce must stay where it is, this would
mean that the merkle root would be split into two pieces.

On Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> As part of the hard-fork proposed in the HK agreement(1) we'd like to make
> the
> patented AsicBoost optimisation useless, and hopefully make further similar
> optimizations useless as well.
>
> What's the best way to do this? Ideally this would be SPV compatible, but
> if it
> requires changes from SPV clients that's ok too. Also the fix this should
> be
> compatible with existing mining hardware.
>
>
> 1)
> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff
>
> 2)
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Tue, Jun 28, 2016 at 10:29:54PM +0200, Eric Voskuil wrote:
> 
> 
> > On Jun 28, 2016, at 10:14 PM, Peter Todd <pete@petertodd.org> wrote:
> > 
> >> On Tue, Jun 28, 2016 at 08:35:26PM +0200, Eric Voskuil wrote:
> >> Hi Peter,
> >> 
> >> What in this BIP makes a MITM attack easier (or easy) to detect, or increases the probability of one being detected?
> > 
> > BIP151 gives users the tools to detect a MITM attack.
> > 
> > It's kinda like PGP in that way: lots of PGP users don't properly check keys,
> 
> PGP requires a secure side channel for transmission of public keys. How does one "check" a key of an anonymous peer? I know you well enough to know you wouldn't trust a PGP key received over an insecure channel.
> 
> All you can prove is that you are talking to a peer and that communications in the session remain with that peer. The peer can be the attacker. As Jonas has acknowledged, authentication is required to actually guard against MITM attacks.

Easy: anonymous peers aren't always actually anonymous.

A MITM attacker can't easily distinguish communications between two nodes that
randomly picked their peers, and nodes that are connected because their
operators manually used -addnode to peer; in the latter case the operators can
check whether or not they're being attacked with an out-of-band key check.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Wed, Mar 02, 2016 at 05:53:46PM +0000, Gregory Maxwell wrote:
> What you are proposing makes sense only if it was believed that a very
> large difficulty drop would be very likely.
>
> This appears to be almost certainly untrue-- consider-- look how long
> ago since hashrate was 50% of what it is now, or 25% of what it is
> now-- this is strong evidence that supermajority of the hashrate is
> equipment with state of the art power efficiency.

To avoid duplication of looking up this statistic among readers, here
are the various recent difficulties:

    $ for i in $( seq 0 2016 60000 ) ; do echo -n $i blocks ago:' ' ; bitcoin-cli getblock $( bitcoin-cli getblockhash $(( 400857 - i )) ) | jshon -e difficulty ; done | column -t
    0      blocks  ago:  163491654908.95929
    2016   blocks  ago:  144116447847.34869
    4032   blocks  ago:  120033340651.237
    6048   blocks  ago:  113354299801.4711
    8064   blocks  ago:  103880340815.4559
    10080  blocks  ago:  93448670796.323807
    12096  blocks  ago:  79102380900.225983
    14112  blocks  ago:  72722780642.54718
    16128  blocks  ago:  65848255179.702606
    18144  blocks  ago:  62253982449.760818
    20160  blocks  ago:  60883825480.098282
    22176  blocks  ago:  60813224039.440353
    24192  blocks  ago:  59335351233.86657
    26208  blocks  ago:  56957648455.01001
    28224  blocks  ago:  54256630327.889961
    30240  blocks  ago:  52699842409.347008
    32256  blocks  ago:  52278304845.591682
    34272  blocks  ago:  51076366303.481934
    36288  blocks  ago:  49402014931.227463
    38304  blocks  ago:  49692386354.893837
    40320  blocks  ago:  47589591153.625008
    42336  blocks  ago:  48807487244.681381
    44352  blocks  ago:  47643398017.803436
    46368  blocks  ago:  47610564513.47126
    48384  blocks  ago:  49446390688.24144
    50400  blocks  ago:  46717549644.706421
    52416  blocks  ago:  47427554950.6483
    54432  blocks  ago:  46684376316.860291
    56448  blocks  ago:  44455415962.343803
    58464  blocks  ago:  41272873894.697021

<50% of current hash rate was last seen roughly six retarget periods (12
weeks) ago and <25% of current hash rate was last seen roughly 29 periods
(58 weeks) ago.

I think that's reasonably strong evidence for your thesis given that
the increases in hash rate from the introduction of new efficient
equipment are likely partly offset by the removal from the hash rate of
lower efficiency equipment, so the one-year tail of ~25% probably means
that less than 25% of operating equipment is one year old or older.

However, it is my understanding that most mining equipment can be run at
different hash rates. Is there any evidence that high-efficiency miners
today are using high clock speeds to produce more hashes per ASIC than
they will after halving?  Is there any way to guess at how many fewer
hashes they might produce?

> If a pre-programmed ramp and drop is set then it has the risk of
> massively under-setting difficulty; which is also strongly undesirable
> (e.g. advanced inflation and exacerbating existing unintentional
> selfish mining)

Maybe I'm not thinking this through thoroughly, but I don't think it's
possible to significantly advance inflation unless the effective hash
rate increases by more than 300% at the halving.  With the proposal
being replied to, if all mining equipment operation before the
halving continued operating after it, the effective increase would be
200%. That doubling in effective hash rate would've been offset in
advance through a reduction in the effective hash rate in the weeks
before the halving.

Exacerbated unintentional selfish mining is a much more significant
concern IMO, even if it's only for a short retarget period or two. This
is especially the case given the current high levels of centralization
and validationless mining on the network today, which we would not want
to reward by making those miners the only ones effectively capable of
creating blocks until difficulty adjusted. I had not thought of this
aspect; thank you for bringing it up.

> and that is before suggesting that miners voluntarily take a loss of
> inflation now.

Yes, I very much don't like that aspect, which is why I made sure to
mention it.

> So while I think this concern is generally implausible; I think it's
> prudent to have a difficulty step patch (e.g. a one time single point
> where a particular block is required to lower bits a set amount) ready
> to go in the unlikely case the network is stalled.

I think having that code ready in general is a good idea, and a one-time
change in nBits is sounds like a good and simple way to go about it.

Thank you for your insightful reply,

-Dave


-------------------------------------
Rune Kjær Svendsen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> writes:
> Dear list
>
> I've spent the past couple of months developing a simple protocol for
> working with payment channels. I've written up a specification of how
> it operates, in an attempt to standardize the operations of opening,
> paying and closing.

Hi!

        CHECKLOCKTIMEVERIFY [...] allows payment channel
        setup to be risk free [...] something that was
        not the case before, when the refund Bitcoin transaction
        depended on another, unconfirmed Bitcoin transaction. Building
        on unconfirmed transactions is currently not safe in Bitcoin

With Segregated Witness, this is now safe.  With that expected soon, I'd
encourage you to take advantage of it.

Cheers,
Rusty.


-------------------------------------
In Febuary, an email intitled "Bitcoin Vaults" was addressed to this mailing list linking to a paper on “covenants” (see mail below) describing a way to apply recursive restrictions temporarily or permanently on bitcoins (for digital asset use-cases) and Bitcoin Vaults were offered as an application (thanks to the authors for sharing their work with the community, I personally found this paper insightful and inspiring). Unfortunately, this proposal isn’t fungibility friendly and could lead Bitcoin to undesirable outcomes.

What follows is an attempt to design Vaults that preserve Bitcoin’s fungibility and keep their defensive attributes private from blockchain observers and from potential insider participants: the Vault’s defence is incrementally revealed when executed. If I am a war chief defending a castle, I’m certainly not going to show my defence strategy to the world and if it leaked to the enemy, it would greatly weaken my chances to succeed: greater privacy leads to greater security.
 
Vaults enable important use-cases for Bitcoin as a store of value, in particular the tricky but critical use-case of successions (heritages).


— General idea — 

This design restricts the bitcoins in a Vault to a private, predefined, finite (no patterns) and unforgeable set of authorized actions defined by the Vault creator at the setup.

Definition: an authorized action (or action) is an authorized address the bitcoins inside a Vault can be sent to, with an authorized timelock.
Action = <pubKeyHash> < timelock>

The Vault can be defined as a set of parent/child authorized actions. This enables the Vault creator to construct a Merkle tree of his Vault. During the setup, the creator computes the hashs of every authorized action, and builds his Merkle tree from the bottom, up to the top Merkle root. The Vault creator must give the appropriate Merkle proofs (authorizations) to the Vault participants (if any) according to the authorizations he grants them, and when someone wants to move funds inside or out of the Vault, he needs to provide to the network (in addition of a valid signature) the Merkle proof that demonstrates that his action is authorized by the Vault. The network can verify that:    
Hash [ Merkle_proof(Action) + Hash(Action) ] == Merkle_proof(Parent_Action)

The Merkle tree must be destroyed once the setup is completed. Storing the tree anywhere is unnecessary and endangers the Vault's privacy.


— Example — 

In this example, the Vault is composed of the actions A, B, C, D:

A--->B--->C
          \
            `--->D

If H is the hash function, the Merkle tree is:
                                                                          Merkle_root  
                                                                              /     \
              H(H(H(H(D)+H(1)) + H(H(C)+H(1))) + H(B))       H(A)
                                                 /     \                                                        
H(H(H(D)+H(1)) + H(H(C)+H(1)))        H(B)                                 
                     /     \                                   
 H(H(D)+H(1))        H(H(C)+H(1))                
           /                            \
          1                             1

Note: 1 are terminations to signal to the network that the coins are now allowed to exit the Vault. If the 1-terminations were not added, the bitcoins would be locked forever in the Vault because it would require to reverse H to spend them.

With notations:
                                                                                   Merkle_root  
                                                                                        /     \
                                                              Merkle_Proof(A)       H(A)
                                                                        /     \                                                        
Merkle_Proof(parent of C) = Merkle_Proof(B)        H(B)                                 
                                      /     \                                   
            Merkle_Proof(C)        H(H(C)+H(1))                
                                                        \
                                                         1

— nSequence —

nSequence has different timelock meanings for the different time related OP codes:
OP_CLTV: a tx spending the outputs of a [parent tx with nSequence] is invalid if current block number <= nSequence
OP_CSV: a tx spending the outputs of a [parent tx with nSequence] is invalid if current block number <= block number of the parent tx + nSequence

New meaning of nSequence for OP_VAULT:
OP_VAULT: a tx with nSequence is invalid if current block number <= block number of the parent tx + nSequence

—OP_VAULT— 

This opcode checks if the tx timelock allows the tx to be included in a block and outputs a hash.

OP_VAULT (nSequence, Merkle_proof(Action), pubKeyHash)
{
IF (current block number >= Max(block number of the parent outputs) + nSequence of current tx)
     hAction=H(H(pubKeyHash)+H(nSequence));
     h=H(Merkle_proof(Action)+hAction);
     return h;

ELSE
     return H(0);                                    // the tx cannot be included in a block yet
}


—Vault transaction structures—

Funding tx
scriptSig=<sig> <pubKey>
scriptPubKey=
<3> OP_PICK OP_HASH160 OP_VAULT <Merkle_root> OP_EQUALVERIFY OP_HASH160 <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG

Vault tx
scriptSig=<sig> <pubKey> <nSequence> <Merkle_proof>
scriptPubKey=
<3> OP_PICK OP_HASH160 OP_VAULT <Merkle_proof> OP_EQUALVERIFY OP_HASH160 <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG

Exit tx
scriptSig=<sig>  <pubKey> <nSequence> <Merkle_proof>
scriptPubKey=
OP_DUP OP_HASH160 <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG

Note: The exit tx can also use OP_VAULT if it is exiting the Vault while funding another Vault.


—New consensus rules— (enforcement of OP_VAULT txs)

IF
// this new rule concerns only Vault txs...
(parent tx VAULT_FLAG_ENABLE)
AND
 // ...that are not permitted to exit the Vault if the action is not terminated by 1 in the Merkle tree 
(    
H(<Merkle_proof> in tx’s scriptSig + H(H(H(pubKeyHash)+H(nSequence))) + H(1))) != <Merkle_proof> in parent tx’s scriptSig
)
AND
{
// the tx must be flagged as a Vault tx
(tx VAULT_FLAG_DISABLE) 
OR
// the tx violates the Merkle tree data structure
(<Merkle_proof> in tx’s scriptSig != <Merkle_proof> in tx’s scriptPubKey)
}

THEN the transaction is INVALID.

—Privacy— 

In this design, Vault txs are CoinJoin/CT compatible (joining with other Vault txs) and perhaps Vault users will be willing to way for days or weeks to achieve maximum privacy, as they are susceptible of holding significant value in these structures.

—Use-cases— 

"Smart successions" : a morbid yet critical use-case for Bitcoin as a store of value

Bitcoin currently struggles in dealing with successions in a trustless manner. How does the Bitcoin system know when the succession should be executed ? What happens in case of conflict between the heirs ? It’s a tricky but important use-case.

Bitcoin successions are dealt with by either sharing decrypted private keys with the heirs (trusting they won’t take the coins before due time or won’t have them stolen), renting a safe at the bank and making a testament (trusting the bank) or simply hiding the keys and hoping the heirs will find them when you disappear. None of these schemes are satisfying, especially when dealing with multiple heirs. This gap could likely hold back investors from investing a significant portion of their wealth in Bitcoin if they don’t have a trustless and secure mechanism that guarantees their succession will be executed according to their will.

Funding addr
    \
      `->Transfert addr—0—>Alice addr                                 (1)
               |          \
               |            `-50000—>Multisig2/2—>Bob addr    
               |                                             \                               (2)
               |                                               `—>Carol addr
               |
                `-100000—>Multisig2/3—>Bob addr                
                                                  \                                          (3) 
                                                    `—>Carol addr             

(1) Alice’s recovery address in case Bob and Carol were too impatient to spend the heritage.
(2) Alice added a Multisig2/2 controlled by Bob and Carol. Alice gave Bob and Carol each, half of the Merkel proof to pull the funds into Multisig2/2: first Bob and Carol need to agree on the conditions of the succession and sign the exit transaction from the Multisig2/2, than they can share their Merkel proof halves and pull the funds.
(3) Arbitration in case of disagreement (or if Bob or Carol is uncooperative, or disappeared): Alice added a Multisig2/3 involving an arbitrator in case Alice and Bob couldn’t find an agreement after 20’000 blocks or something. The arbitrator has no information on the succession until Bob or Carol asks for his assistance. Alice gave each Bob and Carol the full Merkel proof to pull the funds to Multisig2/3.

We can imagine services assisting in the Vault setups and in the blockchain monitoring, enabling successions to occur entirely on-chain, in a trustless, private and peer-to-peer manner, outside of the current financial system. 

Scorched earth policies if the Vault defender is entirely compromised
The following defence strategy is inspired from the paper mentionned in the introduction :

Funding addr
    \
      `->Transfert addr-1000->Spending addr
                \
                  `-0->Recovery addr1-100->Recovery addr2-1000->Recovery addr3
                                                                         \
                                                                           `-0->Hidden addr ??

An attacker broadcasts the Transfer tx from the Funding address. The defender can stay patient and learn if the attacker knows the recovery key (& the corresponding Merkle proofs) and ajust his defence accordingly: if indeed the adversary can move funds (he knows the recovery key(s)) and approches to the Vault exit (he knows also the Merkle proofs), the defender can burn all funds into fees, denying the attacker.

—Thanks for your attention—

Please let me know if you think this idea is worth exploring deeper.

Cheers,
Jerome
                                                                 


> On 27 Feb 2016, at 00:23, bitcoin-dev-request@lists.linuxfoundation.org wrote:
> 
> Send bitcoin-dev mailing list submissions to
> 	bitcoin-dev@lists.linuxfoundation.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> or, via email, send a message with subject or body 'help' to
> 	bitcoin-dev-request@lists.linuxfoundation.org
> 
> You can reach the person managing the list at
> 	bitcoin-dev-owner@lists.linuxfoundation.org
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of bitcoin-dev digest..."
> 
> 
> Today's Topics:
> 
>   1. Bitcoin Vaults. (Emin G?n Sirer)
>   2. The first successful Zero-Knowledge Contingent Payment
>      (Gregory Maxwell)
>   3. Re: The first successful Zero-Knowledge Contingent	Payment
>      (Sergio Demian Lerner)
>   4. Fwd: The first successful Zero-Knowledge Contingent	Payment
>      (Gregory Maxwell)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Fri, 26 Feb 2016 11:05:20 -0500
> From: Emin G?n Sirer <el33th4x0r@gmail.com>
> To: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
> Cc: Malte M?ser <malte.moeser@uni-muenster.de>,	Ittay Eyal
> 	<ittay.eyal@cornell.edu>
> Subject: [bitcoin-dev] Bitcoin Vaults.
> Message-ID:
> 	<CAPkFh0vuLsoNQUEdH-kGqXYvFJt1tXLvt0eMEuFZGm7Pus-_2g@mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
> 
> At the 3rd Bitcoin Workshop being held in conjunction with the Financial
> Cryptography Conference in Barbados, my group will be presenting a new idea
> for improving Bitcoin wallet security and deterring thefts today.
> 
> The write-up is here:
> 
> http://hackingdistributed.com/2016/02/26/how-to-implement-secure-bitcoin-vaults/
> 
> The paper with the nitty gritty details is here:
>    http://fc16.ifca.ai/bitcoin/papers/MES16.pdf
> 
> The core idea:
> 
> Our paper describes a way to create vaults, special accounts whose keys can
> be neutralized if they fall into the hands of attackers. Vaults are
> Bitcoin?s decentralized version of you calling your bank to report a stolen
> credit card -- it renders the attacker?s transactions null and void. And
> here?s the interesting part: in so doing, vaults demotivate key theft in
> the first place. An attacker who knows that he will not be able to get away
> with theft is less likely to attack in the first place, compared to current
> Bitcoin attackers who are guaranteed that their hacking efforts will be
> handsomely rewarded.
> 
> Operationally, the idea is simple. You send your money to a vault address
> that you yourself create. Every vault address has a vault key and a
> recovery key. When spending money from the vault address with the
> corresponding vault key, you must wait for a predefined amount of time
> (called the unvaulting period) that you established at the time you created
> the vault -- say, 24 hours. When all goes well, your vault funds are
> unlocked after the unvaulting period and you can move them to a standard
> address and subsequently spend them in the usual way. Now, in case Harry
> the Hacker gets a hold of your vault key, you have 24 hours to revert any
> transaction issued by Harry, using the recovery key. His theft,
> essentially, gets undone, and the funds are diverted unilaterally to their
> rightful owner. It?s like an ?undo? facility that the modern banking world
> relies on, but for Bitcoin.
> 
> The technical trick relies on a single new opcode, CheckOutputVerify, that
> checks the shape of a redeem transaction. Note that fungibility is not
> affected, as the restrictions are at the discretion of the coin owner alone
> and can only be placed by the coin owner ahead of time.
> 
> We suspect that this modest change could actually be a game-changer for
> bitcoin security: clients and keys are notoriously hard to secure, and a
> facility that allows you to possibly recover, and if not, permanently keep
> the hacker from acquiring your funds, could greatly deter Bitcoin thefts.
> 
> As always, comments and suggestions are welcome.
> - egs, Ittay Eyal and Malte Moeser.


-------------------------------------
On Thu, Jun 30, 2016 at 09:36:57AM -0400, Erik Aronesty via bitcoin-dev wrote:
> Encrypting links in a network without identity doesn't really seem to help
> enough for the costs to be justified.

Passive is still better than none.

> I would like to see a PGP-like "web of trust" proposal for both the
> security of the bitcoin network itself /and/ (eventually) of things like
> transmission of bitcoin addresses.

There already exists an unutilised WoT of "good" actors within the network -
miners via the coinbase transaction. Bootstrapping their own "trusted" pool of
IP addresses would be possible via the 100 bytes coinbase script.

> *Then* you can slap an encryption layer on top of it.   Once you have
> identity & P2P verified pub keys for nodes, encryption becomes easy.

A miner's WoT will give you this.

Alfie

-- 
Alfie John
https://www.alfie.wtf


-------------------------------------
NOTE:

Addresses aren't really meant to be broadcast - you should probably be
encoding BIP32 public seeds, not addresses.

OR simply:

- Send btc to rick@q32.com
- TXT record _btc.rick.q32.com is queried (_<coin-code>.<name>.<domain>)
- DNS-SEC validation is *required*
- TXT record contains addr:[<bip32-pub-seed>]

Then you can just say, in the podcast, "Send your bitcoin donations to
rick@q32.com".   And you can link it to your email address, if your
provider lets you set up a TXT record.   (By structuring the TXT record
that way, many existing email providers will support the standard without
having to change anything.)

This works with audio, video, web and other publishing formats... and very
little infrastructure change is needed.


On Wed, Aug 10, 2016 at 6:41 AM, Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Have you considered CDMA?  This has the nice property that it just sounds
> like noise.  The codes would take longer to send, but you could send
> multiple bits at once and have the codes orthogonal.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Before getting to my reply to Tom's message, I forgot to give my
thoughts on the Nov. 15 date. I think it's a reasonable date. With
various holidays coming up in the West, it's probably best to get the
word out now so that work can progress before some people get sucked
into family obligations and such. A month gives a bit of time without
dragging out the waiting game, IMO.

Now then....

On 2016/10/16 11:20, Tom Zander via bitcoin-dev wrote:
> There have been objections to the way that SegWit has been implemented for a 
> long time, some wallets are taking a "wait and see" approach.  If you look 
> at the page you linked[1], that is a very very sad state of affairs. The 
> vast majority is not ready.  Would be interesting to get a more up-to-date 
> view.

It's not the website's fault if wallet devs aren't updating their
statuses. Besides, "WIP" can mean an awful lot of things. For example, I
know Armory made significant progress with SegWit support as part of
their upcoming 0.95 release. I have full confidence they'll be ready
relatively soon. Other wallets may be ready. Other wallets may be stuck
where they were in the spring. In any event, it's a free country. Unlike
consensus rules and such, it's trivial to move one's funds to a wallet
that fully supports SegWit if that's what one desires.

In addition, I was at the wallet workshop at Scaling Bitcoin last week.
An awful lot of things were on the board as potential discussion points.
I think SegWit was mentioned but wasn't really discussed. I don't think
FlexTrans was even mentioned (and it's off-topic anyway). Wallet devs
are far more concerned about things like UI and standards for HW wallets
than they are about their ability to support SegWit. I think wallet devs
are quite capable of making noise if they felt that SegWit was a bad
feature, or a difficult-to-support feature.

> Wallets probably won't want to invest resources adding support for a feature 
> that will never be activated. The fact that we have a much safer alternative 
> in the form of Flexible Transactions may mean it will not get activated. We 
> won't know until its actually locked in.

A lot of devs have already worked on SegWit support. This has been
covered. Even if they don't support SegWit, the wallets will probably
work just fine. (For awhile, Armory did crash when trying to read SegWit
data in Core's blockchain files. That problem was fixed, and it was
probably a rarity since very few wallets rely directly on Core.) As long
as devs use testnet or regtest to iron out their kinks before hitting
mainnet, I can't think of a single good reason to hold back SegWit
solely due to wallet support.

Also, once again, FlexTrans is off-topic. As others have said, you're
basically being stubborn at this point. If you insist on discussing
FlexTrans, start another thread. It sounds like quite a few devs would
be more than happy to say a word or two about your proposal.

-- 
---
Douglas Roark
Cryptocurrency, network security, travel, and art.
https://onename.com/droark
joroark@vt.edu
PGP key ID: 26623924


-------------------------------------
> I don't see any benefit to
changing that. It is better that coins are burned.

I think this is our fundamental disagreement. People will burn coins to
encode data, why allow this when there's a better alternative?

> You *always* need a key, to redeem inputs... regardless of values.

Correct, but with BIP70 that key is in the user's wallet and you can
construct transactions on another machine (thus not needing a key during
construction). Right now there's no way to do the transaction construction
on another machine with zero value OP_RETURNs.

On Mon, Jan 25, 2016 at 7:04 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Tuesday, January 26, 2016 3:01:13 AM Toby Padilla wrote:
> > > As I explained, none of those reasons apply to PaymentRequests.
> >
> > As they exist today PaymentRequests allow for essentially the same types
> of
> > transactions as non-PaymentRequest based transactions with the limitation
> > that OP_RETURN values must be greater. In that sense they're basically a
> > pre-OP_RETURN environment. OP_RETURN serves a purpose and it can't be
> used
> > with PaymentRequest transactions.
>
> OP_RETURN can be used, but you need to burn coins. I don't see any benefit
> to
> changing that. It is better that coins are burned.
>
> > > I have no idea what you are trying to say here.
> >
> > I think if you think through how you would create an OP_RETURN
> transaction
> > today without this BIP you'll see you need a key at some point if you
> want
> > a zero value.
>
> You *always* need a key, to redeem inputs... regardless of values.
>
> Luke
>

-------------------------------------
Better late than never, I should correct things here. In the future it
would probably be more productive to open an issue. Otherwise there is
no mechanism for someone to take ownership of a response.

On Sun, Aug 30, 2015 at 7:45 PM, Kristov Atlas via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
>> 1.      Does your application take any steps to create ambiguity between
>> transactions which unavoidably spend from multiple addresses at the same
>> time and intentional mixing transactions?
> No, Bitcoin-Qt does not try to make non-mixing transactions look like mixing
> transactions.
>> 2.      What algorithms does your application use for ordering inputs and
>> outputs in a transaction? In particular, how do you handle the change output
>> and do you take into account common practices of other wallet applications
>> when determining ordering?
>
> Not yet BIP 69. These notes suggest that outputs are randomized:
> https://bitcoin.org/en/release/v0.8.1

The ordering used by Bitcoin-QT is cryptographically randomized. This
provides the greatest privacy possible.

The BIP 69 recommendation would currently be equally as private if
universally used, but today would reduce privacy by making the
software more distinguishable.  It is unclear if BIP69 will be equal
in privacy in the future, because external infrastructure may impose
ordering requirements that are incompatible with it.

>> 3.      Does your application minimize the harmful effects of address
>> reuse by spending every spendable input (“sweeping”) from an address when a
>> transaction is created?
>
> Unknown

>> 4.      Does your application fully implement BIP 62?

BIP 62 is withdrawn. The useful mechanisms in it for standardness
rules are, of course, implemeted in Bitcoin Core-- were invented
there, and have been there for years.

>> Mixing
>>
>> 5.      If your application supports mixing:

It's unclear to me precisely what is meant here. I'll answer broadly.

Bitcoin Core is compatible with and can be used with the joinmarket
module to include coinjoins. The raw transaction functionality in
Bitcoin Core was also created specifically to facilitate coinjoins.
Beyond joinmarket there have been several other coinjoin modules
created for Bitcoin Core though today JM is by far the most common,

This functionality is not directly implemented for a number of reasons
including the non-existence of decenteralized tools for this that
don't harm the user's privacy in other ways.

>> a.      What is the average number of participants a user can expect to
>> interact with on a typical join transaction?
>> b.      Does your application attempt to construct join transactions in a
>> way that avoids distinguishing them from non-join transactions?
>> c.      Does your application perform any kind of reversibility analysis
>> on join transactions prior to presenting them to the user for confirmation?
>> d.      Is the mixing technique employed secure against correlation
>> attacks by the facilitator, such as a CoinJoin server or off-chain mixing
>> service?
>> e.      Is the mixing technique employed secure against theft of funds by
>> the facilitator or its participants?

Skipped as these are specific to the implementation in use.

>> Donations
>> 6.      If your application has a fee or donation to the developers
>> feature:
> No donation feature last time I checked.
>> a.      What steps do you take to make the donations indistinguishable
>> from regular spend in terms of output sizes and destination addresses?

As Kristov noted, Bitcoin Core does not implement anti-features like donations.

>> Balance Queries and Tx Broadcasting
>>
>> 7.      Please describe how your application obtains balance information
>> in terms of how queries from the user’s device can reveal a connection
>> between the addresses in their wallet.
>> a.      Does the application keep a complete copy of the blockchain
>> locally (full node)?
> Yes

Optionally, but in all cases the user's privacy is indistinguishable
from keeping all the data locally.

>> b.      Does the user’s device provide a filter which matches some
>> fraction of the blockchain while providing a false positive rate (bloom or
>> prefix filters)?
> No, it just downloads the whole blockchain and performs local queries.

It would be more correct to say that Bitcoin Core always has the
highest possible FP rate.  It uses the only currently available tool
to avoid leaking private address information to indexing services.  As
several academic studies have shown, bloom filters are completely
inadequate for protecting user privacy.

>> i.      If so, approximately what fraction of the blockchain does the
>> filter match in a default configuration (0% - 100%)?
> 100%, unless a user bootstraps downloading the blockchain. Bootstrapping
> will potentially limit the user's anonymity set to other people who have
> downloaded that bootstrap.dat file.

I user that has downloaded a bootstrap.dat is indistinguishable from
any other user on the network; their transaction anonymity set is not
reduced in any way by doing this.  By running bitcoin at all they are
distinguished from other people who do not, but thousands of hosts run
Bitcoin without even having a wallet.

>> c.      Does the user’s device query all of their addresses at the same
>> time?
> N/A

To be clear: This is N/A because there are no queries that would leak
private information about the user's wallet.

  >> d.      Does the user’s device query addresses individually in a manner
>> that does not allow the query responder to correlate queries for different
>> addresses?
> No. Just download blocks and processes that information locally.

Yes. Because the Bitcoin Core downlaods all information, the third
parties cannot correlate responses.

>> e.      Can users opt to obtain their balance information via Tor (or
>> equivalent means)?
> If Tor is set up as a SOCKS proxy, you can configure Bitcoin-QT download the
> blockchain and broadcast txs through a single Tor circuit. This can be
> configured once before opening Bitcoin-Qt.

Bitcoin Core does make remote queries to obtain "balance information",
but it can be directed to perform all commmunications via tor, before
starting it as noted.

>> 8.      Does the applications route outgoing transactions independently
>> from the manner in which it obtains balance information? Can users opt to
>> have their transactions submitted to the Bitcoin network via Tor (or an
>> equivalent means) independently of how they obtain their balance
>> information?
> No, you can only configure a single proxy.

Bitcoin Core can simultaneously connect to both Tor hidden services
and the public IPv4 network for improved partitioning resistance (and
has been able to for years). Instead of setting the socks proxy, the
user configures onion=<tor proxy>.

As of 0.12 inbound tor HS is also auto-configured by default when tor
is installed.

>> 9.      If your application supports multiple identities/wallets, does
>> each one connect to the network as if it were completely independent from
>> the other?
>
>
> No built-in support for multiple identities. You can hotswap wallet files to
> crudely simulate this. You'd have to manually change the Tor connection
> outside of Bitcoin-Qt to create the effect of making the network connections
> independent.

All network connections are independent via Tor by default, no manual
change is required there. Separate "identities" do require separate
wallets, as noted.

>> a.      Does the application ever request balance information for
>> addresses belonging to multiple identities in the same network query?

No it does not and cannot. Freedom from this kind of leak is one of
the benefits of the current design that doesn't allow intermixing
"identities" in wallets.

> Blocks are downloaded and tx broadcasts received/relayed rather than
> querying the utxo set for a particular address. When swapping between wallet
> files, some information may be leaked e.g. the client may be at the same
> block height in terms of what it has downloaded from the p2p network, which
> may leak to global passive adversaries/AS's or sybil attackers the fact that
> a single client was used for multiple wallets.

However, unlikely most other wallets, Bitcoin nodes forward
transactions for third parties and do not make external queries for
private information. Because of this the ability to correlate a
particular node connection multiple times does not necessarily leak
anything about wallet usage.

>> b.      Are outgoing transactions from multiple identities routed
>> independently of each other to the Bitcoin network?
>
> Transactions from multiple identities would not be routed at the same time.
> I'm not clear what happens if you have a single wallet (identity) open and
> then open a new wallet (identity) without closing Bitcoin-Qt -- some of the
> same routing paths may still be used such that an attacker might observe
> transactions broadast signed by private keys from multiple wallets
> (identities) and observe that they appear to come from the same wallet
> client. OBPP should assume the worst unless prevented contrary evidence.

This assumption is incorrect. All the private wallet state is stored
in the wallet. If the wallet is changed the node does know any of them
anymore.  There is no ability to open a new wallet without restarting
the software.

That said, Bitcoin Core normally relays transactions for third
parties-- unlikely virtually all other wallets. This means that where
observation of a transaction from another wallet would give a nearly
guaranteed identification that the system on the other end of the link
is the source, with Bitcoin Core sending a transaction is merely
potentially suggestive of origination.

>> c.      When an identity/wallet is deleted, does the deletion process
>> eliminate all evidence from the user's device that the wallet was previously
>> installed?
> Identity is primarily tied to a wallet.dat file. Deleting it will remove
> most of the evidence that the wallet was installed on that device, but there
> may be some extra information in ancillary files that should also be
> deleted.  This is an OS-level function, as there is no operation built into
> the client to delete a wallet file (identity).

After review and testing we've determined that reliable deletion of
private data is not very feasible on current hardware/OSes. Techniques
which used to work, like overwriting are defeated by write balancing.
We recommend users use OS level encryption to protect their privacy
locally.

>>         Network Privacy
>> 10.     When a user performs a backup operation for their wallet, does
>> this generate any automatic network activity, such as a web query or email?
> No. Backups are local, and no email or SMS is linked. No web queries related
> to backup.

Right.

>> 11.     Does your application perform any lookup external to the user’s
>> device related to identifying transaction senders or recipients?
> No

Not for normal transactions. Bitcoin Core currently supports payment
URIs and BIP70, and if a user follows a payment URI it may instruct
the user to make a connection to a location requested by the payee.

>> 12.     Does you application connect to known endpoints which would be
>> visible to an ISP, such as your domain?
> Yes, some connections to known p2p full nodes to bootstrap the connection to
> the Bitcoin p2p network. This is configurable, but there are defaults. An
> ISP is likely to be able to identify a customer as running the Bitcoin-Qt
> client in particular on this basis.

Kind of.  If a Bitcoin Core node already knows of peers through prior
operation and is able to get at least two network connections within
11 seconds, it will make no further queries.

If a node is completely new and hasn't been otherwise configured; it
will perform four DNS queries to determine lists of candidate nodes.
These queries are frequently answered by caching name servers and do
not go all the way back to their origins. Only if both of these step
fail does it consult a hardcoded list of several hundred nodes to
attempt initialization.

That said, Bitcoin traffic is easily identifiable regardless of how
peers are found. We recommend users run Tor, and if tor is used no
identifiable traffic should happen, except for timing/volume analysis.
And many parties run Bitcoin Core nodes without running wallets; so
the use of Bitcoin does not identify a user as even having a wallet at
all.

>> 13.     If your application connects directly to nodes in the Bitcoin P2P
>> network, does it either use an unremarkable user agent string (Bitcoin Core.
>> BitcoinJ, etc), or randomize its user agent on each connection?
>
>
> BIP12 specifies format for user agents:
> https://github.com/bitcoin/bips/blob/master/bip-0014.mediawiki
>
> It appears that the Bitcoin-QT leaks specific information about its client
> version through User Agent. This file defines the current client version:
> https://github.com/bitcoin/bitcoin/blob/55294a9fb673ab0a7c99b9c18279fe12a5a07890/src/clientversion.h
>
> Various other files seem to use this to build up the UA string, which is
> transmitted to other peers.

Bitcoin Core is this questions _definition_ of an unremarkable useragent.

But yes, the useragent notes the major/minor version. Concealing this
would have little to no privacy advantage, as functional/behavioral
analysis would easily reveal the version with at least that level of
precision.

>> 14.     Does the application uninstall process for your application
>> eliminate all evidence from the user's device that the application was
>> previously installed? Does it also eliminate wallet data?
> Probably depends on the platform. Last time I checked, I think Bitcoin-Qt
> leaves behind a .bitcoin directory on most platforms even after you run an
> uninstall script.

If uninstall deleted the wallet it would reliably result in massive
funds loss for users.

To conceal their user of Bitcoin users should at a minimum do a
security erase of their system.

Other wallets who claim to "delete" private information which was
previously stored on disk are likely giving their users a false sense
of security. Doubly so in that many other wallets are written in
dynamic lanaguages which make it impossible to prevent highly secret
data from being written to system swap.

>> 15.     Does your application use techniques such as steganography to
>> store persistent wallet metadata in a form not identifiable as belong to a
>> Bitcoin wallet application?
> No

I believe any software which claimed to do this would have to meet a
rather high burden of proof.

>> 16.     Please describe the degree to which users can use passwords/PINs
>> to protect their data:
>> a.      Can the user set a password/PIN to protect their private keys?
> You can encrypt the wallet file with a password. The wallet is "locked"
> until the password is entered, preventing decryption of the private keys.

Correct. And unlike some other Wallets the KDF used to harden the
users key takes 100ms with efficient native code; this substantially
limits attacker brute for performance.

>> b.      Can the user set a password/PIN to protect their public keys and
>> balance information?
> No -- any wallet.dat file can be opened and the public keys inspected
> without the password.
>> c.      Can the user set a password/PIN to encrypt other wallet metadata,
>> such as address books and transaction notes?
> No -- any wallet.dat file can be opened and the metadata inspected without
> the password.

We recommend users use full disk encryption. Encrypting the public
data in the wallet would require the wallet to enter their key at
every use and increase the probability that their key was leaked (or
if two keys were used, that they'd forget their spending key).

Even if the public key information were encrypted, other data on their
computer (browser cache, swap, logs) would likely compromise the
user's privacy, thus the full disk encryption recommendation. Full
disk encryption is a common, easily used tool; and I don't believe any
wallet software that stores data locally can provide strong privacy in
practice without it.

>> d.      Does the application use a single password/PIN to cover all
>> protected data, or does it allow the use of multiple passwords/PINs?
> A single password for the wallet file.

Right. Each wallet file can have it's own single password which
protect spending.

>> 17.     Do you as a wallet provider ever have access to unencrypted copies
>> of the user’s private keys, public keys, or any other wallet metadata which
>> may be used to associate a user with their transactions or balances?
> No custodianship.

Right.

>>        Telemetry Data
>> 18.     If your application reports telemetry data, such as usage
>> information or automatic crash reporting, does the user have the opportunity
>> to review and approve all information transmitted before it is sent?
> No obvious telemetry data being sent.

No telemetry data.

>>         Source Code and Building
>> 19.     Can a user of your application compile the application themselves
>> in a manner that produces a binary version identical to the version you
>> distribute (deterministic build system)?

Yes, and a large portion of our user base does their own builds. Our
determinstic build process is also actively audited by a good dozen
parties who post cryptographic signatures of their duplicated builds.


-------------------------------------
Hi,

Is your simulation code available somewhere?

I was just wondering why mycelium generates a very big UTXO set for <1000sat, because change outputs will never be smaller than 
5460sat (=TransactionUtils.MINIMUM_OUTPUT_VALUE). If the change would be lower, it simply is skipped and added to the miner fee:
	-> https://github.com/mycelium-com/wallet/blob/master/public/bitlib/src/main/java/com/mrd/bitlib/StandardTransactionBuilder.java#L334

Does your simulation account for that?

It might also be that the small UTXO came from external tx and we never spend them, bec. of pruning/privacy. Not sure how we could optimize that.

Cheers,
Daniel

On 2016-09-21 14:58, Murch via bitcoin-dev wrote:
> Hi,
> 
> I'm currently compiling my Master's thesis about Coin Selection and my
> presentation proposal to Scaling Bitcoin has been accepted.
> 
> For my thesis, I have analyzed the Coin Selection problem, created a
> framework to simulate wallet behavior on basis of a sequence of
> payments, and have re-implemented multiple coin selection strategies of
> prominent Bitcoin wallets (Bitcoin Core, Mycelium, Breadwallet, and
> Android Wallet for Bitcoin).
> 
> As the Scaling Bitcoin site suggests that research should be made
> available to this mailing list, I would like to invite you to have a
> look at:
> 
> http://murch.one/wp-content/uploads/2016/09/CoinSelection.pdf
> 
> The PDF (176 kB) contains a two page description of my on-going work,
> including preliminary simulation results, and three figures showing the
> simulated wallets' UTXO compositions at the end of the simulation.
> 
> I can provide further information as requested, and would welcome any
> feedback.
> 
> →→ If anyone has another sequence of incoming and outgoing payment
> amounts at hand that I could run my simulation on, I'd love to hear
> about it.
> 
> Regards
> 
> Murch
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
I made a repo to be a home for sync_flags here:
https://github.com/moral-agent/sync_flags

If you see any personally identifying information, please be a good sport
and let me know. I'm a nobody, but I'd still prefer not to get doxxed.

Two changes to the proposal (see repo for explanations)

1. Sync flags now would have the same difficulty as blocks.
2. Blocks now donate to 5 sync flags instead of 1

I also added comments about selfish mining and invalid block spam.

Response to replies:

tomz@freedommail.ch: What is the advantage over optimistic mining?

1. Sync flags can be somewhat smaller than block headers.
2. Sync flags improve variance by doubling the number of chances to win
3. Sync flags can be distinguished from normal blocks, so SPV clients can
ignore them as confirmations.
4. Sync flags reward all miners equally. Optimistic blocks have to be empty
unless you mined the previous block, which damages decentralization.
5. Sync flags result in fewer empty blocks, smoothing out resource usage
6. Sync flags make transaction stuffing by miners either obvious or costly
7. Sync flags give miners something to do while they wait for attractive
transactions to appear.

erik@q32.com: Flags will be selfish mined.

I agree that flags would likely be selfish mined. I have modified the
proposal to say that flags have the same POW target as blocks, so the
selfish mining vulnerability should be equal to the current protocol.

martijn.meijering@mevs.nl: Why expect more selfish mining?

Because flags had small POW relative to blocks. After you find a block, why
not hide it while you take a crack at the flag?

tier.nolan@gmail.com: Effect is same as mandatory empty blocks.

Not quite the same:

1. Sync flags can be somewhat smaller than block headers.
2. Sync flags can be distinguished from normal blocks, so SPV clients can
ignore them as confirmations.
3. Sync flags make transaction stuffing by miners either obvious or costly
4. No one pays for empty blocks, except for the block subsidy. Some miners
may choose to only mine the non-empty blocks, resulting in
hashpower-for-rent to make mischief or hashpower that oscillates, creating
a situation where empty blocks take longer to mine than full blocks.

nickodell@gmail.com: Payout mechanism incompatible with certain mining pools

Hopefully some kind of smart contract structure could be implemented as you
suggested.


On Tue, Jul 26, 2016 at 6:03 PM, Nick ODell <nickodell@gmail.com> wrote:

> Moral,
>
> Mining the sync flag isn't compatible with the payout structure of non
> hot-wallet pools like Eligius or decentralized pools like p2pool.
> Those need the ability to split a reward among multiple parties.
> Instead of giving an address to send the funds to, you could include
> the hash of the transaction allowed to spend the sync flag output.
> You'd have to zero the previous outpoint of the transaction before
> hashing, since you don't know what the hash of the coinbase ten blocks
> from now will be.
>
>
> On Tue, Jul 26, 2016 at 6:47 AM, Moral Agent via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > I posted this to /r/bitcoin yesterday but it got minimal comments. One
> uses
> > suggested I try the mailing list so here it is:
> >
> > The idea presented here could have the following benefits:
> >
> > 1. Improve mining decentralization
> > 2. Reduce variance in mining profitability
> > 3. Reduce or eliminate SPV mined blocks
> > 4. Reduce or eliminate empty blocks, smoothing out resource usage
> > 5. Reduce or eliminate the latency bottleneck on throughput
> > 6. Make transaction stuffing by miners be either obvious or costly
> > 7. Gives miners something to do while they wait for attractive
> transactions
> > to appear
> > 8. Can be easily done with a soft fork
> >
> > #Basic idea:
> >
> > Ideally, all miners would begin hashing the next block at exactly the
> same
> > time. Miners with a head start are more profitable, and the techniques
> that
> > help miners receive and validate blocks quickly create centralization
> > pressure.
> >
> > What if there was something that acted like the starting flag at a race,
> > which could suddenly wave and cause all of the miners to simultaneously
> > begin hashing the next block?
> >
> > #Implementation:
> >
> > Let a sync flag be a message consisting of:
> >
> > 1. Hash of the previous block.
> > 2. Bitcoin address
> > 3. Nonce
> >
> > This tiny message could propagate through the network at maximum speed.
> If
> > miners had to include the hash of this flag in the next block, then all
> > miners wait for this flag, and when it suddenly spread through the
> network,
> > all miners could simultaneously begin hashing the next block.
> >
> > The sync flag should not be produced too quickly. You want to give
> everyone
> > enough time to be ready to hash the next block. Let's say that the hash
> of
> > the sync flag is a proof of work that is targeted for 2 minutes.
> >
> > To fund this proof of work, the protocol is modified to demand that the
> > block produced 10 blocks after the sync flag must allocate 25% of the
> block
> > reward to the address published by the sync flag. In this way, sync flags
> > are produced in 2 minutes, and blocks are produced in 8 minutes, with 10
> > minutes total.
> >
> >
> > Illustration 1: https://s32.postimg.org/wzg0hs8lx/sync_flag.png)
> >
> > Illustration 2: https://s32.postimg.org/vc5y9yz4l/sync_flag2.png
> >
> >
> > #Explanation of reasons:
> >
> > **Improve mining decentralization**
> >
> > One factor driving centralization is the imperative miners have to
> achieve
> > low latency in receiving and validating blocks. To achieve low latency,
> it
> > helps a lot if you have expensive low-latency internet connections,
> curated
> > network topologies, and large pools that have a plausible chance of
> finding
> > consecutive blocks. If miners are expected (or forced) to validate a
> block
> > prior to mining on top of it, the rational end game would be to outsource
> > the validation step to a trusted third party specialist who can choose
> > optimal locations on the globe to serve their (multiple?) mining pool
> > clients. These are all less decentralized than the mining situation
> Satoshi
> > and others imagined.
> >
> > **Reduce variance in mining revenue**
> >
> > Currently, there are about 144 opportunities per day for a pool or solo
> > miner to see any revenue at all. With sync flags, that number doubles to
> > 288. Sync flags are only worth 25% of what a block is worth, but this
> still
> > represents a significant reduction in variance. This variance is one
> factor
> > causing solo miners to group into pools, and large pools to be more
> > attractive than small pools.
> >
> > **Reduce or eliminate SPV mined blocks**
> >
> > One way miners have sought to make
> > full-block-transmission-and-validation-latency irrelevant has been
> through
> > "SPV" mining or "Head-first" mining. There is some evidence that these
> > techniques may be widely used, and that badgering the miners about it is
> an
> > ineffective strategy to stop them.
> >
> > In SPV mining, a miner would simply accept any block header that shows
> the
> > correct proof of work. All other validation is entrusted to other miners.
> > This practice is quite dangerous as the SPV miners can wander off on some
> > invalid chain, taking SPV nodes with them. If this occurs during a soft
> > fork, these blind miners can also fool unupgraded fully validating nodes
> > into following them.
> >
> > "Head-first" mining means that miners start hashing as soon as they
> receive
> > the block header with the correct POW, but they simultaneously validate
> the
> > block, and abandon it if is not valid. I consider this to be pretty
> safe, as
> > it strictly limits the length of an invalid chain that can result from
> > mining without validating. However, "Head-first" mining can plausibly
> > generate 2 or 3 confirmations of an invalid block. It would be nice if
> such
> > confirmations did not happen.
> >
> > The sync flag technique is similar to head-first mining, but rather than
> > hashing the next block while they wait for transmission and validation of
> > the prior block, they hash the sync flag. Nodes can differentiate between
> > sync flags and blocks, and can ignore sync flags when counting
> > confirmations.
> >
> > **Reduce or eliminate empty blocks, smoothing out resource usage**
> >
> > Empty blocks are another consequence of SPV or Headfirst mining, because
> the
> > miner cannot safely include any transactions in the block they are
> hashing
> > until they have validated the prior block. By delaying the start of
> hashing
> > the next block until after validation, miners would not have this reason
> to
> > mine empty blocks.
> >
> > **Reduce or eliminate the latency bottleneck on throughput**
> >
> > Centralization pressure due to latency issues has been a major
> preoccupation
> > over the last year. If latency mattered much less, it could represent a
> > scalability improvement that could enable higher throughput.
> >
> > **Make transaction stuffing by miners be either obvious or costly**
> >
> > Currently, the entire block reward goes to the miner who mines it. One
> > unfortunate consequence of this is that it does not cost the miner
> anything
> > to covertly stuff the block with transactions. These transactions would
> pay
> > fees and be indistinguishable from ordinary transactions, but the fees
> are
> > paid by the miner and then immediately returned to the miner.
> >
> > With sync flags, the miner must share these transaction fees with the
> > address contained in the sync flag 10 blocks prior. This means that if
> the
> > miner gives the transactions a normal looking fee, they will incur a cost
> > that will be paid to the sync flag. If the miner wants to avoid this,
> they
> > must give their stuffing transactions a zero fee, which provides evidence
> > that they are stuffing.
> >
> > Also, when miners stuff with transactions using a zero fee, they cannot
> > manipulate the perception of how much fee it takes to get into a block.
> >
> > Note that miners could still try to covertly stuff blocks that will pay a
> > sync flag that they themselves created. if this is a big concern, it can
> be
> > addressed by forcing blocks to pay multiple sync flags.
> >
> > **Gives miners something to do while they wait for attractive
> transactions
> > to appear**
> >
> > From the Montreal scaling workshop last year, we have [this
> > talk](
> https://scalingbitcoin.org/montreal2015/presentations/Day1/13-miles-carlsten-Mind-the-Gap.pdf
> )
> > which worried that as the block subsidy reduced and transactions became a
> > more important fraction of miner revenue, it would be rational for
> miners to
> > turn off their mining equipment for a "gap" phase after a block is
> found, to
> > allow time to pass as more lucrative transactions entered the mempool.
> >
> > I don't know whether this will actually happen. The presence of a
> suitable
> > backlog of transactions would help prevent this dynamic from emerging.
> But
> > if such idling behavior was the optima mining strategy, it could create a
> > serious vulnerability. Idle hands are the devil's workshop as the saying
> > goes, and idle miners represent a pool of inert hashpower that is
> available
> > to rent for all kinds of destabilizing purposes. It would be better to
> put
> > those miners to profitable work mining a sync flag while they wait.
> >
> > Also, this creates a more efficient price discovery mechanism for
> > transactions, because you allow transactions paying high fees time to
> arrive
> > to the marketplace, rather than take whatever anyone is offering because
> all
> > the "good" transactions got gobbled up in the prior block.
> >
> > **Can be easily done with a soft fork**
> >
> > Although a hard fork would be more efficient, sync flags could be easily
> > implemented using a soft fork by introducing the following rule:
> >
> > Every block must include a transaction which pays 25% of the block
> reward to
> > the address given by the 10th previous sync flag, and commits to the
> hash of
> > the 1st previous sync flag.
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
>

-------------------------------------
Thank you Luke, this makes it clearer.

It doesn't change that this scenario is an attack that doesn't give the 
attacker any benefit and the attacked doesn't loose anything either (as 
Dave pointed out).

This is a completely academical problem that assumes so many stupid 
mistakes from software and from people that its very unlikely. On top of 
that it assumes a rather lengthy 51% attack in concert with this already 
extremely unlikely usecase.

In the scenario you assume stupid people and then you solve it by requiring 
the victim to suddenly be super smart and use a solution specifically 
designed for this super unlikely usecase that probably will never actually 
happen...

I don't buy it.

On Friday, 23 September 2016 22:34:41 CEST Luke Dashjr wrote:
> Joe sends Alice 5 BTC (UTXO 0).
> Fred sends Alice 4 BTC (UTXO 1).
> Alice sends Bob 4 BTC using UTXO 1 (creating UTXO 2).
> Fred double-spends UTXO 1 with UTXO 1-B. This invalidates Alice's
> transfer to Bob.
> Alice has UTXO 0 which she can send to Bob (UTXO 3), but if she does so,
> it is possible that UTXO 0 could be mined, and then both UTXO 2 and UTXO
> 3 which would result in her giving Bob a total of 8 BTC rather than
> merely 4 BTC. Even if Alice waits until Fred's UTXO 1-B confirms 10
> blocks deep, it is not impossible for a reorganization to reverse those
> 10 blocks and confirm UTXO 1 again.
> Using OP_CHECKBLOCKATHEIGHT, however, Alice can create UTXO 3 such that
> it is valid only in the blockchain where Fred's UTXO 1-B has confirmed.
> This way, if that block is reorganized out, UTXO 3 is invalid, and
> either Bob receives only the original UTXO 2, or Alice can create a UTXO
> 3-B which is valid in the reorganized blockchain if it again confirms
> the UTXO 1-B double-spend.
> 
> Luke
> 
> On Friday, September 23, 2016 2:37:39 PM Tom via bitcoin-dev wrote:
> > On Friday 23 Sep 2016 09:57:01 Luke Dashjr via bitcoin-dev wrote:
> > > This BIP describes a new opcode (OP_CHECKBLOCKATHEIGHT) for the
> > > Bitcoin
> > > scripting system to address reissuing bitcoin transactions when the
> > > coins they spend have been conflicted/double-spent.
> > > 
> > > https://github.com/luke-jr/bips/blob/bip-cbah/bip-cbah.mediawiki
> > 
> > Can you walk us through a real live usecase which this solves?  I read
> > it and I think I understand it, but I can't see the attack every
> > giving the attacker any benefit (or the attacked losing anything).
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev




-------------------------------------
Because the blocksize limit is denominated in bytes, miners choose
transactions to add to a block based on fee/byte ratio. This mean that
if you make a transaction with a lot of inputs, your transaction will
be very big, an you'll have a to pay a lot in fees to get that
transaction included in a block.

For a long time I have been of the belief that it is a flaw in bitcoin
that you have to pay more to move coins that are sent to you via small
value UTXOs, compared to coins sent to you through a single high
values UTXO. There are many legitimate uses of bitcoin where you get
the money is very small increments (such as microtransactions). This
is the basis for my "Wildcard inputs" proposal now known as BIP131.
This BIP was rejected because it requires a database index, which
people thought would make bitcoin not scale, which I think is complete
malarkey, but it is what it is. It has recently occurred to me a way
to achieve the same effect without needing the database index.

If the blocksize limit was denominated in outputs, miners would choose
transactions based on maximum fee per output. This would essentially
make it free to include an input to a transaction.

If the blocksize limit were removed and replaced with a "block output
limit", it would have multiple positive effects. First off, like I
said earlier, it would incentivize microtransactions. Secondly it
would serve to decrease the UTXO set. As I described in the text of
BIP131, as blocks fill up and fees rise, there is a "minimum
profitability to include an input to a transaction" which increases.
At the time I wrote BIP131, it was something like 2 cents: Any UTXO
worth less than 2 cents was not economical to add to a transaction,
and therefore likely to never be spent (unless blocks get bigger and
fee's drop). This contributes to the "UTXO bloat problem" which a lot
of people talk about being a big problem.

If the blocksize limit is to be changed to a block output limit, the
number the limit is set to should be roughly the amount of outputs
that are found in 1MB blocks today. This way, the change should be
considered non-controversial. I think its silly that some people think
its a good thing to keep usage restricted, but again, it is what it
is.

Blocks can be bigger than 1MB, but the extra data in the block will
not result in more people using bitcoin, but rather existing users
spending inputs to decrease the UTXO set.

It would also bring about data that can be used to determine how to
scale bitcoin in the future. For instance, we have *no idea* how the
network will handle blocks bigger than 1MB, simply because the network
has never seen blocks bigger than 1MB. People have set up private
networks for testing bigger blocks, but thats not quite the same as
1MB+ blocks on the actual live network. This change will allow us to
see what actually happens when bigger blocks gets published.

Why is this change a bad idea?


-------------------------------------
> I have just PRed a draft version of two BIPs I recently wrote.
> https://github.com/bitcoin/bips/pull/362

Thanks for the feedback and IRC discussions.

I have overhauled both BIPs.
https://github.com/bitcoin/bips/pull/362/files#diff

Main changes for the encryption BIP:
* No message wrapping. Once encryption is established, everything is
encrypted. No timeout.
* Added MAC: proposed AEAD is now ChaCha20-Poly1305 with an alternative
for AES256-GCH
* Independent ECDH negotiation and independent secrets for the symmetric
cipher for both communication directions
* Optimized message format and message-batch-option for encrypted data

It could be that the p2p performance for Chacha20-poly1305 encrypted
message is slightly better then the current plaintext message format
(dropping the network magic and the sha256 per message).

P2p authentication BIP:
* No message wrapping. Peers keep the state once authenticated.
* Simplified and auth now requires encrypted channels.


Some answers...

> How does a peer know what messages the other peer requires to be
authenticated?

This is not covered by the auth BIP. Peers could agree on a protocol
extension outside of any BIP.
Once auth is possible, new BIPs could be written. Things like only
allowing filtering (or other services) to authenticated peers (and
disabling NODE_BLOOM).

> How does banning in this specific case enable fingerprinting as
opposed to any other banning?

Current nodes ignore a unknown message with a command like "auth".
Banning would allow a requesting peer to identify nodes that support
auth and attack them over different channels ("ah, ... this guy supports
auth, they must have some secret data, lets attack over SSH).

>> This proposal is backward compatible. Non supporting peers will
ignore the <code>auth</code> message.
> ... and not process it at all? How is that backward compatible?

Depends how we define backward compatibility. :-)
Peers supporting this "extension" can still interact with older peers.

> This proposal is backward compatible. Non supporting peers will ignore
the > <code>enc*</code> messages.

Current p2p implementation ignores any unknown command.


</jonas>


-------------------------------------
On 3/9/2016 1:30 PM, Dave Hudson via bitcoin-dev wrote:
> The hash rate has jumped up by almost 70% in the last 6 to 7 months and that implies some pretty serious investments by miners who are quite aware of the halving.
There are a few ways in which that information would be irrelevant:
[1.] It is possible that miners expect to breakeven before the halving.
[2.] It is also possible that miners earnestly believe that there will
be no problem -- however:
...  [2a.] This belief may be mistaken.
...  [2b.] Miners may be counting on Core Devs to fix any problems that
come up with anything, this one included.

Also, [3.] many miners believe that the price will increase around the
time of the halving, either for market-microstructure reasons or
marketing reasons. I, personally, think that the price is as likely to
go down as up.

On 3/9/2016 1:30 PM, Dave Hudson via bitcoin-dev wrote:
> These same miners were mining with a coin price around $250 last year so in terms of profitability I'm pretty sure that one around $400 won't be a huge concern.
For some miners, currently it costs $X in electricity per coin mined,
and $400 / 2 is less than X. I do not know how representative this
information is.

Paul



-------------------------------------
Hi Arthur
> 
>     I strongly agree!
>     In crypto we should always follow well-studied open standard rather
>     than custom construction.

I totally agree.
BIP151 does not introduce new cipher types.
The BIP uses ECDH together with ChaCha20-Poly1305@openssh.
Both very well known and broad used crypo.

/jonas


-------------------------------------
On Aug 17, 2016 00:23, "Russell O'Connor via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> If one's goal is to mess with an transaction to prevent it from being
mined, it is more effective to just not relay the transaction rather than
to mess with the witness.  Given two transactions with the same txid and
different witness data, miners and good nodes ought to mine/relay the
version with the lower cost (smaller?) witness data.

That implies that everyone will see both versions and be able to make that
choice. Unfortunately, those two versions will be definition be in conflict
with each other, and thus only one will end up paying a fee. We're can't
relay two transactions for the price of one, or we'd expose the p2p network
to a very cheap DDoS attack: just send increasingly small versions of the
same transaction.

Segwit's third party mallebility protection makes it not an issue for
dependent contracts if transactions are mauled (=apparently the verb
related to malleability), but there are still good reasons for senders not
to gratuitously make their transactions extensible in size or other
resources.

--
Pieter

-------------------------------------

> On Jun 30, 2016, at 2:43 PM, Jonas Schnelli <dev@jonasschnelli.ch> wrote:
> 
>>>> The core problem posed by BIP151 is a MITM attack. The implied solution (BIP151 + authentication) requires that a peer trusts that another is not an attacker.
>>> 
>>> BIP151 would increase the risks for MITM attackers.
>>> What are the benefits for Mallory of he can't be sure Alice and Bob may
>>> know that he is intercepting the channel?
>> 
>> It is not clear to me why you believe an attack on privacy by an anonymous peer is detectable.
> 
> If Mallory has substituted the ephemeral keys in both directions, at the
> point where Alice and Bob will do an authentication, they can be sure
> Mallory is listening.

I understand the mechanics of a tunnel between trusting parties that have a secure side channel. But this assumes that no other peer can connect to these two nodes. How then do they maintain the chain?

The "middle" in this sense does not have to be the wire directly between these two peers. It can be between either of them and any anonymous connection they (must) allow.

Of course this creates pressure to expand their tunnel. Hence the problem of expanding node identity in an effort to preserve privacy. The protection will remain weak until the entire network is "secure". At that point it would necessarily be a private network.

As Pieter rightly observes, there are and always will be tunnels between trusting nodes. Often these are groups of nodes that are in collaboration, so logically they are one node from a system security standpoint. But if people become generally reliant on good node registration, it will become the registrar who controls access to the network. So my concern rests I this proposal becoming widely adopted.

> Simple dummy example:
> 1.) Encryption setup with ECDH with ephemeral keys after BIP151
> 2.) Mallory is MITMling the connection. He is substituting both direction with its own keys
> 3.) Connection is successfully MITMled
> 4.) Alice tells Bob "prove me that you are Bob, please sign the session-ID with your identity key"
> 5.) Bob signs the sessionID (ECDH secret) with his identity key which
> will be unusable for Mallory who has a substituted sessionID in both directions.
> 6.) Alice has successfully detected the Mallory
> 
> Disclaimer: 4) and 5) are _not_ authentication proposals :-)
> 
> </jonas>
> 


-------------------------------------
To say that Bitcoin is strongly consistent is to say that the memory pool
and the last X blocks aren't part of Bitcoin. If you want to avoid making
that claim, you can at best argue that Bitcoin has both a strongly
consistent component AND an eventually consistent component.

The entire point of the definition of eventually consistency is that your
computer system is running continously and DO NOT have a final state, and
therefore you must be able to describe the behavior when your system either
may give responses to queries across time that are either perfectly
consistent *or not* perfectly consistent.

And Bitcoin by default *does not* ignore the contents of the last X blocks.
A Bitcoin node being queried about the current blockchain state WILL give
inconsistent answers when there's block rearrangements = no strong
consistency. Not to mention that your definition ignores the nonzero
probability of a block rearrangement extending beyond your constant omega.

Bitcoin provides a probabilistic, accumulative probability. Not a perfect
one.
Den 2 mar 2016 04:04 skrev "Emin Gün Sirer" <
bitcoin-dev@lists.linuxfoundation.org>:

>
> There seems to be a perception out there that Bitcoin is eventually
> consistent. I wrote this post to describe why this perception is completely
> false.
>
> Bitcoin Guarantees Strong, not Eventual, Consistency
>
> http://hackingdistributed.com/2016/03/01/bitcoin-guarantees-strong-not-eventual-consistency/
>
> I hope we can lay this bad meme to rest. Bitcoin provides a strong
> guarantee.
> - egs
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Is there a link to the IRC discussion?
On Jan 1, 2016 12:49 AM, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> On Tue, Dec 22, 2015 at 05:31:19PM -0800, Peter Todd via bitcoin-dev
wrote:
> > # Summary
>
> Updates from IRC discussion:

Is there a link to the IRC discussion?

-------------------------------------
Open Asset is a simple and well known colored coin protocol made by Flavien
Charlon, which has been around for more than two years ago.
Open Asset is OP_RETURN to store coin's color. Since then, the only
modification to the protocol has been for allowing OA data to be into any
push into an OP_RETURN.

The protocol is here:
https://github.com/OpenAssets/open-assets-protocol/blob/master/specification.mediawiki

I asked to Flavien Charlon if he was OK if I submit the protocol to the
mailing list before posting.

Additional BIP number might be required to cover for example the "colored
address" format:
https://github.com/OpenAssets/open-assets-protocol/blob/master/address-format.mediawiki
But I will do it in a separate request.

Here is the core of the Open Asset specification:

<pre>
  Title: Open Assets Protocol (OAP/1.0)
  Author: Flavien Charlon <flavien@charlon.net>
  Created: 2013-12-12
</pre>

==Abstract==

This document describes a protocol used for storing and transferring
custom, non-native assets on the Blockchain. Assets are represented by
tokens called colored coins.

An issuer would first issue colored coins and associate them with a
formal or informal promise that he will redeem the coins according to
terms he has defined. Colored coins can then be transferred using
transactions that preserve the quantity of every asset.

==Motivation==

In the current Bitcoin implementation, outputs represent a quantity of
Bitcoin, secured by an output script. With the Open Assets Protocol,
outputs can encapsulate a quantity of a user-defined asset on top of
that Bitcoin amount.

There are many applications:

* A company could issue colored coins representing shares. The shares
could then be traded frictionlessly through the Bitcoin
infrastructure.
* A bank could issue colored coins backed by a cash reserve. People
could withdraw and deposit money in colored coins, and trade those, or
use them to pay for goods and services. The Blockchain becomes a
system allowing to transact not only in Bitcoin, but in any currency.
* Locks on cars or houses could be associated with a particular type
of colored coins. The door would only open when presented with a
wallet containing that specific coin.

==Protocol Overview==

Outputs using the Open Assets Protocol to store an asset have two new
characteristics:
* The '''asset ID''' is a 160 bits hash, used to uniquely identify the
asset stored on the output.
* The '''asset quantity''' is an unsigned integer representing how
many units of that asset are stored on the output.

This document describes how the asset ID and asset quantity of an
output are calculated.

Each output in the Blockchain can be either colored or uncolored:
* Uncolored outputs have no asset ID and no asset quantity (they are
both undefined).
* Colored outputs have a strictly positive asset quantity, and a
non-null asset ID.

The ID of an asset is the RIPEMD-160 hash of the SHA-256 hash of the
output script referenced by the first input of the transaction that
initially issued that asset (<code>script_hash =
RIPEMD160(SHA256(script))</code>). An issuer can reissue more of an
already existing asset as long as they retain the private key for that
asset ID. Assets on two different outputs can only be mixed together
if they have the same asset ID.

Like addresses, asset IDs can be represented in base 58. They must use
version byte 23 (115 in TestNet3) when represented in base 58. The
base 58 representation of an asset ID therefore starts with the
character 'A' in MainNet.

The process to generate an asset ID and the matching private key is
described in the following example:
# The issuer first generates a private key:
<code>18E14A7B6A307F426A94F8114701E7C8E774E7F9A47E2C2035DB29A206321725</code>.
# He calculates the corresponding address:
<code>16UwLL9Risc3QfPqBUvKofHmBQ7wMtjvM</code>.
# Next, he builds the Pay-to-PubKey-Hash script associated to that
address: <code>OP_DUP OP_HASH160
010966776006953D5567439E5E39F86A0D273BEE OP_EQUALVERIFY
OP_CHECKSIG</code>.
# The script is hashed: <code>36e0ea8e93eaa0285d641305f4c81e563aa570a2</code>
# Finally, the hash is converted to a base 58 string with checksum
using version byte 23:
<code>ALn3aK1fSuG27N96UGYB1kUYUpGKRhBuBC</code>.

The private key from the first step is required to issue assets
identified by the asset ID
<code>ALn3aK1fSuG27N96UGYB1kUYUpGKRhBuBC</code>. This acts as a
digital signature, and gives the guarantee that nobody else but the
original issuer is able to issue assets identified by this specific
asset ID.

==Open Assets Transactions==

Transactions relevant to the Open Assets Protocol must have a special
output called the marker output. This allows clients to recognize such
transactions. Open Assets transactions can be used to issue new
assets, or transfer ownership of assets.

Transactions that are not recognized as an Open Assets transaction are
considered as having all their outputs uncolored.

===Marker output===

The marker output can have a zero or non-zero value. The marker output
starts with the OP_RETURN opcode, and can be followed by any sequence
of opcodes, but it must contain a PUSHDATA opcode containing a
parsable Open Assets marker payload. If multiple parsable PUSHDATA
opcodes exist in the same output, the first one is used, and the other
ones are ignored.

If multiple valid marker outputs exist in the same transaction, the
first one is used and the other ones are considered as regular
outputs. If no valid marker output exists in the transaction, all
outputs are considered uncolored.

The payload as defined by the Open Assets protocol has the following format:

{|
! Field                !! Description !! Size
|-
! OAP Marker           || A tag indicating that this transaction is an
Open Assets transaction. It is always 0x4f41. || 2 bytes
|-
! Version number       || The major revision number of the Open Assets
Protocol. For this version, it is 1 (0x0100). || 2 bytes
|-
! Asset quantity count || A
[https://en.bitcoin.it/wiki/Protocol_specification#Variable_length_integer
var-integer] representing the number of items in the <code>asset
quantity list</code> field. || 1-9 bytes
|-
! Asset quantity list  || A list of zero or more
[http://en.wikipedia.org/wiki/LEB128 LEB128-encoded] unsigned integers
representing the asset quantity of every output in order (excluding
the marker output). || Variable
|-
! Metadata length      || The
[https://en.bitcoin.it/wiki/Protocol_specification#Variable_length_integer
var-integer] encoded length of the <code>metadata</code> field. || 1-9
bytes
|-
! Metadata             || Arbitrary metadata to be associated with
this transaction. This can be empty. || Variable
|}

Possible formats for the <code>metadata</code> field are outside of
scope of this protocol, and may be described in separate protocol
specifications building on top of this one.

The <code>asset quantity list</code> field is used to determine the
asset quantity of each output. Each integer is encoded using variable
length [http://en.wikipedia.org/wiki/LEB128 LEB128] encoding (also
used in [https://developers.google.com/protocol-buffers/docs/encoding#varints
Google Protocol Buffers]). If the LEB128-encoded asset quantity of any
output exceeds 9 bytes, the marker output is deemed invalid. The
maximum valid asset quantity for an output is 2<sup>63</sup> - 1
units.

If the marker output is malformed, it is considered non-parsable.
Coinbase transactions and transactions with zero inputs cannot have a
valid marker output, even if it would be otherwise considered valid.

If there are less items in the <code>asset quantity list</code> than
the number of colorable outputs (all the outputs except the marker
output), the outputs in excess receive an asset quantity of zero. If
there are more items in the <code>asset quantity list</code> than the
number of colorable outputs, the marker output is deemed invalid. The
marker output is always uncolored.

After the <code>asset quantity list</code> has been used to assign an
asset quantity to every output, asset IDs are assigned to outputs.
Outputs before the marker output are used for asset issuance, and
outputs after the marker output are used for asset transfer.

====Example====

This example illustrates how a marker output is decoded. Assuming the
marker output is output 1:

    Data in the marker output      Description
    -----------------------------
-------------------------------------------------------------------
    0x6a                           The OP_RETURN opcode.
    0x10                           The PUSHDATA opcode for a 16 bytes payload.
    0x4f 0x41                      The Open Assets Protocol tag.
    0x01 0x00                      Version 1 of the protocol.
    0x03                           There are 3 items in the asset quantity list.
    0xac 0x02 0x00 0xe5 0x8e 0x26  The asset quantity list:
                                   - '0xac 0x02' means output 0 has an
asset quantity of 300.
                                   - Output 1 is skipped and has an
asset quantity of 0
                                     because it is the marker output.
                                   - '0x00' means output 2 has an
asset quantity of 0.
                                   - '0xe5 0x8e 0x26' means output 3
has an asset quantity of 624,485.
                                   - Outputs after output 3 (if any)
have an asset quantity of 0.
    0x04                           The metadata is 4 bytes long.
    0x12 0x34 0x56 0x78            Some arbitrary metadata.

===Asset issuance outputs===

All the outputs before the marker output are used for asset issuance.

All outputs preceding the marker output and with a non-zero asset
quantity get assigned the asset ID defined as the RIPEMD-160 hash of
the SHA-256 hash of the output script referenced by the first input of
the transaction. Outputs that have an asset quantity of zero are
uncolored.

===Asset transfer outputs===

All the outputs after the marker output are used for asset transfer.

The asset IDs of those outputs are determined using a method called
order-based coloring.

Inputs are seen as a sequence of asset units, each having an asset ID.
Similarly, outputs are seen as a sequence of asset units to be
assigned an asset ID. These two sequences are built by taking each
input or output in order, each of them adding a number of asset units
equal to their asset quantity. The process starts with the first input
of the transaction and the first output after the marker output.

After the sequences have been built, the asset ID of every asset unit
in the input sequence is assigned to the asset unit at the same
position in the output sequence until all the asset units in the
output sequence have received an asset ID. If there are less asset
units in the input sequence than in the output sequence, the marker
output is considered invalid.

Finally, for each transfer output, if the asset units forming that
output all have the same asset ID, the output gets assigned that asset
ID. If any output is mixing units with more than one distinct asset
ID, the marker output is considered invalid. Outputs with an asset
quantity of zero are always considered uncolored.

===Example===

This is an example of an Open Assets transaction.

The coloring process starts by retrieving the asset quantities and
asset IDs of the outputs referenced by each input of the transaction.
Then, the marker output is identified. In this example, it is output
2, and the <code>asset quantity list</code> field contains the
following values:

    0, 10, 6, 0, 7, 3

This list is used to assign asset quantities to outputs.


    Inputs                          Outputs - Initial state
Outputs - Final result
    =============================   =============================
=============================
    Input 0                         Output 0 (Issuance)
Output 0 (Issuance)
      Asset quantity:     3           Asset quantity:     0
Asset quantity:     <NULL>
      Asset ID:           A1          Asset ID:
Asset ID:           <NULL>
    -----------------------------   -----------------------------
-----------------------------
    Input 1                         Output 1 (Issuance)
Output 1 (Issuance)
      Asset quantity:     2           Asset quantity:     10
Asset quantity:     10
      Asset ID:           A1          Asset ID:
Asset ID:           H
    -----------------------------   -----------------------------
-----------------------------
    Input 2                         Output 2 (Marker)
Output 2 (Marker)
      Asset quantity:     <NULL>      Asset quantity:     <NULL>
Asset quantity:     <NULL>
      Asset ID:           <NULL>      Asset ID:           <NULL>
Asset ID:           <NULL>
    -----------------------------   -----------------------------
-----------------------------
    Input 3                         Output 3 (Transfer)
Output 3 (Transfer)
      Asset quantity:     5           Asset quantity:     6
Asset quantity:     6
      Asset ID:           A1          Asset ID:
Asset ID:           A1
    -----------------------------   -----------------------------
-----------------------------
    Input 4                         Output 4 (Transfer)
Output 4 (Transfer)
      Asset quantity:     3           Asset quantity:     0
Asset quantity:     <NULL>
      Asset ID:           A1          Asset ID:
Asset ID:           <NULL>
    -----------------------------   -----------------------------
-----------------------------
    Input 5                         Output 5 (Transfer)
Output 5 (Transfer)
      Asset quantity:     9           Asset quantity:     7
Asset quantity:     7
      Asset ID:           A2          Asset ID:
Asset ID:           A1
    =============================   -----------------------------
-----------------------------
                                    Output 6 (Transfer)
Output 6 (Transfer)
                                      Asset quantity:     3
Asset quantity:     3
                                      Asset ID:
Asset ID:           A2
                                    =============================
=============================

Outputs are colored from the first to the last. Outputs before the
marker output are issuance outputs:
* Output 0 has an asset quantity of zero, so it is considered uncolored.
* Output 1 gets assigned the asset ID defined by <code>H =
RIPEMD160(SHA256((S))</code> where <code>S</code> is the output script
referenced by the first input of the transaction (input 0).

Output 2 is the marker output, separating issuance outputs from
transfer outputs. The marker output is always uncolored.

Transfer outputs are then colored:
* Output 3 receives 3 units from input 0, 2 units from input 1, 0 unit
from input 2 and 1 unit from input 3. All the 6 units have the same
asset ID <code>A1</code>, so the asset ID <code>A1</code> is assigned
to output 3.
* Output 4 has an asset quantity of zero, so it is considered uncolored.
* Output 5 receives the remaining 4 units of input 3, and 3 units from
input 4. All the 7 units have the same asset ID <code>A1</code>, so
the asset ID <code>A1</code> is assigned to output 5.
* Output 6 receives the first 3 units of input 5. Input 5 has the
asset ID <code>A2</code> so the asset ID <code>A2</code> is assigned
to output 6.

==Rationale==

This approach offers a number of desirable characteristics:

# Economical: The cost of issuing or transferring an asset is
completely independent from the quantity issued or transferred.
# Clients have a way to identify colored outputs simply by traversing
the Blockchain, without needing to be fed external data. Transactions
relevant to the Open Assets Protocol are identified by the special
marker output.
# It is possible to determine the asset ID and asset quantity of an
output by traversing only a limited number of transactions.
# Assets are pseudonymous. They are represented by an asset ID, which
is enough to identify each asset uniquely, while still providing an
adequate level of anonymity for both the issuer and users of the
asset.
# This approach uses the recommended way to embed data in the
Blockchain (OP_RETURN), and therefore does not pollute the UTXO.
# The whole cryptographic infrastructure that Bitcoin provides for
securing the spending of outputs is reused for securing the ability to
issue assets. There is a symmetry between ''an address + private key''
as a way to spend Bitcoins, and ''an address + private key'' as a way
to issue assets.
# Generating a new type of asset is as simple as generating an
address, can be done offline, and for free.
# Reissuing more of an existing asset is easy and can be done quickly
and at no cost (except for the transaction fee) as long as the issuer
retains the private key for the asset ID.
# Single-issuance assets can be achieved by destroying the private key
used to issue the asset immediately after issuing it.
# Since issuance is based on standard Bitcoin output scripts, it is
possible to create an asset that requires multiple signatures for
issuance.

==Compatibility==

For backward compatibility reasons, we consider than an older client
is allowed to see a colored output as uncolored.

===Backward compatibility with existing Bitcoin protocol===

The Open Assets Protocol sits on top of the Bitcoin protocol. It does
not require any change to the existing Bitcoin protocol. Existing
clients that don't support the Open Assets Protocol will see all
outputs as uncolored, and will not be able to perform transfer
transactions.

===Compatibility between different versions of OAP===

New versions with the same major version number (e.g. 1.1) should be
backwards compatible. New versions with a different major version
number (e.g. 2.0) can introduce breaking changes, but transactions
created by newer clients will be identifiable by a different version
number in the output 0 of genesis and transfer transactions.

==Copyright==

This document has been placed in the public domain.


Nicolas Dorier,

-------------------------------------
On Thu, Mar 03, 2016 at 04:04:18PM +0100, Henning Kopp via bitcoin-dev wrote:
> Hi,
> I think there is no need to do a hardfork for this. Rather it should
> be implemented as a safety-mechanism in the client. Perhaps a warning
> can pop up, if one of your conditions A) or B) is met.

Bitcoin Core already implements this safety limit with the "absurd fee"
limit of 10000 * the minimum relay fee. This limit is active in both the
wallet and the sendrawtransaction RPC call. Additionally for the wallet
there is a user configurable -maxtxfee option to limit fees set by the
wallet which currently defaults to 0.1 BTC.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
0000000000000000024eabe5049843ea6d73558e960d6bcead9e91a24cab1161

-------------------------------------
On Wednesday, August 17, 2016 3:02:53 AM Johnson Lau via bitcoin-dev wrote:
> To completely replicate the original behaviour, one may use:
> "DEPTH TOALTSTACK IFDUP DEPTH FROMALTSTACK NUMNOTEQUAL IF 2DROP {if script}
> ELSE DROP {else script} ENDIF"

This is much uglier than expected. IMO if that's the best workaround for the 
current behaviour, people should just use "OP_1 OP_EQUAL OP_IF" when/if they 
need to avoid malleability issues.

I suspect most cases OP_IF would be used, you really want to accept any non-
zero value. For example, the HTLC script I posted on the list about not long 
ago (OP_IF operates on the result from OP_SIZE). Counter-examples would be BIP 
124, the examples in BIP 65 and BIP 112, but I note all of these could be just 
as easily done without the explicit boolean being fed to the OP_IF (you'd need 
an OP_DUP to keep the value, so it wouldn't reduce the byte-size).

Of course, as long as we're talking about a softfork activating together with 
segwit, and only having effect in segwit scripts... there's no reason we can't 
add whatever opcodes we need so long as it gets done before 0.13.1. I suggest 
OP_CASTTOBOOL and OP_DUPASBOOL would be two good candidates if we make OP_IF 
stricter. There's also the possibility of adding an OP_RETAINIF which behaves 
as the current OP_IF, except not popping the conditional value off the stack. 
But perhaps this is getting too complicated for testing in time for segwit...

Luke


-------------------------------------
> Can you elaborate what benefits you would get from the library approach
and how the library API would be different form the proposed URI-scheme?

The main benefit is that you don't need "standard" to solve problem, but
use natural tools in given environment and programming stack. Build a
"standard" on top of URI protocol is a huge limitation, which does not give
any advantage.

We already see issues with dead simple "bitcoin uri" standard, it barely
works in most of bitcoin apps. Think of vague definitions of parameters or
ability to send payment requests over it. HW API would be complicated by an
order of magnitude and I have serious concerns that it will be helpful for
anything. So why complicate things.

> How would the library approach work on mobile platforms? Would USB be
the only supported hardware communication layer?

Interprocess communication/libraries/dependencies on Android are not bound
to specific transport anyhow. Such library could be used by any android
app, and the library would implement proper transports for various
supported vendors. USB for Trezor, NFC for something different etc. If the
point is "make life of app developers easier", let's do this and do not
define artifical "standards".

slush


On Thu, Aug 18, 2016 at 8:54 AM, Jonas Schnelli <dev@jonasschnelli.ch>
wrote:

> Hi
>
> > I fundamentally disagree with the concept of driving signing workflow by
> > the wallet software. Wallet software does not know in advance all data
> > necessary for the signer to do the job. As Jochen mentioned above,
> > Segwit vs Non-segwit use cases are a good example, but there may be many.
>
> I think this is easily solvable. The required data to verify and sign a
> (standard) bitcoin transaction (including P2WSH multi-sig) is manageable.
>
> IMO what a signing devices requires in order to sign a (standard)
> transaction:
> -> serialized tx
> -> serialized tx of the inputs
> -> scriptPubKey of the inputs
> -> inputs redeem-Scripts
> -> input amounts
> -> position of the change output any maybe its keypath
> -> cosigners pubkeys for inputs and changeaddress
>
> This seems to be manageable for a 1 round communication?
> Or do I miss something?
>
>
> > Currently the TREZOR protocol works like device is a server and wallet
> > is a client calling methods on it. It's like: "Sign this for me,
> > please", "Ok, give me this information", "Here it is", "Now I need this
> > another piece".... "There is the signature". Wallet does not know in
> > advance what will go next, and it is for sake of simplicity. I'm quite
> > happy with the protocol so far.
>
> I think multiple rounds would still be possible with a clever design.
> Although I could imaging that >95% of the users transaction would
> require only a single "shot".
>
> Whats the benefits of the multiple rounds communication? Would a single
> round result in to many data transported?
>
> Passing a 300kb chunk (assuming a large transaction) over a URI scheme
> requires a couple of milliseconds on standard Smartphones or PCs.
>
> > Considering the difference in between current hardware, I really don't
> > think it is possible to find any minimal URI-based API good enough for
> > communicating with all vendors. What I see more likely is some 3rd party
> > libraries (JS, C++, Python, ...) defining high-level API and
> > implementing hardware-specific protocols and transports as plugins. That
> > way vendors are not limited by strict standard and application
> > developers and services can integrate wide range of hardware wallets
> > easily. However, this can be done already and we do not need any
> > standardization process (yet).
>
> The URI-based API allows transmitting data of multiple megabytes while
> there is no need for...
> * dependencies of any form (library, etc.)
> * library support for a particular language
> * platform that supports the dependencies of the library (like USBHID,
> not supported by iOS)
>
> Can you elaborate what benefits you would get from the library approach
> and how the library API would be different form the proposed URI-scheme?
>
> How would the library approach work on mobile platforms? Would USB be
> the only supported hardware communication layer?
>
> Thanks
> --
> </jonas>
>
>

-------------------------------------

On Feb 6, 2016, at 9:21 PM, Jannes Faber via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

> They *must* be able to send their customers both coins as separate withdrawals.
> 
Supporting the obsolete chain is unnecessary. Such support has not been offered in any cryptocurrency hard fork before, as far as I know. I do not see why it should start now.
> If not, that amounts to theft of their customers funds.
> 
If they announce their planned behavior before the fork, I do not see any ethical or legal issues.

-------------------------------------
On Tue, Aug 16, 2016 at 3:43 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Tue, Aug 16, 2016 at 07:37:19PM +0000, Luke Dashjr via bitcoin-dev
> wrote:
> > On Tuesday, August 16, 2016 5:53:08 PM Johnson Lau via bitcoin-dev wrote:
> > > A new BIP is prepared to deal with OP_IF and OP_NOTIF malleability in
> > > P2WSH:
> > > https://github.com/jl2012/bips/blob/minimalif/bip-minimalif.mediawiki
> > > https://github.com/bitcoin/bitcoin/pull/8526
> >
> > I am not sure this makes sense. SegWit transactions are already
> non-malleable
> > due to skipping the witness data in calculating the transaction id. What
> is
> > the benefit to this?
>
> SegWit txids aren't malleable, but segwit transactions as a whole still
> are.
> For instance, I could mess with a segwit transaction by replacing part of
> the
> witness that is used as an argument to an OP_IF with a much larger push,
> potentially making the transaction larger, thus making it not get mined
> due to
> the higher fee. There are also potential legal issues if someone replaces a
> push with data where posession in your jurisdiction is illegal.
>

If one's goal is to mess with an transaction to prevent it from being
mined, it is more effective to just not relay the transaction rather than
to mess with the witness.  Given two transactions with the same txid and
different witness data, miners and good nodes ought to mine/relay the
version with the lower cost (smaller?) witness data.

Worries about "illegal data" appearing in the blockchain is not an issue
worth writing a soft-fork over.

There may be good reasons for this BIP, but I don't think the reasons give
above are good.

-------------------------------------
On Wednesday, March 23, 2016 3:24:12 PM Jonas Schnelli via bitcoin-dev wrote:
> I have just PRed a draft version of two BIPs I recently wrote.
> https://github.com/bitcoin/bips/pull/362

In the future, please submit BIP drafts to the mailing list for comment and 
initial peer review before opening a pull request (or requesting a BIP number 
assignment), per BIP 1.

> Each peer that supports p2p authentication, must provide two user editable
> databases (can be a simple record-per-line file).

As long as the format of these databases is not standardised, it seems 
inappropriate to define *any* of this implementation detail in a BIP.

> A peer can send an authenticate message by wrapping the desired message into
> an <code>auth</code>-message-wrapper to the remote peer.

How does a peer know what messages the other peer requires to be 
authenticated?

> 33bytes || identity-pubkey || comp.-pubkey || The identity pubkey of the
> requesting peer

Seems a waste to include this with every single [authenticated] message...

> 8bytes || auth-msg-id || int64 || up-counting auth-msg-id (0 to INT64MAX)

Is this required to persist across connections/restarts/possibly complete 
reinstalls?

Can the same auth-msg-id be used for multiple peers, so a message can be 
signed once and sent to all N peers?

> Responding peers must ignore (banning would lead to fingerprinting) the
> requesting peer after 5 unsuccessfully authentication tries to avoid
> resource attacks.

How does banning in this specific case enable fingerprinting as opposed to any 
other banning?

> The peers should display the identity-pubkey as a identity-address to the
> users, which is a base58-check encoded ripemd160(sha256) hash.

If this is going to become a general-purpose identity system, I think more is 
needed than a simple EC keypair. At the very least, it should probably use a 
HD chain and use a new key for every signature (notice you already have auth-
msg-id to use with this!).

> This proposal is backward compatible. Non supporting peers will ignore the
> <code>auth</code> message.

... and not process it at all? How is that backward compatible?

> Encrypting traffic between peers is already possible with VPN, tor, stunnel,
> curveCP or any other encryption mechanism on a deeper OSI level, however, 
> most mechanism are not practical for SPV or other DHCP/NAT environment and
> will require significant knowhow in how to setup a secure channel.

I don't see how Tor fails this criteria...

> The responding peer will set a session timeout time-interval. The default
> must be 1'800 seconds.

What default? Is the timeout field optional? Why not simply require it?

> This proposal is backward compatible. Non supporting peers will ignore the
> <code>enc*</code> messages.

How should the supporting peer handle the message being ignored?

Luke


-------------------------------------
Luke Dashjr via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:

> This BIP describes a new opcode (OP_CHECKBLOCKATHEIGHT) for the Bitcoin 
> scripting system to address reissuing bitcoin transactions when the coins they 
> spend have been conflicted/double-spent.
>
> https://github.com/luke-jr/bips/blob/bip-cbah/bip-cbah.mediawiki
>
> Does this seem like a good idea/approach?

Prefer a three-arg version (gbits-to-compare, blocknum, hash):
- If <bits> is 0 or > 256, invalid.
- If the hash length is not (<bits> + 7) / 8, invalid.
- If the hash unused bits are not 0, invalid.
- Otherwise <bits> of hash is compared to lower <bits> of blockhash.

This version also lets you play gambling games on-chain!

Or maybe I've just put another nail in CBAH's coffin?

Cheers,
Rusty.


-------------------------------------
That's a valid concern, but I don't see the conflict here. In order to
recover funds from a wallet conforming to BIPXX, you must have wallet
software that handles BIPXX. Simply making BIPXX backwards compatible with
previously created BIP44 or BIP43 purpose 0 wallets doesn't change this at
all.


Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com>

On Fri, May 13, 2016 at 10:57 AM, Pavol Rusnak <stick@satoshilabs.com>
wrote:

> On 13/05/16 18:59, Aaron Voisine wrote:
> > This scheme is independent of the number of accounts. It works with BIP44
> > as well as BIP43 purpose 0, or any other BIP43 purpose/layout. Instead of
> > overloading the account index to indicate the type of address, you use
> the
> > chain index, which is already being used to indicate what the specific
> > address chain is to be used for, i.e. receive vs change addresses.
>
> I see the advantage here. But there is a major problem here.
>
> We came up with BIP44 so a wallet can claim it is BIP44 compatible and
> you can be 100% sure that you can migrate accounts from one wallet
> implementation to another. This was not previously possible when a
> wallet claimed it is BIP32 compatible.
>
> Now we have a similar problem. When there is a BIP44 wallet, does it
> mean it supports segwit or not? For this reason I would like to see
> another BIPXX for segwit, so a wallet can claim it is BIP44, BIP44+BIPXX
> or BIPXX compatible and you'll know what other wallets are compatible
> with it.
>
> --
> Best Regards / S pozdravom,
>
> Pavol "stick" Rusnak
> SatoshiLabs.com
>

-------------------------------------
 t. khan wrote:
> Miners 'gaming' the Block75 system - 
> There is no financial incentive for miners to attempt to game the
> Block75 system. Even if it were attempted and assuming the goal was to
> create bigger blocks, the maximum possible increase would be 25% over
> the previous block size. And, that size would only last for two weeks
> before readjusting down. It would cost them more in transaction fees to
> stuff the network than they could ever make up. To game the system,
> they'd have to game it forever with no possibility of profit.
> 

This is an incentive, if few miners agree to create a large conglomerate
that will ultimately control the network.

You miss something obvious that makes this attack actually free of cost.
Nothing will "cost them more in transaction fees". A miner can create
thousands of transactions paying to himself, and not broadcast them to
the network, but hold them and include them in the blocks he mines. The
fees are collected by him because transactions are included in a block
that he mined and the left amount is in another wallet of the same
person. Repeat this continuously to fill blocks.


> Blocks would get too big - 
> Eventually, blocks would get too big, but only if bandwidth stopped
> increasing and the cost of disk space stopped decreasing. Otherwise, the
> incremental adjustments made by Block75 (especially in combination with
> SegWit) wouldn't break anyone's connection or result in significantly
> more orphaned blocks.
> 

Topology and bandwidth speed / hash rate of the network cannot be
controlled - if we make assumptions about these it might have terrible
consequences.

Even if we take in consideration that bandwidth will only grow and disk
space will only cost less (which is not something we can safely assume,
by the way) the hard limit max. block size cannot grow to unlimited
value (even if the growth happens over time). There is also a validation
cost in time for each block, for the health of the network any node
should be able to download _and_ validate a block, before next block
gets mined.

You said in another post that a permanent solution is preferred, rather
than kicking the can down the road. I fully agree, as well as many
others reading this list, but the permanent solution doesn't necessarily
have to be increasing the max block size dynamically.

If you think about it the other way around, dynamically growing the max
block size is also kicking the can down the road ... just without having
to touch it and get dust on the boot ;)


-------------------------------------
On 21/04/16 17:28, Eric Lombrozo via bitcoin-dev wrote:
> I don't think we've ever had to handle this case. 

This is the main problem: we are not sure, because not a lot of software
does this checks. Also even if you do check, it's hard to handle an
exception (you can't always skip - what if the problematic node is m/44'?).

One of the motivations is to fix BIP-32 so it can be used for
non-secp256k1 curves as well. For NIST P-256 curve this chance is 2^-32.

Jochen even managed to find an example[1]:

m/28578'/33941 where m is derived from
"000102030405060708090a0b0c0d0e0f" seed.

[1]
https://github.com/trezor/trezor-crypto/commit/16ff4387ae79429e629a5454708abf7385b3a9a3

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
I posted this to /r/bitcoin yesterday but it got minimal comments. One uses
suggested I try the mailing list so here it is:

The idea presented here could have the following benefits:

1. Improve mining decentralization
2. Reduce variance in mining profitability
3. Reduce or eliminate SPV mined blocks
4. Reduce or eliminate empty blocks, smoothing out resource usage
5. Reduce or eliminate the latency bottleneck on throughput
6. Make transaction stuffing by miners be either obvious or costly
7. Gives miners something to do while they wait for attractive transactions
to appear
8. Can be easily done with a soft fork

#Basic idea:

Ideally, all miners would begin hashing the next block at exactly the same
time. Miners with a head start are more profitable, and the techniques that
help miners receive and validate blocks quickly create centralization
pressure.

What if there was something that acted like the starting flag at a race,
which could suddenly wave and cause all of the miners to simultaneously
begin hashing the next block?

#Implementation:

Let a sync flag be a message consisting of:

1. Hash of the previous block.
2. Bitcoin address
3. Nonce

This tiny message could propagate through the network at maximum speed. If
miners had to include the hash of this flag in the next block, then all
miners wait for this flag, and when it suddenly spread through the network,
all miners could simultaneously begin hashing the next block.

The sync flag should not be produced too quickly. You want to give everyone
enough time to be ready to hash the next block. Let's say that the hash of
the sync flag is a proof of work that is targeted for 2 minutes.

To fund this proof of work, the protocol is modified to demand that the
block produced 10 blocks after the sync flag must allocate 25% of the block
reward to the address published by the sync flag. In this way, sync flags
are produced in 2 minutes, and blocks are produced in 8 minutes, with 10
minutes total.


Illustration 1: https://s32.postimg.org/wzg0hs8lx/sync_flag.png)

Illustration 2: https://s32.postimg.org/vc5y9yz4l/sync_flag2.png


#Explanation of reasons:

**Improve mining decentralization**

One factor driving centralization is the imperative miners have to achieve
low latency in receiving and validating blocks. To achieve low latency, it
helps a lot if you have expensive low-latency internet connections, curated
network topologies, and large pools that have a plausible chance of finding
consecutive blocks. If miners are expected (or forced) to validate a block
prior to mining on top of it, the rational end game would be to outsource
the validation step to a trusted third party specialist who can choose
optimal locations on the globe to serve their (multiple?) mining pool
clients. These are all less decentralized than the mining situation Satoshi
and others imagined.

**Reduce variance in mining revenue**

Currently, there are about 144 opportunities per day for a pool or solo
miner to see any revenue at all. With sync flags, that number doubles to
288. Sync flags are only worth 25% of what a block is worth, but this still
represents a significant reduction in variance. This variance is one factor
causing solo miners to group into pools, and large pools to be more
attractive than small pools.

**Reduce or eliminate SPV mined blocks**

One way miners have sought to make
full-block-transmission-and-validation-latency irrelevant has been through
"SPV" mining or "Head-first" mining. There is some evidence that these
techniques may be widely used, and that badgering the miners about it is an
ineffective strategy to stop them.

In SPV mining, a miner would simply accept any block header that shows the
correct proof of work. All other validation is entrusted to other miners.
This practice is quite dangerous as the SPV miners can wander off on some
invalid chain, taking SPV nodes with them. If this occurs during a soft
fork, these blind miners can also fool unupgraded fully validating nodes
into following them.

"Head-first" mining means that miners start hashing as soon as they receive
the block header with the correct POW, but they simultaneously validate the
block, and abandon it if is not valid. I consider this to be pretty safe,
as it strictly limits the length of an invalid chain that can result from
mining without validating. However, "Head-first" mining can plausibly
generate 2 or 3 confirmations of an invalid block. It would be nice if such
confirmations did not happen.

The sync flag technique is similar to head-first mining, but rather than
hashing the next block while they wait for transmission and validation of
the prior block, they hash the sync flag. Nodes can differentiate between
sync flags and blocks, and can ignore sync flags when counting
confirmations.

**Reduce or eliminate empty blocks, smoothing out resource usage**

Empty blocks are another consequence of SPV or Headfirst mining, because
the miner cannot safely include any transactions in the block they are
hashing until they have validated the prior block. By delaying the start of
hashing the next block until after validation, miners would not have this
reason to mine empty blocks.

**Reduce or eliminate the latency bottleneck on throughput**

Centralization pressure due to latency issues has been a major
preoccupation over the last year. If latency mattered much less, it could
represent a scalability improvement that could enable higher throughput.

**Make transaction stuffing by miners be either obvious or costly**

Currently, the entire block reward goes to the miner who mines it. One
unfortunate consequence of this is that it does not cost the miner anything
to covertly stuff the block with transactions. These transactions would pay
fees and be indistinguishable from ordinary transactions, but the fees are
paid by the miner and then immediately returned to the miner.

With sync flags, the miner must share these transaction fees with the
address contained in the sync flag 10 blocks prior. This means that if the
miner gives the transactions a normal looking fee, they will incur a cost
that will be paid to the sync flag. If the miner wants to avoid this, they
must give their stuffing transactions a zero fee, which provides evidence
that they are stuffing.

Also, when miners stuff with transactions using a zero fee, they cannot
manipulate the perception of how much fee it takes to get into a block.

Note that miners could still try to covertly stuff blocks that will pay a
sync flag that they themselves created. if this is a big concern, it can be
addressed by forcing blocks to pay multiple sync flags.

**Gives miners something to do while they wait for attractive transactions
to appear**

>From the Montreal scaling workshop last year, we have [this talk](
https://scalingbitcoin.org/montreal2015/presentations/Day1/13-miles-carlsten-Mind-the-Gap.pdf)
which worried that as the block subsidy reduced and transactions became a
more important fraction of miner revenue, it would be rational for miners
to turn off their mining equipment for a "gap" phase after a block is
found, to allow time to pass as more lucrative transactions entered the
mempool.

I don't know whether this will actually happen. The presence of a suitable
backlog of transactions would help prevent this dynamic from emerging. But
if such idling behavior was the optima mining strategy, it could create a
serious vulnerability. Idle hands are the devil's workshop as the saying
goes, and idle miners represent a pool of inert hashpower that is available
to rent for all kinds of destabilizing purposes. It would be better to put
those miners to profitable work mining a sync flag while they wait.

Also, this creates a more efficient price discovery mechanism for
transactions, because you allow transactions paying high fees time to
arrive to the marketplace, rather than take whatever anyone is offering
because all the "good" transactions got gobbled up in the prior block.

**Can be easily done with a soft fork**

Although a hard fork would be more efficient, sync flags could be easily
implemented using a soft fork by introducing the following rule:

Every block must include a transaction which pays 25% of the block reward
to the address given by the 10th previous sync flag, and commits to the
hash of the 1st previous sync flag.

-------------------------------------
I think it's a bad idea to pollute the original idea of this BIP with
other extensions. Other extensions should go to separate BIPs,
especially since methods to clarify the fee have nothing to do with
secure and authenticated bi-directional BIP70 communication.


On 03/10/2016 10:43 PM, James MacWhyte via bitcoin-dev wrote:
> Hi everyone,
> 
> Our BIP (officially proposed on March 1) has tentatively been assigned
> number 75. Also, the title has been changed to "Out of Band Address
> Exchange using Payment Protocol Encryption" to be more accurate.
> 
> We thought it would be good to take this opportunity to add some
> optional fields to the BIP70 paymentDetails message. The new fields are:
> subtractable fee (give permission to the sender to use some of the
> requested amount towards the transaction fee), fee per kb (the minimum
> fee required to be accepted as zeroconf), and replace by fee (whether or
> not a transaction with the RBF flag will be accepted with zeroconf). I
> know it doesn't make much sense for merchants to accept RBF with
> zeroconf, so that last one might be used more to explicitly refuse RBF
> transactions (and allow the automation of choosing a setting based on
> who you are transacting with).
> 
> I see BIP75 as a general modernization of BIP70, so I think it should be
> fine to include these extensions in the new BIP, even though these
> fields are not specific to the features we are proposing. Please take a
> look at the relevant section and let me know if anyone has any concerns:
> https://github.com/techguy613/bips/blob/master/bip-0075.mediawiki#Extending_BIP70_PaymentDetails
> 
> The BIP70 extensions page in our fork has also been updated.
> 
> Thanks!
> 
> James 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 




-------------------------------------
Hi Tom,

On Tue, Sep 20, 2016 at 1:15 PM, Tom via bitcoin-dev <bitcoin-dev@lists.
linuxfoundation.org> wrote:

>
> The OP_CHECKSIG is the most well known and, as its name implies, it
> validates a signature.
> In the new version of 'script' (version 2) the data that is signed is
> changed to be equivalent to the transaction-id. This is a massive
> simplification and also the only change between version 1 and version 2 of
> script.
>

I'm a fan of simplicity too; Unfortunately, your proposal above to change
the semantics of OP_CHECKSIG is too naive.

The SIGHASH data used in both the original Bitcoin script and in Segwit
script contains data indicating which input is being signed.  In Bitcoin
script, the input is being signed is indicated by the input that has a
non-empty scriptSig field.  In the Segwit script, the outpoint
corresponding to the input being signed is explicitly included in the
signature data. By signing only the transaction id, your proposed signature
does not include the data that tells which input of the transaction is
being signed.  Thus if different inputs share the same public key due to
key reuse, then the signatures on those different inputs will be
identical.  Your Flexible Transactions proposal opens up a new line of
attack against Bitcoin that doesn't currently exist.

Consider the following simple example, suppose you and I are jointly
preparing a transaction to mix our coins, or perhaps we are jointly funding
some purchase.  We jointly prepare a transaction with one input from you
and another input from me.  We each sign the transaction and hand the
signature data over to each other so we can produce a completed
transaction.  But oh no! I lied to you. I didn't use my own input to the
transaction.  "My input" was actually the outpoint from one of *your*
transactions; one that has the same public key as the input you have
chosen.  Now I copy your signature you have provided in your input to cover
"my input", which is really your coins.  Surprise, it turns out you are
funding both inputs to our "jointly" funded purchase.  Other protocols are
likely similarly broken by your Flexible Transactions proposal.

I personally rate this flaw as about the same caliber as the transaction
malleability you are trying to fix.  Sure, with enough vigilance, perhaps
you can detect and avoid this trap.  However, it requires a bunch of
unexpected work.  You must always examine every other input to a
transaction you are about to sign to make sure that it isn't one of your
inputs, which means you probably need a copy of the UXTO set to lookup
outpoints, which is a huge burden, especially if you are a hardware
wallet.  If you are not vigilante, your funds may end up stolen. Surely it
is better not to open this line of attack.

For the most part, the SIGHASH works the way it does in Bitcoin for a
reason. You cannot simply throw away the parts you don't understand or
appreciate.  You should take the time to learn why things are the way they
are, and then, only once you are certain that some aspects are not, or no
longer, needed then can you propose removing them.

-------------------------------------
On Wednesday, August 03, 2016 6:16:20 PM Matthew Roberts via bitcoin-dev 
wrote:
> In light of the recent hack: what does everyone think of the idea of
> creating a new address type that has a reversal key and settlement layer
> that can be used to revoke transactions?

This isn't something that makes sense at the address, since it represents the 
recipient and not the sender. Transactions are not sent from addresses ever.

> You could specify so that transactions "sent" from these addresses must
> receive N confirmations before they can't be revoked, after which the
> transaction is "settled" and the coins become redeemable from their
> destination output. A settlement phase would also mean that a transaction's
> progress was publicly visible so transparent fraud prevention and auditing
> would become possible by anyone.

This is already possible. Just nLockTime your withdrawls for some future 
block. Don't sign any transaction that isn't nLockTime'd at least N blocks 
beyond the present tip.

Luke


-------------------------------------
On Sat, Dec 10, 2016 at 9:41 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Saturday, December 10, 2016 9:29:09 PM Tier Nolan via bitcoin-dev wrote:
> > Any new merkle algorithm should use a sum tree for partial validation and
> > fraud proofs.
>
> PR welcome.
>

Fair enough.  It is pretty basic.

https://github.com/luke-jr/bips/pull/2

It sums up sigops, block size, block cost (that is "weight" right?) and
fees.

-------------------------------------
On Wed, May 11, 2016 at 11:01 PM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Secondly, we can probably make the consensus PoW allow blocks to be mined using
> both the existing PoW algorithm, and a very slightly tweaked version where
> implementing AsicBoost gives no advantage. That removes any incentive to
> implement AsicBoost, without making any hardware obsolete

Taking that a step further, the old POW could continue to be accepted
but with a 20% target penalty. (or vice versa, with the new POW having
a 20% target boost.)


-------------------------------------
> Note that "client supplied identification" is being pushed for AML/KYC
> compliance, e.g. Netki's AML/KYC compliance product:
>
>
> http://www.coindesk.com/blockchain-identity-company-netki-launch-ssl-certificate-blockchain/
>
> This is an extremely undesirable feature to be baking into standards given
> it's
> negative impact on fungibility and privacy; we should not be adopting
> standards
> with AML/KYC support, for much the same reasons that the W3C should not be
> standardizing DRM.
>
>
KYC isn't the only use case. There are other situations in which you would
want to confirm who is sending you money. Making it *required* would of
course be a horrible idea, but allowing people to identify themselves, in
many cases with an online-only identity that isn't tied to their real world
identity, will be very useful to newly-developing use cases.

-------------------------------------
On Mon, Oct 17, 2016 at 1:17 PM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> You are asking people to create everyone-can-spend transactions that would
> mean a loss of funds to everyone that used it if we do find a major flaw and
> need to rollback.

Please, nobody is asking for this.
Nobody should produce segwit transactions until the softfork is
activated, after which those transactions aren't anyone-can-spend
anymore.
After activation, nobody can be forced to use the new format
immediately (or ever) if they don't want to reduce their tx fees.
Maybe because they want to be additionally cautious or maybe because
they haven't implemented the new features yet.
Either way, it is fine that some people upgrade later since, as
repeated by many, this is a backward compatible change.


-------------------------------------

To quote:

> HMAC_SHA512(key=ecdh_secret|cipher-type,msg="encryption key").
> 
>  K_1 must be the left 32bytes of the HMAC_SHA512 hash.
>  K_2 must be the right 32bytes of the HMAC_SHA512 hash.

This seems a weak reason to introduce SHA512 to the mix.  Can we just
make:

K_1 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="header encryption key")
K_2 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="body encryption key")

Thanks,
Rusty.


-------------------------------------
On Wed, Aug 10, 2016 at 7:28 PM, Erik Aronesty via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> By sending a public seed,  there's no way for someone to use the transmitted
> address and trace the total amount of payments to it.

Worse. By revealing a public seed, anyone who has seen it (= anyone
who ever pays you through it) can identity all payments to _any_
address derived from that seed.

-- 
Pieter


-------------------------------------
On 9/10/2016 5:41 AM, Johnson Lau via bitcoin-dev wrote:
> 3. After a few months or so, publish the private key.
Why wait a few months? Why not just publish the key a few days after the
final alert?


-------------------------------------
The whole point is in preventing every third party, including miners, from
seeing the details of what is being spent and how.  The burden of
verification is shifted to the owners of the coin (which is fair).

In fact we could have miners recognize spend proofs and check that the same
spend proof is not entered into the blockchain more than once (which would
be a sign of double spend), but it is not required.  The coin owners can
already do that themselves.

2016-08-09 0:41 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:

> Wouldn't you lose the ability to assume transactions in the blockchain are
> verified as valid, since miners can't see the details of what is being
> spent and how? I feel like this ability is bitcoin's greatest asset, and by
> removing it you're creating an altcoin different enough to not be connected
> to/supported by the main bitcoin project.
>
> On Mon, Aug 8, 2016, 09:13 Tony Churyumoff via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Hi Henning,
>>
>> 1. The fees are paid by the enclosing BTC transaction.
>> 2. The hash is encoded into an OP_RETURN.
>>
>> > Regarding the blinding factor, I think you could just use HMAC.
>> How exactly?
>>
>> Tony
>>
>>
>> 2016-08-08 18:47 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:
>>
>>> Hi Tony,
>>>
>>> I see some issues in your protocol.
>>>
>>> 1. How are mining fees handled?
>>>
>>> 2. Assume Alice sends Bob some Coins together with their history and
>>> Bob checks that the history is correct. How does the hash of the txout
>>> find its way into the blockchain?
>>>
>>> Regarding the blinding factor, I think you could just use HMAC.
>>>
>>> All the best
>>> Henning
>>>
>>>
>>> On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via
>>> bitcoin-dev wrote:
>>> > This is a proposal about hiding the entire content of bitcoin
>>> > transactions.  It goes farther than CoinJoin and ring signatures, which
>>> > only obfuscate the transaction graph, and Confidential Transactions,
>>> which
>>> > only hide the amounts.
>>> >
>>> > The central idea of the proposed design is to hide the entire inputs
>>> and
>>> > outputs, and publish only the hash of inputs and outputs in the
>>> > blockchain.  The hash can be published as OP_RETURN.  The plaintext of
>>> > inputs and outputs is sent directly to the payee via a private
>>> message, and
>>> > never goes into the blockchain.  The payee then calculates the hash and
>>> > looks it up in the blockchain to verify that the hash was indeed
>>> published
>>> > by the payer.
>>> >
>>> > Since the plaintext of the transaction is not published to the public
>>> > blockchain, all validation work has to be done only by the user who
>>> > receives the payment.
>>> >
>>> > To protect against double-spends, the payer also has to publish another
>>> > hash, which is the hash of the output being spent.  We’ll call this
>>> hash *spend
>>> > proof*.  Since the spend proof depends solely on the output being
>>> spent,
>>> > any attempt to spend the same output again will produce exactly the
>>> same
>>> > spend proof, and the payee will be able to see that, and will reject
>>> the
>>> > payment.  If there are several outputs consumed by the same
>>> transaction,
>>> > the payer has to publish several spend proofs.
>>> >
>>> > To prove that the outputs being spent are valid, the payer also has to
>>> send
>>> > the plaintexts of the earlier transaction(s) that produced them, then
>>> the
>>> > plaintexts of even earlier transactions that produced the outputs
>>> spent in
>>> > those transactions, and so on, up until the issue (similar to coinbase)
>>> > transactions that created the initial private coins.  Each new owner
>>> of the
>>> > coin will have to store its entire history, and when he spends the
>>> coin, he
>>> > forwards the entire history to the next owner and extends it with his
>>> own
>>> > transaction.
>>> >
>>> > If we apply the existing bitcoin design that allows multiple inputs and
>>> > multiple outputs per transaction, the history of ownership transfers
>>> would
>>> > grow exponentially.  Indeed, if we take any regular bitcoin output and
>>> try
>>> > to track its history back to coinbase, our history will branch every
>>> time
>>> > we see a transaction that has more than one input (which is not
>>> uncommon).
>>> > After such a transaction (remember, we are traveling back in time),
>>> we’ll
>>> > have to track two or more histories, for each respective input.  Those
>>> > histories will branch again, and the total number of history entries
>>> grows
>>> > exponentially.  For example, if every transaction had exactly two
>>> inputs,
>>> > the size of history would grow as 2^N where N is the number of steps
>>> back
>>> > in history.
>>> >
>>> > To avoid such rapid growth of ownership history (which is not only
>>> > inconvenient to move, but also exposes too much private information
>>> about
>>> > previous owners of all the contributing coins), we will require each
>>> > private transaction to have exactly one input (i.e. to consume exactly
>>> one
>>> > previous output).  This means that when we track a coin’s history back
>>> in
>>> > time, it will no longer branch.  It will grow linearly with the number
>>> of
>>> > transfers of ownership.  If a user wants to combine several inputs, he
>>> will
>>> > have to send them as separate private transactions (technically,
>>> several
>>> > OP_RETURNs, which can be included in a single regular bitcoin
>>> transaction).
>>> >
>>> > Thus, we are now forbidding any coin merges but still allowing coin
>>> > splits.  To avoid ultimate splitting into the dust, we will also
>>> require
>>> > that all private coins be issued in one of a small number of
>>> > denominations.  Only integer number of “banknotes” can be transferred,
>>> the
>>> > input and output amounts must therefore be divisible by the
>>> denomination.
>>> > For example, an input of amount 700, denomination 100, can be split
>>> into
>>> > outputs 400 and 300, but not into 450 and 250.  To send a payment, the
>>> > payer has to pick the unspent outputs of the highest denomination
>>> first,
>>> > then the second highest, and so on, like we already do when we pay in
>>> cash.
>>> >
>>> > With fixed denominations and one input per transaction, coin histories
>>> > still grow, but only linearly, which should not be a concern in regard
>>> to
>>> > scalability given that all relevant computing resources still grow
>>> > exponentially.  The histories need to be stored only by the current
>>> owner
>>> > of the coin, not every bitcoin node.  This is a fairer allocation of
>>> > costs.  Regarding privacy, coin histories do expose private
>>> transactions
>>> > (or rather parts thereof, since a typical payment will likely consist
>>> of
>>> > several transactions due to one-input-per-transaction rule) of past
>>> coin
>>> > owners to the future ones, and that exposure grows linearly with time,
>>> but
>>> > it is still much much better than having every transaction immediately
>>> on
>>> > the public blockchain.  Also, the value of this information for
>>> potential
>>> > adversaries arguably decreases with time.
>>> >
>>> > There is one technical nuance that I omitted above to avoid
>>> distraction.
>>> >  Unlike regular bitcoin transactions, every output in a private payment
>>> > must also include a blinding factor, which is just a random string.
>>> When
>>> > the output is spent, the corresponding spend proof will therefore
>>> depend on
>>> > this blinding factor (remember that spend proof is just a hash of the
>>> > output).  Without a blinding factor, it would be feasible to pre-image
>>> the
>>> > spend proof and reveal the output being spent as the search space of
>>> all
>>> > possible outputs is rather small.
>>> >
>>> > To issue the new private coin, one can burn regular BTC by sending it
>>> to
>>> > one of several unspendable bitcoin addresses, one address per
>>> denomination.
>>> >  Burning BTC would entitle one to an equal amount of the new private
>>> coin,
>>> > let’s call it *black bitcoin*, or *BBC*.
>>> >
>>> > Then BBC would be transferred from user to user by:
>>> > 1. creating a private transaction, which consists of one input and
>>> several
>>> > outputs;
>>> > 2. storing the hash of the transaction and the spend proof of the
>>> consumed
>>> > output into the blockchain in an OP_RETURN (the sender pays the
>>> > corresponding fees in regular BTC)
>>> > 3. sending the transaction, together with the history leading to its
>>> input,
>>> > directly to the payee over a private communication channel.  The first
>>> > entry of the history must be a bitcoin transaction that burned BTC to
>>> issue
>>> > an equal amount of BCC.
>>> >
>>> > To verify the payment, the payee:
>>> > 1. makes sure that the amount of the input matches the sum of outputs,
>>> and
>>> > all are divisible by the denomination
>>> > 2. calculates the hash of the private transaction
>>> > 3. looks up an OP_RETURN that includes this hash and is signed by the
>>> > payee.  If there is more than one, the one that comes in the earlier
>>> block
>>> > prevails.
>>> > 4. calculates the spend proof and makes sure that it is included in the
>>> > same OP_RETURN
>>> > 5. makes sure the same spend proof is not included anywhere in the
>>> same or
>>> > earlier blocks (that is, the coin was not spent before).  Only
>>> transactions
>>> > by the same author are searched.
>>> > 6. repeats the same steps for every entry in the history, except the
>>> first
>>> > entry, which should be a valid burning transaction.
>>> >
>>> > To facilitate exchange of private transaction data, the bitcoin network
>>> > protocol can be extended with a new message type.  Unfortunately, it
>>> lacks
>>> > encryption, hence private payments are really private only when
>>> bitcoin is
>>> > used over tor.
>>> >
>>> > There are a few limitations that ought to be mentioned:
>>> > 1. After user A sends a private payment to user B, user A will know
>>> what
>>> > the spend proof is going to be when B decides to spend the coin.
>>> >  Therefore, A will know when the coin was spent by B, but nothing more.
>>> >  Neither the new owner of the coin, nor its future movements will be
>>> known
>>> > to A.
>>> > 2. Over time, larger outputs will likely be split into many smaller
>>> > outputs, whose amounts are not much greater than their denominations.
>>> > You’ll have to combine more inputs to send the same amount.  When you
>>> want
>>> > to send a very large amount that is much greater than the highest
>>> available
>>> > denomination, you’ll have to send a lot of private transactions, your
>>> > bitcoin transaction with so many OP_RETURNs will stand out, and their
>>> > number will roughly indicate the total amount.  This kind of privacy
>>> > leakage, however it applies to a small number of users, is easy to
>>> avoid by
>>> > using multiple addresses and storing a relatively small amount on each
>>> > address.
>>> > 3. Exchanges and large merchants will likely accumulate large coin
>>> > histories.  Although fragmented, far from complete, and likely
>>> outdated, it
>>> > is still something to bear in mind.
>>> >
>>> > No hard or soft fork is required, BBC is just a separate privacy
>>> preserving
>>> > currency on top of bitcoin blockchain, and the same private keys and
>>> > addresses are used for both BBC and the base currency BTC.  Every BCC
>>> > transaction must be enclosed into by a small BTC transaction that
>>> stores
>>> > the OP_RETURNs and pays for the fees.
>>> >
>>> > Are there any flaws in this design?
>>> >
>>> > Originally posted to BCT https://bitcointalk.org/index.
>>> php?topic=1574508.0,
>>> > but got no feedback so far, apparently everybody was consumed with
>>> bitfinex
>>> > drama and now mimblewimble.
>>> >
>>> > Tony
>>>
>>> > _______________________________________________
>>> > bitcoin-dev mailing list
>>> > bitcoin-dev@lists.linuxfoundation.org
>>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>> --
>>> Henning Kopp
>>> Institute of Distributed Systems
>>> Ulm University, Germany
>>>
>>> Office: O27 - 3402
>>> Phone: +49 731 50-24138
>>> Web: http://www.uni-ulm.de/in/vs/~kopp
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>

-------------------------------------
It has been about 1 month since BIP 2 finished receiving comments, so I 
believe it is an appropriate time to begin the process of moving it to Final 
Status. Toward this end, I have opened a pull request:

    https://github.com/bitcoin/bips/pull/350

The current requirement for this is that "the reference implementation is 
complete and accepted by the community". Given the vagueness of this criteria, 
I intend to move forward applying BIP 2's more specific criteria to itself:

> A process BIP may change status from Draft to Active when it achieves rough
> consensus on the mailing list. Such a proposal is said to have rough
> consensus if it has been open to discussion on the development mailing list
> for at least one month, and no person maintains any unaddressed
> substantiated objections to it. Addressed or obstructive objections may be
> ignored/overruled by general agreement that they have been sufficiently
> addressed, but clear reasoning must be given in such circumstances.

Furthermore, there is a reference implementation in the mentioned PR.

Please review the latest draft BIP and provide any objections ASAP.
If there are no outstanding objections on 2016 April 9th, I will consider the 
current draft to have reached rough consensus and update its Status to Final 
by merging the PR.

Thanks,

Luke


-------------------------------------
I think
​the following implementation may be advantageous. It uses the same number
of opcodes, without OP_CAT.

Avoiding use of OP_CAT is still desirable as I think it will be difficult
to agree on semantics for OP_CAT (given necessary measures to prevent
memory abuse) than for OP_LEFT. Another option I would be in support of
would be to have signature flags apply to OP_CHECKSIGFROMSTACK and all
OP_CHECKSIG flags be ignored if they aren't meaningful...

​


























*<signature; SIGHASH_ALL><signatureTxnData>1. <pubkey>
OP_DUP3<pubkey><signature;
SIGHASH_ALL><signatureTxnData><pubkey><signature;
SIGHASH_ALL><signatureTxnData>2.
OP_CHECKSIGVERIFY<signatureTxnData><pubkey><signature;
SIGHASH_ALL><signatureTxnData>3. OP_SHA256 OP_ROT OP_SIZE OP_SUB1
OP_LEFT<signature><sha256(signatureTxnData)><pubkey><signatureTxnData>4.
OP_SWAP OP_ROT OP_CHECKSIGFROMSTACK​VERIFY​ (with same ​argument order​)​*



--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>

On Fri, Nov 4, 2016 at 7:35 AM, Tim Ruffing via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Not a covenant but interesting nevertheless: _One_ of OP_CAT and
> OP_CHECKSIGFROMSTACKVERIFY alone is enough to implement "opt-in miner
> takes double-spend" [1]:
>
> You can create an output, which is spendable by everybody if you ever
> double-spend the output with two different transactions. Then the next
> miner will probably take your money (double-spending against your two
> or more contradicting transactions again).
>
> If you spend such an output, then the recipient may be willing to
> accept a zero-conf transaction, because he knows that you'll lose the
> money when you attempt double-spending (unless you are the lucky
> miner). See the discussion in [1] for details.
>
> The implementation using OP_CHECKSIGFROMSTACKVERIFY is straight-
> forward. You add a case to the script which allows spending if two
> valid signatures on different message under the public key of the
> output are given.
>
> What is less known I think:
> The same functionality can be achieved in a simpler way just using
> OP_CAT, because it's possible to turn Bitcoin's ECDSA to an "opt-in
> one-time signature scheme". With OP_CAT, you can create an output that
> is only spendable using a signature (r,s) with a specific already fixed
> first part r=x_coord(kG). Basically, the creator of this output commits
> on r (and k) already when creating the output. Now, signing two
> different transaction with the same r allows everybody to extract the
> secret key from the two signatures.
>
> The drawbacks of the implementation with OP_CAT is that it's not
> possible to make a distinction between legitimate or illegitimate
> double-spends (yet to be defined) but just every double-spend is
> penalized. Also, it's somewhat hackish and the signer must store k (or
> create it deterministically but that's a good idea anyway).
>
> [1] https://www.mail-archive.com/bitcoin-development@lists.
> sourceforge.net/msg07122.html
>
> Best,
> Tim
>
> On Thu, 2016-11-03 at 07:37 +0000, Daniel Robinson via bitcoin-dev
> wrote:
> > Really cool!
> >
> > How about "poison transactions," the other covenants use case
> > proposed by Möser, Eyal, and Sirer? (I think
> > OP_CHECKSIGFROMSTACKVERIFY will also make it easier to check fraud
> > proofs, the other prerequisite for poison transactions.)
> >
> > Seems a little wasteful to do those two "unnecessary" signature
> > checks, and to have to construct the entire transaction data
> > structure, just to verify a single output in the transaction. Any
> > plans to add more flexible introspection opcodes to Elements, such as
> > OP_CHECKOUTPUTVERIFY?
> >
> > Really minor nit: "Notice that we have appended 0x83 to the end of
> > the transaction data"—should this say "to the end of the signature"?
> >
> > On Thu, Nov 3, 2016 at 12:28 AM Russell O'Connor via bitcoin-dev <bit
> > coin-dev@lists.linuxfoundation.org> wrote:
> > > Right.  There are minor trade-offs to be made with regards to that
> > > design point of OP_CHECKSIGFROMSTACKVERIFY.  Fortunately this
> > > covenant construction isn't sensitive to that choice and can be
> > > made to work with either implementation of
> > > OP_CHECKSIGFROMSTACKVERIFY.
> > >
> > > On Wed, Nov 2, 2016 at 11:35 PM, Johnson Lau <jl2012@xbt.hk> wrote:
> > > > Interesting. I have implemented OP_CHECKSIGFROMSTACKVERIFY in a
> > > > different way from the Elements. Instead of hashing the data on
> > > > stack, I directly put the 32 byte hash to the stack. This should
> > > > be more flexible as not every system are using double-SHA256
> > > >
> > > > https://github.com/jl2012/bitcoin/commits/mast_v3_master
> > > >
> > > >
> > > >
> > > > > On 3 Nov 2016, at 01:30, Russell O'Connor via bitcoin-dev <bitc
> > > > > oin-dev@lists.linuxfoundation.org> wrote:
> > > > >
> > > > > Hi all,
> > > > >
> > > > > It is possible to implement covenants using two script
> > > > > extensions: OP_CAT and OP_CHECKSIGFROMSTACKVERIFY.  Both of
> > > > > these op codes are already available in the Elements Alpha
> > > > > sidechain, so it is possible to construct covenants in Elements
> > > > > Alpha today.  I have detailed how the construction works in a
> > > > > blog post at <https://blockstream.com/2016/11/02/covenants-in-e
> > > > > lements-alpha.html>.  As an example, I've constructed scripts
> > > > > for the Moeser-Eyal-Sirer vault.
> > > > >
> > > > > I'm interested in collecting and implementing other useful
> > > > > covenants, so if people have ideas, please post them.
> > > > >
> > > > > If there are any questions, I'd be happy to answer.
> > > > >
> > > > > --
> > > > > Russell O'Connor
> > > > > _______________________________________________
> > > > > bitcoin-dev mailing list
> > > > > bitcoin-dev@lists.linuxfoundation.org
> > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > >
> > > _______________________________________________
> > > bitcoin-dev mailing list
> > > bitcoin-dev@lists.linuxfoundation.org
> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Hi Peter,

How would a person or exchange decide to accept a payment in BU if it does
not know the gate policy of 51% of the miners?

Suppose that the exchange receives B1,S2,S3,S4 (a big block at height 1,
and 3 small blocks at height 2, 3 and 4), and an alternate chain A1,A2,A3
(three small blocks). The first is the longest, but the second may be the
one 51% of the miners will extend.

Without knowing  the policy of at least 51% of the miners (the maximum
acceptance depth) it's unclear if the exchange has to obey the longest
chain or the chain with higher probability of being extended.
If the maximum acceptance depth of the majority of miners is higher than 6
blocks, accepting a transaction with 6 confirmations is risky.
So BU would set a lower bound on the number of confirmations equal to the
maximum acceptance depth of the majority of miners.But miners do not
publish their acceptance depth, so basically users are clue-less. I think
miners should at least advertise their gate block size and acceptance depth
in their coinbase field.

Is there a game-theoretic analysis of confirmation blocks and their
probabilities in BU ?
Without a detailed analysis, unlimited block size seems a risky change to
Bitcoin, to me.

Regards, Sergio.



On Tue, Nov 22, 2016 at 1:31 PM, Peter R via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Dear all,
>
> Bitcoin Unlimited’s market-based solution to the block-size limit is
> slowly winning support from node operators and miners.  With this increased
> attention, many people are asking for a better explanation of how Bitcoin
> Unlimited actually works.  The article linked below describes how Bitcoin
> Unlimited’s excessive-block logic works from the perspective of a single
> node. (I’m hoping to do a follow-up article that describe how this
> “node-scale” behavior facilitates the emergence of a fluid and organic
> block size limit at the network scale.)
>
> https://medium.com/@peter_r/the-excessive-block-gate-how-
> a-bitcoin-unlimited-node-deals-with-large-blocks-22a4a5c322d4
>
> Best regards,
> Peter R
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On 9/1/2016 9:40 PM, Johnson Lau via bitcoin-dev wrote:
> This BIP will be deployed by "version bits" BIP9 using the same parameters for BIP141 and BIP143, with the name "segwit" and using bit 1.
>

This fix has value outside of segwit.  Why bundle the two together? 
Shouldn't miners have to opportunity to vote on them independently?




-------------------------------------
On Fri, Jul 15, 2016 at 04:08:51PM +0000, Luke Dashjr via bitcoin-dev wrote:
> Daniel Cousens opened the issue a few weeks ago, that BIP 9 should progress to 
> Accepted stage. However, as an informational BIP, it is not entirely clear on 
> whether it falls in the Draft/Accepted/Final classification of proposals 
> requiring implementation, or the Draft/Active classification like process 
> BIPs. Background of this discussion is at:
>     https://github.com/bitcoin/bips/pull/413
> (Discussion on the GitHub BIPs repo is *NOT* recommended, hence bringing this 
> topic to the mailing list)

As of writing the text of BIP68 says:

    'This BIP is to be deployed by "versionbits" BIP9 using bit 0.'

Essentially including BIP9 as part of the BIP68 standard; BIP68 could have
equally been written by including some or all of the text of BIP9. If it had
done that, that text would be part of a "Standard BIP" rather than
"Informational BIP", thus I'll argue that BIP9 should also be a "Standard BIP"

Also, note that if we ever modified BIP9, we'd most likely do so with a new
BIP, and in soft-forks using that new standard, would refer to the new BIP #.

> Reviewing the criteria for status changes, my opinion is that:
> - BIPs 68, 112, 113, and 141 are themselves implementations of BIP 9
> -- therefore, BIP 9 falls under the Draft/Accepted/Final class
> - BIPs 68, 112, and 113 have been deployed to the network successfully
> -- therefore, BIP 9 has satisfied the conditions of not only Accepted status,
>    but also Final status
> -- therefore, BIPs 68, 112, and 113 also ought to be Final status
> 
> If there are no objections, I plan to update the status to Final for BIPs 9, 
> 68, 112, and 113 in one month. Since all four BIPs are currently Draft, I also 
> need at least one author from each BIP to sign-off on promoting them to (and 
> beyond) Accepted.
> 
> BIP   9: Pieter Wuille <pieter.wuille@gmail.com>
>          Peter Todd <pete@petertodd.org>
>          Greg Maxwell <greg@xiph.org>
>          Rusty Russell <rusty@rustcorp.com.au>

ACK "Final" status.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
If somebody is not "running their own validation code"  then they aren't actually using Bitcoin, so their ease in transition is irrelevant. For all they know they are accepting random numbers.

e

> On Oct 16, 2016, at 9:35 AM, Gavin Andresen via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
>> On Sun, Oct 16, 2016 at 10:58 AM, Tom Zander via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> The fallow period sounds waaaay to short. I suggest 2 months at minimum
>> since anyone that wants to be safe needs to upgrade.
> 
> I asked a lot of businesses and individuals how long it would take them to upgrade to a new release over the last year or two.
> 
> Nobody said it would take them more than two weeks.
> 
> If somebody is running their own validation code... then we should assume they're sophisticated enough to figure out how to mitigate any risks associated with segwit activation on their own.
> 
> -- 
> --
> Gavin Andresen
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
Maybe I'm being dense, but I don't see why 2**80 storage is required for
this attack.  Also, I don't see why the attacker ever needs to get the
victim to accept "arbitrary_data".  Perhaps I'm wrong about how the
collision attack works:

   1. Create a script which is perfectly acceptable and would pass the
   sniff test Gavin proposed (no arbitrary_data).
   2. Set off CPU power to construct a second script that lets attacker
   keep his coins and has the same hash. (This is where you get
   "arbitrary_data").
   3. Send a transaction with the first script to the seller as payment.
   4. Wait for the transaction to be included in a block.
   5. Redeem the transaction with the second script, thus stealing the
   coins back.

So the seller would never see the I'd appreciate any correction to my
understanding here.  Where do you need 2**80 storage?  And when does the
seller have to accept "arbitrary_data"?
Thanks!

On Thu, Jan 7, 2016 at 11:19 AM, Adam Back via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> You could say 256 bit ECDSA is overkill lets go to 160 equivalently.
> Saves even more bytes.
>
> The problem with arguing down is where to stop.
>
> As Matt said these things dont degrade gracefully so a best practice
> is to aim for a bit of extra margin.
>
> 256-bit is quite common at this point since AES, SHA256 etc even in
> things with much less at stake than Bitcoin.
>
> You could send the compressed (unhashed) pubkey then there's no hash
> (and omit it from the sig).  Greg had mentioned that in the past.
>
> I think it might be possible to do both (reclaim the hash bits in the
> serialisation of the pub key).
>
> Adam
>
> On 7 January 2016 at 20:02, Gavin Andresen via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > I'm hoisting this from some private feedback I sent on the segregated
> > witness BIP:
> >
> > I said:
> >
> > "I'd also use RIPEMD160(SHA256()) as the hash function and save the 12
> > bytes-- a successful preimage attack against that ain't gonna happen
> before
> > we're all dead. I'm probably being dense, but I just don't see how a
> > collision attack is relevant here."
> >
> > Pieter responded:
> >
> > "The problem case is where someone in a contract setup shows you a
> script,
> > which you accept as being a payment to yourself. An attacker could use a
> > collision attack to construct scripts with identical hashes, only one of
> > which does have the property you want, and steal coins.
> >
> > So you really want collision security, and I don't think 80 bits is
> > something we should encourage for that. Normal pubkey hashes don't have
> that
> > problem, as they can't be constructed to pay to you."
> >
> > ... but I'm unconvinced:
> >
> > "But it is trivial for contract wallets to protect against collision
> > attacks-- if you give me a script that is "gavin_pubkey CHECKSIG
> > arbitrary_data OP_DROP" with "I promise I'm not trying to rip you off,
> just
> > ignore that arbitrary data" a wallet can just refuse. Even more likely, a
> > contract wallet won't even recognize that as a pay-to-gavin transaction.
> >
> > I suppose it could be looking for some form of "gavin_pubkey
> > somebody_else_pubkey CHECKMULTISIG ... with the attacker using
> > somebody_else_pubkey to force the collision, but, again, trivial contract
> > protocol tweaks ("send along a proof you have the private key
> corresponding
> > to the public key" or "everybody pre-commits pubkeys they'll use at
> protocol
> > start") would protect against that.
> >
> > Adding an extra 12 bytes to every segwit to prevent an attack that takes
> > 2^80 computation and 2^80 storage, is unlikely to be a problem in
> practice,
> > and is trivial to protect against is the wrong tradeoff to make."
> >
> > 20 bytes instead of 32 bytes is a savings of almost 40%, which is
> > significant.
> >
> > The general question I'd like to raise on this list is:
> >
> > Should we be worried, today, about collision attacks against RIPEMD160
> (our
> > 160-bit hash)?
> >
> > Mounting a successful brute-force collision attack would require at least
> > O(2^80) CPU, which is kinda-sorta feasible (Pieter pointed out that
> Bitcoin
> > POW has computed more SHA256 hashes than that). But it also requires
> O(2^80)
> > storage, which is utterly infeasible (there is something on the order of
> > 2^35 bytes of storage in the entire world).  Even assuming doubling every
> > single year (faster than Moore's Law), we're four decades away from an
> > attacker with THE ENTIRE WORLD's storage capacity being able to mount a
> > collision attack.
> >
> >
> > References:
> >
> > https://en.wikipedia.org/wiki/Collision_attack
> >
> >
> https://vsatglobalseriesblog.wordpress.com/2013/06/21/in-2013-the-amount-of-data-generated-worldwide-will-reach-four-zettabytes/
> >
> >
> > --
> > --
> > Gavin Andresen
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
Matthew,

You should take a look at OP_DETERMINISTICRANDOM [1] from the Elements
Project.  It aims to achieve a similar goal.

Code is in the `alpha` branch [2].

[1]: https://www.elementsproject.org/elements/opcodes/
[2]:
https://github.com/ElementsProject/elements/blob/alpha/src/script/interpreter.cpp#L1252-L1305

On Fri, May 20, 2016 at 8:29 AM Matthew Roberts via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Good point, to be honest. Maybe there's a better way to combine the block
> hashes like taking the first N bits from each block hash to produce a
> single number but the direction that this is going in doesn't seem ideal.
>
> I just asked a friend about this problem and he mentioned using the hash
> of the proof of work hash as part of the number so you have to throw away a
> valid POW if it doesn't give you the hash you want. I suppose its possible
> to make it infinitely expensive to manipulate the number but I can't think
> of anything better than that for now.
>
> I need to sleep on this for now but let me know if anyone has any better
> ideas.
>
>
>
> On Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012@xbt.hk> wrote:
>
>> Using the hash of multiple blocks does not make it any safer. The miner
>> of the last block always determines the results, by knowing the hashes of
>> all previous blocks.
>>
>>
>> == Security
>>
>> Pay-to-script-hash can be used to protect the details of contracts that
>> use OP_PRANDOM from the prying eyes of miners. However, since there is also
>> a non-zero risk that a participant in a contract may attempt to bribe a
>> miner the inclusion of multiple block hashes as a source of randomness is a
>> must. Every miner would effectively need to be bribed to ensure control
>> over the results of the random numbers, which is already very unlikely. The
>> risk approaches zero as N goes up.
>>
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Fri, Jan 8, 2016 at 10:46 AM, Gavin Andresen <gavinandresen@gmail.com>
wrote:

> And Ethan or Anthony:  can you think of a similar attack scheme if you
> assume we had switched to Schnorr 2-of-2 signatures by then?


Don't answer that, I was being dense again, Anthony's scheme works with
Schnorr...


-- 
--
Gavin Andresen

-------------------------------------
In practice the probability of this case triggering is on the order of 2^-128 or something astronomically tiny. I've been using BIP32 for a few years already as have many others...I don't think we've ever had to handle this case. Justifiably, many app developers feel like the additional complexity of properly handling this case is not worth the effort.

Having said that, if the handling of this case is simple to implement and easy to isolate in the program flow, I am in favor of doing something along the lines of what you propose.

- Eric

On April 20, 2016 9:32:25 AM PDT, Jochen Hoenicke via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>Hello Bitcoin Developers,
>
>I would like to make a proposal to update BIP-32 in a small way.
>
>TL;DR: BIP-32 is hard to use right (due to its requirement to skip
>addresses).  This proposal suggests a modification such that the
>difficulty can be encapsulated in the library.
>
>#MOTIVATION:
>
>The current BIP-32 specifies that if for some node in the hierarchy
>the computed hash I_L is larger or equal to the prime or 0, then the
>node is invalid and should be skipped in the BIP-32 tree.  This has
>several unfortunate consequences:
>
>- All callers of CKDpriv or CKDpub have to check for errors and handle
>  them appropriately.  This shifts the burden to the application
>  developer instead of being able to handle it in the BIP-32 library.
>
>- It is not clear what to do if an intermediate node is
>  missing. E.g. for the default wallet layout, if m/i_H/0 is missing
>  should m/i_H/1 be used for external chain and m/i_H/2 for internal
>  chain?  This would make the wallet handling much more difficult.
>
>- It gets even worse with standards like BIP-44.  If m/44' is missing
>  should we use m/45' instead?  If m/44'/0' is missing should we use
>  m/44'/1' instead, using the same addresses as for testnet?
>  One could also restart with a different seed in this case, but this
>  wouldn't work if one later wants to support another BIP-43 proposal
>  and still keep the same wallet.
>
>I think the first point alone is reason enough to change this.  I am
>not aware of a BIP-32 application that handles errors like this
>correctly in all cases.  It is also very hard to test, since it is
>infeasible to brute-force a BIP-32 key and a path where the node does
>not exists.
>
>This problem can be avoided by repeating the hashing with slightly
>different input data until a valid private key is found.  This would
>be in the same spirit as RFC-6979.  This way, the library will always
>return a valid node for all paths.  Of course, in the case where the
>node is valid according to the current standard the behavior should be
>unchanged.
>
>I think the backward compatibility issues are minimal.  The chance
>that this affects anyone is less than 10^-30.  Even if it happens, it
>would only create some additional addresses (that are not seen if the
>user downgrades).  The main reason for suggesting a change is that we
>want a similar method for different curves where a collision is much
>more likely.
>
>#QUESTIONS:
>
>What is the procedure to update the BIP?  Is it still possible to
>change the existing BIP-32 even though it is marked as final?  Or
>should I make a new BIP for this that obsoletes BIP-32?
>
>What algorithm is preferred? (bike-shedding)  My suggestion:
>
>---
>
>Change the last step of the private -> private derivation functions to:
>
> . In case parse(I_L) >= n or k_i = 0, the procedure is repeated
>   at step 2 with
>    I = HMAC-SHA512(Key = c_par, Data = 0x01 || I_R || ser32(i))
>
>---
>
>I think this suggestion is simple to implement (a bit harder to unit
>test) and the string to hash with HMAC-SHA512 always has the same
>length.  I use I_R, since I_L is obviously not very random if I_L >= n.
>There is a minimal chance that it will lead to an infinite loop if I_R
>is the same in two consecutive iterations, but that has only a chance
>of 1 in 2^512 (if the algorithm is used for different curves that make
>I_L >= n more likely, the chance is still less than 1 in 2^256).  In
>theory, this loop can be avoided by incrementing i in every iteration,
>but this would make an implementation error in the "hard to test" path
>of the program more likely.
>
>The other derivation functions should be updated in a similar matter.
>Also the derivation of the root node from the seed should be updated
>in a similar matter to avoid invalid seeds.
>
>If you followed until here, thanks for reading this long posting.
>
>  Jochen
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev@lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
On 14/05/16 10:16, Jonas Schnelli via bitcoin-dev wrote:
> Importing a bip32 wallet (bip44 or not) is still an expert job IMO.

That's simply not true. All reasonable wallets (reasonable = user
oriented) now use BIP39 mnemonic for doing exactly this.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
On 13/05/16 15:16, Daniel Weigl via bitcoin-dev wrote:
> 2) Define a new derivation path, parallel to Bip44, but a different  'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user choose which account he want to add ("Normal account", "Witness account").  

We had quite a long discussion in our team some time ago and we agreed
on that option #2 is much better and we'd like to implement this way in
myTREZOR.

> 	+) Wallet needs only to take care of 1 address per public key

True, if this BIP only supports P2WPKH.

P2WSH should probably be handled by another account type and another
BIP, anyway.

> Has any Bip44 compliant wallet already done any integration at this point?

We have something in the pipeline, but no visible results yet.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
On Monday, February 01, 2016 10:53:16 PM Luke Dashjr via bitcoin-dev wrote:
> I've completed an initial draft of a BIP that provides clarifications on
> the Status field for BIPs, as well as adding the ability for public
> comments on them, and expanding the list of allowable BIP licenses.

This has moved to:

https://github.com/luke-jr/bips/blob/bip-biprevised/bip-0002.mediawiki

Various changes have been made based on initial input.
Further review and re-review is of course welcome.

Luke


-------------------------------------
Concept ACK. I've been talking about adding this to BIP99 since before
scaling bitcoin Hong Kong, so it will be nice to have a BIP to just point
to. Also I hadn't thought about concurrent deployment of 2 hardforks, nice.

On Feb 4, 2016 23:30, "Gavin Andresen via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> If the worry is full nodes that are not upgraded, then a block with a
negative version number will, indeed, fork them off the the chain, in
exactly the same way a block with new hard-forking consensus rules would.
And with the same consequences (if there is any hashpower not paying
attention, then a worthless minority chain might continue on with the old
rules).

Additionally, a warning or special error could be thrown when a block is
rejected due to the hardfork bit being activated.

> I think a much better idea than this proposed BIP would be a BIP that
recommends that SPV clients to pay attention to block version numbers in
the headers that they download, and warn if there is a soft OR hard fork
that they don't know about.

Although I agree this PR should include such warning/error recommendations,
SPV nodes can't tell whether a change is a hardfork or a softfork just by
looking at the version bits, even in the case of uncontroversial hardforks
deployed with bip9 as recommended by bip99. For controversial hardforks
where bip9 should NOT be used for deployment, setting the hardfork bit is
even more important.

> It is also a very good idea for SPV clients to pay attention to
timestamps in the block headers that the receive, and to warn if blocks
were generated either much slower or faster than statistically likely.
Doing that (as Bitcoin Core already does) will mitigate Sybil attacks in
general.

This seems out of the scope of this PR.

-------------------------------------
On Thursday, February 04, 2016 5:14:49 PM jl2012 via bitcoin-dev wrote:
> ABSTRACT
> 
> This document specifies a proposed change to the semantics of the sign
> bit of the "version" field in Bitcoin block headers, as a mechanism to
> indicate a hardfork is deployed.

Disagree with treating the "version" field as a number, in BIP 9 or this BIP 
which reinterpret it as a bit vector.

> Among the 640 bits in the block header, this is the only one which is
> fixed and serves no purpose, ...

Minor nit (not relevant to actual proposal): This is not true. There are over 
32 other bits (part of the "previous-block" field) which also serve no 
purpose.

> FLAG BLOCK Any planned hardfork must have one and only one flag block
> which is the "point of no return". To ensure monotonicity, flag block
> should be determined by block height, or as the first block with
> GetMedianTimePast() greater than a threshold. Other mechanisms could be
> difficult for SPV nodes to follow. The height/time threshold could be a
> predetermined value or relative to other events (e.g. 10000 blocks / 100
> days after 95% of miner support). The exact mechanism is out of the
> scope of this BIP. No matter what mechanism is used, the threshold is
> consensus critical. It must be publicly verifiable with only blockchain
> data, and preferably SPV-friendly (i.e. verifiable with block headers
> only, without downloading any transaction).

With the current codebase, it is significantly easier to trigger on the block 
timestamp rather than its height or median-time-past. Using either of the 
latter would require refactoring of CBlockIndex. As a hard-fork, even if the 
rules are ineffective for a few blocks following the forking point, using the 
hardfork version bit in this BIP would still ensure a clean break. While I 
agree that median-time-past and height are superior methods that ought to be 
used for hardforks, an emergency hardfork may need to avoid them for 
simplicity, and I don't think they need to be mandated as such in this BIP.

> Although a hardfork is officially deployed when flag block is generated, ...

I would avoid implying the hardfork can be "officially deployed" without 
actual adoption.

> AUTOMATIC WARNING SYSTEM When a flag block for an unknown hardfork is
> found on the network, full nodes and SPV nodes should alert their users
> and/or stop accepting/sending transactions. It should be noted that the
> warning system could become a denial-of-service vector if the attacker
> is willing to give up the block reward. Therefore, the warning may be
> issued only if a few blocks are built on top of the flag block in a
> reasonable time frame. This will in turn increase the risk in case of a
> real planned hardfork so it is up to the wallet programmers to decide
> the optimal strategy. Human warning system (e.g. the emergency alert
> system in Bitcoin Core) could fill the gap.

This seems vulnerable to DoS attacks by rejected hardforks.

> VERSION BITS This proposal is also compatible with the BIP9. The version
> bits mechanism could be employed to measure miner support towards a
> hardfork proposal, and to determine the height or time threshold of the
> flag block. Also, miners of the flag block may still cast votes for
> other concurrent softfork or hardfork proposals as normal.

Rather not imply BIP 9 should be used for hardforks, or that miners have any 
voice in the decision. This is already a serious misconception.

> POINT OF NO RETURN After the flag block is generated, a miner may
> support either the original rules or the new rules, but not both. It is
> not possible for miners in one fork to attack or overtake the other fork
> without giving up the mining reward of their preferred fork.

This is not actually desirable, and would suggest a possible reason *not* to 
comply with this BIP. A legitimate hardfork would never have two continued 
sets of rules for miners to choose from.

Luke


-------------------------------------
Bluetooth exchange of payment requests already has a noticeable lag with 
protocol buffers, so that would be another reason to argue against JSON, 
because JSON is less efficient size wise, correct? I will say that 
although protocol buffers have good platform support, I don't know that 
the documentation for each platform is very good. This is the main 
drawback I see with them. One additional advantage of protocol buffers 
is that the .proto file is a specification, whereas with JSON, you'd 
just have an example file, right?

Isn't keybase a centralized infrastructure? Are you against a blockchain 
based identification? There are a few out there. There is some confusion 
because onename's efforts are breaking away from namecoin though.

I like the idea of PGP signatures of payment requests. This allows for 
manual verification (in my mind, the highest quality) of key 
authenticity (or, with PGP you also have the option to opt into some 
centralized service for key verification). This can be useful when 
dealing with semi-manually issued invoices for goods and services. The 
local bitcoin wallet could just interact with the local PGP keyring. 
Although, one can already just send the payment request in a PGP signed 
e-mail, so I'm not sure if PGP signing is really needed if you're using 
PGP email. The main benefit may just be consolidating/itemizing into 
your bitcoin wallet's transaction history whether the payment 
destination/request was securely received or not. It may also be useful 
for someone to be able to extract a signed payment request from a signed 
PGP e-mail and send it to someone else to make a payment for you (maybe 
you don't want your accounting person to need your entire e-mail 
correspondence with a supplier to be able to just verify the payment 
request and make a payment for your company).

I'm concerned about extending the URI scheme too much. Isn't this going 
to reach the practical size limit of NFC and QR codes pretty quickly?




Andy Schroder

On 06/21/2016 05:43 AM, Andreas Schildbach via bitcoin-dev wrote:
> Protobuf vs. JSON was a deliberate decision. Afaik Protobuf was chosen
> because of its strong types, less vulnerability to malleability and very
> good platform support. Having coded both, I can say Protobuf is not more
> difficult than JSON. (Actually the entire Bitcoin P2P protocol should be
> based on Protobuf, but that's another story.)
>
> Yes, all extensions to BIP70 should go into new BIPs. Note the plural
> here: if you have orthogonal ideas I strongly suggest one BIP per idea
> so they can be discussed and implemented (or rejected) separately.
>
>
> On 06/20/2016 07:33 PM, Erik Aronesty via bitcoin-dev wrote:
>> BIP 0070 has been a a moderate success, however, IMO:
>>
>> - protocol buffers are inappropriate since ease of use and extensibility
>> is desired over the minor gains of efficiency in this protocol.  Not too
>> late to support JSON messages as the standard going forward
>>
>> - problematic reliance on merchant-supplied https (X509) as the sole
>> form of mechant identification.   alternate schemes (dnssec/netki), pgp
>> and possibly keybase seem like good ideas.   personally, i like keybase,
>> since there is no reliance on the existing domain-name system (you can
>> sell with a github id, for example)
>>
>> - missing an optional client supplied identification
>>
>> - lack of basic subscription support
>>
>> /Proposed for subscriptions:/
>>
>> - BIP0047 payment codes are recommended instead of wallet addresses when
>> establishing subscriptions.  Or, merchants can specify replacement
>> addresses in ACK/NACK responses.   UI confirms are /required /when there
>> are no replacement addresses or payment codes used.
>>
>> - Wallets must confirm and store subscriptions, and are responsible for
>> initiating them at the specified interval.
>>
>> - Intervals can /only /be from a preset list: weekly, biweekly, or 1,
>> 2,3,4,6 or 12 months.   Intervals missed by more than 3 days cause
>> suspension until the user re-verifies.
>>
>> - Wallets /may /optionally ask the user whether they want to be notified
>> and confirm every interval - or not.   Wallets that do not ask /must
>> /notify before initiating each payment.   Interval confirmations should
>> begin at /least /1 day in advance of the next payment.
>>
>> /Proposed in general:
>> /
>> - JSON should be used instead of protocol buffers going forward.  Easier
>> to use, explain extend.
>>
>> - "Extendible" URI-like scheme to support multi-mode identity mechanisms
>> on both payment and subscription requests.   Support for keybase://,
>> netki:// and others as alternates to https://.
>>
>> - Support for client as well as merchant multi-mode verification
>>
>> - Ideally, the identity verification URI scheme is somewhat
>> orthogonal/independent of the payment request itself
>>
>> Question:
>>
>> Should this be a new BIP?  I know netki's BIP75 is out there - but I
>> think it's too specific and too reliant on the domain name system.
>>
>> Maybe an identity-protocol-agnostic BIP + solid implementation of a
>> couple major protocols without any mention of payment URI's ... just a
>> way of sending and receiving identity verified messages in general?
>>
>> I would be happy to implement plugins for identity protocols, if anyone
>> thinks this is a good idea.
>>
>> Does anyone think https:// or keybase, or PGP or netki all by
>> themselves, is enough - or is it always better to have an extensible
>> protocol?
>>
>> - Erik Aronesty
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>



-------------------------------------
Thanks for your offer Luke, but we are happy with our own process and,
regardless of historical provenance, see this mailing list and the BIP
process as very Core specific for reasons that are too numerous to describe
here but should be obvious to anyone who has been aware of the last year of
Bitcoin history.

Andrew

On Tue, Mar 8, 2016 at 12:19 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Tuesday, March 08, 2016 2:35:21 AM G. Andrew Stone via bitcoin-dev
> wrote:
> > Not an unreasonable request, however while I personally respect the many
> > great accomplishments of individual engineers loosely affiliated with
> > "Core", Bitcoin Unlimited has our own process for documentation and
> > discussion on an uncensored forum located here:
> > https://bitco.in/forum/threads/buip010-passed-xtreme-thinblocks.774/. We
> > would love to have any interested engineer join us there with ideas and
> > criticisms.
>
> Bitcoin-dev and the BIP process are not affiliated with Core at all. In
> fact,
> the BIP process was created by Amir Taaki, who was a libbitcoin developer
> (libbitcoin is not Core).
>
> I encourage Bitcoin Unlimited to use the BIP process for
> cross-implementation
> standards like this, as do other implementations, so that you can benefit
> from
> peer review from the wider Bitcoin development community, as well as have a
> common repository for these standards.
>
> Many BIPs are discussed on reddit in addition to this mailing list, and you
> would certainly remain free to discuss your own proposals on any forum you
> like - it isn't restricted to only this mailing list.
>
> If this is of interest, I will be happy to try to go over and assign BIP
> numbers to the current (15?) BUIPs assuming they meet the basic
> requirements
> for such assignment (see BIP 1:
> https://github.com/bitcoin/bips/blob/master/bip-0001.mediawiki). Is there
> an
> easy way to get links to each of the BUIPs? I couldn't find BUIP 1 at all,
> for
> example.
>
> Thanks,
>
> Luke
>
>

-------------------------------------
Matthew,

Other than gambling, do you have any specific examples of how this could be
useful?

On Fri, May 20, 2016, 20:34 Johnson Lau via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Using the hash of multiple blocks does not make it any safer. The miner of
> the last block always determines the results, by knowing the hashes of all
> previous blocks.
>
>
> == Security
>
> Pay-to-script-hash can be used to protect the details of contracts that
> use OP_PRANDOM from the prying eyes of miners. However, since there is also
> a non-zero risk that a participant in a contract may attempt to bribe a
> miner the inclusion of multiple block hashes as a source of randomness is a
> must. Every miner would effectively need to be bribed to ensure control
> over the results of the random numbers, which is already very unlikely. The
> risk approaches zero as N goes up.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Hi,

Question if you'll allow me. This is not about Gavin's latest hard fork
proposal but in general about any hard (or soft) fork.

I was surprised to see a period expressed in human time instead of in block
time:

> Blocks with timestamps greater than or equal to the triggering block's
timestamp plus 28 days (60*60*24*28 seconds) shall have the new limits.

But even more so I would expect there to be significant differences in
effects on non-updated clients depending on the moment (expressed as block
number) of applying the new rules. I see a few options, all relating to the
2016 blocks recalibration window.

1) the first block after difficulty adjustment.
2) the last block before the difficulty adjustment.
3) in the middle
4) n blocks before the adjustment with n the smallest number of blocks
calculated such that the adjustment can just manage to do the maximum
possible drop in difficulty.

One of the effects I'm thinking of would be in case of an evil contentious
75-25 hard fork. If that activates at 1) or 2) it will take an awful long
time for the 25% chain to get to 2016 for the next adjustment all the while
having 40 minutes block times. Option 4) sounds a lot better for the
conservative chain. The attacking fork clearly has a choice to make it as
hard as possible for them.

On the other hand when a non-contentious hard fork is rolled out, one could
argue that it's actually best for everyone if the remaining 1% chain
doesn't stand a chance of ever reaching 2016 blocks anymore (not even by a
decent sized attacker trying to double spend on stragglers). Also causing
all alarm bells to go off in the non-updated clients.

Have people thought through all the different scenarios yet?

And would it not make sense to define whatever the best choice is as
mandatory for any hard fork proposal? BIP9? (Realising attackers won't
necessarily follow BIPs anyway.)

Does something like this also play a role for soft forks?

I do realise that it's quite possible for the first few blocks, mined after
the new rules become valid, to still be old style blocks. Thus maybe
defeating the whole planning.

-------------------------------------
Has anyone thought about the effects of the 75% Segregated Witness subsidy
on CoinJoin transactions and CoinJoin-like transactions? Better yet, has
anyone collected data or come up with a methodology for the collection of
data?

>From this link: https://bitcoincore.org/en/2016/01/26/segwit-benefits/

"Segwit improves the situation here by making signature data, which does
not impact the UTXO set size, cost 75% less than data that does impact the
UTXO set size. This is expected to encourage users to favour the use of
transactions that minimise impact on the UTXO set in order to minimise
fees, and to encourage developers to design smart contracts and new
features in a way that will also minimise the impact on the UTXO set."

My expectation from the above is that this will serve as a financial
disincentive against CoinJoin transactions. However, if people have
evidence otherwise, I'd like to hear it.

I noticed jl2012 objected to this characterization here, but has not yet
provided evidence:
https://www.reddit.com/r/Bitcoin/comments/4gyhsj/what_are_the_impacts_of_segwits_75_fee_discount/d2lvxmw

A sample of the 16 transaction id's posted in the JoinMarket thread on
BitcoinTalk shows an average ratio of 1.38 or outputs to inputs:

https://docs.google.com/spreadsheets/d/1p9jZYXxX1HDtKCxTy79Zj5PrQaF20mxbD7BAuz0KC8s/edit?usp=sharing

As we know, a "traditional" CoinJoin transaction creates roughly 2x UTXOs
for everyone 1 it consumes -- 1 spend and 1 change -- unless address reuse
comes into play.

Please refrain from bringing up Schnorr signatures in your reply, since
they are not on any immediate roadmap.

Thanks,
Kristov

-------------------------------------
Hi,

> Does this functionality change peer selection?

This bit will be used for selecting outgoing peers in Bitcoin XT.

On Mon, Mar 7, 2016 at 9:51 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Mon, Mar 7, 2016 at 8:06 PM, G. Andrew Stone via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > The Bitcoin Unlimited client needs a services bit to indicate that the
> node
> > is capable of communicating thin blocks.  We propose to use bit 4 as
> AFAIK
> > bit 3 is already earmarked for Segregated Witness.
>
> Does this functionality change peer selection?  If not, the preferred
> signaling mechanism is probably the one in BIP 130.
>
> Otherwise, I think the standard method for getting numbers has been to
> write a BIP documenting the usage. I don't know if that is intentional
> or just how things have previously happened; and I don't have much of
> an opinion on it.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Thanks, Ethan, that's helpful and I'll stop thinking that collision attacks
require 2^(n/2) memory...

So can we quantify the incremental increase in security of SHA256(SHA256)
over RIPEMD160(SHA256) versus the incremental increase in security of
having a simpler implementation of segwitness?

I'm going to claim that the difference in the first case is very, very,
very small-- the risk of an implementation error caused by having multiple
ways of interpreting the segwitness hash in the scriptPubKey is much, much
greater.

And even if there IS some risk of collision attack now or at some point in
the future, I claim that it is easy for wallets to mitigate that risk. In
fact, the principle of security in depth means wallets that don't
completely control the scriptPubKeys they're creating on behalf of users
SHOULD be coded to mitigate that risk (e.g. not allowing arbitrary data
around a user's public key in a Script so targeted substring attacks are
eliminated entirely).

Purely from a security point of view, I think a single 20-byte segwitness
in the scriptPubKey is the best design.
"Keep the design as simple and small as possible"
https://www.securecoding.cert.org/confluence/plugins/servlet/mobile#content/view/2426

Add in the implied capacity increase of smaller scriptPubKeys and I still
think it is a no-brainer.


On Thu, Jan 7, 2016 at 5:56 PM, Ethan Heilman <eth3rs@gmail.com> wrote:

> >Ethan:  your algorithm will find two arbitrary values that collide. That
> isn't useful as an attack in the context we're talking about here (both of
> those values will be useless as coin destinations with overwhelming
> probability).
>
> I'm not sure exactly the properties you want here and determining
> these properties is not an easy task, but the case is far worse than
> just two random values. For instance: (a). with a small modification
> my algorithm can also find collisions containing targeted substrings,
> (b). length extension attacks are possible with RIPEMD160.
>
> (a). targeted cycles:
>
> target1 = "str to prepend"
> target2 = "str to end with"
>
> seed = {0,1}^160
> x = hash(seed)
>
> for i in 2^80:
> ....x = hash(target1||x||target2)
> x_final = x
>
> y = hash(tartget1||x_final||target2)
>
> for j in 2^80:
> ....if y == x_final:
> ........print "cycle len: "+j
> ........break
> ....y = hash(target1||y||target2)
>
> If a collision is found, the two colliding inputs must both start with
> "str to prepend" and end with the phrase "str to end with". As before
> this only requires 2^81.5 computations and no real memory. For an
> additional 2**80 an adversary has an good change of finding two
> different targeted substrings which collide. Consider the case where
> the attacker mixes the targeted strings with the hash output:
>
> hash("my name is=0x329482039483204324423"+x[1]+", my favorite number
> is="+x) where x[1] is the first bit of x.
>
> (b). length extension attacks
>
> Even if all the adversary can do is create two random values that
> collide, you can append substrings to the input and get collisions.
> Once you find two random values hash(x) = hash(y), you could use a
> length extension attack on RIPEMD-160 to find hash(x||z) = hash(y||z).
>
> Now the bitcoin wiki says:
> "The padding scheme is identical to MD4 using Merkle–Damgård
> strengthening to prevent length extension attacks."[1]
>
> Which is confusing to me because:
>
> 1. MD4 is vulnerable to length extension attacks
> 2. Merkle–Damgård strengthening does not protect against length
> extension: "Indeed, we already pointed out that none of the 64
> variants above can withstand the 'extension' attack on the MAC
> application, even with the Merkle-Damgard strengthening" [2]
> 3. RIPEMD-160 is vulnerable to length extension attacks, is Bitcoin
> using a non-standard version of RIPEMD-160.
>
> RIPEMD160(SHA256()) does not protect against length extension attacks
> on SHA256, but should protect RIPEMD-160 against length extension
> attacks as RIPEMD-160 uses 512-bit message blocks. That being said we
> should be very careful here. Research has been done that shows that
> cascading the same hash function twice is weaker than using HMAC[3]. I
> can't find results on cascading RIPEMD160(SHA256()).
>
> RIPEMD160(SHA256()) seems better than RIPEMD160() though, but security
> should not rest on the notion that an attacker requires 2**80 memory,
> many targeted collision attacks can work without much memory.
>
> [1]: https://en.bitcoin.it/wiki/RIPEMD-160
> [2]: "Merkle-Damgard Revisited: How to Construct a Hash Function"
> https://www.cs.nyu.edu/~puniya/papers/merkle.pdf
> [3]: https://www.cs.nyu.edu/~dodis/ps/h-of-h.pdf
>
> On Thu, Jan 7, 2016 at 4:06 PM, Gavin Andresen via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > Maybe I'm asking this question on the wrong mailing list:
> >
> > Matt/Adam: do you have some reason to think that RIPEMD160 will be broken
> > before SHA256?
> > And do you have some reason to think that they will be so broken that the
> > nested hash construction RIPEMD160(SHA256()) will be vulnerable?
> >
> > Adam: re: "where to stop"  :  I'm suggesting we stop exactly at the
> current
> > status quo, where we use RIPEMD160 for P2SH and P2PKH.
> >
> > Ethan:  your algorithm will find two arbitrary values that collide. That
> > isn't useful as an attack in the context we're talking about here (both
> of
> > those values will be useless as coin destinations with overwhelming
> > probability).
> >
> > Dave: you described a first preimage attack, which is 2**160 cpu time
> and no
> > storage.
> >
> >
> > --
> > --
> > Gavin Andresen
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
>



-- 
--
Gavin Andresen

-------------------------------------
Greg has requested that I take over as the BIP editor responsible for 
assigning BIP numbers. Before I begin, I would like to ensure I have a correct 
record of what has already been assigned or soft-assigned so I don't overlap 
them, as the BIPs repository appears that it may possibly be incomplete.

If you have been assigned (or soft-assigned) a BIP number - or any other 
information that may be relevant to my performing this role, please reply and 
let me know, preferably within the next 24 hours if possible (as there are 
many BIP drafts awaiting assignments).

Getting into some specifics...

- BIP 46 is missing from the repository, but apparently self-soft-assigned by 
Tier Nolan in https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-April/005545.html ; if this was later assigned official, or if he is still 
interested in pursuing this, it seems logical to just keep it at BIP 46.

- BIPs 80 and 81 are currently part of an open pull request 
https://github.com/bitcoin/bips/pull/170, but it is unclear if they were 
formally assigned or not.

- BIP 82 is currently officially assigned and pending in 
https://github.com/bitcoin/bips/pull/171 ; I personally think this is outside 
the scope of BIPs since it does not deal with Bitcoin, and encourage Justus to 
move it to the SLIP standard, but will honour this assignment unless he tells 
me he is moving it. (But understand this will not set a precedent for strictly 
non-Bitcoin things being assigned BIPs...)

- BIP 100 is missing from the repository, and I am uncertain if it was ever 
properly assigned. Considering that the 10x block has mostly been used for 
similar proposals, and BIP 100 is fairly well-established as "BIP 100", it 
seems logical to just make this its official assignment.

- BIP 104 is missing from the repository, but was apparently used unofficially 
by https://drive.google.com/file/d/0BwEbhrQ4ELzBX3hCekFRSUVySWs/view at one 
time. But I do not see an actual specification in this PDF, so as far as I 
know BIP 104 appears to be available?

- BIP 109 was soft-assigned for 
https://gist.github.com/erasmospunk/23040383b7620b525df0, but as this doesn't 
fit with the rest of 10x, I am inclined to give it a new number outside that 
range unless there are objections.

- BIP 122 is missing from the repository, and was self-soft-assigned by Chris 
Priest for "ScaleNet" in https://github.com/bitcoin/bips/pull/222 ; there are 
concerns whether testnets are appropriate for standardisation at all, but 
since it has received sufficient discussion on the mailing list and others 
appear to agree with the effort, it seems reasonable to err in favour of 
assigning it a BIP number (not necessarily 122) if Chris wishes to further 
pursue the idea and add an actual specification to the draft.

To be clear: except for BIPs 82 and 109, and those appearing in the 
https://github.com/bitcoin/bips repository at present, anyone (preferably the 
author, but not necessarily if they are away) aware of any other BIP 
assignments should reply to this message indicating the status of such BIPs 
and their assigned numbers.

Thanks,

Luke


-------------------------------------
Oops, forgot to reply to your last point.

Indeed, we could push for more place by just always having one 0-byte,
but I'm not sure the added complexity helps anything? ASICs can never be
designed which use more extra-nonce-space than what they can reasonably
assume will always be available, so we might as well just set the
maximum number of bytes and let ASIC designers know exactly what they
have available. Currently blocks start with at least 8 0-bytes. We could
just say minimum difficulty is now 6 0-bytes (2**16x harder) and reserve
those? Anyway, someone needs to take a closer look at the midstate
optimization stuff to see what is reasonable required.

Matt


>>> 4) Instead of requiring the first four bytes of the previous block hash
>>> field be 0s, we allow them to contain any value. This allows Bitcoin
>>> mining hardware to reduce the required logic, making it easier to
>>> produce competitive hardware [1].
>>> [1] Simpler here may not be entirely true. There is potential for
>>> optimization if you brute force the SHA256 midstate, but if nothing
>>> else, this will prevent there being a strong incentive to use the
>>> version field as nonce space. This may need more investigation, as we
>>> may wish to just set the minimum difficulty higher so that we can add
>>> more than 4 nonce-bytes.
>>
>> Could you just use leading non-zero bytes of the prevhash as additional
>> nonce?
>>
>> So to work out the actual prev hash, set leading bytes to zero until
>> you hit a zero. Conversely, to add nonce info to a hash, if there are
>> N leading zero bytes, fill up the first N-1 (or less) of them with
>> non-zero values.
>>
>> That would give a little more than 255**(N-1) possible values
>> ((255**N-1)/254) to be exact). That would actually scale automatically
>> with difficulty, and seems easy enough to make use of in an ASIC?


-------------------------------------
I've been asked to post this to this mailing list too. It's time to
clear up some misconceptions floating around about full nodes.

=== Myth: There are only about 5500 full nodes worldwide ===

This number comes from this and similar sites: https://bitnodes.21.co/
and it measured by trying to probe every nodes on their open ports.

Problem is, not all nodes actually have open ports that can be probed.
Either because they are behind firewalls or because their users have
configured them to not listen for connections.

Nobody knows how many full nodes there are, since many people don't know
how to forward ports behind a firewall, and bandwidth can be costly, its
quite likely that the number of nodes with closed ports is at least
another several thousand.

Nodes with open ports are able to upload blocks to new full nodes. In
all other ways they are the same as nodes with closed ports. But because
open-port-nodes can be measured and closed-port-nodes cannot, some
members of the bitcoin community have been mistaken into believing that
open-port-nodes are that matters.

=== Myth: This number of nodes matters and/or is too low. ===

Nodes with open ports are useful to the bitcoin network because they
help bootstrap new nodes by uploading historical blocks, they are a
measure of bandwidth capacity. Right now there is no shortage of
bandwidth capacity, and if there was it could be easily added by renting
cloud servers.

The problem is not bandwidth or connections, but trust, security and
privacy. Let me explain.

Full nodes are able to check that all of bitcoin's rules are being
followed. Rules like following the inflation schedule, no double
spending, no spending of coins that don't belong to the holder of the
private key and all the other rules required to make bitcoin work (e.g.
difficulty)

Full nodes are what make bitcoin trustless. No longer do you have to
trust a financial institution like a bank or paypal, you can simply run
software on your own computer. To put simply, the only node that matters
is the one you use.

=== Myth: There is no incentive to run nodes, the network relies on
altruism ===

It is very much in the individual bitcoin's users rational self interest
to run a full node and use it as their wallet.

Using a full node as your wallet is the only way to know for sure that
none of bitcoin's rules have been broken. Rules like no coins were spent
not belonging to the owner, that no coins were spent twice, that no
inflation happens outside of the schedule and that all the rules needed
to make the system work are followed  (e.g. difficulty.) All other kinds
of wallet involve trusting a third party server.

All these checks done by full nodes also increase the security. There
are many attacks possible against lightweight wallets that do not affect
full node wallets.

This is not just mindless paranoia, there have been real world examples
where full node users were unaffected by turmoil in the rest of the
bitcoin ecosystem. The 4th July 2015 accidental chain fork effected many
kinds of wallets. Here is the wiki page on this event
https://en.bitcoin.it/wiki/July_2015_chain_forks#Wallet_Advice

Notice how updated node software was completely unaffected by the fork.
All other wallets required either extra confirmations or checking that
the third-party institution was running the correct version.

Full nodes wallets are also currently the most private way to use
Bitcoin, with nobody else learning which bitcoin addresses belong to
you. All other lightweight wallets leak information about which
addresses are yours because they must query third-party servers. The
Electrum servers will know which addresses belong to you and can link
them together. Despite bloom filtering, lightweight wallets based on
BitcoinJ do not provide much privacy against nodes who connected
directly to the wallet or wiretappers.

For many use cases, such privacy may not be required. But an important
reason to run a full node and use it as a wallet is to get the full
privacy benefits.

=== Myth: I can just set up a node on a cloud server instance and leave
it ===

To get the benefits of running a full node, you must use it as your
wallet, preferably on hardware you control.

Most people who do this do not use a full node as their wallet.
Unfortunately because Bitcoin has a similar name to Bittorrent, some
people believe that upload capacity is the most important thing for a
healthy network. As I've explained above: bandwidth and connections are
not a problem today, trust, security and privacy are.

=== Myth: Running a full node is not recommended, most people should use
a lightweight client ===

This was common advice in 2012, but since then the full node software
has vastly improved in terms of user experience.

If you cannot spare the disk space to store the blockchain, you can
enable pruning as in:
https://bitcoin.org/en/release/v0.11.0#block-file-pruning. In Bitcoin
Core 0.12, pruning being enabled will leave the wallet enabled.
Altogether this should require less than 1.5GB of hard disk space.

If you cannot spare the bandwidth to upload blocks to other nodes, there
are number of options to reduce or eliminate the bandwidth requirement
found in https://bitcoin.org/en/full-node#reduce-traffic . These include
limiting connections, bandwidth targetting and disabling listening.
Bitcoin Core 0.12 has the new option -blocksonly, where the node will
not download unconfirmed transaction and only download new blocks. This
more than halves the bandwidth usage at the expense of not seeing
unconfirmed transactions.

Synchronizing the blockchain for a new node has improved since 2012 too.
Features like headers-first
(https://bitcoin.org/en/release/v0.10.0#faster-synchronization) and
libsecp256k1 have greatly improved the initial synchronization time.

It can be further improved by setting -dbcache=6000 which keeps more of
the UTXO set in memory. It reduces the amount of time reading from disk
and therefore speeds up synchronization. Tests showed that the entire
blockchain can now be synchronized in less than _3 and a half hours_
(See
https://github.com/bitcoin/bitcoin/pull/6954#issuecomment-154993958)
Note that you'll need Bitcoin Core 0.12 or later to get all these
efficiency improvements.

=== How to run a full node as your wallet ===

I think every moderate user of bitcoin would benefit by running a full
node and using it as their wallet. There are several ways to do this.

* Run a bitcoin-qt full node (https://bitcoin.org/en/download).

* Use wallet software that is backed by a full node e.g. Armory
(https://bitcoinarmory.com/) or JoinMarket
(https://github.com/AdamISZ/JMBinary/#jmbinary)

* Use a lightweight wallet that connects only to your full node (e.g.
Multibit connecting only to your node running at home, Electrum
connecting only to your own Electrum server)

So what are you waiting for? The benefits are many, the downsides are
not that bad. The more people do this, the more robust and healthy the
bitcoin ecosystem is.





-------------------------------------
Alternatively scenario: it will cause a sudden increase of Bitcoin mines
in countries where the algorithm is not patented, possibly causing a
geographical decentralization of miners from countries that already have
a lot of miners like China (if it is patented in China).

On 01/04/16 10:00, Peter Todd via bitcoin-dev wrote:
> On Thu, Mar 31, 2016 at 09:41:40PM -0700, Timo Hanke via bitcoin-dev wrote:
>> Hi.
>>
>> I'd like to announce a white paper that describes a very new and
>> significant algorithmic improvement to the Bitcoin mining process which has
>> never been discussed in public before. The white paper can be found here:
>>
>> http://www.math.rwth-aachen.de/~Timo.Hanke/AsicBoostWhitepaperrev5.pdf
> What steps are you going to take to make sure that this improvement is
> available to all ASIC designers/mfgs on a equal opportunity basis?
>
> The fact that you've chosen to patent this improvement could be a
> centralization concern depending on the licensing model used. For example, one
> could imagine a licensing model that gave one manufacture exclusive rights.
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
On May 19, 2016 01:53, "Peter Todd" <pete@petertodd.org> wrote:
tip of the tree.
> >
> > How expensive it is to update a leaf from this tree from unspent to
spent?
>
> log2(n) operations.

Updating a leaf is just as expensive as adding a new one?
That's not what I expected.
Or is adding a new one O (1) ?

Anyway, thanks, I'll read this in more detail.

> > Wouldn't it be better to have both an append-only TXO and an append-only
> > STXO (with all spent outputs, not only the latest ones like in your
"STXO")?
>
> Nope. The reason why this doesn't work is apparent when you ask how will
the
> STXO be indexed?

Just the same way the TXO is (you just stop updating the txo leafs from
unspent to spent.

> If it's indexed by outpoint - that is H(txid:n) - to update the STXO you
need
> he entire thing, as the position of any new STXO that you need to add to
the
> STXO tree is random.
>
> OTOH, if you index the STXO by txout creation order, with the first txout
ever
> created having position #0, the second #1, etc. the data you may need to
update
> the STXO later has predictable locality... but now you have something
that's
> basically identical to my proposed insertion-ordered TXO commitment
anyway.

Yeah, that's what I want. Like your append only TXO but for STXO (that way
we avoid ever updating leafs in the TXO, and I suspect there are other
advantages for fraud proofs).

> Incidentally, it's interesting how if a merbinner tree is insertion-order
> indexed you end up with a datastructure that's almost identical to a MMR.

No complain with MMR. My point is having 2 of them separated: one for the
TXO (entries unmutable) and one for the STXO (again, entries unmutable).

Maybe it doesn't make sense, but I would like to understand why.

-------------------------------------
On Tuesday, February 02, 2016 5:16:30 PM Pieter Wuille via bitcoin-dev wrote:
> On Feb 2, 2016 18:04, "Peter Todd via bitcoin-dev" <
> 
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> > On Tue, Jan 26, 2016 at 09:44:48AM -0800, Toby Padilla via bitcoin-dev
> 
> wrote:
> > > I really don't like the idea of policing other people's use of the
> > > protocol. If a transaction pays its fee and has a greater than dust
> 
> value,
> 
> > > it makes no sense to object to it.
> > 
> > I'll point out that getting a BIP for a feature is *not* a hard
> > requirement for deployment. I'd encourage you to go write up your BIP
> > document, give it a non-numerical name for ease of reference, and lobby
> > wallet vendors to implement it.
> > 
> > While I'll refrain from commenting on whether or not I think the feature
> > itself is a good idea, I really don't want people to get the impression
> > that we're gatekeepers for how people choose use Bitcoin.
> 
> I'll go further: whatever people have commented here and elsewhere about
> this feature (myself included) are personal opinions on the feature itself,
> in the hope you take the concerns into account.
> 
> These comments are not a judgement on whether this should be accepted as a
> BIP. Specifically, the BIP editor should accept a BIP even if he personally
> dislikes the ideas in it, when the criteria are satisfied.
> 
> Beyond that, having a BIP accepted does not mean wallets have to implement
> it. That's up to the individual wallet authors/maintainers.

Agree with both Peter and Pieter. Note that BIP 74 was assigned to this 
proposal last Friday.

Luke


-------------------------------------
> But I would argue that in this scenario, the only way it
> would become invalid is the equivalent of a double-spend... and therefore
> it
> may be acceptable in relation to this argument.
>

The values returned by OP_COUNT_ACKS vary in their exact value depending on
which block this transaction ends up in.  While the proposed use of this
operation is somewhat less objectionable (although still objectionable to
me), nothing stops users from using OP_EQUALVERIFY and and causing their
transaction fluctuate between acceptable and unacceptable, with no party
doing anything like a double spend.  This is a major problem with the
proposal.

-------------------------------------
On Tue, Jul 19, 2016 at 10:35:39PM -0600, Sean Bowe via bitcoin-dev wrote:
> I'm requesting feedback for Hash Time-Locked Contract (HTLC) transactions
> in Bitcoin.
> 
> HTLC transactions allow you to pay for the preimage of a hash. CSV/CLTV can
> be used to recover your funds if the other party is not cooperative. These
> scripts take the following general form:
> 
>     [HASHOP] <digest> OP_EQUAL
>     OP_IF
>         <seller pubkey>
>     OP_ELSE
>         <num> [TIMEOUTOP] OP_DROP <buyer pubkey>
>     OP_ENDIF
>     OP_CHECKSIG

Note that because you're hashing the top item on the stack regardless
scriptSig's that satisfy HTLC's are malleable: that top stack item can be
changed anything in the digest-not-provided case and the script still passes.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
I've revived BIP 2 (from Deferred Status) and given it some updates. Most 
notably, I have reworked it to be a *replacement* for BIP 1 rather than an 
addendum.

https://github.com/luke-jr/bips/blob/bip0002_squash/bip-0002.mediawiki

Please review it. If things go well, hopefully we can get this done by 
Christmas. ;)

Other recent changes include:
* OPL will no longer be an acceptable license. Many in the community feel that 
prohibiting publication is unacceptable for BIPs, and I haven't heard any 
arguments in favour of allowing it.
* Accepted Status has been renamed to Proposed. The name "Accepted" seems a 
constant source of confusion since it requires only action from the author.
* Non-image auxiliary files are permitted in the bip-XXXX subdirectory. This 
was already the norm despite BIP 1.
* Email addresses are now required for authors. The Travis script has been 
enforcing this for months now already.
* The Post-History header may be provided as a link instead of a simple date. 
A few BIPs were already doing this.
* Markdown format is no longer permitted for BIPs. I don't see the point in 
allowing multiple formats, and so far we've been fine with just MediaWiki.
* The Resolution header has been dropped, as it is not applicable to a 
decentralised system where no authority exists to make final decisions.

Other changes already in the previous draft of BIP 2:
* An implementation is now required (when applicable) before BIPs can proceed 
to Proposed Status.
* BIP Comments are newly introduced.
* The License preamble headers have been added.

Thanks,

Luke


-------------------------------------



>
>     Only large merchants are able to maintain such an infrastructure;
>     (even
>     Coinbase recently failed at it, they forgot to update their
>     certificate). For end users that is completely unpractical.
>
>
> Payment protocol is for when you buy stuff from purse.io 
> <http://purse.io>, not really needed for face-to face transfers, end 
> users, IMO.



I disagree with your statements. There are many face to face use cases 
where the payment protocol is essential. Pretty much anything where the 
payee's hardware device that the payer interacts with is automated in 
public and/or operated or accessible by untrusted employees. In any of 
those cases the software on the payee's hardware device can be modified. 
Providing a signed payment request gives the payer additional confidence 
that they are paying the correct person.

See some examples here: http://andyschroder.com/BitcoinFluidDispenser/2.3/


There was a secure bluetooth protocol that Andreas Schildbach and Eric 
Voskuil and I were working on, but we never pulled it all the way 
together. This would also need a two way exchange for a face to face 
payment. This could be used without using some sort of key/certificate 
verification service if being done between two humans who are the direct 
senders and receivers of the payment and are using hardware that they 
personally own (not necessarily the case of untrusted employees or 
public vulnerable machines).




>     The same benefit can be achieved without the complexity of BIP70, by
>     extending the Bitcoin URI scheme. The requestor is authenticated using
>     DNSSEC, and the payment request is signed using an EC private key. A
>     domain name and an EC signature are short enough to fit in a
>     Bitcoin URI
>     and to be shared by QR code or SMS text.
>
>      bitcoin:address?amount=xx&message=yyy&name=john.example.com
>     <http://john.example.com>&sig=zzz
>
>
> I agree.  A TXT record at that name could contain the pubkey.


Did you not see my previous message about the size of the bitcoin: URI 
getting too big for NFC and QR codes? Do you not care about giving the 
payer the option of using multiple destination payment addresses? This 
is important for many reasons.


>     That extension is sufficient to provide authenticated requests,
>     without
>     requiring a https server. The signed data can be serialized from the
>     URI, and DNSSEC verification succeeds without requesting extra
>     data from
>     the requestor. The only assumption is that the verifier is able to
>     make
>     DNS requests.
>
>
> The problem is that there's no way for a merchant to /refuse /a 
> payment without a direct communication with the merchant's server.    
> Verify first / clear later is the rule.   Check stock, ensure you can 
> deliver, and clear the payment on the way out the door.

So, are you saying first the payer should send an unsigned transaction 
for review, and then once the payee has agreed it's good, they can send 
an ACK message back and then wait for the signed version? I don't think 
this is a bad option to have. Many wallets simultaneously broadcast a 
signed transaction to their peers and and also back to the payee via 
https or bluetooth. So, you'd have to add another step to do the 
unsigned transaction review in order to avoid a transaction being 
accidentally broadcast that both parties don't like.


>
> Also, as a merchant processing monthly subscriptions, you don't want 
> the first time you hear about a user's payment to be /after /it hits 
> the blockchain.  You could add a refund address to deal with it after 
> the fact... stuff a refund address int OP_RETURN somehow?
>
> bitcoin:address?amount=xx&currency=ccc&message=yyy&name=john.example.com 
> <http://john.example.com>&offset=3d&interval=1m&sig=zzz

Again, my comments above about issues with using bitcoin: URI for 
everything. Also, why do you want to bloat the blockchain with 
unnecessary refund transaction data?



-------------------------------------
First of all, and most importantly, I like the idea/concept.

The first issue I see is that this scheme exposes private information in the 
form of which inputs/outputs are related to the user. But IMO this information 
should also be private and kept encrypted, so memo servers don't have anything 
at all to leak. 

Note this necessarily means you can't reuse the keys for the blockchain UTXOs 
for memos. But such key reuse is also a risk that should be avoided anyway. 
Instead, I suggest encrypting all the memos to an arbitrary key which is 
derived from the HD seed and shouldn't ever be used for UTXOs.

Ideally, the memo server shouldn't be tied to a specific wallet schema. So the 
next step is to not tell the memo server anything except your memo-specific 
identifier (which can be a hash of a pubkey, or really anything at all - 
there's no reason the memo server needs to know ANYTHING about the user's 
wallet). Using an arbitrary identifier of sufficient length allows for future 
wallet schemas to continue to use the same memo servers. (The specifics on how 
to derive the identifier can be specified in a separate BIP to ensure wallets 
can be compatible with each other.)

I don't think there is a real need for memo servers to sync data. It should be 
sufficient for users to decide on two or more memo servers they wish to 
entrust their memos with, or possibly trust only their own memo server(s).

There should probably also be a way for memos of different types. Some wallets 
might only support simple memos, but others might associate more data for (eg) 
proof-of-existence schemas. What types are used *might* be desirable to 
encrypt as well, so this should probably be in the second "how wallets use it" 
BIP.

IIRC, Electrum already has some kind of "memo server" interface in a plugin. 
Have you looked at how it works, and considered its features (and/or flaws) 
for your proposal?

Finally, using "?data&data&data" doesn't follow the standard 
"?key=value&key=value" scheme; simple to fix.

Luke

On Thursday, June 02, 2016 12:21:54 AM Chris Priest via bitcoin-dev wrote:
> I'm currently working on a wallet called multiexplorer. You can check
> it at https://multiexplorer.com/wallet
> 
> It supports all the BIPs, including the ones that lets you export and
> import based on a 12 word mnemonic. This lets you easily import
> addresses from one wallet to the next. For instance, you can
> copy+paste your 12 word mnemonic from Coinbase CoPay into
> Multiexplorer wallet and all of your address and transaction history
> is imported (except CoPay doesn't support altcoins, so it will just be
> your BTC balance that shows up). Its actually pretty cool, but not
> everything is transferred over.
> 
> For instance, some people like to add little notes such as "paid sally
> for lunch at Taco Bell", or "Paid rent" to each transaction they make
> through their wallet's UI. When you export and import into another
> wallet these memos are lost, as there is no way for this data to be
> encoded into the mnemonic.
> 
> For my next project, I want to make a stand alone system for archiving
> and serving these memos. After it's been built and every wallet
> supports the system, you should be able to move from one wallet by
> just copy+pasting the mnemonic into the next wallet without losing
> your memos. This will make it easier for people to move off of old
> wallets that may not be safe anymore, to more modern wallets with
> better security features. Some people may want to switch wallets, but
> since its much harder to backup memos, people may feel stuck using a
> certain wallet. This is bad because it creates lock in.
> 
> I wrote up some details of how the system will work:
> 
> https://github.com/priestc/bips/blob/master/memo-server.mediawiki
> 
> Basically the memos are encrypted and then sent to a server where the
> memo is stored. An API exists that allows wallets to get the memos
> through an HTTPS interface. There isn't one single memo server, but
> multiple memo servers all ran by different people. These memo servers
> share data amongst each other through a sync process.
> 
> The specifics of how the memos will be encrypted have not been set in
> stone yet. The memos will be publicly propagated, so it is important
> that they are encrypted strongly. I'm not a cryptography expert, so
> someone else has to decide on the scheme that is appropriate.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
At the 3rd Bitcoin Workshop being held in conjunction with the Financial
Cryptography Conference in Barbados, my group will be presenting a new idea
for improving Bitcoin wallet security and deterring thefts today.

The write-up is here:

http://hackingdistributed.com/2016/02/26/how-to-implement-secure-bitcoin-vaults/

The paper with the nitty gritty details is here:
    http://fc16.ifca.ai/bitcoin/papers/MES16.pdf

The core idea:

Our paper describes a way to create vaults, special accounts whose keys can
be neutralized if they fall into the hands of attackers. Vaults are
Bitcoin’s decentralized version of you calling your bank to report a stolen
credit card -- it renders the attacker’s transactions null and void. And
here’s the interesting part: in so doing, vaults demotivate key theft in
the first place. An attacker who knows that he will not be able to get away
with theft is less likely to attack in the first place, compared to current
Bitcoin attackers who are guaranteed that their hacking efforts will be
handsomely rewarded.

Operationally, the idea is simple. You send your money to a vault address
that you yourself create. Every vault address has a vault key and a
recovery key. When spending money from the vault address with the
corresponding vault key, you must wait for a predefined amount of time
(called the unvaulting period) that you established at the time you created
the vault -- say, 24 hours. When all goes well, your vault funds are
unlocked after the unvaulting period and you can move them to a standard
address and subsequently spend them in the usual way. Now, in case Harry
the Hacker gets a hold of your vault key, you have 24 hours to revert any
transaction issued by Harry, using the recovery key. His theft,
essentially, gets undone, and the funds are diverted unilaterally to their
rightful owner. It’s like an “undo” facility that the modern banking world
relies on, but for Bitcoin.

The technical trick relies on a single new opcode, CheckOutputVerify, that
checks the shape of a redeem transaction. Note that fungibility is not
affected, as the restrictions are at the discretion of the coin owner alone
and can only be placed by the coin owner ahead of time.

We suspect that this modest change could actually be a game-changer for
bitcoin security: clients and keys are notoriously hard to secure, and a
facility that allows you to possibly recover, and if not, permanently keep
the hacker from acquiring your funds, could greatly deter Bitcoin thefts.

As always, comments and suggestions are welcome.
- egs, Ittay Eyal and Malte Moeser.

-------------------------------------
Signed by the key pair that was referenced in the output of the on-chain
transaction? (Bob in my example, actually) Doesn't that mean it's easy to
follow who is paying whom, you just can't see how much is going to reach
recipient?

On Tue, Aug 9, 2016, 04:40 Tony Churyumoff <tony991@gmail.com> wrote:

> This troll is harmless.  A duplicate spend proof should also be signed
> by the same user (Alice, in your example) to be considered a double
> spend.
>
> 2016-08-09 3:18 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
> > One more thought about why verification by miners may be needed.
> >
> > Let's say Alice sends Bob a transaction, generating output C.
> >
> > A troll, named Timothy, broadcasts a transaction with a random hash,
> > referencing C's output as its spend proof. The miners can't tell if it's
> > valid or not, and so they include the transaction in a block. Now Bob's
> > money is useless, because everyone can see the spend proof referenced and
> > thinks it has already been spent, even though the transaction that
> claims it
> > isn't valid.
> >
> > Did I miss something that protects against this?
> >
>

-------------------------------------
Hi Tom

> I think you misunderstand tagged systems at a very basic level.  You think 
> that html can only use a bold tag <b> once in a document? Thats equivalent 
> to what you are saying.

Would the "additional" segment contain the same amount of
nSequence-equivalent token as the number of inputs in the "inputs" segment?
What if you only want to add a per-input-token in the additional segment
for a certain input (assume last input)?
I guess the fundamental difference to html is the possible nesting.

However, I think that should be mentioned/specified in the BIP.

</jonas>


-------------------------------------
On Tue, Jun 21, 2016 at 05:14:31PM -0700, Justin Newton wrote:
> On Tue, Jun 21, 2016 at 3:13 PM, Peter Todd via bitcoin-dev <
> Hi Peter,
>    Certainly AML/KYC compliance is one of the use cases that BIP 75 and our
> certificates can support.  As a quick summary,
> 
> There are individuals and entities that would like to buy, sell, and use
> bitcoin, and other public blockchains, but that have compliance
> requirements that they need to meet before they can do so.  Similarly,
> companies and entrepreneurs in the space suffer under the potential threat
> of fines, or in extreme cases, jail time, also for not meeting AML or
> sanctions list compliance.  We wanted to build tools that allowed
> entrepreneurs to breathe easy, while at the same time allow more people and
> companies to enter the ecosystem.  We also believe that the solution we are
> using has the characteristics that you want in such a solution, for example:
> 
> 1> Only the counterparties (and possibly their service providers in the
> case of hosted services) in a transaction can see the identity data,
> protecting user privacy.
> 
> 2> The counterparties themselves (and possibly their service providers in
> the case of hosted services) decide whether identity information is
> required for any given transaction.
> 
> 3> No trace is left on the blockchain or anywhere else (other than with the
> counterparties) that identity information was even exchanged, protecting
> fungibility
> 
> 4> The solution is based on open source and open standards, allowing open
> permissionless innovation, versus parties building closed networks based on
> closed standards.  The very fact that this solution went through the BIP
> process and was adapted based on feedback is an example of how this is
> better for users than the inevitable closed solution that would arise if
> the open source, community vetted version didn’t already exist.
> 
> I don’t know if you are opposed to organizations that have AML requirements
> from using the bitcoin blockchain, but if you aren’t, why wouldn’t you
> prefer an open source, open standards based solution to exclusionary,
> proprietary ones?

In some (most?) countries, it is illegal to offer telecoms services without
wiretap facilities. Does that mean Tor builds into its software "open source"
"open standards" wiretapping functionality? No. And interestingly, people
trying to add support for that stuff is actually a thing that keeps happening
in the Tor community...

In any case, I'd strongly argue that we remove BIP75 from the bips repository,
and boycott wallets that implement it. It's bad strategy for Bitcoin developers
to willingly participate in AML/KYC, just the same way as it's bad for Tor to
add wiretapping functionality, and W3C to support DRM tech. The minor tactical
wins you'll get our of this aren't worth it.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------

It seems that MeetingBot was deactivated in #bitcoin-dev, so the
last two weeks the weekly developer meeting (Thursday 19:00-20:00 UTC)
was held in #bitcoin-core-dev.

Let's keep it at that.

Wladimir



-------------------------------------


On 10/03/16 00:53, Luke Dashjr wrote:
> On Thursday, March 10, 2016 12:29:16 AM Mustafa Al-Bassam wrote:
>>> the soft-fork does not become Final for as long as such a hard-fork
>>> has potentially-majority support, or at most three months.
>> This wording is awkward. What is "potentially-majority"?
> A group that is large enough that it may constitute a majority.
> How can I reword this better/clearer?
"potentially has majority support"?
>
>>> A hard-fork BIP requires adoption from the entire Bitcoin economy,
>>> particularly including those selling desirable goods and services in
>>> exchange for bitcoin payments, as well as Bitcoin holders who wish to
>>> spend or would spend their bitcoins (including selling for other
>>> currencies) differently in the event of such a hard-fork.
>> What if one shop owner, for example, out of thousands, doesn't adapt the
>> hard-fork? It is expected, and should perhaps be encouraged, for a small
>> minority to not accept a hard fork, but by the wording of the BIP
>> ("entire Bitcoin economy"), one shop owner can veto a hard-fork.
> It's not the hard-fork they can veto (in this context, anyway), but the 
> progression of the BIP Status field. However, one shop cannot operate in a 
> vacuum: if they are indeed alone, they will soon find themselves no longer 
> selling in exchange for bitcoin payments, as nobody else would exist willing 
> to use the previous blockchain to pay them. If they are no longer selling, 
> they cease to meet the criteria here enabling their veto.
I think in general this sounds like a good definition for a hard-fork
becoming active. But I can envision a situation where someone will try
to be annoying about it and point to one instance of one buyer and one
seller using the blockchain to buy and sell from each other, or set one up.

> Luke



-------------------------------------
These are the relevant info BIPs.

NODE_GETUTXO
https://github.com/bitcoin/bips/blob/master/bip-0064.mediawiki

NODE_BLOOM:
https://github.com/bitcoin/bips/blob/master/bip-0111.mediawiki

The relevant code is here:
https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L228

The NODE_GETUTXO bit was included even though it is not supported by core.
(It is one of XT's features).

I think you need to be able to reasonably claim that the bit is useful and
will have actual users, before you can claim a bit.

You can also claim one of the free for all bits 24 - 31, but that is
supposed to be only temporary.

Giving a link to "thin blocks" would help promote discussion about its
merits.

On Mon, Mar 7, 2016 at 8:06 PM, G. Andrew Stone via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> The Bitcoin Unlimited client needs a services bit to indicate that the
> node is capable of communicating thin blocks.  We propose to use bit 4 as
> AFAIK bit 3 is already earmarked for Segregated Witness.
>
> Andrew
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
I'm requesting feedback for Hash Time-Locked Contract (HTLC) transactions
in Bitcoin.

HTLC transactions allow you to pay for the preimage of a hash. CSV/CLTV can
be used to recover your funds if the other party is not cooperative. These
scripts take the following general form:

    [HASHOP] <digest> OP_EQUAL
    OP_IF
        <seller pubkey>
    OP_ELSE
        <num> [TIMEOUTOP] OP_DROP <buyer pubkey>
    OP_ENDIF
    OP_CHECKSIG

These transactions are useful for both the Lightning network and in
zero-knowledge contingent payments. This very script (using CLTV and
SHA256) was used as part of our "pay-to-sudoku" ZKCP demo earlier this
year: https://github.com/zcash/pay-to-sudoku

Members of the community have expressed the desire for a BIP to submitted
in coordination with changes to Bitcoin Core that support these
transactions in the wallet.

Please review my draft BIP here:
https://gist.github.com/ebfull/8306903041d46e4119a39442f72a3741

An implementation is being worked on here:
https://github.com/bitcoin/bitcoin/pull/7601

Thanks!

Sean Bowe
Zcash

-------------------------------------
Is it me or did Gavin ignore Yifu's direct questions? In case you missed it
Gavin --

~
"We can look at the adoption of the last major Bitcoin core release to
guess how long it might take people to upgrade. 0.11.0 was released on 12
July, 2015. Twenty eight days later, about 38% of full nodes were running
that release. Three months later, about 50% of the network was running that
release, and six months later about 66% of the network was running some
flavor of 0.11."

On what grounds do you think it is reasonable to assume that this update
will roll out 6x faster than previous data suggested, as oppose to your own
observation of 66% adoption in 6 month. or do you believe 38% node
upgrade-coverage (in 28 days ) on the network for a hard fork is good
enough?

There are no harm in choosing a longer grace period but picking one short
as 28 days you risk on alienating the nodes who do not upgrade with the
aggressive upgrade timeline you proposed.
~~

When Gavin writes "Responding to "28 days is not long enough" :

I keep seeing this claim made with no evidence to back it up.  As I said, I
surveyed several of the biggest infrastructure providers and the btcd lead
developer and they all agree "28 days is plenty of time."

For individuals... why would it take somebody longer than 28 days to either
download and restart their bitcoind, or to patch and then re-run (the patch
can be a one-line change MAX_BLOCK_SIZE from 1000000 to 2000000)?"

~~

Isn't Yifu's comment, evidence, the very best sort of evidence, it isn't
propositional a priori logic, but empirical evidence that. As for why
people take longer, who knows, we simply know from passed experience that
it in fact does take longer.

It's extremely frustrating to read Gavin's comments, it's hard to believe
he is engaging in earnest discussion.

On Sun, Feb 7, 2016 at 4:01 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Sunday, February 07, 2016 2:16:02 PM Gavin Andresen wrote:
> > On Sat, Feb 6, 2016 at 3:46 PM, Luke Dashjr via bitcoin-dev <
> > bitcoin-dev@lists.linuxfoundation.org> wrote:
> > > On Saturday, February 06, 2016 5:25:21 PM Tom Zander via bitcoin-dev
> wrote:
> > > > If you have a node that is "old" your node will stop getting new
> > > > blocks. The node will essentially just say "x-hours behind" with "x"
> > > > getting larger every hour. Funds don't get confirmed. etc.
> > >
> > > Until someone decides to attack you. Then you'll get 6, 10, maybe more
> > > blocks confirming a large 10000 BTC payment. If you're just a normal
> end
> > > user (or perhaps an automated system), you'll figure that payment is
> good
> > > and irreversibly hand over the title to the house.
> >
> > There will be approximately zero percentage of hash power left on the
> > weaker branch of the fork, based on past soft-fork adoption by miners
> (they
> > upgrade VERY quickly from 75% to over 95%).
>
> I'm assuming there are literally ZERO miners left on the weaker branch.
> The attacker in this scenario simply rents hashing for a few days in
> advance
> to build his fake chain, then broadcasts the blocks to the unsuspecting
> merchant at ~10 block intervals so it looks like everything is working
> normal
> again. There are lots of mining rental services out there, and miners quite
> often do not care to avoid selling hashrate to the highest bidder
> regardless
> of what they're mining. 10 blocks worth costs a little more than 250 BTC -
> soon, that will be 125 BTC.
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
Steven Pine
(510) 517-7075

-------------------------------------
On May 12, 2016 00:43, "Timo Hanke via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
> This is what I meant. If existing hardware gets forked-out it will
inevitably lead to the creation of an altcoin. Simply because the hardware
exists and can't be used for anything else both chains will survive. I was
only comparing the situation to a contentious hardfork that does not fork
out any hardware. If the latter one is suspected to lead to the permanent
existence of two chains then a hardfork that forks out hardware is even
more likely to do so (I claim it's guaranteed).

You are wrong. Whether 2 chains survive in parallel or not depends SOLELY
in whether both chains maintain demand (aka users).
Anyway, this is a discussion I had with Gavin and Rusty on bitcoin-discuss
already. I suggest we move this particular point there since it is more
philosophical than technical.

-------------------------------------
This is completely wrong. SPV wallets will work as normal without upgrade. Full nodes will only provide transactions to SPV in a format they understand, and SPV will accept the transaction since they are not doing any validation anyway. 

The only reason an end user may want to upgrade is for lower transaction fee when they are sending transaction. If they don't upgrade, that means the fee is too low for them to care, which is a good news

 ---- On Mon, 17 Oct 2016 00:42:26 +0800 Tom Zander via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote ---- 
 > On Sunday, 16 October 2016 12:35:58 CEST Gavin Andresen wrote: 
 > > On Sun, Oct 16, 2016 at 10:58 AM, Tom Zander via bitcoin-dev < 
 > >  
 > > bitcoin-dev@lists.linuxfoundation.org> wrote: 
 > > > The fallow period sounds waaaay to short. I suggest 2 months at minimum 
 > > > since anyone that wants to be safe needs to upgrade. 
 > >  
 > > I asked a lot of businesses and individuals how long it would take them to 
 > > upgrade to a new release over the last year or two. 
 > >  
 > > Nobody said it would take them more than two weeks. 
 >  
 > The question you asked them was likely about the block size. The main  
 > difference is that SPV users do not need to update after BIP109, but they do  
 > need to have a new wallet when SegWit transactions are being sent to them. 
 >  
 > This upgrade affects also end users, not just businesses etc. 
 >  
 > Personally, I'd say that 2 months is even too fast. 
 >   
 > --  
 > Tom Zander 
 > Blog: https://zander.github.io 
 > Vlog: https://vimeo.com/channels/tomscryptochannel 
 > _______________________________________________ 
 > bitcoin-dev mailing list 
 > bitcoin-dev@lists.linuxfoundation.org 
 > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev 
 > 




-------------------------------------
Right.  There are minor trade-offs to be made with regards to that design
point of OP_CHECKSIGFROMSTACKVERIFY.  Fortunately this covenant
construction isn't sensitive to that choice and can be made to work with
either implementation of OP_CHECKSIGFROMSTACKVERIFY.

On Wed, Nov 2, 2016 at 11:35 PM, Johnson Lau <jl2012@xbt.hk> wrote:

> Interesting. I have implemented OP_CHECKSIGFROMSTACKVERIFY in a different
> way from the Elements. Instead of hashing the data on stack, I directly put
> the 32 byte hash to the stack. This should be more flexible as not every
> system are using double-SHA256
>
> https://github.com/jl2012/bitcoin/commits/mast_v3_master
>
>
> On 3 Nov 2016, at 01:30, Russell O'Connor via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> Hi all,
>
> It is possible to implement covenants using two script extensions: OP_CAT
> and OP_CHECKSIGFROMSTACKVERIFY.  Both of these op codes are already
> available in the Elements Alpha sidechain, so it is possible to construct
> covenants in Elements Alpha today.  I have detailed how the construction
> works in a blog post at <https://blockstream.com/2016/
> 11/02/covenants-in-elements-alpha.html>.  As an example, I've constructed
> scripts for the Moeser-Eyal-Sirer vault.
>
> I'm interested in collecting and implementing other useful covenants, so
> if people have ideas, please post them.
>
> If there are any questions, I'd be happy to answer.
>
> --
> Russell O'Connor
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
>

-------------------------------------
On Thursday, 22 September 2016 21:59:12 CEST Jonas Schnelli via bitcoin-dev 
wrote:
> Hi Tom
> 
> > I think you misunderstand tagged systems at a very basic level.  You
> > think that html can only use a bold tag <b> once in a document? Thats
> > equivalent to what you are saying.
> 
> Would the "additional" segment contain the same amount of
> nSequence-equivalent token as the number of inputs in the "inputs"
> segment?

At this point I don't know what it should look like, I have not had time to 
look deeply into BIP68.  Is this what you would suggest it to look like?
I rather figured spending limitations would be assigned to an output, not 
an input.

> However, I think that should be mentioned/specified in the BIP.

It can be, and likely should be.  This BIP doesn't pretend to be finished 
yet.

I welcome any and all discussion about this, it only serves to make the end 
result stronger!


-------------------------------------
This looks very interesting. The first time implementing it might be more
painful but that will make subsequent hardforks a lot easier.

Do you think it's good to include the median timestamp of the past 11 blocks
after the block height in coinbase? That would make it easier to use it as
activation threshold of consensus rule changes.

For the witness commitment, it will also be treated as a merge mined
commitment?

It is also good to emphasize that it is the responsibility of miners, not
devs, to ensure that the hardfork is accepted by the supermajority of the
economy.


-----Original Message-----
From: bitcoin-dev-bounces@lists.linuxfoundation.org
[mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Luke
Dashjr via bitcoin-dev
Sent: Sunday, 7 February, 2016 17:53
To: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
Subject: [bitcoin-dev] Pre-BIP Growth Soft-hardfork

Here's a draft BIP I wrote almost a year ago. I'm going to look into
revising and completing it soon, and would welcome any suggestions for doing
so.

This hardfork BIP aims to accomplish a few important things:
- Finally deploying proper merge-mining as Satoshi suggested before he left.
- Expanding the nonce space miners can scan in-chip, avoiding expensive
  calculations on the host controller as blocks get larger.
- Provide a way to safely deploy hardforks without risking leaving old nodes
  vulnerable to attack.

https://github.com/luke-jr/bips/blob/bip-mmhf/bip-mmhf.mediawiki

Luke
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
Nice!

We’ve been talking about doing this forever and it’s so desperately needed.

> On May 17, 2016, at 3:23 PM, Peter Todd via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> # Motivation
> 
> UTXO growth is a serious concern for Bitcoin's long-term decentralization. To
> run a competitive mining operation potentially the entire UTXO set must be in
> RAM to achieve competitive latency; your larger, more centralized, competitors
> will have the UTXO set in RAM. Mining is a zero-sum game, so the extra latency
> of not doing so if they do directly impacts your profit margin. Secondly,
> having possession of the UTXO set is one of the minimum requirements to run a
> full node; the larger the set the harder it is to run a full node.
> 
> Currently the maximum size of the UTXO set is unbounded as there is no
> consensus rule that limits growth, other than the block-size limit itself; as
> of writing the UTXO set is 1.3GB in the on-disk, compressed serialization,
> which expands to significantly more in memory. UTXO growth is driven by a
> number of factors, including the fact that there is little incentive to merge
> inputs, lost coins, dust outputs that can't be economically spent, and
> non-btc-value-transfer "blockchain" use-cases such as anti-replay oracles and
> timestamping.
> 
> We don't have good tools to combat UTXO growth. Segregated Witness proposes to
> give witness space a 75% discount, in part of make reducing the UTXO set size
> by spending txouts cheaper. While this may change wallets to more often spend
> dust, it's hard to imagine an incentive sufficiently strong to discourage most,
> let alone all, UTXO growing behavior.
> 
> For example, timestamping applications often create unspendable outputs due to
> ease of implementation, and because doing so is an easy way to make sure that
> the data required to reconstruct the timestamp proof won't get lost - all
> Bitcoin full nodes are forced to keep a copy of it. Similarly anti-replay
> use-cases like using the UTXO set for key rotation piggyback on the uniquely
> strong security and decentralization guarantee that Bitcoin provides; it's very
> difficult - perhaps impossible - to provide these applications with
> alternatives that are equally secure. These non-btc-value-transfer use-cases
> can often afford to pay far higher fees per UTXO created than competing
> btc-value-transfer use-cases; many users could afford to spend $50 to register
> a new PGP key, yet would rather not spend $50 in fees to create a standard two
> output transaction. Effective techniques to resist miner censorship exist, so
> without resorting to whitelists blocking non-btc-value-transfer use-cases as
> "spam" is not a long-term, incentive compatible, solution.
> 
> A hard upper limit on UTXO set size could create a more level playing field in
> the form of fixed minimum requirements to run a performant Bitcoin node, and
> make the issue of UTXO "spam" less important. However, making any coins
> unspendable, regardless of age or value, is a politically untenable economic
> change.
> 
> 
> # TXO Commitments
> 
> A merkle tree committing to the state of all transaction outputs, both spent
> and unspent, we can provide a method of compactly proving the current state of
> an output. This lets us "archive" less frequently accessed parts of the UTXO
> set, allowing full nodes to discard the associated data, still providing a
> mechanism to spend those archived outputs by proving to those nodes that the
> outputs are in fact unspent.
> 
> Specifically TXO commitments proposes a Merkle Mountain Range¹ (MMR), a
> type of deterministic, indexable, insertion ordered merkle tree, which allows
> new items to be cheaply appended to the tree with minimal storage requirements,
> just log2(n) "mountain tips". Once an output is added to the TXO MMR it is
> never removed; if an output is spent its status is updated in place. Both the
> state of a specific item in the MMR, as well the validity of changes to items
> in the MMR, can be proven with log2(n) sized proofs consisting of a merkle path
> to the tip of the tree.
> 
> At an extreme, with TXO commitments we could even have no UTXO set at all,
> entirely eliminating the UTXO growth problem. Transactions would simply be
> accompanied by TXO commitment proofs showing that the outputs they wanted to
> spend were still unspent; nodes could update the state of the TXO MMR purely
> from TXO commitment proofs. However, the log2(n) bandwidth overhead per txin is
> substantial, so a more realistic implementation is be to have a UTXO cache for
> recent transactions, with TXO commitments acting as a alternate for the (rare)
> event that an old txout needs to be spent.
> 
> Proofs can be generated and added to transactions without the involvement of
> the signers, even after the fact; there's no need for the proof itself to
> signed and the proof is not part of the transaction hash. Anyone with access to
> TXO MMR data can (re)generate missing proofs, so minimal, if any, changes are
> required to wallet software to make use of TXO commitments.
> 
> 
> ## Delayed Commitments
> 
> TXO commitments aren't a new idea - the author proposed them years ago in
> response to UTXO commitments. However it's critical for small miners' orphan
> rates that block validation be fast, and so far it has proven difficult to
> create (U)TXO implementations with acceptable performance; updating and
> recalculating cryptographicly hashed merkelized datasets is inherently more
> work than not doing so. Fortunately if we maintain a UTXO set for recent
> outputs, TXO commitments are only needed when spending old, archived, outputs.
> We can take advantage of this by delaying the commitment, allowing it to be
> calculated well in advance of it actually being used, thus changing a
> latency-critical task into a much easier average throughput problem.
> 
> Concretely each block B_i commits to the TXO set state as of block B_{i-n}, in
> other words what the TXO commitment would have been n blocks ago, if not for
> the n block delay. Since that commitment only depends on the contents of the
> blockchain up until block B_{i-n}, the contents of any block after are
> irrelevant to the calculation.
> 
> 
> ## Implementation
> 
> Our proposed high-performance/low-latency delayed commitment full-node
> implementation needs to store the following data:
> 
> 1) UTXO set
> 
>    Low-latency K:V map of txouts definitely known to be unspent. Similar to
>    existing UTXO implementation, but with the key difference that old,
>    unspent, outputs may be pruned from the UTXO set.
> 
> 
> 2) STXO set
> 
>    Low-latency set of transaction outputs known to have been spent by
>    transactions after the most recent TXO commitment, but created prior to the
>    TXO commitment.
> 
> 
> 3) TXO journal
> 
>    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends
>    must be low-latency; removals can be high-latency.
> 
> 
> 4) TXO MMR list
> 
>    Prunable, ordered list of TXO MMR's, mainly the highest pending commitment,
>    backed by a reference counted, cryptographically hashed object store
>    indexed by digest (similar to how git repos work). High-latency ok. We'll
>    cover this in more in detail later.
> 
> 
> ### Fast-Path: Verifying a Txout Spend In a Block
> 
> When a transaction output is spent by a transaction in a block we have two
> cases:
> 
> 1) Recently created output
> 
>    Output created after the most recent TXO commitment, so it should be in the
>    UTXO set; the transaction spending it does not need a TXO commitment proof.
>    Remove the output from the UTXO set and append it to the TXO journal.
> 
> 2) Archived output
> 
>    Output created prior to the most recent TXO commitment, so there's no
>    guarantee it's in the UTXO set; transaction will have a TXO commitment
>    proof for the most recent TXO commitment showing that it was unspent.
>    Check that the output isn't already in the STXO set (double-spent), and if
>    not add it. Append the output and TXO commitment proof to the TXO journal.
> 
> In both cases recording an output as spent requires no more than two key:value
> updates, and one journal append. The existing UTXO set requires one key:value
> update per spend, so we can expect new block validation latency to be within 2x
> of the status quo even in the worst case of 100% archived output spends.
> 
> 
> ### Slow-Path: Calculating Pending TXO Commitments
> 
> In a low-priority background task we flush the TXO journal, recording the
> outputs spent by each block in the TXO MMR, and hashing MMR data to obtain the
> TXO commitment digest. Additionally this background task removes STXO's that
> have been recorded in TXO commitments, and prunes TXO commitment data no longer
> needed.
> 
> Throughput for the TXO commitment calculation will be worse than the existing
> UTXO only scheme. This impacts bulk verification, e.g. initial block download.
> That said, TXO commitments provides other possible tradeoffs that can mitigate
> impact of slower validation throughput, such as skipping validation of old
> history, as well as fraud proof approaches.
> 
> 
> ### TXO MMR Implementation Details
> 
> Each TXO MMR state is a modification of the previous one with most information
> shared, so we an space-efficiently store a large number of TXO commitments
> states, where each state is a small delta of the previous state, by sharing
> unchanged data between each state; cycles are impossible in merkelized data
> structures, so simple reference counting is sufficient for garbage collection.
> Data no longer needed can be pruned by dropping it from the database, and
> unpruned by adding it again. Since everything is committed to via cryptographic
> hash, we're guaranteed that regardless of where we get the data, after
> unpruning we'll have the right data.
> 
> Let's look at how the TXO MMR works in detail. Consider the following TXO MMR
> with two txouts, which we'll call state #0:
> 
>      0
>     / \
>    a   b
> 
> If we add another entry we get state #1:
> 
>        1
>       / \
>      0   \
>     / \   \
>    a   b   c
> 
> Note how it 100% of the state #0 data was reused in commitment #1. Let's
> add two more entries to get state #2:
> 
>            2
>           / \
>          2   \
>         / \   \
>        /   \   \
>       /     \   \
>      0       2   \
>     / \     / \   \
>    a   b   c   d   e
> 
> This time part of state #1 wasn't reused - it's wasn't a perfect binary
> tree - but we've still got a lot of re-use.
> 
> Now suppose state #2 is committed into the blockchain by the most recent block.
> Future transactions attempting to spend outputs created as of state #2 are
> obliged to prove that they are unspent; essentially they're forced to provide
> part of the state #2 MMR data. This lets us prune that data, discarding it,
> leaving us with only the bare minimum data we need to append new txouts to the
> TXO MMR, the tips of the perfect binary trees ("mountains") within the MMR:
> 
>            2
>           / \
>          2   \
>               \
>                \
>                 \
>                  \
>                   \
>                    e
> 
> Note that we're glossing over some nuance here about exactly what data needs to
> be kept; depending on the details of the implementation the only data we need
> for nodes "2" and "e" may be their hash digest.
> 
> Adding another three more txouts results in state #3:
> 
>                  3
>                 / \
>                /   \
>               /     \
>              /       \
>             /         \
>            /           \
>           /             \
>          2               3
>                         / \
>                        /   \
>                       /     \
>                      3       3
>                     / \     / \
>                    e   f   g   h
> 
> Suppose recently created txout f is spent. We have all the data required to
> update the MMR, giving us state #4. It modifies two inner nodes and one leaf
> node:
> 
>                  4
>                 / \
>                /   \
>               /     \
>              /       \
>             /         \
>            /           \
>           /             \
>          2               4
>                         / \
>                        /   \
>                       /     \
>                      4       3
>                     / \     / \
>                    e  (f)  g   h
> 
> If an archived txout is spent requires the transaction to provide the merkle
> path to the most recently committed TXO, in our case state #2. If txout b is
> spent that means the transaction must provide the following data from state #2:
> 
>            2
>           /
>          2
>         /
>        /
>       /
>      0
>       \
>        b
> 
> We can add that data to our local knowledge of the TXO MMR, unpruning part of
> it:
> 
>                  4
>                 / \
>                /   \
>               /     \
>              /       \
>             /         \
>            /           \
>           /             \
>          2               4
>         /               / \
>        /               /   \
>       /               /     \
>      0               4       3
>       \             / \     / \
>        b           e  (f)  g   h
> 
> Remember, we haven't _modified_ state #4 yet; we just have more data about it.
> When we mark txout b as spent we get state #5:
> 
>                  5
>                 / \
>                /   \
>               /     \
>              /       \
>             /         \
>            /           \
>           /             \
>          5               4
>         /               / \
>        /               /   \
>       /               /     \
>      5               4       3
>       \             / \     / \
>       (b)          e  (f)  g   h
> 
> Secondly by now state #3 has been committed into the chain, and transactions
> that want to spend txouts created as of state #3 must provide a TXO proof
> consisting of state #3 data. The leaf nodes for outputs g and h, and the inner
> node above them, are part of state #3, so we prune them:
> 
>                  5
>                 / \
>                /   \
>               /     \
>              /       \
>             /         \
>            /           \
>           /             \
>          5               4
>         /               /
>        /               /
>       /               /
>      5               4
>       \             / \
>       (b)          e  (f)
> 
> Finally, lets put this all together, by spending txouts a, c, and g, and
> creating three new txouts i, j, and k. State #3 was the most recently committed
> state, so the transactions spending a and g are providing merkle paths up to
> it. This includes part of the state #2 data:
> 
>                  3
>                 / \
>                /   \
>               /     \
>              /       \
>             /         \
>            /           \
>           /             \
>          2               3
>         / \               \
>        /   \               \
>       /     \               \
>      0       2               3
>     /       /               /
>    a       c               g
> 
> After unpruning we have the following data for state #5:
> 
>                  5
>                 / \
>                /   \
>               /     \
>              /       \
>             /         \
>            /           \
>           /             \
>          5               4
>         / \             / \
>        /   \           /   \
>       /     \         /     \
>      5       2       4       3
>     / \     /       / \     /
>    a  (b)  c       e  (f)  g
> 
> That's sufficient to mark the three outputs as spent and add the three new
> txouts, resulting in state #6:
> 
>                        6
>                       / \
>                      /   \
>                     /     \
>                    /       \
>                   /         \
>                  6           \
>                 / \           \
>                /   \           \
>               /     \           \
>              /       \           \
>             /         \           \
>            /           \           \
>           /             \           \
>          6               6           \
>         / \             / \           \
>        /   \           /   \           6
>       /     \         /     \         / \
>      6       6       4       6       6   \
>     / \     /       / \     /       / \   \
>   (a) (b) (c)      e  (f) (g)      i   j   k
> 
> Again, state #4 related data can be pruned. In addition, depending on how the
> STXO set is implemented may also be able to prune data related to spent txouts
> after that state, including inner nodes where all txouts under them have been
> spent (more on pruning spent inner nodes later).
> 
> 
> ### Consensus and Pruning
> 
> It's important to note that pruning behavior is consensus critical: a full node
> that is missing data due to pruning it too soon will fall out of consensus, and
> a miner that fails to include a merkle proof that is required by the consensus
> is creating an invalid block. At the same time many full nodes will have
> significantly more data on hand than the bare minimum so they can help wallets
> make transactions spending old coins; implementations should strongly consider
> separating the data that is, and isn't, strictly required for consensus.
> 
> A reasonable approach for the low-level cryptography may be to actually treat
> the two cases differently, with the TXO commitments committing too what data
> does and does not need to be kept on hand by the UTXO expiration rules. On the
> other hand, leaving that uncommitted allows for certain types of soft-forks
> where the protocol is changed to require more data than it previously did.
> 
> 
> ### Consensus Critical Storage Overheads
> 
> Only the UTXO and STXO sets need to be kept on fast random access storage.
> Since STXO set entries can only be created by spending a UTXO - and are smaller
> than a UTXO entry - we can guarantee that the peak size of the UTXO and STXO
> sets combined will always be less than the peak size of the UTXO set alone in
> the existing UTXO-only scheme (though the combined size can be temporarily
> higher than what the UTXO set size alone would be when large numbers of
> archived txouts are spent).
> 
> TXO journal entries and unpruned entries in the TXO MMR have log2(n) maximum
> overhead per entry: a unique merkle path to a TXO commitment (by "unique" we
> mean that no other entry shares data with it). On a reasonably fast system the
> TXO journal will be flushed quickly, converting it into TXO MMR data; the TXO
> journal will never be more than a few blocks in size.
> 
> Transactions spending non-archived txouts are not required to provide any TXO
> commitment data; we must have that data on hand in the form of one TXO MMR
> entry per UTXO. Once spent however the TXO MMR leaf node associated with that
> non-archived txout can be immediately pruned - it's no longer in the UTXO set
> so any attempt to spend it will fail; the data is now immutable and we'll never
> need it again. Inner nodes in the TXO MMR can also be pruned if all leafs under
> them are fully spent; detecting this is easy the TXO MMR is a merkle-sum tree,
> with each inner node committing to the sum of the unspent txouts under it.
> 
> When a archived txout is spent the transaction is required to provide a merkle
> path to the most recent TXO commitment. As shown above that path is sufficient
> information to unprune the necessary nodes in the TXO MMR and apply the spend
> immediately, reducing this case to the TXO journal size question (non-consensus
> critical overhead is a different question, which we'll address in the next
> section).
> 
> Taking all this into account the only significant storage overhead of our TXO
> commitments scheme when compared to the status quo is the log2(n) merkle path
> overhead; as long as less than 1/log2(n) of the UTXO set is active,
> non-archived, UTXO's we've come out ahead, even in the unrealistic case where
> all storage available is equally fast. In the real world that isn't yet the
> case - even SSD's significantly slower than RAM.
> 
> 
> ### Non-Consensus Critical Storage Overheads
> 
> Transactions spending archived txouts pose two challenges:
> 
> 1) Obtaining up-to-date TXO commitment proofs
> 
> 2) Updating those proofs as blocks are mined
> 
> The first challenge can be handled by specialized archival nodes, not unlike
> how some nodes make transaction data available to wallets via bloom filters or
> the Electrum protocol. There's a whole variety of options available, and the
> the data can be easily sharded to scale horizontally; the data is
> self-validating allowing horizontal scaling without trust.
> 
> While miners and relay nodes don't need to be concerned about the initial
> commitment proof, updating that proof is another matter. If a node aggressively
> prunes old versions of the TXO MMR as it calculates pending TXO commitments, it
> won't have the data available to update the TXO commitment proof to be against
> the next block, when that block is found; the child nodes of the TXO MMR tip
> are guaranteed to have changed, yet aggressive pruning would have discarded that
> data.
> 
> Relay nodes could ignore this problem if they simply accept the fact that
> they'll only be able to fully relay the transaction once, when it is initially
> broadcast, and won't be able to provide mempool functionality after the initial
> relay. Modulo high-latency mixnets, this is probably acceptable; the author has
> previously argued that relay nodes don't need a mempool² at all.
> 
> For a miner though not having the data necessary to update the proofs as blocks
> are found means potentially losing out on transactions fees. So how much extra
> data is necessary to make this a non-issue?
> 
> Since the TXO MMR is insertion ordered, spending a non-archived txout can only
> invalidate the upper nodes in of the archived txout's TXO MMR proof (if this
> isn't clear, imagine a two-level scheme, with a per-block TXO MMRs, committed
> by a master MMR for all blocks). The maximum number of relevant inner nodes
> changed is log2(n) per block, so if there are n non-archival blocks between the
> most recent TXO commitment and the pending TXO MMR tip, we have to store
> log2(n)*n inner nodes - on the order of a few dozen MB even when n is a
> (seemingly ridiculously high) year worth of blocks.
> 
> Archived txout spends on the other hand can invalidate TXO MMR proofs at any
> level - consider the case of two adjacent txouts being spent. To guarantee
> success requires storing full proofs. However, they're limited by the blocksize
> limit, and additionally are expected to be relatively uncommon. For example, if
> 1% of 1MB blocks was archival spends, our hypothetical year long TXO commitment
> delay is only a few hundred MB of data with low-IO-performance requirements.
> 
> 
> ## Security Model
> 
> Of course, a TXO commitment delay of a year sounds ridiculous. Even the slowest
> imaginable computer isn't going to need more than a few blocks of TXO
> commitment delay to keep up ~100% of the time, and there's no reason why we
> can't have the UTXO archive delay be significantly longer than the TXO
> commitment delay.
> 
> However, as with UTXO commitments, TXO commitments raise issues with Bitcoin's
> security model by allowing relatively miners to profitably mine transactions
> without bothering to validate prior history. At the extreme, if there was no
> commitment delay at all at the cost of a bit of some extra network bandwidth
> "full" nodes could operate and even mine blocks completely statelessly by
> expecting all transactions to include "proof" that their inputs are unspent; a
> TXO commitment proof for a commitment you haven't verified isn't a proof that a
> transaction output is unspent, it's a proof that some miners claimed the txout
> was unspent.
> 
> At one extreme, we could simply implement TXO commitments in a "virtual"
> fashion, without miners actually including the TXO commitment digest in their
> blocks at all. Full nodes would be forced to compute the commitment from
> scratch, in the same way they are forced to compute the UTXO state, or total
> work. Of course a full node operator who doesn't want to verify old history can
> get a copy of the TXO state from a trusted source - no different from how you
> could get a copy of the UTXO set from a trusted source.
> 
> A more pragmatic approach is to accept that people will do that anyway, and
> instead assume that sufficiently old blocks are valid. But how old is
> "sufficiently old"? First of all, if your full node implementation comes "from
> the factory" with a reasonably up-to-date minimum accepted total-work
> thresholdⁱ - in other words it won't accept a chain with less than that amount
> of total work - it may be reasonable to assume any Sybil attacker with
> sufficient hashing power to make a forked chain meeting that threshold with,
> say, six months worth of blocks has enough hashing power to threaten the main
> chain as well.
> 
> That leaves public attempts to falsify TXO commitments, done out in the open by
> the majority of hashing power. In this circumstance the "assumed valid"
> threshold determines how long the attack would have to go on before full nodes
> start accepting the invalid chain, or at least, newly installed/recently reset
> full nodes. The minimum age that we can "assume valid" is tradeoff between
> political/social/technical concerns; we probably want at least a few weeks to
> guarantee the defenders a chance to organise themselves.
> 
> With this in mind, a longer-than-technically-necessary TXO commitment delayʲ
> may help ensure that full node software actually validates some minimum number
> of blocks out-of-the-box, without taking shortcuts. However this can be
> achieved in a wide variety of ways, such as the author's prev-block-proof
> proposal³, fraud proofs, or even a PoW with an inner loop dependent on
> blockchain data. Like UTXO commitments, TXO commitments are also potentially
> very useful in reducing the need for SPV wallet software to trust third parties
> providing them with transaction data.
> 
> i) Checkpoints that reject any chain without a specific block are a more
>   common, if uglier, way of achieving this protection.
> 
> j) A good homework problem is to figure out how the TXO commitment could be
>   designed such that the delay could be reduced in a soft-fork.
> 
> 
> ## Further Work
> 
> While we've shown that TXO commitments certainly could be implemented without
> increasing peak IO bandwidth/block validation latency significantly with the
> delayed commitment approach, we're far from being certain that they should be
> implemented this way (or at all).
> 
> 1) Can a TXO commitment scheme be optimized sufficiently to be used directly
> without a commitment delay? Obviously it'd be preferable to avoid all the above
> complexity entirely.
> 
> 2) Is it possible to use a metric other than age, e.g. priority? While this
> complicates the pruning logic, it could use the UTXO set space more
> efficiently, especially if your goal is to prioritise bitcoin value-transfer
> over other uses (though if "normal" wallets nearly never need to use TXO
> commitments proofs to spend outputs, the infrastructure to actually do this may
> rot).
> 
> 3) Should UTXO archiving be based on a fixed size UTXO set, rather than an
> age/priority/etc. threshold?
> 
> 4) By fixing the problem (or possibly just "fixing" the problem) are we
> encouraging/legitimising blockchain use-cases other than BTC value transfer?
> Should we?
> 
> 5) Instead of TXO commitment proofs counting towards the blocksize limit, can
> we use a different miner fairness/decentralization metric/incentive? For
> instance it might be reasonable for the TXO commitment proof size to be
> discounted, or ignored entirely, if a proof-of-propagation scheme (e.g.
> thinblocks) is used to ensure all miners have received the proof in advance.
> 
> 6) How does this interact with fraud proofs? Obviously furthering dependency on
> non-cryptographically-committed STXO/UTXO databases is incompatible with the
> modularized validation approach to implementing fraud proofs.
> 
> 
> # References
> 
> 1) "Merkle Mountain Ranges",
>   Peter Todd, OpenTimestamps, Mar 18 2013,
>   https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md
> 
> 2) "Do we really need a mempool? (for relay nodes)",
>   Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,
>   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009479.html
> 
> 3) "Segregated witnesses and validationless mining",
>   Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,
>   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html
> 
> -- 
> https://petertodd.org 'peter'[:-1]@petertodd.org
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
We use the default BIP32 wallet layout, mentioned in BIP43 as purpose "0".
We were thinking of of having 4 chains below the "account" level, the
original 0 and 1 for receive and change addresses, and then 0x40000000 and
0x40000001 for P2WPKH-in-P2SH versions of receive and change addresses.

I like the idea of specifying the type of address as a bit field flag.
0x80000000 is already used to specify hardened derivation, so 0x40000000
would be the next available to specify witness addresses. This is
compatible with existing accounts and wallet layouts.

As Daniel mentioned, the downside is that trying to recover on non-segwit
software will miss segwit receives, however it does avoid the problem of
having to check multiple address types for each key.

Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com>

On Fri, May 13, 2016 at 8:00 AM, Pavol Rusnak via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On 13/05/16 15:16, Daniel Weigl via bitcoin-dev wrote:
> > 2) Define a new derivation path, parallel to Bip44, but a different
> 'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user
> choose which account he want to add ("Normal account", "Witness account").
>
> We had quite a long discussion in our team some time ago and we agreed
> on that option #2 is much better and we'd like to implement this way in
> myTREZOR.
>
> >       +) Wallet needs only to take care of 1 address per public key
>
> True, if this BIP only supports P2WPKH.
>
> P2WSH should probably be handled by another account type and another
> BIP, anyway.
>
> > Has any Bip44 compliant wallet already done any integration at this
> point?
>
> We have something in the pipeline, but no visible results yet.
>
> --
> Best Regards / S pozdravom,
>
> Pavol "stick" Rusnak
> SatoshiLabs.com
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Mar 10, 2016 16:51, "Mustafa Al-Bassam via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I think in general this sounds like a good definition for a hard-fork
> becoming active. But I can envision a situation where someone will try
> to be annoying about it and point to one instance of one buyer and one
> seller using the blockchain to buy and sell from each other, or set one
up.

And all the attacker will achieve is preventing a field on a text file on
github from moving from "active" to "final".
Seems pretty stupid. Why would an attacker care so much about this? Is
there any way the attacker can make gains or harm bitcoin with this attack?

-------------------------------------
This works for segwit version 1 with the addition of also using a different
chain id.

I presume that segwit version 2 will be implementing schnorr signatures.
What do we know about the likely implementation details? Is there any way
to avoid using a third derivation path to support it?


Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com>

On Tue, Jun 14, 2016 at 8:41 AM, Daniel Weigl via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi List,
>
> Following up to the discussion last month (
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012695.html
> ), ive prepared a proposal for a BIP here:
>
>
> https://github.com/DanielWeigl/bips/blob/master/bip-p2sh-accounts.mediawiki
>
>
> Any comments on it? Does anyone working on a BIP44 compliant wallet
> implement something different?
> If there are no objection, id also like to request a number for it.
>
> Thx,
> Daniel
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
I think the biggest problem of sum tree is the lack of flexibility to redefine the values with softforks. For example, in the future we may want to define a new CHECKSIG with witness script version 1. That would be counted as a SigOp. Without a sum tree design, that’d be easy as we could just define new SigOp through a softfork (e.g. the introduction of P2SH SigOp, and the witness v0 SigOp). In a sum tree, however, since the nSigOp is implied, any redefinition requires either a hardfork or a new sum tree (and the original sum tree becomes a placebo for old nodes. So every softfork of this type creates a new tree)

Similarly, we may have secondary witness in the future, and the tx weight would be redefined with a softfork. We will face the same problem with a sum tree

The only way to fix this is to explicitly commit to the weight and nSigOp, and the committed value must be equal to or larger than the real value. Only in this way we could redefine it with softfork. However, that means each tx will have an overhead of 16 bytes (if two int64 are used)

You could find related discussion here: https://github.com/jl2012/bitcoin/commit/69e613bfb0f777c8dcd2576fe1c2541ee7a17208 <https://github.com/jl2012/bitcoin/commit/69e613bfb0f777c8dcd2576fe1c2541ee7a17208>

Maybe we could make this optional: for nodes running exactly the same rules, they could omit the weight and nSigOp value in transmission. To talk to legacy nodes, they need to transmit the newly defined weight and nSigOp. But this makes script upgrade much complex.


> On 12 Dec 2016, at 00:40, Tier Nolan via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
> On Sat, Dec 10, 2016 at 9:41 PM, Luke Dashjr <luke@dashjr.org <mailto:luke@dashjr.org>> wrote:
> On Saturday, December 10, 2016 9:29:09 PM Tier Nolan via bitcoin-dev wrote:
> > Any new merkle algorithm should use a sum tree for partial validation and
> > fraud proofs.
> 
> PR welcome.
> 
> Fair enough.  It is pretty basic.
> 
> https://github.com/luke-jr/bips/pull/2 <https://github.com/luke-jr/bips/pull/2>
> 
> It sums up sigops, block size, block cost (that is "weight" right?) and fees.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org <mailto:bitcoin-dev@lists.linuxfoundation.org>
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------

Your thoughts are sought on this simple proposal to allow transaction
authors to restrict execution to fewer than all blockchain forks where
the transaction would otherwise be valid.


Proposal

Node implementations select a bit from among the upper 8 bits of the
transaction version space to enforce as a hard fork opt-out bit.

To specify that a transaction NOT be mined by nodes that enforce a
particular bit, authors set that bit in the transaction version.
Opt-out is enforced by consensus among nodes enforcing each bit.

An implementation will relay, process and mine transactions that opt out
of other blockchain forks; just not those that opt out of its own fork.


Notes

Example: Via soft fork, all implementations may begin enforcing hard
fork opt-out bit 30.  Post soft fork, setting this bit would make a
transaction invalid, unless a fork emerges that has stopped enforcing
bit 30.

Example: BIP109 implementations may stop enforcing bit 30 and begin
enforcing bit 28 when the BIP109 hard fork is activated for a chain they
are tracking.

Enforcing more than one hard fork opt-out bit would imply that an
implementation is actively participating in building more than one
blockchain fork, and therefore providing a way to opt out of each.



-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.13.0rc3 are available from:

    https://bitcoin.org/bin/bitcoin-core-0.13.0/test.rc3/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.13.0rc3

This is a release candidate for a new major version release, bringing new
features, bug fixes, as well as other improvements.

Preliminary release notes for the release can be found at

    https://github.com/bitcoin/bitcoin/blob/0.13/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.13.0.

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues

Notable changes since rc2:

### Block and transaction handling

- - #8364 `3f65ba2` Treat high-sigop transactions as larger rather than rejecting them (sipa)

### Tests and QA

- - #8444 `cd0910b` Fix p2p-feefilter.py for changed tx relay behavior (sdaftuar)

### Mining

- - #8489 `8b0eee6` Bugfix: Use pre-BIP141 sigops until segwit activates (GBT) (luke-jr)

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJXrsS5AAoJEHSBCwEjRsmmH1oH/3PKk2rJIgMhsf4a8fuGJD7H
+j+ugsdRpjRGK8XKgnCisLQ8UJc2Z7dRbRYfdUb1ibkkCccMQJdmS6JAahWTe+Hb
N8GbODwZ5m4LrP2PA1gqNE/pwix/pqBY4GfR+TxXEbegNUNYDkvUzeUidcYGMAZd
LEAFrVnvsTAOQiwK3/pwa+sdWVNc0Jx/hHSZhouUtFmaqjXdg5M8ShmnRjyhVovI
GeuJ2s/3+uMtIX0g+kIOv16e0qYHJtIJMexMV5x4x1oWpXMYi2YeVDFSAkcKxA/z
5JOqlhDBK2lVMOKw8kYcNXKpvhXg9UBKImJDS0S4Bye8nMLQC6VEnkscOd7bupU=
=ae21
-----END PGP SIGNATURE-----


-------------------------------------
On Sat, Jan 23, 2016 at 06:33:56AM +0100, xor--- via bitcoin-dev wrote:
> So "+1"ing is OK as long as I provide a technical explanation of why I agree?
> While I still think that this is too much of a restriction because it prevents 
> peer-review, I would say that I could live with it as a last resort if you 
> don't plan to abolish this rule altogether.
> 
> So in that case, to foster peer review, I would recommend you amend the rules 
> to clarify this.
> Example: "+1s are not allowed unless you provide an explanation of why you 
> agree with something".

I would extend this to say that the technical explanation also should
contribute uniquely to the conversation; a +1 with an explanation
the last +1 gave isn't useful.

-- 
'peter'[:-1]@petertodd.org
000000000000000007e2005be0ce25b3f3de67b2dc35fd810b0ccd77b33eb7be

-------------------------------------
On Tue, Jun 21, 2016 at 08:44:37PM +0000, Luke Dashjr via bitcoin-dev wrote:
> On Monday, June 20, 2016 5:33:32 PM Erik Aronesty via bitcoin-dev wrote:
> > BIP 0070 has been a a moderate success, however, IMO:
> > 
> > - protocol buffers are inappropriate since ease of use and extensibility is
> > desired over the minor gains of efficiency in this protocol.  Not too late
> > to support JSON messages as the standard going forward
> 
> IMO JSON is too prone to gratuitous inefficiency (both at network and CPU 
> level), parser bugs, etc. Even the best C implementation (jansson) has serious 
> issues with Number handling.
> 
> A few years ago, I looked into binary alternatives to JSON and concluded they 
> all had problems, while it seems more than reasonable to do even dynamic 
> parsing of protobuf messages. So to conclude, I prefer to stick to protobuf 
> unless a clearly superior protocol turns up.

I'll second that statement.

Ease of use isn't a very good criteria for security-critical software handling
money, and the JSON standard has a very large amount of degrees of freedom in
how people have implemented it historically. Even protobuf I'd personally avoid
using on that basis, as protobuf encoding isn't deterministic: you can encode
the same data in multiple ways.

Unfortunately there isn't a viable alternative, so we're probably stuck with
protobuf right now for standards that want to see wide adoption in the near
future; I've got a few projects that need an alternative, which I'm working on,
but that's a ways off.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On 11/16/2016 05:50 PM, Pieter Wuille wrote:

> If you were trying to point out that buried softforks are similar to
> checkpoints in this regard, I agree.

Yes, that was my point.

> So are checkpoints good now?
> I believe we should get rid of checkpoints because they seem to be
misunderstood as a security feature rather than as an optimization.

Or maybe because they place control of the "true chain" in the hands of
those selecting the checkpoints? It's not a great leap for the parties
distributing the checkpoints to become the central authority.

I recommend users of our node validate the full chain without
checkpoints and from that chain select their own checkpoints and place
them into config. From that point forward they can apply the
optimization. Checkpoints should never be hardcoded into the source.

> I don't think buried softforks have that problem.

I find "buried softfork" a curious name as you are using it. You seem to
be implying that this type of change is itself a softfork as opposed to
a hardfork that changes the activation of a softfork. It was my
understanding that the term referred to the 3 softforks that were being
"buried", or the proposal, but not the burial itself.

Nevertheless, this proposal shouldn't have "that problem" because it is
clearly neither a security feature nor an optimization. That is the
first issue that needs to be addressed.

e


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

As discussed in the 11 Aug 2016 IRC meeting (https://bitcoincore.org/en/meetings/2016/08/11/#softfork-to-make-low-s-required), a new BIP with implementation is prepared to make low S value signature as a consensus rule:

https://github.com/jl2012/bips/blob/biplows/bip-lows.mediawiki

https://github.com/bitcoin/bitcoin/pull/8514

The softfork is proposed to be deployed with segwit (BIP141), likely in v0.13.1

The text is copied below

  BIP: ?
  Title: Low S values signatures
  Author: Pieter Wuille <pieter.wuille@gmail.com>
          Johnson Lau <jl2012@xbt.hk>
  Status: Draft
  Type: Standards Track
  Created: 2016-08-16

Abstract

This document specifies proposed changes to the Bitcoin transaction validity rules to restrict signatures to using low S values.

Motivation

ECDSA signatures are inherently malleable as taking the negative of the number S inside (modulo the curve order) does not invalidate it. This is a nuisance malleability vector as any relay node on the network may transform the signature, with no access to the relevant private keys required. For non-segregated witness transactions, this malleability will change the txid and invalidate any unconfirmed child transactions. Although the txid of segregated witness (BIP141) transactions is not third party malleable, this malleability vector will change the wtxid and may reduce the efficiency of compact block relay (BIP152).

To fix this malleability, we require that the S value inside ECDSA signatures is at most the curve order divided by 2 (essentially restricting this value to its lower half range). The value S in signatures must be between 0x1 and 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0 (inclusive). If S is too high, simply replace it by S' = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE BAAEDCE6 AF48A03B BFD25E8C D0364141 - S.

Specification

Every signature passed to OP_CHECKSIG, OP_CHECKSIGVERIFY, OP_CHECKMULTISIG, or OP_CHECKMULTISIGVERIFY, to which ECDSA verification is applied, MUST use a S value between 0x1 and 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0 (inclusive) with strict DER encoding (see BIP66).

These operators all perform ECDSA verifications on pubkey/signature pairs, iterating from the top of the stack backwards. For each such verification, if the signature does not pass the IsLowDERSignature check, the entire script evaluates to false immediately. If the signature is valid DER with low S value, but does not pass ECDSA verification, opcode execution continues as it used to, causing opcode execution to stop and push false on the stack (but not immediately fail the script) in some cases, which potentially skips further signatures (and thus does not subject them to IsLowDERSignature).

Deployment

This BIP will be deployed by "version bits" BIP9 using the same parameters for BIP141 and BIP143, with the name "segwit" and using bit 1.

For Bitcoin mainnet, the BIP9 starttime will be midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout will be midnight TBD UTC (Epoch timestamp TBD).

For Bitcoin testnet, the BIP9 starttime will be midnight 1 May 2016 UTC (Epoch timestamp 1462060800) and BIP9 timeout will be midnight 1 May 2017 UTC (Epoch timestamp 1493596800).

Compatibility

The reference client has produced compatible signatures since v0.9.0, and the requirement to have low S value signatures has been enforced as a relay policy by the reference client since v0.11.1. As of August 2016, very few transactions violating the requirement are being added to the chain. In addition, every non-compliant signature can trivially be converted into a compliant one, so there is no loss of functionality by this requirement. This proposal has the added benefit of reducing transaction malleability.

Implementation

An implementation for the reference client is available at https://github.com/bitcoin/bitcoin/pull/8514

Acknowledgements

This document is extracted from the previous BIP62 proposal which had input from various people.

Copyright

This document is placed in the public domain.

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - https://gpgtools.org

iQGcBAEBCgAGBQJXsuZLAAoJEO6eVSA0viTSBkIL/RxdKYhfQUcXhWf3wPzJ2rSo
bhxoGOoswf5Npx1ybKvvTRf51IirgO9JkEl8hYfzLr9KSbfTxCKlr2Z/S+snFGDi
Q0bvVPcg8uoK1iBMrFmIqCi/0pW3/lnnpgqt+O5Jup+DfK4S1QbSVNff8uP7ZK9x
NcgXekAbad57JfZ7gki9aERRj4THliTFBlaKkWo4CP+AwCgtKP6BwWvJxnfGpCc5
Esb/7aFvB0OwTWC7bPdS/XSCChxEdK9n5U3LaUH5o1oMQQhaGVHqeR76Wuf2oDvY
YsXX0b1gttpSJhz00ifOhMf7PhFzQuNyI6gM6ee7kMXwHMlrmyvROQh009cUzKeZ
5m7QKiondMsCoyz0zYXncF/MlwoyI7y1M5pQEqF/CHI5yZGu2K3EeDQebEHDzIrd
RyI6j5BbjLQ4w+geswaxzRSJfkoaKTHdh8g49HL7Q7FUj551jExKA8ZM50SbfeRi
T4fAN8BTXWVpfHkeDYdM2fesaqmFuN9wg18/xwTWJA==
=GgxI
-----END PGP SIGNATURE-----


-------------------------------------
On 14/05/16 09:00, Andreas Schildbach via bitcoin-dev wrote:
> The whole idea of BIP43 (which BIP44 bases on) is that how these BIPs
> define balance retrieval never changes. This is to make sure you always
> see the same balance on "same BIP" wallets (and same seed of course).

This! Thanks Andreas for formulating my thought that I was not able to
articulate earlier.

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------


Le 14/05/2016 18:14, Jonas Schnelli via bitcoin-dev a crit :
> 
> AFAIK: Bip39 import (cross-wallet) is not supported by [...] Electrum [2] .
> 

That is correct. There are several reasons why I decided not to use
BIP39 in Electrum. One of them was that BIP39 seed phrases do not
include a version number. A version number is needed in order to
maintain backward compatibility, everytime you change the address
derivation.

Electrum will allocate a new version number for seed phrases that should
be derived to segwit addresses.

I guess BIP39 designers will have to change the semantics of their
checksum bits, in order to encode a version number for segwit.


-------------------------------------
On Tue, Jun 14, 2016 at 05:14:23PM -0700, Bram Cohen via bitcoin-dev wrote:
> This is in response to Peter Todd's proposal for Merkle Mountain Range
> commitments in blocks:
> 
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012715.html
> 
> I'm in strong agreement that there's a compelling need to put UTXO
> commitments in blocks, and that the big barrier to getting it done is
> performance, particularly latency. But I have strong disagreements (or
> perhaps the right word is skepticism) about the details.
> 
> Peter proposes that there should be both UTXO and STXO commitments, and

No, that's incorrect - I'm only proposing TXO commitments, not UTXO nor STXO
commitments.

> they should be based on Merkle Mountain Ranges based on Patricia Tries. My
> first big disagreement is about the need for STXO commitments. I think
> they're unnecessary and a performance problem. The STXO set is much larger
> than the utxo set and requires much more memory and horespower to maintain.

Again, I'm not proposing STXO commitments precisely because the set of _spent_
transactions grows without bound. TXO commitments with committed sums of
remaining unspent TXO's and with pruning of old history are special in this
regard, because once spent the data associated with spent transactions can be
discarded completely, and at the same time, data associated with old history
can be pruned with responsibility for keeping it resting on the shoulders of
those owning those coins.

> Most if not all of its functionality can in practice be done using the utxo
> set. Almost anything accepting proofs of inclusion and exclusion will have
> a complete history of block headers, so to prove inclusion in the stxo set
> you can use a utxo proof of inclusion in the past and a proof of exclusion
> for the most recent block. In the case of a txo which has never been
> included at all, it's generally possible to show that an ancestor of the
> txo in question was at one point included but that an incompatible
> descendant of it (or the ancestor itself) is part of the current utxo set.
> Generating these sorts of proofs efficiently can for some applications
> require a complete STXO set, but that can done with a non-merkle set,
> getting the vastly better performance of an ordinary non-cryptographic
> hashtable.

TXO commitments allows you to do all of this without requiring miners to have
unbounded storage to create new blocks.

> The fundamental approach to handling the latency problem is to have the
> utxo commitments trail a bit. Computing utxo commitments takes a certain
> amount of time, too much to hold up block propagation but still hopefully
> vastly less than the average amount of time between blocks. Trailing by a
> single block is probably a bad idea because you sometimes get blocks back
> to back, but you never get blocks back to back to back to back. Having the
> utxo set be trailing by a fixed amount - five blocks is probably excessive
> - would do a perfectly good job of keeping latency from every becoming an
> issue. Smaller commitments for the utxos added and removed in each block
> alone could be added without any significant performance penalty. That way
> all blocks would have sufficient commitments for a completely up to date
> proofs of inclusion and exclusion. This is not a controversial approach.

Agreed - regardless of approach adding latency to commitment calculations of
all kinds is something I think we all agree can work in principle, although
obviously it should be a last resort technique when optimization fails.

> Now I'm going to go out on a limb. My thesis is that usage of a mountain
> range is unnecessary, and that a merkle tree in the raw can be made
> serviceable by sprinkling magic pixie dust on the performance problem.

It'd help if you specified exactly what type of merkle tree you're talking
about here; remember that the certificate transparency RFC appears to have
reinvented merkle mountain ranges, and they call them "merkle trees".  Bitcoin
meanwhile uses a so-called "merkle tree" that's broken, and Zcash uses a
partially filled fixed-sized perfect tree.

> There are two causes of performance problems for merkle trees: hashing
> operations and memory cache misses. For hashing functions, the difference
> between a mountain range and a straight merkle tree is roughly that in a
> mountain range there's one operation for each new update times the number
> of times that thing will get merged into larger hills. If there are fewer
> levels of hills the number of operations is less but the expense of proof
> of inclusion will be larger. For raw merkle trees the number of operations
> per thing added is the log base 2 of the number of levels in the tree,
> minus the log base 2 of the number of things added at once since you can do
> lazy evaluation. For practical Bitcoin there are (very roughly) a million
> things stored, or 20 levels, and there are (even more roughly) about a
> thousand things stored per block, so each thing forces about 20 - 10 = 10
> operations. If you follow the fairly reasonable guideline of mountain range
> hills go up by factors of four, you instead have 20/2 = 10 operations per
> thing added amortized. Depending on details this comparison can go either
> way but it's roughly a wash and the complexity of a mountain range is
> clearly not worth it at least from the point of view of CPU costs.

I'm having a hard time understanding this paragraph; could you explain what you
think is happening when things are "merged into larger hills"?

> But CPU costs aren't the main performance problem in merkle trees. The
> biggest issues is cache misses, specifically l1 and l2 cache misses. These
> tend to take a long time to do, resulting in the CPU spending most of its
> time sitting around doing nothing. A naive tree implementation is pretty
> much the worst thing you can possibly build from a cache miss standpoint,
> and its performance will be completely unacceptable. Mountain ranges do a
> fabulous job of fixing this problem, because all their updates are merges
> so the metrics are more like cache misses per block instead of cache misses
> per transaction.
>
> The magic pixie dust I mentioned earlier involves a bunch of subtle
> implementation details to keep cache coherence down which should get the
> number of cache misses per transaction down under one, at which point it
> probably isn't a bottleneck any more. There is an implementation in the
> works here:

As UTXO/STXO/TXO sets are all enormously larger than L1/L2 cache, it's
impossible to get CPU cache misses below one for update operations. The closest
thing to an exception is MMR's, which due to their insertion-ordering could
have good cache locality for appends, in the sense that the mountain tips
required to recalculate the MMR tip digest will already be in cache from the
previous append. But that's not sufficient, as you also need to modify old
TXO's further back in the tree to mark them as spent - that data is going to be
far larger than L1/L2 cache.

> https://github.com/bramcohen/MerkleSet
> 
> This implementation isn't finished yet! I'm almost there, and I'm
> definitely feeling time pressure now. I've spent quite a lot of time on
> this, mostly because of a bunch of technical reworkings which proved
> necessary. This is the last time I ever write a database for kicks. But
> this implementation is good on all important dimensions, including:
> 
> Lazy root calculation
> Few l1 and l2 cache misses
> Small proofs of inclusion/exclusion

Have you looked at the pruning system that my proofchains work implements?

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
I think that regardless of merits protocol or limitations of protocols,
once they become used and stable they merit their place as a BIP.
I'd like to submit OA as is on flavien's repository, and update or reword
things once it is there. (so he can ACK easily and we can keep track of
changes instead of using mails back and forth)
It would be useful to have other colored coin protocols as well. (EPOBC and
Colu)

On Thu, Aug 4, 2016 at 9:37 PM, Flavien Charlon <
flavien.charlon@coinprism.com> wrote:

> Hi,
>
> > I would love to see an RFC-style standard "multiple-colored-coin-protocol"
> written by reps from all of the major protocols and that meta-merges the
> features of these implementations
>
> Alex summarizes the situation well. Efforts to come up with a
> "multiple-colored-coin-protocol" have failed since the different
> protocols take different assumptions and different tradeoffs and are built
> for different use cases. In the end, we ended up exactly in the same
> situation as the well known XKCD comic strip about standards (
> https://xkcd.com/927/).
>
> > You can, however, provide a new OA2.0 protocol that improves upon these
> issues, and assure that upgraded wallets maintain support for both versions.
>
> I don't think there is a point in doing that. This would just result in
> having yet another "standard", which nobody uses. Open Assets 1.0 took 3
> years to get where it is today, and is used by many companies across the
> industry. It has well over 20 different implementations, some open source,
> some proprietary.
>
> The goal of this process is to have OA 1.0 becoming the BIP since this is
> the one people are using.
>
> > I was waiting for clarification on the Author thing, but Nicholas hasn't
> responded yet. I am unaware of any reason NOT to assign it, and there
> appear to be no objections, so let's call it BIP 160.
>
> Nicolas is proposing the BIP on my behalf. I'll ACK as needed but he can
> drive the discussions.
>
> Best,
> Flavien
>
> On Tue, Aug 2, 2016 at 6:25 PM, Alex Mizrahi via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> I would love to see an RFC-style standard "multiple-colored-coin-protocol"
>>> written by reps from all of the major protocols and that meta-merges the
>>> features of these implementations
>>>
>>
>> We actually tried to do that in 2014-2015, but that effort have failed...
>> Nobody was really interested in collaboration, each company only cared
>> about it's own product.
>> Especially Colu, they asked everyone for requirements, and then developed
>> a new protocol completely on their own without taking anyone's input.
>>
>> I'm not sure that merging the protocols makes sense, as some protocols
>> value simplicity, and a combined protocol cannot have this feature.
>>
>> I don't think there is much interest in a merged colored coin protocol
>> now.
>> Colu is moving away from colored coins, as far as I can tell.
>> CoinSpark is now doing MultiChain closed-source private blockchain.
>> CoinPrism also seems to be largely disinterested in colored coins.
>>
>> We (ChromaWay) won't mind replacing EPOBC with something better, our
>> software could always support multiple different kernels so adding a new
>> protocol isn't a big deal for us.
>>
>> So if somebody is interested in a new protocol please ping me.
>>
>> One of ideas I have is to decouple input-output mapping/multiplexing from
>> coloring.
>> So one layer will describe a mapping, e.g. "Inputs 0 and 1 should go into
>> outputs 0, 1 and 2".
>> In this case it will be possible to create more advanced protocols (e.g.
>> with support for 'smart contracts' and whatnot) while also keeping them
>> compatible with old ones to some extent, e.g. a wallet can safely engage in
>> p2ptrade or CoinJoin transactions without understanding all protocols used
>> in a transaction.
>>
>>
>>> - in collaboration with feedback from core developers that understand
>>> the direction the protocol will be taking and the issues to avoid.
>>>
>>
>> Core developers generally dislike things like colored coins, so I doubt
>> they are going to help.
>>
>> Blockstream is developing a sidechain with user-defined assets, so I
>> guess they see it as the preferred way of doing things:
>> https://elementsproject.org/elements/asset-issuance/
>>
>>
>>> As it stands, investors have to install multiple wallets to deal with
>>> these varying implementations.
>>>
>>
>> Actually this can be solved without making a new "merged protocol": one
>> can just implement a wallet which supports multiple protocols.
>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>

-------------------------------------
On Wed, 18 May 2016 10:00:44 +0200
Jonas Schnelli via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
wrote:

> Hi Lee
> 
> Thank you very much for the valuable input.
> I'm still processing your feedback....

[...]

> > Why have a fixed MAC length? I think the MAC length should be
> > inferred from the cipher + authentication mode. And the Poly1305
> > tag is 16 bytes.
> > 
> > *Unauthenticated Buffering*
> > Implementations are unlikely to (i.e. should not) process the
> > payload until authentication succeeds. Since the length field is 4
> > bytes, this means an implementation may have to buffer up to 4 GiB
> > of data _per connection_ before it can authenticate the length
> > field. If the outter length field were reduced to 2 or 3 bytes, the
> > unauthenticated buffering requirements drop to 64 KiB and 16 MiB
> > respectively. Inner messages already have their own length, so they
> > can span multiple encrypted blocks without other changes. This will
> > increase the bandwidth requirements when the size of a single
> > message exceeds 64 KiB or 16 MiB, since it will require multiple
> > authentication tags for that message. I think an additional 16
> > bytes per 16 MiB seems like a good tradeoff.
> >   
> 
> Good point.
> I have mentioned this now in the BIP but I think the BIP should allow
> message > 16 MiB.
> I leave the max. message length up to the implementation while keeping
> the 4 byte length on the protocol level.

I expect the implementation defined max size to work (SSH 2.0 does this
after all), but I want to make sure my suggestion is understood
completely.

There is a length field for the encrypted data, and length field(s)
inside of the encrypted data to indicate the length of the plaintext
Bitcoin messages. I am suggesting that the outter (encrypted) length
field be reduced, which will _not limit_ the length of Bitcoin
messages. For example, if a 1 GiB Bitcoin message needed to be sent
and the encrypted length field was 3 bytes - the sender is forced to
send a minimum of 64 MACs for this message. The tradeoff is allowing
the receiver to detect malformed data sooner and have a lower max
buffering window **against** slightly higher bandwidth and CPU
requirements due to the additional headers+MACs (the CPU requirements
should primarily be in "finalizing each Poly1305").

An alternative way to think about the suggestion is tunnelling Bitcoin
messages over TLS or SSH. TLS 1.2 has a 2-byte length field and SSH 2.0
a 4-byte length field, but neither prevents larger Bitcoin messages from
being tunnelled; the lengths are independent.

[...]

> 
> </jonas>
> 

Lee


-------------------------------------
"With a very powerful "Desktop" machine bitcoin-qt dominates CPU/GPU
resources."

That doesn't match my experience.

System responsiveness / user experience can suffer when running bitcoin-qt
on a spinning hard disk. Disk I/O load will cause the whole system to grind
and severely disrupt the user experience.

Move the Bitcoin data to an SSD, though, and it's an entirely different
story.

The initial blockchain synchronization / "catch up" is CPU and disk
intensive, but after initial sync I find bitcoin-qt uses only a trivial
amount of CPU to keep up with verifying new blocks and new transactions.

Running bitcoin-qt occasionally is a much more painful user experience than
running bitcoin-qt continuously.

I'm running Bitcoin Core v0.12.rc2 on an old dual core Pentium E2160 at
1.8GHz, 6GB RAM, 64 bit Windows 10, with the Bitcoin data on SSD. This
system is about 6 years old and was an economy model even when new. Not
what I would call a powerful system. I've only added RAM and the SSD.

On that machine I run two instances of Bitcoin-qt - one for mainnet, and
another for testnet, and an instance of bfgminer to manage a handful of USB
Block Eruptors for testnet mining. Both bitcoin-qt instances are typically
at their max of 25 connections (each). Total CPU load floats around 11%,
with only occasional spikes to 40% for a few seconds.  The mainnet
bitcoin-qt process uses about 700MB of RAM, testnet about 300MB.

This machine did fall into disk grinding paralysis during initial sync /
catchup with the v0.10 and v0.11 builds of bitcoin-qt, when the Bitcoin
data was on a spinning disk. Moving the Bitcoin data to an SSD drive had
the greatest impact on breaking the disk-bound whole-system paralysis.
Increasing the system RAM, upgrading to v0.12, and upgrading the OS to Win
10 all contributed smaller improvements.

It is possible to run a full node on a small desktop machine concurrent
with user apps. Just get the Bitcoin data off of spinning media and onto
SSD, make sure you have plenty of RAM, and leave bitcoin-qt running all the
time.

-Danny



On Wed, Feb 10, 2016 at 11:03 PM, Patrick Shirkey via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> On Thu, February 11, 2016 8:15 am, Chris Belcher via bitcoin-dev wrote:
> > I've been asked to post this to this mailing list too. It's time to
> > clear up some misconceptions floating around about full nodes.
> >
> > === Myth: There are only about 5500 full nodes worldwide ===
> >
> > This number comes from this and similar sites: https://bitnodes.21.co/
> > and it measured by trying to probe every nodes on their open ports.
> >
> > Problem is, not all nodes actually have open ports that can be probed.
> > Either because they are behind firewalls or because their users have
> > configured them to not listen for connections.
> >
> > Nobody knows how many full nodes there are, since many people don't know
> > how to forward ports behind a firewall, and bandwidth can be costly, its
> > quite likely that the number of nodes with closed ports is at least
> > another several thousand.
> >
> > Nodes with open ports are able to upload blocks to new full nodes. In
> > all other ways they are the same as nodes with closed ports. But because
> > open-port-nodes can be measured and closed-port-nodes cannot, some
> > members of the bitcoin community have been mistaken into believing that
> > open-port-nodes are that matters.
> >
> > === Myth: This number of nodes matters and/or is too low. ===
> >
> > Nodes with open ports are useful to the bitcoin network because they
> > help bootstrap new nodes by uploading historical blocks, they are a
> > measure of bandwidth capacity. Right now there is no shortage of
> > bandwidth capacity, and if there was it could be easily added by renting
> > cloud servers.
> >
> > The problem is not bandwidth or connections, but trust, security and
> > privacy. Let me explain.
> >
> > Full nodes are able to check that all of bitcoin's rules are being
> > followed. Rules like following the inflation schedule, no double
> > spending, no spending of coins that don't belong to the holder of the
> > private key and all the other rules required to make bitcoin work (e.g.
> > difficulty)
> >
> > Full nodes are what make bitcoin trustless. No longer do you have to
> > trust a financial institution like a bank or paypal, you can simply run
> > software on your own computer. To put simply, the only node that matters
> > is the one you use.
> >
> > === Myth: There is no incentive to run nodes, the network relies on
> > altruism ===
> >
> > It is very much in the individual bitcoin's users rational self interest
> > to run a full node and use it as their wallet.
> >
> > Using a full node as your wallet is the only way to know for sure that
> > none of bitcoin's rules have been broken. Rules like no coins were spent
> > not belonging to the owner, that no coins were spent twice, that no
> > inflation happens outside of the schedule and that all the rules needed
> > to make the system work are followed  (e.g. difficulty.) All other kinds
> > of wallet involve trusting a third party server.
> >
> > All these checks done by full nodes also increase the security. There
> > are many attacks possible against lightweight wallets that do not affect
> > full node wallets.
> >
> > This is not just mindless paranoia, there have been real world examples
> > where full node users were unaffected by turmoil in the rest of the
> > bitcoin ecosystem. The 4th July 2015 accidental chain fork effected many
> > kinds of wallets. Here is the wiki page on this event
> > https://en.bitcoin.it/wiki/July_2015_chain_forks#Wallet_Advice
> >
> > Notice how updated node software was completely unaffected by the fork.
> > All other wallets required either extra confirmations or checking that
> > the third-party institution was running the correct version.
> >
> > Full nodes wallets are also currently the most private way to use
> > Bitcoin, with nobody else learning which bitcoin addresses belong to
> > you. All other lightweight wallets leak information about which
> > addresses are yours because they must query third-party servers. The
> > Electrum servers will know which addresses belong to you and can link
> > them together. Despite bloom filtering, lightweight wallets based on
> > BitcoinJ do not provide much privacy against nodes who connected
> > directly to the wallet or wiretappers.
> >
> > For many use cases, such privacy may not be required. But an important
> > reason to run a full node and use it as a wallet is to get the full
> > privacy benefits.
> >
> > === Myth: I can just set up a node on a cloud server instance and leave
> > it ===
> >
> > To get the benefits of running a full node, you must use it as your
> > wallet, preferably on hardware you control.
> >
> > Most people who do this do not use a full node as their wallet.
> > Unfortunately because Bitcoin has a similar name to Bittorrent, some
> > people believe that upload capacity is the most important thing for a
> > healthy network. As I've explained above: bandwidth and connections are
> > not a problem today, trust, security and privacy are.
> >
> > === Myth: Running a full node is not recommended, most people should use
> > a lightweight client ===
> >
> > This was common advice in 2012, but since then the full node software
> > has vastly improved in terms of user experience.
> >
> > If you cannot spare the disk space to store the blockchain, you can
> > enable pruning as in:
> > https://bitcoin.org/en/release/v0.11.0#block-file-pruning. In Bitcoin
> > Core 0.12, pruning being enabled will leave the wallet enabled.
> > Altogether this should require less than 1.5GB of hard disk space.
> >
> > If you cannot spare the bandwidth to upload blocks to other nodes, there
> > are number of options to reduce or eliminate the bandwidth requirement
> > found in https://bitcoin.org/en/full-node#reduce-traffic . These include
> > limiting connections, bandwidth targetting and disabling listening.
> > Bitcoin Core 0.12 has the new option -blocksonly, where the node will
> > not download unconfirmed transaction and only download new blocks. This
> > more than halves the bandwidth usage at the expense of not seeing
> > unconfirmed transactions.
> >
> > Synchronizing the blockchain for a new node has improved since 2012 too.
> > Features like headers-first
> > (https://bitcoin.org/en/release/v0.10.0#faster-synchronization) and
> > libsecp256k1 have greatly improved the initial synchronization time.
> >
> > It can be further improved by setting -dbcache=6000 which keeps more of
> > the UTXO set in memory. It reduces the amount of time reading from disk
> > and therefore speeds up synchronization. Tests showed that the entire
> > blockchain can now be synchronized in less than _3 and a half hours_
> > (See
> > https://github.com/bitcoin/bitcoin/pull/6954#issuecomment-154993958)
> > Note that you'll need Bitcoin Core 0.12 or later to get all these
> > efficiency improvements.
> >
> > === How to run a full node as your wallet ===
> >
> > I think every moderate user of bitcoin would benefit by running a full
> > node and using it as their wallet. There are several ways to do this.
> >
> > * Run a bitcoin-qt full node (https://bitcoin.org/en/download).
> >
> > * Use wallet software that is backed by a full node e.g. Armory
> > (https://bitcoinarmory.com/) or JoinMarket
> > (https://github.com/AdamISZ/JMBinary/#jmbinary)
> >
> > * Use a lightweight wallet that connects only to your full node (e.g.
> > Multibit connecting only to your node running at home, Electrum
> > connecting only to your own Electrum server)
> >
> > So what are you waiting for? The benefits are many, the downsides are
> > not that bad. The more people do this, the more robust and healthy the
> > bitcoin ecosystem is.
> >
> >
>
> This is very useful information but from my experience it is not viable to
> have a full node running full time on a desktop system i.e sharing the
> system with a normal desktop workload.
>
> With a very powerful "Desktop" machine bitcoin-qt dominates CPU/GPU
> resources. Surely the majority of nodes NOT running open ports are being
> run on desktop systems.  It's likely that the vast majority of the
> "normal/desktop" user base are not going to setup dedicated machines to
> run a full node full time.
>
> It's likely that the vast majority of full nodes that are not running open
> ports are used occasionally when the user wants to make a transaction or
> "catch up" with the blockchain.
>
> That creates a divide between those who do have the resources to
> contribute to the system on a full time basis (minority) and those who do
> not (majority).
>
> Does the power of p2p decentralization lie with the vast majority or the
> "wealthy" resource rich minority?
>
> How will the move to 2MB hard fork affect the vast majority of nodes?
>
> For example Debian unstable currently provides the following:
>
> apt-cache  madison bitcoin-qt
> bitcoin-qt |   0.11.2-1 | http://ftp.lug.ro/debian/ unstable/main amd64
> Packages
>    bitcoin |   0.11.2-1 | http://ftp.lug.ro/debian/ unstable/main Sources
>
>
> The rollout affect of the hard fork on the entire bitcoin ecosystem is a
> difficult process to plan in advance. It's not viable to simply rely on
> press releases to encourage users to upgrade their nodes. The debacle with
> Pulse Audio during the mid 2000's should be a lesson for those who seek
> that route.
>
> Compare that to the requirements for spinning up "bitcoin-2.0" and
> enabling users to move their wallets to the new blockchain at their
> leisure.
>
> The ecosystem doesn't suffer from instant degradation. Bitcoin "brand"
> loyalty will ensure that users who want to move forward with the economic
> potential of the 2MB blocksize will be able to keep their existing funds
> safe while testing the waters with the new blocksize.
>
> After all Bitcoin is still the only game in town when it comes to scale
> and proven history of financial return.
>
> As the new blockchain builds momentum the old one will eventually become
> obsolete. However it may also become the digital equivalency of Silver and
> that is also a useful, profitable and viable alternative with a proven
> history of success.
>
>
>
>
> --
> Patrick Shirkey
> Boost Hardware Ltd
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Bitcoin Core version 0.13.0 is now available from:

  <https://bitcoin.org/bin/bitcoin-core-0.13.0/>

Or through bittorrent:

  magnet:?xt=urn:btih:35367ed2db6c41f7af9c1ed2dd54ae29d99bc632&dn=bitcoin-core-0.13.0&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2F

This is a new major version release, including new features, various bugfixes
and performance improvements, as well as updated translations.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoin/bitcoin/issues>

To receive security and update notifications, please subscribe to:

  <https://bitcoincore.org/en/list/announcements/join/>

Compatibility
==============

Microsoft ended support for Windows XP on [April 8th, 2014](https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support),
an OS initially released in 2001. This means that not even critical security
updates will be released anymore. Without security updates, using a bitcoin
wallet on a XP machine is irresponsible at least.

In addition to that, with 0.12.x there have been varied reports of Bitcoin Core
randomly crashing on Windows XP. It is [not clear](https://github.com/bitcoin/bitcoin/issues/7681#issuecomment-217439891)
what the source of these crashes is, but it is likely that upstream
libraries such as Qt are no longer being tested on XP.

We do not have time nor resources to provide support for an OS that is
end-of-life. From 0.13.0 on, Windows XP is no longer supported. Users are
suggested to upgrade to a newer verion of Windows, or install an alternative OS
that is supported.

No attempt is made to prevent installing or running the software on Windows XP,
you can still do so at your own risk, but do not expect it to work: do not
report issues about Windows XP to the issue tracker.

Notable changes
===============

Database cache memory increased
- --------------------------------

As a result of growth of the UTXO set, performance with the prior default
database cache of 100 MiB has suffered.
For this reason the default was changed to 300 MiB in this release.

For nodes on low-memory systems, the database cache can be changed back to
100 MiB (or to another value) by either:

- - Adding `dbcache=100` in bitcoin.conf
- - Changing it in the GUI under `Options → Size of database cache`

Note that the database cache setting has the most performance impact
during initial sync of a node, and when catching up after downtime.


bitcoin-cli: arguments privacy
- ------------------------------

The RPC command line client gained a new argument, `-stdin`
to read extra arguments from standard input, one per line until EOF/Ctrl-D.
For example:

    $ src/bitcoin-cli -stdin walletpassphrase
    mysecretcode
    120
    ..... press Ctrl-D here to end input
    $

It is recommended to use this for sensitive information such as wallet
passphrases, as command-line arguments can usually be read from the process
table by any user on the system.


C++11 and Python 3
- ------------------

Various code modernizations have been done. The Bitcoin Core code base has
started using C++11. This means that a C++11-capable compiler is now needed for
building. Effectively this means GCC 4.7 or higher, or Clang 3.3 or higher.

When cross-compiling for a target that doesn't have C++11 libraries, configure with
`./configure --enable-glibc-back-compat ... LDFLAGS=-static-libstdc++`.

For running the functional tests in `qa/rpc-tests`, Python3.4 or higher is now
required.


Linux ARM builds
- ----------------

Due to popular request, Linux ARM builds have been added to the uploaded
executables.

The following extra files can be found in the download directory or torrent:

- - `bitcoin-${VERSION}-arm-linux-gnueabihf.tar.gz`: Linux binaries for the most
  common 32-bit ARM architecture.
- - `bitcoin-${VERSION}-aarch64-linux-gnu.tar.gz`: Linux binaries for the most
  common 64-bit ARM architecture.

ARM builds are still experimental. If you have problems on a certain device or
Linux distribution combination please report them on the bug tracker, it may be
possible to resolve them.

Note that Android is not considered ARM Linux in this context. The executables
are not expected to work out of the box on Android.


Compact Block support (BIP 152)
- -------------------------------

Support for block relay using the Compact Blocks protocol has been implemented
in PR 8068.

The primary goal is reducing the bandwidth spikes at relay time, though in many
cases it also reduces propagation delay. It is automatically enabled between
compatible peers.
[BIP 152](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki)

As a side-effect, ordinary non-mining nodes will download and upload blocks
faster if those blocks were produced by miners using similar transaction
filtering policies. This means that a miner who produces a block with many
transactions discouraged by your node will be relayed slower than one with
only transactions already in your memory pool. The overall effect of such
relay differences on the network may result in blocks which include widely-
discouraged transactions losing a stale block race, and therefore miners may
wish to configure their node to take common relay policies into consideration.


Hierarchical Deterministic Key Generation
- -----------------------------------------
Newly created wallets will use hierarchical deterministic key generation
according to BIP32 (keypath m/0'/0'/k').
Existing wallets will still use traditional key generation.

Backups of HD wallets, regardless of when they have been created, can
therefore be used to re-generate all possible private keys, even the
ones which haven't already been generated during the time of the backup.
**Attention:** Encrypting the wallet will create a new seed which requires
a new backup!

Wallet dumps (created using the `dumpwallet` RPC) will contain the deterministic
seed. This is expected to allow future versions to import the seed and all
associated funds, but this is not yet implemented.

HD key generation for new wallets can be disabled by `-usehd=0`. Keep in
mind that this flag only has affect on newly created wallets.
You can't disable HD key generation once you have created a HD wallet.

There is no distinction between internal (change) and external keys.

HD wallets are incompatible with older versions of Bitcoin Core.

[Pull request](https://github.com/bitcoin/bitcoin/pull/8035/files), [BIP 32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki)


Segregated Witness
- ------------------

The code preparations for Segregated Witness ("segwit"), as described in [BIP
141](https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki), [BIP
143](https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki), [BIP
144](https://github.com/bitcoin/bips/blob/master/bip-0144.mediawiki), and [BIP
145](https://github.com/bitcoin/bips/blob/master/bip-0145.mediawiki) are
finished and included in this release.  However, BIP 141 does not yet specify
activation parameters on mainnet, and so this release does not support segwit
use on mainnet.  Testnet use is supported, and after BIP 141 is updated with
proposed parameters, a future release of Bitcoin Core is expected that
implements those parameters for mainnet.

Furthermore, because segwit activation is not yet specified for mainnet,
version 0.13.0 will behave similarly as other pre-segwit releases even after a
future activation of BIP 141 on the network.  Upgrading from 0.13.0 will be
required in order to utilize segwit-related features on mainnet (such as signal
BIP 141 activation, mine segwit blocks, fully validate segwit blocks, relay
segwit blocks to other segwit nodes, and use segwit transactions in the
wallet, etc).


Mining transaction selection ("Child Pays For Parent")
- ------------------------------------------------------

The mining transaction selection algorithm has been replaced with an algorithm
that selects transactions based on their feerate inclusive of unconfirmed
ancestor transactions.  This means that a low-fee transaction can become more
likely to be selected if a high-fee transaction that spends its outputs is
relayed.

With this change, the `-blockminsize` command line option has been removed.

The command line option `-blockmaxsize` remains an option to specify the
maximum number of serialized bytes in a generated block.  In addition, the new
command line option `-blockmaxweight` has been added, which specifies the
maximum "block weight" of a generated block, as defined by [BIP 141 (Segregated
Witness)] (https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki).

In preparation for Segregated Witness, the mining algorithm has been modified
to optimize transaction selection for a given block weight, rather than a given
number of serialized bytes in a block.  In this release, transaction selection
is unaffected by this distinction (as BIP 141 activation is not supported on
mainnet in this release, see above), but in future releases and after BIP 141
activation, these calculations would be expected to differ.

For optimal runtime performance, miners using this release should specify
`-blockmaxweight` on the command line, and not specify `-blockmaxsize`.
Additionally (or only) specifying `-blockmaxsize`, or relying on default
settings for both, may result in performance degradation, as the logic to
support `-blockmaxsize` performs additional computation to ensure that
constraint is met.  (Note that for mainnet, in this release, the equivalent
parameter for `-blockmaxweight` would be four times the desired
`-blockmaxsize`.  See [BIP 141]
(https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki) for additional
details.)

In the future, the `-blockmaxsize` option may be removed, as block creation is
no longer optimized for this metric.  Feedback is requested on whether to
deprecate or keep this command line option in future releases.


Reindexing changes
- ------------------

In earlier versions, reindexing did validation while reading through the block
files on disk. These two have now been split up, so that all blocks are known
before validation starts. This was necessary to make certain optimizations that
are available during normal synchronizations also available during reindexing.

The two phases are distinct in the Bitcoin-Qt GUI. During the first one,
"Reindexing blocks on disk" is shown. During the second (slower) one,
"Processing blocks on disk" is shown.

It is possible to only redo validation now, without rebuilding the block index,
using the command line option `-reindex-chainstate` (in addition to
`-reindex` which does both). This new option is useful when the blocks on disk
are assumed to be fine, but the chainstate is still corrupted. It is also
useful for benchmarks.


Removal of internal miner
- --------------------------

As CPU mining has been useless for a long time, the internal miner has been
removed in this release, and replaced with a simpler implementation for the
test framework.

The overall result of this is that `setgenerate` RPC call has been removed, as
well as the `-gen` and `-genproclimit` command-line options.

For testing, the `generate` call can still be used to mine a block, and a new
RPC call `generatetoaddress` has been added to mine to a specific address. This
works with wallet disabled.


New bytespersigop implementation
- --------------------------------

The former implementation of the bytespersigop filter accidentally broke bare
multisig (which is meant to be controlled by the `permitbaremultisig` option),
since the consensus protocol always counts these older transaction forms as 20
sigops for backwards compatibility. Simply fixing this bug by counting more
accurately would have reintroduced a vulnerability. It has therefore been
replaced with a new implementation that rather than filter such transactions,
instead treats them (for fee purposes only) as if they were in fact the size
of a transaction actually using all 20 sigops.


Low-level P2P changes
- ----------------------

- - The optional new p2p message "feefilter" is implemented and the protocol
  version is bumped to 70013. Upon receiving a feefilter message from a peer,
  a node will not send invs for any transactions which do not meet the filter
  feerate. [BIP 133](https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki)

- - The P2P alert system has been removed in PR #7692 and the `alert` P2P message
  is no longer supported.

- - The transaction relay mechanism used to relay one quarter of all transactions
  instantly, while queueing up the rest and sending them out in batch. As
  this resulted in chains of dependent transactions being reordered, it
  systematically hurt transaction relay. The relay code was redesigned in PRs
  \#7840 and #8082, and now always batches transactions announcements while also
  sorting them according to dependency order. This significantly reduces orphan
  transactions. To compensate for the removal of instant relay, the frequency of
  batch sending was doubled for outgoing peers.

- - Since PR #7840 the BIP35 `mempool` command is also subject to batch processing.
  Also the `mempool` message is no longer handled for non-whitelisted peers when
  `NODE_BLOOM` is disabled through `-peerbloomfilters=0`.

- - The maximum size of orphan transactions that are kept in memory until their
  ancestors arrive has been raised in PR #8179 from 5000 to 99999 bytes. They
  are now also removed from memory when they are included in a block, conflict
  with a block, and time out after 20 minutes.

- - We respond at most once to a getaddr request during the lifetime of a
  connection since PR #7856.

- - Connections to peers who have recently been the first one to give us a valid
  new block or transaction are protected from disconnections since PR #8084.


Low-level RPC changes
- ----------------------

- - RPC calls have been added to output detailed statistics for individual mempool
  entries, as well as to calculate the in-mempool ancestors or descendants of a
  transaction: see `getmempoolentry`, `getmempoolancestors`, `getmempooldescendants`.

- - `gettxoutsetinfo` UTXO hash (`hash_serialized`) has changed. There was a divergence between
  32-bit and 64-bit platforms, and the txids were missing in the hashed data. This has been
  fixed, but this means that the output will be different than from previous versions.

- - Full UTF-8 support in the RPC API. Non-ASCII characters in, for example,
  wallet labels have always been malformed because they weren't taken into account
  properly in JSON RPC processing. This is no longer the case. This also affects
  the GUI debug console.

- - Asm script outputs replacements for OP_NOP2 and OP_NOP3

  - OP_NOP2 has been renamed to OP_CHECKLOCKTIMEVERIFY by [BIP 
65](https://github.com/bitcoin/bips/blob/master/bip-0065.mediawiki)

  - OP_NOP3 has been renamed to OP_CHECKSEQUENCEVERIFY by [BIP 
112](https://github.com/bitcoin/bips/blob/master/bip-0112.mediawiki)

  - The following outputs are affected by this change:

    - RPC `getrawtransaction` (in verbose mode)
    - RPC `decoderawtransaction`
    - RPC `decodescript`
    - REST `/rest/tx/` (JSON format)
    - REST `/rest/block/` (JSON format when including extended tx details)
    - `bitcoin-tx -json`

- - The sorting of the output of the `getrawmempool` output has changed.

- - New RPC commands: `generatetoaddress`, `importprunedfunds`, `removeprunedfunds`, `signmessagewithprivkey`,
  `getmempoolancestors`, `getmempooldescendants`, `getmempoolentry`,
  `createwitnessaddress`, `addwitnessaddress`.

- - Removed RPC commands: `setgenerate`, `getgenerate`.

- - New options were added to `fundrawtransaction`: `includeWatching`, `changeAddress`, `changePosition` and `feeRate`.


Low-level ZMQ changes
- ----------------------

- - Each ZMQ notification now contains an up-counting sequence number that allows
  listeners to detect lost notifications.
  The sequence number is always the last element in a multi-part ZMQ notification and
  therefore backward compatible. Each message type has its own counter.
  PR [#7762](https://github.com/bitcoin/bitcoin/pull/7762).


0.13.0 Change log
=================

Detailed release notes follow. This overview includes changes that affect
behavior, not code moves, refactors and string updates. For convenience in locating
the code changes and accompanying discussion, both the pull request and
git merge commit are mentioned.

### RPC and other APIs

- - #7156 `9ee02cf` Remove cs_main lock from `createrawtransaction` (laanwj)
- - #7326 `2cd004b` Fix typo, wrong information in gettxout help text (paveljanik)
- - #7222 `82429d0` Indicate which transactions are signaling opt-in RBF (sdaftuar)
- - #7480 `b49a623` Changed getnetworkhps value to double to avoid overflow (instagibbs)
- - #7550 `8b958ab` Input-from-stdin mode for bitcoin-cli (laanwj)
- - #7670 `c9a1265` Use cached block hash in blockToJSON() (rat4)
- - #7726 `9af69fa` Correct importaddress help reference to importpubkey (CypherGrue)
- - #7766 `16555b6` Register calls where they are defined (laanwj)
- - #7797 `e662a76` Fix generatetoaddress failing to parse address (mruddy)
- - #7774 `916b15a` Add versionHex in getblock and getblockheader JSON results (mruddy)
- - #7863 `72c54e3` Getblockchaininfo: make bip9_softforks an object, not an array (rustyrussell)
- - #7842 `d97101e` Do not print minping time in getpeerinfo when no ping received yet (paveljanik)
- - #7518 `be14ca5` Add multiple options to fundrawtransaction (promag)
- - #7756 `9e47fce` Add cursor to iterate over utxo set, use this in `gettxoutsetinfo` (laanwj)
- - #7848 `88616d2` Divergence between 32- and 64-bit when hashing >4GB affects `gettxoutsetinfo` (laanwj)
- - #7827 `4205ad7` Speed up `getchaintips` (mrbandrews)
- - #7762 `a1eb344` Append a message sequence number to every ZMQ notification (jonasschnelli)
- - #7688 `46880ed` List solvability in listunspent output and improve help (sipa)
- - #7926 `5725807` Push back `getaddednodeinfo` dead value (instagibbs)
- - #7953 `0630353` Create `signmessagewithprivkey` rpc (achow101)
- - #8049 `c028c7b` Expose information on whether transaction relay is enabled in `getnetworkinfo` (laanwj)
- - #7967 `8c1e49b` Add feerate option to `fundrawtransaction` (jonasschnelli)
- - #8118 `9b6a48c` Reduce unnecessary hashing in `signrawtransaction` (jonasnick)
- - #7957 `79004d4` Add support for transaction sequence number (jonasschnelli)
- - #8153 `75ec320` `fundrawtransaction` feeRate: Use BTC/kB (MarcoFalke)
- - #7292 `7ce9ac5` Expose ancestor/descendant information over RPC (sdaftuar)
- - #8171 `62fcf27` Fix createrawtx sequence number unsigned int parsing (jonasschnelli)
- - #7892 `9c3d0fa` Add full UTF-8 support to RPC (laanwj)
- - #8317 `304eff3` Don't use floating point in rpcwallet (MarcoFalke)
- - #8258 `5a06ebb` Hide softfork in `getblockchaininfo` if timeout is 0 (jl2012)
- - #8244 `1922e5a` Remove unnecessary LOCK(cs_main) in getrawmempool (dcousens)

### Block and transaction handling

- - #7056 `6a07208` Save last db read (morcos)
- - #6842 `0192806` Limitfreerelay edge case bugfix (ptschip)
- - #7084 `11d74f6` Replace maxFeeRate of 10000*minRelayTxFee with maxTxFee in mempool (MarcoFalke)
- - #7539 `9f33dba` Add tags to mempool's mapTx indices (sdaftuar)
- - #7592 `26a2a72` Re-remove ERROR logging for mempool rejects (laanwj)
- - #7187 `14d6324` Keep reorgs fast for SequenceLocks checks (morcos)
- - #7594 `01f4267` Mempool: Add tracking of ancestor packages (sdaftuar)
- - #7904 `fc9e334` Txdb: Fix assert crash in new UTXO set cursor (laanwj)
- - #7927 `f9c2ac7` Minor changes to dbwrapper to simplify support for other databases (laanwj)
- - #7933 `e26b620` Fix OOM when deserializing UTXO entries with invalid length (sipa)
- - #8020 `5e374f7` Use SipHash-2-4 for various non-cryptographic hashes (sipa)
- - #8076 `d720980` VerifyDB: don't check blocks that have been pruned (sdaftuar)
- - #8080 `862fd24` Do not use mempool for GETDATA for tx accepted after the last mempool req (gmaxwell)
- - #7997 `a82f033` Replace mapNextTx with slimmer setSpends (kazcw)
- - #8220 `1f86d64` Stop trimming when mapTx is empty (sipa)
- - #8273 `396f9d6` Bump `-dbcache` default to 300MiB (laanwj)
- - #7225 `eb33179` Eliminate unnecessary call to CheckBlock (sdaftuar)
- - #7907 `006cdf6` Optimize and Cleanup CScript::FindAndDelete (pstratem)
- - #7917 `239d419` Optimize reindex (sipa)
- - #7763 `3081fb9` Put hex-encoded version in UpdateTip (sipa)
- - #8149 `d612837` Testnet-only segregated witness (sipa)
- - #8305 `3730393` Improve handling of unconnecting headers (sdaftuar)
- - #8363 `fca1a41` Rename "block cost" to "block weight" (sdaftuar)
- - #8381 `f84ee3d` Make witness v0 outputs non-standard (jl2012)
- - #8364 `3f65ba2` Treat high-sigop transactions as larger rather than rejecting them (sipa)

### P2P protocol and network code

- - #6589 `dc0305d` Log bytes recv/sent per command (jonasschnelli)
- - #7164 `3b43cad` Do not download transactions during initial blockchain sync (ptschip)
- - #7458 `898fedf` peers.dat, banlist.dat recreated when missing (kirkalx)
- - #7637 `3da5d1b` Fix memleak in TorController (laanwj, jonasschnelli)
- - #7553 `9f14e5a` Remove vfReachable and modify IsReachable to only use vfLimited (pstratem)
- - #7708 `9426632` De-neuter NODE_BLOOM (pstratem)
- - #7692 `29b2be6` Remove P2P alert system (btcdrak)
- - #7542 `c946a15` Implement "feefilter" P2P message (morcos)
- - #7573 `352fd57` Add `-maxtimeadjustment` command line option (mruddy)
- - #7570 `232592a` Add IPv6 Link-Local Address Support (mruddy)
- - #7874 `e6a4d48` Improve AlreadyHave (morcos)
- - #7856 `64e71b3` Only send one GetAddr response per connection (gmaxwell)
- - #7868 `7daa3ad` Split DNS resolving functionality out of net structures (theuni)
- - #7919 `7617682` Fix headers announcements edge case (sdaftuar)
- - #7514 `d9594bf` Fix IsInitialBlockDownload for testnet (jmacwhyte)
- - #7959 `03cf6e8` fix race that could fail to persist a ban (kazcw)
- - #7840 `3b9a0bf` Several performance and privacy improvements to inv/mempool handling (sipa)
- - #8011 `65aecda` Don't run ThreadMessageHandler at lowered priority (kazcw)
- - #7696 `5c3f8dd` Fix de-serialization bug where AddrMan is left corrupted (EthanHeilman)
- - #7932 `ed749bd` CAddrMan::Deserialize handle corrupt serializations better (pstratem)
- - #7906 `83121cc` Prerequisites for p2p encapsulation changes (theuni)
- - #8033 `18436d8` Fix Socks5() connect failures to be less noisy and less unnecessarily scary (wtogami)
- - #8082 `01d8359` Defer inserting into maprelay until just before relaying (gmaxwell)
- - #7960 `6a22373` Only use AddInventoryKnown for transactions (sdaftuar)
- - #8078 `2156fa2` Disable the mempool P2P command when bloom filters disabled (petertodd)
- - #8065 `67c91f8` Addrman offline attempts (gmaxwell)
- - #7703 `761cddb` Tor: Change auth order to only use password auth if -torpassword (laanwj)
- - #8083 `cd0c513` Add support for dnsseeds with option to filter by servicebits (jonasschnelli)
- - #8173 `4286f43` Use SipHash for node eviction (sipa)
- - #8154 `1445835` Drop vAddrToSend after sending big addr message (kazcw)
- - #7749 `be9711e` Enforce expected outbound services (sipa)
- - #8208 `0a64777` Do not set extra flags for unfiltered DNS seed results (sipa)
- - #8084 `e4bb4a8` Add recently accepted blocks and txn to AttemptToEvictConnection (gmaxwell)
- - #8113 `3f89a53` Rework addnode behaviour (sipa)
- - #8179 `94ab58b` Evict orphans which are included or precluded by accepted blocks (gmaxwell)
- - #8068 `e9d76a1` Compact Blocks (TheBlueMatt)
- - #8204 `0833894` Update petertodd's testnet seed (petertodd)
- - #8247 `5cd35d3` Mark my dnsseed as supporting filtering (sipa)
- - #8275 `042c323` Remove bad chain alert partition check (btcdrak)
- - #8271 `1bc9c80` Do not send witnesses in cmpctblock (sipa)
- - #8312 `ca40ef6` Fix mempool DoS vulnerability from malleated transactions (sdaftuar)
- - #7180 `16ccb74` Account for `sendheaders` `verack` messages (laanwj)
- - #8102 `425278d` Bugfix: use global ::fRelayTxes instead of CNode in version send (sipa)
- - #8408 `b7e2011` Prevent fingerprinting, disk-DoS with compact blocks (sdaftuar)

### Build system

- - #7302 `41f1a3e` C++11 build/runtime fixes (theuni)
- - #7322 `fd9356b` c++11: add scoped enum fallbacks to CPPFLAGS rather than defining them locally (theuni)
- - #7441 `a6771fc` Use Debian 8.3 in gitian build guide (fanquake)
- - #7349 `152a821` Build against system UniValue when available (luke-jr)
- - #7520 `621940e` LibreSSL doesn't define OPENSSL_VERSION, use LIBRESSL_VERSION_TEXT instead (paveljanik)
- - #7528 `9b9bfce` autogen.sh: warn about needing autoconf if autoreconf is not found (knocte)
- - #7504 `19324cf` Crystal clean make clean (paveljanik)
- - #7619 `18b3f1b` Add missing sudo entry in gitian VM setup (btcdrak)
- - #7616 `639ec58`  [depends] Delete unused patches  (MarcoFalke)
- - #7658 `c15eb28` Add curl to Gitian setup instructions (btcdrak)
- - #7710 `909b72b` [Depends] Bump miniupnpc and config.guess+sub (fanquake)
- - #7723 `5131005` build: python 3 compatibility (laanwj)
- - #7477 `28ad4d9` Fix quoting of copyright holders in configure.ac (domob1812)
- - #7711 `a67bc5e` [build-aux] Update Boost & check macros to latest serials (fanquake)
- - #7788 `4dc1b3a` Use relative paths instead of absolute paths in protoc calls (paveljanik)
- - #7809 `bbd210d` depends: some base fixes/changes (theuni)
- - #7603 `73fc922` Build System: Use PACKAGE_TARNAME in NSIS script (JeremyRand)
- - #7905 `187186b` test: move accounting_tests and rpc_wallet_tests to wallet/test (laanwj)
- - #7911 `351abf9` leveldb: integrate leveldb into our buildsystem (theuni)
- - #7944 `a407807` Re-instate TARGET_OS=linux in configure.ac. Removed by 351abf9e035 (randy-waterhouse)
- - #7920 `c3e3cfb` Switch Travis to Trusty (theuni)
- - #7954 `08b37c5` build: quiet annoying warnings without adding new ones (theuni)
- - #7165 `06162f1` build: Enable C++11 in build, require C++11 compiler (laanwj)
- - #7982 `559fbae` build: No need to check for leveldb atomics (theuni)
- - #8002 `f9b4582` [depends] Add -stdlib=libc++ to darwin CXX flags (fanquake)
- - #7993 `6a034ed` [depends] Bump Freetype, ccache, ZeroMQ, miniupnpc, expat (fanquake)
- - #8167 `19ea173` Ship debug tarballs/zips with debug symbols (theuni)
- - #8175 `f0299d8` Add --disable-bench to config flags for windows (laanwj)
- - #7283 `fd9881a` [gitian] Default reference_datetime to commit author date (MarcoFalke)
- - #8181 `9201ce8` Get rid of `CLIENT_DATE` (laanwj)
- - #8133 `fde0ac4` Finish up out-of-tree changes (theuni)
- - #8188 `65a9d7d` Add armhf/aarch64 gitian builds (theuni)
- - #8194 `cca1c8c` [gitian] set correct PATH for wrappers (MarcoFalke)
- - #8198 `5201614` Sync ax_pthread with upstream draft4 (fanquake)
- - #8210 `12a541e` [Qt] Bump to Qt5.6.1 (jonasschnelli)
- - #8285 `da50997` windows: Add testnet link to installer (laanwj)
- - #8304 `0cca2fe` [travis] Update SDK_URL (MarcoFalke)
- - #8310 `6ae20df` Require boost for bench (theuni)
- - #8315 `2e51590` Don't require sudo for Linux (theuni)
- - #8314 `67caef6` Fix pkg-config issues for 0.13 (theuni)
- - #8373 `1fe7f40` Fix OSX non-deterministic dmg (theuni)
- - #8358 `cfd1280` Gbuild: Set memory explicitly (default is too low) (MarcoFalke)

### GUI

- - #7154 `00b4b8d` Add InMempool() info to transaction details (jonasschnelli)
- - #7068 `5f3c670` [RPC-Tests] add simple way to run rpc test over QT clients (jonasschnelli)
- - #7218 `a1c185b` Fix misleading translation (MarcoFalke)
- - #7214 `be9a9a3` qt5: Use the fixed font the system recommends (MarcoFalke)
- - #7256 `08ab906` Add note to coin control dialog QT5 workaround (fanquake)
- - #7255 `e289807` Replace some instances of formatWithUnit with formatHtmlWithUnit (fanquake)
- - #7317 `3b57e9c` Fix RPCTimerInterface ordering issue (jonasschnelli)
- - #7327 `c079d79` Transaction View: LastMonth calculation fixed (crowning-)
- - #7334 `e1060c5` coincontrol workaround is still needed in qt5.4 (fixed in qt5.5) (MarcoFalke)
- - #7383 `ae2db67` Rename "amount" to "requested amount" in receive coins table (jonasschnelli)
- - #7396 `cdcbc59` Add option to increase/decrease font size in the console window (jonasschnelli)
- - #7437 `9645218` Disable tab navigation for peers tables (Kefkius)
- - #7604 `354b03d` build: Remove spurious dollar sign. Fixes #7189 (dooglus)
- - #7605 `7f001bd` Remove openssl info from init/log and from Qt debug window (jonasschnelli)
- - #7628 `87d6562` Add 'copy full transaction details' option (ericshawlinux)
- - #7613 `3798e5d` Add autocomplete to bitcoin-qt's console window (GamerSg)
- - #7668 `b24266c` Fix history deletion bug after font size change (achow101)
- - #7680 `41d2dfa` Remove reflection from `about` icon (laanwj)
- - #7686 `f034bce` Remove 0-fee from send dialog (MarcoFalke)
- - #7506 `b88e0b0` Use CCoinControl selection in CWallet::FundTransaction (promag)
- - #7732 `0b98dd7` Debug window: replace "Build date" with "Datadir" (jonasschnelli)
- - #7761 `60db51d` remove trailing output-index from transaction-id (jonasschnelli)
- - #7772 `6383268` Clear the input line after activating autocomplete (paveljanik)
- - #7925 `f604bf6` Fix out-of-tree GUI builds (laanwj)
- - #7939 `574ddc6` Make it possible to show details for multiple transactions (laanwj)
- - #8012 `b33824b` Delay user confirmation of send (Tyler-Hardin)
- - #8006 `7c8558d` Add option to disable the system tray icon (Tyler-Hardin)
- - #8046 `169d379` Fix Cmd-Q / Menu Quit shutdown on OSX (jonasschnelli)
- - #8042 `6929711` Don't allow to open the debug window during splashscreen & verification state (jonasschnelli)
- - #8014 `77b49ac` Sort transactions by date (Tyler-Hardin)
- - #8073 `eb2f6f7` askpassphrasedialog: Clear pass fields on accept (rat4)
- - #8129 `ee1533e` Fix RPC console auto completer (UdjinM6)
- - #7636 `fb0ac48` Add bitcoin address label to request payment QR code (makevoid)
- - #8231 `760a6c7` Fix a bug where the SplashScreen will not be hidden during startup (jonasschnelli)
- - #8256 `af2421c` BUG: bitcoin-qt crash (fsb4000)
- - #8257 `ff03c50` Do not ask a UI question from bitcoind (sipa)
- - #8288 `91abb77` Network-specific example address (laanwj)
- - #7707 `a914968` UI support for abandoned transactions (jonasschnelli)
- - #8207 `f7a403b` Add a link to the Bitcoin-Core repository and website to the About Dialog (MarcoFalke)
- - #8281 `6a87eb0` Remove client name from debug window (laanwj)
- - #8407 `45eba4b` Add dbcache migration path (jonasschnelli)

### Wallet

- - #7262 `fc08994` Reduce inefficiency of GetAccountAddress() (dooglus)
- - #7537 `78e81b0` Warn on unexpected EOF while salvaging wallet (laanwj)
- - #7521 `3368895` Don't resend wallet txs that aren't in our own mempool (morcos)
- - #7576 `86a1ec5` Move wallet help string creation to CWallet (jonasschnelli)
- - #7577 `5b3b5a7` Move "load wallet phase" to CWallet (jonasschnelli)
- - #7608 `0735c0c` Move hardcoded file name out of log messages (MarcoFalke)
- - #7649 `4900641` Prevent multiple calls to CWallet::AvailableCoins (promag)
- - #7646 `e5c3511` Fix lockunspent help message (promag)
- - #7558 `b35a591` Add import/removeprunedfunds rpc call (instagibbs)
- - #6215 `48c5adf` add bip32 pub key serialization (jonasschnelli)
- - #7913 `bafd075` Fix for incorrect locking in GetPubKey() (keystore.cpp) (yurizhykin)
- - #8036 `41138f9` init: Move berkeleydb version reporting to wallet (laanwj)
- - #8028 `373b50d` Fix insanity of CWalletDB::WriteTx and CWalletTx::WriteToDisk (pstratem)
- - #8061 `f6b7df3` Improve Wallet encapsulation (pstratem)
- - #7891 `950be19` Always require OS randomness when generating secret keys (sipa)
- - #7689 `b89ef13` Replace OpenSSL AES with ctaes-based version (sipa)
- - #7825 `f972b04` Prevent multiple calls to ExtractDestination (pedrobranco)
- - #8137 `243ac0c` Improve CWallet API with new AccountMove function (pstratem)
- - #8142 `52c3f34` Improve CWallet API  with new GetAccountPubkey function (pstratem)
- - #8035 `b67a472` Add simplest BIP32/deterministic key generation implementation (jonasschnelli)
- - #7687 `a6ddb19` Stop treating importaddress'ed scripts as change (sipa)
- - #8298 `aef3811` wallet: Revert input selection post-pruning (laanwj)
- - #8324 `bc94b87` Keep HD seed during salvagewallet (jonasschnelli)
- - #8323 `238300b` Add HD keypath to CKeyMetadata, report metadata in validateaddress (jonasschnelli)
- - #8367 `3b38a6a` Ensure <0.13 clients can't open HD wallets (jonasschnelli)
- - #8378 `ebea651` Move SetMinVersion for FEATURE_HD to SetHDMasterKey (pstratem)
- - #8390 `73adfe3` Correct hdmasterkeyid/masterkeyid name confusion (jonasschnelli)
- - #8206 `18b8ee1` Add HD xpriv to dumpwallet (jonasschnelli)
- - #8389 `c3c82c4` Create a new HD seed after encrypting the wallet (jonasschnelli)

### Tests and QA

- - #7320 `d3dfc6d` Test walletpassphrase timeout (MarcoFalke)
- - #7208 `47c5ed1` Make max tip age an option instead of chainparam (laanwj)
- - #7372 `21376af` Trivial: [qa] wallet: Print maintenance (MarcoFalke)
- - #7280 `668906f` [travis] Fail when documentation is outdated (MarcoFalke)
- - #7177 `93b0576` [qa] Change default block priority size to 0 (MarcoFalke)
- - #7236 `02676c5` Use createrawtx locktime parm in txn_clone (dgenr8)
- - #7212 `326ffed` Adds unittests for CAddrMan and CAddrinfo, removes source of non-determinism (EthanHeilman)
- - #7490 `d007511` tests: Remove May15 test (laanwj)
- - #7531 `18cb2d5` Add bip68-sequence.py to extended rpc tests (btcdrak)
- - #7536 `ce5fc02` test: test leading spaces for ParseHex (laanwj)
- - #7620 `1b68de3` [travis] Only run check-doc.py once (MarcoFalke)
- - #7455 `7f96671` [travis] Exit early when check-doc.py fails (MarcoFalke)
- - #7667 `56d2c4e` Move GetTempPath() to testutil (musalbas)
- - #7517 `f1ca891` test: script_error checking in script_invalid tests (laanwj)
- - #7684 `3d0dfdb` Extend tests (MarcoFalke)
- - #7697 `622fe6c` Tests: make prioritise_transaction.py more robust (sdaftuar)
- - #7709 `efde86b` Tests: fix missing import in mempool_packages (sdaftuar)
- - #7702 `29e1131` Add tests verifychain, lockunspent, getbalance, listsinceblock (MarcoFalke)
- - #7720 `3b4324b` rpc-test: Normalize assert() (MarcoFalke)
- - #7757 `26794d4` wallet: Wait for reindex to catch up (MarcoFalke)
- - #7764 `a65b36c` Don't run pruning.py twice (MarcoFalke)
- - #7773 `7c80e72` Fix comments in tests (btcdrak)
- - #7489 `e9723cb` tests: Make proxy_test work on travis servers without IPv6 (laanwj)
- - #7801 `70ac71b` Remove misleading "errorString syntax" (MarcoFalke)
- - #7803 `401c65c` maxblocksinflight: Actually enable test (MarcoFalke)
- - #7802 `3bc71e1` httpbasics: Actually test second connection (MarcoFalke)
- - #7849 `ab8586e` tests: add varints_bitpatterns test (laanwj)
- - #7846 `491171f` Clean up lockorder data of destroyed mutexes (sipa)
- - #7853 `6ef5e00` py2: Unfiddle strings into bytes explicitly (MarcoFalke)
- - #7878 `53adc83` [test] bctest.py: Revert faa41ee (MarcoFalke)
- - #7798 `cabba24` [travis] Print the commit which was evaluated (MarcoFalke)
- - #7833 `b1bf511` tests: Check Content-Type header returned from RPC server (laanwj)
- - #7851 `fa9d86f` pull-tester: Don't mute zmq ImportError (MarcoFalke)
- - #7822 `0e6fd5e` Add listunspent() test for spendable/unspendable UTXO (jpdffonseca)
- - #7912 `59ad568` Tests: Fix deserialization of reject messages (sdaftuar)
- - #7941 `0ea3941` Fixing comment in script_test.json test case (Christewart)
- - #7807 `0ad1041` Fixed miner test values, gave constants for less error-prone values (instagibbs)
- - #7980 `88b77c7` Smartfees: Properly use ordered dict (MarcoFalke)
- - #7814 `77b637f` Switch to py3 (MarcoFalke)
- - #8030 `409a8a1` Revert fatal-ness of missing python-zmq (laanwj)
- - #8018 `3e90fe6` Autofind rpc tests --srcdir (jonasschnelli)
- - #8016 `5767e80` Fix multithread CScheduler and reenable test (paveljanik)
- - #7972 `423ca30` pull-tester: Run rpc test in parallel  (MarcoFalke)
- - #8039 `69b3a6d` Bench: Add crypto hash benchmarks (laanwj)
- - #8041 `5b736dd` Fix bip9-softforks blockstore issue (MarcoFalke)
- - #7994 `1f01443` Add op csv tests to script_tests.json (Christewart)
- - #8038 `e2bf830` Various minor fixes (MarcoFalke)
- - #8072 `1b87e5b` Travis: 'make check' in parallel and verbose (MarcoFalke)
- - #8056 `8844ef1` Remove hardcoded "4 nodes" from test_framework (MarcoFalke)
- - #8047 `37f9a1f` Test_framework: Set wait-timeout for bitcoind procs (MarcoFalke)
- - #8095 `6700cc9` Test framework: only cleanup on successful test runs (sdaftuar)
- - #8098 `06bd4f6` Test_framework: Append portseed to tmpdir (MarcoFalke)
- - #8104 `6ff2c8d` Add timeout to sync_blocks() and sync_mempools() (sdaftuar)
- - #8111 `61b8684` Benchmark SipHash (sipa)
- - #8107 `52b803e` Bench: Added base58 encoding/decoding benchmarks (yurizhykin)
- - #8115 `0026e0e` Avoid integer division in the benchmark inner-most loop (gmaxwell)
- - #8090 `a2df115` Adding P2SH(p2pkh) script test case (Christewart)
- - #7992 `ec45cc5` Extend #7956 with one more test (TheBlueMatt)
- - #8139 `ae5575b` Fix interrupted HTTP RPC connection workaround for Python 3.5+ (sipa)
- - #8164 `0f24eaf` [Bitcoin-Tx] fix missing test fixtures, fix 32bit atoi issue (jonasschnelli)
- - #8166 `0b5279f` Src/test: Do not shadow local variables (paveljanik)
- - #8141 `44c1b1c` Continuing port of java comparison tool (mrbandrews)
- - #8201 `36b7400` fundrawtransaction: Fix race, assert amounts (MarcoFalke)
- - #8214 `ed2cd59` Mininode: fail on send_message instead of silent return (MarcoFalke)
- - #8215 `a072d1a` Don't use floating point in wallet tests (MarcoFalke)
- - #8066 `65c2058` Test_framework: Use different rpc_auth_pair for each node (MarcoFalke)
- - #8216 `0d41d70` Assert 'changePosition out of bounds'  (MarcoFalke)
- - #8222 `961893f` Enable mempool consistency checks in unit tests (sipa)
- - #7751 `84370d5` test_framework: python3.4 authproxy compat (laanwj)
- - #7744 `d8e862a` test_framework: detect failure of bitcoind startup (laanwj)
- - #8280 `115735d` Increase sync_blocks() timeouts in pruning.py (MarcoFalke)
- - #8340 `af9b7a9` Solve trivial merge conflict in p2p-segwit.py (MarcoFalke)
- - #8067 `3e4cf8f` Travis: use slim generic image, and some fixups (theuni)
- - #7951 `5c7df70` Test_framework: Properly print exception (MarcoFalke)
- - #8070 `7771aa5` Remove non-determinism which is breaking net_tests #8069 (EthanHeilman)
- - #8309 `bb2646a` Add wallet-hd test (MarcoFalke)
- - #8444 `cd0910b` Fix p2p-feefilter.py for changed tx relay behavior (sdaftuar)

### Mining

- - #7507 `11c7699` Remove internal miner (Leviathn)
- - #7663 `c87f51e` Make the generate RPC call function for non-regtest (sipa)
- - #7671 `e2ebd25` Add generatetoaddress RPC to mine to an address (achow101)
- - #7935 `66ed450` Versionbits: GBT support (luke-jr)
- - #7600 `66db2d6` Select transactions using feerate-with-ancestors (sdaftuar)
- - #8295 `f5660d3` Mining-related fixups for 0.13.0 (sdaftuar)
- - #7796 `536b75e` Add support for negative fee rates, fixes `prioritizetransaction` (MarcoFalke)
- - #8362 `86edc20` Scale legacy sigop count in CreateNewBlock (sdaftuar)
- - #8489 `8b0eee6` Bugfix: Use pre-BIP141 sigops until segwit activates (GBT) (luke-jr)

### Documentation and miscellaneous

- - #7423 `69e2a40` Add example for building with constrained resources (jarret)
- - #8254 `c2c69ed` Add OSX ZMQ requirement to QA readme (fanquake)
- - #8203 `377d131` Clarify documentation for running a tor node (nathaniel-mahieu)
- - #7428 `4b12266` Add example for listing ./configure flags (nathaniel-mahieu)
- - #7847 `3eae681` Add arch linux build example (mruddy)
- - #7968 `ff69aaf` Fedora build requirements (wtogami)
- - #8013 `fbedc09` Fedora build requirements, add gcc-c++ and fix typo (wtogami)
- - #8009 `fbd8478` Fixed invalid example paths in gitian-building.md (JeremyRand)
- - #8240 `63fbdbc` Mention Windows XP end of support in release notes (laanwj)
- - #8303 `5077d2c` Update bips.md for CSV softfork (fanquake)
- - #7789 `e0b3e19` Add note about using the Qt official binary installer (paveljanik)
- - #7791 `e30a5b0` Change Precise to Trusty in gitian-building.md (JeremyRand)
- - #7838 `8bb5d3d` Update gitian build guide to debian 8.4.0 (fanquake)
- - #7855 `b778e59` Replace precise with trusty (MarcoFalke)
- - #7975 `fc23fee` Update bitcoin-core GitHub links (MarcoFalke)
- - #8034 `e3a8207` Add basic git squash workflow (fanquake)
- - #7813 `214ec0b` Update port in tor.md (MarcoFalke)
- - #8193 `37c9830` Use Debian 8.5 in the gitian-build guide (fanquake)
- - #8261 `3685e0c` Clarify help for `getblockchaininfo` (paveljanik)
- - #7185 `ea0f5a2` Note that reviewers should mention the id of the commits they reviewed (pstratem)
- - #7290 `c851d8d` [init] Add missing help for args (MarcoFalke)
- - #7281 `f9fd4c2` Improve CheckInputs() comment about sig verification (petertodd)
- - #7417 `1e06bab` Minor improvements to the release process (PRabahy)
- - #7444 `4cdbd42` Improve block validity/ConnectBlock() comments (petertodd)
- - #7527 `db2e1c0` Fix and cleanup listreceivedbyX documentation (instagibbs)
- - #7541 `b6e00af` Clarify description of blockindex (pinheadmz)
- - #7590 `f06af57` Improving wording related to Boost library requirements [updated] (jonathancross)
- - #7635 `0fa88ef` Add dependency info to test docs (elliotolds)
- - #7609 `3ba07bd` RPM spec file project (AliceWonderMiscreations)
- - #7850 `229a17c` Removed call to `TryCreateDirectory` from `GetDefaultDataDir` in `src/util.cpp` (alexreg)
- - #7888 `ec870e1` Prevector: fix 2 bugs in currently unreached code paths (kazcw)
- - #7922 `90653bc` CBase58Data::SetString: cleanse the full vector (kazcw)
- - #7881 `c4e8390` Update release process (laanwj)
- - #7952 `a9c8b74` Log invalid block hash to make debugging easier (paveljanik)
- - #7974 `8206835` More comments on the design of AttemptToEvictConnection (gmaxwell)
- - #7795 `47a7cfb` UpdateTip: log only one line at most per block (laanwj)
- - #8110 `e7e25ea` Add benchmarking notes (fanquake)
- - #8121 `58f0c92` Update implemented BIPs list (fanquake)
- - #8029 `58725ba` Simplify OS X build notes (fanquake)
- - #8143 `d46b8b5` comment nit: miners don't vote (instagibbs)
- - #8136 `22e0b35` Log/report in 10% steps during VerifyDB (jonasschnelli)
- - #8168 `d366185` util: Add ParseUInt32 and ParseUInt64 (laanwj)
- - #8178 `f7b1bfc` Add git and github tips and tricks to developer notes (sipa)
- - #8177 `67db011` developer notes: updates for C++11 (kazcw)
- - #8229 `8ccdac1` [Doc] Update OS X build notes for 10.11 SDK (fanquake)
- - #8233 `9f1807a` Mention Linux ARM executables in release process and notes (laanwj)
- - #7540 `ff46dd4` Rename OP_NOP3 to OP_CHECKSEQUENCEVERIFY (btcdrak)
- - #8289 `26316ff` bash-completion: Adapt for 0.12 and 0.13 (roques)
- - #7453 `3dc3149` Missing patches from 0.12 (MarcoFalke)
- - #7113 `54a550b` Switch to a more efficient rolling Bloom filter (sipa)
- - #7257 `de9e5ea` Combine common error strings for different options so translations can be shared and reused (luke-jr)
- - #7304 `b8f485c` [contrib] Add clang-format-diff.py (MarcoFalke)
- - #7378 `e6f97ef` devtools: replace github-merge with python version (laanwj)
- - #7395 `0893705` devtools: show pull and commit information in github-merge (laanwj)
- - #7402 `6a5932b` devtools: github-merge get toplevel dir without extra whitespace (achow101)
- - #7425 `20a408c` devtools: Fix utf-8 support in messages for github-merge (laanwj)
- - #7632 `409f843` Delete outdated test-patches reference (Lewuathe)
- - #7662 `386f438` remove unused NOBLKS_VERSION_{START,END} constants (rat4)
- - #7737 `aa0d2b2` devtools: make github-merge.py use py3 (laanwj)
- - #7781 `55db5f0` devtools: Auto-set branch to merge to in github-merge (laanwj)
- - #7934 `f17032f` Improve rolling bloom filter performance and benchmark (sipa)
- - #8004 `2efe38b` signal handling: fReopenDebugLog and fRequestShutdown should be type sig_atomic_t (catilac)
- - #7713 `f6598df` Fixes for verify-commits script (petertodd)
- - #8412 `8360d5b` libconsensus: Expose a flag for BIP112 (jtimon)

Credits
=======

Thanks to everyone who directly contributed to this release:

- - 21E14
- - accraze
- - Adam Brown
- - Alexander Regueiro
- - Alex Morcos
- - Alfie John
- - Alice Wonder
- - AlSzacrel
- - Andrew Chow
- - Andrés G. Aragoneses
- - Bob McElrath
- - BtcDrak
- - calebogden
- - Cédric Félizard
- - Chirag Davé
- - Chris Moore
- - Chris Stewart
- - Christian von Roques
- - Chris Wheeler
- - Cory Fields
- - crowning-
- - Daniel Cousens
- - Daniel Kraft
- - Denis Lukianov
- - Elias Rohrer
- - Elliot Olds
- - Eric Shaw
- - error10
- - Ethan Heilman
- - face
- - fanquake
- - Francesco 'makevoid' Canessa
- - fsb4000
- - Gavin Andresen
- - gladoscc
- - Gregory Maxwell
- - Gregory Sanders
- - instagibbs
- - James O'Beirne
- - Jannes Faber
- - Jarret Dyrbye
- - Jeremy Rand
- - jloughry
- - jmacwhyte
- - Joao Fonseca
- - Johnson Lau
- - Jonas Nick
- - Jonas Schnelli
- - Jonathan Cross
- - João Barbosa
- - Jorge Timón
- - Kaz Wesley
- - Kefkius
- - kirkalx
- - Krzysztof Jurewicz
- - Leviathn
- - lewuathe
- - Luke Dashjr
- - Luv Khemani
- - Marcel Krüger
- - Marco Falke
- - Mark Friedenbach
- - Matt
- - Matt Bogosian
- - Matt Corallo
- - Matthew English
- - Matthew Zipkin
- - mb300sd
- - Mitchell Cash
- - mrbandrews
- - mruddy
- - Murch
- - Mustafa
- - Nathaniel Mahieu
- - Nicolas Dorier
- - Patrick Strateman
- - Paul Rabahy
- - paveljanik
- - Pavel Janík
- - Pavel Vasin
- - Pedro Branco
- - Peter Todd
- - Philip Kaufmann
- - Pieter Wuille
- - Prayag Verma
- - ptschip
- - Puru
- - randy-waterhouse
- - R E Broadley
- - Rusty Russell
- - Suhas Daftuar
- - Suriyaa Kudo
- - TheLazieR Yip
- - Thomas Kerin
- - Tom Harding
- - Tyler Hardin
- - UdjinM6
- - Warren Togami
- - Will Binns
- - Wladimir J. van der Laan
- - Yuri Zhykin

As well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).

Hashes for verification
========================

These are the SHA-256 hashes of the released files:

    f94123e37530f9de25988ff93e5568a93aa5146f689e63fb0ec1f962cf0bbfcd  bitcoin-0.13.0-aarch64-linux-gnu.tar.gz
    7c657ec6f6a5dbb93b9394da510d5dff8dd461df8b80a9410f994bc53c876303  bitcoin-0.13.0-arm-linux-gnueabihf.tar.gz
    d6da2801dd9d92183beea16d0f57edcea85fc749cdc2abec543096c8635ad244  bitcoin-0.13.0-i686-pc-linux-gnu.tar.gz
    2f67ac67b935368e06f2f3b83f0173be641eef799e45d0a267efc0b9802ca8d2  bitcoin-0.13.0-osx64.tar.gz
    e7fed095f1fb833d167697c19527d735e43ab2688564887b80b76c3c349f85b0  bitcoin-0.13.0-osx.dmg
    0c7d7049689bb17f4256f1e5ec20777f42acef61814d434b38e6c17091161cda  bitcoin-0.13.0.tar.gz
    213e6626ad1f7a0c7a0ae2216edd9c8f7b9617c84287c17c15290feca0b8f13b  bitcoin-0.13.0-win32-setup.exe
    5c5bd6d31e4f764e33f2f3034e97e34789c3066a62319ae8d6a6011251187f7c  bitcoin-0.13.0-win32.zip
    c94f351fd5266e07d2132d45dd831d87d0e7fdb673d5a0ba48638e2f9f8339fc  bitcoin-0.13.0-win64-setup.exe
    54606c9a4fd32b826ceab4da9335d7a34a380859fa9495bf35a9e9c0dd9b6298  bitcoin-0.13.0-win64.zip
    bcc1e42d61f88621301bbb00512376287f9df4568255f8b98bc10547dced96c8  bitcoin-0.13.0-x86_64-linux-gnu.tar.gz

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJXvGoPAAoJEHSBCwEjRsmm8a4H/j06PHEWZ/NDL+38d//aHLIn
7QTE3ih2aSPsEVtwSnC8cFQVyTAG6ZyJ0T4DvXZ7wizzCqTjtDzDs03GQCBaAHH6
QzqGgdezffFFxYG2/aTcEhf0bW44FMqc9t5ypgn61o1cbrgP9/1edWz4FSj6BGoX
kx4D9pPudST3J1mYEdC77foi7uFCLU2J6m2N02qwCqr/uXxao+2dR+l3nfpeJA8N
1D+D1MdAq55/UEwxankVFeoLdMHSb5AXZZQyvkeimfwxnEDX1k1maoyYyYFx9MJK
MevaNTpRwiAhNtY3m9ucI/LrLETKC3vRvonxoSHBHioz9rZwvBEwNkP0+M/9zGQ=
=wCgV
-----END PGP SIGNATURE-----


-------------------------------------
> "The problem case is where someone in a contract setup shows you a
script, which you accept as being a payment to yourself. An attacker could
use a collision attack to construct scripts with identical hashes, only one
of which does have the property you want, and steal coins.
>
> So you really want collision security, and I don't think 80 bits is
something we should encourage for that. Normal pubkey hashes don't have
that problem, as they can't be constructed to pay to you."
>
> ... but I'm unconvinced:
>
> "But it is trivial for contract wallets to protect against collision
attacks-- if you give me a script that is "gavin_pubkey CHECKSIG
arbitrary_data OP_DROP" with "I promise I'm not trying to rip you off, just
ignore that arbitrary data" a wallet can just refuse. Even more likely, a
contract wallet won't even recognize that as a pay-to-gavin transaction.
>
> I suppose it could be looking for some form of "gavin_pubkey
somebody_else_pubkey CHECKMULTISIG ... with the attacker using
somebody_else_pubkey to force the collision, but, again, trivial contract
protocol tweaks ("send along a proof you have the private key corresponding
to the public key" or "everybody pre-commits pubkeys they'll use at
protocol start") would protect against that.

Yes, this is what I worry about. We're constructing a 2-of-2 multisig
escrow in a contract. I reveal my public key A, you do a 80-bit search for
B and C such that H(A and B) = H(B and C). You tell me your keys B, and I
happily send to H(A and B), which you steal with H(B and C).

Sending along a proof does not help, you can't prove that you do not know
of a collision. Pre-committing does help, but is a very non-obvious
security requirement, something I strongly believe is far riskier in
practice.

Bitcoin does have parts that rely on economic arguments for security or
privacy, but can we please stick to using cryptography that is up to par
for parts where we can? It's a small constant factor of data, and it
categorically removes the worry about security levels.

-- 
Pieter

-------------------------------------
I have updated the GitHub a lot (changed tones to be less chirpy, fixed
some smalls) and made a couple of samples (see attachment for MP3 and FLAC
of both tone tables, first 16 then 4). Is this good enough to warrant an
official BIP number? I haven't built a decoder yet, but it seems like the
encoder is working properly (looked at Audacity, seems like it is working),
and some people on reddit want to "allow for decoding experiments"
<https://www.reddit.com/r/btc/comments/4wsn7v/bip_proposal_addresses_over_audio_thoughts/d69m3st>

What suggestions do you all have for it?

On Mon, Aug 8, 2016 at 8:50 PM, Daniel Hoffman <danielhoffman699@gmail.com>
wrote:

> It wouldn't be feasible in the vast majority of cases, but I can't think
> of a reason why it can't be built into the standard.
>
> On Mon, Aug 8, 2016 at 5:59 PM, Trevin Hofmann via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Would it be feasible to transmit an entire BIP21 URI as audio? If you
>> were to encode any extra information (such as amount), it would be useful
>> to include a checksum for the entire message. This checksum could possibly
>> be used instead of the checksum in the address.
>>
>> Trevin
>>
>> On Aug 8, 2016 3:06 PM, "Justin Newton via bitcoin-dev" <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> Daniel,
>>>    Thanks for proposing this.  I think this could have some useful use
>>> cases as you state.  I was wondering what you would think to adding some
>>> additional tones to optionally denote an amount (in satoshis?).
>>>
>>> (FYI, actual link is here:  https://github.com/Dako300/BIP )
>>>
>>> Justin
>>>
>>> On Mon, Aug 8, 2016 at 2:22 PM, Daniel Hoffman via bitcoin-dev <
>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>> This is my BIP idea: a fast, robust, and standardized for representing
>>>> Bitcoin addresses over audio. It takes the binary representation of the
>>>> Bitcoin address (little endian), chops that up into 4 or 2 bit chunks
>>>> (depending on type, 2 bit only for low quality audio like american
>>>> telephone lines), and generates a tone based upon that value. This started
>>>> because I wanted an easy way to donate to podcasts that I listen to, and
>>>> having a Shazam-esque app (or a media player with this capability) that
>>>> gives me an address automatically would be wonderful for both the consumer
>>>> and producer. Comes with error correction built into the protocol
>>>>
>>>> You can see the full specification of the BIP on my GitHub page (
>>>> https://github.com/Dako300/BIP-0153).
>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev@lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>>
>>>
>>> --
>>>
>>> Justin W. Newton
>>> Founder/CEO
>>> Netki, Inc.
>>>
>>> justin@netki.com
>>> +1.818.261.4248
>>>
>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>

-------------------------------------
I want to pitch a use-case that might have been ignored in this discussion:

I don't think this protocol is only useful for hardware wallets.
Technically any website that wants to request public keys/signatures and
offload the responsibility for managing keys and signing to the user
would also find this valuable.

I hope we can move forward with a protocol that suits both the hardware
people, and the people who find signing transactions in browsers
unsettling.

Maybe we the focus should move away from only servicing hardware, and
asking if the motivation is better captured by "allow users pick their
own ECDSA implementation, hardware or software", then working out what
we need to get us there.


On 08/18/2016 12:23 PM, Nicolas Bacca via bitcoin-dev wrote:
> On Thu, Aug 18, 2016 at 11:49 AM, Jonas Schnelli via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>
>     Hi
>
>     > I have some experience with hardware wallet development and its
>     > integration and I know it's a mess. But it is too early to
>     define such
>     > rigid standards yet. Also, TREZOR concept (device as a server
>     and the
>     > primary source of workflow management) goes directly against your
>     > proposal of wallet software as an workflow manager. So it is
>     clear NACK
>     > for me.
>
>     The current question  as already mentioned  is we ACK to work
>     together
>     on a signing protocol or if we NACK this before we even have started.
>
>
> ACK for Ledger. What's necessary to sign a transaction is well known,
> I don't see how driving any hardware wallet from the wallet itself or
> from a third party daemon implementing that URL scheme would make any
> difference, other than providing better devices interoperability, as
> well as easier maintenance and update paths for the wallets.
>
> -- 
> Nicolas Bacca | CTO, Ledger
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
To my understanding it is purely software thing. It cannot be detected from
outside if miner uses this improvement or not. So patenting it is worthless.

slush

On Tue, Apr 5, 2016 at 1:01 AM, Mustafa Al-Bassam via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Alternatively scenario: it will cause a sudden increase of Bitcoin mines
> in countries where the algorithm is not patented, possibly causing a
> geographical decentralization of miners from countries that already have a
> lot of miners like China (if it is patented in China).
>
> On 01/04/16 10:00, Peter Todd via bitcoin-dev wrote:
>
> On Thu, Mar 31, 2016 at 09:41:40PM -0700, Timo Hanke via bitcoin-dev wrote:
>
> Hi.
>
> I'd like to announce a white paper that describes a very new and
> significant algorithmic improvement to the Bitcoin mining process which has
> never been discussed in public before. The white paper can be found here:
> http://www.math.rwth-aachen.de/~Timo.Hanke/AsicBoostWhitepaperrev5.pdf
>
> What steps are you going to take to make sure that this improvement is
> available to all ASIC designers/mfgs on a equal opportunity basis?
>
> The fact that you've chosen to patent this improvement could be a
> centralization concern depending on the licensing model used. For example, one
> could imagine a licensing model that gave one manufacture exclusive rights.
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing listbitcoin-dev@lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
t. khan via bitcoin-dev wrote:
> BIP Proposal - Managing Bitcoin’s block size the same way we do
> difficulty (aka Block75)
> 
> The every two-week adjustment of difficulty has proven to be a
> reasonably effective and predictable way of managing how quickly blocks
> are mined. Bitcoin needs a reasonably effective and predictable way of
> managing the maximum block size.
> 
> It’s clear at this point that human beings should not be involved in the
> determination of max block size, just as they’re not involved in
> deciding the difficulty.
> 
> Instead of setting an arbitrary max block size (1MB, 2MB, 8MB, etc.) or
> passing the decision to miners/pool operators, the max block size should
> be adjusted every two weeks (2016 blocks) using a system similar to how
> difficulty is calculated.
> 
> Put another way: let’s stop thinking about what the max block size
> should be and start thinking about how full we want the average block to
> be regardless of size. Over the last year, we’ve had averages of 75% or
> higher, so aiming for 75% full seems reasonable, hence naming this
> concept ‘Block75’.
> 
> The target capacity over 2016 blocks would be 75%. If the last 2016
> blocks are more than 75% full, add the difference to the max block size.
> Like this:
> 
> MAX_BLOCK_BASE_SIZE = 1000000
> TARGET_CAPACITY = 750000
> AVERAGE_OVER_CAP = average block size of last 2016 blocks minus
> TARGET_CAPACITY
> 
> To check if a block is valid, ≤ (MAX_BLOCK_BASE_SIZE + AVERAGE_OVER_CAP)
> 
> For example, if the last 2016 blocks are 85% full (average block is 850
> KB), add 10% to the max block size. The new max block size would be
> 1,100 KB until the next 2016 blocks are mined, then reset and
> recalculate. The 1,000,000 byte limit that exists currently would
> remain, but would effectively be the minimum max block size. 
> 
> Another two weeks goes by, the last 2016 blocks are again 85% full, but
> now that means they average 935 KB out of the 1,100 KB max block size.
> This is 93.5% of the 1,000,000 byte limit, so 18.5% would be added to
> that to make the new max block size of 1,185 KB.
> 
> Another two weeks passes. This time, the average block is 1,050 KB. The
> new max block size is calculated to 1,300 KB (as blocks were 105% full,
> minus the 75% capacity target, so 30% added to max block size).
> 
> Repeat every 2016 blocks, forever.
> 
> If Block75 had been applied at the difficulty adjustment on November
> 18th, the max block size would have been 1,080KB, as the average block
> during that period was 83% full, so 8% is added to the 1,000KB limit.
> The current size, after the December 2nd adjustment would be 1,150K.
> 
> Block75 would allow the max block size to grow (or shrink) in response
> to transaction volume, and does so predictably, reasonably quickly, and
> in a method that prevents wild swings in block size or transaction fees.
> It attempts to keep blocks at 75% total capacity over each two week
> period, the same way difficulty tries to keep blocks mined every ten
> minutes. It also keeps blocks as small as possible.
> 
> Thoughts?
> 
> -t.k.
> 

I like the idea. It is good wrt growing the max. block size
automatically without human action, but the main problem (or question)
is not how to grow this number, it is what number can the network
handle, considering both miners and users. While disk space requirements
might not be a big problem, block propagation time is. The time required
for a block to propagate in the network (or at least to all the miners)
is directly dependent of its size.  If blocks take too much time to
propagate in the network, the orphan rate will increase in unpredictable
ways. For example if the internet speed in China is worse than in
Europe, and miners in China have more than 50% of the hashing power,
blocks mined by European miners might get orphaned.

The system as described can also be gamed, by filling the network with
transactions. Miners have the monetary interest to include as many
transactions as possible in a block in order to collect the fees.
Regardless how you think about it, there has to be a maximum block size
that the network will allow as a consensus rule. Increasing it
dynamically based on transaction volume will reach a point where the
number got big enough that it broke things. Bitcoin, because its
fundamental design, can scale by using offchain solutions.


-------------------------------------
How does your system prevent against insider attacks? How do you know
the money is stolen by someone who compromised server #4, and not
stolen by the person who set up server #4? It is my understanding
these days most attacks are inside jobs.

On 8/24/16, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Thu, Aug 25, 2016 at 01:37:34AM +1000, Matthew Roberts wrote:
>> Really nice idea. So its like a smart contract that incentivizes
>> publication that a server has been hacked? I also really like how the
>> funding has been handled -- with all the coins stored in the same address
>> and then each server associated with a unique signature. That way, you
>> don't have to split up all the coins among every server and reduce the
>> incentive for an attacker yet you can still identify which server was
>> hacked.
>>
>> It would be nice if after the attacker broke into the server that they
>> were
>> also incentivized to act on the information as soon as possible
>> (revealing
>> early on when the server was compromised.) I suppose you could split up
>> the
>> coins into different outputs that could optimally be redeemed by the
>> owner
>> at different points in the future -- so they're incentivzed to act lest
>
> Remember that it's _always_ possible for the owner to redeem the coins at
> any
> time, and there's no way to prevent that.
>
> The incentive for the intruder to collect the honeypot in a timely manner
> is
> simple: once they've broken in, the moment the honeypot owner learns about
> the
> compromise they have every reason to attempt to recover the funds, so the
> intruder needs to act as fast as possible to maximize their chances of
> being
> rewarded.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>


-------------------------------------
On Fri, Sep 23, 2016 at 1:43 PM, Russell O'Connor via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I believe Bitcoin currently enjoys the property that during an "innocent"
> re-org, i.e. a reorg in which no affected transactions are being double
> spent, all affected transactions can always eventually get replayed, so long
> as the re-org depth is less than 100.

> My concern with this proposed operation is that it would destroy this
> property.

The reorg safety impact of this proposal could be eliminated and the
mempool handling complexity greatly reduced if the transaction was
required to be locktimed at least 100 blocks after the block its
referencing.

This would also resolve a rather severe DOS weakness that the spec has
with the suggestion that nodes would relay this rule without
validating it. With the depth restriction nodes could relay one (or a
couple) blocks early without creating a situation where someone can
consume relay resources with near zero odds of paying a fee for them.

Irritatingly, applications of this rule would really want to be
applied at signing time (like locktime is), not as part of a
scriptpubkey. With it part of a scriptpubkey two moves are required. I
think solving this is important.

FWIW, this scheme more has been proposed before for another reason--
effectively allowing users to 'vote against' long reorgs by making
sure their transactions can't be included in them. Though for that
application it was only needed to use 32 bits of the block hash.


-------------------------------------

On 6/23/2016 1:56 PM, Peter Todd via bitcoin-dev wrote:
>>
>> I dont know if you are opposed to organizations that have AML requirements
>> from using the bitcoin blockchain, but if you arent, why wouldnt you
>> prefer an open source, open standards based solution to exclusionary,
>> proprietary ones?
> 
> In some (most?) countries, it is illegal to offer telecoms services without
> wiretap facilities. Does that mean Tor builds into its software "open source"
> "open standards" wiretapping functionality? No. And interestingly, people
> trying to add support for that stuff is actually a thing that keeps happening
> in the Tor community...
> 
> In any case, I'd strongly argue that we remove BIP75 from the bips repository,
> and boycott wallets that implement it. It's bad strategy for Bitcoin developers
> to willingly participate in AML/KYC, just the same way as it's bad for Tor to
> add wiretapping functionality, and W3C to support DRM tech. The minor tactical
> wins you'll get our of this aren't worth it.
> 
Exactly!
Totally agree with Peter Todd. There's absolutely no gain for Bitcoin to
willingly participate in AML/KYC. Plus this might come with strings
attached: for example when running a Tor relay in some countries if you
interfere with the traffic (censor, limit, filter, etc.) you become
responsible for it, while when you only relay anonymous traffic without
interfering or having the possibility to do so (installing certain
tools, using a modified Tor which allows you to do so, etc.) you cannot
be held responsible for the traffic.

Any kind of built-in AML/KYC tools in Bitcoin is bad, and might draw
expectations from _all_ users from authorities. Companies or individuals
who want and/or need AML/KYC can find ways and do it at their side
isolated from the entire network, and the solutions shouldn't come from
upstream. AML/KYC/<insert other regulation here> differ from country to
country and will be hard to implement in a global consensus network even
if it would be worth it.


-------------------------------------
On Tuesday, February 16, 2016 8:20:26 PM Alex Morcos via bitcoin-dev wrote:
> # The feefilter message is defined as a message containing an int64_t where
> pchCommand == "feefilter"

What happened to extensibility? And why waste 64 bits for what is almost 
certainly a small number?

> # The fee filter is additive with a bloom filter for transactions so if an
> SPV client were to load a bloom filter and send a feefilter message,
> transactions would only be relayed if they passed both filters.

This seems to make feefilter entirely useless for wallets?

Luke


-------------------------------------
On Saturday, 15 October 2016 12:11:02 CEST Marco Falke wrote:
> On Sat, Sep 24, 2016 at 11:41 AM, Tom via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > I'd suggest saying that "Share alike" is required and "Attribution" is
> > optional.
> 
> Please note there is no CC license that requires SA and at the same
> time has BY as an option.
> 
> Generally, I think CC0 is best suited as license for BIPs. If authors
> are scared that they won't get proper attribution, they can choose
> MIT/BSD or CC-BY.

My suggestion (sorry for not explaining it better) was that for BIPS to be a 
public domain (aka CC0) and a CC-BY option and nothing else.

I like you agree with that part, but I see you added two licenses.
Do you have a good reason to add MIT/BSD to that list? Otherwise I think we 
agree.
Using code-specific licenses (including the GPL) for documentation and 
specifically a specification is a really poor fit and doens't make much sense.

> Other than that I don't think that more restrictive
> licenses are suitable for BIPs. The BIP repo seems like the wrong
> place to promote Open Access (e.g. by choosing a CC-BY-SA license).
> BIP 2 allows such licenses, but does not recommend them, which is
> fine.
> 
> I think that BIP 2 in its current form (
> https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki
> @6e47447b ) 
Well, it has this sentence;

> This BIP is dual-licensed under the Open Publication License and
> BSD 2-clause license. 

Which is a bit odd in light of the initial email from Luke that suggested we 
drop the Open Publication License and we use the CC ones instead in addition 
to the public domain one.

Marco:
> looks good and addressed the feedback which was
> accumulated last year. If there are no objections I'd suggest to move
> forward with BIP 2 in the next couple of days/weeks.

Thats odd, you just stated you like the public domain (aka CC0) license, yet 
you encourage the BIP2 that states we can no longer use public domain for 
BIPs... Did you read it?
It says;
 Public domain is not universally recognised as a legitimate action, thus
  it is inadvisable. [1]


Also;
This list has not seen a lot of traffic, if you want to make sure people keep 
using the BIP process, I think you need to reach out to the rest of the 
community and make sure this has been heard and discussed.
Moving forward the way it is now will likely deminish the importance of the 
BIP process.

I strongly suggest people make very clear any and all changes that are 
proposed and defend each of them with reasons why you want to change things.


1) if you write as a rationale "In some jurisdictions, public domain is not 
recognised as a legitimate legal action" then you can at least name those 
jurisdictions and explain how they *do* support things like GPL. Burden of 
proof is on the man who wants to change things.
It looks fishy when lawyers disagree. See the CC wikipedia page;
 "public domain: cc0 Freeing content globally without restrictions"

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
HMAC has proven security property.
It is still secure even when underlying crypto hashing function has
collision resistant weakness.
For example, MD5 is considered completely insecure now, but HMAC-MD5 is
still considered secure.
When in doubt, we should always use HMAC for MAC(Message Authentication
Code) rather than custom construction

On Wed, Jun 29, 2016 at 9:00 AM, Rusty Russell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Jonas Schnelli <dev@jonasschnelli.ch> writes:
> >> To quote:
> >>
> >>> HMAC_SHA512(key=ecdh_secret|cipher-type,msg="encryption key").
> >>>
> >>>  K_1 must be the left 32bytes of the HMAC_SHA512 hash.
> >>>  K_2 must be the right 32bytes of the HMAC_SHA512 hash.
> >>
> >> This seems a weak reason to introduce SHA512 to the mix.  Can we just
> >> make:
> >>
> >> K_1 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="header encryption
> key")
> >> K_2 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="body encryption key")
> >
> > SHA512_HMAC is used by BIP32 [1] and I guess most clients will somehow
> > make use of bip32 features. I though a single SHA512_HMAC operation is
> > cheaper and simpler then two SHA256_HMAC.
>
> Good point; I would argue that mistake has already been made.  But I was
> looking at appropriating your work for lightning inter-node comms, and
> adding another hash algo seemed unnecessarily painful.
>
> > AFAIK, sha256_hmac is also not used by the current p2p & consensus layer.
> > Bitcoin-Core uses it for HTTP RPC auth and Tor control.
>
> It's also not clear to me why the HMAC, vs just
> SHA256(key|cipher-type|mesg).  But that's probably just my crypto
> ignorance...
>
> Thanks!
> Rusty.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
Xuesong (Arthur) Chen
Senior Principle Engineer
BlockChain Technologist
BTCC

-------------------------------------
As has been mentioned there have been a lot of time to upgrade
software to support segwit. Furthermore, since it is a softfork, there
will be plenty of time after activation too for those taking a "wait
and see" approach.

You keep insisting on "2 months after activation", but that's not how
BIP9 works. We could at most change BIP9's initial date, but if those
who haven't started to work on supporting segwit will keep waiting for
activation, then changing the initial date won't be of any help to
them can only delay those who are ready and waiting.

The new features are not a requirement after activation. And although
it may take some time after activation for the new features to really
get to the users, that's just a fact of life that won't change by
changing the initial BIP9 date.


On Sun, Oct 16, 2016 at 8:20 PM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Sunday, 16 October 2016 09:47:40 CEST Douglas Roark via bitcoin-dev
> wrote:
>> Would I want anyone to lose money due to faulty wallets? Of course not.
>> By the same token, devs have had almost a year to tinker with SegWit and
>> make sure the wallet isn't so poorly written that it'll flame out when
>> SegWit comes along. It's not like this is some untested, mostly unknown
>> feature that's being slipped out at the last minute
>
> There have been objections to the way that SegWit has been implemented for a
> long time, some wallets are taking a "wait and see" approach.  If you look
> at the page you linked[1], that is a very very sad state of affairs. The
> vast majority is not ready.  Would be interesting to get a more up-to-date
> view.
> Wallets probably won't want to invest resources adding support for a feature
> that will never be activated. The fact that we have a much safer alternative
> in the form of Flexible Transactions may mean it will not get activated. We
> won't know until its actually locked in.
> Wallets may not act until its actually locked in either. And I think we
> should respect that.
>
> Even if all wallets support it (and thats a big if), they need to be rolled
> out and people need to actually download those updates.
> This takes time, 2 months after the lock-in of SegWit would be the minimum
> safe time for people to actually upgrade.
>
> 1) https://bitcoincore.org/en/segwit_adoption/
> --
> Tom Zander
> Blog: https://zander.github.io
> Vlog: https://vimeo.com/channels/tomscryptochannel
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
On Wed, May 18, 2016 at 01:14:59PM +0200, Jorge Timn wrote:
> On May 17, 2016 15:23, "Peter Todd via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> > # TXO Commitments
> >
> 
> > Specifically TXO commitments proposes a Merkle Mountain Range (MMR), a
> > type of deterministic, indexable, insertion ordered merkle tree, which
> allows
> > new items to be cheaply appended to the tree with minimal storage
> requirements,
> > just log2(n) "mountain tips". Once an output is added to the TXO MMR it is
> > never removed; if an output is spent its status is updated in place. Both
> the
> > state of a specific item in the MMR, as well the validity of changes to
> items
> > in the MMR, can be proven with log2(n) sized proofs consisting of a
> merkle path
> > to the tip of the tree.
> 
> How expensive it is to update a leaf from this tree from unspent to spent?

log2(n) operations.

I wrote a full MMR implementation with pruning support as part of my
proofchains work:

https://github.com/proofchains/python-proofmarshal/blob/master/proofmarshal/mmr.py

Documentation is a bit lacking, but I'd suggest reading the above source code
and the unit tests(1) to understand what's going on. As of writing item
retrieval by index is implemented(2), and if you follow how that works you'll
see it's log2(n) operations; changing elements in-place isn't yet
implemented(3) but would be a fun homework problem. I'll bet anyone a beer that
you'll find it can be done in k*log2(n) operations, with a reasonably small k. :)

Additionally, I also have a merkelized key:value prefix tree implementation
called a "merbinner tree" in the same library, again with pruning support. It
does implement changing elements in place(4) with log2(n) operations.

Incidentally, something I probably should have made more clear in my TXO
commitments post is that the original MMR scheme I developed for OpenTimestamps
(and independently reinvented for Certificate Transparency) is insufficient:
while you can easily extract a proof that an element is present in the MMR,
that inclusion proof doesn't do a good job of proving the position in the tree
very well. OpenTimestamps didn't need that kind of proof, and I don't think
Certificate Transparency needs it either. However many other MMR applications
do, including many types of TXO commitments.

My proofchains MMR scheme fixes this problem by making each inner node in the
MMR commit to the total number of elements under it(5) - basically it's a
merkle-sum-tree with the size of the tree being what's summed. There may be
more efficient ways to do this, but a committed sum-length is easy to
implement, and the space overhead is only 25% even in the least optimised
implementation possible.

1) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/test/test_mmr.py
2) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/mmr.py#L294
3) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/mmr.py#L230
4) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/merbinnertree.py#L140
5) https://github.com/proofchains/python-proofmarshal/blob/3f0ba0a9d46f36377ad6c1901de19273604e6fbc/proofmarshal/mmr.py#L139

> Wouldn't it be better to have both an append-only TXO and an append-only
> STXO (with all spent outputs, not only the latest ones like in your "STXO")?

Nope. The reason why this doesn't work is apparent when you ask how will the
STXO be indexed?

If it's indexed by outpoint - that is H(txid:n) - to update the STXO you need
he entire thing, as the position of any new STXO that you need to add to the
STXO tree is random.

OTOH, if you index the STXO by txout creation order, with the first txout ever
created having position #0, the second #1, etc. the data you may need to update
the STXO later has predictable locality... but now you have something that's
basically identical to my proposed insertion-ordered TXO commitment anyway.

Incidentally, it's interesting how if a merbinner tree is insertion-order
indexed you end up with a datastructure that's almost identical to a MMR.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------


> On Jun 30, 2016, at 9:06 PM, Peter Todd <pete@petertodd.org> wrote:
> 
> On Thu, Jun 30, 2016 at 08:25:45PM +0200, Eric Voskuil wrote:
>>> To be clear, are you against Bitcoin Core's tor support?
>>> 
>>> Because node-to-node connections over tor are encrypted, and make use of onion
>>> addresses, which are self-authenticated in the exact same way as BIP151 proposes.
>> 
>> BIP151 is self-admittedly insufficient to protect against a MITM attack. It proposes node identity to close this hole (future BIP required). The yet-to-be-specified requirement for node identity is the basis of my primary concern. This is not self-authentication.
>> 
>>> And we're shipping that in production as of 0.12.0, and by default Tor onion support is enabled and will be automatically setup if you have a recent version of Tor installed.
>>> 
>>> Does that "create pressure to expand node identity"?
>> 
>> The orthogonal question of whether Tor is safe for use with the Bitcoin P2P protocol is a matter of existing research.
> 
> I don't think you answered my question.
> 
> Again, we _already have_ the equivalent of BIP151 functionality in Bitcoin
> Core, shipping in production, but implemented with a Tor dependency.
> 
> BIP151 removes that dependency on Tor, enabling encrypted connections
> regardless of whether or not you have Tor installed.
> 
> So any arguments against BIP151 being implemented, are equally arguments
> against our existing Tor onion support. Are you against that support? Because
> if you aren't, you can't have any objections to BIP151 being implemented

Neither Tor nor Bitcoin Core are part of this BIP (or its proposed dependency on node identity).

But again, given that node identity is not part of the Bitcoin Core Tor integration, my objection to the presumption of node identity by BIP151 is unrelated to Bitcoin Core's Tor integration.

e

-------------------------------------
Hi Jonas, I'll follow up in your second reply as well. Responses inline:

> On Jun 28, 2016, at 10:26 AM, Jonas Schnelli via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> Hi Eric
> 
>> I haven't seen much discussion here on the rationale behind BIP 151. Apologies if I missed it. I'm trying to understand why libbitcoin (or any node) would want to support it.
>> 
>> I understand the use, when coupled with a yet-to-be-devised identity system, with Bloom filter features. Yet these features are client-server in nature. Libbitcoin (for example) supports client-server features on an independent port (and implements a variant of CurveCP for encryption and identity). My concern arises with application of identity to the P2P protocol (excluding Bloom filter features).
>> 
>> It seems to me that the desire to secure against the weaknesses of BF is being casually generalized to the P2P network. That generalization may actually weaken the security of the P2P protocol. One might consider the proper resolution is to move the BF features to a client-server protocol.
>> 
>> The BIP does not make a case for other scenarios, or contemplate the significant problems associated with key distribution in any identity system. Given that the BIP relies on identity, these considerations should be fully vetted before heading down another blind alley.

> In my opinion, the question should be "why would you _not_ encrypt".

1) creation of a false sense of security
2) as a tradeoff against anonymity
3) benefit does not justify cost

> 1) Transaction censorship
> ISPs, WIFI provider or any other MITM, can holdback/censor unconfirmed
> transactions. Regardless if you are a miner or a validation/wallet node.
> 
> 2) Peer censorship
> MITM can remove or add entries from a "addr" message.
> 
> 3) Fingerprinting
> ISPs or any other MITM can intercept/inject fingerprinting relevant
> messages like "mempool" to analyze the bitcoin network.

Encryption alone cannot protect against a MITM attack in an anonymous and permissionless network. This is accepted in the BIP (and your follow-up reply).

> 4) SPV
> For obvious reasons regarding BF (see BIP or above).
> 
> 5) Goundwork for a "client-server" model over the P2P channel
> Fee estimation, bloom-filters, or any other message type that requires
> authentication.

I do not challenge the usefulness and appropriateness of encryption with authentication in a client-server blockchain protocol.

> I would not reduce BIP151 to only solve the BF privacy/censorship problem.
> 
> If we agree that censorship-resistance is one of the main properties of Bitcoin,

We do.

> then we should definitively use a form of end-to-end encryption between nodes. Built into the network layer.

This is the assumption that I'm questioning.

> There are plenty of other options to solve this problem. stunnel,
> Bernsteins CurveCP, VPN, etc. which are available since years.
> But the reality has shown that most bitcoin traffic is still unencrypted.

The question arises from concern over the security of the network in the case where encryption (and therefore authentication) is pervasive.

As you point out, anyone can set up a private network of nodes today. These nodes must also connect to the permissionless network to maintain the chain. These nodes constitute a trust zone within Bitcoin. This zone of exclusion operates as a single logical node from the perspective of the Bitcoin security model (one entity controls the validation rules for all nodes).

Widespread application of this model is potentially problematic. It is a non-trivial problem to design a distributed system that requires authentication but without identity and without central control. In fact this may be more challenging than Bitcoin itself. Trust on first use (TOFU) does not solve this problem.

In my opinion this question has not received sufficient consideration to warrant proceeding with a network encryption scheme (which concerns me as well, but as I consider it premature I won't comment).

> Example: IIRC non of the available SPV wallets can "speak" on of the
> possible encryption techniques. Encrypting traffic below the application
> layer is extremely hard to set up for non-experienced users.

Bloom filters can (and IMO should) be isolated from the P2P protocol. Also, if the proposal creates an insecurity its ease of deployment is moot.

> On top of that, encryption allows us to drop the SHA256 checksum per p2p
> message which should result in a better performance on the network layer
> once BIP151 is deployed.

I would not consider this a performance enhancing proposal. Simply dropping the checksum seems like a better option. But again, it is moot if it creates an insecurity.

> I agree that BIP151 _must_ be deployed together with an authentication
> scheme (I'm working on that) to protect again MITM during encryption
> initialization.

At a minimum I would propose that you modify BIP151 to declare a dependency on a future BIP, making BIP151 incomplete without it. I think we can agree that it would be unadvisable to deploy (and therefore to implement) encryption alone.

I'll respond to the question of authentication in your follow-up post.

e

> ---
> </jonas>
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
So just because other attacks are possible we should weaken the crypto
we use? You may feel comfortable weakening crypto used to protect a few
billion dollars of other peoples' money, but I dont.

On 01/07/16 23:39, Gavin Andresen via bitcoin-dev wrote:
> Thanks, Ethan, that's helpful and I'll stop thinking that collision
> attacks require 2^(n/2) memory...
> 
> So can we quantify the incremental increase in security of
> SHA256(SHA256) over RIPEMD160(SHA256) versus the incremental increase in
> security of having a simpler implementation of segwitness?
> 
> I'm going to claim that the difference in the first case is very, very,
> very small-- the risk of an implementation error caused by having
> multiple ways of interpreting the segwitness hash in the scriptPubKey is
> much, much greater.
> 
> And even if there IS some risk of collision attack now or at some point
> in the future, I claim that it is easy for wallets to mitigate that
> risk. In fact, the principle of security in depth means wallets that
> don't completely control the scriptPubKeys they're creating on behalf of
> users SHOULD be coded to mitigate that risk (e.g. not allowing arbitrary
> data around a user's public key in a Script so targeted substring
> attacks are eliminated entirely).
> 
> Purely from a security point of view, I think a single 20-byte
> segwitness in the scriptPubKey is the best design.
> "Keep the design as simple and small as possible"
> https://www.securecoding.cert.org/confluence/plugins/servlet/mobile#content/view/2426
> 
> Add in the implied capacity increase of smaller scriptPubKeys and I
> still think it is a no-brainer.
> 
> 
> On Thu, Jan 7, 2016 at 5:56 PM, Ethan Heilman <eth3rs@gmail.com
> <mailto:eth3rs@gmail.com>> wrote:
> 
>     >Ethan:  your algorithm will find two arbitrary values that collide. That isn't useful as an attack in the context we're talking about here (both of those values will be useless as coin destinations with overwhelming probability).
> 
>     I'm not sure exactly the properties you want here and determining
>     these properties is not an easy task, but the case is far worse than
>     just two random values. For instance: (a). with a small modification
>     my algorithm can also find collisions containing targeted substrings,
>     (b). length extension attacks are possible with RIPEMD160.
> 
>     (a). targeted cycles:
> 
>     target1 = "str to prepend"
>     target2 = "str to end with"
> 
>     seed = {0,1}^160
>     x = hash(seed)
> 
>     for i in 2^80:
>     ....x = hash(target1||x||target2)
>     x_final = x
> 
>     y = hash(tartget1||x_final||target2)
> 
>     for j in 2^80:
>     ....if y == x_final:
>     ........print "cycle len: "+j
>     ........break
>     ....y = hash(target1||y||target2)
> 
>     If a collision is found, the two colliding inputs must both start with
>     "str to prepend" and end with the phrase "str to end with". As before
>     this only requires 2^81.5 computations and no real memory. For an
>     additional 2**80 an adversary has an good change of finding two
>     different targeted substrings which collide. Consider the case where
>     the attacker mixes the targeted strings with the hash output:
> 
>     hash("my name is=0x329482039483204324423"+x[1]+", my favorite number
>     is="+x) where x[1] is the first bit of x.
> 
>     (b). length extension attacks
> 
>     Even if all the adversary can do is create two random values that
>     collide, you can append substrings to the input and get collisions.
>     Once you find two random values hash(x) = hash(y), you could use a
>     length extension attack on RIPEMD-160 to find hash(x||z) = hash(y||z).
> 
>     Now the bitcoin wiki says:
>     "The padding scheme is identical to MD4 using MerkleDamgrd
>     strengthening to prevent length extension attacks."[1]
> 
>     Which is confusing to me because:
> 
>     1. MD4 is vulnerable to length extension attacks
>     2. MerkleDamgrd strengthening does not protect against length
>     extension: "Indeed, we already pointed out that none of the 64
>     variants above can withstand the 'extension' attack on the MAC
>     application, even with the Merkle-Damgard strengthening" [2]
>     3. RIPEMD-160 is vulnerable to length extension attacks, is Bitcoin
>     using a non-standard version of RIPEMD-160.
> 
>     RIPEMD160(SHA256()) does not protect against length extension attacks
>     on SHA256, but should protect RIPEMD-160 against length extension
>     attacks as RIPEMD-160 uses 512-bit message blocks. That being said we
>     should be very careful here. Research has been done that shows that
>     cascading the same hash function twice is weaker than using HMAC[3]. I
>     can't find results on cascading RIPEMD160(SHA256()).
> 
>     RIPEMD160(SHA256()) seems better than RIPEMD160() though, but security
>     should not rest on the notion that an attacker requires 2**80 memory,
>     many targeted collision attacks can work without much memory.
> 
>     [1]: https://en.bitcoin.it/wiki/RIPEMD-160
>     [2]: "Merkle-Damgard Revisited: How to Construct a Hash Function"
>     https://www.cs.nyu.edu/~puniya/papers/merkle.pdf
>     [3]: https://www.cs.nyu.edu/~dodis/ps/h-of-h.pdf
> 
>     On Thu, Jan 7, 2016 at 4:06 PM, Gavin Andresen via bitcoin-dev
>     <bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>     > Maybe I'm asking this question on the wrong mailing list:
>     >
>     > Matt/Adam: do you have some reason to think that RIPEMD160 will be
>     broken
>     > before SHA256?
>     > And do you have some reason to think that they will be so broken
>     that the
>     > nested hash construction RIPEMD160(SHA256()) will be vulnerable?
>     >
>     > Adam: re: "where to stop"  :  I'm suggesting we stop exactly at
>     the current
>     > status quo, where we use RIPEMD160 for P2SH and P2PKH.
>     >
>     > Ethan:  your algorithm will find two arbitrary values that
>     collide. That
>     > isn't useful as an attack in the context we're talking about here
>     (both of
>     > those values will be useless as coin destinations with overwhelming
>     > probability).
>     >
>     > Dave: you described a first preimage attack, which is 2**160 cpu
>     time and no
>     > storage.
>     >
>     >
>     > --
>     > --
>     > Gavin Andresen
>     >
>     > _______________________________________________
>     > bitcoin-dev mailing list
>     > bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>     >
> 
> 
> 
> 
> -- 
> --
> Gavin Andresen
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
Hi Bryan,

On Thu, Feb 25, 2016 at 07:34:24PM -0600, Bryan Bishop wrote:
> Well if you are bothering to draft up a BIP about that SIGHASH flag,
> then perhaps also consider some other SIGHASH flag types as well while
> you are at it?

I'll take a look at those proposals when drafting the BIP. I think for
LN, there is a single clean way to achieve outsourcability, but may be
compatible with other arrangements. I'm somewhat averse to proposing too
much flexibility before there's clear use-cases, though. However, if
others do have uses/examples for other sighash flags, I'd be very
interested while drafting this BIP!

> FWIW there was some concern about replay using SIGHAHS_NOINPUT or something:
> http://gnusha.org/bitcoin-wizards/2015-04-07.log

Yeah, I think the nice thing about SegWit is that you resolve
malleability without worrying about replay attacks in the event of key
reuse. That's why I think it's only safe to do this new sighash type
inside segwit itself -- if you only wanted protection against
malleability you'd use segwit, and not touch this new sighash type
(you'd only use the new sighash flag if you actually need its features).

-- 
Joseph Poon


-------------------------------------
On Mon, Mar 7, 2016 at 8:06 PM, G. Andrew Stone via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> The Bitcoin Unlimited client needs a services bit to indicate that the node
> is capable of communicating thin blocks.  We propose to use bit 4 as AFAIK
> bit 3 is already earmarked for Segregated Witness.

Does this functionality change peer selection?  If not, the preferred
signaling mechanism is probably the one in BIP 130.

Otherwise, I think the standard method for getting numbers has been to
write a BIP documenting the usage. I don't know if that is intentional
or just how things have previously happened; and I don't have much of
an opinion on it.


-------------------------------------
Am 21.04.2016 um 17:28 schrieb Eric Lombrozo:
> In practice the probability of this case triggering is on the order of
> 2^-128 or something astronomically tiny. I've been using BIP32 for a few
> years already as have many others...I don't think we've ever had to
> handle this case. Justifiably, many app developers feel like the
> additional complexity of properly handling this case is not worth the
> effort.
> 
> Having said that, if the handling of this case is simple to implement
> and easy to isolate in the program flow, I am in favor of doing
> something along the lines of what you propose.
> 

Yes, the idea is to handle the problem in the library so that app
developers don't have to handle the case of missing addresses or just
ignore the problem.  It also doesn't add much complexity to the library
as the current implementations already test for invalid keys.  The
library would then just retry instead of returning an error (that most
app developers would then ignore).

  Jochen


-------------------------------------
It will prevent companies from legally selling mining rigs with the
improvement, which stems access to the improvement in patented
countries. Or miners can export rigs with the improvement from companies
that sell it in non-patented countries.

It is not purely a software thing - it is intended to be used by
modifying hardware. From the paper: "The performance gain is achieved
through a high-­level optimization of the Bitcoin mining algorithm which
allows for drastic reduction in gate count on the mining chip. AsicBoost
is applicable to all types of mining hardware and chip designs."

Ultimately though, I think you're right in that Bitcoin's mining and
decentralized design combined with an international economy makes
patenting mining algorithms effectively pointless.

Mustafa

On 06/04/16 12:57, Marek Palatinus wrote:
> To my understanding it is purely software thing. It cannot be detected
> from outside if miner uses this improvement or not. So patenting it is
> worthless.
>
> slush
>
> On Tue, Apr 5, 2016 at 1:01 AM, Mustafa Al-Bassam via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>
>     Alternatively scenario: it will cause a sudden increase of Bitcoin
>     mines in countries where the algorithm is not patented, possibly
>     causing a geographical decentralization of miners from countries
>     that already have a lot of miners like China (if it is patented in
>     China).
>
>     On 01/04/16 10:00, Peter Todd via bitcoin-dev wrote:
>>     On Thu, Mar 31, 2016 at 09:41:40PM -0700, Timo Hanke via bitcoin-dev wrote:
>>>     Hi.
>>>
>>>     I'd like to announce a white paper that describes a very new and
>>>     significant algorithmic improvement to the Bitcoin mining process which has
>>>     never been discussed in public before. The white paper can be found here:
>>>
>>>     http://www.math.rwth-aachen.de/~Timo.Hanke/AsicBoostWhitepaperrev5.pdf
>>>     <http://www.math.rwth-aachen.de/%7ETimo.Hanke/AsicBoostWhitepaperrev5.pdf>
>>     What steps are you going to take to make sure that this improvement is
>>     available to all ASIC designers/mfgs on a equal opportunity basis?
>>
>>     The fact that you've chosen to patent this improvement could be a
>>     centralization concern depending on the licensing model used. For example, one
>>     could imagine a licensing model that gave one manufacture exclusive rights.
>>
>>
>>
>>     _______________________________________________
>>     bitcoin-dev mailing list
>>     bitcoin-dev@lists.linuxfoundation.org
>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-------------------------------------
Although it is technically possible to bundle 2 independent softforks in one release, it increases the burden of testing and maintenance. We need to test and prepare for 4 scenarios: both not activated, only NULLDUMMY activated, only SEGWIT activated, and both activated.

Also, as we learnt from BIP66, softfork activation could be risky. It is evident that today a non-negligible percentage of miners are hard-coding the block version number. This increases the risks of softfork transition as miners may not enforce what they are signaling (btw this is also happening on testnet) Making 2 independently softforks would double the risks, and I believe NULLDUMMY alone is not worth the risks.
 
> On September 2, 2016 at 1:10 PM Tom Harding via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> 
> On 9/1/2016 9:40 PM, Johnson Lau via bitcoin-dev wrote:
> > This BIP will be deployed by "version bits" BIP9 using the same parameters for BIP141 and BIP143, with the name "segwit" and using bit 1.
> >
> 
> This fix has value outside of segwit.  Why bundle the two together? 
> Shouldn't miners have to opportunity to vote on them independently?
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
On Thu, Jun 30, 2016 at 08:25:45PM +0200, Eric Voskuil wrote:
> > To be clear, are you against Bitcoin Core's tor support?
> > 
> > Because node-to-node connections over tor are encrypted, and make use of onion
> > addresses, which are self-authenticated in the exact same way as BIP151 proposes.
> 
> BIP151 is self-admittedly insufficient to protect against a MITM attack. It proposes node identity to close this hole (future BIP required). The yet-to-be-specified requirement for node identity is the basis of my primary concern. This is not self-authentication.
> 
> > And we're shipping that in production as of 0.12.0, and by default Tor onion support is enabled and will be automatically setup if you have a recent version of Tor installed.
> > 
> > Does that "create pressure to expand node identity"?
> 
> The orthogonal question of whether Tor is safe for use with the Bitcoin P2P protocol is a matter of existing research.

I don't think you answered my question.

Again, we _already have_ the equivalent of BIP151 functionality in Bitcoin
Core, shipping in production, but implemented with a Tor dependency.

BIP151 removes that dependency on Tor, enabling encrypted connections
regardless of whether or not you have Tor installed.

So any arguments against BIP151 being implemented, are equally arguments
against our existing Tor onion support. Are you against that support? Because
if you aren't, you can't have any objections to BIP151 being implemented
either.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Thursday, December 01, 2016 5:20:31 PM Johnson Lau via bitcoin-dev wrote:
> Any bitcoin implementation (full nodes and light nodes) supporting this
> softfork should also implement a hardfork warning system described below.

I think this "should" needs to be a "must" be make this useful.

> Hardfork with unknown rules
> If a generalized block header chain with non-trivial total proof-of-work is
> emerging, and is not considered as a valid blockchain, a hardfork with
> unknown rules may be happening.
> 
> A wallet implementation should issue a warning to its users and stop
> processing incoming and outgoing transactions, until further instructions
> are given. It should not attempt to conduct transactions on or otherwise
> interpreting any block data of the hardfork with unknown rules.

This seems too unclear. Specifically, if an invalid chain with *equivalent or 
better* work than the best valid chain exists, nodes ought to treat all blocks 
following the common chain (between the better-invalid and best-valid chains) 
as suspect.

So if we have two chains:

    A->B->C->D (valid)
    A->B->X->Y (invalid)

The node should consider block B as the tip until the valid chain becomes and 
stays longer than the invalid one.

> A mining implementation should issue a warning to its operator. Until
> further instructions are given, it may either stop mining, or ignore the
> hardfork with unknown rules. It should not attempt to confirm a
> generalized block header with unknown rules.

I think we need to decide more specifically which behaviour is sane here.

> Hardfork warning system in light nodes
> 
> Light node (usually wallet implementations) is any bitcoin protocol
> implementations that intentionally not fully enforcing the network rules.
> As an important part of the hardfork warning system, a light node should
> observe the hardfork notification bits in block header, along with any
> other rules it opts to validate. If any of the hardfork notification bits
> is set, it should issue a warning to its users and stop processing
> incoming and outgoing transactions, until further instructions are given.
> It should not attempt to conduct transactions on or otherwise interpreting
> any block data of the hardfork blockchain, even if it might be able to
> decode the block data.

Light nodes should probably not be specified here differently than full nodes. 
If they detect an invalid block through *any* means, they should react the 
same as a full node would.

> Redefining the Merkle root hash field and changing block content validation
> rules The 32-byte Merkle root hash could be redefined, for example, with a
> different hashing algorithm. Any block resources limitation and
> transaction validation rules may also be changed. All such hardforks would
> be detected by the warning system.

Note, some changes may be needed to current nodes for this to work. I think at 
this time this would cause a "deserialisation" error, and not accept NOR 
reject the block...

> Introducing secondary proof-of-work
> Introducing secondary proof-of-work (with non-SHA256 algorithm or fixing
> the block withholding attack against mining pools) may be detectable, as
> long as the generalized block header format is preserved.

Not necessarily. A secondary PoW might drastically change the measurement of 
work. Fixing block withholding may result in block hashes that meet a preimage 
rather than bits directly. I think it may be important to fix the latter 
problem for this BIP.

> Accidental hardfork
> An accidental hardfork may be detectable, if the generalized block headers
> in both forks are valid but no hardfork notification bit is set.

Probably best to not call this a hardfork, since it is a break without 
consensus.

Luke


-------------------------------------
Jorge Timón said..
> What do you mean by "embrace" in the context of a patented optimization
that one miner can prevent the rest from using?

Everyone seems to assume that one ASIC manufacturer will get the advantage
of AsicBoost while others won't. If a patent license is non-exclusive, then
all can.

-------------------------------------
> As said, Open Asset is not a draft proposal and is already used in the
wild since 2014. We can't easily modify the protocol by now for improving
it.

You can, however, provide a new OA2.0 protocol that improves upon these
issues, and assure that upgraded wallets maintain support for both
versions.

It seems like OA's stance has *always *been to focus on integration, rather
than fixing the core protocol and then, by virtue of having the largest
integration, saying things like "it's too late to turn back now".    Colu
and Chromaway/EPOBC also have stuff "in the wild".

I would love to see an RFC-style standard "multiple-colored-coin-protocol"
written by reps from all of the major protocols and that meta-merges the
features of these implementations - in collaboration with feedback from
core developers that understand the direction the protocol will be taking
and the issues to avoid.   HTTP/TCP/IP MCCP/BTC

As it stands, investors have to install multiple wallets to deal with these
varying implementations.   Merging them into one "meta-specification"
fairly soon might be in the best interests of the community and of future
shareholders.

-------------------------------------
On Fri, Jan 8, 2016 at 3:46 PM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> How many years until we think a 2^84 attack where the work is an ECDSA
> private->public key derivation will take a reasonable amount of time?
>

I think the EC multiply is not actually required.  With compressed public
keys, the script selection rule can just be a sha256 call instead.

V is the public key of the victim, and const_pub_key is the attacker's
public key.

     if prev_hash % 2 == 0:
        script = "2 V 0x02%s 2 CHECKMULTISIG" % (sha256(prev_hash)))
    else:
        script = "CHECKSIG %s OP_DROP" % (prev_hash, const_pub_key)

    next_hash = ripemd160(sha256(script))

If a collision is found, there is a 50% chance that the two scripts have
different parity and there is a 50% chance that a compressed key is a valid
key.

This means that you need to run the algorithm 4 times instead of 2.

The advantage is that each step is 2 sha256 calls and a ripemd160 call.  No
EC multiply is required.

-------------------------------------
Maybe there are still some advantages but I don't know why this is not
considered as a major issue by the bitcoin community for the future and
why this looks to be never discussed:

- the size of the bitcoin network in terms of full nodes is ridiculous
and this is continuously decreasing, we cannot consider the bitcoin
network as a decentralized p2p network, what you are proposing is
logical but will of course amplify the problem

>For reasons I am unable to determine a significant number of node
operators do not upgrade their clients.

Why should they? What is the incentive for people to run full nodes and
upgrade? FYI I am part of the 2071 0.13.1 nodes for some good reasons
but will just shut it down when I am done, same for zcash (which as a
matter of fact I upgraded today since by some chance I noticed some
updates I was not aware of neither notified, just running it because I
need it from time to time and just don't kill it so I don't have to wait
for the restart process, maybe others are doing the same or just forgot
that they were running a full node)

Because, again, why should I or we maintain it/them?

I have looked at the proposals in the past (as well as the incentive
program) to reward those that are running full nodes but only found a
very few, never implemented (or even considered)

This is the very same for proposals allowing to start a full node from
zero in an acceptable timeframe (ie not 10 days in my case)

If the consensus is not to solve those two points and have a bitcoin
network controlled then it would be interesting to know why, so people
don't waste time trying to find solutions

Satoshi himself predicted that the full nodes will get centralized, I
think it's wrong, or in that case the bitcoin network cannot pretend to
be a decentralized immutable system (can be compared then to the Tor
network which does not pretend to be decentralized, because it is
centralized, and in addition does not encourage small nodes)

PS: IMHO the email notificiation system makes it difficult to follow
whom is answering to whom/what on this list compared to other lists

-- 
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms




-------------------------------------
On Monday 26 Sep 2016 14:41:36 Peter Todd via bitcoin-dev wrote:
> Note how the OPL is significantly more restrictive than the Bitcoin Core
> license; not good if we can't ship documentation with the code.

Documentation and code can have different licenses, the sole existence of 
various documentation licenses attests to that point.
Shipping your docs under a separate licence has never been a problem before, 
so you don't have to worry that you can't ship documentation with code.

That said, I wrote my suggestion in reply to Luke's BIP2 revival which is a 
more formal suggestion of a solution. Maybe you can ACK that one instead?

Last, in preparation of acceptance of BIP2 I changed the licence of my BIP to 
be dual-licensed.  Now its also available under a Creative Commons license.

Have a nice day!


-------------------------------------
The section that starts "Should two software projects need to release"
addresses issues that are difficult to ascertain from what is written
there.  I'll take a stab at what it means:

Would bitcoin be better off if multiple applications provided their own
implementations of API/RPC and corresponding application layer BIPs?

   - While there is only one such application, its UI will be the obvious
   standard and confusion in usability will be avoided.
   - Any more than a single such application will benefit from the
   coordination encouraged and aided by this BIP and BIP 123.

"To avoid doubt: comments and status are unrelated metrics to judge a BIP,
and neither should be directly influencing the other." makes more sense to
me as "To avoid doubt: comments and status are intended to be unrelated
metrics. Any influence of one over the other indicates a deviation from
their intended use."  This can be expanded with a simple example: "In other
words, a BIP having  the status 'Rejected' is no reason not to write
additional comments about it.  Likewise, overwhelming support for a BIP in
its comments section doesn't change the requirements for the 'Accepted' or
'Active' status."

Since the Bitcoin Wiki can be updated with comments from other places, I
think the author of a BIP should be allowed to specify other Internet
locations for comments.  So "link to a Bitcoin Wiki page" could instead be
"link to a comments page (strongly recommended to be in the Bitcoin
Wiki)".  Also, under "Will BIP comments be censored or limited to
particular participants/"experts"?" You could add:

   - The author of a BIP may indicate any commenting URL they wish.  The
   Bitcoin Wiki is merely a recommendation, though a very strong one.


On Mon, Feb 1, 2016 at 2:53 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I've completed an initial draft of a BIP that provides clarifications on
> the
> Status field for BIPs, as well as adding the ability for public comments on
> them, and expanding the list of allowable BIP licenses.
>
>
> https://github.com/luke-jr/bips/blob/bip-biprevised/bip-biprevised.mediawiki
>
> I plan to open discussion of making this BIP an Active status (along with
> BIP
> 123) a month after initial revisions have completed. Please provide any
> objections now, so I can try to address them now and enable consensus to be
> reached.
>
> Thanks,
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
Hi

> The main benefit is that you don't need "standard" to solve problem, but
> use natural tools in given environment and programming stack. Build a
> "standard" on top of URI protocol is a huge limitation, which does not
> give any advantage.

Standards can help an ecosystem to grow, can help to sustain a good user
experience.

The hardware wallet vendors have used "natural tools" and look where we
are. We have *native* plugins in Electrum, Copay, etc. for different
hardware wallets. Mostly the plugins are in the code base of the wallet,
which makes it – in theory – impossible to change from the perspective
of the hardware wallet vendor (there is no control of the deployment if
there are bugs in the plugins code).
The plugins functions overlap significant.

I think this is a bad design for security critical applications.

What I want as hardware wallet user:
* I'd like to have a trusted application (layer) where I'm sure I'm
using software provided through my hardware wallet vendor.

What I want as hardware wallet vendor:
* I'd like to be able to provide and update a software layer (app) to my
customer with the ability to provide code signatures and security
updates anytime. I do want to control the user experience.


> We already see issues with dead simple "bitcoin uri" standard, it barely
> works in most of bitcoin apps. Think of vague definitions of parameters
> or ability to send payment requests over it. HW API would be complicated
> by an order of magnitude and I have serious concerns that it will be
> helpful for anything. So why complicate things.

As far as I know most bitcoin wallets do support the bitcoin:// URI
scheme quite well.
I agree that BIP70 is a mess (including the bitcoin:// additions).

The proposed URI scheme would be completely different. The only
similarity is using the URI scheme as transport layer (which is the
proposed long term inter-app communication layer by Apple and Google).

>> How would the library approach work on mobile platforms? Would USB be
> the only supported hardware communication layer?
> 
> Interprocess communication/libraries/dependencies on Android are not
> bound to specific transport anyhow. Such library could be used by any
> android app, and the library would implement proper transports for
> various supported vendors. USB for Trezor, NFC for something different
> etc. If the point is "make life of app developers easier", let's do this
> and do not define artifical "standards".

So you propose having one library that would support multiple vendors?
What if new vendors add a new transport layer (lets assume NFC or
Bluetooth), wouldn't that result in every possible consumer of that
library (all wallets) need to update before the new vendors transport
layer could be used, resulting in a huge deployment process probably
require many month until it can be used?

What if there is a critical security issue in the library? How would the
deployment plan looks like?

I really think we should remove the "hardware communication layer" from
wallets and move it towards the hardware vendor app.

What about iOS? Should we just leave that platform unsupported with
hardware wallets?

</jonas>


-------------------------------------
On Sat, Mar 26, 2016 at 1:34 AM Tom via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Friday 25 Mar 2016 19:43:00 Jonas Schnelli via bitcoin-dev wrote:
> > An encrypted channel together with a trusted full node would finally
> > allow to have a secure and save SPV communication.
>
> I guess my question didn't get across.
>
> Why would you want to make your usecase do connections over the peer2peer
> (net.cpp) connection at all?
>
> Mixing messages that are being sent to everyone and encrypted messages is
> asking for trouble.
> Making your private connection out-of-band would work much better.
>
>
I agree doing it out-of-band is the easiest solution for people who need
this privacy right now, but I do like the idea of adding this feature as
the number of SPV wallets is going to increase. I think the best way to
organize things would be to give encrypted messages their own port number,
similar to how http vs. https works.

We don't want two networks to develop, separated by which nodes support
encryption and which don't, so ideally nodes would rebroadcast messages
they receive on both (encrypted and non-encrypted) channels. This would
essentially double the required bandwidth of the network, which is
something to think about.


> > > Also, you didn't actually address the attack-vector.
> >
> > Which attack-vector?
>
> The statistical attack I mentioned earlier.  Which comes from knowing which
> plain text messages are being sent over the encrypted channel, So as long
> as
> you keep saying you want to encrypt data that identical copies of are being
> sent to other nodes at practically the same time, you will keep being
> vulnerable to that.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Hi All,

I'm coming late to the party. I like the Block75 proposal.

Multiple people have said miners would/could stuff blocks with insincere
transactions to increase the block size, but it was never adequately
explained what they would gain from this. If there aren't enough legitimate
transactions to fill up the block, where do you plan to earn extra income
once the block is bigger?

Miners would be incentivized to include as many legitimate transactions as
possible, but if propagation time is as big an issue as some of you have
said it is, miners would also be incentivized to keep their blocks small
enough to propagate. So why not give them the choice? Once the block size
gets too big to propagate effectively, miners would be naturally
incentivized to limit how much data they put in each block, finding the
perfect balance.

In my opinion, none of the downsides presented so far have been a good
argument. Risk of a 51% attack is not unique to this proposal, saying "we
could also do that with hardcoded limits" doesn't actually point out any
problem with this proposal, and miners already have the ability to add or
withhold transactions from their blocks.

We trust our miners to serve us by acting in their own best interests, and
this proposal simply gives them more options for doing that. If anyone can
make a strong argument against that would earn top marks in a high school
debate class, I'd love to hear it!

James

On Sun, Dec 11, 2016 at 3:23 PM s7r via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Andrew Johnson wrote:
> > "You miss something obvious that makes this attack actually free of cost.
> > Nothing will "cost them more in transaction fees". A miner can create
> > thousands of transactions paying to himself, and not broadcast them to
> > the network, but hold them and include them in the blocks he mines. The
> > fees are collected by him because transactions are included in a block
> > that he mined and the left amount is in another wallet of the same
> > person. Repeat this continuously to fill blocks."
> >
> > This is easily detectable as long as the network isn't heavily
> > partitioned(which is an assumption we make today in order for
> > transaction propagation to work reliably as well as for xThin and
> > CompactBlocks to work effectively to reduce block transmission time).
> > Other miners would have an incentive to intentionally orphan blocks that
> > contained a large number of transactions that their nodes were unaware
> of.
> >
> > I don't think this sort of attack would last long.  Even later when
> > subsidies are drastically reduced, you would still lose out on
> > significant genuine fee revenue if your orphan rate increased even
> > 10%(one out of ten of your poison blocks intentionally orphaned by
> > another miner).
> >
>
> I disagree.
>
> I didn't say this is impossible to detect, but it is hard to act against
> it. One miner orphaning the block intentionally is very unlikely if that
> miner acts rationally. It would only make sense if 51% of the hash rate
> would intentionally orphan it. Otherwise the miner who intentionally
> orphans a valid block, let's say block X, has to continue to mine one in
> its place on top of block X-1, and by the time he finds one:
>
> a) his block X' is rejected by other miners because they already have a
> valid block X on top of which they already started to mine;
>
> b) block X+1 was already found and broadcasted, so the miner who
> orphaned X intentionally is on the shorter chain ignored by the network.
>
> So, one miner cannot do anything about it. Even a pool cannot do
> anything about it, because the loss is greater. You need 51% of the hash
> rate to intentionally orphan it, and all the miners forming 51% need to
> be colluding and know for sure that every one will intentionally orphan
> the said block, otherwise there's a huge risk of loss for who does it.
> Nobody would gamble to do this (I am not sure if gambling is the right
> word, since the loss is 100% sure here). But, we are not discussing 51%
> attacks because those are a different topic.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Hi all!

        As planned, this is the three month review[1]: discussion of how
moderation should change is encouraged in this thread.

        First, thanks to everyone for the restraint shown in sending
(and responding to!) inflammatory or sand-in-the-gears mails, and being
tolerant with our mistakes and variances in moderation.

The only changes we made to the plan so far:
1) We've stopped clearing the "needs mod" bit after first posts, and
2) Trivially answerable emails or proposals have been answered in the
   reject message itself.

You can see almost all (there was some lossage) rejects at:
        https://lists.ozlabs.org/pipermail/bitcoin-dev-moderation/

So, what should moderation look like from now on?
- Stop moderating altogether?
- Moderate <topic> more/less harshly?
- Use a different method/criteria for moderation?
- Add/remove moderators?
- Other improvements?

Thanks,
Rusty.
[1] http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-October/011591.html


-------------------------------------
> the network. This would result in a significantly longer block interval, which 
> also means a higher per-block transaction volume, which could cause the block 
> size limit to legitimately be hit much sooner than expected.

If this happens at all (the exchange rate of the coin can accomodate such expectation), the local fee market will develop, fees will raise and complement mined coins, thus bringing more miners back to the game (together with expected higher exchange rate).
--  
Pavel Janík






-------------------------------------
On Wednesday, June 08, 2016 8:23:51 AM Johnson Lau wrote:
> If someday 32 bytes hash is deemed to be unsafe, the txid would also be
> unsafe and a hard fork might be needed. Therefore, I don’t see how a
> witness program larger than 40 bytes would be useful in any case (as it is
> more expensive and takes more UTXO space). I think Pieter doesn’t want to
> make it unnecessarily lenient.

There is no harm in being lenient, but it limits the ability to do softfork 
upgrades in the future. I appreciate Pieter's concern that we'd need to do 
more development and testing to go to this extreme, which is why I am only 
asking the limit raised to 75 bytes.

Luke


-------------------------------------
> This means that all future transactions will have different txids...
rules do guarantee it.

No, it means that the chance is small, there is a difference.

If there is an address collision, someone may lose some money. If there
is a tx hash collision, and implementations handle this differently, it
will produce a chain split. As such this is not something that a node
can just dismiss. If they do they are implementing a hard fork.

e

On 11/16/2016 04:31 PM, Tier Nolan via bitcoin-dev wrote:
> 
> 
> On Thu, Nov 17, 2016 at 12:10 AM, Eric Voskuil via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>     Both of these cases resulted from exact duplicate txs, which BIP34 now
>     precludes. However nothing precludes different txs from having the same
>     hash.
> 
> 
> The only way to have two transactions have the same txid is if their
> parents are identical, since the txids of the parents are included in a
> transaction.
> 
> Coinbases have no parents, so it used to be possible for two of them to
> be identical.
> 
> Duplicate outputs weren't possible in the database, so the later
> coinbase transaction effectively overwrote the earlier one.
> 
> The happened for two coinbases.  That is what the exceptions are for.
> 
> Neither of the those coinbases were spent before the overwrite
> happened.  I don't even think those coinbases were spent at all.
> 
> This means that every activate coinbase transaction has a unique hash
> and all new coinbases will be unique.
> 
> This means that all future transactions will have different txids.
> 
> There might not be an explicit rule that says that txids have to be
> unique, but barring a break of the hash function, they rules do
> guarantee it.


-------------------------------------
There is no way to tell from a block if it was mined with AsicBoost or not.
So you don’t know what percentage of the hashrate uses AsicBoost at any
point in time. How can you risk forking that percentage out? Note that this
would be a GUARANTEED chain fork. Meaning that after you change the block
mining algorithm some percentage of hardware will no longer be able to
produce valid blocks. That hardware cannot “switch over” to the majority
chain even if it wanted to. Hence you are guaranteed to have two
co-existing bitcoin blockchains afterwards.

Again: this is unlike the hypothetical persistence of two chains after a
hardfork that is only contentious but doesn’t change the mining algorithm,
the kind of hardfork you are proposing would guarantee the persistence of
two chains.

Note that “AsicBoost” above is replaceable with “optimization X”. It’s
simply a logical argument: If you want to make optimization X impossible
and someone is already using optimization X you end up with two chains. So
unless you know exactly which optimizations are in use (and therefore also
know which ones are not in use) you can’t make these kind of changes.
AsicBoost is known at least since middle of 2013.

To be more precise, if you change the block validation ruleset R to block
validation ruleset S you have to make sure that every hardware that was
capable of mining R-valid blocks is also capable of mining S-valid blocks.

The problem is that chip manufacturers will not tell you which
optimizations they use. You would have to threaten to irreversibly fork
their hardware out by a rule change, only then would they start shouting
and reveal their optimization. It seems extremely dangerous to set the
precedence of a hardfork that irreversibly forks out a certain type of
mining hardware.

The part "Also the fix should be compatible with existing mining hardware."
is impossible to achieve because it's unclear what "existing mining
hardware" is. There has never been a specification of what mining hardware
should do. There are only acceptance rules.

The only way out is to go the exact opposite way and to embrace as many
optimizations as possible to the point where there are no more
optimizations left to do, or hopefully getting very close to that point.

Timo



On Tue, May 10, 2016 at 11:57 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> As part of the hard-fork proposed in the HK agreement(1) we'd like to make
> the
> patented AsicBoost optimisation useless, and hopefully make further similar
> optimizations useless as well.
>
> What's the best way to do this? Ideally this would be SPV compatible, but
> if it
> requires changes from SPV clients that's ok too. Also the fix this should
> be
> compatible with existing mining hardware.
>
>
> 1)
> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff
>
> 2)
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
The wording is a little strange and I think it *should* work as you state,
but Bitcoin Core will actually reject any output that has zero value (even
a single OP_RETURN output -- I just tested again to make sure).

Here's the blocking code:

https://github.com/bitcoin/bitcoin/blob/master/src/qt/paymentserver.cpp#L584

I agree that this should be made more clear in my BIP though, I'll clean up
the language.

On Tue, Jan 26, 2016 at 6:37 AM, Andreas Schildbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Discussion about reasoning of OP_RETURN aside, I think your
> specification needs to be more precise/less ambiguous.
>
> Here is what BIP70 currently says about PaymentDetails.outputs:
>
> "one or more outputs where Bitcoins are to be sent. If the sum of
> outputs.amount is zero, the customer will be asked how much to pay, and
> the bitcoin client may choose any or all of the Outputs (if there are
> more than one) for payment. If the sum of outputs.amount is non-zero,
> then the customer will be asked to pay the sum, and the payment shall be
> split among the Outputs with non-zero amounts (if there are more than
> one; Outputs with zero amounts shall be ignored)."
>
> As you can see, zero outputs are not ignored at all. They are used as an
> indication to allow the user to set an amount. So if you'd come up with
> one zero-amount OP_RETURN output, it would pop up an amount dialog.
> Certainly not what you want, right?
>
>
> On 01/26/2016 03:54 AM, Toby Padilla via bitcoin-dev wrote:
> > It looks like my draft hasn't been approved by the mailing list so if
> > anyone would like to read it it's also on Gist:
> >
> > https://gist.github.com/toby/9e71811d387923a71a53
> >
> > Luke - As stated in the Github thread, I totally understand where you're
> > coming from but the fact is people *will* encode data on the blockchain
> > using worse methods. For all of the reasons that OP_RETURN was a good
> > idea in the first place, it's a good idea to support it in
> PaymentRequests.
> >
> > As for keyless - there's no way (that I know of) to construct a
> > transaction with a zero value OP_RETURN in an environment without keys
> > since the Payment Protocol is what defines the method for getting a
> > transaction from a server to a wallet. You can make a custom transaction
> > and execute it in the same application but without Payments there's no
> > way to move transactions between two applications. You need to build the
> > transaction where you execute it and thus need a key.
> >
> >
> >
> > On Mon, Jan 25, 2016 at 6:24 PM, Luke Dashjr <luke@dashjr.org
> > <mailto:luke@dashjr.org>> wrote:
> >
> >     This is a bad idea. OP_RETURN attachments are tolerated (not
> >     encouraged!) for
> >     the sake of the network, since the spam cannot be outright stopped.
> >     If it
> >     could be outright stopped, it would not be reasonable to allow
> >     OP_RETURN. When
> >     it comes to the payment protocol, however, changing the current
> >     behaviour has
> >     literally no benefit to the network at all, and the changes proposed
> >     herein
> >     are clearly detrimental since it would both encourage spam, and
> >     potentially
> >     make users unwilling (maybe even unaware) participants in it. For
> these
> >     reasons, *I highly advise against publishing or implementing this
> >     BIP, even if
> >     the later mentioned issues are fixed.*
> >
> >     On Tuesday, January 26, 2016 1:02:44 AM Toby Padilla wrote:
> >     > An example might be a merchant that adds the hash of a plain text
> invoice
> >     > to the checkout transaction. The merchant could construct the
> >     > PaymentRequest with the invoice hash in an OP_RETURN and pass it
> to the
> >     > customer's wallet. The wallet could then submit the transaction,
> including
> >     > the invoice hash from the PaymentRequest. The wallet will have
> encoded a
> >     > proof of purchase to the blockchain without the wallet developer
> having to
> >     > coordinate with the merchant software or add features beyond this
> BIP.
> >
> >     Such a "proof" is useless without wallet support. Even if you argue
> >     it could
> >     be implemented later on, it stands to reason that a scammer will
> >     simply encode
> >     garbage if the wallet is not checking the proof-of-purchase upfront.
> >     To check
> >     it, you would also need further protocol extensions which are not
> >     included in
> >     this draft.
> >
> >     > Merchants and Bitcoin application developers benefit from this BIP
> because
> >     > they can now construct transactions that include OP_RETURN data in
> a
> >     > keyless environment. Again, prior to this BIP, transactions that
> used
> >     > OP_RETURN (with zero value) needed to be constructed and executed
> in the
> >     > same software. By separating the two concerns, this BIP allows
> merchant
> >     > software to create transactions with OP_RETURN metadata on a
> server without
> >     > storing public or private Bitcoin keys. This greatly enhances
> security
> >     > where OP_RETURN applications currently need access to a private
> key to sign
> >     > transactions.
> >
> >     I don't see how this has any relevance to keys at all...
> >
> >     > ## Specification
> >     >
> >     > The specification for this BIP is straightforward. BIP70 should be
> fully
> >     > implemented with two changes:
> >     >
> >     > 1. Outputs where the script is an OP_RETURN and the value is zero
> should be
> >     > accepted by the wallet.
> >     > 2. Outputs where the script is an OP_RETURN and the value is
> greater than
> >     > zero should be rejected.
> >     >
> >     > This is a change from the BIP70 requirement that all zero value
> outputs be
> >     > ignored.
> >
> >     This does not appear to be backward nor even forward compatible. Old
> >     clients
> >     will continue to use the previous behaviour and transparently omit
> any
> >     commitments. New clients on the other hand will fail to include
> >     commitments
> >     produced by old servers. In other words, it is impossible to produce
> >     software
> >     compatible with both BIP 70 and this draft, and implementing either
> >     would
> >     result in severe consequences.
> >
> >     > As it exists today, BIP70 allows for OP_RETURN data storage at the
> expense
> >     > of permanently destroyed Bitcoin.
> >
> >     It is better for the spammers to lose burned bitcoins, than have a
> >     way to
> >     avoid them.
> >
> >     Luke
> >
> >
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Sun, Oct 02, 2016 at 02:26:18PM -0300, Sergio Demian Lerner wrote:
> I'm not a lawyer, and my knowledge on patents is limited. I guess RSK WILL
> endorse DPL or will make the required actions to make sure the things
> developed by RSK remain free and open. This was not a priority until now,
> but coding was. For me, coding always is the priority.
> 
> I will discuss prioritizing this with the team. Remember it took several
> years to BlockStream to decide for DPL and not just publish everything as
> they were doing. I suppose the decision it was not a simple one, involving
> lawyers advise and all. I guess DPL needs to actually patent the things in
> order to open them later, and patenting has a very high cost.
> 
> Give us time to decide which open source strategy is the best and cheaper
> for RSK. At this point I can assert that RSK has not filed any patent not
> is planing to.  This proposal is not encumbered by any patents, and
> drivechains is actually not RSK's idea, but Paul Sztorc's.

Thanks, please let us all know when this is done so we can continue our
collaborations constructively.

I'll likewise prioritise my own adoption of the DPL and will announce it on
this mailing list.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Wed, May 11, 2016 at 11:21:10AM +0200, Jannes Faber via bitcoin-dev wrote:
> On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> > There is no way to tell from a block if it was mined with AsicBoost or
> > not. So you don’t know what percentage of the hashrate uses AsicBoost at
> > any point in time. How can you risk forking that percentage out? Note that
> > this would be a GUARANTEED chain fork. Meaning that after you change the
> > block mining algorithm some percentage of hardware will no longer be able
> > to produce valid blocks. That hardware cannot “switch over” to the majority
> > chain even if it wanted to. Hence you are guaranteed to have two
> > co-existing bitcoin blockchains afterwards.
> >
> > Again: this is unlike the hypothetical persistence of two chains after a
> > hardfork that is only contentious but doesn’t change the mining algorithm,
> > the kind of hardfork you are proposing would guarantee the persistence of
> > two chains.
> >
> 
> Assuming AsicBoost miners are in the minority, their chain will constantly
> get overtaken. So it will not be one endless hard fork as you claim, but
> rather AsicBoost blocks will continue to be ignored (orphaned) until they
> stop making them.

At least until a difficulty adjustment on the AsicBoost chain takes
place. From that point on, both chains, the AsicBoost one and the
forked one will grow approximately at the same speed.

All the best
Henning


-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------
On Saturday, February 06, 2016 06:09:21 PM Jorge Timn via bitcoin-dev wrote:
> None of the reasons you list say anything about the fact that "being lost"
> (kicked out of the network) is a problem for those node's users.

That's because its not.

If you have a node that is "old" your node will stop getting new blocks. 
The node will essentially just say "x-hours behind" with "x" getting larger 
every hour. Funds don't get confirmed. etc.

After upgrading the software they will see the new reality of the network.

Nobody said its a problem, because its not.

-- 
Tom Zander


-------------------------------------
My understanding of the paper is that the blinding factor would be 
included in the extra data which is incorporated into the ring 
signatures used in the range proof.

Although, since I think the range proof is optional for single output 
transactions (or at least, one output per transaction doesn't require a 
range proof since there's only one possible value that it can be to make 
the whole thing work, and that value must be in range, I'm not entirely 
sure how you'd transmit it then, though in any case, since using it will 
pretty much require segwit, adding extraneous data isn't much of a 
problem.  In both cases, I imagine the blinding factor would be 
protected from outside examination via some form of shared secret 
generation... Although that would require the sender to know the 
recipient's unhashed public key; I don't know of any shared secret 
schemes that will work on hashed keys.

Jeremy Papp

On 2/9/2016 7:12 AM, Henning Kopp via bitcoin-dev wrote:
> Hi all,
>
> I am trying to fully grasp confidential transactions.
>
> When a sender creates a confidential transaction and picks the blinding
> values correctly, anyone can check that the transaction is valid. It
> remains publically verifiable.
> But how can the receiver of the transaction check which amount was
> sent to him?
> I think he needs to learn the blinding factor to reveal the commit
> somehow off-chain. Am I correct with this assumption?
> If yes, how does this work?
>
> All the best
> Henning
>



-------------------------------------
Dave Scotese via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:
> It is a shame that the moderated messages require so many steps to
> retrieve.  Is it possible to have the "downloadable version" from
> https://lists.ozlabs.org/pipermail/bitcoin-dev-moderation/ for each month
> contain the text of the moderated emails?  They do contain the subjects, so
> that helps.

Yes, it's because we simply forward them to the bitcoin-dev-moderation
mailing list, and it strips them out as attachments.

I'd really love a filter which I could run them through (on ozlabs.org)
to fix this.  Volunteers welcome :)

Cheers,
Rusty.


-------------------------------------
On Sat, Sep 10, 2016 at 12:58 AM, Peter Todd <pete@petertodd.org> wrote:
> Good to do this sooner rather than later, as alert propagation on the P2P
> network is going to continue to get less reliable as nodes upgrade to software

Yes, this was one of my motivations for doing this soon.

It would only require about 2 LOC to have Bitcoin Core vomit out a
blob containing the final alert to any old protocol version peers that
connect.  I don't know how other people would feel about that, but I
wouldn't mind implementing it, and it would greatly improve the
likelihood that they continue to to get once propagation of it is
gone. This could be left in the codebase for a couple years or until
other changes made those old versions p2p incompatible for other
reasons.


-------------------------------------

> On Feb 7, 2016, at 2:27 PM, <jl2012@xbt.hk> <jl2012@xbt.hk> wrote:
> 
> Normal version number only suggests softforks, which is usually not a concern for SPV clients.

Soft forks affect the security of low-confirmation (zero or one) transactions sent to SPV wallets even more than hard forks, and because many users and businesses choose convenience over airtight security I would argue transaction validation rule changes are a VERY big concern for lightweight clients.

-------------------------------------
There is probably not much loss due to per message encryption.  Even if a
MITM determined that a message was an inv message (or bloom filter
message), it wouldn't be able to extract much information.  Since the
hashes in those messages are fixed size, there is very little leakage.

You could make it so that the the encryption messages effectively create a
second data stream and break/weaken the link between message size and
wrapped message size.  This requires state though, so there is a complexity
tradeoff.

There is no real need to include an IV, since you are including a 32 byte
context hash.  The first 16 bytes of the context hash could be used as IV.

In terms of generating the context hash, it would be easier to make it
linear.

context_hash_n = SHA256(context_hash_(n-1) | message_(n-1))

As the session gets longer, both nodes would have to do more and more
hashing to compute the hash of the entire conversation.

-------------------------------------
On Monday, January 25, 2016 01:03:17 PM Wladimir J. van der Laan wrote:
> > If yes, I would highly recommend advertising it in the new release notes -
> > as said, the disk space reduction is a big deal.
> 
> Good idea, has been added by Marco Falke in commit fa31133,

Thanks. The RC2 changelog now says:

> To enable block pruning set prune=<N> on the command line or in
> bitcoin.conf, where N is the number of MiB to allot for raw block & undo
> data.

From having read the Bitcoin whitepaper quite a few months ago ago, I have the 
very very basic understanding that pruning is meant to:
- delete old transaction data which merely "moves coins around"
- instead only store the "origin" (= block where coins were mined) and 
"current location" of the coins, i.e. the unspent transactions. Notably, I 
understood it as "this is as secure as storing everything, since we know where 
the coins were created, and where they are".

So from that point of view, I would assume that there is a "natural" amount of 
megabytes which a fully pruned blockchain consists of: It would be defined by 
the final amount of unspent coins.
I thereby am confused why it is possible to configure a number of megabytes 
"to allot for raw block & undo data". I would rather expect pruning just to be 
a boolean on/off flag, and the number of megabytes to be an automatically 
computed result from the natural size of the dataset.
And especially, I fear that I could set N too low, and as a result, it would 
delete "too much". I mean could this result in even security relevant 
transaction data being deleted?

Thus, it would be nice if you could yet once more edit the release notes to:
- explain why a N must be given
- what a "safe" value of N is. I.e. how large must N be at least to not delete 
security-relevant stuff?
- maybe mention if there is a "auto" setting for N to ensure that it choses a 
safe value on its own?

Sorry if my descriptions are from a layman's point of view. I intentionally 
did *not* re-read the Bitcoin whitepaper to have a better understanding:
I think having a layman's understanding is a good usability test for such 
stuff.
-------------------------------------
Have you tried out reddit's /r/btc and /r/Bitcoin?

On Sat, Aug 13, 2016 at 10:41 PM, Cannon via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I understand this mailing list is for topics relating to development. Is
> there a general users mailing list for bitcoin related things such as
> questions that are not necessarily related to dev?
>
> --
>
> Cannon
> PGP Fingerprint: 2BB5 15CD 66E7 4E28 45DC 6494 A5A2 2879 3F06 E832
> Email: cannon@cannon-ciota.info
> Bitmessage Address: BM-2cVaTbC8fJ5UDDaBBs4jPQoFNp1PfNhxqU
> Ricochet-IM: ricochet:hfddt2csxnsb2mdq
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Wed, Nov 2, 2016 at 1:30 PM, Russell O'Connor via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I'm interested in collecting and implementing other useful covenants, so if
> people have ideas, please post them.

I know of a good business case that could benefit from two nice
features.

As an example:

  Two parties have initiated a transaction designed with
  counterparty-minimization in mind.  It uses MAST and has many
  different payout distributions.  Both parties enter expecting to
  gain from the transaction, but both take on risk due to external
  factors.

  Because of the risks involved, there exist possible times when one
  party may wish to renegotiate the exit distribution, and might
  threaten to block any exit.  Or, either party might get hit by the
  proverbial bus.  During such times, the other party's eventual exit
  is protected by using a multisig which includes an oracle
  determination.  The oracle's trusted role is bound to this example's
  unstated "external factors" in a very limited sense, and does not
  include broader concerns, such as determining whether a party to the
  transaction is of "sound mind and body".

  The singular term "oracle" hides a set of entities participating in
  m-of-n multisig, which we can name the "oracle-set".

  Transaction terms include a CLTV lasting perhaps several years,
  applied whenever the exit requires the oracle-set's signatures.

  Both parties may mutually select and sign one of the payout
  distributions, to exit early.

The example, as I've described it so far, doesn't need anything other
than MAST.  It isn't a covenant, because it doesn't impose any forward
restrictions when satisfied; despite the contractual complications of
executing the oracle-set's signatures.  As covenant features are
considered across updated instances of what is otherwise a singular
transaction, it's important that none carry into the final payout
distribution, and that this is easy to verify.

Features desired:

  - One party would like to unilaterally sell their participation in
    the transaction, to a previously unknown recipient, before the
    CLTV becomes valid.

    The other originating party's stored MAST should either continue
    to function, or require minimal replacements that can be
    deterministically applied using data visible on the blockchain.
    It should not be necessary to ask permission from - or coordinate
    online communication with - the other originating party.

    (This can also be viewed as a key rotation problem for any
    long-lasting multisig transaction.)

  - Both parties would like to mutually revoke rouge oracle-entities
    from the oracle-set, without exposing each other to any possible
    renegotiation of other terms.

Note that these features affect each other, since if one party sells
their participation after any oracle-entities have been revoked, then
the revocations should not reset, but rather remain in effect, until a
proper payout executes the final agreement in the contract.

Of course, if there's a way to achieve these features with less risk
than evaluating covenant logic, I would very much like to hear how to
do so.


-------------------------------------
On Sunday, May 08, 2016 03:24:22 AM Matt Corallo wrote:
> >> ===Intended Protocol Flow===
> > 
> > I'm not a fan of the solution that a CNode should keep state and talk to
> > its remote nodes differently while announcing new blocks.
> > Its too complicated and ultimately counter-productive.
> > 
> > The problem is that an individual node needs to predict network behaviour
> > in advance. With the downside that if it guesses wrong that both nodes
> > end up paying for the wrong guess.
> > This is not a good way to design a p2p layer.
> 
> Nodes don't need to predict much in advance, and the cost for predicting
> wrong is 0 if your peers receive blocks with a few hundred ms between
> them (as we should expect) and you haven't set the announce bit on more
> than a few peers (as the spec requires for this reason).

You misunderstand the networking effects.
The fact that your node is required to choose which one to set the announce 
bit on implies that it needs to predict which node will have the best data in 
the future.
It needs to predict which nodes will not start being incommunicado and it 
requires them to predict all the things that are not possible to predict in a 
network.
In networking it is even more true than in stocks; results of the past are no 
guarantee for the future.

This means you are creating a fragile system. Your system will only work in 
laboratory situations.  It will fail spectacularly when the network or the 
internet is under stress or some parts fall away.


Another problem with your solution is that nodes send a much larger amount of 
unsolicited data to peers in the form of the thin-block compared to the normal 
inv or header-first data.

Saying this is mitigated by only subscribing on this data from a small 
subsection of nodes means you position yourself in a situation that I 
displayed above. A tradeoff of fragile and fast.  With no possible way to make 
a node automatically decide on a good equilibrium.


> It seems I forgot to add a suggested peer-preforwarding-selection
> algorithm in the text, but the intended use-case is to set the bit on
> peers which recently provided you blocks faster than other peers, up to
> only one or three peers. This is both simple and should be incredibly
> effective.

Network autorepair systems have been researched for decades, no real solution 
has as of yet appeared. 
PHDs are written on the subject and you want to make this a design for Bitcoin 
based on "[it] should be incredibly effective", I think you are underestimating 
the subject matter you are dealing with.


> > I would suggest that a new block is announced to all nodes equally and
> > then
> > individual nodes can respond with a request of either a 'compact' or a
> > normal block.
> > This is much more in line with the current design as well.
> > 
> > Detection if remote nodes support compact blocks, for the purpose of
> > requesting a compact-block, can be done either via a network-bit or just a
> > protocol version. Or something else entirely, if you have better
> > suggestions.
> 
> In line with recent trends, neither service bits nor protocol versions
> are particularly well-suited for this purpose.

Am I to understand that you choose the solution based on the fact that service 
bits are too expensive to extend? (if not, please respond to my previous 
question actually answering the suggestion)

That sounds like a rather bad way of doing design. Maybe you can add a second 
service bits field of message instead and then do the compact blocks correctly.


> >> Variable-length integers: bytes are a MSB base-128 encoding of the
> >> number.
> >> The high bit in each byte signifies whether another digit follows.
> >> [snip bitwise spec]
> > 
> > I suggest just referring to UTF-8 which describes this just fine.
> > it is good practice to refer to existing specs when possible and not copy
> > the details.
> 
> Hmm? There is no UTF anywhere in this protocol. Indeed this section
> needs to be rewritten, as indicated. I'd recommend you read the code
> until I update the section with better text if you're confused.

Wait, you didn't steal the variable length encoding from an existing standard 
and you programmed a new one?
I strongly suggest you don't reinvent this kind of protocol level encodings 
but instead steal from something like UTF8. Which has been around for decades.

Please base your standard on other standards where possible.

Look at UTF-8 on wikipedia, you may have "invented" the same encoding that IBM 
published in 1992.


> >> ====Short transaction IDs====
> >> Short transaction IDs are used to represent a transaction without
> >> sending a full 256-bit hash. They are calculated by:
> >> # single-SHA256 hashing the block header with the nonce appended (in
> >> little-endian)
> >> # XORing each 8-byte chunk of the double-SHA256 transaction hash with
> >> each corresponding 8-byte chunk of the hash from the previous step
> >> # Adding each of the XORed 8-byte chunks together (in little-endian)
> >> iteratively to find the short transaction ID
> > 
> > I don't think this is needed. Just use the first 8 bytes.
> > The reason to do xor-ing doesn't hold up and extra complexity is unneeded.
> > Especially since you mention some lines down;
> > 
> >> The short transaction ID calculation is designed to take absolutely
> >> minimal processing time during block compaction to avoid introducing
> >> serious DoS vulnerabilities
> 
> I'm confused as to what, specifically, you're proposing this be changed
> to.

Just the first (highest) 8 bytes of a sha256 hash.

The amount of collisions will not be less if you start xoring the rest.
The whole reason for doing this extra work is also irrelevant as a spam 
protection. 

-- 
Tom Zander


-------------------------------------
On Thu, Aug 04, 2016 at 04:16:20AM +1000, Matthew Roberts via bitcoin-dev wrote:
> In light of the recent hack: what does everyone think of the idea of
> creating a new address type that has a reversal key and settlement layer
> that can be used to revoke transactions?

I think many of us who think about human - computer interactions see the
need for a well defined process to roll back unexpected behavior in a computer
system. My 2014 era proposal is https://bitbucket.org/tmagik/catoshi/issues/24

The fundamental assumption around cryptocoins is you have a secret (private
key) known only by you. Currently in bitcoin if that assumption changes, the
response is blame the user. 'Incompetence, etc, etc'

This is bad business. For any cryptocurrency to really get mass market, we
need to provide our users with key revocation, to be used when the assumption
about being the only holder of a secret is broken.

I think there's a hardfork-worthy choice here:

1) implement reversal/revocation as an add-on feature
2) implement reversal/revocation as a fundamental that every address gets.

Ethereum made a quick hardfork choice to reverse a *single* instance of
unexpected behavior, and looks a lot like a bank bailout. We have the chance
to learn from this mistake, and, apparently, make a lot of money trading
on both sides of the hardfork.

> You could specify so that transactions "sent" from these addresses must
> receive N confirmations before they can't be revoked, after which the
> transaction is "settled" and the coins become redeemable from their
> destination output. A settlement phase would also mean that a transaction's
> progress was publicly visible so transparent fraud prevention and auditing
> would become possible by anyone.
> 
> The reason why I bring this up is existing OP codes and TX types don't seem
> suitable for a secure clearing mechanism; Nlocktimed TXs won't work for
> this since you can't know ahead of time when and where a withdrawal needs
> to be made, plus there's still the potential for key mismanagement; Similar
> problems with OP_CHECKLOCKTIMEVERIFY apply too ??? unless you keep a private
> key around on the server which would defeat the purpose. The main use case
> here, would be specifically to improve centralized exchange security by
> making it impossible for a hot wallet to be raided all at once.
> 
> Thoughts?
> 
> Some existing background:
> 
> http://hackingdistributed.com/2016/08/03/how-bitfinex-heist-could-have-been-avoided/
> -- Proposed the basic idea for a time-based clearing house but using
> blockchains directly, this is a much better idea than my own.
> 
> roberts.pm/timechain -- My original paper written in 2015 which proposed a
> similar idea for secure wallet design but implemented using time-locked
> ECDSA keys. Obviously a blockchain would work better for this.
> 
> Other -- if the idea has already been brought up by other people, I
> apologize.

> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
On Tue, Jan 26, 2016 at 09:41:01AM -0800, Toby Padilla via bitcoin-dev wrote:
> The wording is a little strange and I think it *should* work as you state,
> but Bitcoin Core will actually reject any output that has zero value (even
> a single OP_RETURN output -- I just tested again to make sure).
> 
> Here's the blocking code:
> 
> https://github.com/bitcoin/bitcoin/blob/master/src/qt/paymentserver.cpp#L584
> 
> I agree that this should be made more clear in my BIP though, I'll clean up
> the language.

Note that because the dust limit is ignored completely for OP_RETURN
outputs, you can work around this by setting the OP_RETURN outputs to 1
satoshi instead.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698

-------------------------------------
https://github.com/bitcoin/bips/pull/440
https://github.com/bitcoin/bitcoin/pull/8636
This document specifies proposed changes to the Bitcoin transaction validity rules to fix the malleability of extra stack element for OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY.

The original plan was to do the LOW_S and NULLDUMMY (BIP146) together with segwit in 0.13.1. However, as we discovered some undocumented behavior in LOW_S, we may want to deploy the LOW_S softfork in a later release. https://github.com/bitcoin/bitcoin/pull/8533#issuecomment-243973512

I will edit the BIP146 later.

  BIP: ?
  Title: Dealing with dummy stack element malleability
  Author: Johnson Lau <jl2012@xbt.hk>
  Status: Draft
  Type: Standards Track
  Created: 2016-09-02

Abstract

This document specifies proposed changes to the Bitcoin transaction validity rules to fix the malleability of extra stack element for OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY.

Motivation

Signature malleability refers to the ability of any relay node on the network to transform the signature in transactions, with no access to the relevant private keys required. For non-segregated witness transactions, signature malleability will change the txid and invalidate any unconfirmed child transactions. Although the txid of segregated witness (BIP141) transactions is not third party malleable, this malleability vector will change the wtxid and may reduce the efficiency of compact block relay (BIP152).

A design flaw in OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY makes them consuming an extra stack element ("dummy element") after signature validation. The dummy element is not inspected in any manner, and could be replaced by any value without invalidating the script. This document specifies a new rule to fix this signature malleability.

Specification

To fix the dummy element malleability, a new consensus rule ("NULLDUMMY") is deployed to require that the dummy element MUST be the empty byte array. Anything else makes the script evaluate to false immediately. The NULLDUMMY rule applies to OP_CHECKMULTISIG and OP_CHECKMULTISIGVERIFY in pre-segregated scripts, and also pay-to-witness-script-hash scripts described in BIP141.

Deployment

This BIP will be deployed by "version bits" BIP9 using the same parameters for BIP141 and BIP143, with the name "segwit" and using bit 1.

For Bitcoin mainnet, the BIP9 starttime is midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout is midnight TBD UTC (Epoch timestamp TBD).

For Bitcoin testnet, the BIP9 starttime is midnight 1 May 2016 UTC (Epoch timestamp 1462060800) and BIP9 timeout is midnight 1 May 2017 UTC (Epoch timestamp 1493596800).

Compatibility

The reference client has produced compatible signatures from the beginning, and the NULLDUMMY rule has been enforced as relay policy by the reference client since v0.10.0. There has been no transactions violating the requirement being added to the chain since at least August 2015. In addition, every non-compliant signature can trivially be converted into a compliant one, so there is no loss of functionality by this requirement.

Implementation

An implementation for the reference client is available at https://github.com/bitcoin/bitcoin/pull/8636

Acknowledgements

This document is extracted from the previous BIP62 proposal, which was composed by Pieter Wuille and had input from various people.

Copyright

This document is placed in the public domain.


-------------------------------------
On Tue, May 10, 2016 at 08:14:33PM -0700, Timo Hanke wrote:
> There is no way to tell from a block if it was mined with AsicBoost or not.
> So you don’t know what percentage of the hashrate uses AsicBoost at any
> point in time. How can you risk forking that percentage out? Note that this
> would be a GUARANTEED chain fork. Meaning that after you change the block
> mining algorithm some percentage of hardware will no longer be able to
> produce valid blocks. That hardware cannot “switch over” to the majority
> chain even if it wanted to. Hence you are guaranteed to have two
> co-existing bitcoin blockchains afterwards.

First of all, we can easily do this in a way where miners show their support
for this change, say with the usual 95% approval threshold we've been using for
soft-forks. That gets the % of hashing power on a AsicBoost chain fork down to
5% at most.

Secondly, we can probably make the consensus PoW allow blocks to be mined using
both the existing PoW algorithm, and a very slightly tweaked version where
implementing AsicBoost gives no advantage. That removes any incentive to
implement AsicBoost, without making any hardware obsolete (such as 21inc's
hardware). This means that no hashing power at all needs to use the AsicBoost
patent.

Obviously, the fact that miners can support such a change (assuming of course
the economic majority approves it as well) changes the negotiation position re:
licensing fees; the actual outcome may simply be you guys make the patent 100%
public for all to use at a much reduced price, given you're lack of negotiation
strength.

> Note that “AsicBoost” above is replaceable with “optimization X”. It’s
> simply a logical argument: If you want to make optimization X impossible
> and someone is already using optimization X you end up with two chains. So
> unless you know exactly which optimizations are in use (and therefore also
> know which ones are not in use) you can’t make these kind of changes.
> AsicBoost is known at least since middle of 2013.

I think _patented_ optimizations where one party has a monopoly are very
different than optimizations that anyone can independently rediscover -
AsicBoost itself looks to be something that two or three parties independently
discovered.

> The only way out is to go the exact opposite way and to embrace as many
> optimizations as possible to the point where there are no more
> optimizations left to do, or hopefully getting very close to that point.

...which is a scenario that may result in a dozen patented optimizations, with
new ASIC manufacturers needing a dozen licenses, from potentially hostile
entities.

For instance, it's not clear to me if you actually own this patent, or
Cointerra's creditors. Obviously in the latter case, it'd be quite possible
that some kind of bankrupcy court ruling results in the patent getting sold to
a hostile entity who will use it against all of Bitcoin. Equally, even if it is
100% owned by you and Sergio, it'd be very easy for a personal bankrupcy to
result in the same scenario (suppose you get into a car accident and lose a
negligence lawsuit over it).

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Monday, January 25, 2016 04:05:59 PM Wladimir J. van der Laan wrote:
> I don't have time to work on the release notes right now, but if someone
> else wants to contribute that'd be awesome.

I cooked my first pull request to resolve this:
https://github.com/bitcoin/bitcoin/pull/7416

Thanks for all the explanations you folks provided! :)

-------------------------------------
> Also how about making timestamp 8 bytes?  2106 is coming up soon :)

AFAICT this was fixed in this commit:
https://github.com/jl2012/bitcoin/commit/fa80b48bb4237b110ceffe11edc14c8130672cd2#diff-499d7ee7998a27095063ed7b4dd7c119R200


2016-12-04 21:00 GMT+01:00 adiabat via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org>:

> Interesting stuff! I have some comments, mostly about the header.
>
> The header of forcenet is mostly described in Luke’s BIP, but I have made
>> some amendments as I implemented it. The format is (size in parentheses;
>> little endian):
>>
>> Height (4), BIP9 signalling field (4), hardfork signalling field (3),
>> merge-mining hard fork signalling field (1), prev hash (32), timestamp (4),
>> nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), Hash TMR (32),
>> Hash WMR (32), total tx size (8) , total tx weight (8), total sigops (8),
>> number of tx (4), merkle branches leading to header C (compactSize + 32 bit
>> hashes)
>>
>
> First, I'd really rather not have variable length fields in the header.
> It's so much nicer to just have a fixed size.
>
> Is having both TMR and WMR really needed?  As segwit would be required
> with this header type, and the WMR covers a superset of the data that the
> TMR does, couldn't you get rid of the TMR?  The only disadvantage I can see
> is that light clients may want a merkle proof of a transaction without
> having to download the witnesses for that transaction.  This seems pretty
> minor, especially as once they're convinced of block inclusion they can
> discard the witness data, and also the tradeoff is that light clients will
> have to download and store and extra 32 bytes per block, likely offsetting
> any savings from omitting witness data.
>
> The other question is that there's a bit that's redundant: height is also
> committed to in the coinbase tx via bip 34 (speaking of which, if there's a
> hard-fork, how about reverting bip 34 and committing to the height with
> coinbase tx nlocktime instead?)
>
> Total size / weight / number of txs also feels pretty redundant.  Not a
> lot of space but it's hard to come up with a use for them.  Number of tx
> could be useful if you want to send all the leaves of a merkle tree, but
> you could also do that by committing to the depth of the merkle tree in the
> header, which is 1 byte.
>
> Also how about making timestamp 8 bytes?  2106 is coming up soon :)
>
> Maybe this is too nit-picky; maybe it's better to put lots of stuff in for
> testing the forcenet and then take out all the stuff that wasn't used or
> had issues as it progresses.
>
> Thanks and looking forward to trying out forcenet!
>
> -Tadge
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
The first issue is that doing two OP_SWAP's in a row will just return
you to the original state. The second issue is that all of them end up
hashing the same key, so anyone on the network can spend this output.
(See https://en.bitcoin.it/wiki/Script for a good resource on opcodes
and what each of them do. There are also a few simulators floating
around, but I can't recommend any in particular.)

Third, if you're concerned about exposing public keys, why not use a
P2SH script? That won't expose your public keys until you spend from
it.

On Thu, Dec 22, 2016 at 11:29 AM, Andrew via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Hi
>
> Is there a worked out scriptPubKey for doing multisig with just hashes
> of the participants? I think it is doable and it is more secure to a
> compromised ECDSA. I'm thinking something like this for the
> scriptPubKey:
>  2 OP_SWAP OP_SWAP OP_SWAP OP_DUP OP_HASH160 <pubKeyHash1>
> OP_EQUALVERIFY OP_DUP OP_HASH160 <pubKeyHash2> OP_EQUALVERIFY OP_DUP
> OP_HASH160 <pubKeyHash3> OP_EQUALVERIFY 3 OP_CHECKMULTISIG
>
> and <sigs><pubkeys> for the scriptSig
>
> Can anyone confirm or send me a link to the worked out script?
>
> Thanks
>
> --
> PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
+1
The distinction we are making importantly requires that contributors
provide readers with another thing to say in favor of something - another
thing which is different than "X people support this instead of only X-1
people."  Evidence trumps votes.

On Sat, Jan 23, 2016 at 1:38 PM, Gavin via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> > On Jan 23, 2016, at 3:59 PM, Peter Todd via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> >
> > I would extend this to say that the technical explanation also should
> > contribute uniquely to the conversation; a +1 with an explanation
> > the last +1 gave isn't useful.
>
> Yes, comments should contribute to the discussion, with either technical
> discussion or additional relevant data. I think a +1 like the following
> should be encouraged:
>
> "+1: we had eleven customer support tickets in just the last week that
> would have been prevented if XYZ.
>
> Jane Doe, CTO CoinBitChainBasely.com"
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
I'm re-submitting this after a little under a year because, though it
generated little discussion, I feel it is still necessary.

The following can also be found at:
https://github.com/Kefkius/bip-kefkius-multicoin-multisig/blob/master/README.md

<pre>
  BIP: bip-kefkius-multicoin-multisig
  Title: Multi-Currency Hierarchy For Use In Multisignature
Deterministic Wallets
  Author: Tyler Willis <kefkius@maza.club>
  Status: Draft
  Type: Standards Track
</pre>

## Abstract ##

This BIP defines a hierarchy based on BIP-0044 for deterministic wallets
intended to facilitate multi-currency, multisignature wallet management.

This BIP is an application of BIP-0043.

## Motivation ##

The hierarchy defined in BIP-0044 places a coin's type level above the
level designating account number. This limits the possible
implementations of multi-currency, multisignature wallets. The hierarchy
defined here facilitates the creation of wallets intended to be both
multi-currency and multisignature.

## Specification ##

### Path Levels ###

<pre>
m / purpose' / wallet' / coin_type / change / address_index
</pre>

#### Purpose ####

Purpose is a constant tentatively set to
`bip-kefkius-multicoin-multisig` as there is no BIP number assigned to
this proposal. Hardened derivation is used here.

#### Wallet ####

This level is incremented for every wallet that one makes. Much like the
account node in BIP-0044, this is intended to organize independent
identities within the key space.
Basically, each index in this level represents a group of cosigners.
Hardened derivation is used here.

#### Coin Type ####

This is the BIP-0044 index of the coin being managed. Public derivation
is used here.
Public derivation is used so that cosigners need only know one of each
other's public keys, rather than needing to distribute public keys for
each coin.

#### Change ####

0 is used for public addresses and 1 is used for change addresses.
Identical to BIP-0044 behavior. Public derivation is used here.

#### Address Index ####

The address index. This increases sequentially as it does in BIP-0044.
Public derivation is used here.


## Examples ##

| wallet | coin      | change? | address | path                         |
|:-------|:----------|:--------|:--------|:-----------------------------|
| first  | Bitcoin   | no      | first   | m /
bip-kefkius-multicoin-multisig' / 0' / 0  / 0 / 0   |
| first  | Bitcoin   | no      | second  | m /
bip-kefkius-multicoin-multisig' / 0' / 0  / 0 / 1   |
| first  | Testnet   | no      | first   | m /
bip-kefkius-multicoin-multisig' / 0' / 1  / 0 / 0   |
| second | Bitcoin   | no      | first   | m /
bip-kefkius-multicoin-multisig' / 1' / 0  / 0 / 0   |
| second | Bitcoin   | yes     | first   | m /
bip-kefkius-multicoin-multisig' / 1' / 0  / 1 / 0   |
| third  | Testnet   | yes     | sixth   | m /
bip-kefkius-multicoin-multisig' / 2' / 1  / 1 / 5   |


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.12.1rc1 are available from:

    https://bitcoin.org/bin/bitcoin-core-0.12.1/test.rc2/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.12.1rc1

This is a release candidate for a new minor version release, including the
BIP9, BIP68 and BIP112 softfork, various bugfixes and updated translations.

Preliminary release notes for the release can be found here:

    https://github.com/bitcoin/bitcoin/blob/0.12/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.12.1.

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJXC1QSAAoJEHSBCwEjRsmmtk4H/1C74eLLVwzOhrrWNx34ANcz
uiyIlkXnMmX+iHWaSC8XdcBOEc4/+YOgDK0KU+FRG6bNDengPKqf8mPDEvyXU/H+
/ed2W9Q0DQ/jxyeKOOVlMWhKCZWQRisxhadB0LAiny2QLsBojTrJtziGIOYXp4Qt
xI3GstbUr42da8kL8NoKxQt6na5FrGiuRAQeucwcoHi1QQodd7R7vA2b84N1ECrr
KWbCfw6a9qHDmk2Vy+9CqGtESHuVW04B+79ui+Dgsh6frG9UH5G7WP4ziUcwm625
CEEbi/cIrpzEOSo4S5ukFhYK6I3o67uxvV8Nc3ocI1UmC4d2BUvAMalBGMgjENg=
=oPfq
-----END PGP SIGNATURE-----


-------------------------------------
On Fri, May 20, 2016 at 11:46:32AM +0200, Johnson Lau wrote:
> How is this compared to my earlier proposal: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html> ?
> 
> In my proposal, only the (pruned) UTXO set, and 32 bytes per archived block, are required for mining. But it is probably more difficult for people to spend an archived output. They need to know the status of other archived outputs from the same block. A full re-scan of the blockchain may be needed to generate the proof but this could be done by a third party archival node.

We're working along the same lines, but my proposal is much better fleshed out;
I think you'll find you missed a few details if you flesh out yours in more
detail. For instance, since your dormant UTXO list is indexed by UTXO
expiration order, it's not possible to do any kind of verification that the
contents of that commitment are correct without the global state of all UTXO
data - you have no ability to locally verify as nothing commits to the contents
of the UTXO set.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Fine by me to update BIP68 and BIP112 to Final status. The forks have
activated.

On Fri, Jul 15, 2016 at 4:30 PM, Luke Dashjr <luke@dashjr.org> wrote:

> Daniel Cousens opened the issue a few weeks ago, that BIP 9 should
> progress to
> Accepted stage. However, as an informational BIP, it is not entirely clear
> on
> whether it falls in the Draft/Accepted/Final classification of proposals
> requiring implementation, or the Draft/Active classification like process
> BIPs. Background of this discussion is at:
>     https://github.com/bitcoin/bips/pull/413
> (Discussion on the GitHub BIPs repo is *NOT* recommended, hence bringing
> this
> topic to the mailing list)
>
> Reviewing the criteria for status changes, my opinion is that:
> - BIPs 68, 112, 113, and 141 are themselves implementations of BIP 9
> -- therefore, BIP 9 falls under the Draft/Accepted/Final class
> - BIPs 68, 112, and 113 have been deployed to the network successfully
> -- therefore, BIP 9 has satisfied the conditions of not only Accepted
> status,
>    but also Final status
> -- therefore, BIPs 68, 112, and 113 also ought to be Final status
>
> If there are no objections, I plan to update the status to Final for BIPs
> 9,
> 68, 112, and 113 in one month. Since all four BIPs are currently Draft, I
> also
> need at least one author from each BIP to sign-off on promoting them to
> (and
> beyond) Accepted.
>
> BIP   9: Pieter Wuille <pieter.wuille@gmail.com>
>          Peter Todd <pete@petertodd.org>
>          Greg Maxwell <greg@xiph.org>
>          Rusty Russell <rusty@rustcorp.com.au>
>
> BIP  68: Mark Friedenbach <mark@friedenbach.org>
>          BtcDrak <btcdrak@gmail.com>
>          Nicolas Dorier <nicolas.dorier@gmail.com>
>          kinoshitajona <kinoshitajona@gmail.com>
>
> BIP 112: BtcDrak <btcdrak@gmail.com>
>          Mark Friedenbach <mark@friedenbach.org>
>          Eric Lombrozo <elombrozo@gmail.com>
>
> BIP 113: Thomas Kerin <me@thomaskerin.io>
>          Mark Friedenbach <mark@friedenbach.org>
>

-------------------------------------
Thomas
In all honesty, I shouldn't have added the multiple feature. It should
always stay to one and have the Bitcoin address's built in checksum do all
of the heavy lifting (or implement another checksum into the protocol).
Having said that, it would make the most sense to not include BIP21
requests. The frequencies that are going to be used are prone to being
changed (probably moving them into the ultrasound range, at least for the
16 tones)

Luke Dashjr
I thought it was only 12, will change it accordingly.

Chris
I looked at Audio Modem and will probably look at the others soon as well.

The general idea of this BIP is to see if you all even approve of the idea
of sending Bitcoin addresses over sound waves. I will push it into the
ultrasound, and I might implement phase modulation.

On Tue, Aug 9, 2016 at 8:09 PM, Thomas Daede via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> If this is just encoding BIP-21 addresses, it is basically an "audio QR
> code". In this case, does publishing it as a BIP still make sense? (Not
> to imply that it doesn't, but it's something you should consider.)
>
> Please look at existing implementations of audio modems when creating
> your design. A lot of this work has been done many times before, so
> there is a lot to learn from.
>
> Your selected frequencies are harmonics of each other, meaning nonlinear
> distortion will make detection more difficult. The Bell 202 and similar
> modem standards chose AFSK frequencies to minimize interference.
>
> Repeating a message multiple times is a very inefficient method of error
> recovery. It works, but there may be better techniques, such as trellis
> modulation or other convolutional codes.
>
> Defining channel models to simulate your various use cases will help a
> lot to determine if you have met your requirements.
>
> - Thomas
>
> P.S. I also briefly considered audio to exchange transactions with a
> hardware wallet. Using GNU Radio made the implementation much easier.
>
> On 08/09/2016 04:06 PM, Daniel Hoffman via bitcoin-dev wrote:
> > I have updated the GitHub a lot (changed tones to be less chirpy, fixed
> > some smalls) and made a couple of samples (see attachment for MP3 and
> > FLAC of both tone tables, first 16 then 4). Is this good enough to
> > warrant an official BIP number? I haven't built a decoder yet, but it
> > seems like the encoder is working properly (looked at Audacity, seems
> > like it is working), and some people on reddit want to "allow for
> > decoding experiments"
> > <https://www.reddit.com/r/btc/comments/4wsn7v/bip_proposal_
> addresses_over_audio_thoughts/d69m3st>
> >
> > What suggestions do you all have for it?
> >
> > On Mon, Aug 8, 2016 at 8:50 PM, Daniel Hoffman
> > <danielhoffman699@gmail.com <mailto:danielhoffman699@gmail.com>> wrote:
> >
> >     It wouldn't be feasible in the vast majority of cases, but I can't
> >     think of a reason why it can't be built into the standard.
> >
> >     On Mon, Aug 8, 2016 at 5:59 PM, Trevin Hofmann via bitcoin-dev
> >     <bitcoin-dev@lists.linuxfoundation.org
> >     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> >
> >         Would it be feasible to transmit an entire BIP21 URI as audio?
> >         If you were to encode any extra information (such as amount), it
> >         would be useful to include a checksum for the entire message.
> >         This checksum could possibly be used instead of the checksum in
> >         the address.
> >
> >         Trevin
> >
> >
> >         On Aug 8, 2016 3:06 PM, "Justin Newton via bitcoin-dev"
> >         <bitcoin-dev@lists.linuxfoundation.org
> >         <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> >
> >             Daniel,
> >                Thanks for proposing this.  I think this could have some
> >             useful use cases as you state.  I was wondering what you
> >             would think to adding some additional tones to optionally
> >             denote an amount (in satoshis?).
> >
> >             (FYI, actual link is here:  https://github.com/Dako300/BIP
> >             <https://github.com/Dako300/BIP> )
> >
> >             Justin
> >
> >             On Mon, Aug 8, 2016 at 2:22 PM, Daniel Hoffman via
> >             bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org
> >             <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> >
> >                 This is my BIP idea: a fast, robust, and standardized
> >                 for representing Bitcoin addresses over audio. It takes
> >                 the binary representation of the Bitcoin address (little
> >                 endian), chops that up into 4 or 2 bit chunks (depending
> >                 on type, 2 bit only for low quality audio like american
> >                 telephone lines), and generates a tone based upon that
> >                 value. This started because I wanted an easy way to
> >                 donate to podcasts that I listen to, and having a
> >                 Shazam-esque app (or a media player with this
> >                 capability) that gives me an address automatically would
> >                 be wonderful for both the consumer and producer. Comes
> >                 with error correction built into the protocol
> >
> >                 You can see the full specification of the BIP on my
> >                 GitHub page (https://github.com/Dako300/BIP-0153
> >                 <https://github.com/Dako300/BIP-0153>).
> >
> >                 _______________________________________________
> >                 bitcoin-dev mailing list
> >                 bitcoin-dev@lists.linuxfoundation.org
> >                 <mailto:bitcoin-dev@lists.linuxfoundation.org>
> >                 https://lists.linuxfoundation.
> org/mailman/listinfo/bitcoin-dev
> >                 <https://lists.linuxfoundation.org/mailman/
> listinfo/bitcoin-dev>
> >
> >
> >
> >
> >             --
> >
> >             Justin W. Newton
> >             Founder/CEO
> >             Netki, Inc.
> >
> >             justin@netki.com <mailto:justin@netki.com>
> >             +1.818.261.4248 <tel:+1.818.261.4248>
> >
> >
> >
> >             _______________________________________________
> >             bitcoin-dev mailing list
> >             bitcoin-dev@lists.linuxfoundation.org
> >             <mailto:bitcoin-dev@lists.linuxfoundation.org>
> >             https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-
> dev
> >             <https://lists.linuxfoundation.org/mailman/
> listinfo/bitcoin-dev>
> >
> >
> >         _______________________________________________
> >         bitcoin-dev mailing list
> >         bitcoin-dev@lists.linuxfoundation.org
> >         <mailto:bitcoin-dev@lists.linuxfoundation.org>
> >         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >         <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
> >
> >
> >
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Really cool!

How about "poison transactions," the other covenants use case proposed by
Möser, Eyal, and Sirer? (I think OP_CHECKSIGFROMSTACKVERIFY will also make
it easier to check fraud proofs, the other prerequisite for poison
transactions.)

Seems a little wasteful to do those two "unnecessary" signature checks, and
to have to construct the entire transaction data structure, just to verify
a single output in the transaction. Any plans to add more flexible
introspection opcodes to Elements, such as OP_CHECKOUTPUTVERIFY?

Really minor nit: "Notice that we have appended 0x83 to the end of the
transaction data"—should this say "to the end of the signature"?

On Thu, Nov 3, 2016 at 12:28 AM Russell O'Connor via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Right.  There are minor trade-offs to be made with regards to that design
point of OP_CHECKSIGFROMSTACKVERIFY.  Fortunately this covenant
construction isn't sensitive to that choice and can be made to work with
either implementation of OP_CHECKSIGFROMSTACKVERIFY.

On Wed, Nov 2, 2016 at 11:35 PM, Johnson Lau <jl2012@xbt.hk> wrote:

Interesting. I have implemented OP_CHECKSIGFROMSTACKVERIFY in a different
way from the Elements. Instead of hashing the data on stack, I directly put
the 32 byte hash to the stack. This should be more flexible as not every
system are using double-SHA256

https://github.com/jl2012/bitcoin/commits/mast_v3_master


On 3 Nov 2016, at 01:30, Russell O'Connor via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Hi all,

It is possible to implement covenants using two script extensions: OP_CAT
and OP_CHECKSIGFROMSTACKVERIFY.  Both of these op codes are already
available in the Elements Alpha sidechain, so it is possible to construct
covenants in Elements Alpha today.  I have detailed how the construction
works in a blog post at <
https://blockstream.com/2016/11/02/covenants-in-elements-alpha.html>.  As
an example, I've constructed scripts for the Moeser-Eyal-Sirer vault.

I'm interested in collecting and implementing other useful covenants, so if
people have ideas, please post them.

If there are any questions, I'd be happy to answer.

-- 
Russell O'Connor
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
Luke Dashjr via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:
> On Wednesday, December 30, 2015 6:22:59 PM Tomas wrote:
>> > The specification itself looks like an inefficient and bloaty reinvention
>> > of version bits.
>> 
>> The actual assignment of version bits isn't clear from the
>> specification. Are you saying that any implementation that wants to
>> propose a change is encouraged to pick a free version bit and use it?
>
> That should probably be clarified in the BIP, I agree. Perhaps it ought to be 
> assigned the same as BIP numbers themselves, by the BIP editor? (Although as a 
> limited resource, maybe that's not the best solution.)

I thought about it, but it's subject to change.  Frankly, the number of
deployed forks is low enough that they can sort it out themselves.  If
we need something more robust, I'm happy to fill that role.

Cheers,
Rusty.



-------------------------------------
On Tuesday, August 16, 2016 5:53:08 PM Johnson Lau via bitcoin-dev wrote:
> A new BIP is prepared to deal with OP_IF and OP_NOTIF malleability in
> P2WSH:
> https://github.com/jl2012/bips/blob/minimalif/bip-minimalif.mediawiki
> https://github.com/bitcoin/bitcoin/pull/8526

I am not sure this makes sense. SegWit transactions are already non-malleable 
due to skipping the witness data in calculating the transaction id. What is 
the benefit to this?

Luke


-------------------------------------
FWIW, BIP44 also doesn't encode a seed birthday. This needed so that SPV
wallets do not need to scan from the beginning of the blockchain.

That doesn't mean BIP44 could not be final. There are some wallets that
interoperate on that standard and that's fine. The whole reason I
insisted on separating BIP43 from BIP44 is that someone else can come up
with a better "BIP44+" standard and not get into the way of existing
standards. I think BIP43 should be made final as well, if it isn't already.


On 08/24/2016 02:51 PM, Thomas Voegtlin via bitcoin-dev wrote:
> Le 23/08/2016  22:12, Luke Dashjr via bitcoin-dev a crit :
>> BIP 39: Mnemonic code for generating deterministic keys
>> - Used by many wallets and hundreds of thousands of users.
>>
>> BIP 44: Multi-Account Hierarchy for Deterministic Wallets
>> - Appears to be implemented by multiple wallets.
>>
> 
> I personally believe that BIP39/BIP44 is a bad design. There is limited
> support for these BIPs in Electrum, in order to provide compatibility
> with hardware wallets. However, I do not plan to use BIP39/BIP44 for
> default Electrum wallets, for the following reasons.
> 
> (Note that it does not make sense to consider BIP39 and BIP44
> independently. Any wallet that decides to implement one without the
> other would be considered as broken.)
> 
> Here is why I rejected this design:
> 
> 1 - BIP44 uses multiple accounts. This means that in order to be
> compatible with the standard, a wallet *must* implement multiple
> accounts. A wallet that decides to keep things simple and use only one
> account, will not allow users to recover all their funds when they
> restore from a BIP39 seed, and will be considered as broken.
> 
> 2 - An appealing feature of deterministic wallets is that you can use
> the same instance of your wallet on different devices. Two instances of
> your wallet can automatically synchronize their Bitcoin addresses, and
> display the same balance. The problem is that hardened derivations break
> this property. Indeed, with hardened derivations, software wallets need
> to ask the user's password in order to derive new accounts. Therefore,
> in order to implement automated detection of newly created accounts, a
> BIP44-compatible software wallets would need to ask the user's password
> whenever a new account is detected. This means that the wallet would ask
> the password without the user initiating any action. This seems to be an
> avenue for malware.
> 
> Of course, hardware wallets do not have that issue, because they can
> derive new accounts without requesting a password from the user. BIP44
> is a standard that has been designed for hardware wallets, but that
> makes things really difficult for software wallets.
> 
> 3 - Unneeded complexity. From an end user perspective, the multiple
> accounts in BIP44 achieve the same result as using different derivation
> passphrases with the same BIP39 seed phrase. The only real difference is
> that BIP44 accounts can be enumerated deterministically, while
> passphrases in general cannot. However, this property is of limited
> interest, because automatic synchronization of multiple accounts cannot
> be guaranteed for bip44 software wallets, as explained in 2.
> 
> 4 - BIP39 is inconsistent. It uses a hash of the utf8 encoded 'seed
> phrase' in order to derive the BIP32 seed. This hash-based derivation
> was added on my suggestion, in order to make the BIP independent from
> the particular wordlist used to generate the seed phrases. However,
> BIP39 also requires the implementation of a checksum, in order to verify
> that a seed phrase is valid. Suprisingly, the specification of the
> checksum involves wordlist indices. This means the checksum (and thus
> the BIP) requires a fixed wordlist. This defeats the purpose of using a
> hash for the derivation of the seed.
> 
> The authors of the BIP should either have used hash functions for both
> the seed AND the checksum (that is what Electrum does), or for none of
> them (in that case case, you can have a bidirectional function between
> seed phrases and entropy, which is nice if you want to perform Shamir
> secret sharing of seed phrases, at the expenses of a fixed wordlist). In
> its current state, BIP39 takes the worst of both worlds.
> 
> 5 - The fact that the wordlist must be part of BIP39, and cannot be
> changed in the future, seems a terrible idea to me. I believe that a
> specification should always try to be minimal. In that case, the
> specification includes a 2000+ words dictionary, when it could have
> avoided that.
> 
> Even if you decide that BIP39 is final, there will always be users
> requiring the addition of wordlists for new languages. So, in practice,
> this BIP will never be final.
> 
> 6 - Finally, and most importantly, BIP39 seed phrases do not have a
> version number. Without a version number, how are you going to derive
> addresses from a BIP39 seed phrase, when wallets start to use to new
> derivation methods (such as SegWit, or Schnorr signatures)? Does it mean
> that a BIP39 compatible wallet will have to check addresses from all the
> derivation methods that ever existed in the past, in order to ensure
> that all coins are correctly retrieved? Or will there be users that
> cannot access their coins because their BIP39 seed phrase is too old for
> newer software?
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 




-------------------------------------
On Thu, Jun 09, 2016 at 01:24:09AM +0000, Gregory Maxwell wrote:
> Reduction to plaintext isn't an interesting attack vector for an active
> attacker: they can simply impersonate the remote side.
>
> This is addressed via authentication, where available, which is done by a
> separate specification that builds on this one.

Are there any links to discussions on how authentication may be done?

Thanks,

Alfie

-- 
Alfie John
https://www.alfie.wtf


-------------------------------------
On 3/9/2016 3:18 PM, Henning Kopp via bitcoin-dev wrote:
> Hi,
>
> > However, I think it could actually increase
> > confidence in the system if the community is able to demonstrate a good
> > process for making such decisions, and show that we can separate the
> > meaningful underlying principles, such as the coin limit and overall
> > inflation rate, from what is more akin to an implementation detail, as I
> > consider the large-step reward reduction to be.
>
> I do not think that a line can be drawn here. As far as I understood,
> you think that the coin limit is a meaningful underlying principle
> which should not be touched, whereas the halving of mining rewards is
> an implementation detail. The two are very closely tied together and
> changes to both of them would result in a hardfork, if I am not
> mistaken.

I believe that you are mistaken.

The two are almost-completely unrelated, and (as Dr. Back has been
pointing out for a very long time now) the halving of mining rewards can
be modified with a soft fork.

http://www.truthcoin.info/blog/mining-heart-attack/#smooth-the-disinflation-out




-------------------------------------
On Thu, Sep 22, 2016 at 10:47:03AM +0200, Tom via bitcoin-dev wrote:
> On Wednesday 21 Sep 2016 18:45:55 adiabat via bitcoin-dev wrote:
> > Hi-
> > 
> > One concern is that this doesn't seem compatible with Lightning as
> > currently written.  Most relevant is that non-cooperative channel close
> > transactions in Lightning use OP_CHECKSEQUENCEVERIFY, which references the
> > sequence field of the txin; if the txin doesn't have a sequence number,
> > OP_CHECKSEQUENCEVERIFY can't work.
> > 
> > LockByBlock and LockByTime aren't described and there doesn't seem to be
> > code for them in the PR (186).  If there's a way to make OP_CLTV and OP_CSV
> > work with this new format, please let us know, thanks!
> 
> LockByBlock and LockByTime are still TODOs because I didn't have time to go
> in-dept into how BIP68 does the encoding.
> The intent is that these tags, while loading, will set the sequence integer in 
> the txin as the old version does. And while saving we do the reverse.
> 
> In other words; the lack of sequence number in the saved format doesn't affect 
> the in-memory format of the transaction. The in-memory version is the one that 
> script will operate on.
> 
> This means that there is no change in how CSV will work before and after on 
> any level other than serialisation.

CSV uses per-input sequence numbers; you only have a per-tx equivalent.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
This troll is harmless.  A duplicate spend proof should also be signed
by the same user (Alice, in your example) to be considered a double
spend.

2016-08-09 3:18 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
> One more thought about why verification by miners may be needed.
>
> Let's say Alice sends Bob a transaction, generating output C.
>
> A troll, named Timothy, broadcasts a transaction with a random hash,
> referencing C's output as its spend proof. The miners can't tell if it's
> valid or not, and so they include the transaction in a block. Now Bob's
> money is useless, because everyone can see the spend proof referenced and
> thinks it has already been spent, even though the transaction that claims it
> isn't valid.
>
> Did I miss something that protects against this?
>


-------------------------------------
From: Gavin Andresen [mailto:gavinandresen@gmail.com] 
Sent: Friday, 5 February, 2016 06:16
To: Gregory Maxwell <greg@xiph.org>
Cc: jl2012 <jl2012@xbt.hk>; Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] Hardfork bit BIP

>It is always possible I'm being dense, but I still don't understand how this proposal makes a chain-forking situation better for anybody.

>If there are SPV clients that don't pay attention to versions in block headers, then setting the block version negative doesn't directly help them, they will ignore it in any case.

It is unfortunate SPV clients are not following that. However, they SHOULD follow that. It becomes a self fulfilling prophecy if we decide not to do that if SPV are not following that.

>If the worry is full nodes that are not upgraded, then a block with a negative version number will, indeed, fork them off the the chain, in exactly the same way a block with new hard-forking consensus rules would. And with the same consequences (if there is any hashpower not paying attention, then a worthless minority chain might continue on with the old rules).

It will distinguish between a planned hardfork and an accidental hardfork, and full nodes may react differently. Particularly, a planned unknown hardfork is a strong indication that the original chain has become economic minority and the non-upgraded full node should stop accepting incoming tx immediately.

>If the worry is not-upgraded SPV clients connecting to the old, not-upgraded full nodes, I don't see how this proposed BIP helps.

Same for not-upgraded full nodes following not-upgraded full nodes. Anyway, the header with enough PoW should still be propagated.

>I think a much better idea than this proposed BIP would be a BIP that recommends that SPV clients to pay attention to block version numbers in the headers that they download, and warn if there is a soft OR hard fork that they don't know about.

Normal version number only suggests softforks, which is usually not a concern for SPV clients. An unknown hardfork is a completely different story as the values of the forks are completely unknown.

>It is also a very good idea for SPV clients to pay attention to timestamps in the block headers that the receive, and to warn if blocks were generated either much slower or faster than statistically likely. Doing that (as Bitcoin Core already does) will mitigate Sybil attacks in general.

Yes, they should.

-- 
--
Gavin Andresen




-------------------------------------
I think a BIP is a good idea, but rather than making such a specific
proposal as "Let's use bit 4 to indicate communication of thin blocks," how
about a more general one like "Let's use bit(s?) 4(-5?) as user-agent
specific service bits so that if you customize your user-agent string, you
can use them for whatever you want"? That way, other clients can choose to
follow suit by saying so, or simply recognize the meaning (or lack thereof)
of those bits based on the user-agent setting.  This relieves future
development from the burden of agreeing on where to put what, and allows
time and utility to show when such a user-agent-specific service bit should
be moved into the protocol section of service bits.

PS I am not well versed in the creation of standards, but the reservation
of digital real estate for self-identified customization (bits, bytes, or
whatever that will never be used by the standard) such as what I'm
proposing seems like something that probably has a standard name.  "Public
provisioning" or something like that?

On Mon, Mar 7, 2016 at 12:51 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Mon, Mar 7, 2016 at 8:06 PM, G. Andrew Stone via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > The Bitcoin Unlimited client needs a services bit to indicate that the
> node
> > is capable of communicating thin blocks.  We propose to use bit 4 as
> AFAIK
> > bit 3 is already earmarked for Segregated Witness.
>
> Does this functionality change peer selection?  If not, the preferred
> signaling mechanism is probably the one in BIP 130.
>
> Otherwise, I think the standard method for getting numbers has been to
> write a BIP documenting the usage. I don't know if that is intentional
> or just how things have previously happened; and I don't have much of
> an opinion on it.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
On 09/21/2016 02:58 PM, Murch via bitcoin-dev wrote:

> Android Wallet for Bitcoin

The correct name is Bitcoin Wallet, or Bitcoin Wallet for Android (if
you want to refer to the Android version).




-------------------------------------
> A 6 month investment with 3 months on the high subsidy and 3 months on low subsidy would not be made…

 

Yes, this is the essential point. All capital investments are made based on expectations of future returns. To the extent that futures are perfectly knowable, they can be perfectly factored in. This is why inflation in Bitcoin is not a tax, it’s a cost. These step functions are made continuous by their predictability, removing that predictability will make them -- unpredictable.

 

Changing these futures punishes those who have planned properly and favors those who have not. Sort of like a Bitcoin bail-in; are some miners are too big to fail? It also creates the expectation that it may happen again. This infects the money with the sort of uncertainty that Bitcoin is designed to prevent.

 

e

 

From: bitcoin-dev-bounces@lists.linuxfoundation.org [mailto:bitcoin-dev-bounces@lists.linuxfoundation.org] On Behalf Of Tier Nolan via bitcoin-dev
Sent: Wednesday, March 2, 2016 10:08 AM
Cc: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] Hardfork to fix difficulty drop algorithm

 

On Wed, Mar 2, 2016 at 4:27 PM, Paul Sztorc via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org <mailto:bitcoin-dev@lists.linuxfoundation.org> > wrote:

For example, it is theoretically possible that 100% of miners (not 50%
or 10%) will shut off their hardware. This is because it is revenue
which ~halves, not profit.

 

It depends on how much is sunk costs and how much is marginal costs too.

If hashing costs are 50% capital and 50% marginal, then the entire network will be able to absorb a 50% drop in subsidy.

50% capital costs means that the cost of the loan to buy the hardware represents half the cost.

Assume that for every $100 of income, you have to pay $49 for the loan and $49 for electricity giving 2% profit.  If the subsidy halves, then you only get $50 of income, so lose $48.  

But if the bank repossesses the operation, they might as well keep things running for the $1 in marginal profit (or sell on the hardware to someone who will keep using it).

Since this drop in revenue is well known in advance, businesses will spend less on capital.  That means that there should be less mining hardware than otherwise.

A 6 month investment with 3 months on the high subsidy and 3 months on low subsidy would not be made if it only generated a small profit for the first 3 and then massive losses for the 2nd period of 3 months.  For it to be made, there needs to be large profit during the first period to compensate for the losses in the 2nd period.


-------------------------------------
On Friday, December 02, 2016 1:42:46 AM Jorge Timn via bitcoin-dev wrote:
> We can already warn users of a hardfork when a block is invalid (at
> least) because of the highest bit in nVersion (as you say, because it
> is forbidden since bip34 was deployed).

The difference is that right now, full nodes will happily follow a shorter 
best-valid chain. This BIP would require them to hold back at the best-common 
block between the best-valid chain and the invalid chain, forcing the user to 
make a decision whether to reject the invalid chain permanently, or upgrade to 
a version which can understand that chain as valid.

> It seems the softfork serves only to warn about soft-hardforks, assuming it
> chooses to use this mechanism (which a malicious soft hardfork may not do).

Note: a malicious "SHF" is not a SHF at all, but an "evil fork".

> In fact, you could reuse another of the prohibited bits to signal a soft-
> hardfork while distinguishing it from a regular hardfork. And this will also
> serve for old nodes that have not upgraded to the softfork. But, wait,
> if you signal a soft-hardfork with an invalid bit, it's not a
> soft-hardfork anymore, is it? It's simply a hardfork.

Nodes implementing this BIP will see it as a simple hardfork, but will 
intentionally provide equivalent behaviour as older nodes which see it as a 
soft-hardfork. In other words, all [compatible] hardforks will now behave like 
a soft-hardfork without any special DMMS design.

If Bitcoin's eventual hardfork is far enough down the road (such that no nodes 
remain from before this BIP are adopted), the SHF design could be safely done 
away with entirely. And either way, it makes it easier to resist an un-
consented-to hardfork.

Luke


-------------------------------------
On Tue, Jun 28, 2016 at 06:45:58PM +0200, Eric Voskuil via bitcoin-dev wrote:
> > then we should definitively use a form of end-to-end encryption between
> > nodes. Built into the network layer.
> 
> Widespread application of this model is potentially problematic. It is a
> non-trivial problem to design a distributed system that requires authentication
> but without identity and without central control. In fact this may be more
> challenging than Bitcoin itself. Trust on first use (TOFU) does not solve this
> problem.

Maybe the following paper can feed into this discussion:

  "Decentralized Anonymous Credentials" by Christina Garman, Matthew Green, Ian Miers
	https://eprint.iacr.org/2013/622.pdf

Alfie

-- 
Alfie John
https://www.alfie.wtf


-------------------------------------
Daniel,
   Thanks for proposing this.  I think this could have some useful use
cases as you state.  I was wondering what you would think to adding some
additional tones to optionally denote an amount (in satoshis?).

(FYI, actual link is here:  https://github.com/Dako300/BIP )

Justin

On Mon, Aug 8, 2016 at 2:22 PM, Daniel Hoffman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> This is my BIP idea: a fast, robust, and standardized for representing
> Bitcoin addresses over audio. It takes the binary representation of the
> Bitcoin address (little endian), chops that up into 4 or 2 bit chunks
> (depending on type, 2 bit only for low quality audio like american
> telephone lines), and generates a tone based upon that value. This started
> because I wanted an easy way to donate to podcasts that I listen to, and
> having a Shazam-esque app (or a media player with this capability) that
> gives me an address automatically would be wonderful for both the consumer
> and producer. Comes with error correction built into the protocol
>
> You can see the full specification of the BIP on my GitHub page (
> https://github.com/Dako300/BIP-0153).
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 

Justin W. Newton
Founder/CEO
Netki, Inc.

justin@netki.com
+1.818.261.4248

-------------------------------------
For sake of example, suppose we have a marginal fee rate of 50 satoshis per
byte.  At that rate reducing the size of the witness data by 1 byte is
approximately equivalent from a miner and relayer's perspective as a
replace by fee that increases the fee by 50 satoshis.  In both cases miners
get an extra potential of 50 satoshis in revenue.

So in this sense replacing witness data with smaller witness data can pay
for its own relay cost as much as RBF can simply by requiring that the
smaller witness be smaller enough to cover the same RBF threshold.

On Tue, Aug 16, 2016 at 6:39 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

> On Aug 17, 2016 00:36, "Russell O'Connor" <roconnor@blockstream.io> wrote:
>
> > Can I already do something similar with replace by fee, or are there
> limits on that?
>
> BIP125 and mempool eviction both require the replacing transaction to have
> higher fee, to compensate for the cost of relaying the replaced
> transaction(s).
>
> --
> Pieter
>

-------------------------------------
The second like "2)" has a link to the paper:
http://www.math.rwth-aachen.de/~Timo.Hanke/AsicBoostWhitepaperrev5.pdf

which does discuss the fact that it is "patent-pending".   Likewise it
discusses ASIC improvements.  Avoiding patents that impact bitcoin and are
not freely licensed, is something that is worthwhile for discussion.


On Tue, May 10, 2016 at 6:17 PM, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
>
> On Tue, May 10, 2016 at 3:57 PM, Peter Todd via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> As part of the hard-fork proposed in the HK agreement(1) we'd like to
>> make the
>> patented AsicBoost optimisation useless, and hopefully make further
>> similar
>> optimizations useless as well.
>>
>>
>> You say that you want to make patented optimization useless, but you
> point to a link that doesn't say anything about ASIC improvements or
> patents, which means that you have been planning to change the protocol
> rules with some miners (but not all the community).
>

> All changes to the protocol should be discussed in public here. If you
> want to make "further similar optimizations useless as well" then maybe you
> should propose a switch to EquiHash.
>
>
>
>>
>> 1)
>> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff
>>
>> 2)
>> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html
>>
>> --
>> https://petertodd.org 'peter'[:-1]@petertodd.org
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Hello all;
is there any number or id that determine uniquely the BTC value.
otherwise; is there any hash or address or key that when found we could say
this is my 50BTC or my 20BTC ?.

-------------------------------------
I think that we're not attacking the real source of the problem: that the
witness data size is not signed. It may be the case that a new source of
malleability is detected in witness programs later, or related to new
opcodes we'll soft-fork in the future.

The problem is real, as some systems (such as hardware wallets or other
low-memory IoT embedded systems) may have hard limits in the size of the
witness program they can accept. So we need a solution for all current and
future segwit extension problems.

We could soft-fork to add an opcode OP_PROGSIZE using segwit script
versioning that pushes in the stack the size of the segwit program being
evaluated, and then the script can take any action it wishes based on that.

Example:
<0x50> OP_PROGSIZE OP_GREATERTHAN OP_VERIFY ..... OP_CHECKSIG

Then an attacker cannot create a clone of the transaction having a witness
ECDSA signature longer than 0x50 bytes. (many details omitted in this
example)



On Wed, Aug 17, 2016 at 7:15 AM, Johnson Lau via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> > On August 17, 2016 at 12:40 AM Luke Dashjr <luke@dashjr.org> wrote:
> >
> >
> > On Wednesday, August 17, 2016 3:02:53 AM Johnson Lau via bitcoin-dev
> wrote:
> > > To completely replicate the original behaviour, one may use:
> > > "DEPTH TOALTSTACK IFDUP DEPTH FROMALTSTACK NUMNOTEQUAL IF 2DROP {if
> script}
> > > ELSE DROP {else script} ENDIF"
> >
> > This is much uglier than expected. IMO if that's the best workaround for
> the
> > current behaviour, people should just use "OP_1 OP_EQUAL OP_IF" when/if
> they
> > need to avoid malleability issues.
>
> It is ugly only if you want to faithfully replicate the behaviour. I'd
> argue that in no real use case you need to do this. For example, "OP_SIZE
> OP_IF" could just become "OP_SIZE OP_0NOTEQUAL OP_IF", since OP_SIZE must
> return a valid MINIMALDATA number.
>
> And your workaround does not fix malleability, since any non-0x01 values
> are valid FALSE
>
> However, in some case, enforcing MINIMALIF does require 1 more witness
> byte: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-August/
> 013036.html
>
> I think the best strategy is to make it a relay policy first, and decide
> whether we want a softfork later.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Wed, Jun 15, 2016 at 06:16:26PM -0700, Bram Cohen wrote:
> On Wed, Jun 15, 2016 at 5:10 PM, Peter Todd <pete@petertodd.org> wrote:
> 
> > On Tue, Jun 14, 2016 at 05:14:23PM -0700, Bram Cohen via bitcoin-dev wrote:
> > >
> > > Peter proposes that there should be both UTXO and STXO commitments, and
> >
> > No, that's incorrect - I'm only proposing TXO commitments, not UTXO nor
> > STXO
> > commitments.
> >
> 
> What do you mean by TXO commitments? If you mean that it only records
> insertions rather than deletions, then that can do many of the same proofs
> but has no way of proving that something is currently in the UTXO set,
> which is functionality I'd like to provide.

I think you need to re-read my original post on TXO commitments, specifically
where I say:

    # TXO Commitments

    A merkle tree committing to the state of __all transaction outputs, both spent
    and unspent__, we can provide a method of compactly proving the current state of
    an output.

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012715.html

> When I say 'merkle tree' what I mean is a patricia trie. What I assume is
> meant by a merkle mountain range is a series of patricia tries of
> decreasing size each of which is an addition to the previous one, and
> they're periodically consolidated into larger tries so the old ones can go
> away. This data structure has the nice property that it's both in sorted
> order and has less than one cache miss per operation because the
> consolidation operations can be batched and done linearly. There are a
> number of different things you could be describing if I misunderstood.

Nope, MMR's are completely unlike what you just described.

> > I'm not proposing STXO commitments precisely because the set of _spent_
> > transactions grows without bound.
> 
> 
> I'm worried that once there's real transaction fees everyone might stop
> consolidating dust and the set of unspent transactions might grow without
> bound as well, but that's a topic for another day.

Ok, but then if you're concerned about that risk, why introduce a data
structure - the STXO set - that's _guaranteed_ to grow without bound?

> > > Now I'm going to go out on a limb. My thesis is that usage of a mountain
> > > range is unnecessary, and that a merkle tree in the raw can be made
> > > serviceable by sprinkling magic pixie dust on the performance problem.
> >
> > It'd help if you specified exactly what type of merkle tree you're talking
> > about here; remember that the certificate transparency RFC appears to have
> > reinvented merkle mountain ranges, and they call them "merkle trees".
> > Bitcoin
> > meanwhile uses a so-called "merkle tree" that's broken, and Zcash uses a
> > partially filled fixed-sized perfect tree.
> >
> 
> What I'm making is a patricia trie. Its byte level definition is very
> similar to the one in your MMR codebase.

Which codebase exactly? I have both a insertion-ordered list (MMR) and a
key:value mapping (referred to as a "merbinner tree" in the codebase) in the
proofchains codebase. They're very different data structures.

> Each level of the tree has a single metadata byte and followed by two
> hashes. The hashes are truncated by one byte and the hash function is a
> non-padding variant of sha256 (right now it's just using regular sha256,
> but that's a nice optimization which allows everything to fit in a single
> block).
> 
> The possible metadata values are: TERM0, TERM1, TERMBOTH, ONLY0, ONLY1,
> MIDDLE. They mean:
> 
> TERM0, TERM1: There is a single thing in the tree on the specified side.
> The thing hashed on that side is that thing verbatim. The other side has
> more than one thing and the hash of it is the root of everything below.
> 
> TERMBOTH: There are exactly two things below and they're included inline.
> Note that two things is a special case, if there are more you sometimes
> have ONLY0 or ONLY1.
> 
> ONLY0, ONLY1: There are more than two things below and they're all on the
> same side. This makes proofs of inclusion and exclusion simpler, and makes
> some implementation details easier, for example there's always something at
> every level with perfect memory positioning. It doesn't cause much extra
> memory usage because of the TERMBOTH exception for exactly two things.
> 
> MIDDLE: There two or more things on both sides.
> 
> There's also a SINGLETON special case for a tree which contains only one
> thing, and an EMPTY special value for tree which doesn't contain anything.
> 
> The main differences to your patricia trie are the non-padding sha256 and
> that each level doesn't hash in a record of its depth and the usage of
> ONLY0 and ONLY1.

I'm rather confused, as the above sounds nothing like what I've implemented,
which only has leaf nodes, inner nodes, and the special empty node singleton,
for both the MMR and merbinner trees.

> > I'm having a hard time understanding this paragraph; could you explain
> > what you
> > think is happening when things are "merged into larger hills"?
> >
> 
> I'm talking about the recalculation of mountain tips, assuming we're on the
> same page about what 'MMR' means.

Yeah, we're definitely not...

In MMR's append operations never need to modify mountain contents.

> > As UTXO/STXO/TXO sets are all enormously larger than L1/L2 cache, it's
> > impossible to get CPU cache misses below one for update operations. The
> > closest
> > thing to an exception is MMR's, which due to their insertion-ordering could
> > have good cache locality for appends, in the sense that the mountain tips
> > required to recalculate the MMR tip digest will already be in cache from
> > the
> > previous append. But that's not sufficient, as you also need to modify old
> > TXO's further back in the tree to mark them as spent - that data is going
> > to be
> > far larger than L1/L2 cache.
> >
> 
> This makes me think we're talking about subtly different things for MMRs.
> The ones I described above have sub-1 cache miss per update due to the
> amortized merging together of old mountains.

Again, see above.

> Technically even a patricia trie utxo commitment can have sub-1 cache
> misses per update if some of the updates in a single block are close to
> each other in memory. I think I can get practical Bitcoin updates down to a
> little bit less than one l2 cache miss per update, but not a lot less.

I'm very confused as to why you think that's possible. When you say "practical
Bitcoin updates", what exactly is the data structure you're proposing to
update? How is it indexed?

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Sun, Feb 07, 2016 at 09:16:02AM -0500, Gavin Andresen via bitcoin-dev wrote:
> There will be approximately zero percentage of hash power left on the
> weaker branch of the fork, based on past soft-fork adoption by miners (they
> upgrade VERY quickly from 75% to over 95%).

The stated reasoning for 75% versus 95% is "because it gives "veto power"
to a single big solo miner or mining pool". But if a 20% miner wants to
"veto" the upgrade, with a 75% threshold, they could instead simply use
their hashpower to vote for an upgrade, but then not mine anything on
the new chain. At that point there'd be as little as 55% mining the new
2MB chain with 45% of hashpower remaining on the old chain. That'd be 18
minute blocks versus 22 minute blocks, which doesn't seem like much of
a difference in practice, and at that point hashpower could plausibly
end up switching almost entirely back to the original consensus rules
prior to the grace period ending.

With a non-consensus fork, I think you need to expect people involved to
potentially act in ways that aren't very gentlemanly, and guard against
it if you want the fork to be anything other than a huge mess.

Cheers,
aj



-------------------------------------
You keep calling flexible transactions "safer", and yet you haven't
mentioned that the current codebase is riddled with blatant and massive
security holes. For example, you seem to have misunderstood C++'s memory
model - you would have no less than three out-of-bound, probably
exploitable memory accesses in your 80-LoC deserialize method at
https://github.com/bitcoinclassic/bitcoinclassic/blob/develop/src/primitives/transaction.cpp#L119
if you were to turn on flexible transactions (and I only reviewed that
method for 2 minutes). If you want to propose an alternative to a
community which has been in desperate need of fixes to many problems for
several years, please do so with something which would not take at least
a year to complete given a large team of qualified developers.

You additionally have not yet bothered to address the discussion of
soft-fork security at
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012014.html
which I believe answers all of your claims about upgrades required in a
much more detailed discussion than I will include here. Please take your
off-topic discussions there instead of this thread.

On 10/16/16 18:20, Tom Zander via bitcoin-dev wrote:
> On Sunday, 16 October 2016 09:47:40 CEST Douglas Roark via bitcoin-dev 
> wrote:
>> Would I want anyone to lose money due to faulty wallets? Of course not.
>> By the same token, devs have had almost a year to tinker with SegWit and
>> make sure the wallet isn't so poorly written that it'll flame out when
>> SegWit comes along. It's not like this is some untested, mostly unknown
>> feature that's being slipped out at the last minute
> 
> There have been objections to the way that SegWit has been implemented for a 
> long time, some wallets are taking a "wait and see" approach.  If you look 
> at the page you linked[1], that is a very very sad state of affairs. The 
> vast majority is not ready.  Would be interesting to get a more up-to-date 
> view.
> Wallets probably won't want to invest resources adding support for a feature 
> that will never be activated. The fact that we have a much safer alternative 
> in the form of Flexible Transactions may mean it will not get activated. We 
> won't know until its actually locked in.
> Wallets may not act until its actually locked in either. And I think we 
> should respect that.
> 
> Even if all wallets support it (and thats a big if), they need to be rolled 
> out and people need to actually download those updates.
> This takes time, 2 months after the lock-in of SegWit would be the minimum 
> safe time for people to actually upgrade.
> 
> 1) https://bitcoincore.org/en/segwit_adoption/
> 


-------------------------------------
Hi Andy

>>
>>> Does openssh have this same problem?
>> No. OpenSSH doesn't make an effort to protect the privacy of its users.
>>
>>> I'm assuming this could be parallelized very easily, so it is not a huge
>>> problem?
>> It's not a issue because we're not aware of any usecase where a node
>> would have a large list of authenticated peers.
>>
>>> Each peer can configure one identity-key (ECC, 32 bytes) per listening
>> network interface (IPv4, IPv6, tor).
>>
>> I'm not aware of any reason for this limitation to exist. A node
>> should be able to have as many listening identities as it wants, with
>> a similar cost to having a large authorized keys list.
>>
> 
> So you are saying that you agree with me that the original text needs to
> be revised slightly or I am just misinterpreting the original text?

Yes. I think this limitation could be removed.
A responding node can have  in theory  multiple identity-keys per
network interface (network interfaces is also confusing, because you
could run multiple bitcoind instances on the same interface with
different ports).

The BIP should just make clear, that it is probably wise, to use
different identity-keys for each network interface (ipv4, v6, tor).

I'll try to overhaul that part.

</jonas>


-------------------------------------
Hi Dana

>> The URI scheme does not require any sorts of wallet app level
>> configuration (where the stdio/pipe approach would require to configure
>> some details about the used hardware wallet).
> 
> Hi everybody, just thought Id throw my opinion in here.
> 
> The URI scheme is a nice idea, but this ignores the fact that hardware wallet vendors do most of the work on talking between the computer/mobile and the wallet on a lower level of communication. In the case of BitLox, the base protocol is Googles ProtoBuf. The commands and transaction data is in a schema which is then encoded in different methods accessible via ProtoBuf (depending on the data being sent). The advantages of this protocol is that it can be implemented on a wide variety of platforms. (but thats a whole 'nother discussion)
> 
> The URI would be handled waaaaay up in the specific application (such as the mytrezor wallet software or the various standalone wallets) - nowhere near the actual hardware communications layer.

This is maybe a question of the scope.
The BIP I'm proposing would make a clear interface cut between
wallet-with-unsigned-transaction and a signing-device (and maybe between
wallet-requires-pubkey, signing-device generate some pubkeys [or
non-hardened xpub]).

The detached-signing proposal does not duplicate work. It just moves the
current plugin design into a separate application. Plugins in security
and privacy critical wallet software is something that should probably
be avoided.

It's intentional at a high level to allow maximum flexibility at the
hardware interaction layer.

Your protobuf example is a good use-case. You could implement your
custom processes behind the URI scheme (which is probably way more
efficient then writing a couple of wallet plugins where you  at the end
 mostly don't control the deployment and the source-code).

Defining a standard on the hardware interaction layer is possible, but a
fairly different approach.

</jonas>


-------------------------------------

>> I have just PRed a draft version of two BIPs I recently wrote.
>> https://github.com/bitcoin/bips/pull/362
> 
> I suggest running a spellchecker ;)

Thanks. Will do.


> * why would you not allow encryption on non-pre-approved connections?

The encryption should be optional.
The proposed authentication scheme does take care of the
identity-management and therefor prevent MITM attacks.
Without the identity management, you might not detect sending/receiving
encrypted data from/to a MITM.

> * we just removed (ssl) encryption from the JSON interface, how do you suggest 
> this encryption to be implemented without openSSL?

The proposed encryption schema is based on ECDSA/ECDH (implemented in
libsecp256k1) and AES256CBC (implementation is on the way see
https://github.com/bitcoin/bitcoin/pull/7689).
OpenSSL is not required.

> * What is the reason for using the p2p code to connect a wallet to a node?
> I suggest using one of the other connection methods to connect to the node. 
> This avoids a change in the bitcoin protocol for a very specific usecase.

Most known use-case: SPV.

> * Why do you want to do a per-message encryption (wrapping the original)? 
> Smaller messages that contain predictable content and are able to be matched 
> to the unencrypted versions on the wire send to other nodes will open this 
> scheme up to various old statistical attacks.

It's probably extremely inefficient to create a constant time stream.
Even most SSL/SSH application leak information because of the
communication message characteristics.

The current wrapping message proposal is not very efficient.
I will change it so that the p2p message header will contain the
encryption metadata. This should lead to a tiny overhead.


> 
>> Responding peers must ignore (banning would lead to fingerprinting) the 
> requesting peer after 5 unsuccessfully authentication tries to avoid resource 
> attacks.
> 
> Any implementation of that kind would itself again be open to resource 
> attacks.
> Why 5? Do you want to allow a node to make a typo?

Good point. Maybe one false try should lead to ignoring the peer.

> 
> 
>> To ensure that no message was dropped or blocked, the complete communication 
> must be hashed (sha256). Both peers keep the SHA256 context of the encryption 
> session. The complete <code>enc</code> message (leaving out the hash itself) 
> must be added to the hash-context by both parties. Before sending a 
> <code>enc</code> command, the sha256 context will be copied and finalized.
> 
> You write "the complete communication must be hashed" and every message has a 
> hash of the state until it is at that point.
> I think you need to explain how that works specifically.

This is a relative simple concept and does not require rehashing the
whole communication. You just append the "new data".

Some pseudocode:

SHA256CTX ctx;

// first com
SHA256CTX_Update(ctx, 1stmessage);

// copy context
SHA256CTX ctxnew = ctx;

// finalize the copied context
sha256hash = SHA256CTX_Finalize(ctxnew); //use as checksum hash


//////// next message
SHA256CTX_Update(ctx, 2ndmessage);

// copy context
SHA256CTX ctxnew = ctx;

// finalize the copied context
sha256hash = SHA256CTX_Finalize(ctxnew); //use as checksum hash

... etc.

</jonas>


-------------------------------------
 

BIP draft: Hard fork opt-in mechanism for SPV nodes:
https://github.com/bitcoin/bips/pull/320

This is a supplement, instead of a replacement, of the hardfork bit BIP:
https://github.com/bitcoin/bips/pull/317

They solves different problems:

The hardfork bit tells full and SPV that a planned hardfork (instead of
a softfork) has happened.

This BIP makes sure SPV nodes won't lose any money in a hardfork, even
if they do not check the hardfork bit.

---------------------

BIP: ?
Title: Hard fork opt-in mechanism for SPV nodes
Author: Johnson Lau <jl2012@xbt.hk>
Status: Draft
Type: Standard Track
Created: 2016-02-05

ABSTRACT

This document specifies a new algorithm for the transaction commitment
in block header, to ensure that SPV nodes will not automatically follow
a planned hard fork without explicit opt-in consent. 

 [1]MOTIVATION

A hard fork in Bitcoin is a consensus rule change where previously
invalid blocks become valid. For the operators of fully validating
nodes, migration to the new fork requires conscious actions. However,
this may not be true for SPV node, as many consensus rules are
transparent to them. SPV nodes may follow the chain with most
proof-of-work, even if the operators do not agree with the economical or
ideological properties of the chain. 

By specifying a new algorithm for the transaction commitment in block
header, migration to the new fork requires explicit opt-in consent for
SPV nodes. It is expected that this proposal will be implemented with
other backward-incompatible consensus rule changes at the same time. 

 [2]SPECIFICATION

The calculation of Merkle root remains unchanged. Instead of directly
committing the Merkle root to the header, we commit 

 Double-SHA256(zero|merkle_root|zero)

where zero is 0x0000....0000 with 32 bytes. 

 [3]RATIONALE

Since the header structure is not changed, non-upgraded SPV nodes will
still be able to verify the proof-of-work of the new chain, and they
will follow the new chain if it has most proof-of-work. However, they
will not be able to the accept any incoming transactions on the new
chain since they cannot verify them with the new commitment format. At
the same time, SPV nodes will not accept any new transactions on the old
chain, as they find it has less proof-of-work. Effectively, SPV nodes
stop accepting any transactions, until their operators take further
actions. 

Zero-padding is applied before and after the merkle_root, so it is not
possible to circumvent the rule change with any current implementations,
even for faulty ones. 

A future hard fork should change the padding value to stop non-upgraded
SPV nodes from processing new transactions. 

Hard forks may sometimes be totally uncontroversial and make barely
noticeable change (BIP50 [4], for example). In such cases, changing the
padding value may not be needed as it may cause unnecessary disruption.
The risk and benefit should be evaluated case-by-case. 

 [5]COMPATIBILITY

As a mechanism to indicate hard fork deployment, this BIP breaks
backward compatibility intentionally. However, without further changes
in the block header format, non-upgraded full nodes and SPV nodes could
still verify the proof-of-work of upgraded blocks. 

INTERACTION WITH FRAUD PROOF SYSTEM A fraud proof system is full nodes
that will generate compact proofs to testify invalid blocks on the
blockchain, verifiable by SPV nodes. Hard forks without any malicious
intention may also be considered as a "fraud" among non-upgraded nodes.
This may not be desirable, as the SPV node may accept devalued tokens on
the old chain with less proof-of-work. With this BIP, non-upgraded SPV
nodes will always believe the new chain is valid (since they cannot
verify any fraud proof), while cannot be defrauded as they will not see
any incoming transactions. 

 [6]COPYRIGHT

This document is placed in the public domain. 

Links:
------
[1]
https://github.com/jl2012/bips/blob/merkleroot/spvoptinhf.mediawiki#motivation
[2]
https://github.com/jl2012/bips/blob/merkleroot/spvoptinhf.mediawiki#specification
[3]
https://github.com/jl2012/bips/blob/merkleroot/spvoptinhf.mediawiki#rationale
[4] https://github.com/jl2012/bips/blob/merkleroot/bip-0050.mediawiki
[5]
https://github.com/jl2012/bips/blob/merkleroot/spvoptinhf.mediawiki#compatibility
[6]
https://github.com/jl2012/bips/blob/merkleroot/spvoptinhf.mediawiki#copyright
-------------------------------------
On Friday, March 18, 2016 9:42:16 AM Btc Drak wrote:
> On Wed, Mar 16, 2016 at 10:24 PM, Luke Dashjr <luke@dashjr.org> wrote:
> > BIP Comments are not a part of the BIP itself, merely post-completion
> > notes from various external parties. So having them external does not
> > make the BIP
> > any less self-contained. Right now, this information takes the form of
> > reddit/forum comments, IRC chats, etc.
> 
> BIP2 does not state the comments section is where discussion happens for
> the BIP, but for a sort of final summary.

Yes, discussion for the BIP still happens on the mailing list.

> > It is important that the forum for comments have a low barrier of use.
> > The Bitcoin Wiki requires only a request for editing privileges, whereas
> > GitHub wiki would require reading and agreeing to a lengthy Terms of
> > Service contract.
> 
> Seems weak, it's much easier to sign up for a Github account and most have
> one already. It's certainly easier than either paying to get edit
> privileges on the Bitcoin Wiki find someone to convince you're genuine an
> obscure IRC channel.

Weak? What does that even mean? GitHub's terms are no trivial list. It's not a 
matter of "easy", but whether you're willing to agree to the terms or not - 
and people should be free to participate without doing so. The Bitcoin Wiki 
has never had a problem with whitelisting people, and isn't exclusively 
available via IRC.

> > In terms of staleness, the Wiki has been shown to stand the test of time,
> > and
> > is frankly less likely to move than the GitHub repository.
> > 
> > The BIP process originated on the Wiki, and was only moved to GitHub
> > because
> > stronger moderation was needed (eg, to prevent random other people from
> > editing someone else's BIP; number self-assignments; etc). Such
> > moderation is
> > not only unnecessary for BIP Comments, but would be an outright nuisance.
> 
> I'm not sure that is the reason why, but in any case, Github is a more
> sensible place because of the collaborative features which is why they
> became the centre of OSS software development for hundreds of thousands of
> projects.

GitHub's collaborative features for the wiki function is clearly inferior.

> > I hope this addresses all your concerns and we can move forward with BIP
> > 2 unmodified?
> 
> I am sorry but it has not. I still strongly object to using the Bitcoin
> Wiki or any external source source for the commentary part of BIP2. I
> believe it should be done on using the Wiki feature at bitcoin/bips. If
> that is not acceptable, then I would suggest a separate page in the bip
> assets folder, called bip<nnnn>/comments.md. On a side note, more complex
> reference implementation code should be stored in that folder too.

Then you're essentially standing in the way of BIP 2 and stalling it.

I have no interest in having to manually approve every single little comment 
on BIPs, and I think it's likely nobody will use it if doing so requires such 
effort.

> > (On another note, I wonder if we should recommend non-reference
> > implementation
> > lists/links be moved to BIP Comments rather than constantly revising the
> > BIPs
> > with them...)
> 
> Certainly those could be on the comments page.


-------------------------------------
Due to a relatively trivial fix for an out-of-place assertion in rc2
(see
https://github.com/bitcoin/bitcoin/commit/58d4fa7da30cb57e5fc3dca62f49a64e126c76cd),
0.13.1rc3 was tagged and is now available on github either via git or at
https://github.com/bitcoin/bitcoin/releases/tag/v0.13.1rc3.

Because of the simplicity of the change, it was decided that 0.13.1
should not be delayed as a result, and thus no official binaries will be
made available for rc3.

However, any additional testing which folks have time for before final
tag on Thursday would be significantly welcomed.

As usual please report bugs using the issue tracker on github at
https://github.com/bitcoin/bitcoin/issues

Thanks,
Matt


-------------------------------------
On Wednesday 23 Mar 2016 22:55:34 Jonas Schnelli via bitcoin-dev wrote:
> >> I have just PRed a draft version of two BIPs I recently wrote.
> > * why would you not allow encryption on non-pre-approved connections?
> 
> The encryption should be optional.
> The proposed authentication scheme does take care of the
> identity-management and therefor prevent MITM attacks.
> Without the identity management, you might not detect sending/receiving
> encrypted data from/to a MITM.

If you want to extend the Bitcoin protocol itself, you will have to resolve 
that. Which many other solutions do (ssh for instance).

It would not be Ok to have an peer-to-peer encryption system that doesn't 
allow non-pre-approved connections.

> > * What is the reason for using the p2p code to connect a wallet to a node?
> > I suggest using one of the other connection methods to connect to the
> > node.
> > This avoids a change in the bitcoin protocol for a very specific usecase.
> 
> Most known use-case: SPV.

You didn't answer the question.

> > * Why do you want to do a per-message encryption (wrapping the original)?
> > Smaller messages that contain predictable content and are able to be
> > matched to the unencrypted versions on the wire send to other nodes will
> > open this scheme up to various old statistical attacks.
> 
> It's probably extremely inefficient to create a constant time stream.

Your use of "probably" makes me wonder if you already have an implementation. 
Doing any encryption and handshaking design *without* actually having it coded 
and gone though testing yet makes no sense.
I do not belief Bitcoin will benefit from "design by committee" where a 
specification is drawn before an implementation is written.

Also, you didn't actually address the attack-vector.

 
> >> Responding peers must ignore (banning would lead to fingerprinting) the
> > 
> > requesting peer after 5 unsuccessfully authentication tries to avoid
> > resource attacks.
> > 
> > Any implementation of that kind would itself again be open to resource
> > attacks.
> > Why 5? Do you want to allow a node to make a typo?
> 
> Good point. Maybe one false try should lead to ignoring the peer.

That doesn't take away the resource attack at all.

 
> >> To ensure that no message was dropped or blocked, the complete
> >> communication> 
> > must be hashed (sha256). Both peers keep the SHA256 context of the
> > encryption session. The complete <code>enc</code> message (leaving out
> > the hash itself) must be added to the hash-context by both parties.
> > Before sending a <code>enc</code> command, the sha256 context will be
> > copied and finalized.
> > 
> > You write "the complete communication must be hashed" and every message
> > has a hash of the state until it is at that point.
> > I think you need to explain how that works specifically.
> 
> This is a relative simple concept and does not require rehashing the
> whole communication. 

Apologies, I should have been more clear; the BIP should specify the actual 
algorithm, otherwise you can't create an implementation from just reading the 
BIP.

Also, this may be a good time to ask why you want to have a per-message 
encryption?
Practically every single popular end-to-end encryption uses one approach or 
another were it just encrypts as another layer. (the  L in ssl). You are 
mixing layers, and unless you do that for a very good reason, or have a very 
good reason why everyone else is doing it wrong, I suggest using a layered 
encryption approach.


-------------------------------------
This sort of statement represents one consequence of the aforementioned bad precedent.

Are checkpoints good now? Are hard forks okay now?

What is the maximum depth of a reorg allowed by this non-machine consensus?

Shouldn't we just define a max depth so that all cruft deeper than that can just be discarded on a regular basis?

Why are there activation heights defined by this hard fork if it's not possible to reorg back to them?

The "BIP" is neither a Proposal (it's been decided, just documenting for posterity), nor an Improvement (there is no actual benefit, just some tidying up in the notoriously obtuse satoshi code base), nor Bitcoin (a hard fork defines an alt coin, so from Aug 4 forward it has been CoreCoin).

e

> On Nov 16, 2016, at 5:29 AM, Jameson Lopp <jameson.lopp@gmail.com> wrote:
> 
> Since "buried deployments" are specifically in reference to historical consensus changes, I think the question is more one of human consensus than machine consensus. Is there any disagreement amongst Bitcoin users that BIP34 activated at block 227931, BIP65 activated at block 388381, and BIP66 activated at block 363725? Somehow I doubt it.
> 
> It seems to me that this change is merely cementing into place a few attributes of the blockchain's history that are not in dispute.
> 
> - Jameson
> 
>> On Tue, Nov 15, 2016 at 5:42 PM, Eric Voskuil via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> Actually this does nothing to provide justification for this consensus rule change. It is just an attempt to deflect criticism from the fact that it is such a change.
>> 
>> e
>> 
>> > On Nov 15, 2016, at 9:45 AM, Btc Drak <btcdrak@gmail.com> wrote:
>> >
>> > I think this is already covered in the BIP text:-
>> >
>> > "As of November 2016, the most recent of these changes (BIP 65,
>> > enforced since December 2015) has nearly 50,000 blocks built on top of
>> > it. The occurrence of such a reorg that would cause the activating
>> > block to be disconnected would raise fundamental concerns about the
>> > security assumptions of Bitcoin, a far bigger issue than any
>> > non-backwards compatible change.
>> >
>> > So while this proposal could theoretically result in a consensus
>> > split, it is extremely unlikely, and in particular any such
>> > circumstances would be sufficiently damaging to the Bitcoin network to
>> > dwarf any concerns about the effects of this proposed change."
>> >
>> >
>> > On Mon, Nov 14, 2016 at 6:47 PM, Eric Voskuil via bitcoin-dev
>> > <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> >> NACK
>> >>
>> >> Horrible precedent (hardcoding rule changes based on the assumption that
>> >> large forks indicate a catastrophic failure), extremely poor process
>> >> (already shipped, now the discussion), and not even a material performance
>> >> optimization (the checks are avoidable once activated until a sufficiently
>> >> deep reorg deactivates them).
>> >>
>> >> e
>> >>
>> >> On Nov 14, 2016, at 10:17 AM, Suhas Daftuar via bitcoin-dev
>> >> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> >>
>> >> Hi,
>> >>
>> >> Recently Bitcoin Core merged a simplification to the consensus rules
>> >> surrounding deployment of BIPs 34, 66, and 65
>> >> (https://github.com/bitcoin/bitcoin/pull/8391), and though the change is a
>> >> minor one, I thought it was worth documenting the rationale in a BIP for
>> >> posterity.
>> >>
>> >> Here's the abstract:
>> >>
>> >> Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated via miner
>> >> signaling in block version numbers. Now that the chain has long since passed
>> >> the blocks at which those consensus rules have triggered, we can (as a
>> >> simplification and optimization) replace the trigger mechanism by caching
>> >> the block heights at which those consensus rules became enforced.
>> >>
>> >> The full draft can be found here:
>> >>
>> >> https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki
>> >>
>> >> _______________________________________________
>> >> bitcoin-dev mailing list
>> >> bitcoin-dev@lists.linuxfoundation.org
>> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >>
>> >>
>> >> _______________________________________________
>> >> bitcoin-dev mailing list
>> >> bitcoin-dev@lists.linuxfoundation.org
>> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 

-------------------------------------
Hello again,

sorry, got a bit derailed on that proposal.
But now I think its time to work on it again.

- Any objections to get a BIP-number for it? 
	If not, can I get one, so I can finish up the test vectors.
	Current version: https://github.com/DanielWeigl/bips/blob/master/bip-p2sh-accounts.mediawiki 

- I decided against extending it for future P2WPKH addresses
	I think that should be a separate account on its own, to reduce implementation work 
	for future wallets, that only want/need to implement P2WPKH accounts. And to keep it simple.
	Was someone working on the P2WPKH address format in the meantime? (ie. alternative for [2])

- We will also need a extension to the BIP32 serialization format[1]
	It should be possible to export/import a xPriv/xPub key across compatible wallets, and they
	should be able without guesswork, fuzzy checks or asking the user to import the correct account type.
	Thinking about some flexible tag-based backwards compatible extensions - but thats a different BIP in itself.


Cheers,
Daniel


[1] https://github.com/DanielWeigl/bips/blob/master/bip-0032.mediawiki#Serialization_format
[2] https://github.com/bitcoin/bips/blob/master/bip-0142.mediawiki

On 2016-06-14 17:41, Daniel Weigl via bitcoin-dev wrote:
> Hi List,
> 
> Following up to the discussion last month ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012695.html ), ive prepared a proposal for a BIP here:
> 	
> 	https://github.com/DanielWeigl/bips/blob/master/bip-p2sh-accounts.mediawiki
> 
> 
> Any comments on it? Does anyone working on a BIP44 compliant wallet implement something different?
> If there are no objection, id also like to request a number for it.
> 
> Thx,
> Daniel
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
>> In my opinion, the question should be "why would you _not_ encrypt".
> 
> 1) creation of a false sense of security

False sense of security is mostly a communication issue.
BIP151 does focus on encryption (not trust).

Are users aware of the fact that ISP/WiFi-Providers can track their
bitcoin spending (if they use SPV+BF) and link it with other internet
traffic or sell the data to anyone who is interested to do correlation?

Are node operators aware of the possibilities that ISPs/Data-Centers,
etc. can hold back peers, etc.?

If there is a false sense of security/anonymity, then we are already
deep into this territory.
BIP151 was designed as a puzzle-pice towards better security and better
censorship resistance. You shouldn't project all sorts of "false sense
of security" into BIP151. Is a stepping stone towards greater security.

> 2) as a tradeoff against anonymity

Can you point out the tradeoffs?
BIP151 does not introduce fingerprinting possibilities.

> 3) benefit does not justify cost

Can you elaborate the costs?
[Extremely simplified]: we need 300 lines of code from openssh
(ChaCha20-Poly1305@openssl) and some ECDH magic (already in
Bitcoin-Cores codebase) together with two or three (maybe payed)
cryptoanalysis once the implementation is done.


>> There are plenty of other options to solve this problem. stunnel,
>> Bernsteins CurveCP, VPN, etc. which are available since years.
>> But the reality has shown that most bitcoin traffic is still unencrypted.
> 
> The question arises from concern over the security of the network in the case where encryption (and therefore authentication) is pervasive.
> 
> As you point out, anyone can set up a private network of nodes today. These nodes must also connect to the permissionless network to maintain the chain. These nodes constitute a trust zone within Bitcoin. This zone of exclusion operates as a single logical node from the perspective of the Bitcoin security model (one entity controls the validation rules for all nodes).
> 
> Widespread application of this model is potentially problematic. It is a non-trivial problem to design a distributed system that requires authentication but without identity and without central control. In fact this may be more challenging than Bitcoin itself. Trust on first use (TOFU) does not solve this problem.

Yes. There is no plan to adopt a TUFO scheme. Bip151 does not use TUFO
it does not cover "trust" (It just encrypt all traffic).

Imaging Bip151 together with a simple form of preshared EC key
authentication (nonce signing or similar). You could drastically
increase the security/censor-resistance-properties between nodes where
owners have preshared identity keys (with nodes I also mean SPV/wallet
nodes).

And I guess there are plenty of awesome identity management system ideas
tied or not tied to the Bitcoin blockchain out there.
This is also a reason to not cover trust/authentication/identity in BIP151.
It is  possible to have multiple authentication schemes.

> In my opinion this question has not received sufficient consideration to warrant proceeding with a network encryption scheme (which concerns me as well, but as I consider it premature I won't comment).

Yes. I think nobody have started implementing BIP151. It's a draft BIP
and I think it's still okay and great that we have this discussion.

BIP151 hopefully has started some brainwork in how encryption and
authentication could work in Bitcoin and I'm happy to deprecate BIP151
if we have found a better solution or if we come to a point where we
agree that BIP151 does make the network security worse.

>> Example: IIRC non of the available SPV wallets can "speak" on of the
>> possible encryption techniques. Encrypting traffic below the application
>> layer is extremely hard to set up for non-experienced users.
> 
> Bloom filters can (and IMO should) be isolated from the P2P protocol. Also, if the proposal creates an insecurity its ease of deployment is moot.

If we assume increasing amount of novice users starting with Bitcoin
every day, how should these users run wallets without increasing
centralization by using webwallets or client/central-server wallets?
(which is OT, but an interesting question)

>> On top of that, encryption allows us to drop the SHA256 checksum per p2p
>> message which should result in a better performance on the network layer
>> once BIP151 is deployed.
> 
> I would not consider this a performance enhancing proposal. Simply dropping the checksum seems like a better option. But again, it is moot if it creates an insecurity.
> 
>> I agree that BIP151 _must_ be deployed together with an authentication
>> scheme (I'm working on that) to protect again MITM during encryption
>> initialization.
> 
> At a minimum I would propose that you modify BIP151 to declare a dependency on a future BIP, making BIP151 incomplete without it. I think we can agree that it would be unadvisable to deploy (and therefore to implement) encryption alone.

I think BIP151 does what it says: encryption and laying groundwork for
authentication.
You wouldn't probably say BIP32 is incomplete because it does not cover
a scheme how to recover funds (or BIP141 [SW consensus] is incomplete
because it does not cover p2p [BIP144]).

The missing MITM protection (solvable over auth) is prominent mentioned
in the BIP [1].


(from your other mail):
>> I don't see reasons why BIP151 could weaken the security of the P2P network. Can you point out some specific concerns?
> 
> TOFU cannot prevent MITM attacks (the goal of the encryption). Authentication requires a secure (trusted) side channel by which to distribute public keys. This presents what I consider a significant problem. If widespread, control over this distribution network would constitute control over who can use Bitcoin.
> The effort to prevent censorship could actually enable it. I don't think it would get that far. Someone would point this out in the process of vetting the authentication BIP, and the result would be the scrapping of BIP151.

I agree that the secure trusted 2nd channel key-sharing problem can be
significant for large networks and/or connecting to unknown identities.

But as said, there could be multiple ways of sharing identity keys.
If you want to connect your node to serval other trusted nodes, you can
simply physically preshare keys or do it over GPG / Signal App, etc..

And if I have followed the news correctly, there are some clever guys
working on various internet of trust 2.0 proposals...

>>
>> BIP151 does not rely on identities. BIP151 does not use persisted keys
>> (only ephemeral keys).
> 
> BIP 151 is incomplete without authentication.

I would agree if you would say, _trusted encryption_ is incomplete with
authentication. But IMO BIP151 is complete and should be deployed
together with one or multiple authentication schemes.


[1] https://github.com/bitcoin/bips/blob/master/bip-0151.mediawiki#risks


-------------------------------------
On Sat, Aug 06, 2016 at 07:15:22AM -0700, Chris Priest via bitcoin-dev wrote:
> If the blocksize limit is to be changed to a block output limit, the
> number the limit is set to should be roughly the amount of outputs
> that are found in 1MB blocks today. This way, the change should be

The largest output on testnet is a bit under 1MB, and encodes a certain
well-known love song...

In many circumstances(1) miners have an incentive to create larger blocks that
take their competitors longer to receive and validate, so protocol-level block
limits need to take all these potential DoS vectors into account; serialized
size is one of the most fundemental things that needs to be limited.

> considered non-controversial. I think its silly that some people think
> its a good thing to keep usage restricted, but again, it is what it
> is.

As mentioned above, and explained in detail in my recent blog post(1),
restrictions are needed to keep a level playing field between all miners.

1) https://petertodd.org/2016/block-publication-incentives-for-miners

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
>
> The point I'm making is simply that to be useful, when you close a seal you
> have to be able to close it over some data, in particular, another seal.
> That's
> the key thing that makes the idea a useful construct for smart contacts,
> value
> transfer/currency systems, etc.
>

OK, your second post ("Closed Seal Sets and Truth Lists for Better Privacy
and Censorship Resistance") seems to clarify that this data is one of
arguments to the condition function.
Frankly this stuff is rather hard to follow. (Or maybe I'm dumb.)

Now I don't get scability properties. Let's consider a simplest scenario
where Alice creates some token, sends it to Bob, who sends it to Claire. So
now Claire needs to get both a proof that Alice sent it to Bob and that Bob
sent it to Claire, right? So Claire needs to verify 2 proofs, and for a
chain of N transfers one would need to verify N proofs, right?

And how it works in general:

1. Alice creates a token. To do that she constructs an unique expression
which checks her signature and signs a message "This token has such and
such meaning and its ownership originally associated with seal <hash of the
expression>" with her PGP key.
2. To transfer this token to Bob, she asks Bob for his auth expression and
sends a seal oracle a message (Alice_expression (Bob_expression .
signature)) where signatures is constructed in such a way that it evaluates
as true. Oracle stores this in a map: Alice_expression -> (Bob_expression .
signatures)
3. Bob sends token to Claire in a same way: (Bob_expression
(Claire_expression . signature))
4. Now Claire asks if Alice_expression->(Bob_expression . _) and
Bob_expression->(Claire_expression . _) are in oracle's map. She might
trust the oracle to verify signatures, but oracle doesn't understand token
semantics. Thus she needs to check if these entries were added.
If I understand correctly, Alice_expression->(Bob_expression . _) record
can be communicated in just 3 * size_of_hash_digest bytes.

So this seems to have rather bad scalability even with trusted oracles, am
I missing something?

-------------------------------------
I believe libsecp256k1 just performs Elliptic Curve operations
required by Bitcoin. OpenSSL is used for all other crypto.

For instance the PRNG appears to be OpenSSL:
https://github.com/bitcoin/bitcoin/blob/master/src/random.h


On Mon, Jan 18, 2016 at 8:39 PM, Andrew C via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> In the release notes for 0.12, it says that we have moved from using OpenSSL
> to libsecp256k1 for signature validation. So what else is it being used for
> that we need to keep it as a dependency?
>
> Thanks,
> Andrew
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
On Tue, Aug 9, 2016 at 11:06 PM, Daniel Hoffman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I have updated the GitHub a lot (changed tones to be less chirpy, fixed
> some smalls) and made a couple of samples (see attachment for MP3 and FLAC
> of both tone tables, first 16 then 4). Is this good enough to warrant an
> official BIP number? I haven't built a decoder yet, but it seems like the
> encoder is working properly (looked at Audacity, seems like it is working),
> and some people on reddit want to "allow for decoding experiments"
> <https://www.reddit.com/r/btc/comments/4wsn7v/bip_proposal_addresses_over_audio_thoughts/d69m3st>
>
> What suggestions do you all have for it?
>


With DSP hat on, your decoder for noisy/distorted channels will be 99.9% of
the complexity and will completely control the design of the encoder.

It's not a proposal yet without a decoder, it's just an idea.  FSK modems
microphone-channel (terrible multipath) is quite challenging and several
other parties have tried to do bitcoin info over audio in the past without
success.

It's very interesting, but I think you do need to go through and get the
whole thing working to really gauge viability.

-------------------------------------
On Fri, Feb 05, 2016 at 03:51:08PM -0500, Gavin Andresen via bitcoin-dev wrote:
> Constructive feedback welcome; [...]
> Summary:
>   Increase block size limit to 2,000,000 bytes.
>   With accurate sigop counting, but existing sigop limit (20,000)
>   And a new, high limit on signature hashing

To me, it seems absurd to have a hardfork but not take the opportunity
to combine these limits into a single weighted sum.

I'd suggest:

   0.5*blocksize + 50*accurate_sigops + 0.001*sighash < 2,000,000

That provides worst case blocksize of 4MB, worst case sigops of 40,000
and worst case sighash bytes of 2GB. Given the separate limit on sighash
bytes and the improvements from libsecp256k1 I think 40k sigops should
be fine, but I'm happy to be corrected.

For a regular transaction, of say 380 bytes with 2 sigops and hashing
about 800 bytes, that uses up about 291 units of the limit, meaning
that if a block was full of transactions of that form, the limit would
be 6872 tx or 2.6MB per block (along with 13.7k sigops and ~5.5MB hashed
for signatures).  Those weightings could probably be improved by doing
some detailed analysis and measurements, but I think they're pretty
reasonable for round figures.

The main advantage is that it would prevent blocks being cheaply filled
up due to hitting one of the secondary limits but only paying for the
contribution to the primary limit (presumably block size), which avoids
denial of service spam attacks.

I think having the limit take UTXO increase (or decrease) into effect
would be helpful too; but I don't have a specific suggestion. If it's
just a matter of making the limit stronger (eg adding "0.25*max(0,change
in UTXO bytes)" to the formula on the left, but not changing the limit on
the right), that would be a soft-forking change that could be introduced
later, and maybe that's fine.

If there was time to actually iterate on this proposal, rather than an
apparent aim to get it out the door in the next month or two, I think it
would be good to also design it so that the parameters of the weighted
sum could be adjusted by a soft-fork in future rather than requiring a
hard fork every time a limit's reached, or a weighting can be relaxed.
But I don't think that's feasible to design within a few weeks, so I
think it's off the table given the activation goal.

Cheers,
aj


-------------------------------------

> But what are the alternatives?  Put an expensive processor and a decent
> amount of memory in every hardware wallet to support scrypt?  Use a
> million iterations and just wait 10 minutes after entering you
> passphrase?  Or compute the secret key on your online computer instead?

What the Digital Bitbox team does, is, PBKDF2 the user entered
passphrase on the computer with >20'000 iterations, then PBKDF2 again
onchip with the 2048 rounds.
If somebody manages to steal your backup (in that case a file/SDCard or
printed PDF), it would at least required > 22'048 iterations to derive
the key from a passphrase which I consider "stronger" then just using
2048 iterations.

> Also, how many iterations are secure?  A million?  Then just add two
> random lower-case letters to the end of your passphrase and you have a
> better protection with 2048 iterations.

I guess you shouldn't delegate KDF security to the user. But sure, this
could help as well. This is part of the UI.

On the other hand, forcing the user to select a long/more-secure
passphrase will very likely lead to many funds get lost behind
encryption because of lost/forgotten passphrases.

> If you want to be able to use
> your passphrase with cheap hardware and be protected against a high-end
> computer with multiple GPUs that is almost a mllion times faster, then
> you have to choose a good passphrase.  Or just make sure nobody steals
> your seed; it is not a brainwallet that is only protected by the
> passphrase after all.

Agree.
But IMO this fact should not be an excuse to reduce/use low iterations
during KDF (especially SHA2 based KDFs).

</jonas>


-------------------------------------
Replying to the "fee" part of BIP75 (which as already noted should go to
a different BIP number imho):

It makes to sense to let the payee define a fee *rate*. The payee
doesn't know anything about how the payer's wallet is structured. In
extreme cases, as a payer I would keep all my tiny UTXOs (which would be
unspendable in a economic way) for the one payee who is willing to pay a
high enough rate...

Rather, I propose an absolute amount that the payee is willing to cover
should be declared.

Also, in order to avoid disputes I suggest the amount should be deducted
from the BIP70 payment message amount already. A wallet which
understands BIP75fee would add these two up for *display* puposes only.
The wallet should continue to use the existing fee policies. If it can
send the amount as specified by BIP70 and the fee is below the BIP75fee
amount, it would not mention any fees to the user. If it exceeds, it
would display just the exceeding amount.




On 03/11/2016 11:43 PM, Justin Newton via bitcoin-dev wrote:
> I think we would be open to either leaving them in, or doing a separate
> BIP.  What do others think?  Id prefer to keep them together if the
> changes are non-controversial just to cut down on #of BIPs, but thats
> not a strong preference.
> 
> On Fri, Mar 11, 2016 at 3:54 AM, Andreas Schildbach via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>     I think it's a bad idea to pollute the original idea of this BIP with
>     other extensions. Other extensions should go to separate BIPs,
>     especially since methods to clarify the fee have nothing to do with
>     secure and authenticated bi-directional BIP70 communication.
> 
> 
>     On 03/10/2016 10:43 PM, James MacWhyte via bitcoin-dev wrote:
>     > Hi everyone,
>     >
>     > Our BIP (officially proposed on March 1) has tentatively been assigned
>     > number 75. Also, the title has been changed to "Out of Band Address
>     > Exchange using Payment Protocol Encryption" to be more accurate.
>     >
>     > We thought it would be good to take this opportunity to add some
>     > optional fields to the BIP70 paymentDetails message. The new
>     fields are:
>     > subtractable fee (give permission to the sender to use some of the
>     > requested amount towards the transaction fee), fee per kb (the minimum
>     > fee required to be accepted as zeroconf), and replace by fee
>     (whether or
>     > not a transaction with the RBF flag will be accepted with zeroconf). I
>     > know it doesn't make much sense for merchants to accept RBF with
>     > zeroconf, so that last one might be used more to explicitly refuse RBF
>     > transactions (and allow the automation of choosing a setting based on
>     > who you are transacting with).
>     >
>     > I see BIP75 as a general modernization of BIP70, so I think it
>     should be
>     > fine to include these extensions in the new BIP, even though these
>     > fields are not specific to the features we are proposing. Please
>     take a
>     > look at the relevant section and let me know if anyone has any
>     concerns:
>     >
>     https://github.com/techguy613/bips/blob/master/bip-0075.mediawiki#Extending_BIP70_PaymentDetails
>     >
>     > The BIP70 extensions page in our fork has also been updated.
>     >
>     > Thanks!
>     >
>     > James
>     >
>     >
>     > _______________________________________________
>     > bitcoin-dev mailing list
>     > bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>     >
> 
> 
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> 
> 
> -- 
> 
> Justin W. Newton
> Founder/CEO
> Netki, Inc.
> 
> justin@netki.com <mailto:justin@netki.com>
> +1.818.261.4248 <tel:+1.818.261.4248>
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 




-------------------------------------
I've proposed a revision to BIP-1 that removes the option to license
the work under the OPL:  https://github.com/bitcoin/bips/pull/446

The OPL contains troublesome terms where the licensor can elect to
prohibit print publication of the work as well as the creation of
modified versions without their approval.

"Distribution of substantively modified versions of this document is
prohibited without the explicit permission of the copyright holder."
"Distribution of the work or derivative of the work in any standard
(paper) book form is prohibited unless prior permission is obtained
from the copyright holder."

Additionally, even without these optional clauses the specific
construction of this licenses' attribution requirements are
restrictive enough that Debian does not consider it acceptable for
works included in their distribution
(https://lists.debian.org/debian-legal/2004/03/msg00226.html).

I can't find any discussion that indicates anyone involved with the
project was aware of these clauses at the time this text was added...
and I believe they are strongly incompatible with having a
transparent, public, collaborative process for the development of
standard for interoperablity. I certainly wasn't aware of it, and
would have argued against it if I was.

Moreover, the project that created this license has recommended people
use creative commons licenses instead since 2007.

The only BIPs that have availed themselves of this are BIP145 (which
is dual licensed under the permissive 2-clause BSD, which I wouldn't
object to adding as an option-- and which doesn't active the
objectionable clauses) and the recently assigned BIP134.


-------------------------------------
On Friday 23 Sep 2016 09:57:01 Luke Dashjr via bitcoin-dev wrote:
> This BIP describes a new opcode (OP_CHECKBLOCKATHEIGHT) for the Bitcoin
> scripting system to address reissuing bitcoin transactions when the coins
> they spend have been conflicted/double-spent.
> 
> https://github.com/luke-jr/bips/blob/bip-cbah/bip-cbah.mediawiki


Can you walk us through a real live usecase which this solves?  I read it and 
I think I understand it, but I can't see the attack every giving the attacker 
any benefit (or the attacked losing anything).


-------------------------------------
On 24.08.2016 16:18, Jonas Schnelli via bitcoin-dev wrote:
> 
> Another thing that I think could be a BIP misdesign:
> 
> BIP44 Gap Limits
> From the BIP:
> 
> ----------
>   "Address gap limit is currently set to 20. If the software hits 20
> unused addresses in a row, it expects there are no used addresses beyond
> this point and stops searching the address chain."
> ----------
> 
> * Does that mean, we do a transaction rescan back to the genesis block
> for every 20 addresses?

As I understand it, you can scan sequentially starting with the genesis
block (or with a block at around the time when BIP44 was written).  Then
if you find a new transaction, which requires to generate new addresses,
you generate them and scan further from that point on.  This way you can
scan in a single pass if the scanning process calls you back when it
finds a transaction and allows you to change the set of addresses on the
fly.

  Jochen



-------------------------------------
On Wed, Dec 14, 2016 at 10:55 AM, Johnson Lau <jl2012@xbt.hk> wrote:

> In a sum tree, however, since the nSigOp is implied, any redefinition
> requires either a hardfork or a new sum tree (and the original sum tree
> becomes a placebo for old nodes. So every softfork of this type creates a
> new tree)
>

That's a good point.


> The only way to fix this is to explicitly commit to the weight and nSigOp,
> and the committed value must be equal to or larger than the real value.
> Only in this way we could redefine it with softfork. However, that means
> each tx will have an overhead of 16 bytes (if two int64 are used)
>

The weight and sigop count could be transmitted as variable length
integers.  That would be around 2 bytes for the sigops and 3 bytes for the
weight, per transaction.

It would mean that the block format would have to include the raw
transaction, "extra"/tree information and witness data for each transaction.

On an unrelated note, the two costs could be combined into a unified cost.
For example, a sigop could have equal cost to 250 bytes.  This would make
it easier for miners to decide what to charge.

On the other hand, CPU cost and storage/network costs are not completely
interchangeable.

Is there anything that would need to be summed fees, raw tx size, weight
and sigops that the greater or equal rule wouldn't cover?

On 12 Dec 2016, at 00:40, Tier Nolan via bitcoin-dev <bitcoin-dev@lists.
linuxfoundation.org> wrote:


On Sat, Dec 10, 2016 at 9:41 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Saturday, December 10, 2016 9:29:09 PM Tier Nolan via bitcoin-dev wrote:
> > Any new merkle algorithm should use a sum tree for partial validation and
> > fraud proofs.
>
> PR welcome.
>

Fair enough.  It is pretty basic.

https://github.com/luke-jr/bips/pull/2

It sums up sigops, block size, block cost (that is "weight" right?) and
fees.
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
On Monday 09 May 2016 13:40:55 Peter Todd wrote:
> >> [It's a little disconcerting that you appear to be maintaining a fork
> >> and are unaware of this.]
> >
> >ehm...
> 
> Can you please explain why you moved the above part of gmaxwell's reply to
> here,

A personal attack had no place in the technical discussion, I moved it out.



Initially I asked him to please avoid personal attacks, but I thought better 
of it and edited my reply to just "ehm...".


The moderators failed to catch his aggressive tone while moderating my post 
(see archives) for being too aggressive.

I'm sure this message will also not be allowed through. I would not even blame 
the moderators since this, and Peters, messages were both off-topic.

I thank you for todays talks, it makes me certain of the thing I said this 
weekend on Reddit that this list is not a suitable place for all the different 
stakeholders to talk on a level playing field.

If any of you agree, please urge the approach that we replace the entire 
moderation team with a new one. This will be the least painful solution for 
everyone in the ecosystem.

Thanks again.


-------------------------------------
On Wed, Aug 24, 2016 at 04:29:19PM +0000, Jimmy wrote:
> Is this unrelated to Bitcoin Vigil idea published in 2014?
> 
> http://www.coindesk.com/bitcoin-vigil-program-guards-against-intrusion-coin-theft/

I think it's very related; to be absolutely clear the idea of a Bitcoin
honeypot is 100% not my idea! Also, if anyone else had previously invented the
techniques I (and Jeff Coleman) invented, I'd love to hear about it so I can
give appropriate credit.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Fri, Feb 26, 2016 at 11:06 PM, Sergio Demian Lerner
<sergio.d.lerner@gmail.com> wrote:
> Congratulations!
>
> It a property of the SKCP system that the person who performed the trusted
> setup cannot extract any information from a proof?
>
> In other words, is it proven hard to obtain information from a proof by the
> buyer?

Yes, the secrecy is information theoretic (assuming no implementation
bugs); beyond the truth of the outcome. This holds even if the
initialization is malicious.

The soundness of this scheme is computational-- we're trusting a deep
stack of cryptographic assumptions that the proofs cannot be forged.


-------------------------------------
On Mon, May 9, 2016 at 9:35 AM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> You misunderstand the networking effects.
> The fact that your node is required to choose which one to set the announce
> bit on implies that it needs to predict which node will have the best data in
> the future.

Not required. It may. If it chooses fortunately, latency is reduced--
to 0.5 RTT in many cases. If not-- nothing harmful happens.

Testing on actual nodes in the actual network (not a "lab") shows that
blocks are normally requested from one of the last three peers they
were requested from 70% of the time, with no special affordances or
skipping samples when peers disconnected.

(77% for last 4, 88% for last 8)

This also _increases_ robustness. Right now a single peer failing at
the wrong time will delay blocks with a long time out. In high
bandwidth mode the redundancy means that node will be much more likely
to make progress without timeout delays-- so long at least one of the
the selected opportunistic mode peers was successful.

Because the decision is non-normative to the protocol, nodes can
decide based on better criteria if better criteria is discovered in
the future.

> Another problem with your solution is that nodes send a much larger amount of
> unsolicited data to peers in the form of the thin-block compared to the normal
> inv or header-first data.

"High bandwidth" mode uses somewhat more bandwidth than low
bandwidth... but still >>10 times less than an ordinary getdata relay
which is used ubiquitously today.

If a node is trying to minimize bandwidth usage, it can choose to not
request the "high bandwidth" mode.

The latency bound cannot be achieved without unsolicited data. The
best we can while achieving 0.5 RTT is try to arrange things so that
the information received is maximally useful and as small as
reasonably possible.

If receivers implemented joint decoding (combining multiple
comprblocks in the event of faild decoding) 4 byte IDs would be
completely reasonable, and were what I originally suggested (along
with forward error correction data, in that case).

> Am I to understand that you choose the solution based on the fact that service
> bits are too expensive to extend? (if not, please respond to my previous
> question actually answering the suggestion)
>
> That sounds like a rather bad way of doing design. Maybe you can add a second
> service bits field of message instead and then do the compact blocks correctly.

Service bits are not generally a good mechanism for negating optional
peer-local parameters.

The settings for compactblocks can change at runtime, having to
reconnect to change them would be obnoxious.

> Wait, you didn't steal the variable length encoding from an existing standard
> and you programmed a new one?

This is one of the two variable length encodings used for years in
Bitcoin Core. This is just the first time it's shown up in a BIP.

[It's a little disconcerting that you appear to be maintaining a fork
and are unaware of this.]

> Look at UTF-8 on wikipedia, you may have "invented" the same encoding that IBM
> published in 1992.

The similarity with UTF-8 is that both are variable length and some
control information is in the high bits. The similarity ends there.

UTF-8 is more complex and less efficient for this application (coding
small numbers), as it has to handle things like resynchronization
which are critical in text but irrelevant in our framed, checksummed,
reliably transported binary protocol.

> Just the first (highest) 8 bytes of a sha256 hash.
>
> The amount of collisions will not be less if you start xoring the rest.
> The whole reason for doing this extra work is also irrelevant as a spam
> protection.

Then you expose it to a trivial collision attack:  To find two 64 bit
hashes that collide I need perform only roughly 2^32 computation. Then
I can send them to the network.  You cannot reason about these systems
just by assuming that bad things happen only according to pure chance.

This issue is eliminated by salting the hash.  Moreover, with
per-source randomization of the hash, when a rare chance collision
happens it only impacts a single node at a time, so the propagation
doesn't stall network wide on an unlucky block; it just goes slower on
a tiny number of links a tiny percent of the time (instead of breaking
everywhere an even tinyer amount of the time)-- in the non-attacker,
chance event case.


-------------------------------------
I see.

But is it really necessary to soft fork over this issue?  Why not just make
it a relay rule?  Miners are already incentivized to modify transactions to
drop excess witness data and/or prioritize (versions of) transactions based
on their cost.  If a miner wants to mine a block with excess witness data,
it is mostly their own loss.

On Tue, Aug 16, 2016 at 6:39 PM, Pieter Wuille <pieter.wuille@gmail.com>
wrote:

> On Aug 17, 2016 00:36, "Russell O'Connor" <roconnor@blockstream.io> wrote:
>
> > Can I already do something similar with replace by fee, or are there
> limits on that?
>
> BIP125 and mempool eviction both require the replacing transaction to have
> higher fee, to compensate for the cost of relaying the replaced
> transaction(s).
>
> --
> Pieter
>

-------------------------------------
On Thu, Feb 4, 2016 at 11:56 AM, jl2012 via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> past the triggering block. A block-chain re-org of two thousand or
>> more blocks on the main Bitcoin chain is unthinkable-- the economic
>> chaos would be massive, and the reaction to such a drastic (and
>> extremely unlikely) event would certainly be a hastily imposed
>> checkpoint to get everybody back onto the chain that everybody was
>> using for economic transactions.
>>
>
> No, the "triggering block" you mentioned is NOT where the hardfork starts.
> Using BIP101 as an example, the hardfork starts when the first >1MB is
> mined. For people who failed to upgrade, the "grace period" is always zero,
> which is the moment they realize a hardfork.
>

Are there any plans written down anywhere about the "hastily imposed
checkpoint" scenario? As far as I know, we would have to check-point on
both blockchains because of the way that hard-forks work (creating two
separate chains and/or networks). Nothing about this should be an
"emergency", we have all the time in the world to prepare a safe and
responsible way to upgrade the network without unilaterally
declaring obsolescence.

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
Your idea of moving the Merkle root to the second chunk does not work.

The AsicBoost can change the version bits and it does not need to find a
collision.
(However *Spondoolies patent *only mentions Merkle collisions:
https://patentscope.wipo.int/search/docservicepdf_pct/id00000032873338/PAMPH/WO2016046820.pdf
)

Back in 2014 I designed a ASIC-compatible block header that prevents
AsicBoost in all its forms.

You can find it here:
https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/

Basically, the idea is to put in the first 64 bytes a 4 byte hash of the
second 64-byte chunk. That design also allows increased nonce space in the
first 64 bytes.

But it you want to do a simpler change, you can more easily use the first
32 bits of the Parent Block Hash (now currently zero) to store the first 4
bytes of the SHA256 of the last 16 bytes of the header. That way to "tie"
the two header chunks. It's a minimal change (but a hard-fork)

But some ASIC companies already have cores that are better (on power, cost,
rate, temperature, etc.) than competing companies ASICs. Why do you think a
10% improvement from AsicBoost is different from many of other improvements
they already have (secretly) added? Maybe we (?) should only allow ASICs
that have a 100% open source designs?

If we change the protocol then the message to the ecosystem is that ASIC
optimizations should be kept secret. It is fair to change the protocol
because we don't like that certain ASIC manufacturer has better chips, if
the chips are sold in the market and anyone can buy them? And what about
using approximate adders (30% improvement), or dual rail asynchronous
adders (also more than 10% improvement) ? How do we repair those?

Disclaimer: I have stake in AsicBoost, but I'm not sure about this.


On Tue, May 10, 2016 at 5:27 PM, Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> The various chunks in the double SHA256 are
>
> Chunk 1: 64 bytes
> version
> previous_block_digest
> merkle_root[31:4]
>
> Chunk 2: 64 bytes
> merkle_root[3:0]
> nonce
> timestamp
> target
>
> Chunk 3: 64 bytes
> digest from first sha pass
>
> Their improvement requires that all data in Chunk 2 is identical except
> for the nonce.  With 4 bytes, the birthday paradox means collisions can be
> found reasonable easily.
>
> If hard forks are allowed, then moving more of the merkle root into the
> 2nd chunk would make things harder.  The timestamp and target could be
> moved into chunk 1.  This increases the merkle root to 12 bytes in the 2nd
> chunk.  Finding collisions would be made much more difficult.
>
> If ASIC limitations mean that the nonce must stay where it is, this would
> mean that the merkle root would be split into two pieces.
>
> On Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> As part of the hard-fork proposed in the HK agreement(1) we'd like to
>> make the
>> patented AsicBoost optimisation useless, and hopefully make further
>> similar
>> optimizations useless as well.
>>
>> What's the best way to do this? Ideally this would be SPV compatible, but
>> if it
>> requires changes from SPV clients that's ok too. Also the fix this should
>> be
>> compatible with existing mining hardware.
>>
>>
>> 1)
>> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff
>>
>> 2)
>> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html
>>
>> --
>> https://petertodd.org 'peter'[:-1]@petertodd.org
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
This post serves to convince you of the economic benefits of smoothing
the payout of fees across blocks. It incentivizes decentralization and
supports the establishment of a fee market.

Idea: currently, the total amount of fees collected in a block is paid
out in full to whoever mined that block. I propose to only pay out,
say, 10% of the collected fees, and to add the remaining 90% to the
collected fees of the next block. Thus, the payout to the miner
constitutes a rolling average of collected fees from the current and
past blocks. This reduces the marginal benefit of including an
additional transaction into a block by an order of magnitude and thus
aligns the incentives of individual miners better with those of the
whole network. As a side-effect, the disadvantage of mining with a
slow connection is reduced.

Example: currently, given a transaction with a fee of 1000 Satoshis
and global processing cost per transaction of 5000 Satoshis, an
individual miner would still include the transaction if it costs him
500 Satoshis to do so, as the remaining burden of 4500 Satoshis is
carried by others (a classic externality). However, with fee
smoothing, the immediate benefit of including that particular
transaction is reduced to 100 Satoshis, aligning the economic
incentives of the miner better with the whole network and leading the
miner to skip it. Generally, the fraction that is paid out immediately
(here 10%) can be used to adjust the incentive, but not arbitrarily.

Benefits:
1. The disadvantage of mining with a slow connection is reduced by an
order of magnitude. If it takes 30 seconds to download the latest
block, a miner loses 5% of the potential income from fees as he does
not know yet which transactions to include in the next block. With fee
smoothing, that loss is reduced to 0.5% as he would still earn 90% of
the average fees per block by mining an empty one based on the latest
header.
2. This is a step towards a free fee market. In an ideal market,
prices form where supply and demand meet, with the fees asymptotically
approaching the marginal costs of a transaction. Currently, supply is
capped and only demand can adjust. Should we ever consider to let
miners decide about supply, it is essential that their marginal
benefit of including an additional transaction is aligned with the
global marginal cost incurred by that additional transaction. Fee
smoothing is a step in this direction.
3. The incentive to form mining pools is reduced. Currently,
solo-mining yields a very volatile income stream due to the random
nature of mining, leading to the formation of pools. This volatility
will increase to even higher levels once the amount of Bitcoins earned
per block is dominated by (volatile) collected fees and not by
(constant) freshly minted coins, thus increasing the economic pressure
to join a large pool. Fee smoothing reduces that volatility and
pressure.

Problems: touching anything related to fee distribution is a political
minefield. This proposal probably requires a hard fork. Its technical
feasibility was only superficially verified.

This is my first post to this list and I am looking forward to your
comments. In case this proposal is received well, I plan to
specify/implement the idea more formally in order to kick off the
usual process for improvements.

-- 
Luzius Meisser
President of Bitcoin Association Switzerland
MSc in Computer Science and MA in Economics


-------------------------------------

>> Good point.
>> I have mentioned this now in the BIP but I think the BIP should allow
>> message > 16 MiB.
>> I leave the max. message length up to the implementation while keeping
>> the 4 byte length on the protocol level.
> 
> I expect the implementation defined max size to work (SSH 2.0 does this
> after all), but I want to make sure my suggestion is understood
> completely.
> 
> There is a length field for the encrypted data, and length field(s)
> inside of the encrypted data to indicate the length of the plaintext
> Bitcoin messages. I am suggesting that the outter (encrypted) length
> field be reduced, which will _not limit_ the length of Bitcoin
> messages. For example, if a 1 GiB Bitcoin message needed to be sent
> and the encrypted length field was 3 bytes - the sender is forced to
> send a minimum of 64 MACs for this message. The tradeoff is allowing
> the receiver to detect malformed data sooner and have a lower max
> buffering window **against** slightly higher bandwidth and CPU
> requirements due to the additional headers+MACs (the CPU requirements
> should primarily be in "finalizing each Poly1305").

Okay. Got your point.
The current BIPs assumption is that an encrypted package/message can
contain 1..n bitcoin messages (a single bitcoin message distributed over
multiple encrypted messages/packages was not specified).

But right, this could make sense.
Let me think this through....

> An alternative way to think about the suggestion is tunnelling Bitcoin
> messages over TLS or SSH. TLS 1.2 has a 2-byte length field and SSH 2.0
> a 4-byte length field, but neither prevents larger Bitcoin messages from
> being tunnelled; the lengths are independent.

TLS/SSH tunneling is already possible with third party software like
stunnel.
Also there is promising projects that would encrypt the traffic "on a
deeper layer" (see CurveCP).

I think what we want is a simple, openssl-independent traffic encryption
built into the core p2p layer.

IMO the risk of screwing up the implementation is moderate.

The implementation is not utterly-complex:
OpenSSH chacha20:
https://github.com/openssh/openssh-portable/blob/0235a5fa67fcac51adb564cba69011a535f86f6b/chacha.c

Chacha20-Poly1305:
https://github.com/openssh/openssh-portable/blob/0235a5fa67fcac51adb564cba69011a535f86f6b/cipher-chachapoly.c

Sure. Before an implementation will be deployed to the endusers it will
require intense cryptoanalysis first.

</jonas>


-------------------------------------
Hello Daniel and others,

A recent version of my Coin Selection Simulator is now available on my
GitHub repository:

https://github.com/Xekyo/CoinSelectionSimulator

Please feel free to write an email or open an issue on GitHub, if you
happen to find errors, have questions about using the simulator, or
(especially!) have interesting results running the simulation on your
own data.

Note that a log and a csv-table with results are posted to the console
by the simulator, so you might want to pipe that somewhere. ;)
There are probably some inefficiency issues, and user experience
improvement opportunities left as I'm currently focusing on my thesis,
yet, I've come to the conclusion that some people might be interested in
taking a look nonetheless even though I haven't gotten around to
polishing the code repository up yet.

Regards
Murch

Am 23.09.2016 um 11:11 schrieb Murch via bitcoin-dev:
> Hi Daniel,
> 
> Thank you for your mail.
> My simulation of the Mycelium coin selection does add small change
> outputs to the fee, but I did get your boundary wrong.
> Instead of the 5460, I dropped at the dust boundary which calculates to
> 4440 in my simulation. Therefore, I think that the results in the table
> might be slightly too big, but likely indicative of the actual Mycelium
> behavior.
> I've corrected the boundary in my simulation now and will update my
> simulation results before Scaling Bitcoin. Thank you very much for your
> correction.
> 
> Sorry, the simulation code has not been published yet, I plan to do that
> around Scaling Bitcoin or after I turn in my thesis (End of October). I
> will let you know when I do.
> 
> It is my understanding that Mycelium doesn't create small change outputs
> but rather hardly ever spends them when received.
> 
> You're probably more familiar with the code base (I think you work for
> Mycelium?), so please correct me when I'm wrong:
> Mycelium appears to select UTXO in a FIFO approach, but, after the
> selection, prunes by removing the smallest selected UTXO until the
> excess beyond the spending target is minimized. This post-selection step
> seems the likely reason for Mycelium's small UTXO build-up. (Bitcoin
> Core intermittenly used post-selection pruning also, and apparently this
> did cause a similar increase in UTXO set size then.)
> 
> I assume that this will also cause Mycelium to create a huge transaction
> every once in a while when this build-up is enough to fund a transaction
> without a bigger UTXO being selected.
> 
> As to how it may be mitigated: BreadWallet uses a very similar FIFO
> approach, but doesn't prune. My simulation result indicates that their
> average UTXO set is much smaller. This has the downside that users could
> be spammed with small transaction outputs that they then would pay for
> spending.
> A balanced approach between these two approaches might be that instead
> of pruning all small inputs, a few of the small inputs could be allowed
> to be selected to slowly drain low-value UTXO out of the wallet by
> spending them over time. In order to avoid the privacy issues such as
> e.g. always spending the oldest UTXO, it would for example be possible
> to implement this as a 75% probability to prune an unnecessary output.
> 
> Regards
> Murch
> 
> Am 22.09.2016 um 11:33 schrieb Daniel Weigl via bitcoin-dev:
>> Hi,
>>
>> Is your simulation code available somewhere?
>>
>> I was just wondering why mycelium generates a very big UTXO set for <1000sat, because change outputs will never be smaller than 
>> 5460sat (=TransactionUtils.MINIMUM_OUTPUT_VALUE). If the change would be lower, it simply is skipped and added to the miner fee:
>> 	-> https://github.com/mycelium-com/wallet/blob/master/public/bitlib/src/main/java/com/mrd/bitlib/StandardTransactionBuilder.java#L334
>>
>> Does your simulation account for that?
>>
>> It might also be that the small UTXO came from external tx and we never spend them, bec. of pruning/privacy. Not sure how we could optimize that.
>>
>> Cheers,
>> Daniel
>>
>> On 2016-09-21 14:58, Murch via bitcoin-dev wrote:
>>> Hi,
>>>
>>> I'm currently compiling my Master's thesis about Coin Selection and my
>>> presentation proposal to Scaling Bitcoin has been accepted.
>>>
>>> For my thesis, I have analyzed the Coin Selection problem, created a
>>> framework to simulate wallet behavior on basis of a sequence of
>>> payments, and have re-implemented multiple coin selection strategies of
>>> prominent Bitcoin wallets (Bitcoin Core, Mycelium, Breadwallet, and
>>> Android Wallet for Bitcoin).
>>>
>>> As the Scaling Bitcoin site suggests that research should be made
>>> available to this mailing list, I would like to invite you to have a
>>> look at:
>>>
>>> http://murch.one/wp-content/uploads/2016/09/CoinSelection.pdf
>>>
>>> The PDF (176 kB) contains a two page description of my on-going work,
>>> including preliminary simulation results, and three figures showing the
>>> simulated wallets' UTXO compositions at the end of the simulation.
>>>
>>> I can provide further information as requested, and would welcome any
>>> feedback.
>>>
>>> →→ If anyone has another sequence of incoming and outgoing payment
>>> amounts at hand that I could run my simulation on, I'd love to hear
>>> about it.
>>>
>>> Regards
>>>
>>> Murch
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
On Tuesday, February 02, 2016 1:40:49 AM Cory Fields wrote:
> On Mon, Feb 1, 2016 at 6:08 PM, Luke Dashjr <luke@dashjr.org> wrote:
> > On Monday, February 01, 2016 9:43:33 PM Cory Fields wrote:
> >> On Mon, Feb 1, 2016 at 2:46 PM, Luke Dashjr <luke@dashjr.org> wrote:
> >> > Allowing for simpler cases both encourages the lazy case, and enables
> >> > pools to require miners use it. It also complicates the server-side
> >> > implementation somewhat, and could in some cases make it more
> >> > vulnerable to DoS attacks. Keep in mind that GBT is not merely a
> >> > bitcoind protocol, but is used between pool<->miner as well... For
> >> > now, it makes sense to leave
> >> > "default_witness_commitment" as a bitcoind-specific extension to
> >> > encourage adoption, but it seems better to leave it out of the
> >> > standard protocol. Let me know if this makes sense or if I'm
> >> > overlooking something.
> >> 
> >> I think that's a bit of a loaded answer. What's to keep a pool from
> >> building its own commitment and requiring miners to use that? I don't
> >> see how providing the known-working commitment for the
> >> passed-in-hashes allows the pool/miner to do anything they couldn't
> >> already, with the exception of skipping some complexity. Please don't
> >> confuse encouraging with enabling.
> > 
> > Making it simpler to do a centralised implementation than a decentralised
> > one, is both enabling and encouraging. GBT has always been designed to
> > make it difficult to do in a centralised manner.
> 
> But your suggestion is "use libblkmaker" which will build the trees
> for me. By that logic, isn't libblkmaker making a centralized
> implementation easier? Shouldn't that usage be discouraged as well?

libblkmaker is miner-side; right now it implies the miner using the templates 
as-is (perhaps after verifying the transactions meet some criteria), but it is 
the miner who is making that decision, not the pool.

> And along those lines, shouldn't the fact that it's used as a pool <->
> miner protocol be discouraged rather than touted as a feature?

???

> >> What's the DoS vector here?
> > 
> > It's more work for the pool to provide it, similar to the "midstate"
> > field was with getwork. Someone performing a DoS needs to do less work
> > to force the pool to do complex calculations (unless the same
> > transaction tree / commitment is used for all miners, which would be an
> > unfortunate limitation).
> 
> It's being provided to them. And if they're using a modified set of
> tx's, they'll need to re-calculate it in order to verify the result
> anyway. I suspect I'm not understanding this argument.

The DoS is against the pool, not the miner. You'd attack by pretending to be 
100000 new miners per second, and the pool then needs to calculate a witness 
commitment for each one. It's a lot cheaper to just serialise and send the 
transaction list.

> >> >> The issue in particular here is that a non-trivial burden is thrust
> >> >> upon mining software, increasing the odds of bugs in the process.
> >> > 
> >> > It can always use libblkmaker to handle the "heavy lifting"... In any
> >> > case, the calculation for the commitment isn't significantly more than
> >> > what it must already do for the stripped merkle tree.
> >> 
> >> Agreed. However for the sake of initial adoption, it's much easier to
> >> have a known-correct value to use. Even if it's just for the sake of
> >> checking against.
> > 
> > Sure, I'm not suggesting we remove this from bitcoind (probably the only
> > place that makes initial adoption easier).
> 
> How about exposing it as a feature/capability, then? That way pools
> can expect it from bitcoind, but won't be required to expose it
> downstream.

Implementation-specific things aren't standards. And besides, they really 
*shouldn't* expect it from bitcoind; it's simply a reasonable compromise to 
provide it encourage adoption of SegWit. Once SegWit is live, there is no more 
value to doing so.

Luke


-------------------------------------
On Thursday, March 10, 2016 2:02:15 PM Mustafa Al-Bassam wrote:
> On 10/03/16 00:53, Luke Dashjr wrote:
> > On Thursday, March 10, 2016 12:29:16 AM Mustafa Al-Bassam wrote:
> >>> A hard-fork BIP requires adoption from the entire Bitcoin economy,
> >>> particularly including those selling desirable goods and services in
> >>> exchange for bitcoin payments, as well as Bitcoin holders who wish to
> >>> spend or would spend their bitcoins (including selling for other
> >>> currencies) differently in the event of such a hard-fork.
> >> 
> >> What if one shop owner, for example, out of thousands, doesn't adapt the
> >> hard-fork? It is expected, and should perhaps be encouraged, for a small
> >> minority to not accept a hard fork, but by the wording of the BIP
> >> ("entire Bitcoin economy"), one shop owner can veto a hard-fork.
> > 
> > It's not the hard-fork they can veto (in this context, anyway), but the
> > progression of the BIP Status field. However, one shop cannot operate in
> > a vacuum: if they are indeed alone, they will soon find themselves no
> > longer selling in exchange for bitcoin payments, as nobody else would
> > exist willing to use the previous blockchain to pay them. If they are no
> > longer selling, they cease to meet the criteria here enabling their
> > veto.
> 
> I think in general this sounds like a good definition for a hard-fork
> becoming active. But I can envision a situation where someone will try
> to be annoying about it and point to one instance of one buyer and one
> seller using the blockchain to buy and sell from each other, or set one up.

In this scenario, it would seem the previous Bitcoin is alive any working, and 
that the hard-fork has failed. How to resolve such a split is outside the 
scope of the BIP process IMO.

Luke


-------------------------------------
How is the adverse scenario you describe different from a plain old 51%
attack? Each proposed protocol change  where 51% or more  of the network
can potentially game the rules and break the system should be considered
just as acceptable/unacceptable as another.

There comes a point where some form of basic honesty must be assumed on
behalf of participants benefiting from the system working properly and
reliably.

Afterall, what magic line of code prohibits all miners from simultaneously
turning all their equipment off...  just because?

Maybe this 'one':

"As long as a majority of CPU power is controlled by nodes that are not
cooperating to attack the network, they'll generate the longest chain and
outpace attackers. The network itself requires minimal structure."

Is there such a thing as an unrecognizable 51% attack?  One where the
remaining 49% get dragged in against their will?

Daniele

On Dec 10, 2016 6:39 PM, "Pieter Wuille" <pieter.wuille@gmail.com> wrote:

> On Sat, Dec 10, 2016 at 4:23 AM, Daniele Pinna via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> We have models for estimating the probability that a block is orphaned
>> given average network bandwidth and block size.
>>
>> The question is, do we have objective measures of these two quantities?
>> Couldn't we target an orphan_rate < max_rate?
>>
>
> Models can predict orphan rate given block size and network/hashrate
> topology, but you can't control the topology (and things like FIBRE hide
> the effect of block size on this as well). The result is that if you're
> purely optimizing for minimal orphan rate, you can end up with a single
> (conglomerate of) pools producing all the blocks. Such a setup has no
> propagation delay at all, and as a result can always achieve 0 orphans.
>
> Cheers,
>
> --
> Pieter
>
>

-------------------------------------

> I have just PRed a draft version of two BIPs I recently wrote.
> https://github.com/bitcoin/bips/pull/362

Hi.
I just updated the PR above with another overhaul of the BIP.
It's still under heavy review/work, nevertheless  at this point  any
feedback is highly welcome.

Changes since last update:
-> Removed AES256-GCM as cipher suite
-> Focusing on Chacha20-Poly1305 (implementation size ~300L)
-> Two symmetric cipher keys must be calculated by HMAC_SHA512 from the
ecdh secret
-> A session-ID (both directions) must be calculated (HMAC_SHA256) for
linking an identity authentication (ecdsa sig of the session-ID) with
the encryption
-> Re-Keying ('=hash(old_key)') can be announced by the responding peer
(after x minutes and/or after x GB, local peer policy but not shorter
then 10mins).
-> AEAD tag is now the last element in the new message format

It is very likely that the encrypted message format performs slightly
better than the current message format (removing the SHA256 checksum).

---
</jonas>


-------------------------------------
On Monday, January 18, 2016 12:14:16 PM Wladimir J. van der Laan wrote:
> It has been tested in git for almost half a year. This RC is the first
> binary release that contains the functionality.
> 
> It is extremely unlikely that the wallet will eat your coins (always backup
> nevertheless), but I can't guarantee there won't be some issue where the
> wallet and chain get out of sync and you're forced to redownload the
> blockchain.

I think I asked the wrong way, sorry: My question was not really meant at 
whether it is bug-free (testing that is the purpose of a release candidate, so 
we of course don't know yet), but rather whether it is at least feature 
complete now.

Remember, the previous v0.11.0 release notes said:
> Block pruning is currently incompatible with running a wallet due to the
> fact that block data is used for rescanning the wallet and importing keys
> or addresses (which require a rescan.) However, running the wallet with
> block pruning will be supported in the near future, subject to those
> limitations.

So I'm interested whether this limitation has been lifted, and the whole 
feature is considered as finished.

If yes, I would highly recommend advertising it in the new release notes - as 
said, the disk space reduction is a big deal.


Thank you!
-------------------------------------
On Tue, Mar 8, 2016 at 5:14 AM, Dave Scotese <dscotese@litmocracy.com> wrote:
> I think a BIP is a good idea, but rather than making such a specific
> proposal as "Let's use bit 4 to indicate communication of thin blocks," how
> about a more general one like "Let's use bit(s?) 4(-5?) as user-agent

Not communicated in address messages, so useless for discovery.

I think any feature which could do this could use the BIP130 approach instead.


-------------------------------------
Even if the Bitcoin Foundation decided to recklessly disregard Bitcoin's
future centralization, I'm not sure going to them and asking them to pay
a license fee in order to keep from holding the rest of the Bitcoin
mining community hostage counts as "regard for centralization pressure".
It also doesn't excuse the lack of transparent licensing being available
today, or the lack of transparency when discussing it in public after
the patent had been filed.

Matt

On 10/02/16 22:58, Gregory Maxwell via bitcoin-dev wrote:
> On Sun, Oct 2, 2016 at 10:51 PM, Matt Corallo via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> If you had acted in a way which indicated even the slightest regard for
>> centralization pressure and the harm it can do to Bitcoin in the
>> long-term, then I dont think many would be blaming you. Instead of any
> 
> Sergio was concerned about centralization pressure in private. He
> reached out to the BCF on 2013-11-23 and asked if they would license
> the patent from him so they could make it equally available to all
> under "fair" terms.  BCF responded that they didn't think it (a
> proprietary patent encumbered enhancement that would make its user(s)
> 30% more effective than others) would be a big deal and basically
> encouraged him to go ahead and seek the patent.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
I am pleased to report that as of December 31, 2015 we have been successfully running a segregated witness testnet, called segnet, and have already implemented rudimentary wallets with support.

For source code, please look at sipa's github repo:
https://github.com/sipa/bitcoin/tree/segwit

And some example signing code at my repo:
https://github.com/CodeShark/BitcoinScriptExperiments/blob/master/src/signwitnesstx.cpp

Several wallets have already committed to supporting it including mSIGNA, GreenAddress, GreenBits, Blocktrail, and NBitcoin. More wallets are expected to be added to this list soon. If you're a wallet dev and are interested in developing and testing on segnet please contact me.

We're right on schedule and are very excited about the fundamental improvements to bitcoin that segwit will enable.

---
Eric


-------------------------------------
Some comments and questions

1. In the BIP you mentioned scriptSig 3 times, but I don't think you are really talking about scriptSig. Especially, segwit has aborted the use of scriptSig to fix malleability. From the context I guess you mean redeemScript (see BIP141)

2. It seems that 51% of miners may steal all money from the peg, right? But I think this is unavoidable for all 2-way-peg proposals. To make it safer you still need notaries.

3. Instead of using a OP_NOPx, I suggest you using an unused code such as 0xba. OP_NOPx should be reserved for some simple "VERIFY"-type codes that does not write to the stack.

4. I don't think you should simply replace "(witversion == 0)" with "((witversion == 0) || (witversion == 1))". There are only 16 available versions. It'd be exhausted very soon if we use a version for every new opcode. As a testing prototype this is fine, but the actual softfork should not waste a witversion this way. We need a better way to coordinate the use of new witness version. BIP114 suggests an additional field in the witness to indicate the script version (https://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki)

5. It seems this is the first BIP in markdown format, not mediawiki (but this is allowed by BIP1)

6. The coinbase space is limited to 100 bytes and is already overloaded by many different purposes. I think any additional consensus critical message should go to a dummy scriptPubKey like the witness commitment. You may consider to  have a new OP_RETURN output like BIP141, with different magic bytes. However, please don't make this output mandatory (cf. witness commitment output is optional if the block does not have witness tx)

6a. "..........due to lack of space to include the proper ack tag in a block": this shouldn't happen if you use a OP_RETURN output

7.  "It can be the case that two different secondary blockchains specify the same transaction candidate, but **at least** one of them will clearly be unauthentic."

8. Question: is an ack-poll valid only for 1 transaction? When the transaction is confirmed, could full nodes prune the corresponding ack-poll data? (I think it has to be prunable after spending because ack-poll data is effectively UTXO data) 

9. No matter how you design a softfork, "Zero risk of invalidating a block" couldn't be true for any softfork. For example, even if a miner does not include any txs with OP_COUNT_ACKS, he may still build on top of blocks with invalid OP_COUNT_ACKS operations.

 ---- On Sun, 02 Oct 2016 23:49:08 +0800 Sergio Demian Lerner via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote ---- 
 > Since ScalingBitcoin is close, I think this is a good moment to publish our proposal on drivechains. This BIP proposed the drivechain we'd like to use in RSK (a.k.a. Rootstock) two-way pegged blockchain and see it implemented in Bitcoin. Until that happens, we're using a federated approach. 
 > I'm sure that adding risk-less Bitcoin extensibility through sidechains/drivechains is what we all want, but it's of maximum importance to decide which technology will leads us there.
 > We hope this work can also be the base of all other new 2-way-pegged blockchains that can take Bitcoin the currency to new niches and test new use cases, but cannot yet be realized because of current limitations/protections.
 > 
 > The full BIP plus a reference implementation can be found here:
 > 
 > BIP (draft):
 > https://github.com/rootstock/bips/blob/master/BIP-R10.md
 > 
 > Code & Test cases:
 > https://github.com/rootstock/bitcoin/tree/op-count-acks_devel
 > (Note: Code is still unaudited)
 > 
 > As a summary, OP_COUNT_ACKS is a new segwit-based and soft-forked opcode that counts acks and nacks tags in coinbase fields, and push the resulting totals in the script stack.
 > 
 > The system was designed with the following properties in mind:
 > 
 > 1. Interoperability with scripting system 
 > 2. Zero risk of invalidating a block
 > 3. No additional computation during blockchain management and re-organization
 > 4. No change in Bitcoin security model
 > 5. Bounded computation of poll results
 > 6. Strong protection from DoS attacks
 > 7. Minimum block space consumption
 > 8. Zero risk of cross-secondary chain invalidation
 > 
 > Please see the BIP draft for a more-detailed explanation on how we achieve these goals.
 > 
 > I'll be in ScalingBitcoin in less than a week and I'll be available to discuss the design rationale, improvements, changes and ideas any of you may have.
 > 
 > Truly yours, 
 > Sergio Demian Lerner
 > Bitcoiner and RSK co-founder
 >  
 >  _______________________________________________ 
 > bitcoin-dev mailing list 
 > bitcoin-dev@lists.linuxfoundation.org 
 > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev 
 > 




-------------------------------------
Andrew Johnson wrote:
> "You miss something obvious that makes this attack actually free of cost.
> Nothing will "cost them more in transaction fees". A miner can create
> thousands of transactions paying to himself, and not broadcast them to
> the network, but hold them and include them in the blocks he mines. The
> fees are collected by him because transactions are included in a block
> that he mined and the left amount is in another wallet of the same
> person. Repeat this continuously to fill blocks."
> 
> This is easily detectable as long as the network isn't heavily
> partitioned(which is an assumption we make today in order for
> transaction propagation to work reliably as well as for xThin and
> CompactBlocks to work effectively to reduce block transmission time). 
> Other miners would have an incentive to intentionally orphan blocks that
> contained a large number of transactions that their nodes were unaware of.
> 
> I don't think this sort of attack would last long.  Even later when
> subsidies are drastically reduced, you would still lose out on
> significant genuine fee revenue if your orphan rate increased even
> 10%(one out of ten of your poison blocks intentionally orphaned by
> another miner).
> 

I disagree.

I didn't say this is impossible to detect, but it is hard to act against
it. One miner orphaning the block intentionally is very unlikely if that
miner acts rationally. It would only make sense if 51% of the hash rate
would intentionally orphan it. Otherwise the miner who intentionally
orphans a valid block, let's say block X, has to continue to mine one in
its place on top of block X-1, and by the time he finds one:

a) his block X' is rejected by other miners because they already have a
valid block X on top of which they already started to mine;

b) block X+1 was already found and broadcasted, so the miner who
orphaned X intentionally is on the shorter chain ignored by the network.

So, one miner cannot do anything about it. Even a pool cannot do
anything about it, because the loss is greater. You need 51% of the hash
rate to intentionally orphan it, and all the miners forming 51% need to
be colluding and know for sure that every one will intentionally orphan
the said block, otherwise there's a huge risk of loss for who does it.
Nobody would gamble to do this (I am not sure if gambling is the right
word, since the loss is 100% sure here). But, we are not discussing 51%
attacks because those are a different topic.


-------------------------------------
Replies inline.

On 05/10/16 21:43, Sergio Demian Lerner via bitcoin-dev wrote:
-snip-

> But some ASIC companies already have cores that are better (on power,
> cost, rate, temperature, etc.) than competing companies ASICs. Why do
> you think a 10% improvement from AsicBoost is different from many of
> other improvements they already have (secretly) added? Maybe we (?)
> should only allow ASICs that have a 100% open source designs?

One is patented and requires paying a license fee to a group, or more
likely, ends up with it being impossible to import hardware from other
jurisdictions into the US/western world. The other requires more
investment in R&D, and over the long run, there is no guaranteed
advantage to such groups.

> If we change the protocol then the message to the ecosystem is that ASIC
> optimizations should be kept secret.

To some extent, this is the case, but there is a strong difference
between a guaranteed advantage enforced by the legal system and one that
is true due to intellectual superiority. In the long run, I am confident
the second will not remain the case. For example, AsicBoost was
independently discovered by at least two companies/individuals within a
year or two.

> It is fair to change the protocol
> because we don't like that certain ASIC manufacturer has better chips,
> if the chips are sold in the market and anyone can buy them? And what
> about using approximate adders (30% improvement), or dual rail
> asynchronous adders (also more than 10% improvement) ? How do we repair
> those?

As far as I'm aware neither of these are patented. Is this not the case?

> Disclaimer: I have stake in AsicBoost, but I'm not sure about this.
>  
> 
> On Tue, May 10, 2016 at 5:27 PM, Tier Nolan via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>     The various chunks in the double SHA256 are
> 
>     Chunk 1: 64 bytes
>     version
>     previous_block_digest
>     merkle_root[31:4]
> 
>     Chunk 2: 64 bytes
>     merkle_root[3:0]
>     nonce
>     timestamp
>     target
> 
>     Chunk 3: 64 bytes
>     digest from first sha pass
> 
>     Their improvement requires that all data in Chunk 2 is identical
>     except for the nonce.  With 4 bytes, the birthday paradox means
>     collisions can be found reasonable easily.
> 
>     If hard forks are allowed, then moving more of the merkle root into
>     the 2nd chunk would make things harder.  The timestamp and target
>     could be moved into chunk 1.  This increases the merkle root to 12
>     bytes in the 2nd chunk.  Finding collisions would be made much more
>     difficult.
> 
>     If ASIC limitations mean that the nonce must stay where it is, this
>     would mean that the merkle root would be split into two pieces.
> 
>     On Tue, May 10, 2016 at 7:57 PM, Peter Todd via bitcoin-dev
>     <bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>         As part of the hard-fork proposed in the HK agreement(1) we'd
>         like to make the
>         patented AsicBoost optimisation useless, and hopefully make
>         further similar
>         optimizations useless as well.
> 
>         What's the best way to do this? Ideally this would be SPV
>         compatible, but if it
>         requires changes from SPV clients that's ok too. Also the fix
>         this should be
>         compatible with existing mining hardware.
> 
> 
>         1)
>         https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff
> 
>         2)
>         http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html
> 
>         --
>         https://petertodd.org 'peter'[:-1]@petertodd.org
>         <http://petertodd.org>
> 
>         _______________________________________________
>         bitcoin-dev mailing list
>         bitcoin-dev@lists.linuxfoundation.org
>         <mailto:bitcoin-dev@lists.linuxfoundation.org>
>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> 
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
On Wed, Mar 9, 2016 at 6:11 PM, G. Andrew Stone via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Thanks for your offer Luke, but we are happy with our own process and,
> regardless of historical provenance, see this mailing list and the BIP
> process as very Core specific for reasons that are too numerous to describe
> here but should be obvious to anyone who has been aware of the last year of
> Bitcoin history.
>

One of the advantages with the BIP process is that it means that there are
hashlocked descriptions of the specs available for people to implement
against.

The BIP process is not the same as getting a PR accepted into core.  It is
not a veto based process.  If you write the BIP and it doesn't have any
serious technical problems, then it will be accepted into the BIP repo.

Getting it marked as "final" is harder but I don't think that matters
much.  I don't think that core would actually use a service bit that was
claimed in a BIP, even if the BIP wasn't final.  Maybe in 20 years if thin
blocks aren't being used, they might recycle it.  It would be pretty
obviously an aggressive act otherwise.

The NODE_GETUTXO bit is a perfect example of that.  They don't think it is
a good idea, but they still accepted the claim on the bit, because there
are nodes actually using it.

On the other hand, the BIP git repository is hosted on the /bitcoin github
site, so in that context it can be seen as linked with core.  I wouldn't be
surprised if that specific objection was raised when it was moved from the
wiki to github.  Luke may be willing to change that if you think that would
be worth changing?

With regards to the proposal, the description on the forum link isn't
sufficient for an alternative client to implement it.  I had a look at the
thread and I think that this is the implementation?

https://github.com/ptschip/bitcoinxt/commit/7ea5854a3599851beffb1323544173f03d45373b

Is the intention here to simply reserve the bit for thin blocks usage or to
define the specification for inter-operation with other clients?

Perhaps there could be a process for claiming service bits as it can be
useful to claim a bit in advance of actually finalizing the feature.

- Claim bit with a reasonable justification (good faith intent to implement
and the bit is useful for the feature)
- Within 3 months have a finalized description of the feature that lets
other clients implement it
- Within 6 months have working software that deploys the feature
- After 6 months of it actually being in active use, the bit is "locked"
and stays assigned to that feature

There could be an expiry process if it ends up not being used after all.
Requiring a public description of the feature seems like a reasonable
requirement in exchange for the community assigning the service bit, but we
don't want to go to far.  There is no point in having lots of free bits
that end up never being used.  Worst case, the addr message could be
updated to add more bits.

-------------------------------------
I'm not convinced you need to hold people's funds to provide those
features. Maybe the millisecond thing.   But 99 out of 100 traders would
accept a 100 millisecond latency in exchange for 0 counterparty risk.

-------------------------------------
On Fri, Nov 25, 2016 at 12:25 PM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Thursday, 24 November 2016 22:39:05 CET Sergio Demian Lerner via
> bitcoin-
> dev wrote:
> > Without a detailed analysis, unlimited block size seems a risky change to
> > Bitcoin, to me.
>
> What exactly do you think is a ‘change’ in bitcoin here?
>
> A change is anything that modifies with a HF the current state of the
Bitcoin Core implementation of the consensus protocol. Sadly (or happily,
for some) there is no "abstract" definition of Bitcoin.



> The concept of proof-of-work is that the longer a chain, the higher
> probability that that one will be extended for the simple reason that
> another chain will have to show a higher amount of proof of work to ‘win’.
>
> We know what Bitcoin the protocol dictates, but if what the protocol
dictates is not in the best interest of miners or full-nodes? then they
will simply choose a rule that maximizes their revenue (or any other
measure of performance, such as lower latency, or less transaction reversal
probability).


As far as I understand the document from Peter, there is no change there at
> all. Only chains with more POW will win.
>

I haven't gone to the code to check, but the video Peter sent does not say
that. It says that miners will mine on top of a block ONLY if the "gate"
has been opened for that block (e.g. there is additional blocks to push a
big block). So a miner having a preferring low block sizes will choose to
mine on top of the A1,A2,A3 chain (3 units of work), while miners
supporting bigger sizes will mine on top of the chain B1,S2,S3,S4 (4 units
of work).

Saying that the chain starting with B1 is not considered by a node X does
not mean that the node X is blind to the information that can be extracted
from the fact that there is a chain of 4 blocks starting from B1.
If there is more information, there may be a better local choice. If there
are better local choices, there is probably a better global equilibrium (or
not equilibrium at all).


> Or, to answer your example, miners will prefer to extend the chain with the
> most POW.
>

Clearly this is not universal: some miners will, and some other miners
won't, because some miners have postponed adding some blocks.



>
> The other fact stays the same as well, if you protect from reorgs by
> expecting more confirmations. Nothing changes here either. The
> common-sense 6
> confirmations for things like exchange-deposits keep having the same
> security.
>

Suppose that I provide a service that accepts payments with 2
confirmations, and in certain time I have the information that the network
is at the same time considering the forks B1 S2 and A1 A2. Then the best I
can do is NOT to accept the 2-confirmation and wait for a resolution of the
fork. Choosing either fork may put me at the risk of immediate reversal.

The existence of fork information changes equilibrium decision to choose
the longest-chain.  This is the same that happens with the GHOST protocol:
the information on the existence of uncles changes the local incentives to
choose the longest chain to some different strategy, and when all nodes
change their strategy, then the supposedly last equilibrium state is that
all follow the GHOST strategy for choosing the heaviest chain.


>
> The basic idea that we have a 3 or 4 deep fork is a huge problem in
> Bitcoin.
> It hasn’t happened for ages, and we like it that way. The miners like it
> that way too. Its disruptive.
> The is a problem that is not created by the ‘excessive block’ concept. It
> does, however, provide a possible solution to this very far-fetched
> problem.
>
> You should also realize that the policy of a miner is stored in the
> coinbase.
>
> This is important, but yet the full node does not use this information
automatically. The amount of confirmations that a node accepts is not
affected by the miner's policies or the size of the blocks mined, but it
should.


> That said, I’m sure there are improvements to be made to the policy that BU
> uses.


Probably a simple wise addition would be to estimate the accepted block
size for the majority of the miners (S), and only count block confirmations
for wallet transactions taking into account only blocks whose size is lower
or equal than S. So for example, if Alice receives a transaction T in block
B1 and it is confirmed by block B2, but size(B1)>S and size(B2)>S, then the
wallet should tell Alice that transaction T has 0 confirmations. This local
strategy reduces the chances that Alice accept T but is then easily
reversed for the opposite fork growing one block ahead.

Regards,
 Sergio

-------------------------------------
On Thu, Feb 04, 2016 at 12:36:06PM -0500, Gavin Andresen via bitcoin-dev wrote:
> This BIP is unnecessary, in my opinion.
> 
> I'm going to take issue with items (2) and (3) that are the motivation for
> this BIP:
> 
> " 2. Full nodes and SPV nodes following original consensus rules may not be
> aware of the deployment of a hardfork. They may stick to an
> economic-minority fork and unknowingly accept devalued legacy tokens."
> 
> If a hardfork is deployed by increasing the version number in blocks (as is
> done for soft forks), then there is no risk-- Full and SPV nodes should
> notice that they are seeing up-version blocks and warn the user that they
> are using obsolete software.

1) There is no way to guarantee that nodes will see those blocks, and
the current network behavior works against such guarantees even in the
non-adversarial case.

2) I know of no currently deployed SPV wallet software that warns users
about unknown block versions anyway.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698

-------------------------------------
On Saturday, 15 October 2016 17:02:30 CEST Marco Falke wrote:
> >> BIP 2 does not forbid you to release your work under PD in
> >> legislations where this is possible
> > 
> > It does, actually.
> 
> Huh, I can't find it in the text I read. The text mentions "not
> acceptable", but I don't read that as "forbidden".

You suggest that a person can dual license something under both CC-BY-SA as 
well as under public domain.
That means you don't understand copyright,

See, all licenses are based on you having copyright. In contrast; public 
domain is not a license, it means a certain text does not have copyright. 
Public domain is the lack of copyright.

One text can not at the same time have copyright and not have copyright, 
making your assumption impossible.

Hence, with PD not explicitly being allowed, you can't use PD.

Personally I prefer copyleft licenses, so the lack of PD is fine with me. The 
lack of a good copyleft we can use in BIPs is what got me involved in this 
discussion in the first place.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------

On 26/01/16 03:30, Toby Padilla via bitcoin-dev wrote:
> There are already valid use cases for OP_RETURN, it only makes sense
> to fully support the feature. The only reason it's not supported now
> is because the Payments protocol came before OP_RETURN.
>
You keep saying OP_RETURN is new, but it has been there from day one.
It's purpose is causing script execution to end if encountered.

Since then, we have tolerated putting pushdata's after it, and even
raised the limit for the size of this data. It still doesn't mean every
proposal has to be rewritten to cater for a new allowance we give
OP_RETURN.


> I've also been exploring this area with key.run
> (https://git.playgrub.com/toby/keyrun) and want the functionality for
> voting based on aggregate OP_RETURN value. *Not* to store data on the
> blockchain, but to associate content pointers with transactions.
>
> I think that since OP_RETURN has already been approved and supported
> it doesn't make much sense for me to have to re-defend it from scratch
> here.

I'd generally agree with Luke. Removing the cost of this hurts bitcoin,
and ironically, your application to a certain degree. Just because you
can do a thing one way, it doesn't mean you should. Especially if your
applications success depends on people spamming OP_RETURN hashes of
every torrent they like.


-------------------------------------
Not strictly speaking a wallet but we (BlockCypher) will also go down the
segwit path as soon as the BIP and branch are mature enough.  All
transactions built from our APIs should eventually be segwitted (just made
up a verb).

Thanks,
Matthieu
*CTO and Founder, Blockcypher*
I have been informed that Breadwallet has also committed to supporting
segwit.

The list now includes Blocktrail, Breadwallet, GreenAddress, GreenBits,
mSIGNA, and NBitcoin.

---
Eric

On January 7, 2016 5:28:18 AM PST, Eric Lombrozo <elombrozo@gmail.com>
wrote:
>
> I am pleased to report that as of December 31, 2015 we have been
> successfully running a segregated witness testnet, called segnet, and have
> already implemented rudimentary wallets with support.
>
> For source code, please look at sipa's github repo:
> https://github.com/sipa/bitcoin/tree/segwit
>
> And some example signing code at my repo:
>
> https://github.com/CodeShark/BitcoinScriptExperiments/blob/master/src/signwitnesstx.cpp
>
> Several wallets have already committed to supporting it including mSIGNA,
> GreenAddress, GreenBits, Blocktrail, and NBitcoin. More wallets are
> expected to be added to this list soon. If you're a wallet dev and are
> interested in developing and testing on segnet please contact me.
>
> We're right on schedule and are very excited about the fundamental
> improvements to bitcoin that segwit will enable.
>
> ---
> Eric
>
>
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
I think we are misunderstanding the effect of this change.
It's still "OK" for a 50k re-org to happen.
We're just saying that if it does, we will now have potentially introduced
a hard fork between new client and old clients if the reorg contains
earlier signaling for the most recent ISM soft fork and then blocks which
do not conform to that soft fork before the block height encoded activation.

I think the argument is this doesn't substantially add to the confusion or
usability of the system as its likely that old software won't even handle
50k block reorgs cleanly anyway and there will clearly have to be human
coordination at the time of the event.  In the unlikely event that the new
chain does cause such a hard fork, that coordination can result in everyone
upgrading to software that supports the new rules anyway.

So no, I don't think we should add a checkpoint.  I think we should all
just agree to a hard fork that only has a very very slim chance of any
practical effect.

In response to Thomas' email.  I think ideally we would treat these soft
forks the way we did BIP30 which is to say that we're just introducing a
further soft fork that applies to all blocks except for the historical
exceptions.  So then its a almost no-op soft fork with no risk of hard
fork.   This however isn't practical with at least BIP 34 without storing
the hashes of all 200K blocks that don't meet the requirement.



On Wed, Nov 16, 2016 at 9:18 AM, Tier Nolan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Wed, Nov 16, 2016 at 1:58 PM, Eric Voskuil via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Are checkpoints good now? Are hard forks okay now?
>>
>
> I think that at least one checkpoint should be included.  The assumption
> is that no 50k re-orgs will happen, and that assumption should be directly
> checked.
>
> Checkpointing only needs to happen during the headers-first part of the
> download.
>
> If the block at the BIP-65 height is checkpointed, then the comparisons
> for the other ones are automatically correct.  They are unnecessary, since
> the checkpoint protects all earlier block, but many people would like to be
> able to verify the legacy chain.
>
> This makes the change a soft-fork rather than a hard fork.  Chains that
> don't go through the checkpoint are rejected but no new chains are allowed.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Wednesday, March 02, 2016 2:56:14 PM Luke Dashjr via bitcoin-dev wrote:
> so it may even be possible to have such a proposal ready in time to be
> deployed alongside SegWit  to take effect in time for the upcoming subsidy
> halving.

Lapse of thinking/clarity here. This probably isn't a practical timeframe for 
deployment, unless/until there's an emergency situation. So if the code were 
bundled with SegWit, it would need some way to avoid its early activation 
outside of such an emergency (which could possibly be detected in code, in 
this case).

Luke


-------------------------------------
On Wednesday, March 02, 2016 3:05:08 PM Pavel Janík wrote:
> > the network. This would result in a significantly longer block interval,
> > which also means a higher per-block transaction volume, which could
> > cause the block size limit to legitimately be hit much sooner than
> > expected.
> 
> If this happens at all (the exchange rate of the coin can accomodate such
> expectation),

The exchange rate is not significantly influenced by these things. 
Historically, it seems fairly obvious that the difficulty has followed value, 
not value following difficulty.

> the local fee market will develop, fees will raise and complement mined
> coins, thus bringing more miners back to the game (together with expected
> higher exchange rate).

Depends on the hashrate drop, and tolerance for higher fees, both of which are 
largely unknown at this time. At least having code prepared for the negative 
scenarios in case of an emergency seems reasonable, even if we don't end up 
needing to deploy it.

Luke


-------------------------------------
On Sun, Feb 7, 2016 at 7:03 PM, Patrick Strateman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I would expect that custodians who fail to produce coins on both sides
> of a fork in response to depositor requests will find themselves in
> serious legal trouble.
>

If the exchange uses an UTXO from before the fork to pay their clients,
then they are guaranteed to count as paying on all forks.  The exchange
doesn't need to specifically pay out for each fork.

As long as the exchange doesn't accidently double spend an output, even
change addresses are valid.

It is handling post-fork deposits where the problem can occur.  If they
only receive coins on one fork, then that should cause the client to be
credited with funds on both forks.

The easiest thing would be to refuse to accept deposits for a while
before/after the fork happens.
<https://www.avast.com/sig-email> This email has been sent from a
virus-free computer protected by Avast.
www.avast.com <https://www.avast.com/sig-email>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

-------------------------------------
Hi there,
   For users who don’t wish a service provider to be able to see their
information, even ephemerally, and they would like to exchange information
via BIP75, they can use a software wallet, such as a breadwallet or others,
and that data will only exist on their phone, and the phone of their
counterparty (assuming the counterparty also chose to exchange info, and
was running on a software wallet).

In this way, we allow users to exchange data as they choose, without having
the risk that a service provider be asked for that data.

If a user chooses to use a hosted platform, and also to store their
identity data there, I do agree it could be subject to a subpoena, the same
as when they host their email, and other services.

Finally, they could choose not to use BIP75 at all, and no one would know
whether they did or didn’t (other than their counterparts) as we don’t
leave any residue on the blockchain, or anywhere else in the public eye.

We believe that this solution, due in part to its narrow data aperture, is
the best solution available to the problem we are solving.  We are eager to
engage in any discussions about how to improve the proposed solution, with
an eye to fungibility, privacy, and usability.

That said, there is a real need for people to know who they are transacting
with for usability reasons, for fraud reduction, and also of regulatory
reasons for some players.  To NOT solve it with a carefully crafted
standard means that it is more likely to be solved with back room, quick
and dirty solutions that are not available for community review and
feedback.

Thanks!

Justin






On Thu, Jun 23, 2016 at 2:31 PM, Police Terror via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> In England under RIPA 2000 legislation, it's irrelevant whether you have
> the data or not. If the authorities compel you to hand over that
> information, and it is within your means to obtain it then you are
> obliged to do so under threat of criminal offense.
>
> So any mechanism whereby data could be collected from Bitcoin users,
> whether it's stored ephemerally or not, if the police have reasonable
> suspicion to think it exists then they can compel all parties to work to
> get them the data they require.
>
> If the mechanism flat out does not exist, that is miles better than
> could exist. Deniability is not a defense when served with a police
> notice for disclosing data.
>
> You have to think not only about the end result, but also about how
> these mechanisms can be used for intimidating users or leveraging
> technologies.
>
> Justin Newton via bitcoin-dev:
> > On Thu, Jun 23, 2016 at 1:46 PM, s7r via bitcoin-dev <
> > bitcoin-dev@lists.linuxfoundation.org> wrote:
> >
> >>
> >>
> >>
> >> Any kind of built-in AML/KYC tools in Bitcoin is bad, and might draw
> >> expectations from _all_ users from authorities. Companies or individuals
> >> who want and/or need AML/KYC can find ways and do it at their side
> >> isolated from the entire network, and the solutions shouldn't come from
> >> upstream. AML/KYC/<insert other regulation here> differ from country to
> >> country and will be hard to implement in a global consensus network even
> >> if it would be worth it.
> >>
> >>
> > This was precisely our thinking as well.
> >
> > This is actually exactly why BIP 75 was designed the way that it was.
> Any
> > (voluntary) identity exchange is done at the application level, on an
> > encrypted https (or other) connection between the sender and receiver.
> > Identity data is not passed through or stored on the blockchain, and
> there
> > is actually no mark left on the blockchain that identity was even
> exchanged
> > on that transaction.
> >
> > The only people who know identity info was exchanged, or what the
> identity
> > was is the counterparties in the transaction, and depending on
> > implementation, their service provider.  (At a high level, many software
> > based wallet providers wouldn’t have any visibility into identity info,
> > where many hosted services would, for example)
> >
> > We did this to protect user privacy as well as fungibility.
> >
> > We are allowing the people who want or need to exchange identtity info
> > (either self signed or 3rd party validated) the option to exchange it,
> in a
> > standards based way, directly between peers, without touching the
> > blockchain or network itself.
> >
> > Is this more clear?
> >
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 

Justin W. Newton
Founder/CEO
Netki, Inc.

justin@netki.com
+1.818.261.4248

-------------------------------------
Unauthenticated link level encryption is wonderful! MITM attacks are overrated; as they require an active attacker.

Stopping passive attacks is the low hanging fruit. This should be taken first.

Automated and secure peer authentication in a mesh network is a huge topic. One of the unsolved problems in computer science.

A simple 'who is that' by asking for the fingerprint of your peers from your other peers is a very simple way to get 'some' authentication.  Semi-trusted index nodes also is a low hanging fruit for authentication.

However, let's first get unauthenticated encryption. Force the attackers to use active attacks. (That are thousands times more costly to couduct).

Sent from my iPhone

> On 29 Jun 2016, at 00:36, Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> On Tue, Jun 28, 2016 at 9:22 PM, Eric Voskuil via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> An "out of band key check" is not part of BIP151.
> 
> It has a session ID for this purpose.
> 
>> It requires a secure channel and is authentication. So BIP151 doesn't provide the tools to detect an attack, that requires authentication. A general requirement for authentication is the issue I have raised.
> 
> One might wonder how you ever use a Bitcoin address, or even why we
> might guess these emails from "you" aren't actually coming from the
> NSA.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
Discussion about reasoning of OP_RETURN aside, I think your
specification needs to be more precise/less ambiguous.

Here is what BIP70 currently says about PaymentDetails.outputs:

"one or more outputs where Bitcoins are to be sent. If the sum of
outputs.amount is zero, the customer will be asked how much to pay, and
the bitcoin client may choose any or all of the Outputs (if there are
more than one) for payment. If the sum of outputs.amount is non-zero,
then the customer will be asked to pay the sum, and the payment shall be
split among the Outputs with non-zero amounts (if there are more than
one; Outputs with zero amounts shall be ignored)."

As you can see, zero outputs are not ignored at all. They are used as an
indication to allow the user to set an amount. So if you'd come up with
one zero-amount OP_RETURN output, it would pop up an amount dialog.
Certainly not what you want, right?


On 01/26/2016 03:54 AM, Toby Padilla via bitcoin-dev wrote:
> It looks like my draft hasn't been approved by the mailing list so if
> anyone would like to read it it's also on Gist:
> 
> https://gist.github.com/toby/9e71811d387923a71a53
> 
> Luke - As stated in the Github thread, I totally understand where you're
> coming from but the fact is people *will* encode data on the blockchain
> using worse methods. For all of the reasons that OP_RETURN was a good
> idea in the first place, it's a good idea to support it in PaymentRequests.
> 
> As for keyless - there's no way (that I know of) to construct a
> transaction with a zero value OP_RETURN in an environment without keys
> since the Payment Protocol is what defines the method for getting a
> transaction from a server to a wallet. You can make a custom transaction
> and execute it in the same application but without Payments there's no
> way to move transactions between two applications. You need to build the
> transaction where you execute it and thus need a key.
> 
> 
> 
> On Mon, Jan 25, 2016 at 6:24 PM, Luke Dashjr <luke@dashjr.org
> <mailto:luke@dashjr.org>> wrote:
> 
>     This is a bad idea. OP_RETURN attachments are tolerated (not
>     encouraged!) for
>     the sake of the network, since the spam cannot be outright stopped.
>     If it
>     could be outright stopped, it would not be reasonable to allow
>     OP_RETURN. When
>     it comes to the payment protocol, however, changing the current
>     behaviour has
>     literally no benefit to the network at all, and the changes proposed
>     herein
>     are clearly detrimental since it would both encourage spam, and
>     potentially
>     make users unwilling (maybe even unaware) participants in it. For these
>     reasons, *I highly advise against publishing or implementing this
>     BIP, even if
>     the later mentioned issues are fixed.*
> 
>     On Tuesday, January 26, 2016 1:02:44 AM Toby Padilla wrote:
>     > An example might be a merchant that adds the hash of a plain text invoice
>     > to the checkout transaction. The merchant could construct the
>     > PaymentRequest with the invoice hash in an OP_RETURN and pass it to the
>     > customer's wallet. The wallet could then submit the transaction, including
>     > the invoice hash from the PaymentRequest. The wallet will have encoded a
>     > proof of purchase to the blockchain without the wallet developer having to
>     > coordinate with the merchant software or add features beyond this BIP.
> 
>     Such a "proof" is useless without wallet support. Even if you argue
>     it could
>     be implemented later on, it stands to reason that a scammer will
>     simply encode
>     garbage if the wallet is not checking the proof-of-purchase upfront.
>     To check
>     it, you would also need further protocol extensions which are not
>     included in
>     this draft.
> 
>     > Merchants and Bitcoin application developers benefit from this BIP because
>     > they can now construct transactions that include OP_RETURN data in a
>     > keyless environment. Again, prior to this BIP, transactions that used
>     > OP_RETURN (with zero value) needed to be constructed and executed in the
>     > same software. By separating the two concerns, this BIP allows merchant
>     > software to create transactions with OP_RETURN metadata on a server without
>     > storing public or private Bitcoin keys. This greatly enhances security
>     > where OP_RETURN applications currently need access to a private key to sign
>     > transactions.
> 
>     I don't see how this has any relevance to keys at all...
> 
>     > ## Specification
>     >
>     > The specification for this BIP is straightforward. BIP70 should be fully
>     > implemented with two changes:
>     >
>     > 1. Outputs where the script is an OP_RETURN and the value is zero should be
>     > accepted by the wallet.
>     > 2. Outputs where the script is an OP_RETURN and the value is greater than
>     > zero should be rejected.
>     >
>     > This is a change from the BIP70 requirement that all zero value outputs be
>     > ignored.
> 
>     This does not appear to be backward nor even forward compatible. Old
>     clients
>     will continue to use the previous behaviour and transparently omit any
>     commitments. New clients on the other hand will fail to include
>     commitments
>     produced by old servers. In other words, it is impossible to produce
>     software
>     compatible with both BIP 70 and this draft, and implementing either
>     would
>     result in severe consequences.
> 
>     > As it exists today, BIP70 allows for OP_RETURN data storage at the expense
>     > of permanently destroyed Bitcoin.
> 
>     It is better for the spammers to lose burned bitcoins, than have a
>     way to
>     avoid them.
> 
>     Luke
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 




-------------------------------------
On Wed, Nov 16, 2016 at 04:43:08PM -0800, Eric Voskuil via bitcoin-dev wrote:
> > This means that all future transactions will have different txids...
> rules do guarantee it.
> 
> No, it means that the chance is small, there is a difference.
> 
> If there is an address collision, someone may lose some money. If there
> is a tx hash collision, and implementations handle this differently, it
> will produce a chain split. As such this is not something that a node
> can just dismiss. If they do they are implementing a hard fork.

If there is a tx hash collision it is almost certainly going to be because
SHA256 has become weak through advances in cryptography, much like MD5. If that
is the case, Bitcoin is fundementally broken because the blockchain no longer
can be relied upon to commit to a unique transaction history: miners would be
able to generate blocks that have SHA256 collisions in transactions and even
the merkle tree itself, making it possible to simultaneously mine two (or more)
contradictory transaction histories at once.

Meanwhile the probability of SHA256 _not_ being broken and a collision being
found is low enough that we should be more worried about earth-killing
asteroids and mutant sharks, among other things.

Quoting Bruce Schneier:

    These numbers have nothing to do with the technology of the devices; they are
    the maximums that thermodynamics will allow. And they strongly imply that
    brute-force attacks against 256-bit keys will be infeasible until computers are
    built from something other than matter and occupy something other than space.

-https://www.schneier.com/blog/archives/2009/09/the_doghouse_cr.html

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
   - Flags will be mined selfishly, and not published until the advantage
   gained from withholding is less than the mining reward.  This effect may
   kill the decentralization features, since big miners will be the only ones
   that can selfish-mine flags.  Indeed, collusion would be encouraged... just
   ship the flag to the miners you do business with, and no one else.   At the
   expense of loss of flag revenue, your in-group would gain a massive
   advantage in main-chain mining.


On Tue, Jul 26, 2016 at 9:51 AM, Tom via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> > #Basic idea:
> >
> > Ideally, all miners would begin hashing the next block at exactly the
> same
> > time. Miners with a head start are more profitable, and the techniques
> that
> > help miners receive and validate blocks quickly create centralization
> > pressure.
> >
> > What if there was something that acted like the starting flag at a race,
> > which could suddenly wave and cause all of the miners to simultaneously
> > begin hashing the next block?
> >
> > #Implementation:
> >
> > Let a sync flag be a message consisting of:
> >
> > 1. Hash of the previous block.
> > 2. Bitcoin address
> > 3. Nonce
> >
> > This tiny message could propagate through the network at maximum speed.
> If
> > miners had to include the hash of this flag in the next block, then all
> > miners wait for this flag, and when it suddenly spread through the
> network,
> > all miners could simultaneously begin hashing the next block.
>
> What you describe in this part of your message can be done with no forks
> whatsoever and I think that this is enough. Don't really see the reason for
> any change in funding.
>
> The idea of sending out a block header is essentially what I called
> "optimistic mining" and has been described in more detail in my blog here;
> http://zander.github.io/posts/Innovation%20-%20OnlineScaling/
>
> The video explains with graphics too...
>
> You may find this interesting :)
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Thu, Jan 28, 2016 at 9:31 PM, Jannes Faber via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi,
>
> Question if you'll allow me. This is not about Gavin's latest hard fork
> proposal but in general about any hard (or soft) fork.
>
> I was surprised to see a period expressed in human time instead of in
> block time:
>
> > Blocks with timestamps greater than or equal to the triggering block's
> timestamp plus 28 days (60*60*24*28 seconds) shall have the new limits.
>
>
Block timestamps are in the 80-byte block header, so activation is
completely deterministic and can be determined from just the sequence of
block headers. There are no edge cases to worry about.

But even more so I would expect there to be significant differences in
> effects on non-updated clients depending on the moment (expressed as block
> number) of applying the new rules. I see a few options, all relating to the
> 2016 blocks recalibration window.
>

It doesn't matter much where in the difficulty period the fork happens; if
it happens in the middle, the lower-power fork's difficulty will adjust a
little quicker.

Example:  (check my math, I'm really good at screwing up at basic
arithmetic):

Fork at block%2016:  25% hashpower will take 8 weeks to produce 2016
blocks, difficulty drops by 4.

Fork one-week (halfway) into difficulty period:  25% hashpower will take 4
weeks to adjust, difficulty drops by 5/2 = 2.5
It will then take another 3.2 weeks to get to the next difficult adjustment
period and normal 10-minute blocks.

That's an unrealisitic scenario, though-- there will not be 25% of hash
power on a minority fork. I wrote about why in a blog post today:

http://gavinandresen.ninja/minority-branches

If you assume a more realistic single-digit-percentage of hash power on the
minority fork, then the numbers get silly (e.g. two or three months of an
hour or three between blocks before a difficulty adjustment).


-- 
--
Gavin Andresen

-------------------------------------
If Alice knows enough to see that she needs CHECKBLOCKATHEIGHT to avoid
paying Bob twice, then she also knows that Fred owes her 4BTC.  If Bob
complains about getting paid faster, Alice can let him know that Fred
essentially stole his coins and that when she is certain he (and she) can't
get them back, she will send a different four coins to Bob.  If she can
establish trust with Bob (She'd trust Bob to pay her back if he gets back
the coins Fred stole), then she can pay him again.  Bob could also make a
transaction to send the first input from Alice back to her (since he
doesn't have those coins anyway), sign it, and send that to her.  She can
then keep it instead of having to use the new opcode.

Or she can let her wallet use the new opcode so that the logic is built in,
if we add this opcode.  Wallet makers who want to help solve this problem
can either implement the new opcode, or they can offer people like Bob the
ability to refund orphaned transactions so that they can be duplicated in
the valid chain without any risk to the original sender.

With the opcode, Alice can solve the problem by herself.  Without it, Bob
can solve it for Alice.

While the opcode adds complexity, it enables victims of double-spends to
pay untrusted creditors (Bob) without the risk that orphaned chains create
of paying them twice.  I'm not sure the added complexity is worth the
reward. The reward is to protect Bitcoiners (Alice) from people we'd call
"untrusted creditors" (Bob) and I think that might be a mistake.  Getting a
refund transaction signed and sent back to Alice is similar to how the LN
will work (where wallets hold transactions that they don't broadcast).

Am I understanding this correctly?

On Fri, Sep 23, 2016 at 3:34 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Joe sends Alice 5 BTC (UTXO 0).
> Fred sends Alice 4 BTC (UTXO 1).
> Alice sends Bob 4 BTC using UTXO 1 (creating UTXO 2).
> Fred double-spends UTXO 1 with UTXO 1-B. This invalidates Alice's transfer
> to
> Bob.
> Alice has UTXO 0 which she can send to Bob (UTXO 3), but if she does so,
> it is
> possible that UTXO 0 could be mined, and then both UTXO 2 and UTXO 3 which
> would result in her giving Bob a total of 8 BTC rather than merely 4 BTC.
> Even if Alice waits until Fred's UTXO 1-B confirms 10 blocks deep, it is
> not
> impossible for a reorganization to reverse those 10 blocks and confirm
> UTXO 1
> again.
> Using OP_CHECKBLOCKATHEIGHT, however, Alice can create UTXO 3 such that it
> is
> valid only in the blockchain where Fred's UTXO 1-B has confirmed. This
> way, if
> that block is reorganized out, UTXO 3 is invalid, and either Bob receives
> only
> the original UTXO 2, or Alice can create a UTXO 3-B which is valid in the
> reorganized blockchain if it again confirms the UTXO 1-B double-spend.
>
> Luke
>
> On Friday, September 23, 2016 2:37:39 PM Tom via bitcoin-dev wrote:
> > On Friday 23 Sep 2016 09:57:01 Luke Dashjr via bitcoin-dev wrote:
> > > This BIP describes a new opcode (OP_CHECKBLOCKATHEIGHT) for the Bitcoin
> > > scripting system to address reissuing bitcoin transactions when the
> coins
> > > they spend have been conflicted/double-spent.
> > >
> > > https://github.com/luke-jr/bips/blob/bip-cbah/bip-cbah.mediawiki
> >
> > Can you walk us through a real live usecase which this solves?  I read it
> > and I think I understand it, but I can't see the attack every giving the
> > attacker any benefit (or the attacked losing anything).
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
On Wed, Mar 16, 2016 at 10:24 PM, Luke Dashjr <luke@dashjr.org> wrote:

> BIP Comments are not a part of the BIP itself, merely post-completion notes
> from various external parties. So having them external does not make the
> BIP
> any less self-contained. Right now, this information takes the form of
> reddit/forum comments, IRC chats, etc.
>

BIP2 does not state the comments section is where discussion happens for
the BIP, but for a sort of final summary.


> It is important that the forum for comments have a low barrier of use. The
> Bitcoin Wiki requires only a request for editing privileges, whereas GitHub
> wiki would require reading and agreeing to a lengthy Terms of Service
> contract.
>

Seems weak, it's much easier to sign up for a Github account and most have
one already. It's certainly easier than either paying to get edit
privileges on the Bitcoin Wiki find someone to convince you're genuine an
obscure IRC channel.


> In terms of staleness, the Wiki has been shown to stand the test of time,
> and
> is frankly less likely to move than the GitHub repository.
>
> The BIP process originated on the Wiki, and was only moved to GitHub
> because
> stronger moderation was needed (eg, to prevent random other people from
> editing someone else's BIP; number self-assignments; etc). Such moderation
> is
> not only unnecessary for BIP Comments, but would be an outright nuisance.
>

I'm not sure that is the reason why, but in any case, Github is a more
sensible place because of the collaborative features which is why they
became the centre of OSS software development for hundreds of thousands of
projects.


> I hope this addresses all your concerns and we can move forward with BIP 2
> unmodified?
>

I am sorry but it has not. I still strongly object to using the Bitcoin
Wiki or any external source source for the commentary part of BIP2. I
believe it should be done on using the Wiki feature at bitcoin/bips. If
that is not acceptable, then I would suggest a separate page in the bip
assets folder, called bip<nnnn>/comments.md. On a side note, more complex
reference implementation code should be stored in that folder too.


> (On another note, I wonder if we should recommend non-reference
> implementation
> lists/links be moved to BIP Comments rather than constantly revising the
> BIPs
> with them...)
>

Certainly those could be on the comments page.

-------------------------------------
Hello, folks.

I wanted to let all of you know a new IRC channel has been created called #segwit-dev where we welcome all discussion pertaining to integrating and supporting segregated witness transactions in wallets as well as comments or suggestions for improvement to the spec. Please come join us. :)


——
Eric

-------------------------------------
On Monday, June 20, 2016 5:33:32 PM Erik Aronesty via bitcoin-dev wrote:
> BIP 0070 has been a a moderate success, however, IMO:
> 
> - protocol buffers are inappropriate since ease of use and extensibility is
> desired over the minor gains of efficiency in this protocol.  Not too late
> to support JSON messages as the standard going forward

IMO JSON is too prone to gratuitous inefficiency (both at network and CPU 
level), parser bugs, etc. Even the best C implementation (jansson) has serious 
issues with Number handling.

A few years ago, I looked into binary alternatives to JSON and concluded they 
all had problems, while it seems more than reasonable to do even dynamic 
parsing of protobuf messages. So to conclude, I prefer to stick to protobuf 
unless a clearly superior protocol turns up.

> - problematic reliance on merchant-supplied https (X509) as the sole form
> of mechant identification.   alternate schemes (dnssec/netki), pgp and
> possibly keybase seem like good ideas.   personally, i like keybase, since
> there is no reliance on the existing domain-name system (you can sell with
> a github id, for example)

X509 is entrenched, so it should remain supported. PGP might make sense for 
people already using it (it provides no real security for un-WoT-networked 
users), but unforunately, few people use it. Correct me if I'm wrong, but IIRC 
Keybase uses blockchain spam, so definitely not something to be encouraged if 
so. Namecoin seems like a more than reasonable decentralised solution, but 
will probably take some real work to implement (not that this is avoidable for 
a general-usage decentralised solution).

> - missing an optional client supplied identification

What do you mean by this? There's the memo field at least.

> - lack of basic subscription support
> 
> *Proposed for subscriptions:*
> 
> - BIP0047 payment codes are recommended instead of wallet addresses when
> establishing subscriptions.  Or, merchants can specify replacement
> addresses in ACK/NACK responses.   UI confirms are *required *when there
> are no replacement addresses or payment codes used.

I'd discourage anything using BIP 47 due to its serious design flaws.
No reason a regular BIP 32 pub seed can't be used instead.

What do you mean by "replacement addresses" and "UI confirms" here?

> - Wallets must confirm and store subscriptions, and are responsible for
> initiating them at the specified interval.
> 
> - Intervals can *only *be from a preset list: weekly, biweekly, or 1,
> 2,3,4,6 or 12 months.   Intervals missed by more than 3 days cause
> suspension until the user re-verifies.

Disagree with hard-coding intervals, or mandating specific policies from the 
service providers.

> - Wallets *may *optionally ask the user whether they want to be notified
> and confirm every interval - or not.   Wallets that do not ask *must
> *notify before initiating each payment.   Interval confirmations should
> begin at *least *1 day in advance of the next payment.

This is wallet policy, but maybe makes sense as a "best practices" BIP.

> *Proposed in general:*
> - JSON should be used instead of protocol buffers going forward.  Easier to
> use, explain extend.
> 
> - "Extendible" URI-like scheme to support multi-mode identity mechanisms on
> both payment and subscription requests.   Support for keybase://, netki://
> and others as alternates to https://.
> 
> - Support for client as well as merchant multi-mode verification
> 
> - Ideally, the identity verification URI scheme is somewhat
> orthogonal/independent of the payment request itself
> 
> Question:
> 
> Should this be a new BIP?  I know netki's BIP75 is out there - but I think
> it's too specific and too reliant on the domain name system.
>
> Maybe an identity-protocol-agnostic BIP + solid implementation of a couple
> major protocols without any mention of payment URI's ... just a way of
> sending and receiving identity verified messages in general?
> 
> I would be happy to implement plugins for identity protocols, if anyone
> thinks this is a good idea.
> 
> Does anyone think https:// or keybase, or PGP or netki all by themselves,
> is enough - or is it always better to have an extensible protocol?
> 
> - Erik Aronesty


-------------------------------------
Hi,

I'm proposing the addition of a new optional p2p message to help reduce
unnecessary network traffic.  The draft BIP is available here and pasted
below:
https://gist.github.com/morcos/9aab223c443c9258c979

The goal of this message is to take advantage of the fact that when a node
has reached its mempool limit, there is a minimum fee below which no
transactions are accepted to the mempool.  Informing peers of this minimum
would save them inv'ing your node for those transaction id's and save your
node requesting them if they are not in your recentRejects filter.

This message is optional and may be ignored as a protocol rule.  There is
also an option to turn off sending the messages in the implementation.

Thanks to Suhas Daftuar, Greg Maxwell, and others for helping develop the
idea.

-Alex

Draft BIP text:

<pre>
  BIP: <unassigned>
  Title: feefilter message
  Author: Alex Morcos <morcos@chaincode.com>
  Status: Draft
  Type: Standards Track
  Created: 2016-02-13
</pre>

==Abstract==

Add a new message, "feefilter", which serves to instruct peers not to send
"inv"'s to the node for transactions with fees below the specified fee rate.

==Motivation==

The concept of a limited mempool was introduced in Bitcoin Core 0.12 to
provide protection against attacks or spam transactions of low fees that
are not being mined. A reject filter was also introduced to help prevent
repeated requests for the same transaction that might have been recently
rejected for insufficient fee. These methods help keep resource utilization
on a node from getting out of control.

However, there are limitations to the effectiveness of these approaches.
The reject filter is reset after every block which means transactions that
are inv'ed over a longer time period will be rerequested and there is no
method to prevent requesting the transaction the first time.  Furthermore,
inv data is sent at least once either to or from each peer for every
transaction accepted to the mempool and there is no mechanism by which to
know that an inv sent to a given peer would not result in a getdata request
because it represents a transaction with too little fee.

After receiving a feefilter message, a node can know before sending an inv
that a given transaction's fee rate is below the minimum currently required
by a given peer, and therefore the node can skip relaying an inv for that
transaction to that peer.

==Specification==

# The feefilter message is defined as a message containing an int64_t where
pchCommand == "feefilter"
# Upon receipt of a "feefilter" message, the node will be permitted, but
not required, to filter transaction invs for transactions that fall below
the feerate provided in the feefilter message interpreted as satoshis per
kilobyte.
# The fee filter is additive with a bloom filter for transactions so if an
SPV client were to load a bloom filter and send a feefilter message,
transactions would only be relayed if they passed both filters.
# Inv's generated from a mempool message are also subject to a fee filter
if it exists.
# Feature discovery is enabled by checking protocol version >= 70013

==Considerations==
The propagation efficiency of transactions across the network should not be
adversely affected by this change. In general, transactions which are not
accepted to your mempool are not relayed and the funcionality implemented
with this message is meant only to filter those transactions.  There could
be a small number of edge cases where a node's mempool min fee is actually
less than the filter value a peer is aware of and transactions with fee
rates between these values will now be newly inhibited.

Feefilter messages are not sent to whitelisted peers if the
"-whitelistforcerelay" option is set. In that case, transactions are
intended to be relayed even if they are not accepted to the mempool.

There are privacy concerns with deanonymizing a node by the fact that it is
broadcasting identifying information about its mempool min fee. To help
ameliorate this concern, the implementaion quantizes the filter value
broadcast with a small amount of randomness, in addition, the messages are
broadcast to different peers at individually randomly distributed times.

If a node is using prioritisetransaction to accept transactions whose
actual fee rates might fall below the node's mempool min fee, it may want
to consider setting "-nofeefilter" to make sure it is exposed to all
possible txid's.

==Backward compatibility==

Older clients remain fully compatible and interoperable after this change.
The sending of feefilter messages can be disabled by unsetting the
"-feefilter" option.

==Implementation==

https://github.com/bitcoin/bitcoin/pull/7542

-------------------------------------
We are coming up on the subsidy halving this July, and there have been some 
concerns raised that a non-trivial number of miners could potentially drop off 
the network. This would result in a significantly longer block interval, which 
also means a higher per-block transaction volume, which could cause the block 
size limit to legitimately be hit much sooner than expected. Furthermore, due 
to difficulty adjustment being measured exclusively in blocks, the time until 
it adjusts to compensate would be prolonged.

For example, if 50% of miners dropped off the network, blocks would be every 
20 minutes on average and contain double the transactions they presently do. 
Even double would be approximately 850-900k, which potentially bumps up 
against the hard limit when empty blocks are taken into consideration. This 
situation would continue for a full month if no changes are made. If more 
miners drop off the network, most of this becomes linearly worse, but due to 
hitting the block size limit, the backlog would grow indefinitely until the 
adjustment occurs.

To alleviate this risk, it seems reasonable to propose a hardfork to the 
difficulty adjustment algorithm so it can adapt quicker to such a significant 
drop in mining rate. BtcDrak tells me he has well-tested code for this in his 
altcoin, which has seen some roller-coaster hashrates, so it may even be 
possible to have such a proposal ready in time to be deployed alongside SegWit 
to take effect in time for the upcoming subsidy halving. If this slips, I 
think it may be reasonable to push for at least code-readiness before July, 
and possibly roll it into any other hardfork proposed before or around that 
time.

I am unaware of any reason this would be controversial, so if anyone has a 
problem with such a change, please speak up sooner rather than later. Other 
ideas or concerns are of course welcome as well.

Thanks,

Luke


-------------------------------------
We absolutely should be worried about 80-bit collision resistance.
Collisions only take 2**80 work if the hash is theoretically perfect,
which is never the case, not to mention that collision resistance is
almost always the first thing to go for hash functions, and often starts
to get easier slowly long, long before anyone is truly worried about the
security of the hash function.

I would never assume RIPEMD160's collision resistance is 2**80, and
would definitely never wager a significant amount of money that this
remains true for, say, five years.

Matt

On 01/07/16 19:02, Gavin Andresen via bitcoin-dev wrote:
> I'm hoisting this from some private feedback I sent on the segregated
> witness BIP:
> 
> I said:
> 
> "I'd also use RIPEMD160(SHA256()) as the hash function and save the 12
> bytes-- a successful preimage attack against that ain't gonna happen
> before we're all dead. I'm probably being dense, but I just don't see
> how a collision attack is relevant here."
> 
> Pieter responded:
> 
> "The problem case is where someone in a contract setup shows you a
> script, which you accept as being a payment to yourself. An attacker
> could use a collision attack to construct scripts with identical hashes,
> only one of which does have the property you want, and steal coins.
> 
> So you really want collision security, and I don't think 80 bits is
> something we should encourage for that. Normal pubkey hashes don't have
> that problem, as they can't be constructed to pay to you."
> 
> ... but I'm unconvinced:
> 
> "But it is trivial for contract wallets to protect against collision
> attacks-- if you give me a script that is "gavin_pubkey CHECKSIG
> arbitrary_data OP_DROP" with "I promise I'm not trying to rip you off,
> just ignore that arbitrary data" a wallet can just refuse. Even more
> likely, a contract wallet won't even recognize that as a pay-to-gavin
> transaction.
> 
> I suppose it could be looking for some form of "gavin_pubkey
> somebody_else_pubkey CHECKMULTISIG ... with the attacker using
> somebody_else_pubkey to force the collision, but, again, trivial
> contract protocol tweaks ("send along a proof you have the private key
> corresponding to the public key" or "everybody pre-commits pubkeys
> they'll use at protocol start") would protect against that.
> 
> Adding an extra 12 bytes to every segwit to prevent an attack that takes
> 2^80 computation and 2^80 storage, is unlikely to be a problem in
> practice, and is trivial to protect against is the wrong tradeoff to make."
> 
> 20 bytes instead of 32 bytes is a savings of almost 40%, which is
> significant.
> 
> The general question I'd like to raise on this list is:
> 
> Should we be worried, today, about collision attacks against RIPEMD160
> (our 160-bit hash)?
> 
> Mounting a successful brute-force collision attack would require at
> least O(2^80) CPU, which is kinda-sorta feasible (Pieter pointed out
> that Bitcoin POW has computed more SHA256 hashes than that). But it also
> requires O(2^80) storage, which is utterly infeasible (there is
> something on the order of 2^35 bytes of storage in the entire world). 
> Even assuming doubling every single year (faster than Moore's Law),
> we're four decades away from an attacker with THE ENTIRE WORLD's storage
> capacity being able to mount a collision attack.
> 
> 
> References: 
> 
> https://en.wikipedia.org/wiki/Collision_attack
> 
> https://vsatglobalseriesblog.wordpress.com/2013/06/21/in-2013-the-amount-of-data-generated-worldwide-will-reach-four-zettabytes/
> 
> 
> -- 
> --
> Gavin Andresen
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
On Jun 29, 2016 07:05, "Ethan Heilman via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> >It's also not clear to me why the HMAC, vs just
SHA256(key|cipher-type|mesg).  But that's probably just my crypto
ignorance...
>
> SHA256(key|cipher-type|mesg) is an extremely insecure MAC because of
> the length extension property of SHA256.

This property does technically not apply here, as the output of the hash is
kept secret, and the possible messages are constants (which are presumably
chosen in such a way that one is never an extension of another).

However, this is a good example of why you can't generically use a hash
function in places where you want a MAC (aka "a hash with a shared
secret"). Furthermore, if you already have a hash function anyway, HMAC is
very easy construct on top of it.

-- 
Pieter

-------------------------------------
Because there was a discussion on reddit about this topic, I want to
clarify that Johnson Lau explained how a check in the code prevents this
attack.
So there is no real attack.

Also note that the subject of this thread has a question mark, which means
that I'm asking the community for clarification, not asserting the
existence of a vulnerability.

The segwit code is complex, and some key parts of the consensus code are
spread over the source files (such as state.CorruptionPossible() relation
to DoS banning, IsNull() check in witness program serialization, etc.).

Thanks again Johnson for your clarifications.


On Wed, Aug 24, 2016 at 10:49 PM, Johnson Lau <jl2012@xbt.hk> wrote:

> Adding witness data to a non-segwit script is invalid by consensus:
>
> https://github.com/bitcoin/bitcoin/blob/d612837814020ae832499d18e6ee5e
> b919a87907/src/script/interpreter.cpp#L1467
>
>
> This PR will detect such violation early and ban the peer:
>
> https://github.com/bitcoin/bitcoin/pull/8499
>
>
> Another approach is to run the scripts of all incoming transactions.
> That's not too bad as you have already fetched the utxos which is a major
> part of validation.
>

-------------------------------------
On Fri, Feb 26, 2016 at 1:07 AM, Joseph Poon via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I'm interested in input and in the level of receptiveness to this. If
> there is interest, I'll write up a draft BIP in the next couple days.

The design of segwit was carefully constructed to make it maximally
easy and safe to soft-fork in future script enhancements after its
deployment with the specific goal of avoiding indefinite delays in its
deployment from inevitable scope creep from additional things that are
"easy" to deploy as part of segwit.  I think to be successful we must
be absolutely ruthless about changes that go in there beyond the
absolute minimum needed for the safe deployment of segwit... so I
think this should probably be constructed as a new segwit script type,
and not a base feature.

The exact construction you're thinking of there isn't clear to me...
one thing that comes to mind is that I think it is imperative that we
do not deploy a without-inputs SIGHASH flag without also deploying at
least a fee-committing sighash-all. The reason for this is that if
hardware wallets are forced to continue transferring input
transactions to check fees or to use without-inputs, they may choose
the latter and leave the users needlessly exposed to replay attacks.

When you do write a BIP for this its imperative that the vulnerability
to replay is called out in bold blinking flaming text, along with the
necessary description of how to use it safely. The fact that without
input commitments transactions are replayable is highly surprising to
many developers... Personally, I'd even go so far as to name the flag
SIGHASH_REPLAY_VULNERABLE. :)


-------------------------------------
This is exactly what segwit does...

On Saturday, August 06, 2016 2:15:22 PM Chris Priest via bitcoin-dev wrote:
> Because the blocksize limit is denominated in bytes, miners choose
> transactions to add to a block based on fee/byte ratio. This mean that
> if you make a transaction with a lot of inputs, your transaction will
> be very big, an you'll have a to pay a lot in fees to get that
> transaction included in a block.
> 
> For a long time I have been of the belief that it is a flaw in bitcoin
> that you have to pay more to move coins that are sent to you via small
> value UTXOs, compared to coins sent to you through a single high
> values UTXO. There are many legitimate uses of bitcoin where you get
> the money is very small increments (such as microtransactions). This
> is the basis for my "Wildcard inputs" proposal now known as BIP131.
> This BIP was rejected because it requires a database index, which
> people thought would make bitcoin not scale, which I think is complete
> malarkey, but it is what it is. It has recently occurred to me a way
> to achieve the same effect without needing the database index.
> 
> If the blocksize limit was denominated in outputs, miners would choose
> transactions based on maximum fee per output. This would essentially
> make it free to include an input to a transaction.
> 
> If the blocksize limit were removed and replaced with a "block output
> limit", it would have multiple positive effects. First off, like I
> said earlier, it would incentivize microtransactions. Secondly it
> would serve to decrease the UTXO set. As I described in the text of
> BIP131, as blocks fill up and fees rise, there is a "minimum
> profitability to include an input to a transaction" which increases.
> At the time I wrote BIP131, it was something like 2 cents: Any UTXO
> worth less than 2 cents was not economical to add to a transaction,
> and therefore likely to never be spent (unless blocks get bigger and
> fee's drop). This contributes to the "UTXO bloat problem" which a lot
> of people talk about being a big problem.
> 
> If the blocksize limit is to be changed to a block output limit, the
> number the limit is set to should be roughly the amount of outputs
> that are found in 1MB blocks today. This way, the change should be
> considered non-controversial. I think its silly that some people think
> its a good thing to keep usage restricted, but again, it is what it
> is.
> 
> Blocks can be bigger than 1MB, but the extra data in the block will
> not result in more people using bitcoin, but rather existing users
> spending inputs to decrease the UTXO set.
> 
> It would also bring about data that can be used to determine how to
> scale bitcoin in the future. For instance, we have *no idea* how the
> network will handle blocks bigger than 1MB, simply because the network
> has never seen blocks bigger than 1MB. People have set up private
> networks for testing bigger blocks, but thats not quite the same as
> 1MB+ blocks on the actual live network. This change will allow us to
> see what actually happens when bigger blocks gets published.
> 
> Why is this change a bad idea?
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
I think this is already covered in the BIP text:-

"As of November 2016, the most recent of these changes (BIP 65,
enforced since December 2015) has nearly 50,000 blocks built on top of
it. The occurrence of such a reorg that would cause the activating
block to be disconnected would raise fundamental concerns about the
security assumptions of Bitcoin, a far bigger issue than any
non-backwards compatible change.

So while this proposal could theoretically result in a consensus
split, it is extremely unlikely, and in particular any such
circumstances would be sufficiently damaging to the Bitcoin network to
dwarf any concerns about the effects of this proposed change."


On Mon, Nov 14, 2016 at 6:47 PM, Eric Voskuil via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> NACK
>
> Horrible precedent (hardcoding rule changes based on the assumption that
> large forks indicate a catastrophic failure), extremely poor process
> (already shipped, now the discussion), and not even a material performance
> optimization (the checks are avoidable once activated until a sufficiently
> deep reorg deactivates them).
>
> e
>
> On Nov 14, 2016, at 10:17 AM, Suhas Daftuar via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> Hi,
>
> Recently Bitcoin Core merged a simplification to the consensus rules
> surrounding deployment of BIPs 34, 66, and 65
> (https://github.com/bitcoin/bitcoin/pull/8391), and though the change is a
> minor one, I thought it was worth documenting the rationale in a BIP for
> posterity.
>
> Here's the abstract:
>
> Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated via miner
> signaling in block version numbers. Now that the chain has long since passed
> the blocks at which those consensus rules have triggered, we can (as a
> simplification and optimization) replace the trigger mechanism by caching
> the block heights at which those consensus rules became enforced.
>
> The full draft can be found here:
>
> https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
Hi,

I have 2 problems with bitcoind that separately are not a problem but
together they make the platform unusable for many projects.

If I have accounts I need to make sure the account holders do not 
overcharge their account. To do this I can now use "createrawtransaction() 
+ fundrawtransaction() + signrawtransaction()" and then make sure the
transaction can be paid by an account.

But since you deprecated the accounts and there is no
sendrawtransactionfrom() method; I either have to build my own account
system (this is no picknick btw, since you need to track all incoming 
funds to all addresses and having an integrated account system in bitcoind 
is 100% necessary to do this effectively).

Or I might be able to go ahead and speculate that you will not be able to
untangle the account code and hack my bitcoind to have a sendfrom with a
fixed fee parameter that overrides the size multiplication and I just do
the math before I send hoping that the transactions go through (this is 
bad but better than having accounts overcharge because they send dust that 
induce high fees).

I understand the privacy problems with using accounts for off-chain
microstransactions but currently it's the best workable option.

I hope you understand that I'm not trolling here, I have been mining since
2011 on FPGAs and built bitcoinbankbook.com 2 years ago. When I descovered
that once transactions will require fees (back then they didn't) and that
your system is not able to handle fees with accounts, I stopped developing
everything related to bitcoin.

There are probably 100s if not 1000s of developers in the same situation.

You can't just deprecate accounts like that because nobody likes the code.
Without accounts bitcoind is only a person-to-person manual client.

To build many-to-many automatic "organisations" on top of bitcoind you 
need accounts and you need fees that are predictable.

Kind Regards,

/marc


-------------------------------------
On Thu, Feb 11, 2016 at 10:20 PM, Thomas Kerin <thomas.kerin@gmail.com>
wrote:

> I wonder if this is possible as a soft fork without using segwit?
Increasing the sigop count for a NOP would be a hard fork, but such a
change would be fine with a new segwit version. It might require specific
support in the altcoin, which might be troublesome..

It is a soft fork since it makes things that were previous allowed
disallowed.  If it decreased the sigop count, then you could create a block
that had to many sigops due to the old rules.

With this rule, it increases the count.  If the sigop count is valid under
the new rules, it is also valid under the old rules.

There is no need for specific support on the altcoin.  It allows the
Bitcoin network act as trusted 3rd party so that you can do channels safely
on the altcoin, even though the altcoin still suffers from malleability and
doesn't have OP_CHECKLOCKTIMEVERIFY.

With regards to seg-witness, Ideally, the opcode would work in both old and
new scripts by re-purposing OP_NOP3.
<https://www.avast.com/sig-email> This email has been sent from a
virus-free computer protected by Avast.
www.avast.com <https://www.avast.com/sig-email>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

-------------------------------------
On Thu, Jun 09, 2016 at 08:57:29AM +0200, Jonas Schnelli via bitcoin-dev wrote:
> > Are there any links to discussions on how authentication may be done?
> 
> I'm currently working on the Auth-BIP which is not worth reviewing it
> right now (I will post it to the mailing list once it has been reached a
> stable level where it can be discusses).
> 
> If you can't wait, here is the current work:
> https://github.com/jonasschnelli/bips/blob/35d7e382cdd6955ff42726c3d06c44e33f61ae52/bip-undef-0.mediawiki
> 
> Most recent MITM/auth discussion (there where plenty of discussions on
> IRC about this topic):
> https://botbot.me/freenode/bitcoin-core-dev/2016-04-04/?msg=63463826&page=3

Awesome, thanks for the link Jonas.

Alfie

-- 
Alfie John
https://www.alfie.wtf


-------------------------------------
You are suggesting that, since a node implements a denial of service policy that actually denies itself otherwise valid blocks, those blocks are conditionally invalid. And that, since the validity condition is based on order of arrival and therefore independently unverifiable, Bitcoin consensus is broken in the face of a hash collision.

I am aware of two other hash collision scenarios that cause Core to declare blocks invalid based on ordering. The block hash duplicate check (it's not fork-point relative) and signature verification caching. Like the "block banning" issue above, the latter is related to an internal optimization. I would categorize the former as a simple oversight that presumably goes way back.

What then is the consequence of validity that is unverifiable? You believe this means that Bitcoin consensus is broken. This is incorrect. First understand that it is not possible for consensus rules to invalidate blocks based on order of arrival. As such any *implementation* that invalidates blocks based on order of arrival is broken. It is an error to claim that these behaviors are part of consensus, despite being implemented in the satoshi node(s).

Validity must be verifiable independent of the state of other nodes. Consensus is a function of block history and time alone. Time is presumed to be universally consistent. To be a consensus rule all nodes must be able to independently reach the same validity conclusion, given the same set of blocks, independent of order. If this is not the case the behavior is not a consensus rule, it is simply a bug. 

Deviating from such bugs is not a break with consensus, since such non-rules cannot be part of consensus. One node implementation can behave deterministically while others are behaving non-deterministically, with the two nodes remaining consistent from a consensus standpoint (deterministic produces a subset of non-deterministic results). But, unlike arbitrary nodes, deterministic nodes will not cause disruption on the network.

You imply that these determinism bugs are necessary, that there is no fix. This is also incorrect.

The block banning hash collision bug is avoided by not using non-chain/clock state to determine validity. Doing otherwise is clearly a bug. The hash of a block is not the block itself, a logically-correct ban would be to compare the wire serialization of the block as opposed to the hash, or not maintain the feature at all.

The signature verification caching hash collision bug is the same problem, an optimization based on an invalid assumption. A full serialization comparison (true identity), or elimination of the feature resolves the  bug.

The block hash check collision bug is trivially resolved by checking at the fork point as opposed to the tip. This prevents arbitrary (and irrational) invalidity based on conflict with irrelevant blocks that may or may not exist above the fork point.

Libbitcoin is deterministic in all three cases (although the third issue is not made consistent until v3). I am not aware of any other non-determinism in Core, but I don't spend a lot of time there. There is no need to study other implementations to ensure determinism, as that can be verified independently.

Any situation in which a node cannot provide deterministic validation of unordered blocks constitutes a non-consensus bug, as the behavior is not consistently verifiable by others under any conditions. Fixing/preventing these bugs is responsible development behavior, and does not require forks or BIPs, since Bitcoin doesn't inherently contain any such bugs. They are the consequence of incorrect implementation, and in two of the three cases above have resulted from supposed optimizations. But any code that creates non-determinism in exchange for speed, etc. is not an optimization, it's a bug. A node must implement its optimizations in a manner that does not alter consensus.

The BIP30 regression hard fork is not a case of non-determinism. This will produce deterministic results (apart from the impact of unrelated bugs). However the results are both a clear break from previous (and documented) consensus but also produce a very undesirable outcome - destruction of all unspent outputs in the "replaced" transaction for starters. So this is a distinct category, not a determinism bug but a hard fork that produces undesired consequences.

The BIP30 regression hard fork actually enables the various pathological scenarios that you were describing, where no such issues existed in Bitcoin consensus previously. It is now possible to produce a block that mutates another arbitrarily deep block, and forces a reorg all the way back to the mutated block. This was done to save microseconds per block. Despite the improbability of hash collisions, I find this deplorable and the lack of public discussion on the decision concerning.

With respect to the original post, the point at issue is the introduction of another hard fork, with some odd behaviors, but without any justification apart from tidying up the small amount of necessary code. These issues are related in that they are both consensus forks that have been introduced as supposed optimizations, with no public discussion prior to release (or at least merging to master with the presumption of shipping in the latter case). Two of the three hash collision issues above are also related in that they are bugs introduced by a desire to optimize internals.

The engineering lesson here should be clear - watch out for developers bearing optimizations. A trade against correctness is not an optimization, it's a break. Satoshi was clearly a fan of the premature optimization. FindAndDelete is a howler. So this is a tradition in Bitcoin. My intent is not to sling mud but to improve the situation.

It is very possible to produce straightforward and deterministic code that abides consensus and materially outperforms Core, without any of the above optimization breaks, even avoiding the utxo set optimization. Even the tx (memory) and block (orphan) pools are complex store denormalizations implemented as optimizations. Optimizing before producing a clean conceptual model architecture and design is a software development anti-pattern (premature optimization). The proposed fork is a premature optimization. There are much more significant opportunities to better organize code (and improve performance). I cannot support the decision to advance it.

I was unaware Core had regressed BIP30. Given that the behavior is catastrophic and that it introduces the *only* hash-collision consensus misbehavior (unless we consider a deep reorg sans the otherwise necessary proof of work desirable behavior), I strongly recommend it be reverted, with a post-mortem BIP.

Finally I recommend people contemplate the difference between unlikely and impossible. The chance of random collision is very small, but not zero. Colliding hashes is extremely difficult, but not impossible. But Bitcoin does not rely on impossibility for correct behavior. It relies of difficulty. This is a subtle but important distinction that people are missing.

Difficulty is a knowable quantity - a function of computing power.  If hash operations remain difficult, Bitcoin is undeterred. Collisions will have no impact, even if they happen with unexpected frequency (which would still be vanishingly infrequent). If the difficulty of producing a collision is reduced to the point where people cannot rely on addresses (for example), then Bitcoin has a problem, as it has become a leaky ship (and then there's mining). But with the unnecessary problems described above, a single hash collision can be catastrophic. Unlike difficulty, which is known, nobody can know when a single collision will show up. Betting Bitcoin, and potentially the world's money, on the unknowable is poor reasoning, especially given that the cost of not doing so is so very low.

e

> On Nov 17, 2016, at 10:08 AM, Johnson Lau <jl2012@xbt.hk> wrote:
> 
> The fact that some implementations ban an invalid block hash and some do not, suggests that it’s not a pure p2p protocol issue. A pure p2p split should be unified by a bridge node. However, a bridge node is not helpful in this case. Banning an invalid block hash is an implicit “first seen” consensus rule.
> 
> jl2012
> 
>> On 18 Nov 2016, at 01:49, Eric Voskuil <eric@voskuil.org> wrote:
>> 
>> Actually both possibilities were specifically covered in my description. Sorry if it wasn't clear.
>> 
>> If you create a new valid block out of an old one it's has potential to cause a reorg. The blocks that previously built on the original are still able to do so but presumably cannot build forever on the *new* block as it has a different tx. But other new blocks can. There is no chain split due to a different interpretation of valid, there are simply two valid competing chains.
>> 
>> Note that this scenario requires not only block and tx validity with a tx hash collision, but also that the tx be valid within the block. Pretty far to reach to not even get a chain split, but it could produce a deep reorg with a very low chance of success. As I keep telling people, deep reorgs can happen, they are just unlikely, as is this scenario.
>> 
>> If you create a new invalid block it is discarded by everyone. That does not invalidate the hash of that block. Permanent blocking as you describe it would be a p2p protocol design choice, having nothing to do with consensus. Libbitcoin for example does not ban invalidated hashes at all. It just discards the block and drops the peer.
>> 
>> e
> 
> 


-------------------------------------
The following draft BIP proposes an update to the Payment Protocol.

Motivation:

The motivation for defining this extension to the BIP70 Payment Protocol is
to allow 2 parties to exchange payment information in a permissioned and
encrypted way such that wallet address communication can become a more
automated process. Additionally, this extension allows for the requestor of
a PaymentRequest to supply a certificate and signature in order to
facilitate identification for address release. This also allows
for automated creation of off blockchain transaction logs that are human
readable, containing who you transacted with, in addition to the
information that it contains today.

The motivation for this extension to BIP70 is threefold:

1. Ensure that the payment details can only be seen by the participants in
the transaction, and not by any third party.
2. Enhance the Payment Protocol to allow for store and forward servers in
order to allow, for example, mobile wallets to sign and serve
Payment Requests.
3. Allow a sender of funds the option of sharing their identity with the
receiver. This information could then be used to:

        * Make bitcoin logs more human readable
        * Give the user the ability to decide who to release payment
details to
        * Allow an entity such as a political campaign to ensure donors
match regulatory and legal requirements
        * Allow for an open standards based way for regulated financial
entities to meet regulatory requirements
        * Automate the active exchange of payment addresses, so static
addresses and BIP32 X-Pubs can be avoided to maintain privacy
and convenience

In short we wanted to make bitcoin more human, while at the same time
improving transaction privacy.

Full proposal here:

https://github.com/techguy613/bips/blob/master/bip-invoicerequest-extension.mediawiki

We look forward to your thoughts and feedback on this proposal!

Justin


-- 

Justin W. Newton
Founder/CEO
Netki, Inc.

justin@netki.com
+1.818.261.4248

-------------------------------------
There was some discussion on the bitcointalk forums about using CLTV for
cross chain transfers.

Many altcoins don't support CLTV, so transfers to those coins cannot be
made secure.

I created a protocol.  It uses on cut and choose to allow commitments to
publish private keys, but it is clunky and not entirely secure.

I created a BIP draft for an opcode which would allow outputs to be locked
unless a private key was published that matches a given public key.

https://github.com/TierNolan/bips/blob/cpkv/bip-cprkv.mediawiki
<https://www.avast.com/sig-email> This email has been sent from a
virus-free computer protected by Avast.
www.avast.com <https://www.avast.com/sig-email>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

-------------------------------------

On Thu, March 3, 2016 10:02 am, Peter Todd via bitcoin-dev wrote:
> On Wed, Mar 02, 2016 at 11:01:36AM -0800, Eric Voskuil via bitcoin-dev
> wrote:
>> > A 6 month investment with 3 months on the high subsidy and 3 months on
>> low subsidy would not be made…
>>
>>
>>
>> Yes, this is the essential point. All capital investments are made based
>> on expectations of future returns. To the extent that futures are
>> perfectly knowable, they can be perfectly factored in. This is why
>> inflation in Bitcoin is not a tax, it’s a cost. These step functions
>> are made continuous by their predictability, removing that
>> predictability will make them -- unpredictable.
>
> You know, I do agree with you.
>
> But see, this is one of the reasons why we keep reminding people that
> strictly speaking a hardfork *is* an altcoin, and the altcoin can change
> any rule currently in Bitcoin.
>
> It'd be perfectly reasonable to create an altcoin with a 22-million-coin
> limit and an inflation schedule that had smooth, rather than abrupt,
> drops. It'd also be reasonable to make that altcoin start with the same
> UTXO set as Bitcoin as a means of initial coin distribution.
>
> If miners choose to start mining that altcoin en-mass on the halving,
> all the more power to them. It's our choice whether or not we buy those
> coins. We may choose not to, but if 95% of the hashing power decides to
> go mine something different we have to accept that under our current
> chosen rules confirmations might take a long time.
>
>
> Of course, personally I agree with Gregory Maxwell: this is all fairly
> unlikely to happen, so the discussion is academic. But we'll see.
>

Bitcoin is a success.

The success has forced various hardfork discussions.

Hard forking is contentious. If a softfork cannot be achieved the
alternate to a hardfork is creating a new bitcoin. ex bitcoin 2.0

Similar to silver, gold, palladium, etc...

Bitcoins success partly stems from it's brand awareness. Any new
officially supported bitcoin will also benefit from this brand awareness.

If the market values the new improved bitcoin they will put their money
into it. This doesn't require any consensus.

Let the market decide which option has the most value. If everyone
switches to the new bitcoin then the old bitcoin miners will follow.





--
Patrick Shirkey
Boost Hardware Ltd


-------------------------------------
BIP30 actually was given similar treatment after a reasonable amount of
time had passed.
https://github.com/bitcoin/bitcoin/blob/master/src/main.cpp#L2392

You are also missing BIP50: 'March 2013 Chain For Post-Mortem', which
neither benefited nor improved bitcoin, but did document an event for
posterity.

This is not a hard fork. Removing ISM just means we've committed to
those soft-forks only locking into the chain we use now.

On 11/16/2016 01:58 PM, Eric Voskuil via bitcoin-dev wrote:
> This sort of statement represents one consequence of the
> aforementioned bad precedent.
>
> Are checkpoints good now? Are hard forks okay now?
>
> What is the maximum depth of a reorg allowed by this non-machine
> consensus?
>
> Shouldn't we just define a max depth so that all cruft deeper than
> that can just be discarded on a regular basis?
>
> Why are there activation heights defined by this hard fork if it's not
> possible to reorg back to them?
>
> The "BIP" is neither a Proposal (it's been decided, just documenting
> for posterity), nor an Improvement (there is no actual benefit, just
> some tidying up in the notoriously obtuse satoshi code base), nor
> Bitcoin (a hard fork defines an alt coin, so from Aug 4 forward it has
> been CoreCoin).
>
> e
>
> On Nov 16, 2016, at 5:29 AM, Jameson Lopp <jameson.lopp@gmail.com
> <mailto:jameson.lopp@gmail.com>> wrote:
>
>> Since "buried deployments" are specifically in reference to
>> historical consensus changes, I think the question is more one of
>> human consensus than machine consensus. Is there any disagreement
>> amongst Bitcoin users that BIP34 activated at block 227931, BIP65
>> activated at block 388381, and BIP66 activated at block 363725?
>> Somehow I doubt it.
>>
>> It seems to me that this change is merely cementing into place a few
>> attributes of the blockchain's history that are not in dispute.
>>
>> - Jameson
>>
>> On Tue, Nov 15, 2016 at 5:42 PM, Eric Voskuil via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org
>> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>>
>>     Actually this does nothing to provide justification for this
>>     consensus rule change. It is just an attempt to deflect criticism
>>     from the fact that it is such a change.
>>
>>     e
>>
>>     > On Nov 15, 2016, at 9:45 AM, Btc Drak <btcdrak@gmail.com
>>     <mailto:btcdrak@gmail.com>> wrote:
>>     >
>>     > I think this is already covered in the BIP text:-
>>     >
>>     > "As of November 2016, the most recent of these changes (BIP 65,
>>     > enforced since December 2015) has nearly 50,000 blocks built on
>>     top of
>>     > it. The occurrence of such a reorg that would cause the activating
>>     > block to be disconnected would raise fundamental concerns about the
>>     > security assumptions of Bitcoin, a far bigger issue than any
>>     > non-backwards compatible change.
>>     >
>>     > So while this proposal could theoretically result in a consensus
>>     > split, it is extremely unlikely, and in particular any such
>>     > circumstances would be sufficiently damaging to the Bitcoin
>>     network to
>>     > dwarf any concerns about the effects of this proposed change."
>>     >
>>     >
>>     > On Mon, Nov 14, 2016 at 6:47 PM, Eric Voskuil via bitcoin-dev
>>     > <bitcoin-dev@lists.linuxfoundation.org
>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>>     >> NACK
>>     >>
>>     >> Horrible precedent (hardcoding rule changes based on the
>>     assumption that
>>     >> large forks indicate a catastrophic failure), extremely poor
>>     process
>>     >> (already shipped, now the discussion), and not even a material
>>     performance
>>     >> optimization (the checks are avoidable once activated until a
>>     sufficiently
>>     >> deep reorg deactivates them).
>>     >>
>>     >> e
>>     >>
>>     >> On Nov 14, 2016, at 10:17 AM, Suhas Daftuar via bitcoin-dev
>>     >> <bitcoin-dev@lists.linuxfoundation.org
>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>>     >>
>>     >> Hi,
>>     >>
>>     >> Recently Bitcoin Core merged a simplification to the consensus
>>     rules
>>     >> surrounding deployment of BIPs 34, 66, and 65
>>     >> (https://github.com/bitcoin/bitcoin/pull/8391
>>     <https://github.com/bitcoin/bitcoin/pull/8391>), and though the
>>     change is a
>>     >> minor one, I thought it was worth documenting the rationale in
>>     a BIP for
>>     >> posterity.
>>     >>
>>     >> Here's the abstract:
>>     >>
>>     >> Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated
>>     via miner
>>     >> signaling in block version numbers. Now that the chain has
>>     long since passed
>>     >> the blocks at which those consensus rules have triggered, we
>>     can (as a
>>     >> simplification and optimization) replace the trigger mechanism
>>     by caching
>>     >> the block heights at which those consensus rules became enforced.
>>     >>
>>     >> The full draft can be found here:
>>     >>
>>     >>
>>     https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki
>>     <https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki>
>>     >>
>>     >> _______________________________________________
>>     >> bitcoin-dev mailing list
>>     >> bitcoin-dev@lists.linuxfoundation.org
>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>>     >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
>>     >>
>>     >>
>>     >> _______________________________________________
>>     >> bitcoin-dev mailing list
>>     >> bitcoin-dev@lists.linuxfoundation.org
>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>>     >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
>>     >>
>>     _______________________________________________
>>     bitcoin-dev mailing list
>>     bitcoin-dev@lists.linuxfoundation.org
>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
>>
>>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512



On 9 May 2016 07:32:59 GMT-04:00, Tom via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>On Monday 09 May 2016 10:43:02 Gregory Maxwell wrote:
>> Service bits are not generally a good mechanism for negating optional
>> peer-local parameters.
>
>Service bits are exactly the right solution to indicate additional p2p
>feature-support.
>
>
>> [It's a little disconcerting that you appear to be maintaining a fork
>> and are unaware of this.]
>
>ehm...

Can you please explain why you moved the above part of gmaxwell's reply to here, when previously it was right after:

>> > Wait, you didn't steal the variable length encoding from an
>existing
>> > standard and you programmed a new one?
>>
>> This is one of the two variable length encodings used for years in
>> Bitcoin Core. This is just the first time it's shown up in a BIP.

here?

Editing gmaxwells reply like that changes the tone of the message significantly.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJXMJNd
AAoJEGOZARBE6K+yz4MH/0fQNM8SQdT7a1zljOSJW17ZLs6cEwVXZc/fOtvrNnOa
CkzXqylPrdT+BWBhPOwDlrzRa/2w5JAJDHRFoR8ZEidasxNDuSfhT3PwulBxmBqs
qoXhg0ujzRv9736vKENzMI4y2HbfHmqOrlLSZrlk8zqBGmlp1fMqVjFriQN66dnV
6cYFVyMVz0x/e4mXw8FigSQxkDAJ6gnfSInecQuZLT7H4g2xomIs6kQbqULHAylS
sFaK4uXy7Vr/sgBbitEQPDHGwywRoA+7EhExb2XpvL6hdyQbL1G1i6SPxGkwKg7R
MAuBPku/FraGo+qfcaA8R7eYKmyP4qZfZly317Aoo6Q=
=NtSN
-----END PGP SIGNATURE-----



-------------------------------------
Hi Alfie,

Yes, this is exactly what I meant. The complexity of the proposed construction is comparable to that of Bitcoin itself. This is not itself prohibitive, but it is clearly worthy of consideration.

A question we should ask is whether decentralized anonymous credentials is applicable to the authentication problem posed by BIP151. I propose that it is not.

The core problem posed by BIP151 is a MITM attack. The implied solution (BIP151 + authentication) requires that a peer trusts that another is not an attacker. 

Authentication of an anonymous peer cannot achieve this objective, since the peer may be anyone and an attack on privacy can be undetectable. The identity of a peer must be known to the relying peer, either directly or transitively.

DAC is applicable in cases where identity is never required.  The prime example in the paper is that of first-come-first-served name registration. No identity is required in that scenario, just proof that a party in question is the original registrant. All participants are presumed to be "good".

I believe that a distributed anonymous system is fundamentally at odds with isolation of "good" vs. "bad" participants who comply with protocol rules (DoS considerations aside), and that any attempt to resolve this conflict will result in the system no longer allowing anonymous participation.

I may be mistaken, but I haven't found a way out of this realization.

e

> On Jun 29, 2016, at 1:17 PM, Alfie John <alfie@alfie.wtf> wrote:
> 
> On Tue, Jun 28, 2016 at 06:45:58PM +0200, Eric Voskuil via bitcoin-dev wrote:
>>> then we should definitively use a form of end-to-end encryption between
>>> nodes. Built into the network layer.
>> 
>> Widespread application of this model is potentially problematic. It is a
>> non-trivial problem to design a distributed system that requires authentication
>> but without identity and without central control. In fact this may be more
>> challenging than Bitcoin itself. Trust on first use (TOFU) does not solve this
>> problem.
> 
> Maybe the following paper can feed into this discussion:
> 
> "Decentralized Anonymous Credentials" by Christina Garman, Matthew Green, Ian Miers
>   https://eprint.iacr.org/2013/622.pdf
> 
> Alfie
> 
> -- 
> Alfie John
> https://www.alfie.wtf


-------------------------------------
Hello Jonas,

thanks for your efforts of writing the draft for the standard.

First, this only describes detached signing.  A wallet also needs to
connect with a hardware wallet at some time to learn the xpubs
controlled by the hardware.  Do you plan to have this in a separate
standard or should this also be included here?  Basically one needs one
operation: get xpub for an HD path.

From a first read over the specification I found the following points
missing, that a fully checking hardware wallet needs to know:

- the amount spent by each input (necessary for segwit).
- the full serialized input transactions (without witness informations)
to prove that the amount really matches (this is not necessary for segwit)
- the position of the change output and its HD Path (to verify that it
really is a change output).
- For multisig change addresses, there are more extensive checks
necessary:  All inputs must be multisig addresses signed with public
keys derived from the same set of xpubs as the change address and use
the same "m of n" scheme.  So for multisig inputs and multisig change
address the standard should allow to give the parent xpubs of the other
public keys and their derivation paths.

It is also a bit ambiguous what the "inputscript" is especially for p2sh
transactions.  Is this always the scriptPubKey of the transaction output
that is spent by this input? For p2wsh nested in BIP16 p2sh transactions
there are three scripts

    witness:      0 <signature1> <1 <pubkey1> <pubkey2> 2 CHECKMULTISIG>
    scriptSig:    <0 <32-byte-hash>>
                  (0x220020{32-byte-hash})
    scriptPubKey: HASH160 <20-byte-hash> EQUAL
                  (0xA914{20-byte-hash}87)
 (quoted from BIP-141).

In principle one could put witness and scriptSig (with "OP_FALSE" in
places of the signatures) in the raw transaction and make inputscript
always the scriptPubKey of the corresponding output.  Then one also
doesn't need to distinguish between p2pkh or p2sh or p2wpkh or "p2wpkh
nested in bip16 p2sh" transactions.

Regards,
  Jochen



-------------------------------------
I'm convinced-- it is a good idea to worry about 80-bit collision attacks
now.

Thanks to all the people smarter than me who contributed to this
discussion, I learned a lot about collision attacks that I didn't know
before.

Would this be a reasonable "executive summary" :

If you are agreeing to lock up funds with somebody else, and they control
what public key to use, you are susceptible to collision attacks.

It is very likely an 80-bit-collision-in-ten-minutes attack will cost less
than $1million in 10 to twenty years (possibly sooner if there are crypto
breaks in that time).

If you don't trust the person with whom you're locking up funds and you're
locking up a significant amount of money (tens of millions of dollars
today, tens of thousands of dollars in a few years):

Then you should avoid using pay-to-script-hash addresses and instead use
the payment protocol and "raw" multisig outputs.

AND/OR

Have them give you a hierarchical deterministic (BIP32) seed, and derive a
public key for them to use.


----------

Following the security in depth and validate all input secure coding
principles would mean doing both-- avoid p2sh AND have all parties to a
transaction exchange HD seeds, add randomness, and use the resulting public
keys in the transaction.


-- 
--
Gavin Andresen

-------------------------------------
On Mon, May 9, 2016 at 11:32 AM, Tom <tomz@freedommail.ch> wrote:
> On Monday 09 May 2016 10:43:02 Gregory Maxwell wrote:
>> On Mon, May 9, 2016 at 9:35 AM, Tom Zander via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > You misunderstand the networking effects.
>> > The fact that your node is required to choose which one to set the
>> > announce
>> > bit on implies that it needs to predict which node will have the best data
>> > in the future.
>>
>> Not required. It may.
>
> It is required, in the reference of wanting to actually use compact block
> relay.

I cannot parse this sentence.

A node implementing this does not have to ask peers to send blocks
without further solicitation.

If they don't, their minimum transfer time increases to the current
1.5 RTT (but sending massively less data).

> Apologies, I thought that the term was wider known.  "Laboratory situations"
> is used where I am from as the opposite of real-world messy and unpredictable
> situations.
>
> So, your measurements may be true, but are not useful to decide how well it
> behaves under less optimal situations. aka "the real world".

My measurements were made in the real world, on a collection of nodes
around the network which were not setup for this purpose and are
running standard configurations, over many weeks of logs.

This doesn't guarantee that they're representative of everything-- but
they don't need to be.

>> This also _increases_ robustness. Right now a single peer failing at
>> the wrong time will delay blocks with a long time out.
>
> If your peers that were supposed to send you a compact block fail, then you'll
> end up in exactly that same situation again.  Only with various timeouts in
> between before you get your block making it a magnitude slower.

That is incorrect.

If a header shows up and a compact block has not shown up, a compact
block will be requested.

If compactblock shows up reconstruction will be attempted.

If any of the requested compact blocks show up (the three in advance,
if high bandwidth mode is used, or a requested one, if there was one)
then reconstruction proceeds without delay.

The addition of the unsolicited input causes no additional timeouts or
delays (ignoring bandwidth usage). It does use some more bandwidth
than not having it, but still massively less than the status quo.

>> > Another problem with your solution is that nodes send a much larger amount
>> > of unsolicited data to peers in the form of the thin-block compared to
>> > the normal inv or header-first data.
>>
>> "High bandwidth" mode
>
> Another place where I may have explained better.
> This is not about the difference about the two modes of your design.
> This is about the design as a whole. As compared to current.

It is massively more efficient than the current protocol, even under
fairly poor conditions. In the absolute worst possible case (miner
sends a block of completely unexpected transactions, and three peers
send compact blocks, it adds about 6% overhead)

> Service bits are exactly the right solution to indicate additional p2p
> feature-support.

With this kind of unsubstantiated axiomatic assertion, I don't think
further discussion with you is likely to be productive-- at least I
gave a reason.

> That's all fine and well, it doesn't at any point take away from my point that
> any specification should NOT invent something new that has for decades had a
> great specification already.

UTF-8 would be a poor fit here for the reasons I explained and others
less significant ones (including the additional error cases that must
be handled resulting from the inefficient encoding; -- poor handing of
invalid UTF-8 have even resulted in security issues in some
applications).

I am a bit baffled that you'd suggest using UTF-8 as a general compact
integer encoding in a binary protocol in the first place.

>> > Just the first (highest) 8 bytes of a sha256 hash.
>> >
>> > The amount of collisions will not be less if you start xoring the rest.
>> > The whole reason for doing this extra work is also irrelevant as a spam
>> > protection.
>>
>> Then you expose it to a trivial collision attack:  To find two 64 bit
>> hashes that collide I need perform only roughly 2^32 computation. Then
>> I can send them to the network.
>
> No, you still need to have done a POW.
>
> Next to that, your scheme is 2^32 computations *and* some XORs. The XORs are
> percentage wise a rounding error on the total time. So your argument also
> destroys your own addition.
>
>> This issue is eliminated by salting the hash.
>
> The issue is better eliminated by not allowing nodes to send uninvited large
> messages.

What are you talking about? You seem profoundly confused here. There
is no proof of work involved anywhere.

I obtain some txouts. I write a transaction spending them in malleable
form (e.g. sighash single and an op_return output).. then grind the
extra output to produce different hashes.  After doing this 2^32 times
I am likely to find two which share the same initial 8 bytes of txid.

I send one to half the nodes, the other to half the nodes.  When a
block shows up carrying one or the other of my transactions
reconstruction will fail on half the nodes in the network in a
protocol with a simple truncated hash.

Of course, doing this is easy, so I can keep it going persistently. If
I am a miner, I can be sure to filter these transactions from my own
blocks-- causing all my competition to suffer higher orphaning.

The salted short-ids do not have this easily exploited, and gratuitous
vulnerability. This was obvious enough that it this feature was in the
very earliest descriptions of these techniques in 2013/2014. The
salted short-ids cannot be collided in pre-computation, and cannot be
collided with respect to multiple nodes at once.


-------------------------------------
On Fri, Jan 8, 2016 at 10:50 AM, Gavin Andresen <gavinandresen@gmail.com>
wrote:

> But as I said earlier in the thread, there is a tradeoff here between
> crypto strength and code complexity, and "the strength of the crypto is all
> that matters" is NOT security first.


I should be more explicit about code complexity:

The big picture is "segwitness will help scale in the very short term."

So the spec gives two ways of stuffing the segwitness hash into the
scriptPubKey -- one way that uses a 32-bit hash, but if used would actually
make scalability a bit worse as coins moved into segwitness-locked
transactions (DUP HASH160 EQUALVERIFY pay-to-script-hash scriptpubkeys are
just 24 bytes).

And another way that add just one byte to the scriptpubkey.

THAT is the code complexity I'm talking about.  Better to always move the
script into the witness data, in my opinion, on the keep the design as
simple as possible principle.

It could be a 32-byte hash... but then the short-term scalability goal is
compromised.

Maybe I'm being dense, but I still think it is a no-brainer....

-- 
--
Gavin Andresen

-------------------------------------
On Monday, 21 November 2016 10:54:19 CET Russell O'Connor wrote:
> Hi Tom,
> 
> On Tue, Sep 20, 2016 at 1:15 PM, Tom via bitcoin-dev <bitcoin-dev@lists.
> 
> linuxfoundation.org> wrote:
> > The OP_CHECKSIG is the most well known and, as its name implies, it
> > validates a signature.
> > In the new version of 'script' (version 2) the data that is signed is
> > changed to be equivalent to the transaction-id. This is a massive
> > simplification and also the only change between version 1 and version 2
> > of script.
> 
> I'm a fan of simplicity too; Unfortunately, your proposal above to change
> the semantics of OP_CHECKSIG is too naive.

Thanks for your email, Russell.

Unfortunately you waited 6 weeks with writing this and the problem you are 
seeing has been fixed quite some time ago.

Thanks again for reviewing, though!
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Tue, May 10, 2016 at 3:57 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> As part of the hard-fork proposed in the HK agreement(1) we'd like to make
> the
> patented AsicBoost optimisation useless, and hopefully make further similar
> optimizations useless as well.
>
>
> You say that you want to make patented optimization useless, but you point
to a link that doesn't say anything about ASIC improvements or patents,
which means that you have been planning to change the protocol rules with
some miners (but not all the community).

All changes to the protocol should be discussed in public here. If you want
to make "further similar optimizations useless as well" then maybe you
should propose a switch to EquiHash.



>
> 1)
> https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff
>
> 2)
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Your description of the two scenarios reduces to one. They both require authentication, and if you intend to connect to potentially evil nodes you aren't securing anything with link level security except the knowledge that your potentially evil node connection remains so.

e

> On Jun 29, 2016, at 12:33 AM, Cameron Garnham <da2ce7@gmail.com> wrote:
> 
> 
> There are two different topics mixed up here.
> 
> 1. Link-level security (secure connection to the node we intended to connect to).
> 
> 2. Node-level security (aka; don't connect to a 'evil node').
> 
> The fist requires link-level encryption and authentication.
> 
> The second requires identity authentication.
> 
> You described the 'evil node' attack; that indeed needs an identity system to stop. However BIP151 doesn't intend to protect against connecting to evil Bitcoin Nodes.
> 
> It is important not to mixup link-level authentication and node-level authentication.
> 
> When your client picks random nodes to connect to, you are not considered whom in particular runs them. (Rather that you have a good random sample of the network).
> 
> If you manually add a friends node; at this point you wish to have node-level authentication.  However, this may (and probably should) happen out-of-band.
> 
> 
> Sent from my iPhone
> 
>> On 29 Jun 2016, at 01:07, Eric Voskuil <eric@voskuil.org> wrote:
>> 
>> Hi Cameron, good to hear from you!
>> 
>>> On Jun 28, 2016, at 11:40 PM, Cameron Garnham <da2ce7@gmail.com> wrote:
>>> 
>>> Unauthenticated link level encryption is wonderful! MITM attacks are overrated; as they require an active attacker.
>> 
>> This is not really the case with Bitcoin. A MITM attack does not require that the attacker find a way to inject traffic into the communication between nodes. Peers will connect to the attacker directly, or accept connections directly from it. Such attacks can be easier than even passive attacks.
>> 
>>> Stopping passive attacks is the low hanging fruit. This should be taken first.
>>> 
>>> Automated and secure peer authentication in a mesh network is a huge topic. One of the unsolved problems in computer science.
>>> 
>>> A simple 'who is that' by asking for the fingerprint of your peers from your other peers is a very simple way to get 'some' authentication.  Semi-trusted index nodes also is a low hanging fruit for authentication.
>> 
>> It is the implication of widespread authentication that is at issue. Clearly there are ways to implement it using a secure side channels.
>> 
>>> However, let's first get unauthenticated encryption. Force the attackers to use active attacks. (That are thousands times more costly to couduct).
>>> 
>>> Sent from my iPhone
>>> 
>>>> On 29 Jun 2016, at 00:36, Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>> 
>>>> On Tue, Jun 28, 2016 at 9:22 PM, Eric Voskuil via bitcoin-dev
>>>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>>> An "out of band key check" is not part of BIP151.
>>>> 
>>>> It has a session ID for this purpose.
>>>> 
>>>>> It requires a secure channel and is authentication. So BIP151 doesn't provide the tools to detect an attack, that requires authentication. A general requirement for authentication is the issue I have raised.
>>>> 
>>>> One might wonder how you ever use a Bitcoin address, or even why we
>>>> might guess these emails from "you" aren't actually coming from the
>>>> NSA.
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev@lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
Proposed release schedule for 0.13.0:

2015-05-01
-----------
- Open Transifex translations for 0.13
- Soft translation string freeze (no large or unnecessary string changes until release)
- Finalize and close translations for 0.11

2015-05-15
-----------
- Feature freeze (bug fixes only until release)
- Translation string freeze (no more source language changes until release)

2016-06-06
-----------
- Split off `0.13` branch from `master`
- Start RC cycle, tag and release `0.13.0rc1`
- Start merging for 0.14 on master branch

2016-07-01
-----------
- Release 0.13.0 final (aim)



-------------------------------------



 ---- On Mon, 17 Oct 2016 02:54:04 +0800 Tom Zander via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote ---- 

 > Honestly, if the reason for the too-short-for-safety timespan is that you  
 > want to use BIP9, then please take a step back and realize that SegWit is a  
 > contriversial soft-fork that needs to be deployed in a way that is extra  
 > safe because you can't roll the feature back a week after deployment. 
 > All transactions that were made in the mean time turn into everyone-can- 
 > spent transactions. 

No one should use, nor anyone is advised to use, segwit transactions before it is fully activated. Having 2 months or 2 weeks of grace period makes totally no difference in this regard. If anyone tried to use segwit tx during your proposed 2 months grace period, all those txs were still everyone-can-spent.

All you are advocating is just stalling the process with no improvement in security.

 >  
 > I stand by the minimum of 2 months. There is no reason to use BIP9 as it was  
 > coded in an older client. That is an excuse that I don't buy. 
 > --  
 > Tom Zander 
 > Blog: https://zander.github.io 
 > Vlog: https://vimeo.com/channels/tomscryptochannel 
 > _______________________________________________ 
 > bitcoin-dev mailing list 
 > bitcoin-dev@lists.linuxfoundation.org 
 > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev 
 > 




-------------------------------------
On Fri, Jun 17, 2016 at 08:22:04PM -0700, Bram Cohen wrote:
> On Wed, Jun 15, 2016 at 5:10 PM, Peter Todd <pete@petertodd.org> wrote:
> > Agreed - regardless of approach adding latency to commitment calculations
> > of
> > all kinds is something I think we all agree can work in principle, although
> > obviously it should be a last resort technique when optimization fails.
> >
> 
> An important point: Adding latency to utxo commitments does not imply
> latency to proofs of inclusion and exclusion! If roots of what's added and
> deleted in each block are added as well, then a proof of inclusion can be
> done by having a proof of inclusion of the trailing utxo set followed by a
> proof of exclusion from all the following deletion sets, or a proof of
> inclusion in one of the single block addition sets followed by proofs of
> exclusion from all the more recent deletion sets. Likewise a proof of
> exclusion can be a proof of exclusion from the utxo set followed by proofs
> of exclusion from all the more recent addition sets or a single proof of
> inclusion in a recent deletion set.
> 
> This does make proofs larger (except in the case of recent deletions and
> maybe recent additions) and adds complexity, so it shouldn't be done unless
> necessary.

So, to be clear you're assuming that blocks commit to key:value maps of the
block contents, specifically a pre-block "UTXO deletion/things that this block
spent" set? First of all, it's interesting how the much smaller dataset of a
pre-block key:value map would make L2/L3 caching optimizations much more likely
to be relevant. :)


That type of solution would be very similar to the solutions treechains would
need to prove coins haven't been doublespent. Basically, in treechains the
system as a whole is a datastructure indexed by time and prefix. So, if you
want to prove a valid spend you need to convince me of three things:

1. The coin existed as of time t1 at prefix p

2. At t2, p, a valid spend was published.

3. Between t1 and t2 at prefix p no other valid spend was published.

Paths to any prefix p as of time t, will have about log2(len(p)) size (beyond
the top-level chain), similar to your above suggestion. Of course, unlike your
above suggestion, in treechains it's not clear if step #1 can be done without
another n*log(N)-ish sized proof in a truly trustless environment!

> But validation before block propagation needs to be extremely
> fast, so for utxo roots this trick is probably both necessary and
> sufficient.

I'm _not_ of the optinion that validation before propagation needs to be done
at all - I think it's perfectly reasonable to propgate blocks that you have not
validated at all (beyond checking PoW as an anti-DoS measure).  The time it
takes miners to start mining the next block - and collecting fees - is however
very important.

In practice, I think we're mostly in agreement here, but because I'm happy to
propagate prior to validating I'd be ok with protocol designs that required
miners to have relatively large amounts of RAM - say 32GB - dedicated to UTXO
lookup because that wouldn't require relay nodes to also have those kinds of
resources available to them once validationless propagation was implemented.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Have you considered CDMA?  This has the nice property that it just sounds
like noise.  The codes would take longer to send, but you could send
multiple bits at once and have the codes orthogonal.

-------------------------------------
ACK

We have already started work on Coinjoin simulated transactions and are
very interested in working on an implementation of this proposal with a
view towards making wallet footprints less identifiable.

On Thu, May 19, 2016 at 5:18 AM, Kristov Atlas via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I've updated the language of the BIP. New version:
>
> <pre>
>   BIP: TBD
>   Title: Best Practices for Heterogeneous Input Script Transactions
>   Author: Kristov Atlas <kristov@openbitcoinprivacyproject.org>
>   Status: Draft
>   Type: Informational
>   Created: 2016-02-10
> </pre>
>
> ==Abstract==
>
> The privacy of Bitcoin users with respect to graph analysis is reduced
> when a transaction is created that contains inputs composed from different
> scripts. However, creating such transactions is often unavoidable.
>
> This document proposes a set of best practice guidelines which minimize
> the adverse privacy consequences of such unavoidable transaction situations
> while simultaneously maximising the effectiveness of user protection
> protocols.
>
> ==Copyright==
>
> This BIP is in the public domain.
>
> ==Definitions==
>
> * '''Heterogenous input script transaction (HIT)''': A transaction
> containing multiple inputs where not all inputs have identical scripts
> (e.g. a transaction spending from more than one Bitcoin address)
> * '''Unavoidable heterogenous input script transaction''': An HIT created
> as a result of a user’s desire to create a new output with a value larger
> than the value of his wallet's largest existing unspent output
> * '''Intentional heterogenous input script transaction''': An HIT created
> as part of a user protection protocol for reducing uncontrolled disclosure
> of personally-identifying information (PII)
>
> ==Motivations==
>
> The recommendations in this document are designed to accomplish three
> goals:
>
> # Maximise the effectiveness of user-protecting protocols: Users may find
> that protection protocols are counterproductive if such transactions have a
> distinctive fingerprint which renders them ineffective.
> # Minimise the adverse consequences of unavoidable heterogenous input
> transactions: If unavoidable HITs are indistinguishable from intentional
> HITs, a user creating an unavoidable HIT benefits from ambiguity with
> respect to graph analysis.
> # Limiting the effect on UTXO set growth: To date, non-standardized
> intentional HITs tend to increase the network's UTXO set with each
> transaction; this standard attempts to minimize this effect by
> standardizing unavoidable and intentional HITs to limit UTXO set growth.
>
> In order to achieve these goals, this specification proposes a set of best
> practices for heterogenous input script transaction creation. These
> practices accommodate all applicable requirements of both intentional and
> unavoidable HITs while maximising the effectiveness of both in terms of
> preventing uncontrolled disclosure of PII.
>
> In order to achieve this, two forms of HIT are proposed: Standard form and
> alternate form.
>
> ==Standard form heterogenous input script transaction==
>
> ===Rules===
>
> An HIT is Standard form if it adheres to all of the following rules:
>
> # The number of unique output scripts must be equal to the number of
> unique inputs scripts (irrespective of the number of inputs and outputs).
> # All output scripts must be unique.
> # At least one pair of outputs must be of equal value.
> # The largest output in the transaction is a member of a set containing at
> least two identically-sized outputs.
>
> ===Rationale===
>
> The requirement for equal numbers of unique input/output scripts instead
> of equal number of inputs/outputs accommodates user-protecting UTXO
> selection behavior. Wallets may contain spendable outputs with identical
> scripts due to intentional or accidental address reuse, or due to dusting
> attacks. In order to minimise the adverse consequences of address reuse,
> any time a UTXO is included in a transaction as an input, all UTXOs with
> the same spending script should also be included in the transaction.
>
> The requirement that all output scripts are unique prevents address reuse.
> Restricting the number of outputs to the number of unique input scripts
> prevents this policy from growing the network’s UTXO set. A standard form
> HIT transaction will always have a number of inputs greater than or equal
> to the number of outputs.
>
> The requirement for at least one pair of outputs in an intentional HIT to
> be of equal value results in optimal behavior, and causes intentional HITs
> to resemble unavoidable HITs.
>
> ==Alternate form heterogenous input script transactions==
>
> The formation of a standard form HIT is not possible in the following
> cases:
>
> # The HIT is unavoidable, and the user’s wallet contains an insufficient
> number or size of UTXOs to create a standard form HIT.
> # The user wishes to reduce the number of utxos in their wallet, and does
> not have any sets of utxos with identical scripts.
>
> When one of the following cases exist, a compliant implementation may
> create an alternate form HIT by constructing a transaction as follows:
>
> ===Procedure===
>
> # Find the smallest combination of inputs whose value is at least the
> value of the desired spend.
> ## Add these inputs to the transaction.
> ## Add a spend output to the transaction.
> ## Add a change output to the transaction containing the difference
> between the current set of inputs and the desired spend.
> # Repeat step 1 to create a second spend output and change output.
> # Adjust the change outputs as necessary to pay the desired transaction
> fee.
>
> Clients which create intentional HITs must have the capability to form
> alternate form HITs, and must do so for a non-zero fraction of the
> transactions they create.
>
> ==Non-compliant heterogenous input script transactions==
>
> If a user wishes to create an output that is larger than half the total
> size of their spendable outputs, or if their inputs are not distributed in
> a manner in which the alternate form procedure can be completed, then the
> user can not create a transaction which is compliant with this procedure.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 

dev@samouraiwallet.com

PGP public key fingerprint:

ED1A 1280 DEFC A603 14CD  15BF 72B5 BACD FEDF 39D7

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.12.0rc1 are available from:

    https://bitcoin.org/bin/bitcoin-core-0.12.0/test/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.12.0rc1

This is a release candidate for a new major version release, bringing new
features, bug fixes, as well as other improvements.

Preliminary release notes for the release can be found here:

    https://github.com/bitcoin/bitcoin/blob/0.12/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.12.0.

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWm2fGAAoJEHSBCwEjRsmm274H/2BH3QD4AlJ87mQ8g6bzzv7h
S8m/EEDmpOuMgM6uF5PzWQ84yNfSyMItq7Y3cU8p9Fv+JD6ic1ZQPPQ0MQc3KtDx
EeF3wQ2iJe/ggBFcwrz0eIxfEOEo1mi5ooWMVSsnCKQU0IpMtq7ToMvhi/39ACnj
GsVRBJYlFoRCBh1LKkcyID7Fh7JstMgMrLEcrCy46T9h2EQEevlLydkwY26ENYUO
BasWXMaysdeKieO5S6tM6MD/50Bd19jHvjzvkeRY5+nZIdrNR1b5n7diCLEUa7b4
79oIqjdKF+4ns5Qgc+iVhIktthRyrHLrWxX7N8Ky+hSVj1OAKFZfdp4skgAzQUE=
=oVOV
-----END PGP SIGNATURE-----


-------------------------------------
>From what I've seen, most people build their own account system separately
(including fee management) and just use bitcoind to send, receive, and
verify transactions. It's not meant to be a drop-in solution for running an
entire bitcoin deposit and withdrawal system, it just provides the bare
tools required to build your own. If you need a pre-built solution, there
are companies that provide those types of services as a platform (BitGo,
for example).

On Wed, Aug 3, 2016, 11:25 Marc Larue via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi,
>
> I have 2 problems with bitcoind that separately are not a problem but
> together they make the platform unusable for many projects.
>
> If I have accounts I need to make sure the account holders do not
> overcharge their account. To do this I can now use "createrawtransaction()
> + fundrawtransaction() + signrawtransaction()" and then make sure the
> transaction can be paid by an account.
>
> But since you deprecated the accounts and there is no
> sendrawtransactionfrom() method; I either have to build my own account
> system (this is no picknick btw, since you need to track all incoming
> funds to all addresses and having an integrated account system in bitcoind
> is 100% necessary to do this effectively).
>
> Or I might be able to go ahead and speculate that you will not be able to
> untangle the account code and hack my bitcoind to have a sendfrom with a
> fixed fee parameter that overrides the size multiplication and I just do
> the math before I send hoping that the transactions go through (this is
> bad but better than having accounts overcharge because they send dust that
> induce high fees).
>
> I understand the privacy problems with using accounts for off-chain
> microstransactions but currently it's the best workable option.
>
> I hope you understand that I'm not trolling here, I have been mining since
> 2011 on FPGAs and built bitcoinbankbook.com 2 years ago. When I descovered
> that once transactions will require fees (back then they didn't) and that
> your system is not able to handle fees with accounts, I stopped developing
> everything related to bitcoin.
>
> There are probably 100s if not 1000s of developers in the same situation.
>
> You can't just deprecate accounts like that because nobody likes the code.
> Without accounts bitcoind is only a person-to-person manual client.
>
> To build many-to-many automatic "organisations" on top of bitcoind you
> need accounts and you need fees that are predictable.
>
> Kind Regards,
>
> /marc
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Sunday, 16 October 2016 09:47:40 CEST Douglas Roark via bitcoin-dev 
wrote:
> Would I want anyone to lose money due to faulty wallets? Of course not.
> By the same token, devs have had almost a year to tinker with SegWit and
> make sure the wallet isn't so poorly written that it'll flame out when
> SegWit comes along. It's not like this is some untested, mostly unknown
> feature that's being slipped out at the last minute

There have been objections to the way that SegWit has been implemented for a 
long time, some wallets are taking a "wait and see" approach.  If you look 
at the page you linked[1], that is a very very sad state of affairs. The 
vast majority is not ready.  Would be interesting to get a more up-to-date 
view.
Wallets probably won't want to invest resources adding support for a feature 
that will never be activated. The fact that we have a much safer alternative 
in the form of Flexible Transactions may mean it will not get activated. We 
won't know until its actually locked in.
Wallets may not act until its actually locked in either. And I think we 
should respect that.

Even if all wallets support it (and thats a big if), they need to be rolled 
out and people need to actually download those updates.
This takes time, 2 months after the lock-in of SegWit would be the minimum 
safe time for people to actually upgrade.

1) https://bitcoincore.org/en/segwit_adoption/
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Fri, Feb 26, 2016 at 01:32:34AM +0000, Gregory Maxwell via bitcoin-dev wrote:
> On Fri, Feb 26, 2016 at 1:07 AM, Joseph Poon via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > I'm interested in input and in the level of receptiveness to this. If
> > there is interest, I'll write up a draft BIP in the next couple days.
> .. I think this should probably be constructed as a new segwit script type,
> and not a base feature.

+1 to both

> The exact construction you're thinking of there isn't clear to me...

I think the idea is that you have three transactions:

 anchor:
   input: whatever
   output:
     - single output, spendable by 2-of-2 multisig
     - [possibly others as well, whatever]

 commitment:
   input: anchor
   outputs:
     1. payment to A
     2. payment to B
     3. HTLC to A on R1, timeout T1
     4. HTLC to A on R2, timeout T2
     5. HTLC to B on R3, timeout T3
     ...

 penalty:
   inputs:
     all the outputs from the commitment tx
   outputs:
     1. 99% as payment to me
     2.  1% as outsourcing fee

As long as the key I use for spending each of commitment transactions
outputs is "single use" -- ie, I don't use it for other channels or
anywhere else on the blockchain, then as long as the signature commits
to the outputs it's safe afaics.

(You still have to send a lot of data to the place you're outsourcing
chain-monitoring to; all the R1,R2,R3 and T1,T2,T3 values are needed in
order to reconstruct the redeem scripts)

> one thing that comes to mind is that I think it is imperative that we
> do not deploy a without-inputs SIGHASH flag without also deploying at
> least a fee-committing sighash-all.

If the fee for commitment transactions changes regularly (eg, a new
commitment transaction is generated every few seconds/minutes, and the fee
is chosen based on whatever estimatefee returns), I think this would cause
problems -- you couldn't use a single signature to cover every revoked
commitment, you'd need one for each different fee level that you'd used
for the lifetime of the channel. Actually, the size of the commitment
transaction will differ anyway depending on how many HTLCs are open,
so even if estimatefee didn't change, the fee would still differ. So I
think commiting to a fee isn't workable for the lightning use case...

> When you do write a BIP for this its imperative that the vulnerability
> to replay is called out in bold blinking flaming text, along with the
> necessary description of how to use it safely. The fact that without
> input commitments transactions are replayable is highly surprising to
> many developers... Personally, I'd even go so far as to name the flag
> SIGHASH_REPLAY_VULNERABLE. :)

+1, though I'm not sure it's so much "vulnerable" to replay as it is
"explicitly designed" to be replayable...

Cheers,
aj



-------------------------------------
That's a valid point, and one we had thought of, which is why I wanted to
get everyone's opinion. I agree the proposed field extensions have nothing
to do with encryption, but does it make sense to propose a completely
separate BIP for such a small thing? If that is the accepted way to go, we
can split it into two and make a separate proposal.

On Fri, Mar 11, 2016 at 5:48 AM Andreas Schildbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I think it's a bad idea to pollute the original idea of this BIP with
> other extensions. Other extensions should go to separate BIPs,
> especially since methods to clarify the fee have nothing to do with
> secure and authenticated bi-directional BIP70 communication.
>
>
> On 03/10/2016 10:43 PM, James MacWhyte via bitcoin-dev wrote:
> > Hi everyone,
> >
> > Our BIP (officially proposed on March 1) has tentatively been assigned
> > number 75. Also, the title has been changed to "Out of Band Address
> > Exchange using Payment Protocol Encryption" to be more accurate.
> >
> > We thought it would be good to take this opportunity to add some
> > optional fields to the BIP70 paymentDetails message. The new fields are:
> > subtractable fee (give permission to the sender to use some of the
> > requested amount towards the transaction fee), fee per kb (the minimum
> > fee required to be accepted as zeroconf), and replace by fee (whether or
> > not a transaction with the RBF flag will be accepted with zeroconf). I
> > know it doesn't make much sense for merchants to accept RBF with
> > zeroconf, so that last one might be used more to explicitly refuse RBF
> > transactions (and allow the automation of choosing a setting based on
> > who you are transacting with).
> >
> > I see BIP75 as a general modernization of BIP70, so I think it should be
> > fine to include these extensions in the new BIP, even though these
> > fields are not specific to the features we are proposing. Please take a
> > look at the relevant section and let me know if anyone has any concerns:
> >
> https://github.com/techguy613/bips/blob/master/bip-0075.mediawiki#Extending_BIP70_PaymentDetails
> >
> > The BIP70 extensions page in our fork has also been updated.
> >
> > Thanks!
> >
> > James
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Thanks for Peter Todd’s detailed report:
https://petertodd.org/2016/segwit-consensus-critical-code-review

I have the following response.

>Since the reserve value is only a single, 32-byte value, we’re setting ourselves up for the same problem again7.

Please note that unlimited space has been reserved after the witness commitment:

  block.vtx[0].vout[o].scriptPubKey.size() >= 38

 Which means anything after 38 bytes has no consensus meaning. Any new consensus critical commitments/metadata could be put there. Anyway, there is no efficient way to add a new commitment with softfork.


> the fact that we do this has a rather odd result: a transaction spending a witness output with an unknown version is valid even if the transaction doesn’t have any witnesses!

I don’t see any reason to have such check. We simply leave unknown witness program as any-one-can-spend without looking at the witness, as described in BIP141.


> Bizzarely segwit has an additonal pay-to-witness-pubkey-hashP2WPKH that lets you use a 160-bit (20 byte) commitment……

Since ~90% of current transactions are P2PKH, we expect many people will keep using this type of transaction in the future. P2WPKH gives the same level of security as P2PKH, and smaller scriptPubKey.

>give users the option instead to choose to accept the less secure 160-bit commitment if their use-case doesn’t need the full 256-bit security level

This is actually discussed on the mailing list. P2WSH with multi-sig is subject to birthday attack, and therefore 256-bit is used to provide 128-bit security. P2WPKH is used as single sig and therefore 160-bit is enough.


>Secondly, if you are going to give a 160-bit commitment option, you don’t need the extra level of indirection in the P2SH case: just make the segwit redeemScript be: <version> <serialized witness script>

Something wrong here? In P2WPKH, the witness is <sig> <pubkey>


>The only downside is the serialized witness script is constrained to 520 bytes max

520 is the original limit. BIP141 tries to mimic the existing behaviour as much as possible. Anyway, normally nothing in the current scripts should use a push with more than 75 bytes


>we haven’t explicitly ensured that signatures for the new signature hash can’t be reused for the old signature hash

How could that be? That’d be a hash collision.






-------------------------------------
On Thu, Jun 16, 2016 at 9:34 PM, Peter Todd <pete@petertodd.org> wrote:

> So above you said that in merbinner trees each node "hash[es] in a record
> of
> its depth" That's actually incorrect: each node commits to the prefix that
> all
> keys below that level start with, not just the depth.


I considered a similar trick at the implementation rather than the
definition level: A node doesn't have to store the prefix which is implicit
in its position. That would create a fair number of headaches though,
because I'm using fixed size stuff in important ways, and it could at most
save about 10% of memory, so it goes into the 'maybe later' bucket.


>
> This means that in merbinner trees, cases where multiple keys share parts
> of
> the same prefix are handled efficiently, without introducing extra levels
> unnecessarily; there's no need for the ONLY0/1 nodes as the children of an
> inner node will always be on different sides.
>
> When keys are randomly distributed, this isn't a big deal; OTOH against
> attackers who are choosing keys, e.g. by grinding hashes, merbinner trees
> always have maximum depths in proportion to log2(n) of the actual number of
> items in the tree. Grinding is particularly annoying to deal with due to
> the
> birthday attack: creating a ground prefix 64 bits long only takes 32 bits
> worth
> of work.
>

Yes an attacker can force the tree to be deeper in places, but it's
mitigated in several ways: (1) The way I'm using memory it won't cause a
whole new block to be allocated, it will just force log(attack strength) -
log(n) nodes to be used (2) logarithmic growth being what it is that isn't
such a big amount (3) With the special casing of TERMBOTH an attacker needs
three things with the same prefix to pull off an attack rather than two,
which is quite a bit harder to pull off.

That said, it wouldn't be all that hard to change how the hashing function
works to do a single hash for a whole series of ONLY in a row instead of a
new one at every level, which would make the attacker only able to force
extra memory usage instead of extra CPU, but this is a slightly annoying
thing to write to stop a fairly lame attack, so I'm at least not doing it
for my initial implementation. I could likely be convinced that it's worth
doing before an actual release though. There's another implementation trick
to do the same thing for memory usage, which is much more in the 'do later'
category because it doesn't involve changing the format and hence it can be
put off.


> In particular, case #2 handles your leaf node optimizations generically,
> without special cases and additional complexity. It'd also be a better way
> to
> do the ONLY0/1 cases, as if the "nothing on this side" symbol is a single
> byte,
> each additional colliding level would simply extend the commitment without
> hashing. In short, you'd have nearly the same level of optimization even
> if at
> the cryptography level your tree consists of only leaves, inner nodes, and
> nil.
>

I'm taking pains to make all the hashing be of fixed-size things, so that a
non-padding variant of a secure hashing algorithm can be used. The chains
of ONLY thing above would force a special exception to that, which can be
done but is annoying. Making things smaller than a single block (64 bytes)
won't speed up hashing time, and making things a single byte longer than
that doubles it.


> Another advantage of variable sized commitments is that it can help make
> clear
> to users when it's possible to brute force the message behind the
> commitment.
> For instance, digest from a hashed four byte integer can be trivially
> reversed
> by just trying all combinations. Equally, if that integer is concatenated
> with
> a 32 byte digest that the attacker knows, the value of the integer can be
> brute
> forced.
>

I'm hashing all strings before inserting to get them to be a fixed size and
avoid a few different attacks. In Bitcoin most of the strings added are
longer than that so it's a form of compression. A custom hash function
could be used which 'hashes' very short strings by repeating them verbatim
could be used, but seems like not such a hot idea. I'm making extensive use
of things being fixed size everywhere, which improves performance in a lot
of ways.


> > > Technically even a patricia trie utxo commitment can have sub-1 cache
> > > > misses per update if some of the updates in a single block are close
> to
> > > > each other in memory. I think I can get practical Bitcoin updates
> down
> > > to a
> > > > little bit less than one l2 cache miss per update, but not a lot
> less.
> > >
> > > I'm very confused as to why you think that's possible. When you say
> > > "practical
> > > Bitcoin updates", what exactly is the data structure you're proposing
> to
> > > update? How is it indexed?
>

I'll re-answer this because I did a terrible job before. The entire data
structure consists of nodes which contain a metadata byte (TERM0, ONLY1,
etc.) followed by fixes size secure hashes, and (in some cases) pointers to
where the children are. The secure hashes in parent nodes are found by
hashing their children verbatim (or the stored string in the case of a
TERM). This is very conventional. All of the cleverness is in where in
memory these nodes are stored so that tracing down the tree causes very few
cache misses.

(The alternate approach is to have each node store its own hash rather than
that be stored by the parent. That approach means that when you're
recalculating you have to look up siblings which doubles the number of
cache misses. Not such a hot idea.)

At the root there's a branch block. It consists of all nodes up to some
fixed depth - let's say 12 - with that depth set so that it roughly fits
within a single memory page. Branch blocks are arranged with the nodes in
fixed position defined by the prefix they correspond to, and the terminals
have outpointers to other blocks. Because they're all clustered together, a
lookup or update will only require a single

Below the root block are other branch blocks. Each of them has a fixed 12
bit prefix it is responsible for. When doing a lookup a second cache miss
will be hit for levels 13-24, because those are all clustered in the same
branch block.

Below the second level of root block (at Bitcoin utxo set scale - this
varies based on how much is stored) there are leaf blocks. A leaf block
consists of nodes with outpointers to its own children which must be within
the same leaf block. All entry points into a leaf block are from the same
branch block, and the leaf block has no out pointers to other blocks. When
a leaf block overflows the entry point into it which overflowed is moved
into the active leaf for that branch, and if that's full a new one is
allocated. There's some subtlety to exactly how this is done, but I've
gotten everything down to simple expedient tricks with straightforward
implementations. The thing which matters for now is that there's only a
single cache miss for each leaf node, because they also fit in a page.

So at Bitcoin scale there will probably only be 3 cache misses for a
lookup, and that's a worst case scenario. The first one is probably always
warm, bringing it down to 2, and if you do a bunch in sorted order they'll
probably hit the same second level branches repeatedly bringing it down to
1, and might even average less than that if there are enough that the leaf
block has multiple things being accessed.

(These same tricks can be applied to merbinner tree implementation as well,
although that would be a bit less parsimonious with memory, by a small
constant factor.)


> Anyway hashing is pretty slow. The very fast BLAKE2 is about 3 cycles/byte
> (SHA256 is about 15 cycles/byte) so hashing that same data would take
> around
> 200 cycles, and probably quite a bit more in practice due to overheads
> from our
> short message lengths; fetching a cache line from DRAM only takes about
> 1,000
> cycles. I'd guess that once other overheads are taken into account, even
> if you
> could eliminate L2/L3 cache-misses it wouldn't be much of an improvement.
>

Those numbers actually back up my claims about performance. If you're doing
a single update and recalculating the root afterwards, then the amount of
rehashing to be done is about 30 levels deep times 64 bytes per thing
hashed times 15 cycles per byte then it's about 28,800 cycles of hashing.
If you have a naive radix tree implementation which hits a cache miss at
every level then that's 30,000 cycles, which is about half the performance
time, certainly worth optimizing. If instead of sha256 you use blake2
(Which sounds like a very good idea!) then hashing for an update will be
about 5760 cycles and performance will be completely dominated by cache
misses. If a more cache coherent implementation is used, then the cost of
cache misses will be 3000 cycles, which will be a non-factor with sha256
and a significant but not dominating one with blake2.

It's reasonable to interpret those numbers as saying that blake2 and cache
coherent implementation are both clearly worth it (probably necessary for
real adoption) and that an amortized binary radix tree is tempting but not
worth it.

-------------------------------------
Johnson Lau via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
writes:
> Restriction for segwit OP_IF argument as a policy has got a few concept ACK. I would like to have more people to ACK or NACK, especially the real users of OP_IF. I think Lightning network would use that at lot.

My current scripts use OP_IF and OP_NOTIF only after OP_EQUAL, except
for one place where they use OP_EQUAL ... OP_EQUAL... OP_ADD OP_IF
(where the two OP_EQUALs are comparing against different hashes, so only
0 or 1 of the two OP_EQUAL can return 1).

So there's no effect either way on the c-lightning implementation, at
least.

Thanks!
Rusty.


-------------------------------------
On Sun, Dec 11, 2016 at 3:31 PM, James Hilliard <james.hilliard1@gmail.com>
wrote:

> What's most likely to happen is miners will max out the blocks they
> mine simply to try and get as many transaction fees as possible like
> they are doing right now(there will be a backlog of transactions at
> any block size). Having the block size double every year would likely
> cause major problems and this proposal allows over a 7x increase it
> seems.


Block75 is not exponential scaling. It's true the max theoretical increase
in the first year would be 7x, but the next year would be a max of 2x, and
the next could only increase by 50% and so on.

However, to reach the max in the first year: 1) ALL blocks would have to be
100% full and 2) transactions would have to increase at the same rate. We'd
have to be doing 2.1 million transactions a day within a year to make that
happen, and would therefore need blocks to be that big.

Realistically, max block size will grow (and shrink) at a much slower rate
... even more so with SegWit.


>  The main problem with this proposal I think is that users effectively

have no way to stop the miners from increasing block size
> continuously.


Yes they could, simply by not sending transactions. Users don't care at all
about block size. They just want their transactions to be fast and
relatively cheap.

-t.k.

-------------------------------------
Great discussion, Sergio and Tom!

> I now think my reasoning and conclusions are based on a false premise: that BU block size policies for miners can be heterogeneous.


Right, miners who set their block size limits (BSL) above OR below the "effective BSL" are disadvantaged.  Imagine that we plot the distribution (by hash power) for all miners' BSLs.  We might get a chart that looks like this:

http://imgur.com/a/tWNr6 <http://imgur.com/a/tWNr6>

In this chart, the "effective BSL" is defined as the largest block size that no less than half the hash power will accept.  

If a block is mined with a size Q that is less than the "effective BSL," then all the hash power with BSLs between BSL_min and Q will be forked from the longest chain (until they update their software if they're running Core or until their acceptance depth is hit if they're running BU).  This wastes these miners' hash power.  

However, if a block is mined with a size Q that is greater than the effective BSL, then all the hash power with BSLs between Q and BSL_max will temporarily be mining on a "destined to be orphaned" chain.  This also wastes these miners' hash power.  

Therefore, it is in the best interest of miners to all set the same block size limit (and reliably signal in their coinbase TX what that limit is, as done by Bitcoin Unlimited miners).  

We have empirical evidence the miners in fact behave this way: 

(1) No major miner has ever set his block size limit to less than 1 MB (not even those such as Luke-Jr who think 1 MB is too big) because doing so would just waste money.  

(2) After switching to Bitcoin Unlimited, both ViaBTC and the Bitcoin.com pool temporarily set their BSLs to 2 MB and 16 MB, respectively (of course keeping their _generation limit_ at 1MB).  However, both miners quickly reduced these limits back to 1 MB when they realized how it was possible to lose money in an attack scenario.  (This actually surprised me because the only way they could lose money is if some _other_ miner wasted even more money by purposely mining a destined-to-be-orphaned block.)   

The follow-up article I'm working on is about the topics we're discussing now, particularly about how Bitcoin Unlimited's “node-scale” behavior facilitates the emergence of a fluid and organic block size limit at the network scale.  Happy to keep continue with this current discussion, however.

Best regards
Peter


-------------------------------------
On Friday, July 15, 2016 4:46:57 PM Wladimir J. van der Laan wrote:
> On Fri, Jul 15, 2016 at 03:52:37PM +0000, Luke Dashjr wrote:
> > On Friday, July 15, 2016 3:46:28 PM Wladimir J. van der Laan wrote:
> > > I'm not sure why it is labeled as only "Informational" in the first
> > > place, as BIP9 is part of the consensus logic.
> > 
> > Only by proxy/inclusion from another BIP, such as 68, 112, and 113. In
> > other words, BIP 9 is informational in that it advises how other BIPs
> > might deploy themselves.
> 
> It's a bit of grey area, as indeed, only the BIPs that are actual softforks
> are consensus changes - which employ this mechanism for deployment. But I
> think such an important deployment mechanism, which is supposed to be used
> by all softforks from now onwards, shouldn't just be an informational BIP.

As things stand right now, none of the Authors have commented on changing the 
type. It has been a month, and I am prepared to change the status to Final or 
Active; but I am unclear if your comments were an objection to changing the 
status or not.

Last call: Does anyone mind if I update BIP 9 to Final status?

Luke


-------------------------------------
https://github.com/bitcoin/bips/pull/340

BIP: ?
Title: 2016 Multi-Stage Merge-Mine Headers Hard-Fork
Author: James Hilliard <james.hilliard1@gmail.com>
Status: Draft
Type: Standards Track
Created: 2016-02-23

==Abstract==

Use a staged hard fork to implement a headers format change that is
merge mine incompatible along with a timewarp to kill the previous
chain.

==Specification==

We use a block version flag to activate this fork when 3900 out of the
previous 4032 blocks have this the version flag set. This flag locks
in both of the below stages at the same time.

Merge Mine Stage: The initial hard fork is implemented using a merge
mine which requires that the original pre-fork chain be mined with a
generation transaction that creates no new coins in addition to not
containing any transactions. Additionally we have a consensus rule
that requires that ntime be manipulated on the original chain to
artificially increase difficulty and hold back the original chain so
that all non-upgraded clients can never catch up with current time.
The artificial ntime is implemented as a consensus rule for blocks in
the new chain.

Headers Change Stage: This is the final stage of the hard fork where
the header format is made incompatible with merge mining, this is
activated ~50,000 blocks after the Merge Mine Stage and only at the
start of the 2016 block difficulty boundary.

==Motivation==

There are serious issues with pooled mining such as block withhold
attacks that can only be fixed by making major changes to the headers
format.

There are a number of other desirable header format changes that can
only be made in a non-merge mine compatible way.

There is a high risk of there being two viable chains if we don't have
a way to permanently disable the original chain.

==Rationale==

Our solution is to use a two stage hard fork with a single lock in period.

The first stage is designed to kill off the previous chain by holding
back ntime to artificially increase network difficulty on the original
chain to the point where it would be extremely difficult to mine the
2016 blocks needed to trigger a difficulty adjustment. This also makes
it obvious to unupgraded clients that they are not syncing properly
and need to upgrade.

By locking in both stages at the same time we ensure that any clients
merge mining are also locked in for the headers change stage so that
the original chain is dead by the time the headers change takes place.

We timewarp over a year of merge mining to massively increase the
difficulty on the original chain to the point that it would be
incredibly expensive to reduce the difficulty enough that the chain
would be able to get caught up to current time.

==Backward Compatibility==

This hardfork will permanently disable all nodes, both full and light,
which do not explicitly add support for it.
However, their security will not be compromised due to the implementation.
To migrate, all nodes must choose to upgrade, and miners must express
supermajority support.

==Reference Implementation==

TODO


-------------------------------------
Based on previous crypto analysis result, the actual security of SHA512 is
not significantly higher than SHA256.
maybe we should consider SHA3?


On Tue, Jun 28, 2016 at 3:19 PM, Jonas Schnelli via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> > To quote:
> >
> >> HMAC_SHA512(key=ecdh_secret|cipher-type,msg="encryption key").
> >>
> >>  K_1 must be the left 32bytes of the HMAC_SHA512 hash.
> >>  K_2 must be the right 32bytes of the HMAC_SHA512 hash.
> >
> > This seems a weak reason to introduce SHA512 to the mix.  Can we just
> > make:
> >
> > K_1 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="header encryption
> key")
> > K_2 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="body encryption key")
>
> SHA512_HMAC is used by BIP32 [1] and I guess most clients will somehow
> make use of bip32 features. I though a single SHA512_HMAC operation is
> cheaper and simpler then two SHA256_HMAC.
>
> AFAIK, sha256_hmac is also not used by the current p2p & consensus layer.
> Bitcoin-Core uses it for HTTP RPC auth and Tor control.
>
> I don't see big pros/cons for SHA512_HMAC over SHA256_HMAC.
>
> </jonas>
>
> [1]
>
> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#child-key-derivation-ckd-functions
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
Xuesong (Arthur) Chen
Senior Principle Engineer
BlockChain Technologist
BTCC

-------------------------------------
Le 23/08/2016  22:12, Luke Dashjr via bitcoin-dev a crit :
> BIP 39: Mnemonic code for generating deterministic keys
> - Used by many wallets and hundreds of thousands of users.
> 
> BIP 44: Multi-Account Hierarchy for Deterministic Wallets
> - Appears to be implemented by multiple wallets.
> 

I personally believe that BIP39/BIP44 is a bad design. There is limited
support for these BIPs in Electrum, in order to provide compatibility
with hardware wallets. However, I do not plan to use BIP39/BIP44 for
default Electrum wallets, for the following reasons.

(Note that it does not make sense to consider BIP39 and BIP44
independently. Any wallet that decides to implement one without the
other would be considered as broken.)

Here is why I rejected this design:

1 - BIP44 uses multiple accounts. This means that in order to be
compatible with the standard, a wallet *must* implement multiple
accounts. A wallet that decides to keep things simple and use only one
account, will not allow users to recover all their funds when they
restore from a BIP39 seed, and will be considered as broken.

2 - An appealing feature of deterministic wallets is that you can use
the same instance of your wallet on different devices. Two instances of
your wallet can automatically synchronize their Bitcoin addresses, and
display the same balance. The problem is that hardened derivations break
this property. Indeed, with hardened derivations, software wallets need
to ask the user's password in order to derive new accounts. Therefore,
in order to implement automated detection of newly created accounts, a
BIP44-compatible software wallets would need to ask the user's password
whenever a new account is detected. This means that the wallet would ask
the password without the user initiating any action. This seems to be an
avenue for malware.

Of course, hardware wallets do not have that issue, because they can
derive new accounts without requesting a password from the user. BIP44
is a standard that has been designed for hardware wallets, but that
makes things really difficult for software wallets.

3 - Unneeded complexity. From an end user perspective, the multiple
accounts in BIP44 achieve the same result as using different derivation
passphrases with the same BIP39 seed phrase. The only real difference is
that BIP44 accounts can be enumerated deterministically, while
passphrases in general cannot. However, this property is of limited
interest, because automatic synchronization of multiple accounts cannot
be guaranteed for bip44 software wallets, as explained in 2.

4 - BIP39 is inconsistent. It uses a hash of the utf8 encoded 'seed
phrase' in order to derive the BIP32 seed. This hash-based derivation
was added on my suggestion, in order to make the BIP independent from
the particular wordlist used to generate the seed phrases. However,
BIP39 also requires the implementation of a checksum, in order to verify
that a seed phrase is valid. Suprisingly, the specification of the
checksum involves wordlist indices. This means the checksum (and thus
the BIP) requires a fixed wordlist. This defeats the purpose of using a
hash for the derivation of the seed.

The authors of the BIP should either have used hash functions for both
the seed AND the checksum (that is what Electrum does), or for none of
them (in that case case, you can have a bidirectional function between
seed phrases and entropy, which is nice if you want to perform Shamir
secret sharing of seed phrases, at the expenses of a fixed wordlist). In
its current state, BIP39 takes the worst of both worlds.

5 - The fact that the wordlist must be part of BIP39, and cannot be
changed in the future, seems a terrible idea to me. I believe that a
specification should always try to be minimal. In that case, the
specification includes a 2000+ words dictionary, when it could have
avoided that.

Even if you decide that BIP39 is final, there will always be users
requiring the addition of wordlists for new languages. So, in practice,
this BIP will never be final.

6 - Finally, and most importantly, BIP39 seed phrases do not have a
version number. Without a version number, how are you going to derive
addresses from a BIP39 seed phrase, when wallets start to use to new
derivation methods (such as SegWit, or Schnorr signatures)? Does it mean
that a BIP39 compatible wallet will have to check addresses from all the
derivation methods that ever existed in the past, in order to ensure
that all coins are correctly retrieved? Or will there be users that
cannot access their coins because their BIP39 seed phrase is too old for
newer software?


-------------------------------------
> On Wed, Jun 29, 2016 at 08:34:06PM +0200, Jonas Schnelli via bitcoin-dev wrote:
>>> Based on previous crypto analysis result, the actual security of SHA512
>>> is not significantly higher than SHA256.
>>> maybe we should consider SHA3?
>>
>> As far as I know the security of the symmetric cipher key mainly depends
>> on the PRNG and the ECDH scheme.
>>
>> The HMAC_SHA512 will be used to "drive" keys from the ECDH shared secret.
>> HMAC_SHA256 would be sufficient but I have specified SHA512 to allow to
>> directly derive 512bits which allows to have two 256bit keys with one
>> HMAC operation (same pattern is used in BIP for the key/chaincode
>> derivation).
> 
> What's the rational for doing that "directly" rather than with two SHA256
> operations? (specifcially SHA256(0 . thing), SHA256(1 + thing) for the two
> parts we need to derive)

SHA256 and SHA512 are both from the SHA-2 family.

I have specified SHA512 to (slightly) increase the brute-force security
of the ecdh shared secret when knowing K_1 and K_2.

And I assumed (haven't measured the required cpu cycles) that a single
SHA512_HMAC is less expensive then two SHA256_HMAC.

</jonas>


-------------------------------------
Joseph Poon via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:
> Ideally, a 3rd-party can be handed a transaction which can encompass all
> prior states in a compact way. For currently-designed Segregated Witness
> transactions, this requires storing all previous signatures, which can
> become very costly if individuals to thousands of channel state updates
> per day.

AFAICT we need more than this.  Or are you using something other than
the deployable lightning commit tx style?

If each HTLC output is a p2sh[1], you need the timeout and rhash for
each one to build the script to redeem it.  In practice, there's not
much difference between sending a watcher a tx for every commit tx and
sending it information for every new HTLC (roughly a factor of 2).

So we also need to put more in the scriptPubKey for this to work; either
the entire redeemscript, or possibly some kind of multiple-choice P2SH
where any one of the hashes will redeem the payment.

Cheers,
Rusty.
[1] eg. from https://github.com/ElementsProject/lightning/blob/master/doc/deployable-lightning.pdf
        OP_HASH160 OP_DUP # Replace top element with two copies of its hash
        <R-HASH> OP_EQUAL # Test if they supplied the HTLC R value
        OP_SWAP <COMMIT-REVOCATION-HASH> OP_EQUAL OP_ADD
                          # Or the commitment revocation hash
        OP_IF # If any hash matched.
                <KEY-B> # Pay to B.
        OP_ELSE # Must be A, after HTLC has timed out.
                <HTLC-TIMEOUT> OP_CHECKLOCKTIMEVERIFY Ensure (absolute) time has passed.
                <DELAY> OP_CHECKSEQUENCEVERIFY # Delay gives B enough time to use revocation if it has it.
                OP_2DROP # Drop the delay and htlc-timeout from the stack.
                <KEY-A> # Pay to A.
        OP_ENDIF
        OP_CHECKSIG # Verify A or B's signature is correct.

Cheers,
Rusty.


-------------------------------------
Sorry, I completely forgot about having submitted the BIP as I was busy at
this time.
Thanks for the review.

Open Asset is actually not an abandoned project and is a protocol already
used in production with multiple implementation.
Wallet: https://www.coinprism.com/
Implementation C#: https://github.com/NicolasDorier/NBitcoin with heavy
documentation (
https://programmingblockchain.gitbooks.io/programmingblockchain/content/other_types_of_asset/colored_coins.html
)
Implementation Ruby: https://github.com/haw-itn/openassets-ruby/
Usage stats: http://opreturn.org/

Concerning whether or not we can put my name in the BIP, I'll ask the
original author that I know personally.

> Quite a bit ugly, giving a meaning to an input's pubkey script like that.
> But more problematically: how can this work with other pubkey scripts?
> Particularly relevant now that this old script format is being deprecated.
> Another possible problem is that I don't see a way to provably guarantee
an
> asset issuance is final.

Yes, with open asset it is not possible to do provably limited issuance.
The scriptPubKey can be anything, not necessarily P2PK.
If you can spend the scriptPubkey, then you are the issuer.

> And the assets attached to its inputs are destroyed? Or?

Correct, if you spend a colored output incorrectly, it is effectively
destroyed.

> Is it intentional that the first case is "parsable", and the second
"valid"?
> I think these need to be better specified; for example, it is not so
clear how
> to reach if the OAP version number is something other than 1: is that
> parsable? valid?

The terminology is correct we are parsing PUSHDATA, if there is a parsable
pushdata, the output is considered valid.
If there is multiple valid output, then we take the first one.

> What determines the asset id? How would one issue and/or transfer multiple
> asset ids in the same transaction?

You can't issue more than one asset type in a transaction. (as the asset
issued is defined by the scriptPubKey of the first input)
For multiple transfer it is possible, imagine a transaction with the
following 3 inputs and 6 outputs:

Inputs: {0, 10a, 20b}
Outputs: {5, OP_RETURN; 7; 3; 11; 9)

Inputs1: 0
Inputs2: Enqueue 10a in the queue ( {10a} )
Input3: Enqueue 20b in the queue ( { 20b, 10a} )

Output1: Before OP_RETURN, so is issuance whose color is defined by the
scriptPubKey of Input1. (say c)
Output2: No color (marker)
Output3: Dequeue 7a ( {20b, 3a} ), color output with a.
Output4: Dequeue 3a ( {20b} ), color output with a
Output5: Dequeue 11b ( {9b} ), color output with b
Output5: Dequeue 9b ( {0} ), color output with b

Finally, outputs color are
Outputs: {5c, OP_RETURN; 7a; 3a; 11b; 9b)

> What if I have a transaction with 5 outputs, the marker output at
position 3,
> and all 4 other outputs are to receive assets? Does the marker output get
> skipped in the list (ie, the list is 4 elements long) or must it be set to
> zero quantity (ie, the list is 5 elements long)?

Marker output is skipped (explained in the example)

> Addresses are not used for spending bitcoins, only for receiving them.
The way
> this BIP uses inputs' pubkey script is extremely unusual and probably a
bad
> idea.

Actually there is no "issuance address", just the AssetId is defined by the
scriptPubKey of the issuer.

> As I understand it, this would require address reuse to setup, which is
not
> supported behaviour and insecure.

Yes, it requires address reuse for issuing.

> Won't an older client then accidentally destroy assets?

Correct. Actually we prevent users sending asset to wallet which does not
support OA via another address scheme described in another document (
https://github.com/OpenAssets/open-assets-protocol/blob/master/address-format.mediawiki
)

As said, Open Asset is not a draft proposal and is already used in the wild
since 2014. We can't easily modify the protocol by now for improving it.

PS:
https://github.com/OpenAssets/open-assets-protocol/blob/master/specification.mediawiki
is
more readable than a mail.

Nicolas,

On Tue, Jul 5, 2016 at 7:14 PM, James MacWhyte <macwhyte@gmail.com> wrote:

> I'm curious to hear the answers to the questions Luke asked earlier. I
> also read through the documentation and wasn't convinced it was thought out
> well enough to actually build something on top of, but there's no reason it
> can't get a number as a work-in-progress.
>
> I hope it does continue to get worked on, though. The lack of response or
> discussion worries me that it might become an abandoned project.
>
> On Tue, Jul 5, 2016, 18:32 Luke Dashjr via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Tuesday, July 05, 2016 5:46:36 PM Peter Todd wrote:
>> > On Thu, May 26, 2016 at 03:53:04AM +0000, Luke Dashjr via bitcoin-dev
>> wrote:
>> > > On Thursday, May 26, 2016 2:50:26 AM Nicolas Dorier via bitcoin-dev
>> wrote:
>> > > >   Author: Flavien Charlon <flavien@charlon.net>
>> >
>> > What's the status of this BIP? Will it be assigned?
>>
>> I was waiting for clarification on the Author thing, but Nicholas hasn't
>> responded yet. I am unaware of any reason NOT to assign it, and there
>> appear
>> to be no objections, so let's call it BIP 160.
>>
>> Luke
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>

-------------------------------------
Hello Bitcoin Developers,

I would like to make a proposal to update BIP-32 in a small way.

TL;DR: BIP-32 is hard to use right (due to its requirement to skip
addresses).  This proposal suggests a modification such that the
difficulty can be encapsulated in the library.

#MOTIVATION:

The current BIP-32 specifies that if for some node in the hierarchy
the computed hash I_L is larger or equal to the prime or 0, then the
node is invalid and should be skipped in the BIP-32 tree.  This has
several unfortunate consequences:

- All callers of CKDpriv or CKDpub have to check for errors and handle
  them appropriately.  This shifts the burden to the application
  developer instead of being able to handle it in the BIP-32 library.

- It is not clear what to do if an intermediate node is
  missing. E.g. for the default wallet layout, if m/i_H/0 is missing
  should m/i_H/1 be used for external chain and m/i_H/2 for internal
  chain?  This would make the wallet handling much more difficult.

- It gets even worse with standards like BIP-44.  If m/44' is missing
  should we use m/45' instead?  If m/44'/0' is missing should we use
  m/44'/1' instead, using the same addresses as for testnet?
  One could also restart with a different seed in this case, but this
  wouldn't work if one later wants to support another BIP-43 proposal
  and still keep the same wallet.

I think the first point alone is reason enough to change this.  I am
not aware of a BIP-32 application that handles errors like this
correctly in all cases.  It is also very hard to test, since it is
infeasible to brute-force a BIP-32 key and a path where the node does
not exists.

This problem can be avoided by repeating the hashing with slightly
different input data until a valid private key is found.  This would
be in the same spirit as RFC-6979.  This way, the library will always
return a valid node for all paths.  Of course, in the case where the
node is valid according to the current standard the behavior should be
unchanged.

I think the backward compatibility issues are minimal.  The chance
that this affects anyone is less than 10^-30.  Even if it happens, it
would only create some additional addresses (that are not seen if the
user downgrades).  The main reason for suggesting a change is that we
want a similar method for different curves where a collision is much
more likely.

#QUESTIONS:

What is the procedure to update the BIP?  Is it still possible to
change the existing BIP-32 even though it is marked as final?  Or
should I make a new BIP for this that obsoletes BIP-32?

What algorithm is preferred? (bike-shedding)  My suggestion:

---

Change the last step of the private -> private derivation functions to:

 . In case parse(I_L) >= n or k_i = 0, the procedure is repeated
   at step 2 with
    I = HMAC-SHA512(Key = c_par, Data = 0x01 || I_R || ser32(i))

---

I think this suggestion is simple to implement (a bit harder to unit
test) and the string to hash with HMAC-SHA512 always has the same
length.  I use I_R, since I_L is obviously not very random if I_L >= n.
There is a minimal chance that it will lead to an infinite loop if I_R
is the same in two consecutive iterations, but that has only a chance
of 1 in 2^512 (if the algorithm is used for different curves that make
I_L >= n more likely, the chance is still less than 1 in 2^256).  In
theory, this loop can be avoided by incrementing i in every iteration,
but this would make an implementation error in the "hard to test" path
of the program more likely.

The other derivation functions should be updated in a similar matter.
Also the derivation of the root node from the seed should be updated
in a similar matter to avoid invalid seeds.

If you followed until here, thanks for reading this long posting.

  Jochen


-------------------------------------
Well I think empirical game-theory observed on the network involves more
types of strategy than honest vs dishonest.  At least 4, maybe 5 types of
strategy and I would argue lumping the strategies together results in
incorrect game theory conclusions and predictions.

A) altruistic players (protocol following by principle to be good network
citizens, will forgo incremental profits to aid network health) eg aim to
decentralize hashrate, will mine stuck transactions for free, run pools
with zero fee, put more effort into custom spam filtering, tend to be power
users, or long term invested etc.

B) honest players (protocol following but non-altruistic or just
lazy/asleep run default software, but still leaving some dishonest profit
untaken). Eg reject spy mining, but no charitable actions, will not
retaliate in kind to semi-honest zero sum attacks that reduce their profits.

C) semi-honest (will violate protocol if their attack can be plausibly
deniable or argued to be not hugely damaging to network security). Eg spy
mining, centralised pools increasing other miners orphan rates.

D) rational players (will violate the protocol for profit: will not overtly
steal from users via double spends, but anything short particularly
disadvantaging other miners even if it results in centralisation is treated
as fair game) eg selfish mining. Would increase block size by filling with
pay to self transactions, if it increased orphans for others.

E) dishonest players (aka hyper-rational: will actually steal from users
probabilistically if possible, not as worried about detection). Eg double
spend and probabilistic double spends (against onchain gambling games).
Would DDoS competing pools.

In part the strategies depend on investment horizon, it is long term
rational for altruistic behavior to forgo incremental short term profit to
improve user experience.  Hyper-rational to buy votes in a "ends justify
means" mentality though fortunately most network players are not dishonest.

So called meta-incentive (unwillingness to risk hurting bitcoin due to
intended long term ho dling coins or ASICs) can also explain bias towards
honest or altruistic strategies.

Renting too much hashrate is risky as it can avoid the meta-incentive and
increase rational or dishonest strategies.

In particular re differentiating from 51% attack so long as > 50% are
semi-honest, honest or altruistic it won't happen.  It would seem actually
that > 66-75% are because we have not seen selfish mining on the network.
Though I think conveniently slow block publication by some players in the
60% spy mining semi-honest cartel was seen for a while, the claim has been
it was short-lived and due to technical issue.

It would be interesting to try to categorise and estimate the network %
engaging in each strategy.  I think the information is mostly known.

Adam

On Dec 11, 2016 03:22, "Daniele Pinna via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> How is the adverse scenario you describe different from a plain old 51%
> attack? Each proposed protocol change  where 51% or more  of the network
> can potentially game the rules and break the system should be considered
> just as acceptable/unacceptable as another.
>
> There comes a point where some form of basic honesty must be assumed on
> behalf of participants benefiting from the system working properly and
> reliably.
>
> Afterall, what magic line of code prohibits all miners from simultaneously
> turning all their equipment off...  just because?
>
> Maybe this 'one':
>
> "As long as a majority of CPU power is controlled by nodes that are not
> cooperating to attack the network, they'll generate the longest chain and
> outpace attackers. The network itself requires minimal structure."
>
> Is there such a thing as an unrecognizable 51% attack?  One where the
> remaining 49% get dragged in against their will?
>
> Daniele
>
> On Dec 10, 2016 6:39 PM, "Pieter Wuille" <pieter.wuille@gmail.com> wrote:
>
>> On Sat, Dec 10, 2016 at 4:23 AM, Daniele Pinna via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> We have models for estimating the probability that a block is orphaned
>>> given average network bandwidth and block size.
>>>
>>> The question is, do we have objective measures of these two quantities?
>>> Couldn't we target an orphan_rate < max_rate?
>>>
>>
>> Models can predict orphan rate given block size and network/hashrate
>> topology, but you can't control the topology (and things like FIBRE hide
>> the effect of block size on this as well). The result is that if you're
>> purely optimizing for minimal orphan rate, you can end up with a single
>> (conglomerate of) pools producing all the blocks. Such a setup has no
>> propagation delay at all, and as a result can always achieve 0 orphans.
>>
>> Cheers,
>>
>> --
>> Pieter
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
No, BIP30 prevents duplicate tx hashes in the case where the new tx hash
duplicates that of a preceding tx with unspent outputs.

There was one such case that had already become buried in the chain at
the time, so it was exempted from validation. There was another case of
a duplicate hash, but it's predecessor was spent so it complied with the
new rule.

Both of these cases resulted from exact duplicate txs, which BIP34 now
precludes. However nothing precludes different txs from having the same
hash.

e

On 11/16/2016 04:06 PM, Jorge Timón wrote:
> On Thu, Nov 17, 2016 at 1:00 AM, Eric Voskuil <eric@voskuil.org> wrote:
>> This is a misinterpretation of BIP30. Duplicate transaction hashes can
>> and will happen and are perfectly valid in Bitcoin. BIP34 does not
>> prevent this.
> 
> Sorry for moving the topic, but isn't duplication of tx hashes
> precisely what BIP30 prevents?
> That was my undesrtanding but should read it again.
> Since regular txs take inputs, the collision is extremely unlikely
> (again, this is my understanding, please correct me when wrong), the
> worrying case is coinbase txs (which don't have input to take entropy
> from). By introducing the committed height, collisions on coinbase txs
> are prevented too.
> 
> If I'm wrong on any of this I'm more than happy to learn why.
> 


-------------------------------------
Hi,

> However, I think it could actually increase
> confidence in the system if the community is able to demonstrate a good
> process for making such decisions, and show that we can separate the
> meaningful underlying principles, such as the coin limit and overall
> inflation rate, from what is more akin to an implementation detail, as I
> consider the large-step reward reduction to be.

I do not think that a line can be drawn here. As far as I understood,
you think that the coin limit is a meaningful underlying principle
which should not be touched, whereas the halving of mining rewards is
an implementation detail. The two are very closely tied together and
changes to both of them would result in a hardfork, if I am not
mistaken.

Regarding the effects of the mining reward halving, there is a nice
paper from courtois:
http://arxiv.org/abs/1405.0534

All the best
Henning



On Thu, Mar 03, 2016 at 10:27:35AM -0800, Corey Haddad via bitcoin-dev wrote:
> Since the root cause of what you are trying to address is the reward
> having, I'd suggest considering an adjustment to the having schedule.
> Instead of their being a large supply shock every four years, perhaps the
> reward could drop every 52,500 blocks (yearly), or even at each difficulty
> adjustment, in such a way that the inflation curve is smoothed out.  The
> exponential decay rate would be preserved, so overall economic philosophy
> would be preserved.
> 
> I'm guessing hesitance to this approach would lie in a reluctance to tinker
> with Bitcoin's 'economic contract', and slippery slope concerns about might
> be the next change (21M?).  However, I think it could actually increase
> confidence in the system if the community is able to demonstrate a good
> process for making such decisions, and show that we can separate the
> meaningful underlying principles, such as the coin limit and overall
> inflation rate, from what is more akin to an implementation detail, as I
> consider the large-step reward reduction to be.
> 
> I'm not too worried about the impact of the having as is, but adjusting the
> economic parameter would be a safer and simpler way to address the concerns
> than to tinker with the difficulty targeting mechanism, which is at the
> heart of Bitcoin's security
> 
> On Wed, Mar 2, 2016 at 6:56 AM, Luke Dashjr via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> > We are coming up on the subsidy halving this July, and there have been some
> > concerns raised that a non-trivial number of miners could potentially drop
> > off
> > the network. This would result in a significantly longer block interval,
> > which
> > also means a higher per-block transaction volume, which could cause the
> > block
> > size limit to legitimately be hit much sooner than expected. Furthermore,
> > due
> > to difficulty adjustment being measured exclusively in blocks, the time
> > until
> > it adjusts to compensate would be prolonged.
> >
> > For example, if 50% of miners dropped off the network, blocks would be
> > every
> > 20 minutes on average and contain double the transactions they presently
> > do.
> > Even double would be approximately 850-900k, which potentially bumps up
> > against the hard limit when empty blocks are taken into consideration. This
> > situation would continue for a full month if no changes are made. If more
> > miners drop off the network, most of this becomes linearly worse, but due
> > to
> > hitting the block size limit, the backlog would grow indefinitely until the
> > adjustment occurs.
> >
> > To alleviate this risk, it seems reasonable to propose a hardfork to the
> > difficulty adjustment algorithm so it can adapt quicker to such a
> > significant
> > drop in mining rate. BtcDrak tells me he has well-tested code for this in
> > his
> > altcoin, which has seen some roller-coaster hashrates, so it may even be
> > possible to have such a proposal ready in time to be deployed alongside
> > SegWit
> > to take effect in time for the upcoming subsidy halving. If this slips, I
> > think it may be reasonable to push for at least code-readiness before July,
> > and possibly roll it into any other hardfork proposed before or around that
> > time.
> >
> > I am unaware of any reason this would be controversial, so if anyone has a
> > problem with such a change, please speak up sooner rather than later. Other
> > ideas or concerns are of course welcome as well.
> >
> > Thanks,
> >
> > Luke
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >

> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------

On Jun 29, 2016, at 3:01 AM, Gregory Maxwell <greg@xiph.org> wrote:
> 
>> On Tue, Jun 28, 2016 at 11:33 PM, Eric Voskuil <eric@voskuil.org> wrote:
>> I don't follow this comment. The BIP aims quite clearly at "SPV" wallets as its justifying scenario.
> 
> It cites SPV as an example, doesn't mention bloom filters.. and sure-- sounds like the bip text should make the

"MOTIVATION:
The Bitcoin network does not encrypt communication between peers today. This opens up security issues (eg: traffic manipulation by others) and allows for mass surveillance / analysis of bitcoin users. Mostly this is negligible because of the nature of Bitcoins trust model, however for SPV nodes this can have significant privacy impacts [1] and could reduce the censorship-resistance of a peer."

This is not an example, this is the exception that is described as "significant" in comparison to the other issues, which are described as "negligible".

The Bloom filters messages are of course the unique aspects of the protocol as it pertains to "SPV".

The RISKS section declares that the BIP cannot prevent MITM attacks and that "identity authentication" will  be defined in a forthcoming BIP.

The obvious implication (accepted by the author) is that authentication is required to prevent a MITM attack, and furthermore establishment of identity will be required to ensure that the authenticated party is not a bad actor.

>>> Without something like BIP151 network participants cannot have privacy for the transactions they originate within the protocol against network observers.
>> 
>> And they won't get it with BIP151 either. Being a peer is easier than observing the network.
> 
> Not passively, undetectable, and against thousands of users at once at low cost.

This is a straw man, as the BIP does not state that its objective is to moderately raise the cost of passive attack against large numbers of users.

It is also a red herring, as passivity is not itself a benefit. It implies that the attack is easier and therefore less costly. But a trivial active attack may be a larger security problem than a complex passive attack. Attacks against privacy under this BIP (and with authentication) can be carried out by passively monitoring traffic and operating one or more nodes. Operating a node may be considered "active" because the node communicates, but technically it is not. In either case the activeness itself hardly raises the difficulty, especially for a global (thousands of users) passive attacker.

Depending on the attacker, cost may not be an issue at all, so raising it can have zero effect. Certainly we are not talking about prohibitive (cryptographically hard) cost. Raising the cost *any* amount is not likely a reasonable cost-benefit tradeoff.

Privacy attacks would remain entirely undetectable under this proposal, and under any additional proposal that required authentication in the absence of identity. Only with all users of the network identified as "good" would such proposals be effective. Until that point any bad actors can become an integral part of the network. I will investigate the question of identity in a follow-up to an independent post.

>> If one can observe the encrypted traffic one can certainly use a timing attack to determine what the node has sent.
> 
> Not against Bitcoin Core, transactions are batched and relayed in
> sorted order.  (obviously there are limits at what this provides;
> ironically, the lack of link encryption has been used to argue against
> privacy preserving relay behavior)

It cannot be both impossible ("not against Bitcoin Core") and limited in effectiveness ("obviously there are limits").

We should be clear at this point that the transaction-posting security provided against a privacy attack, based on the assumption of "good" (identified) peers in the first few hops, derives entirely from the ability of the good peers to break the timing attack, which is itself "limited".

This is a compound pair of weak assumptions, that to be made stronger will require widespread use of identity (not just authentication).

The proliferation of node identity is my primary concern - this relates to privacy and the security of the network. Secondarily I am concerned about users operating under a false assumption about the strength of privacy. Thirdly I am concerned about the risk of vulnerability introduced by the integration into the P2P network layer of an totally new network security scheme. Fourthly I'm concerned about the cost of the above based on the belief that the benefit may not be material and that it may lead to increased centralization.

>>> Even if, through some extraordinary effort, their own first hop is encrypted, unencrypted later hops would rapidly
>>> expose significant information about transaction origins in the network.
>> 
>> As will remain the case until all connections are encrypted and authenticated, and all participants are known to be good guys. Starting to sound like PKI?
> 
> Huh? The first and subsequent hops obscures the origin and timing.

Described as "limited" in effectiveness, and clearly useful only if these hops are not attacker nodes.

So back to my comment on how we maintain a pool of "good" nodes for people to connect to, and raising the question of how effective is this strategy (which is itself unspecified and so cannot be assumed to even exist in the context of the BIP).

>>> Without something like BIP151 authenticated links are not possible, so
>>> manually curated links (addnode/connect) cannot be counted on to provide protection against partitioning sybils.
>> 
>> If we trust the manual links we don't need/want the other links. In fact retaining the other links enables the attack you described above. Of course there is no need to worry about Sybil attacks when all of your peers are authenticated. But again, let us not ignore the problems of requiring all peers on the network be authenticated.
> 
> Don't need and want them for what?  For _partitioning_ resistance,
> you are not partitioned if you have one honest connection to the
> functional network. Additional peers purely reduce your partition vulnerability-- so long as an active network attacker isn't
> intercepting all your connections out.

Don't want them as peers for the purpose of tx relay. As I said this, "enables the attack you described above."

> For privacy, you have improve transaction privacy so long as your
> transaction isn't initially relayed to a malicious peer-- but
> malicious peers can lie further out because transit nodes obscure the
> order of message creation.  Bitcoin Core currently relays transactions
> first and more frequently to outbound and whitelisted peers.

This whitelisting is simply a stand-in for a more formal identity system. One doesn't whitelist anonymous peers, one whitelists peers controlled by trusted parties. Preferring trusted peers is another aspect of trying to break the timing attack. So I would lump this under the same analysis as above (batching).

>> Maybe I was insufficiently explicit. By "relies on identity" I meant that the BIP is not effective without it. I did not mean to imply that the BIP itself implements an identity scheme. I thought this was clear from the context.
> 
> I understood that, but my point was that Bitcoin cannot be used at all_unless users have secure communication channels to share addresses.

This is true but not relevant. The parties with whom we transact are not in the same space as the nodes with which we connect. The fact that I am face-to-face with a counterparty does not help me find a "good" node, nor does my ability to PGP email a payment address or to send a stealth address in the clear.

But the fact that you raise this point is itself instructive. The solution that was devised to resolve the problem of verifying that a counterparty is who one thinks it is ended up being based on the use of certificate authorities - despite the fact the the BIP did not require this. Some people consider this extremely dangerous for Bitcoin, enough so that Peter Todd recently proposed scrapping the BIP.

It's not clear to me how the Bitcoin community intends to establish what nodes are good nodes. But one thing is certain, any anonymous node may be an undetectable attacker.

>> then there is no reason to expect any effective improvement, since nodes will necessarily have to connect with anonymous peers.
> 
> They're not required to _only_ connect with anonymous peers. And partition resistance requires that you have any one good link.

As a minimum requirement, it implies that only need only to connect to one or more "good" peers. Anonymous peers are gravy for partition resistance, yet they are potential attackers for tx tainting. In other words the logical topology is to only connect to good peers. That is a problem.

>> Anyone with a node and the ability to monitor traffic should remain very effective.
> 
> Not via passive observation.

See above commentary on the irrelevance of this distinction.

>> Defining an auth implementation is not a hard problem, nor is it the concern I have raised.
> 
> Glad you agree.

I don't get your point here. It seems like you are just trying to antagonize.

> We seem to be looping now. Feel free to not implement this proposal,

At this point I think it's fair for me to say that nobody needs your permission.

> no one suggests making it mandatory.

Have you ever debated an optional feature proposal?

e

-------------------------------------
I wrote a bip last year about extended transaction information.  The idea
was to include the scriptPubKey that was being spent along with
transactions.

https://github.com/TierNolan/bips/blob/extended_transactions/bip-etx.mediawiki

This makes it easier possible to verify the transactions locally.  An
extended transaction would contain the current transaction and also the
CTxOuts that are being spent.

For each entry in the UTXO set, a node could store

UTXO_hash = hash(txid_parent | n | CTxOut)

Witness transactions will do something similar.  I wonder if it would be
possible to include the CTxOut for each input that isn't a segregated
witness output, as part of the witness data.  Even for witness data, it
would be good to commit to the value of the output as part of the witness.

There was a suggestion at one of the conferences to have the witness data
include info about the block height/index of the output that each input is
spending.

The effect of this change is that nodes would only have to store the
UTXO_hashes for each UTXO value in the database.  This would make it much
more efficient.

It would also make it easier to create a simple consensus library.  You
give the library the transaction and the witness and it returns the
UTXO_hashes that are spent, the UTXO_hashes that are created, the fee,
sigops and anything that needs to be summed.

Validating a block would mostly (famous last words) mean validating the
transactions in the block and then adding up the totals.

The advantage of including the info with the transactions is that it saves
each node having to include a lookup table to find the data.

-------------------------------------
 

https://github.com/bitcoin/bips/pull/317

ABSTRACT

This document specifies a proposed change to the semantics of the sign
bit of the "version" field in Bitcoin block headers, as a mechanism to
indicate a hardfork is deployed. It alleviates certain risks related to
a hardfork by introducing an explicit "point of no return" in the
blockchain. This is a general mechanism which should be employed by any
planned hardfork in the future. 

 [1]MOTIVATION

Hardforks in Bitcoin are usually considered as difficult and risky,
because: 

 	* Hardforks require not only support of miners, but also, most
importantly, supermajority support of the Bitcoin economy. As a result,
softfork deployment mechanisms described in BIP 34 [2] or BIP 9 [3] are
not enough for introducing hardforks safely.
 	* Full nodes and SPV nodes following original consensus rules may not
be aware of the deployment of a hardfork. They may stick to an
economic-minority fork and unknowingly accept devalued legacy tokens.
 	* In the case which the original consensus rules are also valid under
the new consensus rules, users following the new chain may unexpectedly
reorg back to the original chain if it grows faster than the new one.
People may find their confirmed transactions becoming unconfirmed and
lose money.

The first issue involves soliciting support for a hardfork proposal,
which is more a political topic than a technical one. This proposal aims
at alleviating the risks related to the second and third issues. It
should be employed by any planned hardfork in the future.

 [4]DEFINITIONS

See BIP99 [5] 

 [6]SPECIFICATION

HARDFORK BIT The sign bit in nVersion is defined as the hardfork bit.
Currently, blocks with this header bit setting to 1 are invalid, since
BIP65 [7] interprets nVersion as a signed number and requires it to be ≥
4. Among the 640 bits in the block header, this is the only one which is
fixed and serves no purpose, and therefore the best way to indicate the
deployment of a hardfork. 

FLAG BLOCK Any planned hardfork must have one and only one flag block
which is the "point of no return". To ensure monotonicity, flag block
should be determined by block height, or as the first block with
GetMedianTimePast() greater than a threshold. Other mechanisms could be
difficult for SPV nodes to follow. The height/time threshold could be a
predetermined value or relative to other events (e.g. 10000 blocks / 100
days after 95% of miner support). The exact mechanism is out of the
scope of this BIP. No matter what mechanism is used, the threshold is
consensus critical. It must be publicly verifiable with only blockchain
data, and preferably SPV-friendly (i.e. verifiable with block headers
only, without downloading any transaction). 

Flag block is constructed in a way that nodes with the original
consensus rules must reject. On the other hand, nodes with the new
consensus rules must reject a block if it is not a flag block while it
is supposed to be. To achieve these goals, the flag block must 

 	* have the hardfork bit setting to 1, and
 	* follow any other rules required by the hardfork

If these conditions are not fully satisfied, upgraded nodes shall reject
the block.

The hardfork bit must be turned off in the successors of the flag block,
until the deployment of the next hardfork. 

Although a hardfork is officially deployed when flag block is generated,
the exact behavioural change is out of the scope of this BIP. For
example, a hardfork may not be fully active until certain time after the
flag block. 

CONCURRENT HARDFORK PROPOSALS To avoid confusion and unexpected
behaviour, a flag block should normally signify the deployment of only
one hardfork. Therefore, a hardfork proposal has to make sure that its
flag block threshold is not clashing with other ongoing hardfork
proposals. 

In the case that the version bits mechanism is used in deploying a
hardfork, height of the flag block should take a value of32N + B, where
N is a positive integer and B is the position of bit B defined in BIP9
[8]. This guarantees that no clash may happen with another hardfork
proposal using BIP9. 

UNCONTROVERSIAL SUBTLE HARDFORKS Hardforks may sometimes be totally
uncontroversial and make barely noticeable change (BIP50 [9], for
example). In such cases, the use of hardfork bit may not be needed as it
may cause unnecessary disruption. The risk and benefit should be
evaluated case-by-case. 

AUTOMATIC WARNING SYSTEM When a flag block for an unknown hardfork is
found on the network, full nodes and SPV nodes should alert their users
and/or stop accepting/sending transactions. It should be noted that the
warning system could become a denial-of-service vector if the attacker
is willing to give up the block reward. Therefore, the warning may be
issued only if a few blocks are built on top of the flag block in a
reasonable time frame. This will in turn increase the risk in case of a
real planned hardfork so it is up to the wallet programmers to decide
the optimal strategy. Human warning system (e.g. the emergency alert
system in Bitcoin Core) could fill the gap. 

 [10]COMPATIBILITY

As a mechanism to indicate hardfork deployment, this BIP breaks backward
compatibility intentionally. However, without further changes in the
block header format, full nodes and SPV nodes could still verify the
Proof-of-Work of a flag block and its successors. 

HARDFORK INVOLVING CHANGE IN BLOCK HEADER FORMAT If a hardfork involves
a new block header format, the original format should still be used for
the flag block and a reasonable period afterwards, to make sure existing
nodes realize that an unknown hardfork has been deployed. 

VERSION BITS This proposal is also compatible with the BIP9. The version
bits mechanism could be employed to measure miner support towards a
hardfork proposal, and to determine the height or time threshold of the
flag block. Also, miners of the flag block may still cast votes for
other concurrent softfork or hardfork proposals as normal. 

POINT OF NO RETURN After the flag block is generated, a miner may
support either the original rules or the new rules, but not both. It is
not possible for miners in one fork to attack or overtake the other fork
without giving up the mining reward of their preferred fork. 

 [11]COPYRIGHT

This document is placed in the public domain. 

Links:
------
[1]
https://github.com/jl2012/bips/blob/hardforkbit/hardforkbit.mediawiki#motivation
[2] https://github.com/jl2012/bips/blob/hardforkbit/bip-0034.mediawiki
[3] https://github.com/jl2012/bips/blob/hardforkbit/bip-0009.mediawiki
[4]
https://github.com/jl2012/bips/blob/hardforkbit/hardforkbit.mediawiki#definitions
[5] https://github.com/jl2012/bips/blob/hardforkbit/bip-0099.mediawiki
[6]
https://github.com/jl2012/bips/blob/hardforkbit/hardforkbit.mediawiki#specification
[7] https://github.com/jl2012/bips/blob/hardforkbit/bip-0065.mediawiki
[8]
https://github.com/jl2012/bips/blob/hardforkbit/bip-0009.mediawiki#Mechanism
[9] https://github.com/jl2012/bips/blob/hardforkbit/bip-0050.mediawiki
[10]
https://github.com/jl2012/bips/blob/hardforkbit/hardforkbit.mediawiki#compatibility
[11]
https://github.com/jl2012/bips/blob/hardforkbit/hardforkbit.mediawiki#copyright
-------------------------------------
You could say 256 bit ECDSA is overkill lets go to 160 equivalently.
Saves even more bytes.

The problem with arguing down is where to stop.

As Matt said these things dont degrade gracefully so a best practice
is to aim for a bit of extra margin.

256-bit is quite common at this point since AES, SHA256 etc even in
things with much less at stake than Bitcoin.

You could send the compressed (unhashed) pubkey then there's no hash
(and omit it from the sig).  Greg had mentioned that in the past.

I think it might be possible to do both (reclaim the hash bits in the
serialisation of the pub key).

Adam

On 7 January 2016 at 20:02, Gavin Andresen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I'm hoisting this from some private feedback I sent on the segregated
> witness BIP:
>
> I said:
>
> "I'd also use RIPEMD160(SHA256()) as the hash function and save the 12
> bytes-- a successful preimage attack against that ain't gonna happen before
> we're all dead. I'm probably being dense, but I just don't see how a
> collision attack is relevant here."
>
> Pieter responded:
>
> "The problem case is where someone in a contract setup shows you a script,
> which you accept as being a payment to yourself. An attacker could use a
> collision attack to construct scripts with identical hashes, only one of
> which does have the property you want, and steal coins.
>
> So you really want collision security, and I don't think 80 bits is
> something we should encourage for that. Normal pubkey hashes don't have that
> problem, as they can't be constructed to pay to you."
>
> ... but I'm unconvinced:
>
> "But it is trivial for contract wallets to protect against collision
> attacks-- if you give me a script that is "gavin_pubkey CHECKSIG
> arbitrary_data OP_DROP" with "I promise I'm not trying to rip you off, just
> ignore that arbitrary data" a wallet can just refuse. Even more likely, a
> contract wallet won't even recognize that as a pay-to-gavin transaction.
>
> I suppose it could be looking for some form of "gavin_pubkey
> somebody_else_pubkey CHECKMULTISIG ... with the attacker using
> somebody_else_pubkey to force the collision, but, again, trivial contract
> protocol tweaks ("send along a proof you have the private key corresponding
> to the public key" or "everybody pre-commits pubkeys they'll use at protocol
> start") would protect against that.
>
> Adding an extra 12 bytes to every segwit to prevent an attack that takes
> 2^80 computation and 2^80 storage, is unlikely to be a problem in practice,
> and is trivial to protect against is the wrong tradeoff to make."
>
> 20 bytes instead of 32 bytes is a savings of almost 40%, which is
> significant.
>
> The general question I'd like to raise on this list is:
>
> Should we be worried, today, about collision attacks against RIPEMD160 (our
> 160-bit hash)?
>
> Mounting a successful brute-force collision attack would require at least
> O(2^80) CPU, which is kinda-sorta feasible (Pieter pointed out that Bitcoin
> POW has computed more SHA256 hashes than that). But it also requires O(2^80)
> storage, which is utterly infeasible (there is something on the order of
> 2^35 bytes of storage in the entire world).  Even assuming doubling every
> single year (faster than Moore's Law), we're four decades away from an
> attacker with THE ENTIRE WORLD's storage capacity being able to mount a
> collision attack.
>
>
> References:
>
> https://en.wikipedia.org/wiki/Collision_attack
>
> https://vsatglobalseriesblog.wordpress.com/2013/06/21/in-2013-the-amount-of-data-generated-worldwide-will-reach-four-zettabytes/
>
>
> --
> --
> Gavin Andresen
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
On Mon, Jun 20, 2016 at 05:33:32PM +0000, Erik Aronesty via bitcoin-dev wrote:
> BIP 0070 has been a a moderate success, however, IMO:
> 
> - protocol buffers are inappropriate since ease of use and extensibility is
> desired over the minor gains of efficiency in this protocol.  Not too late
> to support JSON messages as the standard going forward
> 
> - problematic reliance on merchant-supplied https (X509) as the sole form
> of mechant identification.   alternate schemes (dnssec/netki), pgp and
> possibly keybase seem like good ideas.   personally, i like keybase, since
> there is no reliance on the existing domain-name system (you can sell with
> a github id, for example)
> 
> - missing an optional client supplied identification

Note that "client supplied identification" is being pushed for AML/KYC
compliance, e.g. Netki's AML/KYC compliance product:

http://www.coindesk.com/blockchain-identity-company-netki-launch-ssl-certificate-blockchain/

This is an extremely undesirable feature to be baking into standards given it's
negative impact on fungibility and privacy; we should not be adopting standards
with AML/KYC support, for much the same reasons that the W3C should not be
standardizing DRM.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Jun 23, 2016 12:56, "Peter Todd via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> In any case, I'd strongly argue that we remove BIP75 from the bips
repository,
> and boycott wallets that implement it. It's bad strategy for Bitcoin
developers
> to willingly participate in AML/KYC, just the same way as it's bad for
Tor to
> add wiretapping functionality, and W3C to support DRM tech. The minor
tactical
> wins you'll get our of this aren't worth it.

I hope you're not seriously suggesting to censor a BIP because you feel it
is a bad idea.

-- 
Pieter

-------------------------------------
On 05/03/2016 12:13 AM, lf-lists at mattcorallo.com (Matt Corallo) wrote:
> Hi all,
> 
> The following is a BIP-formatted design spec for compact block relay
> designed to limit on wire bytes during block relay. You can find the
> latest version of this document at
> https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.

Hi Matt,

thank you for working on this!

> ===New data structures===
> Several new data structures are added to the P2P network to relay
> compact blocks: PrefilledTransaction, HeaderAndShortIDs,
> BlockTransactionsRequest, and BlockTransactions. Additionally, we
> introduce a new variable-length integer encoding for use in these data
> structures.
> 
> For the purposes of this section, CompactSize refers to the
> variable-length integer encoding used across the existing P2P protocol
> to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.

This is a not, but I think it's a bit strange to have two separate
variable length integers in the same specification. I understand is one
is already the default for variable-length integers currently, and there
are reasons to use the other one for efficiency reasons in some places,
but perhaps we should aim to get everything using the latter?

> ====New VarInt====
> Variable-length integers: bytes are a MSB base-128 encoding of the number.
> The high bit in each byte signifies whether another digit follows. To make
> sure the encoding is one-to-one, one is subtracted from all but the last
> digit.

Maybe it's worth mentioning that it is based on ASN.1 BER's compressed
integer format (see
https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf
section 8.1.3.5), though with a small modification to make every integer
have a single unique encoding.

> ====HeaderAndShortIDs====
> A HeaderAndShortIDs structure is used to relay a block header, the short
> transactions IDs used for matching already-available transactions, and a
> select few transactions which we expect a peer may be missing.
> 
> |shortids||List of uint64_ts||8*shortids_length bytes||Little
> Endian||The short transaction IDs calculated from the transactions which
> were not provided explicitly in prefilledtxn

I tried to derive what length of short ids is actually necessary (some
write-up is on
https://gist.github.com/sipa/b2eb2e486156b5509ac711edd16153ed but it's
incomplete).

For any reasonable numbers I can come up with (in a very wide range),
the number of bits needed is very well approximated by:

  log2(#receiver_mempool_txn * #block_txn_not_in_receiver_mempool /
acceptable_per_block_failure_rate)

For example, with 20000 mempool transactions, 2500 transactions in a
block, 95% hitrate, and a chance of 1 in 10000 blocks to fail to
reconstruct, needed_bits = log2(20000 * 2500 * (1 - 0.95) / 0.0001) =
34.54, or 5 byte txids would suffice.

Note that 1 in 10000 failures may sound like a lot, but this is for each
individual connection, and since every transmission uses separately
salted identifiers, occasional failures should not affect global
propagation. Given that transmission failures due to timeouts, network
connectivity, ... already occur much more frequently than once every few
gigabytes (what 10000 blocks corresponds to), that's probably already
more than enough.

In short: I believe 5 or 6 byte txids should be enough, but perhaps it
makes sense to allow the sender to choose (so he can weigh trying
multiple nonces against increasing the short txid length).

> ====Short transaction IDs====
> Short transaction IDs are used to represent a transaction without
> sending a full 256-bit hash. They are calculated by:
> # single-SHA256 hashing the block header with the nonce appended (in
> little-endian)
> # XORing each 8-byte chunk of the double-SHA256 transaction hash with
> each corresponding 8-byte chunk of the hash from the previous step
> # Adding each of the XORed 8-byte chunks together (in little-endian)
> iteratively to find the short transaction ID

An alternative would be using SipHash-1-3 (a form of SipHash with
reduced iteration counts; the default is SipHash-2-4). SipHash was
designed as a Message Authentication Code, where the security
requirements are much stronger than in our case (in particular, we don't
care about observers being able to finding the key, as the key is just
public knowledge here). One of the designers of SipHash has commented
that SipHash-1-3 for collision resistance in hash tables may be enough:
https://github.com/rust-lang/rust/issues/29754#issuecomment-156073946

Using SipHash-1-3 on modern hardware would take ~32 CPU cycles per txid.

> ===Implementation Notes===

There are a few more heuristics that MAY be used to improve performance:

* Receivers should treat short txids in blocks that match multiple
mempool transactions as non-matches, and request the transactions. This
significantly reduces the failure to reconstruct.

* When constructing a compact block to send, the sender can verify it
against its own mempool to check for collisions, and if so, choose to
either try another nonce, or increase the short txid length.

Cheers,

-- 
Pieter


-------------------------------------
Hi

> I have some experience with hardware wallet development and its
> integration and I know it's a mess. But it is too early to define such
> rigid standards yet. Also, TREZOR concept (device as a server and the
> primary source of workflow management) goes directly against your
> proposal of wallet software as an workflow manager. So it is clear NACK
> for me.

The current question – as already mentioned – is we ACK to work together
on a signing protocol or if we NACK this before we even have started.

I'm not saying that the draft proposal I made is the way to go, I'm
happy to NACK it myself in favor of a better proposal.

I strongly recommend to work together on a standard that will have one
central winner: the end user.

</jonas>


-------------------------------------
>This is already possible. Just nLockTime your withdrawls for some future
block. Don't sign any transaction that isn't nLockTime'd at least N blocks
beyond the present tip.

The problem with nLockTimed transactions is a centralized exchange isn't
going to know ahead of time where those locked transactions need to go or
the amount that needs to be signed so you will end up having to keep the
private key around. If there was a way to create these transactions offline
with special SIG_HASH flags (and I don't think there is) there's nothing
about nLockTime that forces that the transactions be broadcast straight
away and plus: since the TXs aren't confirmed until the lock-time expires
they can be overwritten anyway.

I think given the requirements that a centralized exchange has: TierNolan's
idea is the best so far. Essentially, you have a new type of output script
that forces the redeemer to use a designated output script template in the
redeeming transaction, meaning that you can actually force people to send
coins into another transaction with "relative lock-timed" outputs. The new
transaction can then only be redeemed at the destination after the relative
lock-time expires OR it can be moved before-hand to a designated off-line
recovery address. This is much better than creating a new transaction
system, IMO.

>And the refund TXN would need to be able to go to a new address entirely.

Agreed.

On Thu, Aug 4, 2016 at 1:49 PM, Andrew Johnson <andrew.johnson83@gmail.com>
wrote:

> "This is already possible. Just nLockTime your withdrawls for some future
> block. Don't sign any transaction that isn't nLockTime'd at least N blocks
> beyond the present tip."
>
> This would have prevented the Bitfinex hack if BitGo did this, but it
> wouldn't have helped if the Bitfinex offline key had been compromised
> instead of BitGo doing the 2nd sig.  In the BFX hack the TXNs were signed
> by Bitfinex's hot key and BitGo's key, they required 2 of 2.
>
> If I'm understanding correctly, what Matthew is proposing is a new type of
> UTXO that is only valid to be spent as an nLockTime transaction and can be
> reversed by some sort of RBF-type transaction within that time period, I
> believe.
>
> But I don't think this will work. What do you do if the keys are
> compromised?  What's to stop the attacker from locking the coins up
> indefinitely by repeatedly broadcasting a refund transaction each time you
> try to spend to an uncompromised address?
>
> You'd need a third distinct key required for the refund TXN that's
> separate from the keys used to sign the initial nLockTime TXN.  And the
> refund TXN would need to be able to go to a new address entirely.
>
> On Aug 3, 2016 11:28 PM, "Luke Dashjr via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Wednesday, August 03, 2016 6:16:20 PM Matthew Roberts via bitcoin-dev
>> wrote:
>> > In light of the recent hack: what does everyone think of the idea of
>> > creating a new address type that has a reversal key and settlement layer
>> > that can be used to revoke transactions?
>>
>> This isn't something that makes sense at the address, since it represents
>> the
>> recipient and not the sender. Transactions are not sent from addresses
>> ever.
>>
>> > You could specify so that transactions "sent" from these addresses must
>> > receive N confirmations before they can't be revoked, after which the
>> > transaction is "settled" and the coins become redeemable from their
>> > destination output. A settlement phase would also mean that a
>> transaction's
>> > progress was publicly visible so transparent fraud prevention and
>> auditing
>> > would become possible by anyone.
>>
>> This is already possible. Just nLockTime your withdrawls for some future
>> block. Don't sign any transaction that isn't nLockTime'd at least N blocks
>> beyond the present tip.
>>
>> Luke
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>

-------------------------------------


On 10/03/16 15:59, Jorge Timón wrote:
>
>
> On Mar 10, 2016 16:51, "Mustafa Al-Bassam via bitcoin-dev"
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>
> > I think in general this sounds like a good definition for a hard-fork
> > becoming active. But I can envision a situation where someone will try
> > to be annoying about it and point to one instance of one buyer and one
> > seller using the blockchain to buy and sell from each other, or set
> one up.
>
> And all the attacker will achieve is preventing a field on a text file
> on github from moving from "active" to "final".
> Seems pretty stupid. Why would an attacker care so much about this? Is
> there any way the attacker can make gains or harm bitcoin with this
> attack?
>
It's extremely naive to think that just because you can't think of an
incentive for a reason for an attack to do this, an attacker will never
to do this. There are many people that would be willing to spend some
time to cause some trouble for the enjoyment of it, if the attack is
free to execute.

The fact that it takes very little time and effort to prevent a BIP from
reaching final status, means that in an base of millions of users it's
guaranteed that some disgruntled or bored person out there will attack
it, even if it's for the lulz.

To reasonably expect that any hark fork - including an uncontroversial
one - will be adapted by every single person in a ecosystem of millions
of people, is wishful thinking and the BIP may as well say "hard fork
BIPs shall never reach final status."

-------------------------------------
On Sun, Mar 27, 2016 at 5:49 AM Jonas Schnelli via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> >     I guess my question didn't get across.
> >
> >     Why would you want to make your usecase do connections over the
> >     peer2peer
> >     (net.cpp) connection at all?
> >
> >     Mixing messages that are being sent to everyone and encrypted
> >     messages is
> >     asking for trouble.
> >     Making your private connection out-of-band would work much better.
> >
> >
> > I agree doing it out-of-band is the easiest solution for people who need
> > this privacy right now, but I do like the idea of adding this feature as
> > the number of SPV wallets is going to increase. I think the best way to
> > organize things would be to give encrypted messages their own port
> > number, similar to how http vs. https works.
>
> I'm not sure if different ports would make sense. I can't see a benefit
> (happy if someone can convince me).
> How would this affect p2p address management (address relay)? Wouldn't
> this require to extend the current address message to support two port
> numbers?
>
> I'm assuming clients that connect with encryption don't want to use
unencrypted connections, and are only interested in other peers that
support encryption. From their perspective, it is quite inefficient to get
a generic list of peers and then have to connect to each one searching for
those that accept encryption. If we use port numbers, we can assume any
connection that comes on the encrypted port is only interested in encrypted
communication, so a getaddr to an encrypted port would only return a list
of other encryption-capable peers.

This isn't an issue if the plan is to require all peers to support
encryption, and we assume the majority of the network will upgrade before
too long.


>
> > We don't want two networks to develop, separated by which nodes support
> > encryption and which don't, so ideally nodes would rebroadcast messages
> > they receive on both (encrypted and non-encrypted) channels. This would
> > essentially double the required bandwidth of the network, which is
> > something to think about.
>
> It can be the same "p2p network". The only difference would be, that
> once two peers has negotiated encryption, the whole traffic between
> _these two peers_, and _only_ these two pears, would be encrypted (would
> _not_ affect traffic to/from other peers).
>
>
You're right, there would not be an increase in bandwidth. Please forget I
said that :) But following the logic I wrote above, it would be possible
for peers to become segregated (those who require encryption would only
connect to each other). It wouldn't be a problem as long as there are
enough peers that provide both encrypted and non-encrypted connections; or,
as I said above, if we can assume every peer will support it. Maybe the
issues I'm thinking of are just growing pains that will be solved once the
majority of people upgrade?


> A simplified example:
> 1. Peer Alice connects to peer Bob
> 2. Alice asks Bob: "lets do encrypted communication, here is my session
> pubkey"
> 3. Bob also supports encryption and answers "Yes, let's do this, here is
> my session pubkey"
> 4. Alice tells Bob (encrypted now): "Perfect. Here I prove that I'm
> Alice by signing the session ID with my identity pubkey"
> 5. Bob checks his "authorized-peers" database and look-up Alices pubkey
> and verifies the signatures.
> 6. Bob tells Alice: "Good! I trust you now Alice, here is my identity
> pubkey with a signature of our session-ID"
> 7. Alice looks up Bobs pubkey in her "known-peers" database and verifies
> the signature.
> 8. Alice response to bob: "Perfect. Indeed, you are Bob!"
> ---
> At this point, the communication is encrypted and the identities has
> been verified (MITM protection).
>
>
> (simplified negotiation [only one-way, missing dh explanation, missing
> KDF, session-ID, cipher suite nego., missing re-keying, etc.])
>
>
> </jonas>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Thu, May 26, 2016 at 03:53:04AM +0000, Luke Dashjr via bitcoin-dev wrote:
> On Thursday, May 26, 2016 2:50:26 AM Nicolas Dorier via bitcoin-dev wrote:
> >   Author: Flavien Charlon <flavien@charlon.net>

What's the status of this BIP? Will it be assigned?

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
I agree that finding the right line is difficult and purposefully crippling
(too strong a term?) the software is not necessarily the best way to
encourage long term adoption.

For example, I ran version 0.3.x from July/August 2010 for several years on
a miner without upgrading to anything higher than the 0.3.24 release since
the usage pattern on that machine didn't require it.  It might have been to
the ~0.7.0 release, I am not sure when I finally upgraded it.  On a machine
that had my wallet, I kept it updated, but forcing upgrades may not be the
best plan given that hard forks should be few and far between.  Security
updates, are important, but leaving it up to the operator of the node to
determine when to upgrade is an important feature.

Chris


On Sun, Dec 18, 2016 at 5:34 AM, Matt Corallo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> One thing which hasn't been addressed yet in this thread is developer
> centralization. Unlike other applications we want to ensure that it's not
> only possible for users to refuse an upgrade, but easy. While this by no
> means lessens the retirement that users run up to date software for
> security reasons, finding the right line to draw is difficult.
>
> Matt
>
> On December 15, 2016 2:44:55 PM PST, Ethan Heilman via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> >I assume this has been well discussed in at some point in the Bitcoin
> >community, so I apologize if I'm repeating old ideas.
> >
> >Problem exploitable nodes:
> >It is plausible that people running these versions of bitcoind may not
> >be applying patches. Thus, these nodes may be vulnerable to known
> >exploits. I would hope none of these nodes are gateway nodes for
> >miners, web wallets or exchanges. How difficult would it be to crawl
> >the network to find vulnerable nodes and exploit them? What percentage
> >of the network is running vulnerable versions of bitcoind?
> >
> >Problem eclipsable nodes:
> >Currently a bitcoind node disconnects from any node with a version
> >below MIN_PEER_PROTO_VERSION. Such nodes become be ripe for an eclipse
> >attack because they are partitioned from the newer nodes, especially
> >when they are "freshly obsolete". I have not examined how protocol
> >versioning works in detail so I could be missing something.
> >
> >One option could be that after a grace period:
> >1. to still connect to obsolete nodes and even to transmit
> >blockheaders,
> >2. but to stop sending the full-blocks and transactions to these
> >nodes, thereby alerting the operator that something is wrong and
> >causing them to upgrade.
> >It may make sense to create this as a rule, if your longest chain
> >consists of only blockheaders and no one will tell you the
> >transactions for over 1000 blocks you are obsolete, spit out an error
> >message and shutdown.
> >
> >This would not address the issue of alt-coins which are forked from
> >old vulnerable versions of bitcoind, but that is probably out of
> >scope.
> >
> >On Thu, Dec 15, 2016 at 1:48 PM, Jorge Timón via bitcoin-dev
> ><bitcoin-dev@lists.linuxfoundation.org> wrote:
> >> On Thu, Dec 15, 2016 at 4:38 AM, Juan Garavaglia via bitcoin-dev
> >> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> >>> Older node versions may generate issues because some upgrades will
> >make
> >>> several of the nodes running older protocol versions obsolete and or
> >>> incompatible. There may be other hard to predict behaviors on older
> >versions
> >>> of the client.
> >>
> >> Hard to predict or not, you can't force people to run newer software.
> >>
> >>> In order to avoid such wide fragmentation of "Bitcoin Core” node
> >versions
> >>> and to help there be a more predictable protocol improvement
> >process, I
> >>> consider it worth it to analyze introducing some planned
> >obsolescence in
> >>> each new version. In the last year we had 4 new versions so if each
> >version
> >>> is valid for about 1 year (52560 blocks) this may be a reasonable
> >time frame
> >>> for node operators to upgrade. If a node does not upgrade it will
> >stop
> >>> working instead of participating in the network with an outdated
> >protocol
> >>> version.
> >>
> >> When you introduce anti-features like this in free software they can
> >> be trivially removed and they likely will.
> >>
> >>> These changes may also simplify the developer's jobs in some cases
> >by
> >>> avoiding them having to deal with ancient versions of the client.
> >>
> >> There's a simpler solution for this which is what is being done now:
> >> stop maintaining and giving support for older versions.
> >> There's limited resources and developers are rarely interested in
> >> fixing bugs for very old versions. Users shouldn't expect things to
> >be
> >> backported to old versions (if developers do it and there's enough
> >> testing, there's no reason not to do more releases of old versions,
> >it
> >> is just rarely the case).
> >> _______________________________________________
> >> bitcoin-dev mailing list
> >> bitcoin-dev@lists.linuxfoundation.org
> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >_______________________________________________
> >bitcoin-dev mailing list
> >bitcoin-dev@lists.linuxfoundation.org
> >https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------


On 3/2/2016 12:53 PM, Gregory Maxwell via bitcoin-dev wrote:
> What you are proposing makes sense only if it was believed that a very
> large difficulty drop would be very likely.
> 
> This appears to be almost certainly untrue-- consider-- look how long
> ago since hashrate was 50% of what it is now, or 25% of what it is
> now-- this is strong evidence that supermajority of the hashrate is
> equipment with state of the art power efficiency.

I don't understand the relevance of this.

In my view, we would prefer miners to invest in hardware just a mere
2016 blocks away from the halving. Instead, they've made them too soon.
Assuming that miners are already located in low-power-cost areas, the
difficulty will be quickly rising to compensate for "state of the art
power efficiency".

So it will have canceled out by July.

If anything, the more efficient miners become today, the bigger our
potential problem in July, because chip-manufacturers may have used up
all of the easy efficiency-increasing moves, such that investments do
not take place in June.

Paul


-------------------------------------
2016-01-28 13:00 GMT+01:00  Warren Togami Jr. <wtogami@gmail.com>:
> Myself and a few other developers think proposals like BIP100 where the
> block size is subject to a vote by the miners is suboptimal because this
> type of vote is costless.

The cost of the vote is completely irrelevant. What matters are the
resulting block sizes and transaction fees. Assuming rational,
profit-maximizing miners, BIP100 would allow them to effectively
enforce a cartel and to set block sizes (and thereby indirectly also
fees) at monopoly price levels. Charging something for a vote would
not affect that equilibrium and thus also neither affect block sizes
nor fees. Also note that monopoly prices are always at least as high
as competitive market prices. In other words: the transaction fees
that emerge under BIP100 will be higher than those that would emerge
with a flex cap mechanism that is based on the total marginal costs of
the miners. If you do not believe that, I'll happily go into the gory
details.

> You were astute in recognizing in your post it's
> a good thing to somehow align the global marginal cost with the miner's
> incentive.  I feel a costless vote is not great because it aligns only to
> the miner's marginal cost, and not the marginal cost to the entire flood
> network.  Flex Cap is superior as "vote" mechanism as there is an actual
> cost associated, allowing block size to grow with actual demand.

There are two types of flex cap mechanisms: First, there mechanisms
like the one I described previously which ensures that supply is based
on the actual costs of the miners. If done right, they can lead to a
competitive equilibrium with free market prices. Second, there are
flex cap mechanisms that simply replace todays centrally planned
constant cap with a centrally planned supply curve. If you believe in
central planning, that's ok. I for one prefer to avoid it. Also, it is
not much better than the constant cap, maybe even worse.

Note that neither type of flex cap adjusts to the marginal cost of the
entire network, simply because none of them can measure the cost of
running a full node, yet alone reliably detect the number of running
full nodes. Any attempt to do so would be futile anyway because it
would too easy to pretend-run full nodes in order to manipulate the
mechanism. When reasoning about full nodes, completely different
forces are at play. The only connection between full nodes and the fee
market is that larger blocks make it more expensive to run a full
node.

However, a holistic analysis must also reason about the benefits of
running a full node. I often see one-sided arguments saying that
increasing block sizes will make running one more expensive and thus
there will be fewer nodes. This logic is flawed because the economic
reasons for running a full node are not understood and taken into
account. An example reason could be the ability to monitor the network
and to verify transactions, which is very valuable to exchanges,
merchants and wallet services. To them, this value of running a full
node even grows with the number of customers. Thus, depending on the
circumstances, increase block sizes can counter-intuitively make it
more attractive to run a full node. The big picture from a systemic
perspective can look completely different than the conventional
micro-view that only sees first-order effects. Unfortunately, most
people are not great systems-thinkers.



2016-01-27 11:12 GMT+01:00 Luzius Meisser <luzius.meisser@gmail.com>:
> 2016-01-27 3:45 GMT+01:00 Warren Togami Jr. <wtogami@gmail.com>:
>> On Tue, Jan 26, 2016 at 9:42 AM, Luzius Meisser via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>> Idea: currently, the total amount of fees collected in a block is paid
>>> out in full to whoever mined that block. I propose to only pay out,
>>> say, 10% of the collected fees, and to add the remaining 90% to the
>>> collected fees of the next block. Thus, the payout to the miner
>>> constitutes a rolling average of collected fees from the current and
>>> past blocks.
>>
>> [...] Another major issue with mandatory sharing is
>> if the miner doesn't want to share, nothing stops them from taking payment
>> out-of-band and confirming the transaction with little or no fees visible in
>> the block.
>
> While I find the other points you raised debatable, the out-of-band
> argument looks strong enough to kill the idea. To work around it, one
> would need to create rules about the transactions that can be included
> in a block, for example by mandating that all included transactions
> must have a fee at least as high as 0.9 times the 5th percentile of
> the transactions in the previous 10 blocks. However, having to tell
> the miners what fees they are allowed to accept destroys some of the
> elegance of the idea. Maybe I should put it to rest for now and see if
> a more elegant solution comes to mind later.
>
>> While I don't agree with the rest of your logic, it is agreeable that you
>> care about aligning the miner's supply incentives with the global marginal
>> cost.  If you believe that is an important goal, you might like the Flex Cap
>> approach as presented by Mark Friedenbach at Scaling Bitcoin Hong Kong.
>> Under the general idea of the Flex Cap approach block size is no longer
>> fixed, it can be bursted higher on a per-block basis if the miner is willing
>> to defer a tiny portion of the current block subsidy to pay out to the miner
>> of later blocks.
>> [...]
>> Flex Cap is an area of ongoing research that I strongly believe would
>> benefit Bitcoin in the long-term.  For this reason it requires careful study
>> and simulations to figure out specifics.
>
> I agree that flex cap is promising. However, for it to be a viable
> long-term solution, it must not depend on significant block subsidies
> to work as the block subsidy will become less and less relevant over
> time.
>
> Picking up your thoughts, I guess this is how flex cap should be done:
> 1. There is a flexible block cap (e.g. 1 MB). This first MB is free to fill.
> 2. Miners can buy additional space for an exponentially increasing
> fee. For example, the first KiB might cost 200 Satoshis, the second
> KiB 400 Satoshis, the tenth KiB 102400 Satoshis etc.
> 3. The price of the purchased space is subtracted from the collected
> fees and added to the reward of the next block.
> 4. The amount miners are willing to spend on additional space allows
> to calculate the marginal costs of a transaction of a miner. For
> example, if a miner pays 6000 Satoshis to include a 1 KB transaction
> with a fee of 6100 Satoshis, the marginal costs must be below 100
> Satoshis, assuming a rational miner. This cost is multiplied by say 50
> to account for the costs of decentralization to get a global cost
> estimate of 5000 Satoshis per KB.
> 5. Every 1000 blocks or so, the basic cap is adjusted upwards or
> downwards (e.g. by 10%) depending on whether the average fees per KB
> were above or below the global cost estimate.
>
> Under such a scheme, prices should get very close to free market
> prices. However, ruthless competition can get ugly in markets where
> fixed costs dominate. We can currently witness this in the oil
> industry. Thus, from an economic point of view, it might be more
> advisable to simply let miners vote on block size, as has been
> proposed by others. The drawback of voting is that it allows miners to
> enforce a cartel among themselves and to charge monopoly prices
> instead of competitive prices. However, monopoly prices would already
> be much better than having an artificial cap.
>
> Warren, thank you for your thoughts! I appreciate the opportunity to
> discuss ideas at such a high level.
>
> --
> Luzius Meisser
> President of Bitcoin Association Switzerland
> MSc in Computer Science and MA in Economics



-- 
Luzius Meisser
luzius.meisser@gmail.com


-------------------------------------
Is there a way for Joe Mobile Wallet User to upload a set of N PaymentRequests 
authenticated by his key to an untrusted server, which encrypts and passes 
them on in response to InvoiceRequests? Or does this necessarily require the 
recipient to be online?

On Tuesday, March 01, 2016 6:58:16 PM Justin Newton via bitcoin-dev wrote:
> The following draft BIP proposes an update to the Payment Protocol.
> 
> Motivation:
> 
> The motivation for defining this extension to the BIP70 Payment Protocol is
> to allow 2 parties to exchange payment information in a permissioned and
> encrypted way such that wallet address communication can become a more
> automated process. Additionally, this extension allows for the requestor of
> a PaymentRequest to supply a certificate and signature in order to
> facilitate identification for address release. This also allows
> for automated creation of off blockchain transaction logs that are human
> readable, containing who you transacted with, in addition to the
> information that it contains today.
> 
> The motivation for this extension to BIP70 is threefold:
> 
> 1. Ensure that the payment details can only be seen by the participants in
> the transaction, and not by any third party.
> 2. Enhance the Payment Protocol to allow for store and forward servers in
> order to allow, for example, mobile wallets to sign and serve
> Payment Requests.
> 3. Allow a sender of funds the option of sharing their identity with the
> receiver. This information could then be used to:
> 
>         * Make bitcoin logs more human readable
>         * Give the user the ability to decide who to release payment
> details to
>         * Allow an entity such as a political campaign to ensure donors
> match regulatory and legal requirements
>         * Allow for an open standards based way for regulated financial
> entities to meet regulatory requirements
>         * Automate the active exchange of payment addresses, so static
> addresses and BIP32 X-Pubs can be avoided to maintain privacy
> and convenience
> 
> In short we wanted to make bitcoin more human, while at the same time
> improving transaction privacy.
> 
> Full proposal here:
> 
> https://github.com/techguy613/bips/blob/master/bip-invoicerequest-extension
> .mediawiki
> 
> We look forward to your thoughts and feedback on this proposal!
> 
> Justin


-------------------------------------
On 9/19/2016 10:56 AM, Peter Todd wrote:
> I should state that assumption more clearly.

Glad to get you thinking, and I need to change my suggestion.  The
catch-up formula is not applicable because it doesn't limit how long the
dishonest miners have to catch up.

Instead you want the probability that the honest miners can build a
chain N blocks long before the dishonest miners do the same, which is

CDF[Erlang(N, q) - Erlang(N, 1 - q), 0]

I have some apparatus for doing this numerically without simulation if
you're interested.



-------------------------------------
I have published a new version for BIP114 MAST. It's a bit more complicated with some new features:

1. It allows different parties in a contract not to expose their scripts to each other until redemption.

2. It includes a field to indicate the script language version so new opcodes could be added without touching the version byte nor the witness program.

You can find the updated BIP and code at:

https://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki

https://github.com/jl2012/bitcoin/tree/bip114v2


The old version:

https://github.com/bitcoin/bips/blob/7478ee3260c0d3c0cef39233931b307691764edc/bip-0114.mediawiki

https://github.com/jl2012/bitcoin/tree/segwit_mast
-------------------------------------
It would be a shame to prohibit someone from rewarding whoever mines their
transaction.  A good example would be a transaction designed to record some
information which is damning to powerful authorities, sort of like the
service cryptograffiti offers.  When we try to protect others by
prohibiting behavior we think is foolish, we may save some fools, but at
the same time, we hurt the best of us.

On Thu, Mar 3, 2016 at 7:36 AM, Jorge Timón <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> There's  an absurd fee (non-consensus) check already. Maybe that check can
> be improved, but probably the wallet layer is more appropriate for this.
> On Mar 3, 2016 16:23, "Henning Kopp via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Hi,
>> I think there is no need to do a hardfork for this. Rather it should
>> be implemented as a safety-mechanism in the client. Perhaps a warning
>> can pop up, if one of your conditions A) or B) is met.
>>
>> All the best
>> Henning Kopp
>>
>>
>> On Thu, Mar 03, 2016 at 05:02:11AM -0800, Alice Wonder via bitcoin-dev
>> wrote:
>> > I think the next hard fork should require a safety rule for TX fees.
>> >
>> >
>> https://blockchain.info/tx/6fe69404e6c12b25b60fcd56cc6dc9fb169b24608943def6dbe1eb0a9388ed08
>> >
>> > 15 BTC TX fee for < 7 BTC of outputs.
>> >
>> > Probably either a typo or client bug.
>> >
>> > My guess is the user was using a client that does not adjust TX fee, and
>> > needed to manually set it in order to get the TX in the block sooner,
>> and
>> > meant 15 mBTC or something.
>> >
>> > I suggest that either :
>> >
>> > A) TX fee may not be larger than sum of outputs
>> > B) TX fee per byte may not be larger than 4X largest fee per byte in
>> > previous block
>> >
>> > Either of those would have prevented this TX from going into a block.
>> >
>> > Many people I know are scared of bitcoin, that they will make a TX and
>> make
>> > a mistake they can't undo.
>> >
>> > Adding protections may help give confidence and there is precedence to
>> doing
>> > things to prevent typo blunders - a public address has a four byte
>> checksum
>> > to reduce the odds of a typo.
>> >
>> > This kind of mistake is rare, so a fix could be included in the coming
>> HF
>> > for the possible July 2017 block increase.
>> >
>> > Thank you for your time.
>> >
>> > Alice Wonder
>> > _______________________________________________
>> > bitcoin-dev mailing list
>> > bitcoin-dev@lists.linuxfoundation.org
>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >
>>
>> --
>> Henning Kopp
>> Institute of Distributed Systems
>> Ulm University, Germany
>>
>> Office: O27 - 3402
>> Phone: +49 731 50-24138
>> Web: http://www.uni-ulm.de/in/vs/~kopp
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto

-------------------------------------
Using the hash of multiple blocks does not make it any safer. The miner of the last block always determines the results, by knowing the hashes of all previous blocks.

> 
> == Security
> Pay-to-script-hash can be used to protect the details of contracts that use OP_PRANDOM from the prying eyes of miners. However, since there is also a non-zero risk that a participant in a contract may attempt to bribe a miner the inclusion of multiple block hashes as a source of randomness is a must. Every miner would effectively need to be bribed to ensure control over the results of the random numbers, which is already very unlikely. The risk approaches zero as N goes up.


-------------------------------------
# Motivation

UTXO growth is a serious concern for Bitcoin's long-term decentralization. To
run a competitive mining operation potentially the entire UTXO set must be in
RAM to achieve competitive latency; your larger, more centralized, competitors
will have the UTXO set in RAM. Mining is a zero-sum game, so the extra latency
of not doing so if they do directly impacts your profit margin. Secondly,
having possession of the UTXO set is one of the minimum requirements to run a
full node; the larger the set the harder it is to run a full node.

Currently the maximum size of the UTXO set is unbounded as there is no
consensus rule that limits growth, other than the block-size limit itself; as
of writing the UTXO set is 1.3GB in the on-disk, compressed serialization,
which expands to significantly more in memory. UTXO growth is driven by a
number of factors, including the fact that there is little incentive to merge
inputs, lost coins, dust outputs that can't be economically spent, and
non-btc-value-transfer "blockchain" use-cases such as anti-replay oracles and
timestamping.

We don't have good tools to combat UTXO growth. Segregated Witness proposes to
give witness space a 75% discount, in part of make reducing the UTXO set size
by spending txouts cheaper. While this may change wallets to more often spend
dust, it's hard to imagine an incentive sufficiently strong to discourage most,
let alone all, UTXO growing behavior.

For example, timestamping applications often create unspendable outputs due to
ease of implementation, and because doing so is an easy way to make sure that
the data required to reconstruct the timestamp proof won't get lost - all
Bitcoin full nodes are forced to keep a copy of it. Similarly anti-replay
use-cases like using the UTXO set for key rotation piggyback on the uniquely
strong security and decentralization guarantee that Bitcoin provides; it's very
difficult - perhaps impossible - to provide these applications with
alternatives that are equally secure. These non-btc-value-transfer use-cases
can often afford to pay far higher fees per UTXO created than competing
btc-value-transfer use-cases; many users could afford to spend $50 to register
a new PGP key, yet would rather not spend $50 in fees to create a standard two
output transaction. Effective techniques to resist miner censorship exist, so
without resorting to whitelists blocking non-btc-value-transfer use-cases as
"spam" is not a long-term, incentive compatible, solution.

A hard upper limit on UTXO set size could create a more level playing field in
the form of fixed minimum requirements to run a performant Bitcoin node, and
make the issue of UTXO "spam" less important. However, making any coins
unspendable, regardless of age or value, is a politically untenable economic
change.


# TXO Commitments

A merkle tree committing to the state of all transaction outputs, both spent
and unspent, we can provide a method of compactly proving the current state of
an output. This lets us "archive" less frequently accessed parts of the UTXO
set, allowing full nodes to discard the associated data, still providing a
mechanism to spend those archived outputs by proving to those nodes that the
outputs are in fact unspent.

Specifically TXO commitments proposes a Merkle Mountain Range¹ (MMR), a
type of deterministic, indexable, insertion ordered merkle tree, which allows
new items to be cheaply appended to the tree with minimal storage requirements,
just log2(n) "mountain tips". Once an output is added to the TXO MMR it is
never removed; if an output is spent its status is updated in place. Both the
state of a specific item in the MMR, as well the validity of changes to items
in the MMR, can be proven with log2(n) sized proofs consisting of a merkle path
to the tip of the tree.

At an extreme, with TXO commitments we could even have no UTXO set at all,
entirely eliminating the UTXO growth problem. Transactions would simply be
accompanied by TXO commitment proofs showing that the outputs they wanted to
spend were still unspent; nodes could update the state of the TXO MMR purely
from TXO commitment proofs. However, the log2(n) bandwidth overhead per txin is
substantial, so a more realistic implementation is be to have a UTXO cache for
recent transactions, with TXO commitments acting as a alternate for the (rare)
event that an old txout needs to be spent.

Proofs can be generated and added to transactions without the involvement of
the signers, even after the fact; there's no need for the proof itself to
signed and the proof is not part of the transaction hash. Anyone with access to
TXO MMR data can (re)generate missing proofs, so minimal, if any, changes are
required to wallet software to make use of TXO commitments.


## Delayed Commitments

TXO commitments aren't a new idea - the author proposed them years ago in
response to UTXO commitments. However it's critical for small miners' orphan
rates that block validation be fast, and so far it has proven difficult to
create (U)TXO implementations with acceptable performance; updating and
recalculating cryptographicly hashed merkelized datasets is inherently more
work than not doing so. Fortunately if we maintain a UTXO set for recent
outputs, TXO commitments are only needed when spending old, archived, outputs.
We can take advantage of this by delaying the commitment, allowing it to be
calculated well in advance of it actually being used, thus changing a
latency-critical task into a much easier average throughput problem.

Concretely each block B_i commits to the TXO set state as of block B_{i-n}, in
other words what the TXO commitment would have been n blocks ago, if not for
the n block delay. Since that commitment only depends on the contents of the
blockchain up until block B_{i-n}, the contents of any block after are
irrelevant to the calculation.


## Implementation

Our proposed high-performance/low-latency delayed commitment full-node
implementation needs to store the following data:

1) UTXO set

    Low-latency K:V map of txouts definitely known to be unspent. Similar to
    existing UTXO implementation, but with the key difference that old,
    unspent, outputs may be pruned from the UTXO set.


2) STXO set

    Low-latency set of transaction outputs known to have been spent by
    transactions after the most recent TXO commitment, but created prior to the
    TXO commitment.


3) TXO journal

    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends
    must be low-latency; removals can be high-latency.


4) TXO MMR list

    Prunable, ordered list of TXO MMR's, mainly the highest pending commitment,
    backed by a reference counted, cryptographically hashed object store
    indexed by digest (similar to how git repos work). High-latency ok. We'll
    cover this in more in detail later.


### Fast-Path: Verifying a Txout Spend In a Block

When a transaction output is spent by a transaction in a block we have two
cases:

1) Recently created output

    Output created after the most recent TXO commitment, so it should be in the
    UTXO set; the transaction spending it does not need a TXO commitment proof.
    Remove the output from the UTXO set and append it to the TXO journal.

2) Archived output

    Output created prior to the most recent TXO commitment, so there's no
    guarantee it's in the UTXO set; transaction will have a TXO commitment
    proof for the most recent TXO commitment showing that it was unspent.
    Check that the output isn't already in the STXO set (double-spent), and if
    not add it. Append the output and TXO commitment proof to the TXO journal.

In both cases recording an output as spent requires no more than two key:value
updates, and one journal append. The existing UTXO set requires one key:value
update per spend, so we can expect new block validation latency to be within 2x
of the status quo even in the worst case of 100% archived output spends.


### Slow-Path: Calculating Pending TXO Commitments

In a low-priority background task we flush the TXO journal, recording the
outputs spent by each block in the TXO MMR, and hashing MMR data to obtain the
TXO commitment digest. Additionally this background task removes STXO's that
have been recorded in TXO commitments, and prunes TXO commitment data no longer
needed.

Throughput for the TXO commitment calculation will be worse than the existing
UTXO only scheme. This impacts bulk verification, e.g. initial block download.
That said, TXO commitments provides other possible tradeoffs that can mitigate
impact of slower validation throughput, such as skipping validation of old
history, as well as fraud proof approaches.


### TXO MMR Implementation Details

Each TXO MMR state is a modification of the previous one with most information
shared, so we an space-efficiently store a large number of TXO commitments
states, where each state is a small delta of the previous state, by sharing
unchanged data between each state; cycles are impossible in merkelized data
structures, so simple reference counting is sufficient for garbage collection.
Data no longer needed can be pruned by dropping it from the database, and
unpruned by adding it again. Since everything is committed to via cryptographic
hash, we're guaranteed that regardless of where we get the data, after
unpruning we'll have the right data.

Let's look at how the TXO MMR works in detail. Consider the following TXO MMR
with two txouts, which we'll call state #0:

      0
     / \
    a   b

If we add another entry we get state #1:

        1
       / \
      0   \
     / \   \
    a   b   c

Note how it 100% of the state #0 data was reused in commitment #1. Let's
add two more entries to get state #2:

            2
           / \
          2   \
         / \   \
        /   \   \
       /     \   \
      0       2   \
     / \     / \   \
    a   b   c   d   e

This time part of state #1 wasn't reused - it's wasn't a perfect binary
tree - but we've still got a lot of re-use.

Now suppose state #2 is committed into the blockchain by the most recent block.
Future transactions attempting to spend outputs created as of state #2 are
obliged to prove that they are unspent; essentially they're forced to provide
part of the state #2 MMR data. This lets us prune that data, discarding it,
leaving us with only the bare minimum data we need to append new txouts to the
TXO MMR, the tips of the perfect binary trees ("mountains") within the MMR:

            2
           / \
          2   \
               \
                \
                 \
                  \
                   \
                    e

Note that we're glossing over some nuance here about exactly what data needs to
be kept; depending on the details of the implementation the only data we need
for nodes "2" and "e" may be their hash digest.

Adding another three more txouts results in state #3:

                  3
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               3
                         / \
                        /   \
                       /     \
                      3       3
                     / \     / \
                    e   f   g   h

Suppose recently created txout f is spent. We have all the data required to
update the MMR, giving us state #4. It modifies two inner nodes and one leaf
node:

                  4
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               4
                         / \
                        /   \
                       /     \
                      4       3
                     / \     / \
                    e  (f)  g   h

If an archived txout is spent requires the transaction to provide the merkle
path to the most recently committed TXO, in our case state #2. If txout b is
spent that means the transaction must provide the following data from state #2:

            2
           /
          2
         /
        /
       /
      0
       \
        b

We can add that data to our local knowledge of the TXO MMR, unpruning part of
it:

                  4
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               4
         /               / \
        /               /   \
       /               /     \
      0               4       3
       \             / \     / \
        b           e  (f)  g   h

Remember, we haven't _modified_ state #4 yet; we just have more data about it.
When we mark txout b as spent we get state #5:

                  5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         /               / \
        /               /   \
       /               /     \
      5               4       3
       \             / \     / \
       (b)          e  (f)  g   h

Secondly by now state #3 has been committed into the chain, and transactions
that want to spend txouts created as of state #3 must provide a TXO proof
consisting of state #3 data. The leaf nodes for outputs g and h, and the inner
node above them, are part of state #3, so we prune them:

                  5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         /               /
        /               /
       /               /
      5               4
       \             / \
       (b)          e  (f)

Finally, lets put this all together, by spending txouts a, c, and g, and
creating three new txouts i, j, and k. State #3 was the most recently committed
state, so the transactions spending a and g are providing merkle paths up to
it. This includes part of the state #2 data:

                  3
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          2               3
         / \               \
        /   \               \
       /     \               \
      0       2               3
     /       /               /
    a       c               g

After unpruning we have the following data for state #5:

                  5
                 / \
                /   \
               /     \
              /       \
             /         \
            /           \
           /             \
          5               4
         / \             / \
        /   \           /   \
       /     \         /     \
      5       2       4       3
     / \     /       / \     /
    a  (b)  c       e  (f)  g

That's sufficient to mark the three outputs as spent and add the three new
txouts, resulting in state #6:

                        6
                       / \
                      /   \
                     /     \
                    /       \
                   /         \
                  6           \
                 / \           \
                /   \           \
               /     \           \
              /       \           \
             /         \           \
            /           \           \
           /             \           \
          6               6           \
         / \             / \           \
        /   \           /   \           6
       /     \         /     \         / \
      6       6       4       6       6   \
     / \     /       / \     /       / \   \
   (a) (b) (c)      e  (f) (g)      i   j   k

Again, state #4 related data can be pruned. In addition, depending on how the
STXO set is implemented may also be able to prune data related to spent txouts
after that state, including inner nodes where all txouts under them have been
spent (more on pruning spent inner nodes later).


### Consensus and Pruning

It's important to note that pruning behavior is consensus critical: a full node
that is missing data due to pruning it too soon will fall out of consensus, and
a miner that fails to include a merkle proof that is required by the consensus
is creating an invalid block. At the same time many full nodes will have
significantly more data on hand than the bare minimum so they can help wallets
make transactions spending old coins; implementations should strongly consider
separating the data that is, and isn't, strictly required for consensus.

A reasonable approach for the low-level cryptography may be to actually treat
the two cases differently, with the TXO commitments committing too what data
does and does not need to be kept on hand by the UTXO expiration rules. On the
other hand, leaving that uncommitted allows for certain types of soft-forks
where the protocol is changed to require more data than it previously did.


### Consensus Critical Storage Overheads

Only the UTXO and STXO sets need to be kept on fast random access storage.
Since STXO set entries can only be created by spending a UTXO - and are smaller
than a UTXO entry - we can guarantee that the peak size of the UTXO and STXO
sets combined will always be less than the peak size of the UTXO set alone in
the existing UTXO-only scheme (though the combined size can be temporarily
higher than what the UTXO set size alone would be when large numbers of
archived txouts are spent).

TXO journal entries and unpruned entries in the TXO MMR have log2(n) maximum
overhead per entry: a unique merkle path to a TXO commitment (by "unique" we
mean that no other entry shares data with it). On a reasonably fast system the
TXO journal will be flushed quickly, converting it into TXO MMR data; the TXO
journal will never be more than a few blocks in size.

Transactions spending non-archived txouts are not required to provide any TXO
commitment data; we must have that data on hand in the form of one TXO MMR
entry per UTXO. Once spent however the TXO MMR leaf node associated with that
non-archived txout can be immediately pruned - it's no longer in the UTXO set
so any attempt to spend it will fail; the data is now immutable and we'll never
need it again. Inner nodes in the TXO MMR can also be pruned if all leafs under
them are fully spent; detecting this is easy the TXO MMR is a merkle-sum tree,
with each inner node committing to the sum of the unspent txouts under it.

When a archived txout is spent the transaction is required to provide a merkle
path to the most recent TXO commitment. As shown above that path is sufficient
information to unprune the necessary nodes in the TXO MMR and apply the spend
immediately, reducing this case to the TXO journal size question (non-consensus
critical overhead is a different question, which we'll address in the next
section).

Taking all this into account the only significant storage overhead of our TXO
commitments scheme when compared to the status quo is the log2(n) merkle path
overhead; as long as less than 1/log2(n) of the UTXO set is active,
non-archived, UTXO's we've come out ahead, even in the unrealistic case where
all storage available is equally fast. In the real world that isn't yet the
case - even SSD's significantly slower than RAM.


### Non-Consensus Critical Storage Overheads

Transactions spending archived txouts pose two challenges:

1) Obtaining up-to-date TXO commitment proofs

2) Updating those proofs as blocks are mined

The first challenge can be handled by specialized archival nodes, not unlike
how some nodes make transaction data available to wallets via bloom filters or
the Electrum protocol. There's a whole variety of options available, and the
the data can be easily sharded to scale horizontally; the data is
self-validating allowing horizontal scaling without trust.

While miners and relay nodes don't need to be concerned about the initial
commitment proof, updating that proof is another matter. If a node aggressively
prunes old versions of the TXO MMR as it calculates pending TXO commitments, it
won't have the data available to update the TXO commitment proof to be against
the next block, when that block is found; the child nodes of the TXO MMR tip
are guaranteed to have changed, yet aggressive pruning would have discarded that
data.

Relay nodes could ignore this problem if they simply accept the fact that
they'll only be able to fully relay the transaction once, when it is initially
broadcast, and won't be able to provide mempool functionality after the initial
relay. Modulo high-latency mixnets, this is probably acceptable; the author has
previously argued that relay nodes don't need a mempool² at all.

For a miner though not having the data necessary to update the proofs as blocks
are found means potentially losing out on transactions fees. So how much extra
data is necessary to make this a non-issue?

Since the TXO MMR is insertion ordered, spending a non-archived txout can only
invalidate the upper nodes in of the archived txout's TXO MMR proof (if this
isn't clear, imagine a two-level scheme, with a per-block TXO MMRs, committed
by a master MMR for all blocks). The maximum number of relevant inner nodes
changed is log2(n) per block, so if there are n non-archival blocks between the
most recent TXO commitment and the pending TXO MMR tip, we have to store
log2(n)*n inner nodes - on the order of a few dozen MB even when n is a
(seemingly ridiculously high) year worth of blocks.

Archived txout spends on the other hand can invalidate TXO MMR proofs at any
level - consider the case of two adjacent txouts being spent. To guarantee
success requires storing full proofs. However, they're limited by the blocksize
limit, and additionally are expected to be relatively uncommon. For example, if
1% of 1MB blocks was archival spends, our hypothetical year long TXO commitment
delay is only a few hundred MB of data with low-IO-performance requirements.


## Security Model

Of course, a TXO commitment delay of a year sounds ridiculous. Even the slowest
imaginable computer isn't going to need more than a few blocks of TXO
commitment delay to keep up ~100% of the time, and there's no reason why we
can't have the UTXO archive delay be significantly longer than the TXO
commitment delay.

However, as with UTXO commitments, TXO commitments raise issues with Bitcoin's
security model by allowing relatively miners to profitably mine transactions
without bothering to validate prior history. At the extreme, if there was no
commitment delay at all at the cost of a bit of some extra network bandwidth
"full" nodes could operate and even mine blocks completely statelessly by
expecting all transactions to include "proof" that their inputs are unspent; a
TXO commitment proof for a commitment you haven't verified isn't a proof that a
transaction output is unspent, it's a proof that some miners claimed the txout
was unspent.

At one extreme, we could simply implement TXO commitments in a "virtual"
fashion, without miners actually including the TXO commitment digest in their
blocks at all. Full nodes would be forced to compute the commitment from
scratch, in the same way they are forced to compute the UTXO state, or total
work. Of course a full node operator who doesn't want to verify old history can
get a copy of the TXO state from a trusted source - no different from how you
could get a copy of the UTXO set from a trusted source.

A more pragmatic approach is to accept that people will do that anyway, and
instead assume that sufficiently old blocks are valid. But how old is
"sufficiently old"? First of all, if your full node implementation comes "from
the factory" with a reasonably up-to-date minimum accepted total-work
thresholdⁱ - in other words it won't accept a chain with less than that amount
of total work - it may be reasonable to assume any Sybil attacker with
sufficient hashing power to make a forked chain meeting that threshold with,
say, six months worth of blocks has enough hashing power to threaten the main
chain as well.

That leaves public attempts to falsify TXO commitments, done out in the open by
the majority of hashing power. In this circumstance the "assumed valid"
threshold determines how long the attack would have to go on before full nodes
start accepting the invalid chain, or at least, newly installed/recently reset
full nodes. The minimum age that we can "assume valid" is tradeoff between
political/social/technical concerns; we probably want at least a few weeks to
guarantee the defenders a chance to organise themselves.

With this in mind, a longer-than-technically-necessary TXO commitment delayʲ
may help ensure that full node software actually validates some minimum number
of blocks out-of-the-box, without taking shortcuts. However this can be
achieved in a wide variety of ways, such as the author's prev-block-proof
proposal³, fraud proofs, or even a PoW with an inner loop dependent on
blockchain data. Like UTXO commitments, TXO commitments are also potentially
very useful in reducing the need for SPV wallet software to trust third parties
providing them with transaction data.

i) Checkpoints that reject any chain without a specific block are a more
   common, if uglier, way of achieving this protection.

j) A good homework problem is to figure out how the TXO commitment could be
   designed such that the delay could be reduced in a soft-fork.


## Further Work

While we've shown that TXO commitments certainly could be implemented without
increasing peak IO bandwidth/block validation latency significantly with the
delayed commitment approach, we're far from being certain that they should be
implemented this way (or at all).

1) Can a TXO commitment scheme be optimized sufficiently to be used directly
without a commitment delay? Obviously it'd be preferable to avoid all the above
complexity entirely.

2) Is it possible to use a metric other than age, e.g. priority? While this
complicates the pruning logic, it could use the UTXO set space more
efficiently, especially if your goal is to prioritise bitcoin value-transfer
over other uses (though if "normal" wallets nearly never need to use TXO
commitments proofs to spend outputs, the infrastructure to actually do this may
rot).

3) Should UTXO archiving be based on a fixed size UTXO set, rather than an
age/priority/etc. threshold?

4) By fixing the problem (or possibly just "fixing" the problem) are we
encouraging/legitimising blockchain use-cases other than BTC value transfer?
Should we?

5) Instead of TXO commitment proofs counting towards the blocksize limit, can
we use a different miner fairness/decentralization metric/incentive? For
instance it might be reasonable for the TXO commitment proof size to be
discounted, or ignored entirely, if a proof-of-propagation scheme (e.g.
thinblocks) is used to ensure all miners have received the proof in advance.

6) How does this interact with fraud proofs? Obviously furthering dependency on
non-cryptographically-committed STXO/UTXO databases is incompatible with the
modularized validation approach to implementing fraud proofs.


# References

1) "Merkle Mountain Ranges",
   Peter Todd, OpenTimestamps, Mar 18 2013,
   https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md

2) "Do we really need a mempool? (for relay nodes)",
   Peter Todd, bitcoin-dev mailing list, Jul 18th 2015,
   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-July/009479.html

3) "Segregated witnesses and validationless mining",
   Peter Todd, bitcoin-dev mailing list, Dec 23rd 2015,
   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012103.html

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.12.0rc3 are available from:

    https://bitcoin.org/bin/bitcoin-core-0.12.0/test/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.12.0rc3

This is a release candidate for a new major version release, bringing new
features, bug fixes, as well as other improvements.

Preliminary release notes for the release can be found here:

    https://github.com/bitcoin/bitcoin/blob/0.12/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.12.0.

Diff since rc2:
- - #7440 `c76bfff` Rename permitrbf to mempoolreplacement and provide minimal string-list forward compatibility
- - #7415 `cb83beb` net: Hardcoded seeds update January 2016
- - #7438 `e2d9a58` Do not absolutely protect local peers; decide group ties based on time
- - #7439 `86755bc` Add whitelistforcerelay to control forced relaying. [#7099 redux]
- - #7424 `aa26ee0` Add security/export checks to gitian and fix current failures
- - #7384 `294f432` [qt] Peertable: Increase SUBVERSION_COLUMN_WIDTH

Also, a new certificate was used to sign the Windows installer, which should solve
Win7 compatibility issues.

Thanks to the gitian builders for keeping up so quickly, thanks
to them there are executables so quickly after tagging.

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWtIe6AAoJEHSBCwEjRsmmuX0IAJP7JJ4OozZZ5psY7QF35ouV
E0Vxws470pFyn+iFvz1OwLbeSyhIiLvR1xHZCrFkLbt5vrolJGILQb5xWaFfqDVv
uXIPDzbQ+mJ/cPr2BXWrkjkVC33TBuwiLGethDDb4xlQhSki79EvZqbTkhIz7HxX
jrW8d+zUq+2pOilhqDyZGlzCRhQOZI6W+TFwo4jEunZN+m1BSD2/vhVxIZQzP6jf
Vt6xw23SFbTH+b9dY3Skho/A+gdXSitVpYmDttbOlcIX4AQ7lUmsaqFeaV4z92d+
YqipqLiNkGqXdEYFikyQgM24J4fYm4htZhTBg5y5W8tsIWO6z36tUXVBxmqq6A0=
=mevA
-----END PGP SIGNATURE-----


-------------------------------------
On Mon, Feb 1, 2016 at 4:55 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> * The coinbase scriptSig gets a second number push (similar to the
> current BIP34 height push), which pushes a number O. O is a byte
> offset inside the coinbase transaction (excluding its witness data)
> that points to a 32-byte hash H. This is more flexible and more
> compact than what we have now (a suggestion by jl2012).
>

So, the script sig is  "<height> <offset> ..... <H>"?

Why is this just not the offset in the extra nonce?

> A significant design consideration is that if arbitrary data can be
> > added, it is very likely that miners will make use of that ability for
> > non-Bitcoin purposes;
> I agree with the concern, but I don't really understand how this idea
> solves it.
>
>
It could be enforced that the data in the coinbase witness stack has a
fixed number of entries, which depends on the block version number.
Version 5 blocks would only have 1 entry.

This would mean a soft-fork could be used to add new entries in the stack.
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
This
email has been sent from a virus-free computer protected by Avast.
www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

-------------------------------------
On Fri, Jan 22, 2016 at 04:36:58PM +0000, Andrew C via bitcoin-dev wrote:
> Spending a time locked output requires setting nSequence to less than
> MAX_INT but opting into RBF also requires setting nSequence to less than
> MAX_INT. 

Hi Andrew,

Opt-in RBF requires setting nSequence to less than MAX-1 (not merely
less than MAX), so an nSequence of exactly MAX-1 (which appears in
hex-encoded serialized transactions as feffffff) enables locktime
enforcement but doesn't opt in to RBF.

For more information, please see BIP125:

    https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki

-Dave


-------------------------------------
Following on my earlier message
<https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-March/012485.html>,
I am happy to announce a new soft fork to be deployed using BIP 9
<https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki> - Version
bits.

Please review BIP 9
<https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki> as it has
been updated for information on how Version bits soft forks activate.

This deployment is being referred to as CSV (CheckSequenceVerify) and will
activate the following 3 BIPS as consensus rules:
BIP 68 <https://github.com/bitcoin/bips/blob/master/bip-0068.mediawiki> -
Relative lock-time using consensus-enforced sequence numbers
BIP 112 <https://github.com/bitcoin/bips/blob/master/bip-0112.mediawiki> -
CHECKSEQUENCEVERIFY
BIP 113 <https://github.com/bitcoin/bips/blob/master/bip-0113.mediawiki> -
Median time-past as endpoint for lock-time calculations

These BIP's have been updated with the deployment information:
bit: 0
startTime: 1462060800 "May 1st, 2016"   (mainnet)
           1456790400 "March 1st, 2016" (testnet)
endTime:   1493596800 "May 1st, 2017"   (mainnet and testnet)


Bitcoin Core will release 0.11.3 and 0.12.1 software which implements these
soft forks in the near future.


Thanks,
Alex

-------------------------------------
I believe Bitcoin currently enjoys the property that during an "innocent"
re-org, i.e. a reorg in which no affected transactions are being double
spent, all affected transactions can always eventually get replayed, so
long as the re-org depth is less than 100.

My concern with this proposed operation is that it would destroy this
property.

On Fri, Sep 23, 2016 at 5:57 AM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> This BIP describes a new opcode (OP_CHECKBLOCKATHEIGHT) for the Bitcoin
> scripting system to address reissuing bitcoin transactions when the coins
> they
> spend have been conflicted/double-spent.
>
> https://github.com/luke-jr/bips/blob/bip-cbah/bip-cbah.mediawiki
>
> Does this seem like a good idea/approach?
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Hi bitcoin-dev,

I'm well aware that discussion of moderation on bitcoin-dev is
discouraged*. However, I think that we should, as a year of moderation
approaches, discuss openly as a community what the impact of such policy
has been. Making such a post now is timely given that people will have the
opportunity to discuss in-person as well as online as Scaling Bitcoin is
currently underway. On the suggestion of others, I've also CC'd
bitcoin-discuss on this message.

Below, I'll share some of my own personal thoughts as a starter, but would
love to hear others feelings as well.

For me, the bitcoin-dev mailing list was a place where I started
frequenting to learn a lot about bitcoin and the development process and
interact with the community. Since moderation has begun, it seems that the
messages/day has dropped drastically. This may be a nice outcome overall
for our sanity, but I think that it has on the whole made the community
less accessible. I've heard from people (a > 1 number, myself included)
that they now self-censor because they think they will put a lot of work
into their email only for it to get moderated away as trolling/spam. Thus,
while we may not observe a high rate of moderated posts, it does mean the
"chilling effect" of moderation still manifests -- I think that people not
writing emails because they think it may be moderated reduces the rate of
people writing emails which is a generally valuable thing as it offers
people a vehicle through which they try to think through and communicate
their ideas in detail.

Overall, I think that at the time that moderation was added to the list, it
was probably the right thing to do. We're in a different place as a
community now, so I feel we should attempt to open up this valuable
communication channel once again. My sentiment is that we enacted
moderation to protect a resource that we all felt was valuable, but in the
process, the value of the list was damaged, but not irreparably so.

Best,

Jeremy


* From the email introducing the bitcoin-dev moderation policy, "Generally
discouraged: shower thoughts, wild speculation, jokes, +1s, non-technical
bitcoin issues, rehashing settled topics without new data, moderation
 concerns."


--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>

-------------------------------------
I am actually suggesting 1 hardfork, not 2. However, different rules are
activated at different time to enhance safety and reduce disruption. The
advantage is people are required to upgrade once, not twice. Any clients
designed for stage 2 should also be ready for stage 3.


-----Original Message-----
From: Matt Corallo [mailto:lf-lists@mattcorallo.com] 
Sent: Wednesday, 10 February, 2016 06:15
To: jl2012@xbt.hk; bitcoin-dev@lists.linuxfoundation.org
Subject: Re: [bitcoin-dev] A roadmap to a better header format and bigger
block size

As for your stages idea, I generally like the idea (and mentioned it may be
a good idea in my proposal), but am worried about scheduling two hard-forks
at once....Lets do our first hard-fork first with the things we think we
will need anytime in the visible future that we have reasonable designs for
now, and talk about a second one after we've seen what did/didnt blow up
with the first one.

Anyway, this generally seems reasonable - it looks like most of this matches
up with what I said more specifically in my mail yesterday, with the
addition of timewarp fixes, which we should probably add, and Luke's header
changes, which I need to spend some more time thinking about.

Matt




-------------------------------------
On Tue, Feb 09, 2016 at 10:00:44PM +0000, Matt Corallo wrote:
> Indeed, we could push for more place by just always having one 0-byte,
> but I'm not sure the added complexity helps anything? ASICs can never be
> designed which use more extra-nonce-space than what they can reasonably
> assume will always be available,

I was thinking ASICs could be passed a mask of which bytes they could
use for nonce; in which case the variable-ness can just be handled prior
to passing the work to the ASIC.

But on second thoughts, the block already specifies the target difficulty,
so maybe that could be used to indicate which bytes of the previous hash
must be zero? You have to be a bit careful to deal with the possibility
that you just did a maximum difficulty increase compared to the previous
block (in which case there may be fewer bits in the previous hash that
are zero), but that's just a factor of 4, so:

    #define RETARGET_THRESHOLD ((1ul<<24) / 4)
    y = 32 - bits[0];
    if (bits[1]*65536 + bits[2]*256 + bits[3] >= RETARGET_THRESHOLD)
        y -= 1;
    memset(prevhash, 0x00, y); // clear "first" y bytes of prevhash

should work correctly/safely, and give you 8 bytes of additional nonce
to play with at current difficulty (or 3 bytes at minimum difficulty),
and scale as difficulty increases. No need to worry about avoiding zeroes
that way either.



As far as midstate optimisations go, rearranging the block to be:

 version ; time ; bits ; merkleroot ; prevblock ; nonce

would mean that the last 12 bytes of prevblock and the 4 bytes of nonce
would be available for manipulation [0] if the first round of sha256
was pre-calculated prior to being sent to ASICs (and also that version
and time wouldn't be available). Worth considering?



I don't see how you'd make either of these changes compatible
with Luke-Jr's soft-hardfork approach [1] to ensuring non-upgraded
clients/nodes can't be tricked into following a shorter chain, though.
I think the approach I suggested in my mail avoid Gavin's proposed hard
fork might work though [2].



Combining these with making merge-mining easier [1] and Luke-Jr/Peter
Todd's ideas [3] about splitting the proof of work between something
visible to miners, and something only visible to pool operators to avoid
the block withholding attack on pooled mining would probably make sense
though, to reduce the number of hard forks visible to lightweight clients?

Cheers,
aj

[0] Giving a total of 128 bits, or 96 bits with difficulty such that
    only the last 8 bytes of prevblock are available.

[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012377.html

[2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012046.html

[3] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012384.html
    In particular, the paragraph beginning "Alternatively, if the old
    blockchain has 10% of less hashpower ..."


-------------------------------------
huh?
can you give an example of how a duplicate transaction hash (in the same
chain) can happen given BIP34?


On Wed, Nov 16, 2016 at 7:00 PM, Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On 11/16/2016 03:58 PM, Jorge Timón via bitcoin-dev wrote:
> > On Wed, Nov 16, 2016 at 3:18 PM, Thomas Kerin via bitcoin-dev
> > <bitcoin-dev@lists.linuxfoundation.org> wrote:
> >> BIP30 actually was given similar treatment after a reasonable amount of
> time
> >> had passed.
> >> https://github.com/bitcoin/bitcoin/blob/master/src/main.cpp#L2392
> >
> > This is not really the same. BIP30 is not validated after BIP34 is
> > active because blocks complying with BIP34 will always necessarily
> > comply with BIP30 (ie coinbases cannot be duplicated after they
> > include the block height).
>
> This is a misinterpretation of BIP30. Duplicate transaction hashes can
> and will happen and are perfectly valid in Bitcoin. BIP34 does not
> prevent this.
>
> e
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Sometimes I think there's concerted resistance to making Bitcoin usable for
the average person.   Clearly the primary purpose of BIP0075 is to enshrine
a DNSSEC protocol for giving wallet addresses memorable names.


On Thu, Jun 23, 2016 at 6:44 PM, Justin Newton via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi there,
>    For users who don’t wish a service provider to be able to see their
> information, even ephemerally, and they would like to exchange information
> via BIP75, they can use a software wallet, such as a breadwallet or others,
> and that data will only exist on their phone, and the phone of their
> counterparty (assuming the counterparty also chose to exchange info, and
> was running on a software wallet).
>
> In this way, we allow users to exchange data as they choose, without
> having the risk that a service provider be asked for that data.
>
> If a user chooses to use a hosted platform, and also to store their
> identity data there, I do agree it could be subject to a subpoena, the same
> as when they host their email, and other services.
>
> Finally, they could choose not to use BIP75 at all, and no one would know
> whether they did or didn’t (other than their counterparts) as we don’t
> leave any residue on the blockchain, or anywhere else in the public eye.
>
> We believe that this solution, due in part to its narrow data aperture, is
> the best solution available to the problem we are solving.  We are eager to
> engage in any discussions about how to improve the proposed solution, with
> an eye to fungibility, privacy, and usability.
>
> That said, there is a real need for people to know who they are
> transacting with for usability reasons, for fraud reduction, and also of
> regulatory reasons for some players.  To NOT solve it with a carefully
> crafted standard means that it is more likely to be solved with back room,
> quick and dirty solutions that are not available for community review and
> feedback.
>
> Thanks!
>
> Justin
>
>
>
>
>
>
> On Thu, Jun 23, 2016 at 2:31 PM, Police Terror via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> In England under RIPA 2000 legislation, it's irrelevant whether you have
>> the data or not. If the authorities compel you to hand over that
>> information, and it is within your means to obtain it then you are
>> obliged to do so under threat of criminal offense.
>>
>> So any mechanism whereby data could be collected from Bitcoin users,
>> whether it's stored ephemerally or not, if the police have reasonable
>> suspicion to think it exists then they can compel all parties to work to
>> get them the data they require.
>>
>> If the mechanism flat out does not exist, that is miles better than
>> could exist. Deniability is not a defense when served with a police
>> notice for disclosing data.
>>
>> You have to think not only about the end result, but also about how
>> these mechanisms can be used for intimidating users or leveraging
>> technologies.
>>
>> Justin Newton via bitcoin-dev:
>> > On Thu, Jun 23, 2016 at 1:46 PM, s7r via bitcoin-dev <
>> > bitcoin-dev@lists.linuxfoundation.org> wrote:
>> >
>> >>
>> >>
>> >>
>> >> Any kind of built-in AML/KYC tools in Bitcoin is bad, and might draw
>> >> expectations from _all_ users from authorities. Companies or
>> individuals
>> >> who want and/or need AML/KYC can find ways and do it at their side
>> >> isolated from the entire network, and the solutions shouldn't come from
>> >> upstream. AML/KYC/<insert other regulation here> differ from country to
>> >> country and will be hard to implement in a global consensus network
>> even
>> >> if it would be worth it.
>> >>
>> >>
>> > This was precisely our thinking as well.
>> >
>> > This is actually exactly why BIP 75 was designed the way that it was.
>> Any
>> > (voluntary) identity exchange is done at the application level, on an
>> > encrypted https (or other) connection between the sender and receiver.
>> > Identity data is not passed through or stored on the blockchain, and
>> there
>> > is actually no mark left on the blockchain that identity was even
>> exchanged
>> > on that transaction.
>> >
>> > The only people who know identity info was exchanged, or what the
>> identity
>> > was is the counterparties in the transaction, and depending on
>> > implementation, their service provider.  (At a high level, many software
>> > based wallet providers wouldn’t have any visibility into identity info,
>> > where many hosted services would, for example)
>> >
>> > We did this to protect user privacy as well as fungibility.
>> >
>> > We are allowing the people who want or need to exchange identtity info
>> > (either self signed or 3rd party validated) the option to exchange it,
>> in a
>> > standards based way, directly between peers, without touching the
>> > blockchain or network itself.
>> >
>> > Is this more clear?
>> >
>> >
>> >
>> > _______________________________________________
>> > bitcoin-dev mailing list
>> > bitcoin-dev@lists.linuxfoundation.org
>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
>
> --
>
> Justin W. Newton
> Founder/CEO
> Netki, Inc.
>
> justin@netki.com
> +1.818.261.4248
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
One more thought about why verification by miners may be needed.

Let's say Alice sends Bob a transaction, generating output C.

A troll, named Timothy, broadcasts a transaction with a random hash,
referencing C's output as its spend proof. The miners can't tell if it's
valid or not, and so they include the transaction in a block. Now Bob's
money is useless, because everyone can see the spend proof referenced and
thinks it has already been spent, even though the transaction that claims
it isn't valid.

Did I miss something that protects against this?

On Mon, Aug 8, 2016 at 4:42 PM Tony Churyumoff <tony991@gmail.com> wrote:

> The whole point is in preventing every third party, including miners, from
> seeing the details of what is being spent and how.  The burden of
> verification is shifted to the owners of the coin (which is fair).
>
> In fact we could have miners recognize spend proofs and check that the
> same spend proof is not entered into the blockchain more than once (which
> would be a sign of double spend), but it is not required.  The coin owners
> can already do that themselves.
>
> 2016-08-09 0:41 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
>
>> Wouldn't you lose the ability to assume transactions in the blockchain
>> are verified as valid, since miners can't see the details of what is being
>> spent and how? I feel like this ability is bitcoin's greatest asset, and by
>> removing it you're creating an altcoin different enough to not be connected
>> to/supported by the main bitcoin project.
>>
>> On Mon, Aug 8, 2016, 09:13 Tony Churyumoff via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> Hi Henning,
>>>
>>> 1. The fees are paid by the enclosing BTC transaction.
>>> 2. The hash is encoded into an OP_RETURN.
>>>
>>> > Regarding the blinding factor, I think you could just use HMAC.
>>> How exactly?
>>>
>>> Tony
>>>
>>>
>>> 2016-08-08 18:47 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:
>>>
>>>> Hi Tony,
>>>>
>>>> I see some issues in your protocol.
>>>>
>>>> 1. How are mining fees handled?
>>>>
>>>> 2. Assume Alice sends Bob some Coins together with their history and
>>>> Bob checks that the history is correct. How does the hash of the txout
>>>> find its way into the blockchain?
>>>>
>>>> Regarding the blinding factor, I think you could just use HMAC.
>>>>
>>>> All the best
>>>> Henning
>>>>
>>>>
>>>> On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via
>>>> bitcoin-dev wrote:
>>>> > This is a proposal about hiding the entire content of bitcoin
>>>> > transactions.  It goes farther than CoinJoin and ring signatures,
>>>> which
>>>> > only obfuscate the transaction graph, and Confidential Transactions,
>>>> which
>>>> > only hide the amounts.
>>>> >
>>>> > The central idea of the proposed design is to hide the entire inputs
>>>> and
>>>> > outputs, and publish only the hash of inputs and outputs in the
>>>> > blockchain.  The hash can be published as OP_RETURN.  The plaintext of
>>>> > inputs and outputs is sent directly to the payee via a private
>>>> message, and
>>>> > never goes into the blockchain.  The payee then calculates the hash
>>>> and
>>>> > looks it up in the blockchain to verify that the hash was indeed
>>>> published
>>>> > by the payer.
>>>> >
>>>> > Since the plaintext of the transaction is not published to the public
>>>> > blockchain, all validation work has to be done only by the user who
>>>> > receives the payment.
>>>> >
>>>> > To protect against double-spends, the payer also has to publish
>>>> another
>>>> > hash, which is the hash of the output being spent.  We’ll call this
>>>> hash *spend
>>>> > proof*.  Since the spend proof depends solely on the output being
>>>> spent,
>>>> > any attempt to spend the same output again will produce exactly the
>>>> same
>>>> > spend proof, and the payee will be able to see that, and will reject
>>>> the
>>>> > payment.  If there are several outputs consumed by the same
>>>> transaction,
>>>> > the payer has to publish several spend proofs.
>>>> >
>>>> > To prove that the outputs being spent are valid, the payer also has
>>>> to send
>>>> > the plaintexts of the earlier transaction(s) that produced them, then
>>>> the
>>>> > plaintexts of even earlier transactions that produced the outputs
>>>> spent in
>>>> > those transactions, and so on, up until the issue (similar to
>>>> coinbase)
>>>> > transactions that created the initial private coins.  Each new owner
>>>> of the
>>>> > coin will have to store its entire history, and when he spends the
>>>> coin, he
>>>> > forwards the entire history to the next owner and extends it with his
>>>> own
>>>> > transaction.
>>>> >
>>>> > If we apply the existing bitcoin design that allows multiple inputs
>>>> and
>>>> > multiple outputs per transaction, the history of ownership transfers
>>>> would
>>>> > grow exponentially.  Indeed, if we take any regular bitcoin output
>>>> and try
>>>> > to track its history back to coinbase, our history will branch every
>>>> time
>>>> > we see a transaction that has more than one input (which is not
>>>> uncommon).
>>>> > After such a transaction (remember, we are traveling back in time),
>>>> we’ll
>>>> > have to track two or more histories, for each respective input.  Those
>>>> > histories will branch again, and the total number of history entries
>>>> grows
>>>> > exponentially.  For example, if every transaction had exactly two
>>>> inputs,
>>>> > the size of history would grow as 2^N where N is the number of steps
>>>> back
>>>> > in history.
>>>> >
>>>> > To avoid such rapid growth of ownership history (which is not only
>>>> > inconvenient to move, but also exposes too much private information
>>>> about
>>>> > previous owners of all the contributing coins), we will require each
>>>> > private transaction to have exactly one input (i.e. to consume
>>>> exactly one
>>>> > previous output).  This means that when we track a coin’s history
>>>> back in
>>>> > time, it will no longer branch.  It will grow linearly with the
>>>> number of
>>>> > transfers of ownership.  If a user wants to combine several inputs,
>>>> he will
>>>> > have to send them as separate private transactions (technically,
>>>> several
>>>> > OP_RETURNs, which can be included in a single regular bitcoin
>>>> transaction).
>>>> >
>>>> > Thus, we are now forbidding any coin merges but still allowing coin
>>>> > splits.  To avoid ultimate splitting into the dust, we will also
>>>> require
>>>> > that all private coins be issued in one of a small number of
>>>> > denominations.  Only integer number of “banknotes” can be
>>>> transferred, the
>>>> > input and output amounts must therefore be divisible by the
>>>> denomination.
>>>> > For example, an input of amount 700, denomination 100, can be split
>>>> into
>>>> > outputs 400 and 300, but not into 450 and 250.  To send a payment, the
>>>> > payer has to pick the unspent outputs of the highest denomination
>>>> first,
>>>> > then the second highest, and so on, like we already do when we pay in
>>>> cash.
>>>> >
>>>> > With fixed denominations and one input per transaction, coin histories
>>>> > still grow, but only linearly, which should not be a concern in
>>>> regard to
>>>> > scalability given that all relevant computing resources still grow
>>>> > exponentially.  The histories need to be stored only by the current
>>>> owner
>>>> > of the coin, not every bitcoin node.  This is a fairer allocation of
>>>> > costs.  Regarding privacy, coin histories do expose private
>>>> transactions
>>>> > (or rather parts thereof, since a typical payment will likely consist
>>>> of
>>>> > several transactions due to one-input-per-transaction rule) of past
>>>> coin
>>>> > owners to the future ones, and that exposure grows linearly with
>>>> time, but
>>>> > it is still much much better than having every transaction
>>>> immediately on
>>>> > the public blockchain.  Also, the value of this information for
>>>> potential
>>>> > adversaries arguably decreases with time.
>>>> >
>>>> > There is one technical nuance that I omitted above to avoid
>>>> distraction.
>>>> >  Unlike regular bitcoin transactions, every output in a private
>>>> payment
>>>> > must also include a blinding factor, which is just a random string.
>>>> When
>>>> > the output is spent, the corresponding spend proof will therefore
>>>> depend on
>>>> > this blinding factor (remember that spend proof is just a hash of the
>>>> > output).  Without a blinding factor, it would be feasible to
>>>> pre-image the
>>>> > spend proof and reveal the output being spent as the search space of
>>>> all
>>>> > possible outputs is rather small.
>>>> >
>>>> > To issue the new private coin, one can burn regular BTC by sending it
>>>> to
>>>> > one of several unspendable bitcoin addresses, one address per
>>>> denomination.
>>>> >  Burning BTC would entitle one to an equal amount of the new private
>>>> coin,
>>>> > let’s call it *black bitcoin*, or *BBC*.
>>>> >
>>>> > Then BBC would be transferred from user to user by:
>>>> > 1. creating a private transaction, which consists of one input and
>>>> several
>>>> > outputs;
>>>> > 2. storing the hash of the transaction and the spend proof of the
>>>> consumed
>>>> > output into the blockchain in an OP_RETURN (the sender pays the
>>>> > corresponding fees in regular BTC)
>>>> > 3. sending the transaction, together with the history leading to its
>>>> input,
>>>> > directly to the payee over a private communication channel.  The first
>>>> > entry of the history must be a bitcoin transaction that burned BTC to
>>>> issue
>>>> > an equal amount of BCC.
>>>> >
>>>> > To verify the payment, the payee:
>>>> > 1. makes sure that the amount of the input matches the sum of
>>>> outputs, and
>>>> > all are divisible by the denomination
>>>> > 2. calculates the hash of the private transaction
>>>> > 3. looks up an OP_RETURN that includes this hash and is signed by the
>>>> > payee.  If there is more than one, the one that comes in the earlier
>>>> block
>>>> > prevails.
>>>> > 4. calculates the spend proof and makes sure that it is included in
>>>> the
>>>> > same OP_RETURN
>>>> > 5. makes sure the same spend proof is not included anywhere in the
>>>> same or
>>>> > earlier blocks (that is, the coin was not spent before).  Only
>>>> transactions
>>>> > by the same author are searched.
>>>> > 6. repeats the same steps for every entry in the history, except the
>>>> first
>>>> > entry, which should be a valid burning transaction.
>>>> >
>>>> > To facilitate exchange of private transaction data, the bitcoin
>>>> network
>>>> > protocol can be extended with a new message type.  Unfortunately, it
>>>> lacks
>>>> > encryption, hence private payments are really private only when
>>>> bitcoin is
>>>> > used over tor.
>>>> >
>>>> > There are a few limitations that ought to be mentioned:
>>>> > 1. After user A sends a private payment to user B, user A will know
>>>> what
>>>> > the spend proof is going to be when B decides to spend the coin.
>>>> >  Therefore, A will know when the coin was spent by B, but nothing
>>>> more.
>>>> >  Neither the new owner of the coin, nor its future movements will be
>>>> known
>>>> > to A.
>>>> > 2. Over time, larger outputs will likely be split into many smaller
>>>> > outputs, whose amounts are not much greater than their denominations.
>>>> > You’ll have to combine more inputs to send the same amount.  When you
>>>> want
>>>> > to send a very large amount that is much greater than the highest
>>>> available
>>>> > denomination, you’ll have to send a lot of private transactions, your
>>>> > bitcoin transaction with so many OP_RETURNs will stand out, and their
>>>> > number will roughly indicate the total amount.  This kind of privacy
>>>> > leakage, however it applies to a small number of users, is easy to
>>>> avoid by
>>>> > using multiple addresses and storing a relatively small amount on each
>>>> > address.
>>>> > 3. Exchanges and large merchants will likely accumulate large coin
>>>> > histories.  Although fragmented, far from complete, and likely
>>>> outdated, it
>>>> > is still something to bear in mind.
>>>> >
>>>> > No hard or soft fork is required, BBC is just a separate privacy
>>>> preserving
>>>> > currency on top of bitcoin blockchain, and the same private keys and
>>>> > addresses are used for both BBC and the base currency BTC.  Every BCC
>>>> > transaction must be enclosed into by a small BTC transaction that
>>>> stores
>>>> > the OP_RETURNs and pays for the fees.
>>>> >
>>>> > Are there any flaws in this design?
>>>> >
>>>> > Originally posted to BCT
>>>> https://bitcointalk.org/index.php?topic=1574508.0,
>>>> > but got no feedback so far, apparently everybody was consumed with
>>>> bitfinex
>>>> > drama and now mimblewimble.
>>>> >
>>>> > Tony
>>>>
>>>> > _______________________________________________
>>>> > bitcoin-dev mailing list
>>>> > bitcoin-dev@lists.linuxfoundation.org
>>>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>>> --
>>>> Henning Kopp
>>>> Institute of Distributed Systems
>>>> Ulm University, Germany
>>>>
>>>> Office: O27 - 3402
>>>> Phone: +49 731 50-24138
>>>> Web: http://www.uni-ulm.de/in/vs/~kopp
>>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
>

-------------------------------------

> On August 16, 2016 at 8:27 PM Russell O'Connor via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> Okay.
> 
> I'm not really opposed to this BIP, but I am worried that fighting script malleability is a battle that can never be won; even leaving one avenue of malleability open is probably just as bad as having many avenues of malleability, so it just doesn't seem worthwhile to me.

Not really. I think the goal is to protect as many common scripts as possible.

For example:
1) BIP146 (Low S values signatures) will eliminate all malleability for P2WPKH
2) BIP146 + null dummy value for CHECKMULTISIG ("NULLDUMMY") will eliminate all malleability for simple multi-sig in P2WSH. This is particularly interesting since without NULLDUMMY, attackers are able to replace the dummy value with anything.
3) BIP146 + NULLDUMMY + minimal IF argument ("MINIMALIF") will eliminate malleability for any Lightening Network scripts that I'm aware of.

With 3), 99.99% of segwit transactions in foreseeable future should be fully protected.

The plan is to implement MINIMALIF as a relay policy first, and enforce the softfork after further risks assessment. This BIP serves as a warning to users for not using incompatible script.

Peter Todd:
> Having said that, a better approach may be a separate CHECKBOOLVERIFY opcode that fails unless the top item on the stack is a minimally encoded true or false value, to allow script writers to opt into this behavior; it's not always ideal.

I believe all Lightening Network scripts (the only real users of IF/NOTIF in foreseeable future) are already compatible with MINIMALIF. It may not be a good idea for them to spend 1 more byte to get protected.

If people want to have the original OP_IF behaviour, a simple way would be using "0NOTEQUAL IF". However, this works only if the argument is a valid number (also beware of MINIMALDATA rule in BIP62).

To completely replicate the original behaviour, one may use:
"DEPTH TOALTSTACK IFDUP DEPTH FROMALTSTACK NUMNOTEQUAL IF 2DROP {if script} ELSE DROP {else script} ENDIF"

This is because we don't have a simple OP_CASTTOBOOL, and IFDUP is 1 of the 4 codes that perform CastToBool on top stack item (the others are VERIFY, IF, and NOTIF; and VERIFY can't be used here since it terminates the script with a False).


-------------------------------------
> That is a good point. As you said, it puts a lot more burden on the coin
> holders. One big downside would be data management. Instead of simply
> backing up a single HD private key, the user would have to back up entire
> histories of every output that has been sent to them if they want to secure
> their funds.

You are correct.  It is somewhat similar to bearer assets: if you lose
the histories, you lose money.

> It also requires them to be online to receive payments, and I think finding
> a method of sending the private message containing the coin's history is
> going to be a bit of a challenge. If you connect directly to the recipient
> to convey the information through traditional channels, anonymity is lost.
> Sending messages through the bitcoin network is one option to protect
> anonymity, but without active pathfinding there's no guarantee the payee
> will even get the message. I'm assuming you'd have to essentially replace tx
> messages with encrypted BBC histories, and mempools are quite full as it is.
>
> Tony, do you have any more thoughts on exactly how users would convey the
> private messages to payees?

You are right conveying the private messages is not trivial.  While we
can adapt the existing bitcoin network protocol for a limited set of
use cases, such as both parties being online at the same time and
connected directly, BBC would work best if we design a whole new
communication layer specifically for conveying private messages.  We
could route the end-to-end encrypted messages through hubs who are
constantly online and would store and forward the messages, thus the
peers don't have to be online at the same time and don't have to
connect directly.  The hubs could simultaneously serve as lightening
network hubs.


2016-08-09 3:03 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
> That is a good point. As you said, it puts a lot more burden on the coin
> holders. One big downside would be data management. Instead of simply
> backing up a single HD private key, the user would have to back up entire
> histories of every output that has been sent to them if they want to secure
> their funds.
>
> It also requires them to be online to receive payments, and I think finding
> a method of sending the private message containing the coin's history is
> going to be a bit of a challenge. If you connect directly to the recipient
> to convey the information through traditional channels, anonymity is lost.
> Sending messages through the bitcoin network is one option to protect
> anonymity, but without active pathfinding there's no guarantee the payee
> will even get the message. I'm assuming you'd have to essentially replace tx
> messages with encrypted BBC histories, and mempools are quite full as it is.
>
> Tony, do you have any more thoughts on exactly how users would convey the
> private messages to payees?
>
> On Mon, Aug 8, 2016 at 4:42 PM Tony Churyumoff <tony991@gmail.com> wrote:
>>
>> The whole point is in preventing every third party, including miners, from
>> seeing the details of what is being spent and how.  The burden of
>> verification is shifted to the owners of the coin (which is fair).
>>
>> In fact we could have miners recognize spend proofs and check that the
>> same spend proof is not entered into the blockchain more than once (which
>> would be a sign of double spend), but it is not required.  The coin owners
>> can already do that themselves.
>>
>> 2016-08-09 0:41 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
>>>
>>> Wouldn't you lose the ability to assume transactions in the blockchain
>>> are verified as valid, since miners can't see the details of what is being
>>> spent and how? I feel like this ability is bitcoin's greatest asset, and by
>>> removing it you're creating an altcoin different enough to not be connected
>>> to/supported by the main bitcoin project.
>>>
>>>
>>> On Mon, Aug 8, 2016, 09:13 Tony Churyumoff via bitcoin-dev
>>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>>
>>>> Hi Henning,
>>>>
>>>> 1. The fees are paid by the enclosing BTC transaction.
>>>> 2. The hash is encoded into an OP_RETURN.
>>>>
>>>> > Regarding the blinding factor, I think you could just use HMAC.
>>>> How exactly?
>>>>
>>>> Tony
>>>>
>>>>
>>>> 2016-08-08 18:47 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:
>>>>>
>>>>> Hi Tony,
>>>>>
>>>>> I see some issues in your protocol.
>>>>>
>>>>> 1. How are mining fees handled?
>>>>>
>>>>> 2. Assume Alice sends Bob some Coins together with their history and
>>>>> Bob checks that the history is correct. How does the hash of the txout
>>>>> find its way into the blockchain?
>>>>>
>>>>> Regarding the blinding factor, I think you could just use HMAC.
>>>>>
>>>>> All the best
>>>>> Henning
>>>>>
>>>>>
>>>>> On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via
>>>>> bitcoin-dev wrote:
>>>>> > This is a proposal about hiding the entire content of bitcoin
>>>>> > transactions.  It goes farther than CoinJoin and ring signatures,
>>>>> > which
>>>>> > only obfuscate the transaction graph, and Confidential Transactions,
>>>>> > which
>>>>> > only hide the amounts.
>>>>> >
>>>>> > The central idea of the proposed design is to hide the entire inputs
>>>>> > and
>>>>> > outputs, and publish only the hash of inputs and outputs in the
>>>>> > blockchain.  The hash can be published as OP_RETURN.  The plaintext
>>>>> > of
>>>>> > inputs and outputs is sent directly to the payee via a private
>>>>> > message, and
>>>>> > never goes into the blockchain.  The payee then calculates the hash
>>>>> > and
>>>>> > looks it up in the blockchain to verify that the hash was indeed
>>>>> > published
>>>>> > by the payer.
>>>>> >
>>>>> > Since the plaintext of the transaction is not published to the public
>>>>> > blockchain, all validation work has to be done only by the user who
>>>>> > receives the payment.
>>>>> >
>>>>> > To protect against double-spends, the payer also has to publish
>>>>> > another
>>>>> > hash, which is the hash of the output being spent.  We’ll call this
>>>>> > hash *spend
>>>>> > proof*.  Since the spend proof depends solely on the output being
>>>>> > spent,
>>>>> > any attempt to spend the same output again will produce exactly the
>>>>> > same
>>>>> > spend proof, and the payee will be able to see that, and will reject
>>>>> > the
>>>>> > payment.  If there are several outputs consumed by the same
>>>>> > transaction,
>>>>> > the payer has to publish several spend proofs.
>>>>> >
>>>>> > To prove that the outputs being spent are valid, the payer also has
>>>>> > to send
>>>>> > the plaintexts of the earlier transaction(s) that produced them, then
>>>>> > the
>>>>> > plaintexts of even earlier transactions that produced the outputs
>>>>> > spent in
>>>>> > those transactions, and so on, up until the issue (similar to
>>>>> > coinbase)
>>>>> > transactions that created the initial private coins.  Each new owner
>>>>> > of the
>>>>> > coin will have to store its entire history, and when he spends the
>>>>> > coin, he
>>>>> > forwards the entire history to the next owner and extends it with his
>>>>> > own
>>>>> > transaction.
>>>>> >
>>>>> > If we apply the existing bitcoin design that allows multiple inputs
>>>>> > and
>>>>> > multiple outputs per transaction, the history of ownership transfers
>>>>> > would
>>>>> > grow exponentially.  Indeed, if we take any regular bitcoin output
>>>>> > and try
>>>>> > to track its history back to coinbase, our history will branch every
>>>>> > time
>>>>> > we see a transaction that has more than one input (which is not
>>>>> > uncommon).
>>>>> > After such a transaction (remember, we are traveling back in time),
>>>>> > we’ll
>>>>> > have to track two or more histories, for each respective input.
>>>>> > Those
>>>>> > histories will branch again, and the total number of history entries
>>>>> > grows
>>>>> > exponentially.  For example, if every transaction had exactly two
>>>>> > inputs,
>>>>> > the size of history would grow as 2^N where N is the number of steps
>>>>> > back
>>>>> > in history.
>>>>> >
>>>>> > To avoid such rapid growth of ownership history (which is not only
>>>>> > inconvenient to move, but also exposes too much private information
>>>>> > about
>>>>> > previous owners of all the contributing coins), we will require each
>>>>> > private transaction to have exactly one input (i.e. to consume
>>>>> > exactly one
>>>>> > previous output).  This means that when we track a coin’s history
>>>>> > back in
>>>>> > time, it will no longer branch.  It will grow linearly with the
>>>>> > number of
>>>>> > transfers of ownership.  If a user wants to combine several inputs,
>>>>> > he will
>>>>> > have to send them as separate private transactions (technically,
>>>>> > several
>>>>> > OP_RETURNs, which can be included in a single regular bitcoin
>>>>> > transaction).
>>>>> >
>>>>> > Thus, we are now forbidding any coin merges but still allowing coin
>>>>> > splits.  To avoid ultimate splitting into the dust, we will also
>>>>> > require
>>>>> > that all private coins be issued in one of a small number of
>>>>> > denominations.  Only integer number of “banknotes” can be
>>>>> > transferred, the
>>>>> > input and output amounts must therefore be divisible by the
>>>>> > denomination.
>>>>> > For example, an input of amount 700, denomination 100, can be split
>>>>> > into
>>>>> > outputs 400 and 300, but not into 450 and 250.  To send a payment,
>>>>> > the
>>>>> > payer has to pick the unspent outputs of the highest denomination
>>>>> > first,
>>>>> > then the second highest, and so on, like we already do when we pay in
>>>>> > cash.
>>>>> >
>>>>> > With fixed denominations and one input per transaction, coin
>>>>> > histories
>>>>> > still grow, but only linearly, which should not be a concern in
>>>>> > regard to
>>>>> > scalability given that all relevant computing resources still grow
>>>>> > exponentially.  The histories need to be stored only by the current
>>>>> > owner
>>>>> > of the coin, not every bitcoin node.  This is a fairer allocation of
>>>>> > costs.  Regarding privacy, coin histories do expose private
>>>>> > transactions
>>>>> > (or rather parts thereof, since a typical payment will likely consist
>>>>> > of
>>>>> > several transactions due to one-input-per-transaction rule) of past
>>>>> > coin
>>>>> > owners to the future ones, and that exposure grows linearly with
>>>>> > time, but
>>>>> > it is still much much better than having every transaction
>>>>> > immediately on
>>>>> > the public blockchain.  Also, the value of this information for
>>>>> > potential
>>>>> > adversaries arguably decreases with time.
>>>>> >
>>>>> > There is one technical nuance that I omitted above to avoid
>>>>> > distraction.
>>>>> >  Unlike regular bitcoin transactions, every output in a private
>>>>> > payment
>>>>> > must also include a blinding factor, which is just a random string.
>>>>> > When
>>>>> > the output is spent, the corresponding spend proof will therefore
>>>>> > depend on
>>>>> > this blinding factor (remember that spend proof is just a hash of the
>>>>> > output).  Without a blinding factor, it would be feasible to
>>>>> > pre-image the
>>>>> > spend proof and reveal the output being spent as the search space of
>>>>> > all
>>>>> > possible outputs is rather small.
>>>>> >
>>>>> > To issue the new private coin, one can burn regular BTC by sending it
>>>>> > to
>>>>> > one of several unspendable bitcoin addresses, one address per
>>>>> > denomination.
>>>>> >  Burning BTC would entitle one to an equal amount of the new private
>>>>> > coin,
>>>>> > let’s call it *black bitcoin*, or *BBC*.
>>>>> >
>>>>> > Then BBC would be transferred from user to user by:
>>>>> > 1. creating a private transaction, which consists of one input and
>>>>> > several
>>>>> > outputs;
>>>>> > 2. storing the hash of the transaction and the spend proof of the
>>>>> > consumed
>>>>> > output into the blockchain in an OP_RETURN (the sender pays the
>>>>> > corresponding fees in regular BTC)
>>>>> > 3. sending the transaction, together with the history leading to its
>>>>> > input,
>>>>> > directly to the payee over a private communication channel.  The
>>>>> > first
>>>>> > entry of the history must be a bitcoin transaction that burned BTC to
>>>>> > issue
>>>>> > an equal amount of BCC.
>>>>> >
>>>>> > To verify the payment, the payee:
>>>>> > 1. makes sure that the amount of the input matches the sum of
>>>>> > outputs, and
>>>>> > all are divisible by the denomination
>>>>> > 2. calculates the hash of the private transaction
>>>>> > 3. looks up an OP_RETURN that includes this hash and is signed by the
>>>>> > payee.  If there is more than one, the one that comes in the earlier
>>>>> > block
>>>>> > prevails.
>>>>> > 4. calculates the spend proof and makes sure that it is included in
>>>>> > the
>>>>> > same OP_RETURN
>>>>> > 5. makes sure the same spend proof is not included anywhere in the
>>>>> > same or
>>>>> > earlier blocks (that is, the coin was not spent before).  Only
>>>>> > transactions
>>>>> > by the same author are searched.
>>>>> > 6. repeats the same steps for every entry in the history, except the
>>>>> > first
>>>>> > entry, which should be a valid burning transaction.
>>>>> >
>>>>> > To facilitate exchange of private transaction data, the bitcoin
>>>>> > network
>>>>> > protocol can be extended with a new message type.  Unfortunately, it
>>>>> > lacks
>>>>> > encryption, hence private payments are really private only when
>>>>> > bitcoin is
>>>>> > used over tor.
>>>>> >
>>>>> > There are a few limitations that ought to be mentioned:
>>>>> > 1. After user A sends a private payment to user B, user A will know
>>>>> > what
>>>>> > the spend proof is going to be when B decides to spend the coin.
>>>>> >  Therefore, A will know when the coin was spent by B, but nothing
>>>>> > more.
>>>>> >  Neither the new owner of the coin, nor its future movements will be
>>>>> > known
>>>>> > to A.
>>>>> > 2. Over time, larger outputs will likely be split into many smaller
>>>>> > outputs, whose amounts are not much greater than their denominations.
>>>>> > You’ll have to combine more inputs to send the same amount.  When you
>>>>> > want
>>>>> > to send a very large amount that is much greater than the highest
>>>>> > available
>>>>> > denomination, you’ll have to send a lot of private transactions, your
>>>>> > bitcoin transaction with so many OP_RETURNs will stand out, and their
>>>>> > number will roughly indicate the total amount.  This kind of privacy
>>>>> > leakage, however it applies to a small number of users, is easy to
>>>>> > avoid by
>>>>> > using multiple addresses and storing a relatively small amount on
>>>>> > each
>>>>> > address.
>>>>> > 3. Exchanges and large merchants will likely accumulate large coin
>>>>> > histories.  Although fragmented, far from complete, and likely
>>>>> > outdated, it
>>>>> > is still something to bear in mind.
>>>>> >
>>>>> > No hard or soft fork is required, BBC is just a separate privacy
>>>>> > preserving
>>>>> > currency on top of bitcoin blockchain, and the same private keys and
>>>>> > addresses are used for both BBC and the base currency BTC.  Every BCC
>>>>> > transaction must be enclosed into by a small BTC transaction that
>>>>> > stores
>>>>> > the OP_RETURNs and pays for the fees.
>>>>> >
>>>>> > Are there any flaws in this design?
>>>>> >
>>>>> > Originally posted to BCT
>>>>> > https://bitcointalk.org/index.php?topic=1574508.0,
>>>>> > but got no feedback so far, apparently everybody was consumed with
>>>>> > bitfinex
>>>>> > drama and now mimblewimble.
>>>>> >
>>>>> > Tony
>>>>>
>>>>> > _______________________________________________
>>>>> > bitcoin-dev mailing list
>>>>> > bitcoin-dev@lists.linuxfoundation.org
>>>>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>
>>>>>
>>>>> --
>>>>> Henning Kopp
>>>>> Institute of Distributed Systems
>>>>> Ulm University, Germany
>>>>>
>>>>> Office: O27 - 3402
>>>>> Phone: +49 731 50-24138
>>>>> Web: http://www.uni-ulm.de/in/vs/~kopp
>>>>
>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev@lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>


-------------------------------------
On Fri, Oct 14, 2016 at 07:38:07AM -0300, Sergio Demian Lerner via bitcoin-dev wrote:
> I read the DPL v1.1 and I find it dangerous for Bitcoin users. Current
> users may be confident they are protected but in fact they are not, as the
> future generations of users can be attacked, making Bitcoin technology
> fully proprietary and less valuable.

Glad to hear you're taking a conservative approach.

So I assume Rootstock is going to do something stronger then, like
Blockstream's DPL + binding patent pledge to only use patents defensively?

    https://www.blockstream.com/about/patent_pledge/

Because if not, the DPL is still better than the status quo.

> If you read the DPL v1.1 you will see that companies that join DPL can
> enforce their patents against anyone who has chosen not to join the DPL.
> (http://defensivepatentlicense.org/content/defensive-patent-license)
> 
> So basically most users of Bitcoin could be currently under threat of being
> sued by Bitcoin companies and individuals that joined DPL in the same way
> they might be under threat by the remaining companies. And even if they
> joined DPL, they may be asked to pay royalties for the use of the
> inventions prior joining DPL.
> 
> DPL changes nothing for most individuals that cannot and will not hire
> patent attorneys to advise them on what the DPL benefits are and what
> rights they are resigning. Remember that patten attorneys fees may be
> prohibitive for individuals in under-developed countries.
> 
> Also DPL is revocable by the signers (with only a 180-day notice), so if
> Bitcoin Core ends up using ANY DPL covered patent, the company owning the
> patent can later force all new Bitcoin users to pay royalties.

Indeed. However, you're also free to adopt the DPL irrevocably by additionally
stating that you will never invoke that 180-day notice provision (or more
humorously, make it a 100 year notice period to ensure any patents expire!).

If you're concerned about this problem, I'd suggest that Rootstock do exactly
that.

> Because Bitcoin user base grows all the time with new individuals, the sole
> existence of DPL licensed patents in Bitcoin represents a danger to Bitcoin
> future almost the same as the existence of non-DPL license patents.

To be clear, modulo the revocability provision, it's a danger mainly to those
who are unwilling to adopt the DPL themselves, perhaps because they support
software patents.

> If you're publishing all your ideas and code (public disclosure), you
> cannot later go and file a patent in most of the world except the US, where
> you have a 1 year grace period. So we need to do something specific to
> prevent the publishers filing a US patent.

Again, lets remember that you personally proposed a BIP[1] that had the effect
of aiding your ASICBOOST patent[2] without disclosing that fact in your BIP nor
your pull-req[3]. The simple fact is we can't rely solely on voluntary
disclosure - your own behavior is a perfect example of why not.

[1]: BIP: https://github.com/BlockheaderNonce2/bitcoin/wiki
[2]: ASICBOOST PATENT https://www.google.com/patents/WO2015077378A1?cl=en
[3]: Extra nonce pull request: https://github.com/bitcoin/bitcoin/pull/5102

> What we need much more than DPL, we need that every BIP and proposal to the
> Bitcoin mailing list contains a note that grants all Bitcoin users a
> worldwide, royalty-free, no-charge, non-exclusive, irrevocable license for
> the content of the e-mail or BIP.

A serious problem here is the definition of "Bitcoin users". Does Bitcoin
Classic count? Bitcoin Unlimited? What if Bitcoin forks?

Better to grant _everyone_ a irrevocable license.


Along those lines, it'd be reasonable to consider changing the Bitcoin Core
license to something like an Apache2/LGPL3 dual license to ensure the copyright
license also has anti-patent protections.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
> Additionally, BIP 111 (NODE_BLOOM service bit) has been implemented in Bitcoin
> Core and derivatives; it is unclear if used by clients yet. Can developers of
> such clients please comment and let me know: 1) if their software supports
> this BIP already; 2) if not, do they intend to support it in the future?
> If and only if there are any clients using this service bit already, I will
> update BIP 111 to Final Status in 2 weeks also.

Multibit is adding detection of the NODE_BLOOM bit in the next 2-3 weeks.

SPV is kinda broken if the wallet doesn’t do this detection. If your wallet connects only to nodes that don’t support bloom filtering, the wallet never gets updates. We have had a spike in users reporting that their wallet isn't getting updated. To compound the problem, they rescan the blockchain and lose all of their transaction history. It has caused much panic among less technical users.

We believe that failing to detect the NODE_BLOOM bit is the culprit, although it is non-deterministic, so we aren't certain.

I imagine that other SPV wallets are having similar issues. BIP 111 really isn’t optional at this point, so it should be marked final.


-------------------------------------
On 03/23/2016 01:36 PM, Tom via bitcoin-dev wrote:
> On Wednesday 23 Mar 2016 16:24:12 Jonas Schnelli via bitcoin-dev wrote:
> * why would you not allow encryption on non-pre-approved connections?

Agree

> * we just removed (ssl) encryption from the JSON interface, how do you suggest 
> this encryption to be implemented without openSSL?

CurveCP

> * What is the reason for using the p2p code to connect a wallet to a node?
> I suggest using one of the other connection methods to connect to the node. 
> This avoids a change in the bitcoin protocol for a very specific usecase.

Agree, P2P and client-server protocols are distinct use-cases. Missing
this distinction is the root cause of problems with the bloom filters
feature.

> * Why do you want to do a per-message encryption (wrapping the original)? 
> Smaller messages that contain predictable content and are able to be matched 
> to the unencrypted versions on the wire send to other nodes will open this 
> scheme up to various old statistical attacks.

Privacy cannot currently be achieved unless the server is trusted. In
most wallet scenarios that's not a reasonable assumption unless one
controls the full node. So this is only useful in the case where the
wallet is trusting a remote server, and as you point out - message
encryption is weak in this case. In a trustless server scenario
encryption would be unnecessary overhead.

>> Responding peers must ignore (banning would lead to fingerprinting) the 
> requesting peer after 5 unsuccessfully authentication tries to avoid resource 
> attacks.
> 
> Any implementation of that kind would itself again be open to resource 
> attacks.
> Why 5? Do you want to allow a node to make a typo?

Agree, denial of service protection can and should be much more flexible
than this. It's not necessary to incorporate DoS protection into a
protocol. I think maybe this stems from the ill-advised attempt at
messaging reliability.

>> To ensure that no message was dropped or blocked, the complete communication 
> must be hashed (sha256). Both peers keep the SHA256 context of the encryption 
> session. The complete <code>enc</code> message (leaving out the hash itself) 
> must be added to the hash-context by both parties. Before sending a 
> <code>enc</code> command, the sha256 context will be copied and finalized.
> 
> You write "the complete communication must be hashed" and every message has a 
> hash of the state until it is at that point.
> I think you need to explain how that works specifically.

Also, this gets into the area of messaging reliability. This is
certainly not something I would recommend for a P2P protocol optimized
for maintaining a cache of public data.

e


-------------------------------------
On Fri, Sep 02, 2016 at 12:40:58AM -0400, Johnson Lau via bitcoin-dev wrote:
> Deployment
> 
> This BIP will be deployed by "version bits" BIP9 using the same parameters for BIP141 and BIP143, with the name "segwit" and using bit 1.
> 
> For Bitcoin mainnet, the BIP9 starttime is midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout is midnight TBD UTC (Epoch timestamp TBD).
> 
> For Bitcoin testnet, the BIP9 starttime is midnight 1 May 2016 UTC (Epoch timestamp 1462060800) and BIP9 timeout is midnight 1 May 2017 UTC (Epoch timestamp 1493596800).
> 
> Compatibility
> 
> The reference client has produced compatible signatures from the beginning, and the NULLDUMMY rule has been enforced as relay policy by the reference client since v0.10.0. There has been no transactions violating the requirement being added to the chain since at least August 2015. In addition, every non-compliant signature can trivially be converted into a compliant one, so there is no loss of functionality by this requirement.

This should say "for all scriptPubKey types in actual use, non-compliant
signatures can trivially be converted into compliant ones"

You can of course create a scriptPubKey where that's not possible, but
fortunately no-one appears to do that.


Also, as original author of NULLDUMMY, thanks for finally making it into a
soft-fork!

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Thu, Jun 16, 2016 at 02:07:26AM -0700, Bram Cohen wrote:
> On Wed, Jun 15, 2016 at 8:26 PM, Peter Todd <pete@petertodd.org> wrote:
> Okay, clearly my assumptions about the parts of that post I didn't read
> carefully were way off. I'll have to look through it carefully to be able
> to make coherent apples to apples comparisons.

Thanks!

> > I'm worried that once there's real transaction fees everyone might stop
> > > consolidating dust and the set of unspent transactions might grow without
> > > bound as well, but that's a topic for another day.
> >
> > Ok, but then if you're concerned about that risk, why introduce a data
> > structure - the STXO set - that's _guaranteed_ to grow without bound?
> >
> 
> I'm not proposing STXO set commitments either. My point was that there
> should be incentives for collecting dust. That has nothing to do with this
> thread though and should be discussed separately (also I don't feel like
> discussing it because I don't have a good proposal).

Ah, yeah, I misunderstood you there; as expected absolutely no-one is proposing
STXO set commitments. :)

> > > The main differences to your patricia trie are the non-padding sha256 and
> > > that each level doesn't hash in a record of its depth and the usage of
> > > ONLY0 and ONLY1.
> >
> > I'm rather confused, as the above sounds nothing like what I've
> > implemented,
> > which only has leaf nodes, inner nodes, and the special empty node
> > singleton,
> > for both the MMR and merbinner trees.
> >
> 
> It's quite a bit like merbinner trees. I've basically taken the leaf nodes
> and smushed them into the inner nodes above them, thus saving a hashing
> operation and some memory. They're both binary radix trees.

Ah, I see what you mean now.

So above you said that in merbinner trees each node "hash[es] in a record of
its depth" That's actually incorrect: each node commits to the prefix that all
keys below that level start with, not just the depth.

This means that in merbinner trees, cases where multiple keys share parts of
the same prefix are handled efficiently, without introducing extra levels
unnecessarily; there's no need for the ONLY0/1 nodes as the children of an
inner node will always be on different sides.

When keys are randomly distributed, this isn't a big deal; OTOH against
attackers who are choosing keys, e.g. by grinding hashes, merbinner trees
always have maximum depths in proportion to log2(n) of the actual number of
items in the tree. Grinding is particularly annoying to deal with due to the
birthday attack: creating a ground prefix 64 bits long only takes 32 bits worth
of work.


In my deterministic expressions work one of the ideas I've been tossing around
is rather than always using hash digests directly for when you need to commit
to some data, we could instead extend the idea of a digest to that of a
"commitment", where a commitment is simply some short, but variable-sized,
string that uniquely maps to a given set of data. Secondly, commitments do
*not* always guarantee that the original data can't be recovered from the
commitment itself.

By allowing commitments to be variable sized - say 0 to ~64 bytes - we get a
number of advantages:

1) Data shorter than the length of a digest (32 bytes) can be included in the
commitment itself, improving efficiency.

2) Data a little longer than a digest can have hashing delayed, to better fill
up blocks.

In particular, case #2 handles your leaf node optimizations generically,
without special cases and additional complexity. It'd also be a better way to
do the ONLY0/1 cases, as if the "nothing on this side" symbol is a single byte,
each additional colliding level would simply extend the commitment without
hashing. In short, you'd have nearly the same level of optimization even if at
the cryptography level your tree consists of only leaves, inner nodes, and nil.

Another advantage of variable sized commitments is that it can help make clear
to users when it's possible to brute force the message behind the commitment.
For instance, digest from a hashed four byte integer can be trivially reversed
by just trying all combinations. Equally, if that integer is concatenated with
a 32 byte digest that the attacker knows, the value of the integer can be brute
forced.

> > Technically even a patricia trie utxo commitment can have sub-1 cache
> > > misses per update if some of the updates in a single block are close to
> > > each other in memory. I think I can get practical Bitcoin updates down
> > to a
> > > little bit less than one l2 cache miss per update, but not a lot less.
> >
> > I'm very confused as to why you think that's possible. When you say
> > "practical
> > Bitcoin updates", what exactly is the data structure you're proposing to
> > update? How is it indexed?
> 
> 
> My calculations are: a Bitcoin block contains about 2000 updates. The l2
> cache is about 256 kilobytes, and if an update is about 32 bytes times two
> for the parents, grandparents, etc. then an l2 cache can contain about 4000
> values. If the current utxo size is about 2000 * 4000 = 8,000,000 in size
> then about half the pages which contain a transaction will contain a second
> one. I think the utxo set is currently about an order of magnitude greater
> than that, so the number of such collisions will be fairly mall, hence my
> 'less than one but not a lot less' comment.

Your estimate of updates requiring 32 bytes of data is *way* off.

Each inner node updated on the path to a leaf node will itself require 32 bytes
of data to be fetched - the digest of the sibling. As of block 416,628, there
are 39,167,128 unspent txouts, giving us a tree about 25 levels deep.

So if I want to update a single leaf, I need to read:

    25 nodes * 32 bytes/node = 800 bytes

of data. Naively, that'd mean our 2,000 updates needs to read 1.6MB from RAM,
which is 6.4x bigger than the L2 cache - it's just not going to fit.

Taking into account the fact that this is a batched update improves things a
little bit. For a node at level i with random access patterns and N accesses
total our amortised cost is 1/(1 + N/2^i) Summing that over 2,000 leaf updates
and 25 levels gives us ~29,000 total updates, 0.9MB, which is still a lot
larger than L2 cache.

While this might fit in L3 cache - usually on the order of megabytes - this is
a rather optimistic scenario anyway: we're assuming no other cache pressure and
100% hit rate.

Anyway hashing is pretty slow. The very fast BLAKE2 is about 3 cycles/byte
(SHA256 is about 15 cycles/byte) so hashing that same data would take around
200 cycles, and probably quite a bit more in practice due to overheads from our
short message lengths; fetching a cache line from DRAM only takes about 1,000
cycles. I'd guess that once other overheads are taken into account, even if you
could eliminate L2/L3 cache-misses it wouldn't be much of an improvement.

> As for how it's indexed, at a crypto definition level it's just a binary
> radix tree. In terms of how it's indexed in memory, that involves some
> optimizations to avoid cache misses. Memory is allocated into blocks of
> about the size of an 12 cache (or maybe an l1 cache, it will require some
> testing and optimization). Blocks are either branch blocks, which keep
> everything in fixed positions, or leaf blocks, which contain fixed size
> entries for nodes plus indexes within the same leaf block of their
> children. Branch blocks can have many children which can be either branch
> blocks or leaf blocks, but typically are either all branch blocks or all
> leaf blocks. Branch blocks always have exactly one parent. Leaf blocks
> always have all their inputs come from a single branch block, but there can
> be multiple ones of those. When a branch block overflows it first tries to
> put stuff into the last leaf block it used, and if there's no more room it
> allocates a new one. It's fairly common for branches to have just a few
> leaf children, but they also could have a lot, depending on whether the
> base 2 log of the number of things currently in the set modulo the number
> levels in a branch is a small number.
> 
> Usually when an update is done it consists of first checking the
> appropriate output of the root block (it's jumped to directly to avoid
> unnecessary memory lookups. If there's nothing there the algorithm will
> walk back until it finds something.) That leads directly to (usually)
> another branch whose output is jumped to directly again. At Bitcoin utxo
> set sizes that will usually lead to a leaf block, which is then walked down
> manually to find the actual terminal node, which is then updated, and the
> parent, grandparent, etc. is then marked invalid until something which was
> already marked invalid is hit, and it exits. Calculation of hash values is
> done lazily.

I think it's safe to say that given our working set is significantly larger
than the L2/L3 cache available, none of the above optimizations are likely to
help much. Better to just keep the codebase simple and use standard techniques.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
https://www.schneier.com/crypto-gram/archives/1998/1015.html#cipherdesign

On Mon, Jul 4, 2016 at 1:23 AM, Arthur Chen <arthur.chen@btcc.com> wrote:

> I strongly agree!
> In crypto we should always follow well-studied open standard rather than
> custom construction.
>
> On Fri, Jul 1, 2016 at 10:42 PM, Zooko Wilcox via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> I haven't been able to find the beginning of this thread, so apologies
>> if I've misunderstood what this is for, but it _sounds_ like we're
>> re-inventing HKDF.
>>
>> I'd recommend reading the paper about HKDF. It stands out among crypto
>> papers for having a nice clear justification for each of its design
>> decisions, so you can see why they did it (very slightly) differently
>> than the various constructions proposed up-thread.
>>
>> https://eprint.iacr.org/2010/264
>>
>> Also, of course, it is a great idea to re-use a standard
>> (https://tools.ietf.org/html/rfc5869) and widely-understood crypto
>> algorithm to reduce risk of both cryptographer errors and implementor
>> errors.
>>
>> Of course, the cost of that is the you sometimes end up computing
>> something that is a tiny bit more complicated or inefficient than a
>> custom algorithm for our current use case. IMHO that's a cheap price
>> to pay.
>>
>> Regards,
>>
>> Zooko
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
>
> --
> Xuesong (Arthur) Chen
> Senior Principle Engineer
> BlockChain Technologist
> BTCC
>



-- 
Xuesong (Arthur) Chen
Senior Principle Engineer
BlockChain Technologist
BTCC

-------------------------------------
On Wed, Jun 29, 2016 at 08:34:06PM +0200, Jonas Schnelli via bitcoin-dev wrote:
> > Based on previous crypto analysis result, the actual security of SHA512
> > is not significantly higher than SHA256.
> > maybe we should consider SHA3?
> 
> As far as I know the security of the symmetric cipher key mainly depends
> on the PRNG and the ECDH scheme.
> 
> The HMAC_SHA512 will be used to "drive" keys from the ECDH shared secret.
> HMAC_SHA256 would be sufficient but I have specified SHA512 to allow to
> directly derive 512bits which allows to have two 256bit keys with one
> HMAC operation (same pattern is used in BIP for the key/chaincode
> derivation).

What's the rational for doing that "directly" rather than with two SHA256
operations? (specifcially SHA256(0 . thing), SHA256(1 + thing) for the two
parts we need to derive)

Reducing the # of basic cryptographic primitives you need to implement a
standard needs is a good thing.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Mon, May 9, 2016 at 11:37 PM, Peter R <peter_r@gmx.com> wrote:
> It is a standard result that there are
>     m! / [n! (m-n)!]
> ways of picking n numbers from a set of m numbers, so there are
>
>     (2^32)! / [2! (2^32 - 2)!] ~ 2^63
> possible pairs in a set of 2^32 transactions.  So wouldn’t you have to perform approximately 2^63 comparisons in order to identify which pair of transactions are the two that collide?
>
> Perhaps I made an error or there is a faster way to scan your set to find the collision.  Happy to be corrected…

$ echo -n Perhaps. 00000000f2736d91 |sha256sum
359dfa6d4c2eb2ac81535392d68af4b5e1cb6d9c6321e8f111d3244329b6a4d8
$ echo -n Perhaps. 0000000011ac0388 |sha256sum
359dfa6d4c2eb2ac44d54d0ceeb2212500cb34617b9360695432f6c0fde9b006

Try search term "collision", or there may be an undergrad Data
structures and algorithms coarse online-- you want something covering
"cycle finding".

(Though even ignoring efficient cycle finding, your factorial argument
doesn't hold... you can simply sort the data... Search term
"quicksort" for a relevant algorithm).


-------------------------------------
I've been asked one question quite regularly and recently with more force.
The question is about Segregated Witness and specifically what a hard
fork based version would look like.


This is available online at my blog;
  http://zander.github.io/posts/Flexible_Transactions/

But I'll publish the actual text here as well, hoping to hear from others in 
the Bitcoin industry what they think about this approach.




Segregated Witness (or SegWit for short) is complex. It tries to solve
quite a lot of completely different and not related issues and it tries to
do this in a backwards compatible manner. Not a small feat!

So, what exactly does SegWit try to solve? We can find info of that in the
[benefits](https://bitcoincore.org/en/2016/01/26/segwit-benefits/) document.

* Malleability Fixes
* Linear scaling of sighash operations
* Signing of input values
* Increased security for multisig via pay-to-script-hash (P2SH)
* Script versioning
* Reducing UTXO growth
* Compact fraud proofs

As mentioned above, SegWit tries to solve these problems in a backwards
compatible way. This requirement is there only because the authors of
SegWit set themselves this requirement. They set this because they wished
to use a softfork to roll out this protocol upgrade.
**This post is going to attempt to answer the question if that is indeed
the best way of solving these problems.**


Starting with Malleability, the problem is that a transaction between being
created by the owner of the funds and being mined in a block is possible to
change in such a way that it still is valid, but the transaction identifier
(TX-id) has been changed. But before we dive into the deep, lets have some
general look at the transaction data first.

If we look at a
[Transaction](http://bitcoinfactswiki.github.io/Transaction) as it is
today, we notice some issues.

<table>
<tr><td colspan=2>Version</td><td>4 bytes</td></tr>
<tr><td colspan=2>Number of inputs</td><td>VarInt (between 1 and 9 
bytes)</td></tr>
<tr><td rowspan=5 class=vertical>inputs</td><td>Prev transaction 
hash</td><td>32 bytes.
            <font color="red">Stored in reverse</font></td></tr>
<tr><td>Prev transaction index</td><td>4 bytes</td></tr>
<tr><td>TX-in script length</td><td><font color=red>Compact-
int</font></td></tr>
<tr><td>TX-in script</td><td>This is the witness data</td></tr>
<tr><td>Sequence-no/<a 
href="https://github.com/bitcoin/bips/blob/master/bip-0068.mediawiki">CSV</a>
  </td><td>4 bytes</td></tr>
<tr><td colspan=2>Number of outputs</td><td>VarInt (between 1 and 9 
byte)</td></tr>
<tr><td rowspan=3 class=vertical>outputs</td><td>Value</td><td>Var 
int</td></tr>
<tr><td>TX-out script length</td><td><font color=red>Compact-
int</font></td></tr>
<tr><td>TX-out script</td><td>bytearray</td></tr>
<tr><td colspan=2>NLockTime</td><td>4 bytes</td></tr>
</table>
</table>

The original transaction format as designed by
[Satoshi Nakamoto](http://bitcoinfactswiki.github.io/Satoshi_Nakamoto/)
had a 4 byte version. This design approach is common in the industry and
the way that this is used is that a new version is defined whenever any
field in the data structure needs changing.  In Bitcoin we have not done
this and we are still at version 1.

What Bitcoin has done instead is make small, semi backwards-compatible, 
changes.
For instance the [CHECKSEQUENCEVERIFY]
(http://bitcoinfactswiki.github.io/Script/#Locktime)
feature repurposes the sequence field as a way to add data
that would not break old clients.  Incidentally, this specific change
(described in BIP68)
is not backwards compatible in the main clients as it depends on a
transaction version number being greater than 1, they all check for
Standard transactions and say that only version 1 is standard.

The design of having a version number implies that the designer wanted to
use hard forks for changes. A new client is required to know how to parse a
newly designed data structure, this should be obvious. So the idea is to
change the version number and so older clients would know they can't parse
this new transaction version. To keep operating, everyone would have to
upgrade to a client that supports this new transaction version.

Lets look at why we would want to change the version; I marked some items in
red that are confusing. Most specifically is that numbers are stored in 3
different, incompatible formats in transactions. Not really great and
certainly a source of bugs.

Transactions are cryptographically signed by the owner of the coin so
others can validate that he is actually allowed to move the coins.
The signature is stored in the `TX-in-script`.  
Crypto-geeks may have noticed something weird that goes against any
textbooks knowledge.  What this is is that a digital
signature has to be placed outside of the thing it signs. This is because
a digital signature protects against changes. But a signature itself would
cause this change. So you have to store the signature outside the thing you
sign.

Bitcoin's creator did something smart with how transactions are actually
signed so the signature actually doesn't have to be outside the
transaction. It works. Mostly. But we want it to work flawlessly
because currently this being too smart causes the dreaded malleability
issues where people have been known to lose money.


## What about SegWit?

SegWit actually solves only one of these items. It moves the signature out
of the transaction. SegWit doesn't fix any of the other problems in Bitcoin
transactions, it also doesn't change the version after making the
transaction's-meaning essentially unable to be understood by old clients.

Old clients will stop being able to check the SegWit type of transaction,
because the authors of SegWit made it so that SegWit transactions just have
a sticker of "All-Ok" on the car while moving the real data to the trailer,
knowing that the old clients will ignore the trailer.

SegWit wants to keep the data-structure of the transaction unchanged and it
tries to fix the data structure of the transaction.  This causes friction
as you can't do both at the same time, so there will be a non-ideal
situation and hacks are to be expected.

The problem, then, is that SegWit introduces more technical debt, a term
software developers use to say the system-design isn't done and needs
significant more work.  And the term 'debt' is accurate as over time
everyone that uses transactions will have to understand the defects to work
with this properly. Which is quite similar to paying interest.

Using a Soft fork means old clients will stop being able to validate
transactions, or even parses them fully. But these old clients are
themselves convinced they are doing full validation.

## Can we improve on that?

I want to suggest a way to **one-time** change the data-structure of the
transaction so it becomes much more future-proof and fix the issues it
gained over time as well. Including the malleability issue. It turns out
that this new data-structure makes all the other issues that SegWit fixes
quite trivial to fix.

I'm going to propose an upgrade I called;

> **Flexible Transactions**

Last weekend I wrote a little app (sources [here]
(http://zander.github.io/scaling/transactions))
that reads a transaction and then writes it out in a new format I've
designed for Bitcoin. Its based on ideas I've used for some time in other
projects as well, but this is the first open source version.

The basic idea is to change the transaction to be much more like modern
systems like JSON, HTML and XML. Its a 'tag' based format and has various
advantages over the closed binary-blob format.  
For instance if you add a new field, much like tags in HTML, your old
browser will just ignore that field making it backwards compatible and
friendly to future upgrades.

Further advantages;

* Solving the malleability problem becomes trivial.
* tag based systems allow you to skip writing of unused or default values.
* Since we are changing things anyway, we can default to use only var-int
  encoded data instead of having 3 different types in transactions.
* Adding a new tag later, (for instance ScriptVersion) is easy and doesn't
  require further changes to the transaction data structure. All old clients
  can still make sense of all the known data.
* The actual transaction turns out to be about 3% shorter average (calculated
  over 200K transactions)
* Where SegWit adds a huge amount of technical debt, my Flexible
  Transactions proposal instead amortizes a good chunk of technical debt.

An average **Flexible Transaction** will look like this;

<table>
<tr><td colspan=2>TxStart (Version)</td><td>0x04</td>
<td rowspan=5 class="vertical">TX-ID data</td></tr>
<tr><td rowspan=2 class=vertical>inputs</td><td>TX-ID I try to spent</td><td>1 
+ 32 bytes</td></tr>
<tr><td>Index in prev TX-ID</td><td>varint</td></tr>
<tr><td rowspan=2 class=vertical>outputs</td><td>TX-out Value (in 
Satoshis)</td><td>VarInt</td></tr>
<tr><td>TX-out script</td><td>bytearray</td></tr>
<tr><td>inputs</td><td>TX-in-script (Witness data)<td>bytearray</td>
    <td rowspan=2 class=vertical>WID-data</td></tr>
<tr><td colspan=2>TxEnd</td><td>0x2C</td></tr>
</table>

Notice how the not used tags are skipped. The `NLockTime` and the
`Sequence` were not used, so they are skipped in the transaction.

The Flexible Transaction proposal uses a list of tags. Like JSON; `"Name:"
"Value"`. Which makes the content very flexible and extensible. Just
instead of using text, Flexible Transactions use a binary format.

The biggest change here is that the `TX-in-script` (aka the witness data) is
moved to be at the end of the transaction. When a wallet generates this new
type of transaction they will append the witness data at the end but the
transaction ID is calculated by hashing the data that ends before the
witness data.

The witness data typically contains a public key as well as a signature.
In the Flexible Transactions proposal the signature is made by signing exactly
the same set of data as is being hashed to generate the TX-input. Thereby
solving the malleability issue. If someone would change the transaction, it
would invalidate the signature.

I took 187000 recent transactions and checked what this change would do to
the size of a transaction with my test app I linked to above.

* Transactions went from a average size of 1712 bytes to 1660 bytes and a
  median size of 333 to 318 bytes.
* Transactions can be pruned (removing of signatures) after they have been
  confirmed. Then the size goes down to an average of 450 bytes or a median
  of 101 bytes
* In contrary to SegWit new transactions get smaller for all clients with this
  upgrade.
* Transactions, and blocks, where signatures are removed can expect up to
  75% reduction in size.

## Broken OP_CHECKSIG scripting instruction

To actually fix the malleability issues at its source we need to fix this
instruction. But we can't change the original until we decide to make a
version 2 of the Script language.  
This change is not really a good trigger to do a version two, and it
would be madness to do that at the same time as we roll out a new format of
the transaction itself. (too many changes at the same time is bound to
cause issues)

This means that in order to make the Flexible Transaction proposal actually
work we need to use one of the NOP codes unused in Script right now and
make it do essentially the same as OP_CHECKSIG, but instead of using the
overly complicated manner of deciding what it will sign, we just define it
to sign exactly the same area of the transaction that we also use to create
the TX-ID. (see right most column in the above table)

This new opcode should be relatively easy to code and it becomes really
easy to clean up the scripting issues we introduce in a future version of
script.

## So, how does this compare to SegWit.

First of all, introducing a new version of a transaction doesn't mean we
stop supporting the current version. So all this is perfectly backwards
compatible because clients can just continue making old style transactions.
Naturally, with the problems that had, but nobody will end up stuck.

Using a tagged format for a transaction is a one time hard fork to upgrade
the protocol and allow many more changes to be made with much lower impact
on the system in the future.  There are parallels to SegWit, it strives for
the same goals, after-all. But where SegWit tries to adjust a static
memory-format by re-purposing existing fields, Flexible transactions presents
a coherent simple design that removes lots of conflicting concepts.

Most importantly, years after Flexible transactions has been introduced we
can continue to benefit from the tagged system to extend and fix issues we
find then we haven't thought of today. In the same, consistent, concepts.

We can fit more transactions in the same (block) space similarly to SegWit, 
the
signatures (witness part) can be pruned by full nodes without causing any
security implications in both solutions.  What SegWit doesn't do is
allowing unused features to not use space. So if a transaction doesn't use
NLockTime (which is near 100% of them) they will take space in SegWit but
not in this proposal. Expect your transactions to be smaller and thus lower
fee!

On size, SegWit proposes to gain 60% space. Which is by removing the
signatures minus the overhead introduced.  In my tests Flexible
transactions showed 75% gain.

SegWit also describes changes how data is stored in the block. It creates
an extra 'branch' in the merkle tree.  The Flexible Transactions proposal
is in essence solving the same problem as SegWit and the same solution for
blocks can be applied.  Which means we can have that merkle tree solution
as well. No change.


At the start of the blog I mentioned a list of advantages that the authors
of SegWit included.  It turns out that those advantages themselves are
completely not related to each other and they each have a very separate
solution to their individual problems. The tricky part is that due to the
requirement of old clients staying forwards-compatible they are forced to
push them all into the one 'fix'.

Lets go over them individually;


### Malleability Fixes

Using this new version of a transaction data-structure solves all forms of
known malleability.

### Linear scaling of sighash operations

This has been fixed in the BIP109 2MB hardfork quite some months ago.

### Signing of input values

This is included in this proposal.

### Increased security for multisig via pay-to-script-hash (P2SH)

The *Flexible transactions* proposal outlined in this document makes many
of the additional changes in SegWit really easy to add at a later time.
This change is one of them.

Bottom line, changing the security with a bigger hash in SegWit is only
included in SegWit because SegWit didn't solve the transaction versioning
problem making it trivial to do separately.  
With flexible transactions this change can now be done at any time in the
future with minimal impact.

### Script versioning

Notice that this *only* introduces the versioning byte. It doesn't actually
introduce a new version of script.  
This is an excellent example where tagged formats shine brighter than a
static memory format that SegWit uses because adding such a versioning tag
is much cleaner and much easier and less intrusive to do with
flexible transactions. Just add a new tag that defaults to version 1 so the
old transactions not having the tag stay consistent.

Imagine having to include "body background=white" in each and every html
page because it was not allowed to leave it out. Thats what SegWit does
right now. Even though it doesn't actually support changing it yet.

### Reducing UTXO growth

I suggest you read this 
[point](https://bitcoincore.org/en/2016/01/26/segwit-benefits/#reducing-utxo-growth)
for yourself, its rather interestingly technical and I'm sure many will not
fully grasp the idea.  The bottom line of that they are claiming the
UTXO database will avoid growing because SegWit doesn't allow more
customers to be served.

I don't even know how to respond to such a solution. Its throwing out the
baby with the bath water.

Database technology has matured immensely over the last 20 years, the
database is tiny in comparison to what free and open source databases can
do today. Granted, the UTXO database is slightly unfit for a normal SQL
database, but telling customers to go elsewhere has never worked out for
products in the long term.

### Compact fraud proofs

Again, not really included in SegWit, just started as a basis. The exact
same basis is suggested for flexible transactions, and as such this is
identical.

### What do we offer that SegWit doesn't offer?

* A transaction becomes extensible. Future modifications are cheap.
* A transaction gets smaller. Using less features also takes less space.
* We only use one instead of 3 types of encodings for integers.
* We remove technical debt and simplify implementations. SegWit does the
  opposite.

## Conclusions

SegWit has some good ideas and some needed fixes. Stealing all the good
ideas and improving on them can be done, but require a hard fork. This post
shows that the advantages are quite significant and certainly worth it.

We introduced a tagged data structure. Conceptually like JSON and XML in
that it is flexible, but the proposal is a compact and fast binary format.
Using the Flexible Transaction data format allows many future
innovations to be done cleanly in a consistent and, at a later stage, a
more backwards compatible manner than SegWit is able to do, even if given
much more time. We realize that separating the fundamental issues that
SegWit tries to fix all in one go, is possible and each becomes much lower
risk to Bitcoin as a system.

After SegWit has been in the design stage for a year and still we find
show-stopping issues, delaying the release, I would argue that dropping
the requirement of staying backwards compatible should be on the table.

The introduction of the *Flexible Transaction* upgrade has big benefits
because the transaction design becomes extensible.
A hardfork is done once to allow us to do soft upgrades in the future.

The Flexible transaction solution lowers the amount of changes required in
the entire ecosystem. Not just for full nodes. Especially considering that
many tools and wallets use shared libraries between them to actually create
and parse transactions.

The Flexible Transaction upgrade proposal should be considered by anyone
that cares about the protocol stability because its risk of failures
during or after upgrading is several magnitudes lower than SegWit is and
it removes technical debt that will allow us to innovate better into the
future.

Flexible transactions are smaller, giving significant savings after pruning
over SegWit.


Thanks


-------------------------------------
Hi all,

It is possible to implement covenants using two script extensions: OP_CAT
and OP_CHECKSIGFROMSTACKVERIFY.  Both of these op codes are already
available in the Elements Alpha sidechain, so it is possible to construct
covenants in Elements Alpha today.  I have detailed how the construction
works in a blog post at <
https://blockstream.com/2016/11/02/covenants-in-elements-alpha.html>.  As
an example, I've constructed scripts for the Moeser-Eyal-Sirer vault.

I'm interested in collecting and implementing other useful covenants, so if
people have ideas, please post them.

If there are any questions, I'd be happy to answer.

-- 
Russell O'Connor

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512



On 11 May 2016 22:27:09 GMT-04:00, Tom Harding via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>On 5/10/2016 2:43 PM, Sergio Demian Lerner via bitcoin-dev wrote:
>>
>> If we change the protocol then the message to the ecosystem is that
>> ASIC optimizations should be kept secret.
>
>Further to that point, if THIS optimization had been kept secret,
>nobody
>would be talking about doing anything, as with countless other
>optimizations.

The optimisation has been independently discovered two or three times (Spondoolies and maybe Bitmain).
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJXM+tK
AAoJEGOZARBE6K+yz4MH/j9TstqbVNG3nU+SJ9+Q9aZ0mZSQfR+4qgybGridjo7H
TzGCnBVCLHt0LnbmZheFv/k9p+m2PojvGGKfODLIDFDHVPHv2wKflKIANIqxpXh/
Bl1SObDoKlRyby4fT22dW5SVSJsjVwTrYwTr2fmRfroeCLgJrHrr03AD7qmMf7CN
MPrlpitLHZiEoSThTas3pTEEgL2EBgfZnxaaj96jQaMJloz0WjQaocllahl/gsme
40BQ9TnSHZ02bBf9iEN/FqGhrEN8m2JL7AEyOCuGwrWJtfQ5b9kSpL2QSpuXSfQ7
1d+OialY2G2L3QMPlnBMKdWGscUyapkYax3FmyA6wxI=
=j9k+
-----END PGP SIGNATURE-----



-------------------------------------
I would really like to be able to create transactions that are immune to
transaction ID malleability now, so I have been thinking of the simplest
solution possible, in order to get a BIP through without too much trouble.

An opcode we could call OP_TXHASHVERIFY could be introduced. It would be
defined to work only if added to a scriptSig as the very first operation,
and would abort if the hash of the transaction **with all OP_TXHASHVERIFY
operations (including stack push) removed** does not match what has been
pushed on the stack.

So, in order to produce a transaction with one or more inputs protected
against tx ID malleability, one would:

1. Calculate tx ID of the tx: TX_HASH
2. For each input you wish to protect, add "0x32 $TX_HASH OP_TXHASHVERIFY"
to the beginning of the scriptSig

When evaluating OP_TXHASHVERIFY, we make a copy of the tx in question, and
remove the "0x32 <32 bytes> OP_TXHASHVERIFY" sequence from the beginning of
all scriptSigs (if present), and abort if the tx copy hash does not match
the top stack item.

This is a very simple solution that only adds 34 bytes per input, and when
something better becomes available (eg. Segwit), we will stop using this.
But in the meantime it's very valuable to be able to not worry about tx ID
malleability.

Please let me know what you think.



            /Rune

-------------------------------------
On 11/16/2016 06:18 AM, Thomas Kerin wrote:
> BIP30 actually was given similar treatment after a reasonable amount of
> time had passed.
> https://github.com/bitcoin/bitcoin/blob/master/src/main.cpp#L2392

BIP30 was the resolution to a catostrophic protocol flaw that would
impact any block whether above or below the point where the rule was
applied. Applying it to all future blocks, regardless of whether there
is a reorg back to genesis, was the only option as far as I can tell. So
the comparison to an unnecessary fork is hardly apt.

> You are also missing BIP50: 'March 2013 Chain For Post-Mortem', which
> neither benefited nor improved bitcoin, but did document an event for
> posterity.

BIP50 documents the release of an "unexpected" hard fork to a large
number of users. Given that Core code is considered by some to be the
*definition* of the true protocol, this led to two "legitimate" Bitcoin
chains. Leveraging the centralized state of Bitcoin mining, the
development team was able to kill the newer chain. This was simply an
altcoin that didn't survive because people stopped using it.

Anyone can create an altcoin - the question here is specifically, why
would we want to do so in this case.

> This is not a hard fork. Removing ISM just means we've committed to
> those soft-forks only locking into the chain we use now.

There didn't seem to be any confusion among the implementers that it is
a hard fork.

I will correct one implication I made below. The heights in the proposal
are required in the absence of BIP34-style activation so that the soft
fork validation rules can be properly enforced at those points (whether
or not a deep reorg happens).

e

> On 11/16/2016 01:58 PM, Eric Voskuil via bitcoin-dev wrote:
>> This sort of statement represents one consequence of the
>> aforementioned bad precedent.
>>
>> Are checkpoints good now? Are hard forks okay now?
>>
>> What is the maximum depth of a reorg allowed by this non-machine
>> consensus?
>>
>> Shouldn't we just define a max depth so that all cruft deeper than
>> that can just be discarded on a regular basis?
>>
>> Why are there activation heights defined by this hard fork if it's not
>> possible to reorg back to them?
>>
>> The "BIP" is neither a Proposal (it's been decided, just documenting
>> for posterity), nor an Improvement (there is no actual benefit, just
>> some tidying up in the notoriously obtuse satoshi code base), nor
>> Bitcoin (a hard fork defines an alt coin, so from Aug 4 forward it has
>> been CoreCoin).
>>
>> e
>>
>> On Nov 16, 2016, at 5:29 AM, Jameson Lopp <jameson.lopp@gmail.com
>> <mailto:jameson.lopp@gmail.com>> wrote:
>>
>>> Since "buried deployments" are specifically in reference to
>>> historical consensus changes, I think the question is more one of
>>> human consensus than machine consensus. Is there any disagreement
>>> amongst Bitcoin users that BIP34 activated at block 227931, BIP65
>>> activated at block 388381, and BIP66 activated at block 363725?
>>> Somehow I doubt it.
>>>
>>> It seems to me that this change is merely cementing into place a few
>>> attributes of the blockchain's history that are not in dispute.
>>>
>>> - Jameson
>>>
>>> On Tue, Nov 15, 2016 at 5:42 PM, Eric Voskuil via bitcoin-dev
>>> <bitcoin-dev@lists.linuxfoundation.org
>>> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>>>
>>>     Actually this does nothing to provide justification for this
>>>     consensus rule change. It is just an attempt to deflect criticism
>>>     from the fact that it is such a change.
>>>
>>>     e
>>>
>>>     > On Nov 15, 2016, at 9:45 AM, Btc Drak <btcdrak@gmail.com
>>>     <mailto:btcdrak@gmail.com>> wrote:
>>>     >
>>>     > I think this is already covered in the BIP text:-
>>>     >
>>>     > "As of November 2016, the most recent of these changes (BIP 65,
>>>     > enforced since December 2015) has nearly 50,000 blocks built on
>>>     top of
>>>     > it. The occurrence of such a reorg that would cause the activating
>>>     > block to be disconnected would raise fundamental concerns about the
>>>     > security assumptions of Bitcoin, a far bigger issue than any
>>>     > non-backwards compatible change.
>>>     >
>>>     > So while this proposal could theoretically result in a consensus
>>>     > split, it is extremely unlikely, and in particular any such
>>>     > circumstances would be sufficiently damaging to the Bitcoin
>>>     network to
>>>     > dwarf any concerns about the effects of this proposed change."
>>>     >
>>>     >
>>>     > On Mon, Nov 14, 2016 at 6:47 PM, Eric Voskuil via bitcoin-dev
>>>     > <bitcoin-dev@lists.linuxfoundation.org
>>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>>>     >> NACK
>>>     >>
>>>     >> Horrible precedent (hardcoding rule changes based on the
>>>     assumption that
>>>     >> large forks indicate a catastrophic failure), extremely poor
>>>     process
>>>     >> (already shipped, now the discussion), and not even a material
>>>     performance
>>>     >> optimization (the checks are avoidable once activated until a
>>>     sufficiently
>>>     >> deep reorg deactivates them).
>>>     >>
>>>     >> e
>>>     >>
>>>     >> On Nov 14, 2016, at 10:17 AM, Suhas Daftuar via bitcoin-dev
>>>     >> <bitcoin-dev@lists.linuxfoundation.org
>>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>>>     >>
>>>     >> Hi,
>>>     >>
>>>     >> Recently Bitcoin Core merged a simplification to the consensus
>>>     rules
>>>     >> surrounding deployment of BIPs 34, 66, and 65
>>>     >> (https://github.com/bitcoin/bitcoin/pull/8391
>>>     <https://github.com/bitcoin/bitcoin/pull/8391>), and though the
>>>     change is a
>>>     >> minor one, I thought it was worth documenting the rationale in
>>>     a BIP for
>>>     >> posterity.
>>>     >>
>>>     >> Here's the abstract:
>>>     >>
>>>     >> Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated
>>>     via miner
>>>     >> signaling in block version numbers. Now that the chain has
>>>     long since passed
>>>     >> the blocks at which those consensus rules have triggered, we
>>>     can (as a
>>>     >> simplification and optimization) replace the trigger mechanism
>>>     by caching
>>>     >> the block heights at which those consensus rules became enforced.
>>>     >>
>>>     >> The full draft can be found here:
>>>     >>
>>>     >>
>>>     https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki
>>>     <https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki>
>>>     >>
>>>     >> _______________________________________________
>>>     >> bitcoin-dev mailing list
>>>     >> bitcoin-dev@lists.linuxfoundation.org
>>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>>>     >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
>>>     >>
>>>     >>
>>>     >> _______________________________________________
>>>     >> bitcoin-dev mailing list
>>>     >> bitcoin-dev@lists.linuxfoundation.org
>>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>>>     >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
>>>     >>
>>>     _______________________________________________
>>>     bitcoin-dev mailing list
>>>     bitcoin-dev@lists.linuxfoundation.org
>>>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>>>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
>>>
>>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
> The OP's proposal sounds quite similar to my earlier one along similar lines:
>
>    https://petertodd.org/2016/closed-seal-sets-and-truth-lists-for-privacy

Similar indeed, thank you for the link.


2016-08-09 0:53 GMT+03:00 Peter Todd <pete@petertodd.org>:
>
> On Mon, Aug 08, 2016 at 09:41:27PM +0000, James MacWhyte via bitcoin-dev wrote:
> > Wouldn't you lose the ability to assume transactions in the blockchain are
> > verified as valid, since miners can't see the details of what is being
> > spent and how? I feel like this ability is bitcoin's greatest asset, and by
> > removing it you're creating an altcoin different enough to not be connected
> > to/supported by the main bitcoin project.
>
> The fact that miners verify transactions is just an optimisation:
>
>     https://petertodd.org/2013/disentangling-crypto-coin-mining
>
> Preventing double-spending however is a fundemental requirement of Bitcoin, and
> this proposal does prevent double-spending perfectly well (although there may
> be better ways to do it).
>
> The OP's proposal sounds quite similar to my earlier one along similar lines:
>
>     https://petertodd.org/2016/closed-seal-sets-and-truth-lists-for-privacy
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org


-------------------------------------
BIP 0070 has been a a moderate success, however, IMO:

- protocol buffers are inappropriate since ease of use and extensibility is
desired over the minor gains of efficiency in this protocol.  Not too late
to support JSON messages as the standard going forward

- problematic reliance on merchant-supplied https (X509) as the sole form
of mechant identification.   alternate schemes (dnssec/netki), pgp and
possibly keybase seem like good ideas.   personally, i like keybase, since
there is no reliance on the existing domain-name system (you can sell with
a github id, for example)

- missing an optional client supplied identification

- lack of basic subscription support

*Proposed for subscriptions:*

- BIP0047 payment codes are recommended instead of wallet addresses when
establishing subscriptions.  Or, merchants can specify replacement
addresses in ACK/NACK responses.   UI confirms are *required *when there
are no replacement addresses or payment codes used.

- Wallets must confirm and store subscriptions, and are responsible for
initiating them at the specified interval.

- Intervals can *only *be from a preset list: weekly, biweekly, or 1,
2,3,4,6 or 12 months.   Intervals missed by more than 3 days cause
suspension until the user re-verifies.

- Wallets *may *optionally ask the user whether they want to be notified
and confirm every interval - or not.   Wallets that do not ask *must *notify
before initiating each payment.   Interval confirmations should begin at *least
*1 day in advance of the next payment.


*Proposed in general:*
- JSON should be used instead of protocol buffers going forward.  Easier to
use, explain extend.

- "Extendible" URI-like scheme to support multi-mode identity mechanisms on
both payment and subscription requests.   Support for keybase://, netki://
and others as alternates to https://.

- Support for client as well as merchant multi-mode verification

- Ideally, the identity verification URI scheme is somewhat
orthogonal/independent of the payment request itself

Question:

Should this be a new BIP?  I know netki's BIP75 is out there - but I think
it's too specific and too reliant on the domain name system.

Maybe an identity-protocol-agnostic BIP + solid implementation of a couple
major protocols without any mention of payment URI's ... just a way of
sending and receiving identity verified messages in general?

I would be happy to implement plugins for identity protocols, if anyone
thinks this is a good idea.

Does anyone think https:// or keybase, or PGP or netki all by themselves,
is enough - or is it always better to have an extensible protocol?

- Erik Aronesty

-------------------------------------
Its mostly a problem for exchanges and miners. Those entities need to
be on the network 100% of the time because they are using the network
100% of the time. A normal wallet user isn't taking payments every few
minutes like the exchanges are. "Getting booted off the network" is
not something to worry about for normal wallet users.

If miners aren't up to date, that is the biggest problem. A sudden
drop in hashpower will effect the network for all users, including
normal wallet users (by them having to wait longer for confirmations).
Miners must not be booted off the network ever. Hashpower voting is
the best way to make sure this never happens.

On 2/6/16, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Saturday, February 06, 2016 06:09:21 PM Jorge Timón via bitcoin-dev
> wrote:
>> None of the reasons you list say anything about the fact that "being
>> lost"
>> (kicked out of the network) is a problem for those node's users.
>
> That's because its not.
>
> If you have a node that is "old" your node will stop getting new blocks.
> The node will essentially just say "x-hours behind" with "x" getting larger
>
> every hour. Funds don't get confirmed. etc.
>
> After upgrading the software they will see the new reality of the network.
>
> Nobody said its a problem, because its not.
>
> --
> Tom Zander
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
We need to make sure the revocation message is widely distributed before making the private key public


 ---- On Sat, 10 Sep 2016 21:23:37 +0800 Andrew C <achow101@gmail.com> wrote ---- 
 > On 9/10/2016 5:41 AM, Johnson Lau via bitcoin-dev wrote: 
 > > 3. After a few months or so, publish the private key. 
 > Why wait a few months? Why not just publish the key a few days after the 
 > final alert? 
 > 




-------------------------------------
On Thu, Jan 7, 2016 at 8:26 PM, Matt Corallo <lf-lists@mattcorallo.com>
wrote:

> So just because other attacks are possible we should weaken the crypto
> we use? You may feel comfortable weakening crypto used to protect a few
> billion dollars of other peoples' money, but I dont.
>

No...

I'm saying we can eliminate one somewhat unlikely attack (that there is a
bug in the code or test cases, today or some future version, that has to
decide what to do with "version 0" versus "version 1" witness programs) by
accepting the risk of another insanely, extremely unlikely attack.

Reference for those who are lost:

https://github.com/CodeShark/bips/blob/segwit/bip-codeshark-jl2012-segwit.mediawiki#witness-program

My proposal would be to just do a version 0 witness program now, that is
RIPEMD160(SHA256(script)).

And ten or twenty years from now, if there is a plausible attack on
RIPEMD160 and/or SHA256, revisit and do a version 11 (or whatever).

It will simplify the BIP, means half as many test cases have to be written,
means a little more scalability, and is as secure as the P2SH and P2PKH
everybody is using to secure their bitcoin today.

Tell you what:  I'll change my mind if anybody can describe a plausible
attack if we were using MD5(SHA256), given what we know about how MD5 is
broken.


---

I'm really disappointed with the "Here's the spec, take it or leave it"
attitude. What's the point of having a BIP process if the discussion just
comes down to "We think more is better. We don't care what you think."

-- 
--
Gavin Andresen

-------------------------------------
There are several opening pull requests for segwit related consensus and policy rules. This email summarize and explain the rationale.

As a general warning, people must not assume that a script spendable in pre-segwit system would also be spendable as a segwit script. They share much similarity but there are also notable differences, such as BIP143 and those proposals listed below. In any case, test your segwit system on testnet with the standard rules turned on, and a small amount of money after segwit is activated on mainnet.

*******************
Script Malleability fixes: Segwit (BIP141) fixes the most nasty malleability in Bitcoin: transaction ID malleability. However, due to the flexibility of scripting system, it is still possible for a relay node to insert arbitrary data to the witness without invalidating the transaction. Although segwit makes such attacks much harmless, this could still be annoying as people may write data to the blockchain at others costs.

NULLDUMMY, MINIMALIF, NULLFAIL are fixing this type of problem. NULLDUMMY has been implemented as a policy for more than a year and a softfork is proposed in the upcoming 0.13.1. MINIMALIF and NULLFAIL are both new policy proposed for 0.13.1, and may become softforks in the future. Script designers must pay attention to these potential softforks to avoid creation of unspendable scripts.

Consensus:
BIP147 "NULLDUMMY" softfork (for both segwit and pre-segwit scripts)
PR: https://github.com/bitcoin/bitcoin/pull/8636
Related discussion: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-September/013096.html

Policy:
"MINIMALIF" Minimal OP_IF/NOTIF argument (segwit scripts only)
PR: https://github.com/bitcoin/bitcoin/pull/8526
Related discussion: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-August/013014.html

Policy:
"NULLFAIL" Null signature for failed CHECK(MULTI)SIG (for both segwit and pre-segwit scripts)
PR: https://github.com/bitcoin/bitcoin/pull/8634
Related discussion: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-September/013098.html

*******************

Policy: Resources limit for P2WSH
PR: https://github.com/bitcoin/bitcoin/pull/8499

For P2WSH, a policy limit is proposed with witnessScript <= 3600 bytes, witness stack item size <= 80 bytes, and witness stack items <= 100

3600 bytes witnessScript and 100 stack items are adequate for a n-of-100 multisig using 100 OP_CHECKSIG, 99 OP_ADD, and 1 OP_EQUAL. Before segwit, the biggest standard mutlisig is n-of-15 with P2SH.

The max size for ECDSA signature is 73 bytes and nothing (except hashing opcodes) should use more than that with the current scripting language.

This is to prevent abuse of witness space, and reduce the risks of DoS attack with some unknown special and big scripts.

The consensus limits described in BIP141 are not changed, as witnessScript <= 10000 bytes and  witness stack item size <= 520 bytes. (There is also an implied limit for witness stack items of 412, see the inline comments in #8499)

*******************

Policy: Public key must be compressed (segwit only)
PR: https://github.com/bitcoin/bitcoin/pull/8499

It is proposed that only compressed keys (33 bytes starting with 0x02 or 0x03) are allowed in segwit scripts.

This is a policy only and non-compressed keys are still valid in a block. A softfork based on this may be proposed with further risks and benefits analysis

We can't have such policy or softfork in non-segwit scripts since there are many UTXOs being stored that way. Since segwit is a completely new script system, there is no strong reasons to support non-compressed keys.

Wallet developers must pay attention to this policy and must not assume that existing P2PKH hashes or P2SH scripts are spendable in segwit.

The RPC command addwitnessaddress will refuse to return a segwit address if the given key/multi-sig is unknown or is not compressed.

createwitnessaddress will return an address for whatever scripts given, without checking the validity at all. (even an OP_RETURN is provided, it will still return a P2WSH address). We may need to give a warning, or simply remove this command.

*******************

DoS protection: Banning peers for sending certain types of consensus invalid witness
PR: https://github.com/bitcoin/bitcoin/pull/8499

Peers sending certain types of invalid witness will be banned before fee and SigOp policy are checked. Those are all based on explicit or implicit consensus rules, and will protect P2WPKH and canonical multisigs against the DoS issues described in #8279. The rest of P2WSH scripts will be covered by #8525 by not storing witness txs in rejection cache.

*******************

DoS protection:  Mandatory softfork flags for segwit txs
PR: https://github.com/bitcoin/bitcoin/pull/8499

Since all segwit-aware nodes must be aware of all existing softforks, including BIP66, 65, 112, 141, and 143, the verification flags for these BIPs will be mandatory for transactions with non-empty witness.  Wallets relaying witness transactions violating these rules will be banned (even if the violation happens in a non-segwit input).






-------------------------------------
Hi Ben,

not sure if this is the right mailing list for that. I think it rather
belongs to bitcoin-discuss.

And I am also not sure if I understand your question. What you may
mean is the private key of a user. If you find this, you can spend his
funds and also prove that you own the funds.

Depending on your level of understanding of Bitcoin, this blogpost may
be insightful for you:
http://www.michaelnielsen.org/ddi/how-the-bitcoin-protocol-actually-works/

All the best
Henning


On Tue, Dec 13, 2016 at 11:13:24AM +0000, Ben West via bitcoin-dev wrote:
> Hello all;
> is there any number or id that determine uniquely the BTC value.
> otherwise; is there any hash or address or key that when found we could say
> this is my 50BTC or my 20BTC ?.

> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------
On Sun, Oct 02, 2016 at 02:00:01PM -0300, Sergio Demian Lerner wrote:
> Peter, are you really going to try to down vote a decent free and
> open-source proposal that benefits all the Bitcoin community including
> you and your future children because a personal attack to me without any
> logic or basis?

I've suggested a way that you can rectify this situation so we can continue to
collaborate: Have Rootstock adopt a legally binding patent pledge/license. I'd
suggest you do as Blockstream has done and at minimum adopt the Defensive
Patent License (DPL); I personally will be doing so in the next week or two for
my own consulting company (I'm discussing exactly how to do so with my lawyer
right now).

If Rootstock is not planning on getting any patents for offensive purposes,
then there is no issue with doing so - the DPL in particular is designed in a
minimally intrusive way.

Please fix this issue so we can in fact continue to collaborate to improve
Bitcoin.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
There are already valid use cases for OP_RETURN, it only makes sense to
fully support the feature. The only reason it's not supported now is
because the Payments protocol came before OP_RETURN.

I give one example use case in the BIP. I agree that special wallet support
would make the feature even better, but if someone tried to use Core the
transaction would at least not be rejected.

I've also been exploring this area with key.run (
https://git.playgrub.com/toby/keyrun) and want the functionality for voting
based on aggregate OP_RETURN value. *Not* to store data on the blockchain,
but to associate content pointers with transactions.

I think that since OP_RETURN has already been approved and supported it
doesn't make much sense for me to have to re-defend it from scratch here.

On Mon, Jan 25, 2016 at 7:23 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Tuesday, January 26, 2016 3:17:12 AM Toby Padilla wrote:
> > I don't think every application of OP_RETURN could be classified as
> "spam".
>
> Perhaps not, but in this context I cannot think of any non-spam use cases.
> Use cases should come before changes to support them.
>
> > I also don't think burning the value is going to dissuade anyone from
> going
> > down that route. I don't think lost value is better for anyone.
>
> Lost value is better because it has a cost to the spammer, and deflates the
> rest of the bitcoins.
>
> Luke
>

-------------------------------------
On 2016-01-05 09:26, joe2015--- via bitcoin-dev wrote:
> On 2016-01-05 02:04, Nick ODell wrote:
>> How are you collecting fees from the transactions in the block?
> 
> Probably the simplest way to do this is to map the new-rules coinbase
> tx (which collects the block reward and fees) into an old-rules legacy
> coinbase tx (which collects the block reward only).  Care must be
> taken to ensure the mapping is not reversible.  I will update my
> implementation in due course.

The redesigned implementation is here:

https://github.com/ZoomT/bitcoin/tree/2015_2mb_blocksize
https://github.com/jgarzik/bitcoin/compare/2015_2mb_blocksize...ZoomT:2015_2mb_blocksize

The new version maps the Merkle root onto a 'legacy' coinbase 
transaction, solving the problem with fees.

--joe.


-------------------------------------
Binaries for Bitcoin Knots version 0.12.0.knots20160226.rc1 are available from:

    https://bitcoinknots.org/files/0.12.x/0.12.0.knots20160226/test/rc1/

Source code can be found on GitHub under the signed tag:

    https://github.com/bitcoinknots/bitcoin/tree/v0.12.0.knots20160226.rc1

This is a release candidate for a new major version release, bringing new
features, bug fixes, as well as other improvements.

Preliminary release notes for the release can be found here:

    https://github.com/bitcoinknots/bitcoin/blob/0.12.x-knots/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as final.

Please report bugs using the issue tracker at GitHub:

    https://github.com/bitcoinknots/bitcoin/issues

Additional Gitian signatures are welcome. Please submit via GitHub pull
request to:

    https://github.com/bitcoinknots/gitian.sigs


-------------------------------------
First, non-practicing entities are definitely a problem, but they're far
from the only companies involved in software patent litigation. As you say
yourself, one reason companies obtain patents is to "prevent
competition"—meaning they produce a competing product. Look at the
billion-dollar lawsuits in the smartphone patent wars.

(Unless you're referring to "patent privateers" that assert claims and are
only indirectly sponsored and controlled by real tech companies. I don't
think the Defensive Patent License directly addresses that problem—Section
7.1 defines an "affiliate" relatively narrowly—although the Apache 2.0
license arguably does, with its broad definition of "Legal Entity." I don't
profess to know anything about the reasoning behind the DPL's wording, and
I may be missing something; maybe a future version of the DPL will close
that loophole if it becomes an actual problem.)

Second, as several people have noted on this list, patent applications
unfortunately seem to be more effective than defensive publication at
getting prior art under the noses of the patent examiners. So obtaining a
patent for defensive purposes makes it more difficult for others to obtain
patents on the same subject matter.

(Usual disclaimers apply. Nothing you read on bitcoin-dev is legal advice;
don't take legal advice from mailing lists; come on)

On Fri, Oct 14, 2016 at 12:01 PM Nick ODell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

Pledging to not use patents offensively defeats the point of owning patents.
The point of owning a patent is so that you can use it offensively, either
to
prevent competition, or get licensing fees.

Obtaining a patent for defense doesn't make sense. The litigants you need to
worry about do not produce or make anything. Their 'product' is patent
lawsuits.

Unless you have a patent on using a mail-merge program to sue people, your
defensive patents are useless in that situation.

On Fri, Oct 14, 2016 at 4:57 AM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> On Fri, Oct 14, 2016 at 07:38:07AM -0300, Sergio Demian Lerner via
bitcoin-dev wrote:
>> I read the DPL v1.1 and I find it dangerous for Bitcoin users. Current
>> users may be confident they are protected but in fact they are not, as
the
>> future generations of users can be attacked, making Bitcoin technology
>> fully proprietary and less valuable.
>
> Glad to hear you're taking a conservative approach.
>
> So I assume Rootstock is going to do something stronger then, like
> Blockstream's DPL + binding patent pledge to only use patents defensively?
>
>     https://www.blockstream.com/about/patent_pledge/
>
> Because if not, the DPL is still better than the status quo.
>
>> If you read the DPL v1.1 you will see that companies that join DPL can
>> enforce their patents against anyone who has chosen not to join the DPL.
>> (http://defensivepatentlicense.org/content/defensive-patent-license)
>>
>> So basically most users of Bitcoin could be currently under threat of
being
>> sued by Bitcoin companies and individuals that joined DPL in the same way
>> they might be under threat by the remaining companies. And even if they
>> joined DPL, they may be asked to pay royalties for the use of the
>> inventions prior joining DPL.
>>
>> DPL changes nothing for most individuals that cannot and will not hire
>> patent attorneys to advise them on what the DPL benefits are and what
>> rights they are resigning. Remember that patten attorneys fees may be
>> prohibitive for individuals in under-developed countries.
>>
>> Also DPL is revocable by the signers (with only a 180-day notice), so if
>> Bitcoin Core ends up using ANY DPL covered patent, the company owning the
>> patent can later force all new Bitcoin users to pay royalties.
>
> Indeed. However, you're also free to adopt the DPL irrevocably by
additionally
> stating that you will never invoke that 180-day notice provision (or more
> humorously, make it a 100 year notice period to ensure any patents
expire!).
>
> If you're concerned about this problem, I'd suggest that Rootstock do
exactly
> that.
>
>> Because Bitcoin user base grows all the time with new individuals, the
sole
>> existence of DPL licensed patents in Bitcoin represents a danger to
Bitcoin
>> future almost the same as the existence of non-DPL license patents.
>
> To be clear, modulo the revocability provision, it's a danger mainly to
those
> who are unwilling to adopt the DPL themselves, perhaps because they
support
> software patents.
>
>> If you're publishing all your ideas and code (public disclosure), you
>> cannot later go and file a patent in most of the world except the US,
where
>> you have a 1 year grace period. So we need to do something specific to
>> prevent the publishers filing a US patent.
>
> Again, lets remember that you personally proposed a BIP[1] that had the
effect
> of aiding your ASICBOOST patent[2] without disclosing that fact in your
BIP nor
> your pull-req[3]. The simple fact is we can't rely solely on voluntary
> disclosure - your own behavior is a perfect example of why not.
>
> [1]: BIP: https://github.com/BlockheaderNonce2/bitcoin/wiki
> [2]: ASICBOOST PATENT https://www.google.com/patents/WO2015077378A1?cl=en
> [3]: Extra nonce pull request:
https://github.com/bitcoin/bitcoin/pull/5102
>
>> What we need much more than DPL, we need that every BIP and proposal to
the
>> Bitcoin mailing list contains a note that grants all Bitcoin users a
>> worldwide, royalty-free, no-charge, non-exclusive, irrevocable license
for
>> the content of the e-mail or BIP.
>
> A serious problem here is the definition of "Bitcoin users". Does Bitcoin
> Classic count? Bitcoin Unlimited? What if Bitcoin forks?
>
> Better to grant _everyone_ a irrevocable license.
>
>
> Along those lines, it'd be reasonable to consider changing the Bitcoin
Core
> license to something like an Apache2/LGPL3 dual license to ensure the
copyright
> license also has anti-patent protections.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

A new BIP is prepared to deal with OP_IF and OP_NOTIF malleability in P2WSH:
https://github.com/jl2012/bips/blob/minimalif/bip-minimalif.mediawiki
https://github.com/bitcoin/bitcoin/pull/8526

   BIP: x
   Title: Dealing with OP_IF and OP_NOTIF malleability in P2WSH
   Author: Johnson Lau <jl2012@xbt.hk>
   Status: Draft
   Type: Standards Track
   Created: 2016-08-17

Abstract

This document specifies proposed changes to the Bitcoin script validity rules in order to make transaction malleability related to OP_IF and OP_NOTIF impossible in pay-to-witness-script-hash (P2WSH) scripts.

Motivation

OP_IF and OP_NOTIF are flow control codes in the Bitcoin script system. The programme flow is decided by whether the top stake value is True or False. However, this behaviour opens a source of malleability as a third party may replace a True (False) stack item with any other True (False) value without invalidating the transaction.

The proposed rules apply only to pay-to-witness-script-hash (P2WSH) scripts described in BIP141, which has not been activated on the Bitcoin mainnet as of writing. To ensure OP_IF and OP_NOTIF transactions created before the introduction of this BIP will still be accepted by the network, the new rules are not applied to non-segregated witness scripts.

Specification

In P2WSH, the argument for OP_IF and OP_NOTIF MUST be exactly an empty vector or 0x01, or the script evaluation fails immediately.

This is deployed using BIP9 after segregated witness (BIP141) is activated. Details TBD.

Compatibility

This is a softfork on top of BIP141. The rules are enforced as a relay policy by the reference client since the first release of BIP141 (v0.13.1). To avoid risks of fund loss, users MUST NOT create P2WSH scripts that are incompatible with this BIP. An OP_0NOTEQUAL may be used before OP_IF or OP_NOTIF to imitate the original behaviour (which may also re-enable the malleability vector depending on the exact script).

Implementation

https://github.com/bitcoin/bitcoin/pull/8526

Copyright

This work is placed in the public domain.
-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - https://gpgtools.org

iQGcBAEBCgAGBQJXs1LgAAoJEO6eVSA0viTSrJQL/A/womJKgi4FuyBTL9oykCss
aBMNN9+SLtmuH7SBgEUGZ8TFxa2st+6RP6Imu+Vvn4O5sXQl3DIXV+X38X93sUYk
wrjdpvdpqFFYJezPDESz6pR/6bZ1ES0aO2QqX578/8sqr8GO6L388s66vJeIGj4n
0LWW8sdEypMuV3HUG/9FFdUNHgiVX1U0sS1rT3P4aN30JYtb7PQpd7r8KTMta7Rt
L1VOZB+W3m2m2YZ9gB7IRmMfzzNm2QXRTPIZXt2x3mYDBuMkp+zEd5+ogA4sBpgP
wp2+l/aos686v0w8QYiNUX2+9Qpe7+238qUpw75d2XJYmLzdotWFvmp4g1hP+awX
HEfwe4BUM+El17LjrHkNeMWNJXMlhTtXb2i0XMj8tU5lZVHep4WpQ+LEahrNlsUl
FdFsi3q8HeWh8JsGaNCL41Bgbg/rKb5hUXyF6hTRHa//E6llOrpXRnsloKgBLv8c
QezgKTAPwwgdjcS6Ek0AqgLp7bCFRijCduYH9i9uaQ==
=lLIZ
-----END PGP SIGNATURE-----


-------------------------------------
That's the reason for this post! All current major ASIC manufacturers
have made warrants that they are not using AsicBoost (with the exception
of the 21 Inc Bitcoin computer).

The fact that the optimization was patented is what has required that we
work to hardfork it out, not that people might have such private
optimizations. The fact that AsicBoost was independently discovered by
at least two (if not three) organizations seems to lend credence to the
idea that private optimizations will only provide a temporary win over
competitors.

Matt

On 05/11/16 03:14, Timo Hanke via bitcoin-dev wrote:
> There is no way to tell from a block if it was mined with AsicBoost or
> not. So you dont know what percentage of the hashrate uses AsicBoost at
> any point in time. How can you risk forking that percentage out? Note
> that this would be a GUARANTEED chain fork. Meaning that after you
> change the block mining algorithm some percentage of hardware will no
> longer be able to produce valid blocks. That hardware cannot switch
> over to the majority chain even if it wanted to. Hence you are
> guaranteed to have two co-existing bitcoin blockchains afterwards.
> 
> Again: this is unlike the hypothetical persistence of two chains after a
> hardfork that is only contentious but doesnt change the mining
> algorithm, the kind of hardfork you are proposing would guarantee the
> persistence of two chains.
> 
> Note that AsicBoost above is replaceable with optimization X. Its
> simply a logical argument: If you want to make optimization X impossible
> and someone is already using optimization X you end up with two chains.
> So unless you know exactly which optimizations are in use (and therefore
> also know which ones are not in use) you cant make these kind of
> changes. AsicBoost is known at least since middle of 2013.
> 
> To be more precise, if you change the block validation ruleset R to
> block validation ruleset S you have to make sure that every hardware
> that was capable of mining R-valid blocks is also capable of mining
> S-valid blocks. 
> 
> The problem is that chip manufacturers will not tell you which
> optimizations they use. You would have to threaten to irreversibly fork
> their hardware out by a rule change, only then would they start shouting
> and reveal their optimization. It seems extremely dangerous to set the
> precedence of a hardfork that irreversibly forks out a certain type of
> mining hardware.
> 
> The part "Also the fix should be compatible with existing mining
> hardware." is impossible to achieve because it's unclear what "existing
> mining hardware" is. There has never been a specification of what mining
> hardware should do. There are only acceptance rules.
> 
> The only way out is to go the exact opposite way and to embrace as many
> optimizations as possible to the point where there are no more
> optimizations left to do, or hopefully getting very close to that point. 
> 
> Timo
> 
> 
> 
> On Tue, May 10, 2016 at 11:57 AM, Peter Todd via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>     As part of the hard-fork proposed in the HK agreement(1) we'd like
>     to make the
>     patented AsicBoost optimisation useless, and hopefully make further
>     similar
>     optimizations useless as well.
> 
>     What's the best way to do this? Ideally this would be SPV
>     compatible, but if it
>     requires changes from SPV clients that's ok too. Also the fix this
>     should be
>     compatible with existing mining hardware.
> 
> 
>     1)
>     https://medium.com/@bitcoinroundtable/bitcoin-roundtable-consensus-266d475a61ff
> 
>     2)
>     http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-April/012596.html
> 
>     --
>     https://petertodd.org 'peter'[:-1]@petertodd.org <http://petertodd.org>
> 
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
> Signed by the key pair that was referenced in the output of the on-chain
> transaction?

Signed by the key pair referenced in the private output.

>  (Bob in my example, actually)

I misread your example.  If it was Bob, then the troll couldn't
generate the correct spend proof because he didn't see the private
output C.  The troll could try to replay the spend proof in the
Alice's transaction as soon as he sees it in the mempool, but then the
spend proof would be signed by the wrong user.

> Doesn't that mean it's easy to
> follow who is paying whom, you just can't see how much is going to reach
> recipient?

Only the recipients of the private outputs can see the previous owners
of the coins they receive (including amounts).  What everybody else
sees, is just meaningless hashes that hide both the recipient of the
coin and the amount.


2016-08-10 7:31 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
> Signed by the key pair that was referenced in the output of the on-chain
> transaction? (Bob in my example, actually) Doesn't that mean it's easy to
> follow who is paying whom, you just can't see how much is going to reach
> recipient?
>
> On Tue, Aug 9, 2016, 04:40 Tony Churyumoff <tony991@gmail.com> wrote:
>>
>> This troll is harmless.  A duplicate spend proof should also be signed
>> by the same user (Alice, in your example) to be considered a double
>> spend.
>>
>> 2016-08-09 3:18 GMT+03:00 James MacWhyte <macwhyte@gmail.com>:
>> > One more thought about why verification by miners may be needed.
>> >
>> > Let's say Alice sends Bob a transaction, generating output C.
>> >
>> > A troll, named Timothy, broadcasts a transaction with a random hash,
>> > referencing C's output as its spend proof. The miners can't tell if it's
>> > valid or not, and so they include the transaction in a block. Now Bob's
>> > money is useless, because everyone can see the spend proof referenced
>> > and
>> > thinks it has already been spent, even though the transaction that
>> > claims it
>> > isn't valid.
>> >
>> > Did I miss something that protects against this?
>> >


-------------------------------------
>> Block data that is stored can be used by other software, or potentially be
>> served to other nodes. The latter is not implemented at the moment - it would require a change to the P2P protocol, thus right now pruning nodes don't serve block data at all.

Why is the minimum storage quota of 550 MiB necessary for pruning nodes
if the block data is not served to other nodes ? Could the client just do transaction verification and transaction relaying and only keep the block(s) 
being verified on disk ?


On Jan 25, 2016, at 10:05 AM, Wladimir J. van der Laan via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:

>>> To enable block pruning set prune=<N> on the command line or in
>>> bitcoin.conf, where N is the number of MiB to allot for raw block & undo
>>> data.
> 
>> From having read the Bitcoin whitepaper quite a few months ago ago, I have the 
>> very very basic understanding that pruning is meant to:
>> - delete old transaction data which merely "moves coins around"
>> - instead only store the "origin" (= block where coins were mined) and 
>> "current location" of the coins, i.e. the unspent transactions. Notably, I 
>> understood it as "this is as secure as storing everything, since we know where 
>> the coins were created, and where they are".
>> 
>> So from that point of view, I would assume that there is a "natural" amount of 
>> megabytes which a fully pruned blockchain consists of: It would be defined by 
>> the final amount of unspent coins.
>> I thereby am confused why it is possible to configure a number of megabytes 
>> "to allot for raw block & undo data". I would rather expect pruning just to be 
>> a boolean on/off flag, and the number of megabytes to be an automatically 
>> computed result from the natural size of the dataset.
>> And especially, I fear that I could set N too low, and as a result, it would 
>> delete "too much". I mean could this result in even security relevant 
>> transaction data being deleted?
> 
> The term 'pruning', unfortunately is very much overused and overloaded in the
> bitcoin ecosystem. Satoshi's paper refers to UTXO pruning. This is Pieter Wuille's "ultraprune",
> which has been part of Bitcoin Core for more than three years.
> 
> Block pruning is not mentioned in that paper because it is just administrative,
> the only reason that nodes store historical blocks at all is to serve it to other nodes,
> as well as to catch up the wallet status and for -reindexes.
> 
>> Thus, it would be nice if you could yet once more edit the release notes to:
> 
> I don't have time to work on the release notes right now, but if someone else
> wants to contribute that'd be awesome.
> 
>> - explain why a N must be given
> 
> To give a quotum. The point is that the user can choose how much harddisk space they want to
> dedicate to block storage.
> 
> Block data that is stored can be used by other software, or potentially be
> served to other nodes. The latter is not implemented at the moment - it would require
> a change to the P2P protocol, thus right now pruning nodes don't serve block
> data at all.
> 
>> - what a "safe" value of N is. I.e. how large must N be at least to not delete 
>> security-relevant stuff?
> 
> There is no security compromise with pruning. Any value of N is intended to be safe.
> 
> Very low values would delete undo data that may be necessary in a reorganization,
> but this is prohibited by argument checks.
> 
> Release notes are not meant as a replacement or supplement for documentation.
> The documentation for -prune is this:
> 
>  -prune=<n>
>       Reduce storage requirements by pruning (deleting) old blocks. This mode
>       is incompatible with -txindex and -rescan. Warning: Reverting this
>       setting requires re-downloading the entire blockchain. (default: 0 =
>       disable pruning blocks, >550 = target size in MiB to use for block
>       files)
> 
>> - maybe mention if there is a "auto" setting for N to ensure that it choses a 
>> safe value on its own?
> 
> As said, there is no safe or unsafe value. The lowest acceptable value is just as safe
> as storing everything.
> 
> Wladimir
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
In light of Ethereum's recent problems with its imperative, account-based,
programming model, I thought I'd do a quick writeup outlining the building
blocks of the state-machine approach to so-called "smart contract" systems, an
extension of Bitcoin's own design that I personally have been developing for a
number of years now as my Proofchains/Dex research work.


# Deterministic Code / Deterministic Expressions

We need to be able to run code on different computers and get identical
results; without this consensus is impossible and we might as well just use a
central authoritative database. Traditional languages and surrounding
frameworks make determinism difficult to achieve, as they tend to be filled
with undefined and underspecified behavior, ranging from signed integer
overflow in C/C++ to non-deterministic behavior in databases. While some
successful systems like Bitcoin are based on such languages, their success is
attributable to heroic efforts by their developers.

Deterministic expression systems such as Bitcoin's scripting system and the
author's Dex project improve on this by allowing expressions to be precisely
specified by hash digest, and executed against an environment with
deterministic results. In the case of Bitcoin's script, the expression is a
Forth-like stack-based program; in Dex the expression takes the form of a
lambda calculus expression.


## Proofs

So far the most common use for deterministic expressions is to specify
conditions upon which funds can be spent, as seen in Bitcoin (particularly
P2SH, and the upcoming Segwit). But we can generalize their use to precisely
defining consensus protocols in terms of state machines, with each state
defined in terms of a deterministic expression that must return true for the
state to have been reached. The data that causes a given expression to return
true is then a "proof", and that proof can be passed from one party to another
to prove desired states in the system have been reached.

An important implication of this model is that we need deterministic, and
efficient, serialization of proof data.


## Pruning

Often the evaluation of an expression against a proof doesn't require all all
data in the proof. For example, to prove to a lite client that a given block
contains a transaction, we only need the merkle path from the transaction to
the block header. Systems like Proofchains and Dex generalize this process -
called "pruning" - with built-in support to both keep track of what data is
accessed by what operations, as well as support in their underlying
serialization schemes for unneeded data to be elided and replaced by the hash
digest of the pruned data.


# Transactions

A common type of state machine is the transaction. A transaction history is a
directed acyclic graph of transactions, with one or more genesis transactions
having no inputs (ancestors), and one or more outputs, and zero or more
non-genesis transactions with one or more inputs, and zero or more outputs. The
edges of the graph connect inputs to outputs, with every input connected to
exactly one output. Outputs with an associated input are known as spent
outputs; outputs with out an associated input are unspent.

Outputs have conditions attached to them (e.g. a pubkey for which a valid
signature must be produced), and may also be associated with other values such
as "# of coins". We consider a transaction valid if we have a set of proofs,
one per input, that satisfy the conditions associated with each output.
Secondly, validity may also require additional constraints to be true, such as
requiring the coins spent to be >= the coins created on the outputs. Input
proofs also must uniquely commit to the transaction itself to be secure - if
they don't the proofs can be reused in a replay attack.

A non-genesis transaction is valid if:

1. Any protocol-specific rules such as coins spent >= coins output are
   followed.

2. For every input a valid proof exists.

3. Every input transaction is itself valid.

A practical implementation of the above for value-transfer systems like Bitcoin
could use two merkle-sum trees, one for the inputs, and one for the outputs,
with inputs simply committing to the previous transaction's txid and output #
(outpoint), and outputs committing to a scriptPubKey and output amount.
Witnesses can be provided separately, and would sign a signature committing to
the transaction or optionally, a subset of of inputs and/or outputs (with
merkle trees we can easily avoid the exponential signature validation problems
bitcoin currently has).

As so long as all genesis transactions are unique, and our hash function is
secure, all transaction outputs can be uniquely identified (prior to BIP34 the
Bitcoin protocol actually failed at this!).


## Proof Distribution

How does Alice convince Bob that she has done a transaction that puts the
system into the state that Bob wanted? The obvious answer is she gives Bob data
proving that the system is now in the desired state; in a transactional system
that proof is some or all of the transaction history. Systems like Bitcoin
provide a generic flood-fill messaging layer where all participants have the
opportunity to get a copy of all proofs in the system, however we can also
implement more fine grained solutions based on peer-to-peer message passing -
one could imagine Alice proving to Bob that she transferred title to her house
to him by giving him a series of proofs, not unlike the same way that property
title transfer can be demonstrated by providing the buyer with a series of deed
documents (though note the double-spend problem!).


# Uniqueness and Single-Use Seals

In addition to knowing that a given transaction history is valid, we also want
to know if it's unique. By that we mean that every spent output in the
transaction history is associated with exactly one input, and no other valid
spends exist; we want to ensure no output has been double-spent.

Bitcoin (and pretty much every other cryptocurrency like it) achieves this goal
by defining a method of achieving consensus over the set of all (valid)
transactions, and then defining that consensus as valid if and only if no
output is spent more than once.

A more general approach is to introduce the idea of a cryptographic Single-Use
Seal, analogous to the tamper-evidence single-use seals commonly used for
protecting goods during shipment and storage. Each individual seals is
associated with a globally unique identifier, and has two states, open and
closed. A secure seal can be closed exactly once, producing a proof that the
seal was closed.

All practical single-use seals will be associated with some kind of condition,
such as a pubkey, or deterministic expression, that needs to be satisfied for
the seal to be closed. Secondly, the contents of the proof will be able to
commit to new data, such as the transaction spending the output associated with
the seal.

Additionally some implementations of single-use seals may be able to also
generate a proof that a seal was _not_ closed as of a certain
time/block-height/etc.


## Implementations

### Transactional Blockchains

A transaction output on a system like Bitcoin can be used as a single-use seal.
In this implementation, the outpoint (txid:vout #) is the seal's identifier,
the authorization mechanism is the scriptPubKey of the output, and the proof
is the transaction spending the output. The proof can commit to additional
data as needed in a variety of ways, such as an OP_RETURN output, or
unspendable output.

This implementation approach is resistant to miner censorship if the seal's
identifier isn't made public, and the protocol (optionally) allows for the
proof transaction to commit to the sealed contents with unspendable outputs;
unspendable outputs can't be distinguished from transactions that move funds.


### Unbounded Oracles

A trusted oracle P can maintain a set of closed seals, and produce signed
messages attesting to the fact that a seal was closed. Specifically, the seal
is identified by the tuple (P, q), with q being the per-seal authorization
expression that must be satisfied for the seal to be closed. The first time the
oracle is given a valid signature for the seal, it adds that signature and seal
ID to its closed seal set, and makes available a signed message attesting to
the fact that the seal has been closed. The proof is that message (and
possibly the signature, or a second message signed by it).

The oracle can publish the set of all closed seals for transparency/auditing
purposes. A good way to do this is to make a merkelized key:value set, with the
seal identifiers as keys, and the value being the proofs, and in turn create a
signed certificate transparency log of that set over time. Merkle-paths from
this log can also serve as the closed seal proof, and for that matter, as
proof of the fact that a seal has not been closed.


### Bounded Oracles

The above has the problem of unbounded storage requirements as the closed seal
set grows without bound. We can fix that problem by requiring users of the
oracle to allocate seals in advance, analogous to the UTXO set in Bitcoin.

To allocate a seal the user provides the oracle P with the authorization
expression q. The oracle then generates a nonce n and adds (q,n) to the set of
unclosed seals, and tells the user that nonce. The seal is then uniquely
identified by (P, q, n)

To close a seal, the user provides the oracle with a valid signature over (P,
q, n). If the open seal set contains that seal, the seal is removed from the
set and the oracle provides the user with a signed message attesting to the
valid close.

A practical implementation would be to have the oracle publish a transparency
log, with each entry in the log committing to the set of all open seals with a
merkle set, as well as any seals closed during that entry. Again, merkle paths
for this log can serve as proofs to the open or closed state of a seal.

Note how with (U)TXO commitments, Bitcoin itself is a bounded oracle
implementation that can produce compact proofs.


### Group Seals

Multiple seals can be combined into one, by having the open seal commit to a
set of sub-seals, and then closing the seal over a second set of closed seal
proofs. Seals that didn't need to be closed can be closed over a special
re-delegation message, re-delegating the seal to a new open seal.

Since the closed sub-seal proof can additionally include a proof of
authorization, we have a protcol where the entity with authorization to close
the master seal has the ability to DoS attack sub-seals owners, but not the
ability to fraudulently close the seals over contents of their choosing. This
may be useful in cases where actions on the master seal is expensive - such as
seals implemented on top of decentralized blockchains - by amortising the cost
over all sub-seals.


## Atomicity

Often protocols will require multiple seals to be closed for a transaction to
be valid. If a single entity controls all seals, this is no problem: the
transaction simply isn't valid until the last seal is closed.

However if multiple parties control the seals, a party could attack another
party by failing to go through with the transaction, after another party has
closed their seal, leaving the victim with an invalid transaction that they
can't reverse.

We have a few options to resolve this problem:

### Use a single oracle

The oracle can additionally guarantee that a seal will be closed iff some other
set of seals are also closed; seals implemented with Bitcoin can provide this
guarantee. If the parties to a transaction aren't already all on the same
oracle, they can add an additional transaction reassigning their outputs to a
common oracle.

Equally, a temporary consensus between multiple mutually trusting oracles can
be created with a consensus protocol they share; this option doesn't need to
change the proof verification implementation.


### Two-phase Timeouts

If a proof to the fact that a seal is open can be generated, even under
adversarial conditions, we can make the seal protocol allow a close to be
undone after a timeout if evidence can be provided that the other seal(s) were
not also closed (in the specified way).

Depending on the implementation - especially in decentralized systems - the
next time the seal is closed, the proof it has been closed may in turn provide
proof that a previous close was in fact invalid.


# Proof-of-Publication and Proof-of-Non-Publication

Often we need to be able to prove that a specified audience was able to receive
a specific message. For example, the author's PayPub protocol[^paypub],
Todd/Taaki's timelock encryption protocol[^timelock], Zero-Knowledge Contingent
Payments[^zkcp], and Lightning, among others work by requiring a secret key to
be published publicly in the Bitcoin blockchain as a condition of collecting a
payment. At a much smaller scale - in terms of audience - in certain FinTech
applications for regulated environments a transaction may be considered invalid
unless it was provably published to a regulatory agency.  Another example is
Certificate Transparency, where we consider a SSL certificate to be invalid
unless it has been provably published to a transparency log maintained by a
third-party.

Secondly, many proof-of-publication schemes also can prove that a message was
_not_ published to a specific audience. With this type of proof single-use
seals can be implemented, by having the proof consist of proof that a specified
message was not published between the time the seal was created, and the time
it was closed (a proof-of-publication of the message).

## Implementations

### Decentralized Blockchains

Here the audience is all participants in the system. However miner censorship
can be a problem, and compact proofs of non-publication aren't yet available
(requires (U)TXO commitments).

The authors treechains proposal is a particularly generic and scalable
implementation, with the ability to make trade offs between the size of
audience (security) and publication cost.

### Centralized Public Logs

Certificate Transparency works this way, with trusted (but auditable) logs run
by well known parties acting as the publication medium, who promise to allow
anyone to obtain copies of the logs.

The logs themselves may be indexed in a variety of ways; CT simply indexes logs
by time, however more efficient schemes are possible by having the operator
commit to a key:value mapping of "topics", to allow publication (and
non-publication) proofs to be created for specified topics or topic prefixes.

Auditing the logs is done by verifying that queries to the state of the log
return the same state at the same time for different requesters.

### Receipt Oracles

Finally publication can be proven by a receipt proof by the oracle, attesting
to the fact that the oracle has successfully received the message. This is
particularly appropriate in cases where the required audience is the oracle
itself, as in the FinTech regulator case.


# Validity Oracles

As transaction histories grow longer, they may become impractical to move from
one party to another. Validity oracles can solve this problem by attesting to
the validity of transactions, allowing history prior to the attested
transactions to be discarded.

A particularly generic validity oracle can be created using deterministic
expressions systems. The user gives the oracle an expression, and the oracle
returns a signed message attesting to the validity of the expression.
Optionally, the expression may be incomplete, with parts of the expression
replaced by previously generated attestations. For example, an expression that
returns true if a transaction is valid could in turn depend on the previous
transaction also being valid - a recursive call of itself - and that recursive
call can be proven with a prior attestation.

## Implementations

### Proof-of-Work Decentralized Consensus

Miners in decentralized consensus systems act as a type of validity oracle, in
that the economic incentives in the system are (supposed to be) designed to
encourage only the mining of valid blocks; a user who trusts the majority of
hashing power can trust that any transaction with a valid merkle path to a
block header in the most-work chain is valid. Existing decentralized consensus
systems like Bitcoin and Ethereum conflate the roles of validity oracle and
single-use seal/anti-replay oracle, however in principle that need not be true.


### Trusted Oracles

As the name suggests. Remote-attestation-capable trusted hardware is a
particularly powerful implementation - a conspiracy theory is that the reason
why essentially zero secure true remote attestation implementations exist is
because they'd immediately make untraceable digital currency systems easy to
implement (Finney's RPOW[^rpow] is a rare counter-example).

Note how a single-use seal oracle that supports a generic deterministic
expressions scheme for seal authorization can be easily extended to provide a
validity oracle service as well. The auditing mechanisms for a single-use seal
oracle can also be applied to validity oracles.


# Fraud Proofs

Protocols specified with deterministic expressions can easily generate "fraud
proofs", showing that claimed states/proof in the system are actually invalid.
Additionally many protocols can be specified with expressions of k*log2(n)
depth, allowing these fraud proofs to be compact.

A simple example is proving fraud in merkle-sum tree, where the validity
expression would be something like:

    (defun valid? (node)
        (or (== node.type leaf)
            (and (== node.sum (+ node.left.sum node.right.sum))
                 (and (valid? node.left)
                      (valid? node.right)))))

To prove the above expression evaluates to true, we'll need the entire contents
of the tree. However, to prove that it evaluates to false, we only need a
subset of the tree as proving an and expression evaluates to false only
requires one side, and requires log2(n) data. Secondly, with pruning, the
deterministic expressions evaluator can automatically keep track of exactly
what data was needed to prove that result, and prune all other data when
serializing the proof.


## Validity Challenges

However how do you guarantee it will be possible to prove fraud in the first
place? If pruning is allowed, you may simply not have access to the data
proving fraud - an especially severe problem in transactional systems where a
single fraudulent transaction can counterfeit arbitrary amounts of value out of
thin air.

A possible approach is the validity challenge: a subset of proof data, with
part of the data marked as "potentially fraudulent". The challenge can be
satisfied by providing the marked data and showing that the proof in question
is in fact valid; if the challenge is unmet participants in the system can
choose to take action, such as refusing to accept additional transactions.

Of course, this raises a whole host of so-far unsolved issues, such as DoS
attacks and lost data.


# Probabilistic Validation

Protocols that can tolerate some fraud can make use of probabilistic
verification techniques to prove that the percentage of undetected fraud within
the system is less than a certain amount, with a specified probability.

A common way to do this is the Fiat-Shamir transform, which repeatedly samples
a data structure deterministically, using the data's own hash digest as a seed
for a PRNG. Let's apply this technique to our merkle-sum tree example. We'll
first need a recursive function to check a sample, weighted by value:

    (defun prefix-valid? (node nonce)
        (or (== node.type leaf)
            (and (and (== node.sum (+ node.left.sum node.right.sum))
                      (> 0 node.sum)) ; mod by 0 is invalid, just like division by zero
                                      ; also could guarantee this with a type system
                 (and (if (< node.left.sum (mod nonce node.sum))
                          (prefix-valid? node.right (hash nonce))
                          (prefix-valid? node.left (hash nonce)))))))

Now we can combine multiple invocations of the above, in this case 256
invocations:

    (defun prob-valid? (node)
        (and (and (and .... (prefix-valid? node (digest (cons (digest node) 0)))
             (and (and ....
                            (prefix-valid? node (digest (cons (digest node) 255)))

As an exercise for a reader: generalize the above with a macro, or a suitable
types/generics system.

If we assume our attacker can grind up to 128 bits, that leaves us with 128
random samples that they can't control. If the (value weighted) probability of
a given node is fraudulent q, then the chance of the attacker getting away with
fraud is (1-q)^128 - for q=5% that works out to 0.1%

(Note that the above analysis isn't particularly well done - do a better
analysis before implementing this in production!)


## Random Beacons and Transaction History Linearization

The Fiat-Shamir transform requires a significant number of samples to defeat
grinding attacks; if we have a random beacon available we can significantly
reduce the size of our probabilistic proofs. PoW blockchains can themselves act
as random beacons, as it is provably expensive for miners to manipulate the
hash digests of blocks they produce - to do so requires discarding otherwise
valid blocks.

An example where this capability is essential is the author's transaction
history linearization technique. In value transfer systems such as Bitcoin, the
history of any given coin grows quasi-exponentially as coins are mixed across
the entire economy. We can linearize the growth of history proofs by redefining
coin validity to be probabilistic.

Suppose we have a transaction with n inputs. Of those inputs, the total value
of real inputs is p, and the total claimed value of fake inputs is q. The
transaction commits to all inputs in a merkle sum tree, and we define the
transaction as valid if a randomly chosen input - weighted by value - can
itself be proven valid. Finally, we assume that creating a genuine input is a
irrevocable action which irrevocable commits to the set of all inputs, real and
fake.

If all inputs are real, 100% of the time the transaction will be valid; if all
inputs are fake, 100% of the time the transaction will be invalid. In the case
where some inputs are real and some are fake the probability that the fraud
will be detected is:

    q / (q + p)

The expected value of the fake inputs is then the sum of the potential upside -
the fraud goes detected - and the potential downside - the fraud is detected
and the real inputs are destroyed:

    E = q(1 - q/(q + p)) - p(q/(q + p)
      = q(p/(q + p)) - p(q/(q + p)
      = (q - q)(p/(q + p))
      = 0

Thus so long as the random beacon is truly unpredictable, there's no economic
advantage to creating fake inputs, and it is sufficient for validity to only
require one input to be proven, giving us O(n) scaling for transaction history
proofs.


### Inflationary O(1) History Proofs

We can further improve our transaction history proof scalability by taking
advantage of inflation. We do this by occasionally allowing a transaction proof
to be considered valid without validating _any_ of the inputs; every time a
transaction is allowed without proving any inputs the size of the transaction
history proof is reset. Of course, this can be a source of inflation, but
provided the probability of this happening can be limited we can limit the
maximum rate of inflation to the chosen value.

For example, in Bitcoin as of writing every block inflates the currency supply
by 25BTC, and contains a maximum of 1MB of transaction data, 0.025BTC/KB. If we
check the prior input proof with probability p, then the expected value of a
transaction claiming to spend x BTC is:

    E = x(1-p)

We can rewrite that in terms of the block reward per-byte R, and the transaction size l:

    lR = x(1-p)

And solving for p:

    p = 1 - lR/x

For example, for a 1KB transaction proof claiming to spending 10BTC we can omit
checking the input 0.25% of the time without allowing more monetary inflation
than the block reward already does. Secondly, this means that after n
transactions, the probability that proof shortening will _not_ happen is p^n,
which reaches 1% after 1840 transactions.

In a system like Bitcoin where miners are expected to validate, a transaction
proof could consist of just a single merkle path showing that a single-use seal
was closed in some kind of TXO commitment - probably under 10KB of data. That
gives us a history proof less than 18.4MB in size, 99% of the time, and less
than 9.2MB in size 90% of the time.

An interesting outcome of thing kind of design is that we can institutionalize
inflation fraud: the entire block reward can be replaced by miners rolling the
dice, attempting to create valid "fake" transactions. However, such a pure
implementation would put a floor on the lowest transaction fee possible, so
better to allow both transaction fee and subsidy collection at the same time.


# References

[^paypub] https://github.com/unsystem/paypub
[^timelock] https://github.com/petertodd/timelock
[^zkcp] https://bitcoincore.org/en/2016/02/26/zero-knowledge-contingent-payments-announcement/
[^rpow] https://cryptome.org/rpow.htm

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Saturday, February 06, 2016 5:25:21 PM Tom Zander via bitcoin-dev wrote:
> On Saturday, February 06, 2016 06:09:21 PM Jorge Timn via bitcoin-dev 
wrote:
> > None of the reasons you list say anything about the fact that "being
> > lost" (kicked out of the network) is a problem for those node's users.
> 
> That's because its not.
> 
> If you have a node that is "old" your node will stop getting new blocks.
> The node will essentially just say "x-hours behind" with "x" getting larger
> every hour. Funds don't get confirmed. etc.

Until someone decides to attack you. Then you'll get 6, 10, maybe more blocks 
confirming a large 10000 BTC payment. If you're just a normal end user (or 
perhaps an automated system), you'll figure that payment is good and 
irreversibly hand over the title to the house.

Luke


-------------------------------------
Hi Akiva,

   I have also given a little thought to partitioning, in a totally
different way a Merkel Tree Forrest. Generally the idea here would have be
to create new Merkel Trees every so often as currency supply was added. It
would partition the mining process and therefore improve the distribution
of the verification.

It would work as follows, and NO I haven't really thought this through it's
just an idea!


Imagine it was 2009 and there was a small number of 250 BTC in 'Batch 1',
once the number of BTC needed to go above 250 BTC two new Batches would be
created each one with it's own Merkel Tree until 750 BTC and so on.
Eventually there would be a large number of trees, allowing small scale
pool miners to dominate a single or small number of the trees and their
block chains.

This would also create a potential partial payment problem, where you send
3 BTC but only receive 2 BTC since 1 BTC ends up on a bad block and needs
to be resent.


Since most of the BTC currency supply is already available it's a bit late
for BitCoin, but could be used for new crypto currencies.


Any thoughts on this idea?


Cheers,

Scott

-------------------------------------
On Monday, February 08, 2016 10:41:00 PM Peter Todd wrote:
> On Mon, Feb 08, 2016 at 10:17:55PM +0000, Luke Dashjr via bitcoin-dev wrote:
> > Additionally, https://github.com/bitcoin/bips/pull/315 proposes to
> > upgrade five additional from Draft to Final status, and preferably needs
> > ACKs from the champions of the BIPs:
> > 
> > BIP 50: March 2013 Chain Fork Post-Mortem, by Gavin Andresen
> 
> It may be good to update BIP 50 with the new information that calling it
> a "hard fork" misses subtleties about what happened during that fork. In
> particular, 0.7 rejection of the chain was non-deterministic, based on
> having seen a re-org in a specific way.

I agree BIP 50 could use some rephrasing, but the May 2013 change was 
definitely a hardfork, despite the problems with the pre-March protocol.

Luke


-------------------------------------
Maybe something trivial like lack of Python 3 dependency on older CentOS
builds?

On Mon, Dec 19, 2016 at 2:22 AM, Matt Corallo via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Please do report bugs to https://github.com/bitcoin/bitcoin . If you
> never report them of course they won't get fixed. I'm not aware of test
> suite failures and know a bunch of folks who use CentOS, though not sure
> how many develop on it.
>
> On December 18, 2016 12:07:36 PM PST, Alice Wonder via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> >On 12/14/2016 07:38 PM, Juan Garavaglia via bitcoin-dev wrote:
> >
> >>
> >> For reasons I am unable to determine a significant number of node
> >> operators do not upgrade their clients.
> >
> >I almost did not update to 0.13.0 because the test suite was failing
> >due
> >to python errors. How to fix them was posted on bitcointalk.
> >
> >0.13.1 came with new python errors in the test suite. So I just said
> >fuck it.
> >
> >When the test suite actually works in my fairly standard environment
> >(CentOS) in the distributed release, I will upgrade.
> >
> >Until then, I'm not jumping through hoops to make the test suite work
> >and I'm not running clients that haven't passed the test suite so
> >that's
> >why I almost didn't update to 0.13.0 and haven't updated since.
> >_______________________________________________
> >bitcoin-dev mailing list
> >bitcoin-dev@lists.linuxfoundation.org
> >https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Summary: This describes a new transaction format which allows most
transactions to take up less space (and thus fit more per block) and a
method to implement it requiring only a (non generalized) softfork.

= Compressed transactions =
This format is designed to allow the majority of transactions to take up
less space, by removing flexibility and unnecessary data. The requirements
to use a compressed transaction are

* Non-coinbase
* 1-8 inputs and 1-8 outputs
* Pay to pubkey hash only

Transactions which want to use arbitrary scripts or a larger number of
inputs and outputs can still use the existing transaction format.

A compressed transaction consists of
header byte, compressed inputs, compressed outputs, optional lock_time

header byte has the following format
* bit 7: Always 1, to make it easy to distinguish compressed and
uncompressed transactions
* bit 6: 1 if lock_time is used, otherwise 0
* bit 5-3: Number of inputs - 1
* bit 2-0: Number of outputs - 1

This saves 5+ bytes from omitting the version number and the input and
output count fields. Additionally, most transactions will not have
lock_time, saving another 4 bytes.

Compressed input:
previous transaction hash, index byte, signature, pubkey, optional
sequence_no

This has the following differences from a normal input: Index is only 1
byte, since it is at most 8 anyway. The top bit of the index byte indicates
whether the input has a sequence number. ScriptSig length is completely
omitted, and signature and public key are included directly, saving space
from the data push and check opcodes. And as before, sequence_no is
optional and usually omitted.

Compressed output:
compressed value (1-7 bytes), pubkeyhash

compressed value format: The high 3 bits of the first byte give the number
of following bytes. The lower 5 bits and the n following bytes comprise the
output value. The maximum possible value is 2099999997690000 satoshis,
which requires 7 bytes to encode, but most values will be far shorter. For
example, a value of 0.01 BTC could be encoded in just 3 bytes, saving 5.

As before the script length field is completely omitted, and the pubkeyhash
is included directly, without extra opcodes.


= Consensus =

Like all softforks, adoption by a minority of miners would cause problems.
Therefore, these changes would only take effect after a consensus. Miners
can advertise support for the new format by increment the version code.
Once X% of Y consecutive blocks have this version, the new changes take
effect. Users who do not upgrade will still work but will not always see
accurate balances in other addresses and miners who do not upgrade risk
mining an invalid block, encouraging them to upgrade.

= The Shadow Chain =

Now for the interesting part: Implementing the new format with only a
softfork. In order to qualify as a softfork, every valid block under the
new rules also has to be valid under the old rules.

Among other things this means that compressed transactions can't just be
included in place of an ordinary transaction in a block, since the legacy
(non-upgraded) clients will consider that invalid. Instead, they will be
hidden as extra data inside the coinbase transaction, which is allowed to
contain arbitrary data.

Additionally, in order to support interoperability between compressed and
uncompressed transactions, uncompressed transactions can hide compressed
inputs and ouputs inside of the normal inputs and outputs using a currently
unused opcode (OP_NOP1, hereafter referred to as OP_SHADOW). OP_SHADOW
isn't a script operation per se; instead it marked scripts that should be
interpreted differently under the new rules.


In the following, shadow input/output refers to a compressed input/output,
which is hidden as metadata and hence not visible to legacy clients.

The blockchain must also still be valid when all the hidden data is
ignored. When moving money from the visible to the shadow chain, there is
no problem, but when moving money back, things get trickier, since the
legacy client won't know about any of the shadow transactions. Therefore,
when sending money to the shadow chain, the transaction includes a
specially marked anyone-can-spend output. When moving money back from the
shadow chain, the transaction "spends" any available such outputs.

Since an arbitrary amount of splitting and combining can occur inside the
shadow chain, these will not be 1:1. Instead a pool of available ouputs is
maintained with a total balance equal to the total balance inside the
shadow chain. The validation rules of upgraded clients ensure that this is
always maintained. A legacy client may try to spend these outputs, but it
would fail validation under the new rules and quickly become orphaned.

= Sending money from the visible to the shadow chain =
An uncompressed transaction is created with a specially formatted output.

OP_SHADOW OP_PUSHDATA1 <shadow output>

Where <shadow output> is a compressed output using the format described in
the previous section.

A legacy client will interpret this as an anyone-can-spend output. An
upgraded client will see the OP_SHADOW and interpret this specially, rather
than as a normal script. Instead it will interpret the data as a compressed
output, and add it as a shadow UTXO, which can be spent by compressed
transactions. Additionally, it will note that the visible output can be
used later when withdrawing from the shadow chain.

= Sending money from the shadow chain to the visible chain =
An uncompressed transaction is created with a specially formatted input.

OP_SHADOW OP_PUSHDATA1 <shadow input>

Where <shadow input> is a compressed input using the format described in
the previous section.

The legacy client will interpret this as spending one of the
anyone-can-spend outputs from earlier. The upgraded client will see the
leading OP_SHADOW and recognize that it should be interpreted specially. It
will perform all the normal verification that <shadow input> is a valid
input and not already spent in the shadow chain, etc. Thus the blockchain
is seen as valid by both legacy and upgraded clients.

Note: These scripts are currently considered nonstandard and will not be
relayed by legacy clients. As part of implementing the new protocol,
upgraded clients will obviously be modified to relay these transactions.
Since the consensus step earlier ensures that these are a majority of the
network before the changes take effect, this shouldn't be much of a problem.

= Combining and splitting inputs =

The above illustrates the simplest case. In practice, it will often by the
case that the available pool of OP_SHADOW marked anyone-can-spend UTXOs
doesn't match up exactly with the amount being withdrawn.

If the amounts available are too small, the uncompressed transaction can
include multiple inputs. The first one will contain the shadow input data
as above, and the subsequent inputs will just say

OP_SHADOW OP_TRUE

Likewise, the left over change will be included as an extra output with the
script
OP_SHADOW

Each uncompressed transaction can include up to 8 shadow inputs and up to 8
shadow outputs. The validation rules require that the total amount of
marked anyone-can-spend outputs being spent and created matches up with the
total balance leaving and entering the shadow chain.

What if you want to create an actual anyone-can-spend output under the new
rules? Just include an empty script as before. Only scripts that begin with
OP_SHADOW take part in the shadow deposit/withdrawal process.

I hope I explained my idea well enough. It's fairly complex, but I think it
works. Unlike the "generalized softfork" proposals, this is a true
softfork, as the new blockchain is still valid under the old rules, just
interpreted a bit differently.

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.12.0rc5 are available from:

    https://bitcoin.org/bin/bitcoin-core-0.12.0/test/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.12.0rc5

This is a release candidate for a new major version release, bringing new
features, bug fixes, as well as other improvements.

Preliminary release notes for the release can be found here:

    https://github.com/bitcoin/bitcoin/blob/0.12/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.12.0.

Diff since rc3 (rc4 was DOA):

- - #7472 `b2f2b85` rpc: Add WWW-Authenticate header to 401 response (Wladimir J. van der Laan)
- - #7469 `9cb31e6` net.h fix spelling: misbeha{b,v}ing (Matt)
- - #7482 `e16f5b4` Ensure headers count is correct (Suhas Daftuar)
- - #7500 `889e5b3` Correctly report high-S violations (Pieter Wuille)
- - #7491 `00ec73e` wallet: Ignore MarkConflict if block hash is not known (Wladimir J. van der Laan)
- - #7502 `1329963` Update the wallet best block marker before pruning (Pieter Wuille)
- - #7468 `947c4ff` [rpc-tests] Change solve() to use rehash (Brad Andrews)

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWwbHqAAoJEHSBCwEjRsmmVJwH/3gvb5LAAL88R7ZbcKAzehdc
BnAmCTWX+mJENWq9MX3OWmddetbZSBU0x9MzV6atQHMTmcxmMkCIzZrysoSq3uDg
1IylViVPSr+36PPv2k1/chTun0yRWUGwLEz09JZscFILa0oJODvDISiOp0NEkDup
bewkpkrpzxroAqlTFNuSUl9KDCQPXUGvqCDH7RwHC3D8L8apVIT6bE8FHW8je278
Qjf3Z5AehXVzOyrhg02tT0Ow3EueKtNDASmopX+aM70ErzUbxe8/mYP3GAsQwbMi
WVdx7dvUdQQkNDIWGLH/V0AJlkbxDfBmAI0Ti2J9LxtbCOZdGAzId2aPpEOrfnU=
=UUfq
-----END PGP SIGNATURE-----


-------------------------------------
On Sat, Jul 30, 2016 at 6:18 PM, Paul Sztorc via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> I've also heard that segwit will help, but don't understand why.
>

There are some helpful discussions that happened over here:
https://botbot.me/freenode/bitcoin-core-dev/2015-12-28/?msg=56907496&page=2

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
On Tue, Aug 16, 2016 at 7:14 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> The other serious problem - and this is a problem with smartcards in
> general
> anyway - is that without Bitcoin-specific logic you're just signing
> blindly; we
> recently saw the problems with that with the Bitfinex/BitGo hack. And even
> then, without a screen most of the hardware wallets in are still just
> signing
> blindly, with at best hard-to-use limits on maximum funds moved
> per-transaction. Also note how even hardware wallets with a screen, like
> Trezor, aren't yet able to authenticate who you are paying.
>

"Welcome to my threat model."

In multisig scenarios, there must be a different "trust root" for each key.
For example, storing two private keys next to each other on the same web
server is broken because if one key is compromised it is infinitely trivial
to compromise the second key. Using multiple web servers is also broken if
the two servers are controlled by the same AWS keys or same "help me get my
servers back" support email request to whatever single sign-on service is
used. In some cases, it can be better to write software such that
transaction data is served at a particular location, and another
security-critical step is responsible for downloading that data from the
first machine, rather than the first computer directly pushing (with
authentication credentials in place for the attacker to compromise) the
data to the second computer.

I recommend using hardware security modules (HSMs). It's important to have
a public, reviewed bitcoin standard for hardware wallets, especially HSMs.
I expect this is something that the entire industry has a tremendous
interest in following and contributing to, which could even lead to
additional resources contributed (or at the very least, more detailed
requirements) towards libconsensus work.

Instead of signing any bitcoin transaction that the hardware wallet is
given, the hardware should be responsible for running bitcoin validation
rules and business logic, which I recommend for everyone, not only
businesses. Without running business logic and bitcoin validation rules,
the actual bitcoin history on the blockchain could be a very different
reality from what the hardware thinks is happening. Using a different
out-of-band communication channel, the hardware could query for information
from another database in another trust root, which would be useful for
business logic to validate against.

As for a screen, I consider that somewhat limited because you only get text
output (and I don't know if I can reasonably suggest QR codes here). With a
screen, you are limited to text output, which can compromise privacy of the
device's operations and info about the wallet owner. An alternative would
be to have a dedicated port that is responsibly only for sending out data
encrypted to the key of the wallet owner, to report information such as
whatever the hardware's transaction planner has decided, or to report about
the state of the device, state of the bitcoin validation rules, or any
accounting details, etc. Additionally, even a signed transaction should be
encrypted to the key of the device owner because a signed transaction can
be harmless as long as the owner still has the ability to control whether
the signed transaction is broadcasted to the network. It's "separation of
concerns" for transaction signing and decrypting a signed transaction
should be unrelated and uncoupled.

Also I am eager to see what the community proposes regarding signed and
authenticated payment requests.

((insert here general promotional statement regarding the value of reusable
checklists used during every signing ritual ceremony))

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
Not a covenant but interesting nevertheless: _One_ of OP_CAT and
OP_CHECKSIGFROMSTACKVERIFY alone is enough to implement "opt-in miner
takes double-spend" [1]:

You can create an output, which is spendable by everybody if you ever
double-spend the output with two different transactions. Then the next
miner will probably take your money (double-spending against your two
or more contradicting transactions again).

If you spend such an output, then the recipient may be willing to
accept a zero-conf transaction, because he knows that you'll lose the
money when you attempt double-spending (unless you are the lucky
miner). See the discussion in [1] for details. 

The implementation using OP_CHECKSIGFROMSTACKVERIFY is straight-
forward. You add a case to the script which allows spending if two
valid signatures on different message under the public key of the
output are given.

What is less known I think:
The same functionality can be achieved in a simpler way just using
OP_CAT, because it's possible to turn Bitcoin's ECDSA to an "opt-in
one-time signature scheme". With OP_CAT, you can create an output that
is only spendable using a signature (r,s) with a specific already fixed
first part r=x_coord(kG). Basically, the creator of this output commits
on r (and k) already when creating the output. Now, signing two
different transaction with the same r allows everybody to extract the
secret key from the two signatures.

The drawbacks of the implementation with OP_CAT is that it's not
possible to make a distinction between legitimate or illegitimate
double-spends (yet to be defined) but just every double-spend is
penalized. Also, it's somewhat hackish and the signer must store k (or
create it deterministically but that's a good idea anyway).

[1] https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07122.html

Best,
Tim

On Thu, 2016-11-03 at 07:37 +0000, Daniel Robinson via bitcoin-dev
wrote:
> Really cool!
> 
> How about "poison transactions," the other covenants use case
> proposed by Möser, Eyal, and Sirer? (I think
> OP_CHECKSIGFROMSTACKVERIFY will also make it easier to check fraud
> proofs, the other prerequisite for poison transactions.)
> 
> Seems a little wasteful to do those two "unnecessary" signature
> checks, and to have to construct the entire transaction data
> structure, just to verify a single output in the transaction. Any
> plans to add more flexible introspection opcodes to Elements, such as
> OP_CHECKOUTPUTVERIFY?
> 
> Really minor nit: "Notice that we have appended 0x83 to the end of
> the transaction data"—should this say "to the end of the signature"?
> 
> On Thu, Nov 3, 2016 at 12:28 AM Russell O'Connor via bitcoin-dev <bit
> coin-dev@lists.linuxfoundation.org> wrote:
> > Right.  There are minor trade-offs to be made with regards to that
> > design point of OP_CHECKSIGFROMSTACKVERIFY.  Fortunately this
> > covenant construction isn't sensitive to that choice and can be
> > made to work with either implementation of
> > OP_CHECKSIGFROMSTACKVERIFY.
> > 
> > On Wed, Nov 2, 2016 at 11:35 PM, Johnson Lau <jl2012@xbt.hk> wrote:
> > > Interesting. I have implemented OP_CHECKSIGFROMSTACKVERIFY in a
> > > different way from the Elements. Instead of hashing the data on
> > > stack, I directly put the 32 byte hash to the stack. This should
> > > be more flexible as not every system are using double-SHA256
> > > 
> > > https://github.com/jl2012/bitcoin/commits/mast_v3_master
> > > 
> > > 
> > > 
> > > > On 3 Nov 2016, at 01:30, Russell O'Connor via bitcoin-dev <bitc
> > > > oin-dev@lists.linuxfoundation.org> wrote:
> > > > 
> > > > Hi all,
> > > > 
> > > > It is possible to implement covenants using two script
> > > > extensions: OP_CAT and OP_CHECKSIGFROMSTACKVERIFY.  Both of
> > > > these op codes are already available in the Elements Alpha
> > > > sidechain, so it is possible to construct covenants in Elements
> > > > Alpha today.  I have detailed how the construction works in a
> > > > blog post at <https://blockstream.com/2016/11/02/covenants-in-e
> > > > lements-alpha.html>.  As an example, I've constructed scripts
> > > > for the Moeser-Eyal-Sirer vault.
> > > > 
> > > > I'm interested in collecting and implementing other useful
> > > > covenants, so if people have ideas, please post them.
> > > > 
> > > > If there are any questions, I'd be happy to answer.  
> > > > 
> > > > -- 
> > > > Russell O'Connor
> > > > _______________________________________________
> > > > bitcoin-dev mailing list
> > > > bitcoin-dev@lists.linuxfoundation.org
> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > 
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
Another use for the audio would be for watches that can listen but can't
use a camera (ie: Samsung S2), so sound would be great.

On Wed, Aug 10, 2016 at 7:42 AM, Erik Aronesty via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> NOTE:
>
> Addresses aren't really meant to be broadcast - you should probably be
> encoding BIP32 public seeds, not addresses.
>
> OR simply:
>
> - Send btc to rick@q32.com
> - TXT record _btc.rick.q32.com is queried (_<coin-code>.<name>.<domain>)
> - DNS-SEC validation is *required*
> - TXT record contains addr:[<bip32-pub-seed>]
>
> Then you can just say, in the podcast, "Send your bitcoin donations to
> rick@q32.com".   And you can link it to your email address, if your
> provider lets you set up a TXT record.   (By structuring the TXT record
> that way, many existing email providers will support the standard without
> having to change anything.)
>
> This works with audio, video, web and other publishing formats... and very
> little infrastructure change is needed.
>
>
> On Wed, Aug 10, 2016 at 6:41 AM, Tier Nolan via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Have you considered CDMA?  This has the nice property that it just sounds
>> like noise.  The codes would take longer to send, but you could send
>> multiple bits at once and have the codes orthogonal.
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Restriction for segwit OP_IF argument as a policy has got a few concept ACK. I would like to have more people to ACK or NACK, especially the real users of OP_IF. I think Lightning network would use that at lot.

Pull request: https://github.com/bitcoin/bitcoin/pull/8526

more related discussion could be found at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-August/013036.html

It does have impact if your script uses the combination of "OP_SIZE OP_IF" or "OP_DEPTH OP_IF". With this policy/softfork, you need to use  "OP_SIZE OP_0NOTEQUAL OP_IF" or "OP_DEPTH OP_0NOTEQUAL OP_IF", or reconstruct your scripts.

> 
>     On August 16, 2016 at 1:53 PM Johnson Lau via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
>     -----BEGIN PGP SIGNED MESSAGE-----
>     Hash: SHA512
> 
>     A new BIP is prepared to deal with OP_IF and OP_NOTIF malleability in P2WSH:
>     https://github.com/jl2012/bips/blob/minimalif/bip-minimalif.mediawiki
>     https://github.com/bitcoin/bitcoin/pull/8526
> 
>     BIP: x
>     Title: Dealing with OP_IF and OP_NOTIF malleability in P2WSH
>     Author: Johnson Lau <jl2012@xbt.hk>
>     Status: Draft
>     Type: Standards Track
>     Created: 2016-08-17
> 
>     Abstract
> 
>     This document specifies proposed changes to the Bitcoin script validity rules in order to make transaction malleability related to OP_IF and OP_NOTIF impossible in pay-to-witness-script-hash (P2WSH) scripts.
> 
>     Motivation
> 
>     OP_IF and OP_NOTIF are flow control codes in the Bitcoin script system. The programme flow is decided by whether the top stake value is True or False. However, this behaviour opens a source of malleability as a third party may replace a True (False) stack item with any other True (False) value without invalidating the transaction.
> 
>     The proposed rules apply only to pay-to-witness-script-hash (P2WSH) scripts described in BIP141, which has not been activated on the Bitcoin mainnet as of writing. To ensure OP_IF and OP_NOTIF transactions created before the introduction of this BIP will still be accepted by the network, the new rules are not applied to non-segregated witness scripts.
> 
>     Specification
> 
>     In P2WSH, the argument for OP_IF and OP_NOTIF MUST be exactly an empty vector or 0x01, or the script evaluation fails immediately.
> 
>     This is deployed using BIP9 after segregated witness (BIP141) is activated. Details TBD.
> 
>     Compatibility
> 
>     This is a softfork on top of BIP141. The rules are enforced as a relay policy by the reference client since the first release of BIP141 (v0.13.1). To avoid risks of fund loss, users MUST NOT create P2WSH scripts that are incompatible with this BIP. An OP_0NOTEQUAL may be used before OP_IF or OP_NOTIF to imitate the original behaviour (which may also re-enable the malleability vector depending on the exact script).
> 
>     Implementation
> 
>     https://github.com/bitcoin/bitcoin/pull/8526
> 
>     Copyright
> 
>     This work is placed in the public domain.
>     -----BEGIN PGP SIGNATURE-----
>     Comment: GPGTools - https://gpgtools.org
> 
>     iQGcBAEBCgAGBQJXs1LgAAoJEO6eVSA0viTSrJQL/A/womJKgi4FuyBTL9oykCss
>     aBMNN9+SLtmuH7SBgEUGZ8TFxa2st+6RP6Imu+Vvn4O5sXQl3DIXV+X38X93sUYk
>     wrjdpvdpqFFYJezPDESz6pR/6bZ1ES0aO2QqX578/8sqr8GO6L388s66vJeIGj4n
>     0LWW8sdEypMuV3HUG/9FFdUNHgiVX1U0sS1rT3P4aN30JYtb7PQpd7r8KTMta7Rt
>     L1VOZB+W3m2m2YZ9gB7IRmMfzzNm2QXRTPIZXt2x3mYDBuMkp+zEd5+ogA4sBpgP
>     wp2+l/aos686v0w8QYiNUX2+9Qpe7+238qUpw75d2XJYmLzdotWFvmp4g1hP+awX
>     HEfwe4BUM+El17LjrHkNeMWNJXMlhTtXb2i0XMj8tU5lZVHep4WpQ+LEahrNlsUl
>     FdFsi3q8HeWh8JsGaNCL41Bgbg/rKb5hUXyF6hTRHa//E6llOrpXRnsloKgBLv8c
>     QezgKTAPwwgdjcS6Ek0AqgLp7bCFRijCduYH9i9uaQ==
>     =lLIZ
>     -----END PGP SIGNATURE-----
> 
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev@lists.linuxfoundation.org
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 

-------------------------------------
On Thu, Aug 25, 2016 at 01:37:34AM +1000, Matthew Roberts wrote:
> Really nice idea. So its like a smart contract that incentivizes
> publication that a server has been hacked? I also really like how the
> funding has been handled -- with all the coins stored in the same address
> and then each server associated with a unique signature. That way, you
> don't have to split up all the coins among every server and reduce the
> incentive for an attacker yet you can still identify which server was
> hacked.
> 
> It would be nice if after the attacker broke into the server that they were
> also incentivized to act on the information as soon as possible (revealing
> early on when the server was compromised.) I suppose you could split up the
> coins into different outputs that could optimally be redeemed by the owner
> at different points in the future -- so they're incentivzed to act lest

Remember that it's _always_ possible for the owner to redeem the coins at any
time, and there's no way to prevent that.

The incentive for the intruder to collect the honeypot in a timely manner is
simple: once they've broken in, the moment the honeypot owner learns about the
compromise they have every reason to attempt to recover the funds, so the
intruder needs to act as fast as possible to maximize their chances of being
rewarded.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Actually this does nothing to provide justification for this consensus rule change. It is just an attempt to deflect criticism from the fact that it is such a change.

e

> On Nov 15, 2016, at 9:45 AM, Btc Drak <btcdrak@gmail.com> wrote:
> 
> I think this is already covered in the BIP text:-
> 
> "As of November 2016, the most recent of these changes (BIP 65,
> enforced since December 2015) has nearly 50,000 blocks built on top of
> it. The occurrence of such a reorg that would cause the activating
> block to be disconnected would raise fundamental concerns about the
> security assumptions of Bitcoin, a far bigger issue than any
> non-backwards compatible change.
> 
> So while this proposal could theoretically result in a consensus
> split, it is extremely unlikely, and in particular any such
> circumstances would be sufficiently damaging to the Bitcoin network to
> dwarf any concerns about the effects of this proposed change."
> 
> 
> On Mon, Nov 14, 2016 at 6:47 PM, Eric Voskuil via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> NACK
>> 
>> Horrible precedent (hardcoding rule changes based on the assumption that
>> large forks indicate a catastrophic failure), extremely poor process
>> (already shipped, now the discussion), and not even a material performance
>> optimization (the checks are avoidable once activated until a sufficiently
>> deep reorg deactivates them).
>> 
>> e
>> 
>> On Nov 14, 2016, at 10:17 AM, Suhas Daftuar via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> 
>> Hi,
>> 
>> Recently Bitcoin Core merged a simplification to the consensus rules
>> surrounding deployment of BIPs 34, 66, and 65
>> (https://github.com/bitcoin/bitcoin/pull/8391), and though the change is a
>> minor one, I thought it was worth documenting the rationale in a BIP for
>> posterity.
>> 
>> Here's the abstract:
>> 
>> Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated via miner
>> signaling in block version numbers. Now that the chain has long since passed
>> the blocks at which those consensus rules have triggered, we can (as a
>> simplification and optimization) replace the trigger mechanism by caching
>> the block heights at which those consensus rules became enforced.
>> 
>> The full draft can be found here:
>> 
>> https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki
>> 
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> 
>> 
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> 


-------------------------------------
On Tuesday, January 26, 2016 2:54:16 AM Toby Padilla wrote:
> Luke - As stated in the Github thread, I totally understand where you're
> coming from but the fact is people *will* encode data on the blockchain
> using worse methods. For all of the reasons that OP_RETURN was a good idea
> in the first place, it's a good idea to support it in PaymentRequests.

As I explained, none of those reasons apply to PaymentRequests.

> As for keyless - there's no way (that I know of) to construct a transaction
> with a zero value OP_RETURN in an environment without keys since the
> Payment Protocol is what defines the method for getting a transaction from
> a server to a wallet. You can make a custom transaction and execute it in
> the same application but without Payments there's no way to move
> transactions between two applications. You need to build the transaction
> where you execute it and thus need a key.

I have no idea what you are trying to say here.

Luke


-------------------------------------
Hey all,

Interestingly enough, the original BIP75 idea started by trying to move the Payment Protocol to use JSON, but because of all of the reasons mentioned by Andreas, we ended up with protobuf. There is quite a bit of language support on both desktop and mobile platforms so that's become mostly a non-issue.

Regarding the lack of optional client-supplied identification, BIP75 was designed to solve this issue. It allows both parties in a transaction to share identity information in an out-of-band fashion in order to keep specific identity information off-chain.

With regards to extensibility of PKI usage, both BIP70 and BIP75 provide plenty of flexibility. Both the InvoiceRequest and PaymentRequest contain the pki_type and pki_data fields to allow for the use of non X.509 certificates. Currently, the only pki_types specified in both BIPs are none or x509_sha256, but there isn't any specific limit on what can be used as long as you can define a PKI type to be used, include a public key and a signature that proves control of the keypair. Perhaps a new BIP allowing for additional PKI types can be submitted, similar to how RFCs extend usage of ciphers for TLS (ie., RFC 5932).

Regarding subscriptions, and as proposed in the address book example use case in BIP75, a wallet can be setup to automatically create BIP75 transactions in order to retrieve a wallet address to pay for a subscription on whatever frequency you would like to use. The service provider can approve the first BIP75 transaction and then store the public key for that client for future use. For subsequent subscription payments, the service provider may automatically return wallet addresses for each BIP75 transaction, understanding that the subsequent BIP75 transactions are linked to the public key that was used for the first transaction and therefore the subscription has been paid for. Additionally, the BIP75 InvoiceRequest message contains a memo field that can be used to include any additional subscription information required by the subscription provider (and can be different for both first and subsequent BIP75 transactions).

This is a very interesting idea and I'd love to see how the community can work together to make Bitcoin more user and mainstream friendly while increasing security for all parties involved. All movement toward this is really the goal at Netki.

Best,

Matt David
Sr. Software Engineer
Netki, Inc.

matt@netki.com



> On Jun 21, 2016, at 1:56 PM, James MacWhyte via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> Thanks for starting this discussion, Erik.
> 
> 
> Should this be a new BIP?  I know netki's BIP75 is out there - but I think it's too specific and too reliant on the domain name system.
> 
> This is not quite accurate. BIP75 is designed to be independent of any name resolution system. You could use it with a static URL that you share, for example, or even use it to implement a mesh-network payment system over bluetooth. Netki's wallet names do use DNS, but that isn't related to this discussion.
> 
> What BIP75 *does* do is provide a way for a client to get a new payment address for every payment. I personally think it is better than BIP47 for the uses you mentioned (subscriptions, etc).
> 
> I'm glad you brought up identity methods other than x509. At breadwallet we are thinking about how to establish the most universal system, and letting users identify themselves with any of a selection of identity systems is ideal. I think the pki_data slot should be constantly expanded to allow new identity types, but they should be explained/standardized in the BIPs that add them and use universal names. "netki://" wouldn't be appropriate, for example, if their method is open sourced and possibly used by others--it should instead be given a product name like "dnswallet://" or something more clever.
> 
> James
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
On Tuesday, July 05, 2016 5:46:36 PM Peter Todd wrote:
> On Thu, May 26, 2016 at 03:53:04AM +0000, Luke Dashjr via bitcoin-dev wrote:
> > On Thursday, May 26, 2016 2:50:26 AM Nicolas Dorier via bitcoin-dev wrote:
> > >   Author: Flavien Charlon <flavien@charlon.net>
> 
> What's the status of this BIP? Will it be assigned?

I was waiting for clarification on the Author thing, but Nicholas hasn't 
responded yet. I am unaware of any reason NOT to assign it, and there appear 
to be no objections, so let's call it BIP 160.

Luke


-------------------------------------
Hi folks,

Overall I think BIP 151 is a good idea. However unless I'm mistaken, what's to
prevent someone between peers to suppress the initial 'encinit' message during
negotiation, causing both to fallback to plaintext?

Peers should negotiate a secure channel from the outset or backout entirely
with no option of falling back. This can be indicated loudly by the daemon
listening on an entirely new port.

Alfie

-- 
Alfie John
https://www.alfie.wtf


-------------------------------------
(This response was originally off-list as moderators were still
deciding, here it is for those interested).

Hi Tom,

Thanks for reading the draft text and commenting! Replies inline.

Matt

On 05/08/16 00:40, Johnathan Corgan wrote:
> ---------- Forwarded message ----------
> From: Tom <tomz@freedommail.ch <mailto:tomz@freedommail.ch>>
> To: bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>, Matt Corallo <lf-lists@mattcorallo.com <mailto:lf-lists@mattcorallo.com>>
> Cc: 
> Date: Fri, 06 May 2016 13:31:15 +0100
> Subject: Re: [bitcoin-dev] Compact Block Relay BIP
> On Monday 02 May 2016 22:13:22 Matt Corallo via bitcoin-dev wrote:
> 
> Thanks for putting in the time to make a spec!
> 
> It looks good already, but I do think some more improvements can be made.
> 
> 
>> ===Intended Protocol Flow===
> I'm not a fan of the solution that a CNode should keep state and talk to
> its remote nodes differently while announcing new blocks.
> Its too complicated and ultimately counter-productive.
> 
> The problem is that an individual node needs to predict network behaviour in
> advance. With the downside that if it guesses wrong that both nodes end up
> paying for the wrong guess.
> This is not a good way to design a p2p layer.

Nodes don't need to predict much in advance, and the cost for predicting
wrong is 0 if your peers receive blocks with a few hundred ms between
them (as we should expect) and you haven't set the announce bit on more
than a few peers (as the spec requires for this reason). As for
complexity of keeping state, think of it as a version flag in much the
same way sendheaders operates.

It seems I forgot to add a suggested peer-preforwarding-selection
algorithm in the text, but the intended use-case is to set the bit on
peers which recently provided you blocks faster than other peers, up to
only one or three peers. This is both simple and should be incredibly
effective.

[This has now been clarified in the BIP text]

> I would suggest that a new block is announced to all nodes equally and then
> individual nodes can respond with a request of either a 'compact' or a
> normal block.
> This is much more in line with the current design as well.
> 
> Detection if remote nodes support compact blocks, for the purpose of
> requesting a compact-block, can be done either via a network-bit or just a
> protocol version. Or something else entirely, if you have better
> suggestions.

In line with recent trends, neither service bits nor protocol versions
are particularly well-suited for this purpose. Protocol versions are
impossible to handle sanely across different nodes on the network, as
they cannot indicate optional features. Service bits, while somewhat
more appropriate for this purpose, are a very limited resource which is
generally better suited to indicating significant new features which
nodes might need for correct operation, and thus might wish to actively
seek out when making connections. I'm not sure anyone is suggesting that
here, and absent that recent agreement preferred message-based feature
indication instead of version-message-extension.

>> Variable-length integers: bytes are a MSB base-128 encoding of the
>> number.
>> The high bit in each byte signifies whether another digit follows.
>> [snip bitwise spec]
> 
> I suggest just referring to UTF-8 which describes this just fine.
> it is good practice to refer to existing specs when possible and not copy
> the details.

Hmm? There is no UTF anywhere in this protocol. Indeed this section
needs to be rewritten, as indicated. I'd recommend you read the code
until I update the section with better text if you're confused.

>> ====Short transaction IDs====
>> Short transaction IDs are used to represent a transaction without
>> sending a full 256-bit hash. They are calculated by:
>> # single-SHA256 hashing the block header with the nonce appended (in
>> little-endian)
>> # XORing each 8-byte chunk of the double-SHA256 transaction hash with
>> each corresponding 8-byte chunk of the hash from the previous step
>> # Adding each of the XORed 8-byte chunks together (in little-endian)
>> iteratively to find the short transaction ID
> 
> I don't think this is needed. Just use the first 8 bytes.
> The reason to do xor-ing doesn't hold up and extra complexity is unneeded.
> Especially since you mention some lines down;
> 
>> The short transaction ID calculation is designed to take absolutely
>> minimal processing time during block compaction to avoid introducing
>> serious DoS vulnerabilities

I'm confused as to what, specifically, you're proposing this be changed
to. I'm pretty sure the proposed protocol is about as simple as you can
get while retaining some reasonable collision resistance. I might,
however, decide to switch to siphash with a very low round count, given
that it's probably faster than the cache-fill-time taken by just
iterating over the mempool. Needs a bit further investigation.

> ==Acknowledgements==
> 
> I think you need to acknowledge some more people, or just remove this
> paragraph.
> 
> Cheers

Greg was the only large contributor to the document (and was a very
large contributor, as mentioned - the work is based hugely on a protocol
recommendation he wrote up several years ago) don't see why this should
mean he doesn't get credit.

[For those interested, I'm referring here to
https://en.bitcoin.it/wiki/User:Gmaxwell/block_network_coding. This
BIP/the implementation is a precursor to an implementation that looks
similar to what Greg proposes there which can be found on my udp-wip
branch, which is based on and uses the data structures involved here.]


-------------------------------------
> 
> So I'm interested whether this limitation has been lifted, and the whole 
> feature is considered as finished.

Yes, it's exactly that limitation that has been lifted!

> If yes, I would highly recommend advertising it in the new release notes - as 
> said, the disk space reduction is a big deal.

Good idea, has been added by Marco Falke in commit fa31133,

Wladimir


-------------------------------------
On 05/11/2016 02:01 PM, Matt Corallo via bitcoin-dev wrote:
> Indeed, I think the "ASICs are bad, because 1-CPU-1-vote" arguments
> mostly died out long ago, and, indeed, the goal that many making those
> arguments had of building "unoptimizeable" ASICs failed with them.

Discussion quietened down but never went away.  With centralization of
mining in China, the topic is up for discussion again.  For example,
Z.Cash will now use Equihash as their proof-of-work scheme.

> giving one
> manufacturer/licenser a huge influence in who is successful in a market
> that we're all relying on remaining rather flat.

Central planning is a slippery slope.  Let the market decide the winners
and losers.  It's not feasible to hard fork every time an innovation or
perceived unfair advantage appears in the space.

--Simon


-------------------------------------
On Sat, May 14, 2016 at 7:08 AM, Pavol Rusnak via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On 14/05/16 09:00, Andreas Schildbach via bitcoin-dev wrote:
> > The whole idea of BIP43 (which BIP44 bases on) is that how these BIPs
> > define balance retrieval never changes. This is to make sure you always
> > see the same balance on "same BIP" wallets (and same seed of course).
>
> This! Thanks Andreas for formulating my thought that I was not able to
> articulate earlier.
>

Indeed, this would still be the case when using a new BIPXX to define
adding segwit chains to what were previously BIP43/44 wallets. In this case
retrieval of a BIP44 wallet remains exactly the same as it did before. A
BIP44 wallet can still be recovered with any BIP44 compatible wallet
software. After you upgrade an existing BIP44 wallet to a BIPXX wallet, now
it is no longer a BIP44 wallet. It is now a BIPXX wallet, and can only be
recovered using BIPXX compatible wallet software.

If you are concerned about making a new BIP that fits in the BIP43
framework, i.e. a new purpose number, there's no reason this can't also be
done. You could create a new purpose number YY. Wallets that follow BIPYY
look just like BIPXX, except that they may only contain segwit address
chains, no standard P2PKH address chains.

On Sat, May 14, 2016 at 9:14 AM, Jonas Schnelli via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> AFAIK: Bip39 import (cross-wallet) is not supported by Schildbachs
> android wallet [1] and Electrum [2] and Breadwallet [3].


Breadwallet is BIP39, with the BIP43 purpose 0 derivation path, and I
believe Schlindbachs is as well. Electrum has their own format. I don't
know if it also supports sweeping other mnemonics and wallet layouts.

Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com/>

-------------------------------------
Hi,

I fundamentally disagree with the concept of driving signing workflow by
the wallet software. Wallet software does not know in advance all data
necessary for the signer to do the job. As Jochen mentioned above, Segwit
vs Non-segwit use cases are a good example, but there may be many.

Currently the TREZOR protocol works like device is a server and wallet is a
client calling methods on it. It's like: "Sign this for me, please", "Ok,
give me this information", "Here it is", "Now I need this another
piece".... "There is the signature". Wallet does not know in advance what
will go next, and it is for sake of simplicity. I'm quite happy with the
protocol so far.

Considering the difference in between current hardware, I really don't
think it is possible to find any minimal URI-based API good enough for
communicating with all vendors. What I see more likely is some 3rd party
libraries (JS, C++, Python, ...) defining high-level API and implementing
hardware-specific protocols and transports as plugins. That way vendors are
not limited by strict standard and application developers and services can
integrate wide range of hardware wallets easily. However, this can be done
already and we do not need any standardization process (yet).

slush

On Wed, Aug 17, 2016 at 1:34 PM, Jonas Schnelli via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Hi Dana
>
> >> The URI scheme does not require any sorts of wallet app level
> >> configuration (where the stdio/pipe approach would require to configure
> >> some details about the used hardware wallet).
> >
> > Hi everybody, just thought I’d throw my opinion in here.
> >
> > The URI scheme is a nice idea, but this ignores the fact that hardware
> wallet vendors do most of the work on talking between the computer/mobile
> and the wallet on a lower level of communication. In the case of BitLox,
> the base protocol is Google’s ProtoBuf. The commands and transaction data
> is in a “schema” which is then encoded in different methods accessible via
> ProtoBuf (depending on the data being sent). The advantages of this
> protocol is that it can be implemented on a wide variety of platforms. (but
> that’s a whole 'nother discussion)
> >
> > The URI would be handled waaaaay up in the specific application (such as
> the mytrezor wallet software or the various standalone wallets) - nowhere
> near the actual hardware communications layer.
>
> This is maybe a question of the scope.
> The BIP I'm proposing would make a clear interface cut between
> wallet-with-unsigned-transaction and a signing-device (and maybe between
> wallet-requires-pubkey, signing-device generate some pubkeys [or
> non-hardened xpub]).
>
> The detached-signing proposal does not duplicate work. It just moves the
> current plugin design into a separate application. Plugins in security
> and privacy critical wallet software is something that should probably
> be avoided.
>
> It's intentional at a high level to allow maximum flexibility at the
> hardware interaction layer.
>
> Your protobuf example is a good use-case. You could implement your
> custom processes behind the URI scheme (which is probably way more
> efficient then writing a couple of wallet plugins where you – at the end
> – mostly don't control the deployment and the source-code).
>
> Defining a standard on the hardware interaction layer is possible, but a
> fairly different approach.
>
> </jonas>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On 21/04/16 14:08, Marek Palatinus via bitcoin-dev wrote:
> Sipa, you are probably the most competent to answer this.
> Could you please tell us your opinion? For me, this is
> straightforward, backward compatible fix and I like it a lot.
> Not sure about the process of changing "Final" BIP though.

Sipa: Marek told me you posted your answer and he received it, but it
never reached the list. Could you please resend after figuring out what
went wrong?

-- 
Best Regards / S pozdravom,

Pavol "stick" Rusnak
SatoshiLabs.com


-------------------------------------
For continuity, Matt took the discussion to the bitcoin-discuss lists here
https://lists.linuxfoundation.org/pipermail/bitcoin-discuss/2016-October/000104.html

On Sun, Oct 16, 2016 at 9:45 PM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Sunday, 16 October 2016 19:35:52 CEST Matt Corallo wrote:
> > You keep calling flexible transactions "safer", and yet you haven't
> > mentioned that the current codebase is riddled with blatant and massive
> > security holes.
>
> I am not afraid of people finding issues with my code, I'm only human.
> Would
> appreciate you reporting actual issues instead of hinting at things here.
> Can't fix things otherwise :)
>
> But, glad you brought it up, the reason that FT is safer is because of the
> amount of conceps that SegWit changes in a way that anyone doing
> development
> on Bitcoin later will need to know about them in order to do proper
> development.
> I counted 10 in my latest vlog entry.  FT only changes 2.
>
> Its safer because its simpler.
>
> > For example, you seem to have misunderstood C++'s memory
> > model - you would have no less than three out-of-bound, probably
> > exploitable memory accesses in your 80-LoC deserialize method at
> > https://github.com/bitcoinclassic/bitcoinclassic/
> blob/develop/src/primitiv
> > es/transaction.cpp#L119 if you were to turn on flexible transactions (and
> > I only reviewed that method for 2 minutes).
>
> The unit test doesn't hit any of them. Valgrind only reports such possibly
> exploitable issues in secp256k and CKey::MakeNewKey. The same as in Core.
>
> I don't doubt that your 2 minute look shows stuff that others missed, and
> that valgrind doesn't find either, but I'd be really grateful if you can
> report them specifically to me in an email off list (or github, you know
> the
> drill).
> More feedback will only help to make the proposal stronger and even better.
> Thanks!
>
> > If you want to propose an
> > alternative to a community which has been in desperate need of fixes to
> > many problems for several years, please do so with something which would
> > not take at least a year to complete given a large team of qualified
> > developers.
>
> I think FT fits the bill just fine :)  After your 2 minute look, take a bit
> longer and check the rest of the code. You may be surprised with the
> simplicity of the approach.
> --
> Tom Zander
> Blog: https://zander.github.io
> Vlog: https://vimeo.com/channels/tomscryptochannel
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Bitcoin Knots version 0.13.0.knots20160814 is now available from:

  <https://bitcoinknots.org/files/0.13.x/0.13.0.knots20160814/>

This is a new major version release, including new features, various bugfixes
and performance improvements, as well as updated translations.

Please report bugs using the issue tracker at github:

  <https://github.com/bitcoinknots/bitcoin/issues>

Compatibility
==============

Microsoft ended support for Windows XP on [April 8th, 2014]
(https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support),
an OS initially released in 2001. This means that not even critical security
updates will be released anymore. Without security updates, using a bitcoin
wallet on a XP machine is irresponsible at least.

In addition to that, with 0.12.x there have been varied reports of Bitcoin
Core randomly crashing on Windows XP. It is [not clear]
(https://github.com/bitcoin/bitcoin/issues/7681#issuecomment-217439891)
what the source of these crashes is, but it is likely that upstream libraries
such as Qt are no longer being tested on XP.

We do not have time nor resources to provide support for an OS that is
end-of-life. From 0.13.0 on, Windows XP is no longer supported. Users are
suggested to upgrade to a newer verion of Windows, or install an alternative 
OS that is supported.

No attempt is made to prevent installing or running the software on Windows 
XP, you can still do so at your own risk, but do not expect it to work: do not
report issues about Windows XP to the issue tracker.

Notable changes
===============

Non-mining nodes may influence miner policy
-------------------------------------------

As a side-effect of Compact Blocks support (see below), ordinary non-mining
nodes will download and upload blocks faster if those blocks were produced by
miners using similar transaction filtering policies. This means that a miner
who produces a block with many transactions discouraged by your node will be
relayed slower than one with only transactions already in your memory pool.

The overall effect of such relay differences on the network may result in
blocks which include widely-discouraged transactions losing a stale block
race, and therefore miners may wish to configure their node to take common
relay policies into consideration.

Because of this influence, ordinary nodes should review their mempool policy
configuration, and explicitly make informed decisions about what they wish
their policy to be. Miners should think about whether their current policy is
widely accepted by the community, and consider possibly making adjustments.

Many policy options are available in the GUI settings. For reference, the
equivalent bitcoin.conf settings are: `permitbaremultisig`, `acceptnonstdtxn`,
`bytespersigop`, `bytespersigopstrict`, `datacarrier`, `datacarriersize`,
`mempoolreplacement`, `spamfilter`, `maxorphantx`, `maxmempool`,
`mempoolexpiry`, `limitancestorcount`, `limitancestorsize`,
`limitdescendantcount`, and `limitdescendantsize`. Further details on the
config file options can be seen with the `-help` command line option.

(Note that nodes still respect a strict first-seen order for competing tip
blocks, and this change only affects relay speed to peer nodes.)

Database cache memory increased
--------------------------------

As a result of growth of the UTXO set, performance with the prior default
database cache of 100 MiB has suffered.
For this reason the default was changed to 300 MiB in this release.

For nodes on low-memory systems, the database cache can be changed back to
100 MiB (or to another value) by either:

- Adding `dbcache=100` in bitcoin.conf
- Changing it in the GUI under `Options → Size of database cache`

Note that the database cache setting has the most performance impact
during initial sync of a node, and when catching up after downtime.

bitcoin-cli: arguments privacy
------------------------------

The RPC command line client gained a new argument, `-stdin`
to read extra arguments from standard input, one per line until EOF/Ctrl-D.
For example:

    $ src/bitcoin-cli -stdin walletpassphrase
    mysecretcode
    120
    ..... press Ctrl-D here to end input
    $

It is recommended to use this for sensitive information such as wallet
passphrases, as command-line arguments can usually be read from the process
table by any user on the system.

C++11 and Python 3
------------------

Various code modernizations have been done. The Bitcoin Knots code base has
started using C++11. This means that a C++11-capable compiler is now needed 
for building. Effectively this means GCC 4.7 or higher, or Clang 3.3 or 
higher.

For running the functional tests in `qa/rpc-tests`, Python3.4 or higher is now
required.

Linux ARM builds
----------------

Due to popular request, Linux ARM builds have been added to the uploaded
executables.

The following extra files can be found in the download directory or torrent:

- `bitcoin-${VERSION}-arm-linux-gnueabihf.tar.gz`: Linux binaries for the most
  common 32-bit ARM architecture.
- `bitcoin-${VERSION}-aarch64-linux-gnu.tar.gz`: Linux binaries for the most
  common 64-bit ARM architecture.

ARM builds are still experimental. If you have problems on a certain device or
Linux distribution combination please report them on the bug tracker, it may 
be possible to resolve them.

Note that Android is not considered ARM Linux in this context. The executables
are not expected to work out of the box on Android.

Fee filtering of invs (BIP 133)
-------------------------------

The optional new p2p message "feefilter" is implemented and the protocol
version is bumped to 70013. Upon receiving a feefilter message from a peer,
a node will not send invs for any transactions which do not meet the filter
feerate. [BIP 133]
(https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki)

Compact Block support (BIP 152)
-------------------------------

Support for block relay using the Compact Blocks protocol has been implemented
in PR 8068.

The primary goal is reducing the bandwidth spikes at relay time, though in 
many cases it also reduces propagation delay. It is automatically enabled 
between compatible peers.
[BIP 152](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki)

Hierarchical Deterministic Key Generation
-----------------------------------------
Newly created wallets will use hierarchical deterministic key generation
according to BIP32 (keypath m/0'/0'/k').
Existing wallets will still use traditional key generation.

Backups of HD wallets, regardless of when they have been created, can
therefore be used to re-generate all possible private keys, even the
ones which haven't already been generated during the time of the backup.
**Attention:** Encrypting the wallet will create a new seed which requires
a new backup!

HD key generation for new wallets can be disabled by `-usehd=0`. Keep in
mind that this flag only has affect on newly created wallets.
You can't disable HD key generation once you have created a HD wallet.

There is no distinction between internal (change) and external keys.

HD wallets are incompatible with older versions of Bitcoin Knots.

[Pull request](https://github.com/bitcoin/bitcoin/pull/8035/files), [BIP 32]
(https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki)

Segregated Witness
------------------

The code preparations for Segregated Witness ("segwit"), as described in [BIP
141](https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki), [BIP
143](https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki), [BIP
144](https://github.com/bitcoin/bips/blob/master/bip-0144.mediawiki), and [BIP
145](https://github.com/bitcoin/bips/blob/master/bip-0145.mediawiki) are
finished and included in this release.  However, BIP 141 does not yet specify
activation parameters on mainnet, and so this release does not support segwit
use on mainnet.  Testnet use is supported, and after BIP 141 is updated with
proposed parameters, a future release of Bitcoin Knots is expected that
implements those parameters for mainnet.

Furthermore, because segwit activation is not yet specified for mainnet,
version 0.13.0 will behave similarly as other pre-segwit releases even after a
future activation of BIP 141 on the network.  Upgrading from 0.13.0 will be
required in order to utilize segwit-related features on mainnet (such as 
signal BIP 141 activation, mine segwit blocks, fully validate segwit blocks, 
relay segwit blocks to other segwit nodes, and use segwit transactions in the
wallet, etc).

Mining transaction selection ("Child Pays For Parent")
------------------------------------------------------

The mining transaction selection algorithm has been replaced with an algorithm
that selects transactions based on their feerate inclusive of unconfirmed
ancestor transactions.  This means that a low-fee transaction can become more
likely to be selected if a high-fee transaction that spends its outputs is
relayed.

With this change, the `-blockminsize` command line option has been removed.

The command line option `-blockmaxsize` remains an option to specify the
maximum number of serialized bytes in a generated block.  In addition, the new
command line option `-blockmaxweight` has been added, which specifies the
maximum "block weight" of a generated block, as defined by [BIP 141 
(Segregated Witness)] 
(https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki).

In preparation for Segregated Witness, the mining algorithm has been modified
to choose transactions to mine based on their block weight, rather than number
of serialized bytes.  In this release, transaction selection is unaffected by
this distinction (as BIP 141 activation is not supported on mainnet in this
release, see above), but in future releases and after BIP 141 activation,
these calculations would be expected to differ.

Reindexing changes
------------------

In earlier versions, reindexing did validation while reading through the block
files on disk. These two have now been split up, so that all blocks are known
before validation starts. This was necessary to make certain optimizations 
that are available during normal synchronizations also available during 
reindexing.

The two phases are distinct in the Bitcoin-Qt GUI. During the first one,
"Reindexing blocks on disk" is shown. During the second (slower) one,
"Processing blocks on disk" is shown.

It is possible to only redo validation now, without rebuilding the block 
index, using the command line option `-reindex-chainstate` (in addition to
`-reindex` which does both). This new option is useful when the blocks on disk
are assumed to be fine, but the chainstate is still corrupted. It is also
useful for benchmarks.

Removal of internal miner
--------------------------

As CPU mining has been useless for a long time, the internal miner has been
removed in this release, and replaced with a simpler implementation for the
test framework.

The overall result of this is that `setgenerate` RPC call has been removed, as
well as the `-gen` and `-genproclimit` command-line options.

For testing, the `generate` call can still be used to mine a block, and a new
RPC call `generatetoaddress` has been added to mine to a specific address. 
This works with wallet disabled.

New bytespersigop implementation
--------------------------------

The former implementation of the bytespersigop filter accidentally broke bare
multisig (which is meant to be controlled by the `permitbaremultisig` option),
since the consensus protocol always counts these older transaction forms as 20
sigops for backwards compatibility. Simply fixing this bug by counting more
accurately would have reintroduced a vulnerability. It has therefore been
replaced with a new implementation that rather than filter such transactions,
instead treats them (for fee purposes only) as if they were in fact the size
of a transaction actually using all 20 sigops.

The original filtering behaviour is also available under the new
`bytespersigopstrict` option, but with fixed/accurate sigop counting.

Low-level P2P changes
----------------------

- The P2P alert system has been removed in PR #7692 and the `alert` P2P
  message is no longer supported.

- The transaction relay mechanism used to relay one quarter of all
  transactions instantly, while queueing up the rest and sending them out in
  batch. As this resulted in chains of dependent transactions being reordered,
  it systematically hurt transaction relay. The relay code was redesigned in
  PRs #7840 and #8082, and now always batches transactions announcements while
  also sorting them according to dependency order. This significantly reduces
  orphan transactions. To compensate for the removal of instant relay, the
  frequency of batch sending was doubled for outgoing peers.

- Since PR #7840 the BIP35 `mempool` command is also subject to batch
  processing. Also the `mempool` message is no longer handled for
  non-whitelisted peers when `NODE_BLOOM` is disabled through
  `-peerbloomfilters=0`.

- The maximum size of orphan transactions that are kept in memory until their
  ancestors arrive has been raised in PR #8179 from 5000 to 99999 bytes. They
  are now also removed from memory when they are included in a block, conflict
  with a block, and time out after 20 minutes.

- We respond at most once to a getaddr request during the lifetime of a
  connection since PR #7856.

- Connections to peers who have recently been the first one to give us a valid
  new block or transaction are protected from disconnections since PR #8084.

Low-level RPC changes
----------------------

- `gettxoutsetinfo` UTXO hash (`hash_serialized`) has changed. There was a
  divergence between 32-bit and 64-bit platforms, and the txids were missing
  in the hashed data. This has been fixed, but this means that the output will
  be different than from previous versions.

- Full UTF-8 support in the RPC API. Non-ASCII characters in, for example,
  wallet labels have always been malformed because they weren't taken into
  account properly in JSON RPC processing. This is no longer the case. This
  also affects the GUI debug console. (This may require upgrading system
  UniValue library.)

- Asm script outputs replacements for OP_NOP2 and OP_NOP3

  - OP_NOP2 has been renamed to OP_CHECKLOCKTIMEVERIFY by [BIP 
65](https://github.com/bitcoin/bips/blob/master/bip-0065.mediawiki)

  - OP_NOP3 has been renamed to OP_CHECKSEQUENCEVERIFY by [BIP 
112](https://github.com/bitcoin/bips/blob/master/bip-0112.mediawiki)

  - The following outputs are affected by this change:

    - RPC `getrawtransaction` (in verbose mode)
    - RPC `decoderawtransaction`
    - RPC `decodescript`
    - REST `/rest/tx/` (JSON format)
    - REST `/rest/block/` (JSON format when including extended tx details)
    - `bitcoin-tx -json`

- The sorting of the output of the `getrawmempool` output has changed.

- New RPC commands: `generatetoaddress`, `importprunedfunds`, 
`removeprunedfunds`, `signmessagewithprivkey`,
  `createwitnessaddress`, `addwitnessaddress`.

- Removed RPC commands: `setgenerate`, `getgenerate`.

- New `feeRate` option was added to `fundrawtransaction`

Low-level ZMQ changes
----------------------

- Each ZMQ notification now contains an up-counting sequence number that
  allows listeners to detect lost notifications.
  The sequence number is always the last element in a multi-part ZMQ
  notification and therefore backward compatible. Each message type has its
  own counter. PR [#7762](https://github.com/bitcoin/bitcoin/pull/7762).

0.13.0.knots20160814 Change log
===============================

Detailed release notes follow. This overview includes changes that affect
behavior, not code moves, refactors and string updates. For convenience in 
locating the code changes and accompanying discussion, both the pull request
and git merge commit are mentioned. Changes specific to Bitcoin Knots (beyond
Core) are flagged with an asterisk ('*') before the description.

### RPC and other APIs

- #7156 `9ee02cf` Remove cs_main lock from `createrawtransaction` (laanwj)
- #7326 `2cd004b` Fix typo, wrong information in gettxout help text 
(paveljanik)
- #7222 `82429d0` Indicate which transactions are signaling opt-in RBF 
(sdaftuar)
- #7480 `b49a623` Changed getnetworkhps value to double to avoid overflow 
(instagibbs)
- #7550 `8b958ab` Input-from-stdin mode for bitcoin-cli (laanwj)
- #7670 `c9a1265` Use cached block hash in blockToJSON() (rat4)
- #7726 `9af69fa` Correct importaddress help reference to importpubkey 
(CypherGrue)
- #7766 `16555b6` Register calls where they are defined (laanwj)
- #7797 `e662a76` Fix generatetoaddress failing to parse address (mruddy)
- #7774 `916b15a` Add versionHex in getblock and getblockheader JSON results 
(mruddy)
- #7863 `72c54e3` Getblockchaininfo: make bip9_softforks an object, not an 
array (rustyrussell)
- #7842 `d97101e` Do not print minping time in getpeerinfo when no ping 
received yet (paveljanik)
- #7518 `be14ca5` Add multiple options to fundrawtransaction (promag)
- #7756 `9e47fce` Add cursor to iterate over utxo set, use this in 
`gettxoutsetinfo` (laanwj)
- #7848 `88616d2` Divergence between 32- and 64-bit when hashing >4GB affects 
`gettxoutsetinfo` (laanwj)
- #7827 `4205ad7` Speed up `getchaintips` (mrbandrews)
- #7762 `a1eb344` Append a message sequence number to every ZMQ notification 
(jonasschnelli)
- #7688 `46880ed` List solvability in listunspent output and improve help 
(sipa)
- #7926 `5725807` Push back `getaddednodeinfo` dead value (instagibbs)
- #7953 `0630353` Create `signmessagewithprivkey` rpc (achow101)
- #8049 `c028c7b` Expose information on whether transaction relay is enabled 
in `getnetworkinfo` (laanwj)
- #7967 `8c1e49b` Add feerate option to `fundrawtransaction` (jonasschnelli)
- #8118 `9b6a48c` Reduce unnecessary hashing in `signrawtransaction` 
(jonasnick)
- #7957 `79004d4` Add support for transaction sequence number (jonasschnelli)
- #8153 `75ec320` `fundrawtransaction` feeRate: Use BTC/kB (MarcoFalke)
- #7292 `7ce9ac5` Expose ancestor/descendant information over RPC (sdaftuar)
- #8171 `62fcf27` Fix createrawtx sequence number unsigned int parsing 
(jonasschnelli)
- #7892 `9c3d0fa` Add full UTF-8 support to RPC (laanwj)
- #8317 `304eff3` Don't use floating point in rpcwallet (MarcoFalke)
- #8258 `5a06ebb` Hide softfork in `getblockchaininfo` if timeout is 0 
(jl2012)
- #8244 `1922e5a` Remove unnecessary LOCK(cs_main) in getrawmempool (dcousens)

### Block and transaction handling

- #7056 `6a07208` Save last db read (morcos)
- #6842 `0192806` Limitfreerelay edge case bugfix (ptschip)
- #7084 `11d74f6` Replace maxFeeRate of 10000*minRelayTxFee with maxTxFee in 
mempool (MarcoFalke)
- #7539 `9f33dba` Add tags to mempool's mapTx indices (sdaftuar)
- #7592 `26a2a72` Re-remove ERROR logging for mempool rejects (laanwj)
- #7187 `14d6324` Keep reorgs fast for SequenceLocks checks (morcos)
- #7594 `01f4267` Mempool: Add tracking of ancestor packages (sdaftuar)
- #7904 `fc9e334` Txdb: Fix assert crash in new UTXO set cursor (laanwj)
- #7927 `f9c2ac7` Minor changes to dbwrapper to simplify support for other 
databases (laanwj)
- #7933 `e26b620` Fix OOM when deserializing UTXO entries with invalid length 
(sipa)
- #8020 `5e374f7` Use SipHash-2-4 for various non-cryptographic hashes (sipa)
- #8076 `d720980` VerifyDB: don't check blocks that have been pruned 
(sdaftuar)
- #8080 `862fd24` Do not use mempool for GETDATA for tx accepted after the 
last mempool req (gmaxwell)
- #7997 `a82f033` Replace mapNextTx with slimmer setSpends (kazcw)
- #8220 `1f86d64` Stop trimming when mapTx is empty (sipa)
- #8273 `396f9d6` Bump `-dbcache` default to 300MiB (laanwj)
- #7225 `eb33179` Eliminate unnecessary call to CheckBlock (sdaftuar)
- #7907 `006cdf6` Optimize and Cleanup CScript::FindAndDelete (pstratem)
- #7917 `239d419` Optimize reindex (sipa)
- #7763 `3081fb9` Put hex-encoded version in UpdateTip (sipa)
- #8149 `d612837` Testnet-only segregated witness (sipa)
- #8305 `3730393` Improve handling of unconnecting headers (sdaftuar)
- #8363 `fca1a41` Rename "block cost" to "block weight" (sdaftuar)
- #8381 `f84ee3d` Make witness v0 outputs non-standard (jl2012)
- #8364 `3f65ba2` Treat high-sigop transactions as larger rather than 
rejecting them (sipa)
- n/a   `15edeeb` *Add a new checkpoint at block 421,888 (luke-jr)
- n/a   `6ae2e2d` *Restore original bytespersigop as bytespersigopstrict 
(luke-jr)

### P2P protocol and network code

- #6589 `dc0305d` Log bytes recv/sent per command (jonasschnelli)
- #7164 `3b43cad` Do not download transactions during initial blockchain sync 
(ptschip)
- #7458 `898fedf` peers.dat, banlist.dat recreated when missing (kirkalx)
- #7637 `3da5d1b` Fix memleak in TorController (laanwj, jonasschnelli)
- #7553 `9f14e5a` Remove vfReachable and modify IsReachable to only use 
vfLimited (pstratem)
- #7708 `9426632` De-neuter NODE_BLOOM (pstratem)
- #7692 `29b2be6` Remove P2P alert system (btcdrak)
- #7542 `c946a15` Implement "feefilter" P2P message (morcos)
- #7573 `352fd57` Add `-maxtimeadjustment` command line option (mruddy)
- #7570 `232592a` Add IPv6 Link-Local Address Support (mruddy)
- #7874 `e6a4d48` Improve AlreadyHave (morcos)
- #7856 `64e71b3` Only send one GetAddr response per connection (gmaxwell)
- #7868 `7daa3ad` Split DNS resolving functionality out of net structures 
(theuni)
- #7919 `7617682` Fix headers announcements edge case (sdaftuar)
- #7514 `d9594bf` Fix IsInitialBlockDownload for testnet (jmacwhyte)
- #7959 `03cf6e8` fix race that could fail to persist a ban (kazcw)
- #7840 `3b9a0bf` Several performance and privacy improvements to inv/mempool 
handling (sipa)
- #8011 `65aecda` Don't run ThreadMessageHandler at lowered priority (kazcw)
- #7696 `5c3f8dd` Fix de-serialization bug where AddrMan is left corrupted 
(EthanHeilman)
- #7932 `ed749bd` CAddrMan::Deserialize handle corrupt serializations better 
(pstratem)
- #7906 `83121cc` Prerequisites for p2p encapsulation changes (theuni)
- #8033 `18436d8` Fix Socks5() connect failures to be less noisy and less 
unnecessarily scary (wtogami)
- #8082 `01d8359` Defer inserting into maprelay until just before relaying 
(gmaxwell)
- #7960 `6a22373` Only use AddInventoryKnown for transactions (sdaftuar)
- #8078 `2156fa2` Disable the mempool P2P command when bloom filters disabled 
(petertodd)
- #8065 `67c91f8` Addrman offline attempts (gmaxwell)
- #7703 `761cddb` Tor: Change auth order to only use password auth if -
torpassword (laanwj)
- #8083 `cd0c513` Add support for dnsseeds with option to filter by 
servicebits (jonasschnelli)
- #8173 `4286f43` Use SipHash for node eviction (sipa)
- #8154 `1445835` Drop vAddrToSend after sending big addr message (kazcw)
- #7749 `be9711e` Enforce expected outbound services (sipa)
- #8208 `0a64777` Do not set extra flags for unfiltered DNS seed results 
(sipa)
- #8084 `e4bb4a8` Add recently accepted blocks and txn to 
AttemptToEvictConnection (gmaxwell)
- #8113 `3f89a53` Rework addnode behaviour (sipa)
- #8179 `94ab58b` Evict orphans which are included or precluded by accepted 
blocks (gmaxwell)
- #8068 `e9d76a1` Compact Blocks (TheBlueMatt)
- #8204 `0833894` Update petertodd's testnet seed (petertodd)
- #8247 `5cd35d3` Mark my dnsseed as supporting filtering (sipa)
- #8275 `042c323` Remove bad chain alert partition check (btcdrak)
- #8271 `1bc9c80` Do not send witnesses in cmpctblock (sipa)
- #8312 `ca40ef6` Fix mempool DoS vulnerability from malleated transactions 
(sdaftuar)
- #7180 `16ccb74` Account for `sendheaders` `verack` messages (laanwj)
- #8102 `425278d` Bugfix: use global ::fRelayTxes instead of CNode in version 
send (sipa)
- #8408 `b7e2011` Prevent fingerprinting, disk-DoS with compact blocks 
(sdaftuar)

### Build system

- #7302 `41f1a3e` C++11 build/runtime fixes (theuni)
- #7322 `fd9356b` c++11: add scoped enum fallbacks to CPPFLAGS rather than 
defining them locally (theuni)
- #7441 `a6771fc` Use Debian 8.3 in gitian build guide (fanquake)
- #7349 `152a821` Build against system UniValue when available (luke-jr)
- #7520 `621940e` LibreSSL doesn't define OPENSSL_VERSION, use 
LIBRESSL_VERSION_TEXT instead (paveljanik)
- #7528 `9b9bfce` autogen.sh: warn about needing autoconf if autoreconf is not 
found (knocte)
- #7504 `19324cf` Crystal clean make clean (paveljanik)
- #7619 `18b3f1b` Add missing sudo entry in gitian VM setup (btcdrak)
- #7616 `639ec58`  [depends] Delete unused patches  (MarcoFalke)
- #7658 `c15eb28` Add curl to Gitian setup instructions (btcdrak)
- #7710 `909b72b` [Depends] Bump miniupnpc and config.guess+sub (fanquake)
- #7723 `5131005` build: python 3 compatibility (laanwj)
- #7477 `28ad4d9` Fix quoting of copyright holders in configure.ac (domob1812)
- #7711 `a67bc5e` [build-aux] Update Boost & check macros to latest serials 
(fanquake)
- #7788 `4dc1b3a` Use relative paths instead of absolute paths in protoc calls 
(paveljanik)
- #7809 `bbd210d` depends: some base fixes/changes (theuni)
- #7603 `73fc922` Build System: Use PACKAGE_TARNAME in NSIS script 
(JeremyRand)
- #7905 `187186b` test: move accounting_tests and rpc_wallet_tests to 
wallet/test (laanwj)
- #7911 `351abf9` leveldb: integrate leveldb into our buildsystem (theuni)
- #7944 `a407807` Re-instate TARGET_OS=linux in configure.ac. Removed by 
351abf9e035 (randy-waterhouse)
- #7920 `c3e3cfb` Switch Travis to Trusty (theuni)
- #7954 `08b37c5` build: quiet annoying warnings without adding new ones 
(theuni)
- #7165 `06162f1` build: Enable C++11 in build, require C++11 compiler 
(laanwj)
- #7982 `559fbae` build: No need to check for leveldb atomics (theuni)
- #8002 `f9b4582` [depends] Add -stdlib=libc++ to darwin CXX flags (fanquake)
- #7993 `6a034ed` [depends] Bump Freetype, ccache, ZeroMQ, miniupnpc, expat 
(fanquake)
- #8167 `19ea173` Ship debug tarballs/zips with debug symbols (theuni)
- #8175 `f0299d8` Add --disable-bench to config flags for windows (laanwj)
- #7283 `fd9881a` [gitian] Default reference_datetime to commit author date 
(MarcoFalke)
- #8181 `9201ce8` Get rid of `CLIENT_DATE` (laanwj)
- #8133 `fde0ac4` Finish up out-of-tree changes (theuni)
- #8188 `65a9d7d` Add armhf/aarch64 gitian builds (theuni)
- #8194 `cca1c8c` [gitian] set correct PATH for wrappers (MarcoFalke)
- #8198 `5201614` Sync ax_pthread with upstream draft4 (fanquake)
- #8210 `12a541e` [Qt] Bump to Qt5.6.1 (jonasschnelli)
- #8285 `da50997` windows: Add testnet link to installer (laanwj)
- #8304 `0cca2fe` [travis] Update SDK_URL (MarcoFalke)
- #8310 `6ae20df` Require boost for bench (theuni)
- #8315 `2e51590` Don't require sudo for Linux (theuni)
- #8314 `67caef6` Fix pkg-config issues for 0.13 (theuni)
- #8373 `1fe7f40` Fix OSX non-deterministic dmg (theuni)
- #8358 `cfd1280` Gbuild: Set memory explicitly (default is too low) 
(MarcoFalke)
- #8492 `216d796` *configure: Allow building bench_bitcoin by itself (luke-jr)
- n/a   `2271350` *Qt/Options: Fix warning about comparing signed/unsigned 
(luke-jr)

### GUI

- #7154 `00b4b8d` Add InMempool() info to transaction details (jonasschnelli)
- #7068 `5f3c670` [RPC-Tests] add simple way to run rpc test over QT clients 
(jonasschnelli)
- #7218 `a1c185b` Fix misleading translation (MarcoFalke)
- #7214 `be9a9a3` qt5: Use the fixed font the system recommends (MarcoFalke)
- #7256 `08ab906` Add note to coin control dialog QT5 workaround (fanquake)
- #7255 `e289807` Replace some instances of formatWithUnit with 
formatHtmlWithUnit (fanquake)
- #7317 `3b57e9c` Fix RPCTimerInterface ordering issue (jonasschnelli)
- #7327 `c079d79` Transaction View: LastMonth calculation fixed (crowning-)
- #7334 `e1060c5` coincontrol workaround is still needed in qt5.4 (fixed in 
qt5.5) (MarcoFalke)
- #7383 `ae2db67` Rename "amount" to "requested amount" in receive coins table 
(jonasschnelli)
- #7396 `cdcbc59` Add option to increase/decrease font size in the console 
window (jonasschnelli)
- #7437 `9645218` Disable tab navigation for peers tables (Kefkius)
- #7604 `354b03d` build: Remove spurious dollar sign. Fixes #7189 (dooglus)
- #7605 `7f001bd` Remove openssl info from init/log and from Qt debug window 
(jonasschnelli)
- #7628 `87d6562` Add 'copy full transaction details' option (ericshawlinux)
- #7613 `3798e5d` Add autocomplete to bitcoin-qt's console window (GamerSg)
- #7668 `b24266c` Fix history deletion bug after font size change (achow101)
- #7680 `41d2dfa` Remove reflection from `about` icon (laanwj)
- #7686 `f034bce` Remove 0-fee from send dialog (MarcoFalke)
- #7506 `b88e0b0` Use CCoinControl selection in CWallet::FundTransaction 
(promag)
- #7732 `0b98dd7` Debug window: replace "Build date" with "Datadir" 
(jonasschnelli)
- #7761 `60db51d` remove trailing output-index from transaction-id 
(jonasschnelli)
- #7772 `6383268` Clear the input line after activating autocomplete 
(paveljanik)
- #7925 `f604bf6` Fix out-of-tree GUI builds (laanwj)
- #7939 `574ddc6` Make it possible to show details for multiple transactions 
(laanwj)
- #8012 `b33824b` Delay user confirmation of send (Tyler-Hardin)
- #8006 `7c8558d` Add option to disable the system tray icon (Tyler-Hardin)
- #8046 `169d379` Fix Cmd-Q / Menu Quit shutdown on OSX (jonasschnelli)
- #8042 `6929711` Don't allow to open the debug window during splashscreen & 
verification state (jonasschnelli)
- #8014 `77b49ac` Sort transactions by date (Tyler-Hardin)
- #8073 `eb2f6f7` askpassphrasedialog: Clear pass fields on accept (rat4)
- #8129 `ee1533e` Fix RPC console auto completer (UdjinM6)
- #7636 `fb0ac48` Add bitcoin address label to request payment QR code 
(makevoid)
- #8231 `760a6c7` Fix a bug where the SplashScreen will not be hidden during 
startup (jonasschnelli)
- #8256 `af2421c` BUG: bitcoin-qt crash (fsb4000)
- #8257 `ff03c50` Do not ask a UI question from bitcoind (sipa)
- #8288 `91abb77` Network-specific example address (laanwj)
- #7707 `a914968` UI support for abandoned transactions (jonasschnelli)
- #8207 `f7a403b` Add a link to the Bitcoin-Core repository and website to the 
About Dialog (MarcoFalke)
- #8281 `6a87eb0` Remove client name from debug window (laanwj)
- #8407 `45eba4b` Add dbcache migration path (jonasschnelli)
- n/a   `1e345d2` *Qt/Options: Replace blockminsize with blockmaxweight (luke-
jr)
- n/a   `2185e93` *Qt/Options: Update for bytespersigopstrict (luke-jr)
- n/a   `2da1d28` *Recognise NODE_XTHIN service bit (luke-jr)

### Wallet

- #7262 `fc08994` Reduce inefficiency of GetAccountAddress() (dooglus)
- #7537 `78e81b0` Warn on unexpected EOF while salvaging wallet (laanwj)
- #7521 `3368895` Don't resend wallet txs that aren't in our own mempool 
(morcos)
- #7576 `86a1ec5` Move wallet help string creation to CWallet (jonasschnelli)
- #7577 `5b3b5a7` Move "load wallet phase" to CWallet (jonasschnelli)
- #7608 `0735c0c` Move hardcoded file name out of log messages (MarcoFalke)
- #7649 `4900641` Prevent multiple calls to CWallet::AvailableCoins (promag)
- #7646 `e5c3511` Fix lockunspent help message (promag)
- #7558 `b35a591` Add import/removeprunedfunds rpc call (instagibbs)
- #7691 `30c2dd8` Refactor wallet/init interaction (jonasschnelli)
- #6215 `48c5adf` add bip32 pub key serialization (jonasschnelli)
- #7913 `bafd075` Fix for incorrect locking in GetPubKey() (keystore.cpp) 
(yurizhykin)
- #7816 `0c95ebc` Slighly refactor GetOldestKeyPoolTime() (jonasschnelli)
- #8036 `41138f9` init: Move berkeleydb version reporting to wallet (laanwj)
- #8028 `373b50d` Fix insanity of CWalletDB::WriteTx and 
CWalletTx::WriteToDisk (pstratem)
- #8061 `f6b7df3` Improve Wallet encapsulation (pstratem)
- #7891 `950be19` Always require OS randomness when generating secret keys 
(sipa)
- #7689 `b89ef13` Replace OpenSSL AES with ctaes-based version (sipa)
- #7825 `f972b04` Prevent multiple calls to ExtractDestination (pedrobranco)
- #8137 `243ac0c` Improve CWallet API with new AccountMove function (pstratem)
- #8142 `52c3f34` Improve CWallet API  with new GetAccountPubkey function 
(pstratem)
- #8035 `b67a472` Add simplest BIP32/deterministic key generation 
implementation (jonasschnelli)
- #7687 `a6ddb19` Stop treating importaddress'ed scripts as change (sipa)
- #8298 `aef3811` wallet: Revert input selection post-pruning (laanwj)
- #8324 `bc94b87` Keep HD seed during salvagewallet (jonasschnelli)
- #8323 `238300b` Add HD keypath to CKeyMetadata, report metadata in 
validateaddress (jonasschnelli)
- #8367 `3b38a6a` Ensure <0.13 clients can't open HD wallets (jonasschnelli)
- #8378 `ebea651` Move SetMinVersion for FEATURE_HD to SetHDMasterKey 
(pstratem)
- #8390 `73adfe3` Correct hdmasterkeyid/masterkeyid name confusion 
(jonasschnelli)
- #8206 `18b8ee1` Add HD xpriv to dumpwallet (jonasschnelli)
- #8389 `c3c82c4` Create a new HD seed after encrypting the wallet 
(jonasschnelli)
- n/a   `9480ef4` *wallet: Prevent key origin support for HD wallets, since 
they are incompatible (luke-jr)

### Tests and QA

- #7320 `d3dfc6d` Test walletpassphrase timeout (MarcoFalke)
- #7208 `47c5ed1` Make max tip age an option instead of chainparam (laanwj)
- #7372 `21376af` Trivial: [qa] wallet: Print maintenance (MarcoFalke)
- #7280 `668906f` [travis] Fail when documentation is outdated (MarcoFalke)
- #7177 `93b0576` [qa] Change default block priority size to 0 (MarcoFalke)
- #7236 `02676c5` Use createrawtx locktime parm in txn_clone (dgenr8)
- #7212 `326ffed` Adds unittests for CAddrMan and CAddrinfo, removes source of 
non-determinism (EthanHeilman)
- #7490 `d007511` tests: Remove May15 test (laanwj)
- #7531 `18cb2d5` Add bip68-sequence.py to extended rpc tests (btcdrak)
- #7536 `ce5fc02` test: test leading spaces for ParseHex (laanwj)
- #7620 `1b68de3` [travis] Only run check-doc.py once (MarcoFalke)
- #7455 `7f96671` [travis] Exit early when check-doc.py fails (MarcoFalke)
- #7667 `56d2c4e` Move GetTempPath() to testutil (musalbas)
- #7517 `f1ca891` test: script_error checking in script_invalid tests (laanwj)
- #7684 `3d0dfdb` Extend tests (MarcoFalke)
- #7697 `622fe6c` Tests: make prioritise_transaction.py more robust (sdaftuar)
- #7709 `efde86b` Tests: fix missing import in mempool_packages (sdaftuar)
- #7702 `29e1131` Add tests verifychain, lockunspent, getbalance, 
listsinceblock (MarcoFalke)
- #7720 `3b4324b` rpc-test: Normalize assert() (MarcoFalke)
- #7757 `26794d4` wallet: Wait for reindex to catch up (MarcoFalke)
- #7764 `a65b36c` Don't run pruning.py twice (MarcoFalke)
- #7773 `7c80e72` Fix comments in tests (btcdrak)
- #7489 `e9723cb` tests: Make proxy_test work on travis servers without IPv6 
(laanwj)
- #7778 `ff5874b` Bug fixes and refactor (MarcoFalke)
- #7801 `70ac71b` Remove misleading "errorString syntax" (MarcoFalke)
- #7803 `401c65c` maxblocksinflight: Actually enable test (MarcoFalke)
- #7802 `3bc71e1` httpbasics: Actually test second connection (MarcoFalke)
- #7818 `3911a0a` Refactor script tests (sipa)
- #7849 `ab8586e` tests: add varints_bitpatterns test (laanwj)
- #7846 `491171f` Clean up lockorder data of destroyed mutexes (sipa)
- #7853 `6ef5e00` py2: Unfiddle strings into bytes explicitly (MarcoFalke)
- #7878 `53adc83` [test] bctest.py: Revert faa41ee (MarcoFalke)
- #7798 `cabba24` [travis] Print the commit which was evaluated (MarcoFalke)
- #7833 `b1bf511` tests: Check Content-Type header returned from RPC server 
(laanwj)
- #7851 `fa9d86f` pull-tester: Don't mute zmq ImportError (MarcoFalke)
- #7822 `0e6fd5e` Add listunspent() test for spendable/unspendable UTXO 
(jpdffonseca)
- #7912 `59ad568` Tests: Fix deserialization of reject messages (sdaftuar)
- #7941 `0ea3941` Fixing comment in script_test.json test case (Christewart)
- #7807 `0ad1041` Fixed miner test values, gave constants for less error-prone 
values (instagibbs)
- #7980 `88b77c7` Smartfees: Properly use ordered dict (MarcoFalke)
- #7814 `77b637f` Switch to py3 (MarcoFalke)
- #8030 `409a8a1` Revert fatal-ness of missing python-zmq (laanwj)
- #8018 `3e90fe6` Autofind rpc tests --srcdir (jonasschnelli)
- #7971 `4e14afe` Refactor test_framework and pull tester (MarcoFalke)
- #8016 `5767e80` Fix multithread CScheduler and reenable test (paveljanik)
- #7972 `423ca30` pull-tester: Run rpc test in parallel  (MarcoFalke)
- #8039 `69b3a6d` Bench: Add crypto hash benchmarks (laanwj)
- #8041 `5b736dd` Fix bip9-softforks blockstore issue (MarcoFalke)
- #7994 `1f01443` Add op csv tests to script_tests.json (Christewart)
- #8038 `e2bf830` Various minor fixes (MarcoFalke)
- #8072 `1b87e5b` Travis: 'make check' in parallel and verbose (MarcoFalke)
- #8056 `8844ef1` Remove hardcoded "4 nodes" from test_framework (MarcoFalke)
- #8047 `37f9a1f` Test_framework: Set wait-timeout for bitcoind procs 
(MarcoFalke)
- #8095 `6700cc9` Test framework: only cleanup on successful test runs 
(sdaftuar)
- #8098 `06bd4f6` Test_framework: Append portseed to tmpdir (MarcoFalke)
- #8104 `6ff2c8d` Add timeout to sync_blocks() and sync_mempools() (sdaftuar)
- #8111 `61b8684` Benchmark SipHash (sipa)
- #8107 `52b803e` Bench: Added base58 encoding/decoding benchmarks 
(yurizhykin)
- #8115 `0026e0e` Avoid integer division in the benchmark inner-most loop 
(gmaxwell)
- #8090 `a2df115` Adding P2SH(p2pkh) script test case (Christewart)
- #7992 `ec45cc5` Extend #7956 with one more test (TheBlueMatt)
- #8139 `ae5575b` Fix interrupted HTTP RPC connection workaround for Python 
3.5+ (sipa)
- #8164 `0f24eaf` [Bitcoin-Tx] fix missing test fixtures, fix 32bit atoi issue 
(jonasschnelli)
- #8166 `0b5279f` Src/test: Do not shadow local variables (paveljanik)
- #8141 `44c1b1c` Continuing port of java comparison tool (mrbandrews)
- #8201 `36b7400` fundrawtransaction: Fix race, assert amounts (MarcoFalke)
- #8214 `ed2cd59` Mininode: fail on send_message instead of silent return 
(MarcoFalke)
- #8215 `a072d1a` Don't use floating point in wallet tests (MarcoFalke)
- #8066 `65c2058` Test_framework: Use different rpc_auth_pair for each node 
(MarcoFalke)
- #8216 `0d41d70` Assert 'changePosition out of bounds'  (MarcoFalke)
- #8222 `961893f` Enable mempool consistency checks in unit tests (sipa)
- #7751 `84370d5` test_framework: python3.4 authproxy compat (laanwj)
- #7744 `d8e862a` test_framework: detect failure of bitcoind startup (laanwj)
- #8280 `115735d` Increase sync_blocks() timeouts in pruning.py (MarcoFalke)
- #8340 `af9b7a9` Solve trivial merge conflict in p2p-segwit.py (MarcoFalke)
- #8067 `3e4cf8f` Travis: use slim generic image, and some fixups (theuni)
- #7951 `5c7df70` Test_framework: Properly print exception (MarcoFalke)
- #8070 `7771aa5` Remove non-determinism which is breaking net_tests #8069 
(EthanHeilman)
- #8309 `bb2646a` Add wallet-hd test (MarcoFalke)
- #8444 `cd0910b` Fix p2p-feefilter.py for changed tx relay behavior 
(sdaftuar)
- #6996 `5e6af82` *qa: Adapt preciousblock test to current test framework (and 
Py3) (luke-jr)

### Mining

- #7507 `11c7699` Remove internal miner (Leviathn)
- #7663 `c87f51e` Make the generate RPC call function for non-regtest (sipa)
- #7671 `e2ebd25` Add generatetoaddress RPC to mine to an address (achow101)
- #7935 `66ed450` Versionbits: GBT support (luke-jr)
- #7598 `e1486eb` Refactor CreateNewBlock to be a method of the BlockAssembler 
class (morcos)
- #7600 `66db2d6` Select transactions using feerate-with-ancestors (sdaftuar)
- #8295 `f5660d3` Mining-related fixups for 0.13.0 (sdaftuar)
- #7796 `536b75e` Add support for negative fee rates, fixes 
`prioritizetransaction` (MarcoFalke)
- #8362 `86edc20` Scale legacy sigop count in CreateNewBlock (sdaftuar)
- #8489 `8b0eee6` Bugfix: Use pre-BIP141 sigops until segwit activates (GBT) 
(luke-jr)
- n/a   `5a716a3` *Trivially map blockmaxsize to blockmaxweight while segwit 
is unactivated (luke-jr)

### Documentation and miscellaneous

- #7423 `69e2a40` Add example for building with constrained resources (jarret)
- #8254 `c2c69ed` Add OSX ZMQ requirement to QA readme (fanquake)
- #8203 `377d131` Clarify documentation for running a tor node (nathaniel-
mahieu)
- #7428 `4b12266` Add example for listing ./configure flags (nathaniel-mahieu)
- #7847 `3eae681` Add arch linux build example (mruddy)
- #7968 `ff69aaf` Fedora build requirements (wtogami)
- #8013 `fbedc09` Fedora build requirements, add gcc-c++ and fix typo 
(wtogami)
- #8009 `fbd8478` Fixed invalid example paths in gitian-building.md 
(JeremyRand)
- #8240 `63fbdbc` Mention Windows XP end of support in release notes (laanwj)
- #8303 `5077d2c` Update bips.md for CSV softfork (fanquake)
- #7789 `e0b3e19` Add note about using the Qt official binary installer 
(paveljanik)
- #7791 `e30a5b0` Change Precise to Trusty in gitian-building.md (JeremyRand)
- #7838 `8bb5d3d` Update gitian build guide to debian 8.4.0 (fanquake)
- #7855 `b778e59` Replace precise with trusty (MarcoFalke)
- #7975 `fc23fee` Update bitcoin-core GitHub links (MarcoFalke)
- #8034 `e3a8207` Add basic git squash workflow (fanquake)
- #7813 `214ec0b` Update port in tor.md (MarcoFalke)
- #8193 `37c9830` Use Debian 8.5 in the gitian-build guide (fanquake)
- #8261 `3685e0c` Clarify help for `getblockchaininfo` (paveljanik)
- #7185 `ea0f5a2` Note that reviewers should mention the id of the commits 
they reviewed (pstratem)
- #7290 `c851d8d` [init] Add missing help for args (MarcoFalke)
- #7281 `f9fd4c2` Improve CheckInputs() comment about sig verification 
(petertodd)
- #7417 `1e06bab` Minor improvements to the release process (PRabahy)
- #7444 `4cdbd42` Improve block validity/ConnectBlock() comments (petertodd)
- #7527 `db2e1c0` Fix and cleanup listreceivedbyX documentation (instagibbs)
- #7541 `b6e00af` Clarify description of blockindex (pinheadmz)
- #7590 `f06af57` Improving wording related to Boost library requirements 
[updated] (jonathancross)
- #7635 `0fa88ef` Add dependency info to test docs (elliotolds)
- #7609 `3ba07bd` RPM spec file project (AliceWonderMiscreations)
- #7850 `229a17c` Removed call to `TryCreateDirectory` from 
`GetDefaultDataDir` in `src/util.cpp` (alexreg)
- #7888 `ec870e1` Prevector: fix 2 bugs in currently unreached code paths 
(kazcw)
- #7922 `90653bc` CBase58Data::SetString: cleanse the full vector (kazcw)
- #7881 `c4e8390` Update release process (laanwj)
- #7952 `a9c8b74` Log invalid block hash to make debugging easier (paveljanik)
- #7974 `8206835` More comments on the design of AttemptToEvictConnection 
(gmaxwell)
- #7795 `47a7cfb` UpdateTip: log only one line at most per block (laanwj)
- #8110 `e7e25ea` Add benchmarking notes (fanquake)
- #8121 `58f0c92` Update implemented BIPs list (fanquake)
- #8029 `58725ba` Simplify OS X build notes (fanquake)
- #8143 `d46b8b5` comment nit: miners don't vote (instagibbs)
- #8136 `22e0b35` Log/report in 10% steps during VerifyDB (jonasschnelli)
- #8168 `d366185` util: Add ParseUInt32 and ParseUInt64 (laanwj)
- #8178 `f7b1bfc` Add git and github tips and tricks to developer notes (sipa)
- #8177 `67db011` developer notes: updates for C++11 (kazcw)
- #8229 `8ccdac1` [Doc] Update OS X build notes for 10.11 SDK (fanquake)
- #8233 `9f1807a` Mention Linux ARM executables in release process and notes 
(laanwj)
- #7540 `ff46dd4` Rename OP_NOP3 to OP_CHECKSEQUENCEVERIFY (btcdrak)
- #8289 `26316ff` bash-completion: Adapt for 0.12 and 0.13 (roques)
- #7453 `3dc3149` Missing patches from 0.12 (MarcoFalke)
- #7113 `54a550b` Switch to a more efficient rolling Bloom filter (sipa)
- #7257 `de9e5ea` Combine common error strings for different options so 
translations can be shared and reused (luke-jr)
- #7304 `b8f485c` [contrib] Add clang-format-diff.py (MarcoFalke)
- #7378 `e6f97ef` devtools: replace github-merge with python version (laanwj)
- #7395 `0893705` devtools: show pull and commit information in github-merge 
(laanwj)
- #7402 `6a5932b` devtools: github-merge get toplevel dir without extra 
whitespace (achow101)
- #7425 `20a408c` devtools: Fix utf-8 support in messages for github-merge 
(laanwj)
- #7632 `409f843` Delete outdated test-patches reference (Lewuathe)
- #7662 `386f438` remove unused NOBLKS_VERSION_{START,END} constants (rat4)
- #7737 `aa0d2b2` devtools: make github-merge.py use py3 (laanwj)
- #7781 `55db5f0` devtools: Auto-set branch to merge to in github-merge 
(laanwj)
- #7934 `f17032f` Improve rolling bloom filter performance and benchmark 
(sipa)
- #8004 `2efe38b` signal handling: fReopenDebugLog and fRequestShutdown should 
be type sig_atomic_t (catilac)
- #7713 `f6598df` Fixes for verify-commits script (petertodd)
- #8412 `8360d5b` libconsensus: Expose a flag for BIP112 (jtimon)
- n/a   `d5d0ce6` *corepolicy: Add bytespersigopstrict=0 (luke-jr)
- #7483 `f8bf558` *Update SVG icon rendering for 0.13 (bitcoin_test.ico, RPM 
spec, VPATH builds) (luke-jr)

Credits
=======

Thanks to everyone who directly contributed to this release:

- 21E14
- accraze
- Adam Brown
- Alexander Regueiro
- Alex Morcos
- Alfie John
- Alice Wonder
- AlSzacrel
- Andrew Chow
- Andrés G. Aragoneses
- Bob McElrath
- BtcDrak
- calebogden
- Cédric Félizard
- Chirag Davé
- Chris Moore
- Chris Stewart
- Christian von Roques
- Chris Wheeler
- Cory Fields
- crowning-
- Daniel Cousens
- Daniel Kraft
- Denis Lukianov
- Elias Rohrer
- Elliot Olds
- Eric Shaw
- error10
- Ethan Heilman
- face
- fanquake
- Francesco 'makevoid' Canessa
- fsb4000
- Gavin Andresen
- gladoscc
- Gregory Maxwell
- Gregory Sanders
- instagibbs
- James O'Beirne
- Jannes Faber
- Jarret Dyrbye
- Jeremy Rand
- jloughry
- jmacwhyte
- Joao Fonseca
- Johnson Lau
- Jonas Nick
- Jonas Schnelli
- Jonathan Cross
- João Barbosa
- Jorge Timón
- Kaz Wesley
- Kefkius
- kirkalx
- Krzysztof Jurewicz
- Leviathn
- lewuathe
- Luke Dashjr
- Luv Khemani
- Marcel Krüger
- Marco Falke
- Mark Friedenbach
- Matt
- Matt Bogosian
- Matt Corallo
- Matthew English
- Matthew Zipkin
- mb300sd
- Mitchell Cash
- mrbandrews
- mruddy
- Murch
- Mustafa
- Nathaniel Mahieu
- Nicolas Dorier
- Patrick Strateman
- Paul Rabahy
- paveljanik
- Pavel Janík
- Pavel Vasin
- Pedro Branco
- Peter Todd
- Philip Kaufmann
- Pieter Wuille
- Prayag Verma
- ptschip
- Puru
- randy-waterhouse
- R E Broadley
- Rusty Russell
- Suhas Daftuar
- Suriyaa Kudo
- TheLazieR Yip
- Thomas Kerin
- Tom Harding
- Tyler Hardin
- UdjinM6
- Warren Togami
- Will Binns
- Wladimir J. van der Laan
- Yuri Zhykin

As well as everyone that helped translating on [Transifex]
(https://www.transifex.com/projects/p/bitcoin/).

-------------------------------------
> the soft-fork does not become Final for as long as such a hard-fork
has potentially-majority support, or at most three months.
This wording is awkward. What is "potentially-majority"?

>A hard-fork BIP requires adoption from the entire Bitcoin economy,
particularly including those selling desirable goods and services in
exchange for bitcoin payments, as well as Bitcoin holders who wish to
spend or would spend their bitcoins (including selling for other
currencies) differently in the event of such a hard-fork.
What if one shop owner, for example, out of thousands, doesn't adapt the
hard-fork? It is expected, and should perhaps be encouraged, for a small
minority to not accept a hard fork, but by the wording of the BIP
("entire Bitcoin economy"), one shop owner can veto a hard-fork.

Mustafa

On 08/03/16 19:04, Luke Dashjr via bitcoin-dev wrote:
> It has been about 1 month since BIP 2 finished receiving comments, so I 
> believe it is an appropriate time to begin the process of moving it to Final 
> Status. Toward this end, I have opened a pull request:
>
>     https://github.com/bitcoin/bips/pull/350
>
> The current requirement for this is that "the reference implementation is 
> complete and accepted by the community". Given the vagueness of this criteria, 
> I intend to move forward applying BIP 2's more specific criteria to itself:
>
>> A process BIP may change status from Draft to Active when it achieves rough
>> consensus on the mailing list. Such a proposal is said to have rough
>> consensus if it has been open to discussion on the development mailing list
>> for at least one month, and no person maintains any unaddressed
>> substantiated objections to it. Addressed or obstructive objections may be
>> ignored/overruled by general agreement that they have been sufficiently
>> addressed, but clear reasoning must be given in such circumstances.
> Furthermore, there is a reference implementation in the mentioned PR.
>
> Please review the latest draft BIP and provide any objections ASAP.
> If there are no outstanding objections on 2016 April 9th, I will consider the 
> current draft to have reached rough consensus and update its Status to Final 
> by merging the PR.
>
> Thanks,
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
On Fri, Feb 12, 2016 at 10:31:56PM +1100, gladoscc via bitcoin-dev wrote:
> Here's a method of fixing block withholding attacks with a soft fork:

So, while you're technique I believe works, it's not a soft-fork, at
least under the definition most of the Bitcoin dev/research community
have been using.

The reason is if it's adopted by a majority of hashing power, less than
a majority of hashing power can create a chain that appears to be the
most-work chain, from the perspective of non-adopting nodes. Those nodes
would then be following a weaker chain.

A better term for what you're proposing might be a "pseudo-soft-fork",
given that you don't quite meet the requirements for a true soft-fork.
Having said that, it may be the case that overall your technique still
reduces risk compared to a simpler hard-fork implementation of the idea;
more analysis is needed there.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000006d243cee301d792809a7d4d00c13ac24b43d5e9548625e4

-------------------------------------
On Fri, Apr 29, 2016 at 02:22:32PM -0400, Kristov Atlas via bitcoin-dev wrote:
> A sample of the 16 transaction id's posted in the JoinMarket thread on
> BitcoinTalk shows an average ratio of 1.38 or outputs to inputs:
> 
> https://docs.google.com/spreadsheets/d/1p9jZYXxX1HDtKCxTy79Zj5PrQaF20mxbD7BAuz0KC8s/edit?usp=sharing
> 
> As we know, a "traditional" CoinJoin transaction creates roughly 2x UTXOs
> for everyone 1 it consumes -- 1 spend and 1 change -- unless address reuse
> comes into play.

Note how this is obviously an unsustainable situation - at some point that
change needs to be combined again, or you're throwing away money in the form of
UTXO's that aren't ever getting spent.

Meanwhile, if you put it another way the segwit discount is an obvious
advantage for coinjoin: by making spending UTXO's cheaper, we can recover those
funds that would otherwise get lost to dust, becoming ever more difficult to
spend.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
It would be nice if the detached signer and the normal wallet could both
verify the correctness of generated addresses before you cause coins to be
sent there.

e.g. the hardware wallet could give its master public key to Bitcoin Core
and you can thereafter generate your receiving addresses on Core, with the
option to have the HW wallet validate them.

One of my biggest fears about using any wallet is the "whoops, cosmic ray
flipped a bit while producing receiving address; SFYL!" possibility. For
high value cold storage, I always generate my addresses on two independent
machines using two different pieces of software. Am I nuts for doing that?

With the above scheme, you are pretty well protected from losing money if
your HW wallet is defective. You could still lose it if the HW wallet was
evil of course, but that strikes me as much more likely to be discovered
quickly.

-------------------------------------
Daniele Pinna via bitcoin-dev [bitcoin-dev@lists.linuxfoundation.org] wrote:
> This seems unnecessarily complicated ("don't use cannon to kill mosquito" kind
> of thing). If the community were interested in a realtime hashrate rebalancing
> proposal one could simply adjust difficulty at each new block using the current
> method.

That proposal is equivalent to an under-damped oscillator, and would still see
significant oscillation between blocks if miners were switching on and off
hardware.

> If faster relaxation in case of adversity is required, it suspect that it would
> suffice to perform a weighted average of the previous 2016 blocks instead of
> the standard averaging that is currently done. It should be possible to find an
> optimal weighting based on historical interblock timing data. I look into it
> over the next couple of days.

The optimal solution is the one I quote, and is well known, just not in the
bitcoin community.

"faster relaxation time" refers to the time constant of the exponential damping.
if you make it too fast, you create an over-damped oscillator.  The system
cannot measure oscillation faster than the block time, so damping on shorter
timescales is useless.  The optimal damping is given by the critically damped
oscillator.

Tonight at BitDevsNYC, Mike Wozniak pointed out that SPV wallets must also
calculate retargeting, but this is a terribly simple calculation, and while more
complex from a coding perspective, would not be noticeable in run-time of SPV
wallets.

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 



-------------------------------------
On Tue, Jan 12, 2016 at 8:17 PM, Eric Voskuil <eric@voskuil.org> wrote:
> Jorge, first, thanks again for your work on this.
>
> Without creating and using a public blockchain interface in phase 2, how
> will you isolate the database dependency from consensus critical code?
> Is it that the interface will exist but you will recommend against its use?

The interface will exist but it will be a C++ interface that fits
better with Bitcoin Core internals.
See an initial draft of what could be the storage interface:
https://github.com/jtimon/bitcoin/blob/1717db89c6db17ea65ddbd5eb19034315db0b059/src/consensus/storage_interfaces_cpp.h

Phase 3 will consist on discussing and refining that interface to also
define the C interfaces using structs of function pointers instead of
classes (see https://github.com/jtimon/bitcoin/blob/2bcc07c014e5dd000030e666be047dfa11f54c10/src/consensus/interfaces.h
for an early draft) that is needed for the "final" C API.
But since I think there will be more discussion and work defining
those interfaces, I would rather start with ANY interface that allows
us to decouple the consensus code from chain.o and coins.o, which we
don't want to be built as part of the consensus building package
(which is used for building both libbitcoinconsensus and Bitcoin
Core).
Future potential users are more than welcomed to draft their own C
APIs and that experience should be useful for phase 3.
I was expecting you, for example, to include the whole consensus code
(even if it lacks a C API) in
https://github.com/libbitcoin/libbitcoin-consensus for better testing
of the equivalent code in libbitcoin. You are kind of taking the C API
part out already, so this time you will just have less things to
delete/ignore.

> This work presumes that the users of the library reject the argument
> that the database implementation is consensus critical code. Faithful
> reproduction of stored data is a prerequisite for a validity. But a
> common store implementation is only slightly more reasonable for this
> library than a common RAM implementation.

This is a concern that has been risen repeatedly.
I am aware that faithful reproduction of the stored data is a
prerequisite for consensus validity. On the other hand, my presumption
is that a libbitcoinconsensus that forces its users to a given unifrom
storage will likely had much less users and any alternative
implementation that wants to implement its own custom storage would
have to necessarily reimplement the consensus validation code.
Doing it this way is more flexible. We can relatively easily implement
another library (if I remember correctly, last time we talked about it
we reffered to it as "libconsensus plus", but there's probably better
names) also takes care of storage for the users that don't want to
take the risks of reimplementing the storage (probably just using
Bitcoin Core's structures).

Unlike me, Luke Dashjr, for example, advocated for the
storage-dependent version, but I believe that implementing both
versions was an acceptable solution to him.
It is certainly an acceptable solution for me. I don't want to force
anyone that doesn't want or need to take the risks reimplementing the
consensus storage part to do so. But at the same time I really believe
that it would be a mistake to not allow it optionally.

Does that make sense?


-------------------------------------
Missing link to paper: https://arxiv.org/abs/1605.04559

Another relevant paper:

On Bitcoin as a public randomness source
Joseph Bonneau, Jeremy Clark, and Steven Goldfeder
https://eprint.iacr.org/2015/1015.pdf

On Tue, May 24, 2016 at 11:30 AM, Sergio Demian Lerner <
sergio.d.lerner@gmail.com> wrote:

> Bitcoin Beacon paper relevant here
>
> Basically is suggest using deciding a random bit on the majority 1s or 0s
> of lsb bits taken from last block hashes.
>
> Iddo Bentov∗ Technion, Ariel Gabizon,  David Zuckerman
>
> We examine a protocol πbeacon that outputs unpredictable and publicly
> verifiable randomness, meaning that the output is unknown at the time that
> πbeacon starts, yet everyone can verify that the output is close to uniform
> after πbeacon terminates. We show that πbeacon can be instantiated via
> Bitcoin under sensible assumptions; in particular we consider an adversary
> with an arbitrarily large initial budget who may not operate at a loss
> indefinitely.
> In case the adversary has an infinite budget, we provide an impossibility
> result that stems from the similarity between the Bitcoin model and
> Santha-Vazirani sources. We also give a hybrid protocol that combines
> trusted parties and a Bitcoin-based beacon.
>
> On Sun, May 22, 2016 at 10:30 AM, Jeremy via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> nack -- not secure.
>>
>> OP_PRANDOM also adds extra validation overhead on a block potentially
>> composed of transactions all spending an OP_PRANDOM output from all
>> different blocks.
>>
>> I do agree that random numbers are highly desirable though.
>>
>> I think it would be much better for these use cases to add OP_XOR back
>> and then use something like Blum's fair coin-flipping over the phone.
>> OP_XOR may have other uses too.
>>
>> I have a write-up from a while back which does Blum's without OP_XOR
>> using OP_SIZE for off-chain probabilistic payments if anyone is interested.
>> No fork needed, but of course it is more limited and broken in a number of
>> ways.
>>
>> (sorry to those of you seeing this twice, my first email bounced the list)
>>
>> --
>> @JeremyRubin <https://twitter.com/JeremyRubin>
>> <https://twitter.com/JeremyRubin>
>>
>> On Fri, May 20, 2016 at 2:32 PM, Eric Martindale via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> Matthew,
>>>
>>> You should take a look at OP_DETERMINISTICRANDOM [1] from the Elements
>>> Project.  It aims to achieve a similar goal.
>>>
>>> Code is in the `alpha` branch [2].
>>>
>>> [1]: https://www.elementsproject.org/elements/opcodes/
>>> [2]:
>>> https://github.com/ElementsProject/elements/blob/alpha/src/script/interpreter.cpp#L1252-L1305
>>>
>>> On Fri, May 20, 2016 at 8:29 AM Matthew Roberts via bitcoin-dev <
>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>> Good point, to be honest. Maybe there's a better way to combine the
>>>> block hashes like taking the first N bits from each block hash to produce a
>>>> single number but the direction that this is going in doesn't seem ideal.
>>>>
>>>> I just asked a friend about this problem and he mentioned using the
>>>> hash of the proof of work hash as part of the number so you have to throw
>>>> away a valid POW if it doesn't give you the hash you want. I suppose its
>>>> possible to make it infinitely expensive to manipulate the number but I
>>>> can't think of anything better than that for now.
>>>>
>>>> I need to sleep on this for now but let me know if anyone has any
>>>> better ideas.
>>>>
>>>>
>>>>
>>>> On Fri, May 20, 2016 at 6:34 AM, Johnson Lau <jl2012@xbt.hk> wrote:
>>>>
>>>>> Using the hash of multiple blocks does not make it any safer. The
>>>>> miner of the last block always determines the results, by knowing the
>>>>> hashes of all previous blocks.
>>>>>
>>>>>
>>>>> == Security
>>>>>
>>>>> Pay-to-script-hash can be used to protect the details of contracts
>>>>> that use OP_PRANDOM from the prying eyes of miners. However, since there is
>>>>> also a non-zero risk that a participant in a contract may attempt to bribe
>>>>> a miner the inclusion of multiple block hashes as a source of randomness is
>>>>> a must. Every miner would effectively need to be bribed to ensure control
>>>>> over the results of the random numbers, which is already very unlikely. The
>>>>> risk approaches zero as N goes up.
>>>>>
>>>>>
>>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev@lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>

-------------------------------------
I would expect that custodians who fail to produce coins on both sides
of a fork in response to depositor requests will find themselves in
serious legal trouble.

Especially if the price moves against either fork.

On 02/07/2016 10:55 AM, Jonathan Toomim via bitcoin-dev wrote:
>
> On Feb 6, 2016, at 9:21 PM, Jannes Faber via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org
> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>
>> They *must* be able to send their customers both coins as separate
>> withdrawals.
>>
> Supporting the obsolete chain is unnecessary. Such support has not
> been offered in any cryptocurrency hard fork before, as far as I know.
> I do not see why it should start now.
>>
>> If not, that amounts to theft of their customers funds.
>>
> If they announce their planned behavior before the fork, I do not see
> any ethical or legal issues. 
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
I agree, audio-based transference isn't really great for a podcast or radio
ad. It could be used to transmit payment details between phones that don't
have cameras, though. I think it would be better to define a standard for
transmitting information over audio, but not define what information is to
be conveyed so people could use the method for sending pub keys, payment
protocol requests, or anything else developers might want to make use of.

I'm guessing some sort of data-over-audio standard already exists? In which
case the bip could just say "we use [standard] to convey any
bitcoin-related data".

On Wed, Aug 10, 2016, 10:55 Daniel Hoffman via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> thats how i thought it worked originally, but im not well versed on that,
> so i took his word for it
>
> On Aug 10, 2016 12:38 PM, "Pieter Wuille via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> On Wed, Aug 10, 2016 at 7:28 PM, Erik Aronesty via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > By sending a public seed,  there's no way for someone to use the
>> transmitted
>> > address and trace the total amount of payments to it.
>>
>> Worse. By revealing a public seed, anyone who has seen it (= anyone
>> who ever pays you through it) can identity all payments to _any_
>> address derived from that seed.
>>
>> --
>> Pieter
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

In case it helps:
The elements alpha sidechain uses a different address format, which
includes an ECDH pubkey used for creating an ECDH shared secret.
That shared secret is used to seed a RFC6979 prng, which allows both
sides to generate the blinding factors used in the rangeproof.

So the situation is: both sides can generate the blinding factors, but
also the fake signatures used in the rangeproof (the basic idea there
is to have N signatures in a ring, but only one of them real; the rest
are forged and can be (must be) entirely random numbers. I say 'basic'
because the Borromean sig design is to link together several rings,
not just one). This allows the sender to embed the amount into one of
those fake signatures (usually the last one) using xor, with certain
formatting details.

It would be possible to not bother to embed the amount in this way;
the receiver, knowing the stream of fake/real signatures (again -
because he knows the seed for the prng), could simply observe which
ones are real and therefore know the digits of the amount. But if he
did it this way, it would not be possible to embed any other data into
the range proof (such as: auditing related information) using xor as
above.

I did some detailed explanation/investigation of this in sections 3.3
and 3.4 of
https://github.com/AdamISZ/ConfidentialTransactionsDoc/blob/master/essayonCT.pdf
; with apologies for any errors, it was just an investigation I did
last summer.


On 02/10/2016 06:39 PM, Jeremy Papp via bitcoin-dev wrote:
> On 2/10/2016 5:53 AM, Henning Kopp wrote:
>> Hi Jeremy,
>> 
>>> My understanding of the paper is that the blinding factor would
>>> be included in the extra data which is incorporated into the
>>> ring signatures used in the range proof.
>> Yep, that is a possibility. The blinding factor could be
>> encrypted with the public key of the receiver. Thus it is only
>> visible for the receiver who can then check that the correct
>> amount has been sent.
> ECC doesn't work like RSA; you can't encrypt directly with a
> public key.  That's why you generate a shared secret between sender
> and receiver.  See also, ECDH. (Basically, if (m, M = m*G) is your 
> private/public key pair, and (n, N = n*G) is your recipient's
> private public key pair, you can both generate shared secret S =
> m*N = n*M = m*n*G without revealing your private keys to each
> other, and without revealing the secret to anyone else as long as
> they don't know either private key. You then use S as the basis for
> the key to some symmetric algorithm.)
>>> you'd transmit it then, though in any case, since using it
>>> will pretty much require segwit, adding extraneous data isn't
>>> much of a problem.  In both cases, I imagine the blinding
>>> factor would be protected from outside examination via some
>>> form of shared secret generation... Although that would require
>>> the sender to know the recipient's unhashed public key; I don't
>>> know of any shared secret schemes that will work on hashed
>>> keys.
>> Here you lost me. Why do we need to create a shared secret? Is
>> this shared secret used as the blinding factor? Also I think the
>> sender knows the unhashed public key of the receiver. The only
>> reason not to include it in the transaction script is that an 
>> external observer is unable to see the receiver directly in the 
>> blockchain.
> Normal Bitcoin transactions are made to the hash of a public key
> because once the public key is known, it becomes easier to break it
> if we ever develop quantum computers. That's why it's recommended
> that you only spend from a particular address once (if possible)
> since its only in spending that you are required to reveal your
> public key.   Since you can't do a shared secret with a public key
> hash, AFAIK, you'd have to know the public key of your recipient to
> be able to do ECDH.
> 
> Jeremy Papp _______________________________________________ 
> bitcoin-dev mailing list bitcoin-dev@lists.linuxfoundation.org 
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBAgAGBQJWv1/8AAoJELOuCfHpoxl6dV8H/AvlEUebgKBAZdSdIEDKm0m0
pSXNWH62v327YdJ2wFqPCB2zG9HKXP76XhCGx39PEEvBmAFAoD6URAWPk8o03kTo
aJZUeRB7wLqIALuUub/0JzAJwcxZtTIhYu3ygfyZZuvpomG8yXlERwfjB+BcCXnm
D7TJ2qOyq3X3uaneb/OnUEvDxOrl9zAp9q7CUnFQB2xagCRnHyGNcrWaH43RmpHl
Eima6eonQUR4AAcIUu0CKSRjgM6q46bMbXTFt9I4XeqQxsMB5Gfe9Ggk15TNRoUm
ENVaJnPL4qlJqODSrO9R4xrurVCcp7HVeR9B5aztFQszVNxhMoZtFlyn5U3J0gY=
=+I00
-----END PGP SIGNATURE-----


-------------------------------------
I like this idea, but let's run some numbers...

bfd--- via bitcoin-dev [bitcoin-dev@lists.linuxfoundation.org] wrote:
> A Bloom Filter Digest is deterministically created of every block

Bloom filters completely obfuscate the required size of the filter for a desired
false-positive rate.  But, an optimal filter is linear in the number of elements
it contains for fixed false-positive rate, and logarithmic in the false-positive
rate.  (This comment applies to a RLL encoded Bloom filter Greg mentioned, but
that's not the only way)  That is for N elements and false positive rate
\epsilon:

    filter size = - N \log_2 \epsilon

Given that the data that would be put into this particular filter is *already*
hashed, it makes more sense and is faster to use a Cuckoo[1] filter, choosing a
fixed false-positive rate, given expected wallet sizes.  For Bloom filters,
multiply the above formula by 1.44.

To prevent light clients from downloading more blocks than necessary, the
false-positive rate should be roughly less than 1/(block height).  If we take
the false positive rate to be 1e-6 for today's block height ~ 410000, this is
about 20 bits per element.  So for todays block's, this is a 30kb filter, for a
3% increase in block size, if blocks commit to the filter.  Thus the required
size of the filter commitment is roughly:

    filter size = N \log_2 H

where H is the block height.  If bitcoin had these filters from the beginning, a
light client today would have to download about 12MB of data in filters.  My
personal SPV wallet is using 31MB currently.  It's not clear this is a bandwidth
win, though it's definitely a win for computing load on full nodes.


[1] https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


-------------------------------------
On Jun 8, 2016 18:46, "Luke Dashjr via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> On Wednesday, June 08, 2016 8:23:51 AM Johnson Lau wrote:
> > If someday 32 bytes hash is deemed to be unsafe, the txid would also be
> > unsafe and a hard fork might be needed. Therefore, I don’t see how a
> > witness program larger than 40 bytes would be useful in any case (as it
is
> > more expensive and takes more UTXO space). I think Pieter doesn’t want
to
> > make it unnecessarily lenient.
>
> There is no harm in being lenient, but it limits the ability to do
softfork
> upgrades in the future. I appreciate Pieter's concern that we'd need to do
> more development and testing to go to this extreme, which is why I am only
> asking the limit raised to 75 bytes.

No strong opinion, but I'd rather not change it anymore, as I don't see the
point. Any data you would want to encode there can be moved to the witness
at 1/4 the cost and replaced by a 256-bit hash. If the data is 43 bytes or
higher, that is even cheaper. The only thing that cannot be in the hash is
metadata to indicate what hashing/rule scheme itself is used. I think 68
bits (OP_n + 8 bytes) for that is plenty.

-- 
Pieter

-------------------------------------
This is not the place to discuss the merits and/or issues of these BIPs,
only whether they should be treated as final.

On Aug 25, 2016 10:51, "Marek Palatinus via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> As Luke pointed, BIP44 is already used by many wallets and to my knowledge
> people don't have any real world issues with that, including loading funds
> in another BIP44 wallet. I'm not saying that BIP44 is perfect from all
> points of view, but IMO it just works for most use cases. Let's set it as
> final, and propose competing standards which cover all your concerns.
>
> slush
>
> On Thu, Aug 25, 2016 at 10:12 AM, Jonas Schnelli via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>>
>> > The development paradigm of "maybe detect funds" is not something we
>> > should *not* encourage for Bitcoin IMO.
>>
>> Sorry. That was one "not" to many.
>>
>> </jonas>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
> When you proposed the extra nonce space BIP [1], you had already
> applied for your ASICBOOST patent [2] without disclosure in the BIP
> [1] nor in your Bitcoin Core pull request #5102 [2].

There may be quite a few things to clarify here, and a possible
misunderstanding:

The BIP proposal [1] and accompanying pull request [3] does not increase or
decrease the entanglement of Bitcoin consensus code with any patents. This
is indicated by the title of the pull request: "No forking Extra nonce
added to Bitcoin header." It is not a fork at all (soft or hard). The
consensus is not changed.

AsicBoost is possible with or without adoption of that BIP proposal. Of
several ways to implement AsicBoost (all described in the patent
application), making use of the version field is only one. And even that
particular one has always been possible since the beginning of Bitcoin and
is still possible today. It is not the case that the BIP proposal enables
AsicBoost in a way that wasn't possible before.

The rationale behind the BIP proposal was to eliminate incentives to mess
with the merkle root and, in the extreme case, to mine empty blocks. This
incentive is real, and it is real with or without AsicBoost. It costs
hardware manufacturers real $ in additional hardware components right now
to cope with the pre-hashing load.

Timo


On Sun, Oct 2, 2016 at 12:36 PM, Btc Drak via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Sergio,
>
> It is critically important to the future of Bitcoin that consensus
> code avoid any unnecessary entanglements with patents because "the
> free market" allows you and anyone else to make consensus change
> proposals that rely on (unknown) patents - but this is something we
> should all be working to avoid, as it unnecessarily hinders Bitcoin
> development and everyone's ability to deploy. Consensus code must not
> be hindered by patents and Bitcoin should retain its permissionless
> qualities.
>
> When you proposed the extra nonce space BIP [1], you had already
> applied for your ASICBOOST patent [2] without disclosure in the BIP
> [1] nor in your Bitcoin Core pull request #5102 [2].
>
> The ASICBOOST patent [2] describes the same process as in the BIP [1]
> and proposed code [3] "As we explained in our Provisional Application,
> it has been proposed to partition the 4-byte Version field in the
> block header (see, Fig. 6) and use, e.g., the high 2-byte portion as
> additional nonce range."
>
> Today when you proposed a new sidechain BIP [4], Peter Todd was
> (rightly) concerned about the prior lack of disclosure of your patents
> related to your prior consensus modification proposal. Hence the
> concern is that this might be happening this time as well.
>
> There is no evidence that any of the other filers for the
> ASICBOOST-like patents by mining companies other than your own were
> going to be using it offensively as those other companies appeared to
> understand the decentralization risk of having an advantage enforced
> by legal and not technical means.
>
> It's great that you have now committed to looking into the Defensive
> Patent License. This seems likely to mitigate some of the patent
> concerns. Although it would be a show of good faith if you also agreed
> to license ASICBOOST under the DPL.
>
> [1]: BIP: https://github.com/BlockheaderNonce2/bitcoin/wiki
> [2]: ASICBOOST PATENT https://www.google.com/patents/WO2015077378A1?cl=en
> [3]: Extra nonce pull request: https://github.com/bitcoin/
> bitcoin/pull/5102
> [4]: COUNT_ACKS
> [https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-October/
> 013174.html
>
> On Sun, Oct 2, 2016 at 6:13 PM, Sergio Demian Lerner via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > Please Peter Todd explain here all what you want to say about a patent
> of a
> > hardware design for an ASIC.
> >
> > Remember that ASICBoost is not the only patent out there, there are at
> least
> > three similar patents, filed by major Bitcoin ASIC manufacturers in three
> > different countries, on similar technologies.
> >
> > That suggest that the problem is not ASICBoot's: you cannot blame any
> > company from doing lawful commerce in a FREE MARKET.
> >
> > It is a flaw in Bitcoin design that could be corrected if the guidelines
> I
> > posted in [1] had been followed.
> >
> > [1]
> > https://bitslog.wordpress.com/2014/03/18/the-re-design-of-
> the-bitcoin-block-header/
> >
> >
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
If I understand this BIP correctly, the values pushed onto the stack by the
OP_COUNT_ACKS operation depends on the ack and nack counts relative to the
block that this happens to be included in.

This isn't going to be acceptable.  The validity of a transaction should
always be monotone in the sense that if a transaction was acceptable in a
given block, it must always be acceptable in any subsequent block, with the
only exception being if one of the transaction's inputs get double spent.

The added check lock time and check sequence number operations were
carefully constructed to ensure this property.

On Sun, Oct 2, 2016 at 11:49 AM, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Since ScalingBitcoin is close, I think this is a good moment to publish
> our proposal on drivechains. This BIP proposed the drivechain we'd like to
> use in RSK (a.k.a. Rootstock) two-way pegged blockchain and see it
> implemented in Bitcoin. Until that happens, we're using a federated
> approach.
> I'm sure that adding risk-less Bitcoin extensibility through
> sidechains/drivechains is what we all want, but it's of maximum importance
> to decide which technology will leads us there.
> We hope this work can also be the base of all other new 2-way-pegged
> blockchains that can take Bitcoin the currency to new niches and test new
> use cases, but cannot yet be realized because of current
> limitations/protections.
>
> The full BIP plus a reference implementation can be found here:
>
> BIP (draft):
> https://github.com/rootstock/bips/blob/master/BIP-R10.md
>
> Code & Test cases:
> https://github.com/rootstock/bitcoin/tree/op-count-acks_devel
> (Note: Code is still unaudited)
>
> As a summary, OP_COUNT_ACKS is a new segwit-based and soft-forked opcode
> that counts acks and nacks tags in coinbase fields, and push the resulting
> totals in the script stack.
>
> The system was designed with the following properties in mind:
>
> 1. Interoperability with scripting system
> 2. Zero risk of invalidating a block
> 3. No additional computation during blockchain management and
> re-organization
> 4. No change in Bitcoin security model
> 5. Bounded computation of poll results
> 6. Strong protection from DoS attacks
> 7. Minimum block space consumption
> 8. Zero risk of cross-secondary chain invalidation
>
> Please see the BIP draft for a more-detailed explanation on how we achieve
> these goals.
>
> I'll be in ScalingBitcoin in less than a week and I'll be available to
> discuss the design rationale, improvements, changes and ideas any of you
> may have.
>
> Truly yours,
> Sergio Demian Lerner
> Bitcoiner and RSK co-founder
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Thu, Jun 23, 2016 at 03:58:29PM +0300, Alex Mizrahi wrote:
> >
> > The point I'm making is simply that to be useful, when you close a seal you
> > have to be able to close it over some data, in particular, another seal.
> > That's
> > the key thing that makes the idea a useful construct for smart contacts,
> > value
> > transfer/currency systems, etc.
> >
> 
> OK, your second post ("Closed Seal Sets and Truth Lists for Better Privacy
> and Censorship Resistance") seems to clarify that this data is one of
> arguments to the condition function.
> Frankly this stuff is rather hard to follow. (Or maybe I'm dumb.)
> 
> Now I don't get scability properties. Let's consider a simplest scenario
> where Alice creates some token, sends it to Bob, who sends it to Claire. So
> now Claire needs to get both a proof that Alice sent it to Bob and that Bob
> sent it to Claire, right? So Claire needs to verify 2 proofs, and for a
> chain of N transfers one would need to verify N proofs, right?

Not necessarily. In my writeup I outlined two ways that those chains can be
shortened: trusted validity oracles and the probabalistic, inflationary,
history proof concept.

Equally, even if history grows over time, that's no worse than Bitcoin.

> And how it works in general:
> 
> 1. Alice creates a token. To do that she constructs an unique expression
> which checks her signature and signs a message "This token has such and
> such meaning and its ownership originally associated with seal <hash of the
> expression>" with her PGP key.

Alice isn't _creating_ a tokne, she's _defining_ a token.

> 2. To transfer this token to Bob, she asks Bob for his auth expression and
> sends a seal oracle a message (Alice_expression (Bob_expression .
> signature)) where signatures is constructed in such a way that it evaluates
> as true. Oracle stores this in a map: Alice_expression -> (Bob_expression .
> signatures)

Nope.

In Alice's token definition, the genesis state of the token is defined to be
associated with a specific single-use seal. To transfer the token to Bob, she
asks Bob for the seal he wishes to use, and then closes the genesis seal over a
new state committing to Bob's seal.

Now Alice could construct the seal for Bob, in which case she'd just need to
know the auth expression Bob wants to use, but that's not the most fundamental
way of implementing this.

Regardless, the seal oracle doesn't need to know that any of the above is
happening; all it needs to do is spit out seal closed witnesses when the
authorization expressions are satisfied appropriately; the oracle does not and
should not know what the seals have been closed over. Whether or not the oracle
stores anything when seals are closed is an implementation decision - see my
original writeup on the unbounded vs. bounded oracle case. And of course, seals
implemented with decentralized blockchains are a different matter entirely.

> 3. Bob sends token to Claire in a same way: (Bob_expression
> (Claire_expression . signature))
> 4. Now Claire asks if Alice_expression->(Bob_expression . _) and
> Bob_expression->(Claire_expression . _) are in oracle's map. She might
> trust the oracle to verify signatures, but oracle doesn't understand token
> semantics. Thus she needs to check if these entries were added.
> If I understand correctly, Alice_expression->(Bob_expression . _) record
> can be communicated in just 3 * size_of_hash_digest bytes.
> 
> So this seems to have rather bad scalability even with trusted oracles, am
> I missing something?

Yes, as I mentioned above, there exists multiple techniques that can shorten
history proofs in a variety of ways, depending on what kinds of tradeoffs your
application needs.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Tue, Jun 21, 2016 at 3:13 PM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Mon, Jun 20, 2016 at 05:33:32PM +0000, Erik Aronesty via bitcoin-dev
> wrote:
>
> > - missing an optional client supplied identification
>
> Note that "client supplied identification" is being pushed for AML/KYC
> compliance, e.g. Netki's AML/KYC compliance product:
>
>
> http://www.coindesk.com/blockchain-identity-company-netki-launch-ssl-certificate-blockchain/
>
> This is an extremely undesirable feature to be baking into standards given
> it's
> negative impact on fungibility and privacy; we should not be adopting
> standards
> with AML/KYC support, for much the same reasons that the W3C should not be
> standardizing DRM.
>

Hi Peter,
   Certainly AML/KYC compliance is one of the use cases that BIP 75 and our
certificates can support.  As a quick summary,

There are individuals and entities that would like to buy, sell, and use
bitcoin, and other public blockchains, but that have compliance
requirements that they need to meet before they can do so.  Similarly,
companies and entrepreneurs in the space suffer under the potential threat
of fines, or in extreme cases, jail time, also for not meeting AML or
sanctions list compliance.  We wanted to build tools that allowed
entrepreneurs to breathe easy, while at the same time allow more people and
companies to enter the ecosystem.  We also believe that the solution we are
using has the characteristics that you want in such a solution, for example:

1> Only the counterparties (and possibly their service providers in the
case of hosted services) in a transaction can see the identity data,
protecting user privacy.

2> The counterparties themselves (and possibly their service providers in
the case of hosted services) decide whether identity information is
required for any given transaction.

3> No trace is left on the blockchain or anywhere else (other than with the
counterparties) that identity information was even exchanged, protecting
fungibility

4> The solution is based on open source and open standards, allowing open
permissionless innovation, versus parties building closed networks based on
closed standards.  The very fact that this solution went through the BIP
process and was adapted based on feedback is an example of how this is
better for users than the inevitable closed solution that would arise if
the open source, community vetted version didn’t already exist.

I don’t know if you are opposed to organizations that have AML requirements
from using the bitcoin blockchain, but if you aren’t, why wouldn’t you
prefer an open source, open standards based solution to exclusionary,
proprietary ones?

BIP 70 and BIP 75 are standards for voluntary information exchange between
counterparties in a transaction.  This is exactly the kind of thing we want
standards for, in my experience.


-- 

Justin W. Newton
Founder/CEO
Netki, Inc.

justin@netki.com
+1.818.261.4248

-------------------------------------
Some Bitcoin developers and miners went to visit with Dan Boneh at Stanford
earlier today, and I thought I would share what we talked about.

Transcript:
http://diyhpl.us/wiki/transcripts/2016-july-bitcoin-developers-miners-meeting/dan-boneh/

Topics discussed include elliptic curve crypto, ECDSA, Schnorr signatures,
signature aggregation, BLS signature schemes, pairing crypto, group
signatures, block-level signature aggregation, transaction-level signature
aggregation, post-quantum crypto, quantum mining ASICs, provable solvency
schemes, scrypt password hashing, and balloon hashing.

(I would include the text directly but it's nearly 60 kilobytes in size and
past the point where I am presently comfortable with gunking up other
mailboxes.)

- Bryan
http://heybryan.org/
1 512 203 0507

-------------------------------------
I hadn't thought of that... There is a solution, I think, but it makes the
operation less simple.

If a transaction contains at least two OP_TXHASHVERIFY-protected inputs,
signed without ANYONECANPAY, their signatures would cover the other
input's OP_TXHASHVERIFY hash, right?


            /Rune


On Sat, Sep 17, 2016 at 10:56 PM, Matt Corallo <lf-lists@mattcorallo.com>
wrote:

> (removing the list)
>
> Because the tx hash in your construction is not signed, someone wishing to
> maleate a transaction may do so by also updating the hash in the scriptSig.
>
> Matt
>
> On September 17, 2016 4:45:17 PM EDT, "Rune K. Svendsen via bitcoin-dev" <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> I would really like to be able to create transactions that are immune to
>> transaction ID malleability now, so I have been thinking of the simplest
>> solution possible, in order to get a BIP through without too much trouble.
>>
>> An opcode we could call OP_TXHASHVERIFY could be introduced. It would be
>> defined to work only if added to a scriptSig as the very first operation,
>> and would abort if the hash of the transaction **with all OP_TXHASHVERIFY
>> operations (including stack push) removed** does not match what has been
>> pushed on the stack.
>>
>> So, in order to produce a transaction with one or more inputs protected
>> against tx ID malleability, one would:
>>
>> 1. Calculate tx ID of the tx: TX_HASH
>> 2. For each input you wish to protect, add "0x32 $TX_HASH
>> OP_TXHASHVERIFY" to the beginning of the scriptSig
>>
>> When evaluating OP_TXHASHVERIFY, we make a copy of the tx in question,
>> and remove the "0x32 <32 bytes> OP_TXHASHVERIFY" sequence from the
>> beginning of all scriptSigs (if present), and abort if the tx copy hash
>> does not match the top stack item.
>>
>> This is a very simple solution that only adds 34 bytes per input, and
>> when something better becomes available (eg. Segwit), we will stop using
>> this. But in the meantime it's very valuable to be able to not worry about
>> tx ID malleability.
>>
>> Please let me know what you think.
>>
>>
>>
>>             /Rune
>>
>> ------------------------------
>>
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>

-------------------------------------
On Thu, Feb 25, 2016 at 7:07 PM, Joseph Poon wrote:
> This would be achieved using a SIGHASH flag, termed SIGHASH_NOINPUT. It
> does not include as part of the signature, the outpoint being spent
> (txid and index), nor the amount. It however, would include the spent
> outpoint's script as part of the signature. Note that this is just a

Well if you are bothering to draft up a BIP about that SIGHASH flag,
then perhaps also consider some other SIGHASH flag types as well while
you are at it?

Various proposed sighash types:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010759.html

"Build your own nHashType" proposal draft:
https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md

jl2012's reply:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010779.html

petertodd's reply about OP_CODESEPARATOR linked back to this thread
regarding "Build your own nHashType":
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-April/007771.html
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-April/007802.html
http://gnusha.org/bitcoin-wizards/2014-12-09.log

((That particular thread had other replies which can be viewed here:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-April/thread.html
))

Also, there was a draft implementation of SIGHASH_NOINPUT:
https://github.com/Roasbeef/bitcoin/commit/4b3c3f1baf7985208ceb6887261ee150ab6e3328
https://github.com/Roasbeef/btcd/commit/67830e506fa135d5239177340918cea39909e6a4

FWIW there was some concern about replay using SIGHAHS_NOINPUT or something:
http://gnusha.org/bitcoin-wizards/2015-04-07.log

- Bryan
http://heybryan.org/
1 512 203 0507


-------------------------------------

 >  3. Instead of using a OP_NOPx, I suggest you using an unused code such as 0xba. OP_NOPx should be reserved for some simple "VERIFY"-type codes that does not write to the stack.
 > 
 > Ok. I'm not sure, but if everyone agrees to it, I will. Also Segwit versioning allows to create new opcode multiplexing opcodes, so I was thinking about adding an "opcode index" to a more generic OP_OPERATE. But that prevents using all NOP space, but prevents easily counting OP_ACK_COUNT for checksig block limit.
 
The other reason not to touch NOPx is they are shared by SIGVERSION_BASE and SIGVERSION_WITNESS_V0. If we later decide to introduce new opcodes to legacy versions, we may still use this space.

And yes, I think you should keep OP_ACT_COUNT easily countable for block sigop limit.

 >  
 >  4. I don't think you should simply replace "(witversion == 0)" with "((witversion == 0) || (witversion == 1))". There are only 16 available versions. It'd be exhausted very soon if we use a version for every new opcode. As a testing prototype this is fine, but the actual softfork should not waste a witversion this way. We need a better way to coordinate the use of new witness version. BIP114 suggests an additional field in the witness to indicate the script version (https://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki)
 >  
 > Good. But currently that version is not enforced, so this BIP cannot make use of it. I can use (witversion == 1) but add the BIP114 version field so that the next BIP can make use of it.
 
Probably BIP114 would never be deployed. I don't know. But I think we should try to move the script version to witness, as it is cheaper. The major witness version could be reserved for some fundamental changes in language.

  
 >  6. The coinbase space is limited to 100 bytes and is already overloaded by many different purposes. I think any additional consensus critical message should go to a dummy scriptPubKey like the witness commitment. You may consider to  have a new OP_RETURN output like BIP141, with different magic bytes. However, please don't make this output mandatory (cf. witness commitment output is optional if the block does not have witness tx)
 >  
 >  6a. "..........due to lack of space to include the proper ack tag in a block": this shouldn't happen if you use a OP_RETURN output
 >  
 > I'm not sure about this. The fact that the space for acknowledge and proposal is short has been seen by other developers a benefit and not a drawback. It prevent hundreds of sidechains to be offered, which might hurt solo miners. 70 bytes allows for approximately 10 active polls.

That's 1 active poll per minute on average, which sounds very small if it ever gets really popular. Have you made any forecast? I could foresee people have to bid for the coinbase space for their ack-poll, and they will yell at the devs asking for more poll space (well.....)

We used an OP_RETURN output for segwit as some miners wanted to retain the coinbase space for other purpose like advertisement.  Even if you want to set an artificial limit, you could still use an OP_RETURN output. It just means you will need a OP_COUNT_ACKS2 softfork when you want to expand the space.  Since polls are not fixed size, if an artificial limit is desired, maybe it makes more sense to limit the number of polls, instead of number of bytes.


 >  8. Question: is an ack-poll valid only for 1 transaction? When the transaction is confirmed, could full nodes prune the corresponding ack-poll data? (I think it has to be prunable after spending because ack-poll data is effectively UTXO data)
 >  
 > Yes, there is no ack-poll data stored except for the coinbase field cache, which periodically cleans itself by using a FIFO approach.
 
If the target tx of a ack-poll is never confirmed on the blockchain, I guess you need to keep the data of the poll forever?  It's like creating an unspendable and unprunable UTXO (just want you to clarify. People are spamming the UTXO already so your proposal won't make it worse anyway)
 
 >   9. No matter how you design a softfork, "Zero risk of invalidating a block" couldn't be true for any softfork. For example, even if a miner does not include any txs with OP_COUNT_ACKS, he may still build on top of blocks with invalid OP_COUNT_ACKS operations.
 >  
 > I'm not sure. I assumed that transactions redeeming a script using OP_COUNT_ACKS  would be non-standard, so the the problem you point out would only happen if the block including the transaction would be mined specifically for the purpose to defeat subsequent miners. A honest pre-fork miner would never include a redeemScript that that verifies an OP_COUNT_ACKS, since that transaction would never be received by the p2p protocol (it could if the miner accepts non-standard txs by a different channnel).   
 > 
 > But I must check this in the BIP source code if OP_COUNT_ACKS is really non-standard as I designed it to be.
 
It must be non-standard because witversion != 0 are non-standard already. I mean, you proposal is probably as safe as OP_CSV, but no one sold OP_CSV as "zero risk".

Johnson



-------------------------------------
In the release notes for 0.12, it says that we have moved from using
OpenSSL to libsecp256k1 for signature validation. So what else is it being
used for that we need to keep it as a dependency?

Thanks,
Andrew

-------------------------------------
On Friday, February 26, 2016 1:07:46 AM Joseph Poon via bitcoin-dev wrote:
> This would be achieved using a SIGHASH flag, termed SIGHASH_NOINPUT. It
> does not include as part of the signature, the outpoint being spent
> (txid and index), nor the amount. It however, would include the spent
> outpoint's script as part of the signature. Note that this is just a
> SIGHASH flag, and the outpoints are still being included as part of the
> txins (if they are mutated, the new txids can be updated by the wallet
> without resigning). This allows for a signature to apply to anything
> with that pubkey (therefore pubkeys with this flag should not be
> reused). 

I'd like this regardless of Lightning, as it makes it possible to write fully 
malleability-proof wallet software also.

> For safety, this only applies in SegWit transactions, as segwit
> provides a sufficient malleability solution, there is no incentive to
> improperly use this sighash flag as a roundabout way to resolve
> malleability.

SegWit's malleability solution is not really sufficient in comparison, but I 
don't think there's a need to make this available to pre-SegWit transactions 
anyway (and doing so would probably complicate it).

Luke


-------------------------------------
On Mon, Feb 1, 2016 at 6:53 PM, Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Please provide any
> objections now, so I can try to address them now and enable consensus to be
> reached.

For section "Formally defining consensus",

Where objections were not deemed substantiated by the community, clear
reasoning must be offered.

For section "BIP Comments",

Comments should be solicited on the bitcoin-dev mailing list, and
summarized fairly in the wiki; with notice of summarization and time
for suggesting edits on the mailing list.  Wiki registration and
monitoring should not be a required hurdle to participation.


-------------------------------------
> As I explained, none of those reasons apply to PaymentRequests.

As they exist today PaymentRequests allow for essentially the same types of
transactions as non-PaymentRequest based transactions with the limitation
that OP_RETURN values must be greater. In that sense they're basically a
pre-OP_RETURN environment. OP_RETURN serves a purpose and it can't be used
with PaymentRequest transactions.

> I have no idea what you are trying to say here.

I think if you think through how you would create an OP_RETURN transaction
today without this BIP you'll see you need a key at some point if you want
a zero value.

On Mon, Jan 25, 2016 at 6:56 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Tuesday, January 26, 2016 2:54:16 AM Toby Padilla wrote:
> > Luke - As stated in the Github thread, I totally understand where you're
> > coming from but the fact is people *will* encode data on the blockchain
> > using worse methods. For all of the reasons that OP_RETURN was a good
> idea
> > in the first place, it's a good idea to support it in PaymentRequests.
>
> As I explained, none of those reasons apply to PaymentRequests.
>
> > As for keyless - there's no way (that I know of) to construct a
> transaction
> > with a zero value OP_RETURN in an environment without keys since the
> > Payment Protocol is what defines the method for getting a transaction
> from
> > a server to a wallet. You can make a custom transaction and execute it in
> > the same application but without Payments there's no way to move
> > transactions between two applications. You need to build the transaction
> > where you execute it and thus need a key.
>
> I have no idea what you are trying to say here.
>
> Luke
>

-------------------------------------
Dave Hudson via bitcoin-dev [bitcoin-dev@lists.linuxfoundation.org] wrote:
> I think the biggest question here would be how would the difficulty
> retargeting be changed?  Without seeing the algorithm proposal it's difficult
> to assess the impact that it would have, but my intuition is that this is
> likely to be problematic.

I have no comment on whether this will be *needed* but there's a simple
algorithm that I haven't seen any coin adopt, that I think needs to be: the
critically damped harmonic oscillator:

    http://mathworld.wolfram.com/CriticallyDampedSimpleHarmonicMotion.html

In dynamical systems one does a derivative expansion.  Here we want to find the
first and second derivatives (in time) of the hashrate.  These can be determined
by a method of finite differences, or fancier algorithms which use a quadratic
or quartic polynomial approximation.  Two derivatives are generally all that is
needed, and the resulting dynamical system is a damped harmonic oscillator.  

A damped harmonic oscillator is basically how your car's shock absorbers work.
The relevant differential equation has two parameters: the oscillation frequency
and damping factor.  The maximum oscillation frequency is the block rate.  Any
oscillation faster than the block rate cannot be measured by block times.  The
damping rate is an exponential decay and for critical damping is twice the
oscillation frequency.

So, this is a zero parameter, optimal damping solution for a varying hashrate.
This is inherently a numeric approximation solution to a differential equation,
so questions of approximations for the hashrate enter, but that's all.  Weak
block proposals will be able to get better approximations to the hashrate.

If solving this problem is deemed desirable, I can put some time into this, or
direct others as to how to go about it.

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 



-------------------------------------
After talking to some people about libconsensus in the last Hong Kong
conference I realized that my initial plan of exposing one more thing
at a time would actually probably slow things down.

There's still a promised pdf with pictures that will be released, and
actually drafting the UML pictures helped realize that the whole
explanation could be much simpler if #7091 was merged first as the
last step in phase 1 (that phase has so many contributors that I will
probably never get finished documenting it). Matt Corallo's idea of
exposing VerifyScript() through a C API certainly helped a lot in
cementing the more-minimal-than-earlier dependencies (thanks to Cory
Fields among many other people before him) that are not part of the
incomplete but existing libbitcoinconsensus library.

Given this success in protecting encapsulation by exposing things in a
new library, my instinct was to expose more things: VerifyHeader(),
VerifyTx() and VerifyBlock() [in that order].
But all those three new functions depend on storage in one way or
another. That was part of my reasoning to expose VerifyHeader() first,
because I believe there will be less discussion on a common interface
for the stored longest chain than for the utxo view (which may depend
on other transactions spent within the same block).
In any case, I realized we should finish putting all the consensus
critical code in the libconsensus lib and then worry about its "final"
API.

Therefore I changed the goal of the phase 2 in my libconsensus
encapsulation planning from "expose VerifyHeader() in the existing
libconsensus library" to "build all the consensus critical code within
the existing libconsensus library, even if we don't expose anything
else". I believe this is much feasible for a single Bitcoin Core
release cycle and also more of a priority. Other implementations
experimenting with libconsensus like
https://github.com/libbitcoin/libbitcoin-consensus will have the
chance to compare their reimplementations with the future complete
libbitcoinconsensus without having to worry about the C API, which
ideally they will help to define.

I repeat, the goal of phase 2 in my upcoming libconsensus
encapsulation plan is to fully decouple libconsensus from Bitcoin
Core.
In phase 3, we can refine the storage interfaces and focus on a
quasi-final C API.
In phase 4, we can refine and take out code that doesn't belong in
libconsensus like CTxOut::GetDustThreshold() in
primitives/transaction.h and move all those consensus files to the
consensus folder before creating a separate sub-repository like for
libsecp256k1. Note that most of the file-moving work can be in
parallel to phases 2 and 3 and, in fact, by any new developer that is
willing to exchange rebase-patience for meaningless github stats (I'll
do it if nobody else wants, but I'm more than happy to delegate there:
I have more than enough github meaningless stats already).

As said, the document with pictures and the update to #6714 are still
promised, but until they're ready, merging/reviewing #7091, #7287,
#7310 and #7311 could do a great deal to make later steps in
libconsensus phase 2 more readable.
Most reviewers probably don't need to see any "big picture" to tell
whether certain functions on Bitcoin Core are consensus-critical or
not, or whether consensus critical code needs to depend on util.o or
not.
But I wouldn't be writing to the mailing list without a plan with
further words nor pictures if I didn't had what I believe is a
complete implementation of what I just defined as "libconsensus phase
2".

Phase 3 should finish long pending discussions like "should
libconsensus be C++14 or plain C" which should NOT delay phase 2.
Phase 4 should be mostly trivial: rename files to the target dir and
move the remaining unused code out of libconsensus.
Phase 5 should make Bitcoin Core eat its own dog food and use
libbitcoinconsensus oonly by its generic C API (I'm sorry if this
looks too far away for me to even think about detailing it).

The work in progress branch (but hopefully being finished, nit and
merged within the 0.12.99 cycle) can be found in:
https://github.com/jtimon/bitcoin/commits/libconsensus-f2

Before sipa asks, signing code may make it into a new library but
SHOULDN'T BE PART OF LIBBITCOINCONSENSUS. Ideally, all exposed
functions will return true or false and an error string. It is based
on last-0.12.99 3cd836c1 but by popular demand I can open it as a
"DEPENDENT-tagged" PR linking to smaller steps and keeping track of
steps done. Analogous to the about to be replaced (for a simpler and
more maintainable example of testchain) #6382. If people like
Wladimir, Cory and Pieter cannot see that I've been able to reduce my
overall cry-for-review noise thanks to github adoption of emacs'
org-mode's [ ] vs [X] I can alwways leave those "big picture" branches
as "private" branches out of the pull request count.

I expect to publish a phase 3 branch very shortly. But as said I
expect a lot of discussion on the API part, so I don't expect big
movements in phase 3 until phase 2 is done (as said phase 4 is
orthogonal to anything, this time git will say "verified MOVEONLY" for
us).

To finish this long mail, if you are new to free software and would
like to get familiarized with Bitcoin Core development in particular,
moving one file is a simple task that you can always besure you can do
right.
The way I plan to hand this to you, you won't need to convince anyone
to publicly confirm that your "MOVEONLY" commit being legit, because
all your remaining work will be to build on one platform (ideally you
should do a gitian build, but embarrassingly enough for someone
touching consensus code I just trust travis ) and trust travis (as
said, that's what I do from my laptop, but I plan to buy my own
building machine [and maybe outsource it for free in some protocol
that hasn't been invented, sorry again for the distraction]) and fix
the includes that have stopped working.

I intend to create an issue to move all the files in this list one by one:

https://github.com/bitcoin/bitcoin/pull/7091/files#diff-480477e89f9b6ddafb30c4383dcdd705R250

But don't hesitate to contact me if are eager for moving some files,
because I believe we can save a few lines of total diff if we chose
the order of the movements properly.

Sorry, I forgot many people read this list again.
Happy to answer any question.

Specially about https://github.com/jtimon/bitcoin/commits/libconsensus-f2


-------------------------------------
On Jun 28, 2016, at 10:06 PM, Jonas Schnelli <dev@jonasschnelli.ch> wrote:

>>> In my opinion, the question should be "why would you _not_ encrypt".
>> 
>> 1) creation of a false sense of security
> 
> False sense of security is mostly a communication issue.
> BIP151 does focus on encryption (not trust).
> 
> Are users aware of the fact that ISP/WiFi-Providers can track their
> bitcoin spending (if they use SPV+BF) and link it with other internet
> traffic or sell the data to anyone who is interested to do correlation?

The relevant question would be to ask whether encryption would prevent an ISP from doing so (which it would not). This is a good example of false sense of security.

> Are node operators aware of the possibilities that ISPs/Data-Centers,
> etc. can hold back peers, etc.?
> 
> If there is a false sense of security/anonymity, then we are already
> deep into this territory.
> BIP151 was designed as a puzzle-pice towards better security and better
> censorship resistance. You shouldn't project all sorts of "false sense
> of security" into BIP151. Is a stepping stone towards greater security.

FWIW I was just answering your question comprehensively. Relationship to BIP151 is incidental (though apparently applicable).

Keep in mind my specific concern is not with the design of BIP151, it is with the implication of its dependency on an unspecified authentication proposal.

>> 2) as a tradeoff against anonymity
> 
> Can you point out the tradeoffs?
> BIP151 does not introduce fingerprinting possibilities.

The security tradeoff would arise from widespread deployment of authentication - which is necessary to make encryption useful against envisioned MITM attacks. See my previous discussion of trust zones below.

>> 3) benefit does not justify cost
> 
> Can you elaborate the costs?
> [Extremely simplified]: we need 300 lines of code from openssh
> (ChaCha20-Poly1305@openssl) and some ECDH magic (already in
> Bitcoin-Cores codebase) together with two or three (maybe payed)
> cryptoanalysis once the implementation is done.

Simply put, any code that is unnecessary does not justify its cost.

>>> There are plenty of other options to solve this problem. stunnel,
>>> Bernsteins CurveCP, VPN, etc. which are available since years.
>>> But the reality has shown that most bitcoin traffic is still unencrypted.
>> 
>> The question arises from concern over the security of the network in the case where encryption (and therefore authentication) is pervasive.
>> 
>> As you point out, anyone can set up a private network of nodes today. These nodes must also connect to the permissionless network to maintain the chain. These nodes constitute a trust zone within Bitcoin. This zone of exclusion operates as a single logical node from the perspective of the Bitcoin security model (one entity controls the validation rules for all nodes).
>> 
>> Widespread application of this model is potentially problematic. It is a non-trivial problem to design a distributed system that requires authentication but without identity and without central control. In fact this may be more challenging than Bitcoin itself. Trust on first use (TOFU) does not solve this problem.
> 
> Yes. There is no plan to adopt a TUFO scheme. Bip151 does not use TUFO
> it does not cover "trust" (It just encrypt all traffic).

TOFU (trust on first use) was a reference to what was discussed on IRC as a potential solution to the (deferred) authentication problem. I didn't mean to imply that it was part of BIP151.

> Imaging Bip151 together with a simple form of preshared EC key
> authentication (nonce signing or similar). You could drastically
> increase the security/censor-resistance-properties between nodes where
> owners have preshared identity keys (with nodes I also mean SPV/wallet
> nodes).

This is a restatement of what I have accepted as a premise - that authentication, and as such, key distribution, will be a necessary part of making any encryption scheme effective. "Preshared" implies a secure side channel for key distribution.

> And I guess there are plenty of awesome identity management system ideas
> tied or not tied to the Bitcoin blockchain out there.
> This is also a reason to not cover trust/authentication/identity in BIP151.
> It is  possible to have multiple authentication schemes.

Whether or not there are multiple schemes is not relevant to the point I have raised. The issue is that authentication is necessary.

>> In my opinion this question has not received sufficient consideration to warrant proceeding with a network encryption scheme (which concerns me as well, but as I consider it premature I won't comment).
> 
> Yes. I think nobody have started implementing BIP151. It's a draft BIP
> and I think it's still okay and great that we have this discussion.
> 
> BIP151 hopefully has started some brainwork in how encryption and
> authentication could work in Bitcoin and I'm happy to deprecate BIP151 if we have found a better solution or if we come to a point where we agree that BIP151 does make the network security worse.

We should contemplate what the distributed permissionless network of anonymous peers looks like once every node authenticates every one of its peers using one or more key distribution side channels.

>>> Example: IIRC non of the available SPV wallets can "speak" on of the
>>> possible encryption techniques. Encrypting traffic below the application
>>> layer is extremely hard to set up for non-experienced users.
>> 
>> Bloom filters can (and IMO should) be isolated from the P2P protocol. Also, if the proposal creates an insecurity its ease of deployment is moot.
> 
> If we assume increasing amount of novice users starting with Bitcoin every day, how should these users run wallets without increasing centralization by using webwallets or client/central-server wallets?
> (which is OT, but an interesting question)

I fully appreciate the significant security risk arising from the proliferation of web wallets. This can only be resolved by people validating using code under their own control.

Encryption/authentication are orthogonal to this question, assuming people have wallets directly attached to full nodes. Remoting a wallet from a full node does not require use of the P2P protocol, and can use encryption/authentication without the concerns I've raised. It properly places the trust boundary around a wallet and its trusted node(s), as opposed to spanning (independent) nodes.

>>> On top of that, encryption allows us to drop the SHA256 checksum per p2p
>>> message which should result in a better performance on the network layer
>>> once BIP151 is deployed.
>> 
>> I would not consider this a performance enhancing proposal. Simply dropping the checksum seems like a better option. But again, it is moot if it creates an insecurity.
>> 
>>> I agree that BIP151 _must_ be deployed together with an authentication
>>> scheme (I'm working on that) to protect again MITM during encryption
>>> initialization.
>> 
>> At a minimum I would propose that you modify BIP151 to declare a dependency on a future BIP, making BIP151 incomplete without it. I think we can agree that it would be unadvisable to deploy (and therefore to implement) encryption alone.
> 
> I think BIP151 does what it says: encryption and laying groundwork for authentication.
> You wouldn't probably say BIP32 is incomplete because it does not cover
> a scheme how to recover funds (or BIP141 [SW consensus] is incomplete
> because it does not cover p2p [BIP144]).

This is an unfair statement. You have acknowledged that BIP151 requires authentication to accomplish its sole objective.

> The missing MITM protection (solvable over auth) is prominent mentioned in the BIP [1].

As I pointed out.

> (from your other mail):
>>> I don't see reasons why BIP151 could weaken the security of the P2P network. Can you point out some specific concerns?
>> 
>> TOFU cannot prevent MITM attacks (the goal of the encryption). Authentication requires a secure (trusted) side channel by which to distribute public keys. This presents what I consider a significant problem. If widespread, control over this distribution network would constitute control over who can use Bitcoin.
>> The effort to prevent censorship could actually enable it. I don't think it would get that far. Someone would point this out in the process of vetting the authentication BIP, and the result would be the scrapping of BIP151.
> 
> I agree that the secure trusted 2nd channel key-sharing problem can be significant for large networks and/or connecting to unknown identities.
> 
> But as said, there could be multiple ways of sharing identity keys.
> If you want to connect your node to serval other trusted nodes, you can simply physically preshare keys or do it over GPG / Signal App, etc..

Again, it's the fact that authentication is required that produces the issue, not that there are multiple ways to implement it.

> And if I have followed the news correctly, there are some clever guys
> working on various internet of trust 2.0 proposals...

I don't see how this is relevant.

>>> BIP151 does not rely on identities. BIP151 does not use persisted keys
>>> (only ephemeral keys).
>> 
>> BIP 151 is incomplete without authentication.
> 
> I would agree if you would say, _trusted encryption_ is incomplete with
> authentication. But IMO BIP151 is complete and should be deployed together with one or multiple authentication schemes.

It seems that we are talking past each other. You haven't yet addressed the issue that I have raised.

It is the requirement for authentication of any node that any other node may wish to connect to that is the issue. We end up with something that looks like WoT or PKI. And if not fully controlled by PKI (so using WoT) we will have hybrid nodes that accept untrusted connections and propagate information between trusted and untrusted nodes.

> [1] https://github.com/bitcoin/bips/blob/master/bip-0151.mediawiki#risks
> 


-------------------------------------
I agree that it seems like a safe assumption that adoption would be faster,
whether it is "very safe" and "significantly faster", whether it will be 6
times faster, all of those assumptions seems significantly less safe and
robust to me.

The nature of the bitcoin protocol, that it is a decentralized census based
protocol involving currency, suggests to me that roll out schedules ought
to be conservative with a minimum of assumptions. In light of the most
recent protocol upgrade, 6 months for this hard fork seems to me to be the
most conservative time frame with the fewest assumptions.

As for why it needs to be so fast, ie what are the dangers of it being as
slow as 6 months?

Gavin writes:

"I strongly disagree with the statement that there is no cost to a longer
grace period. There is broad agreement that a capacity increase is needed
NOW."

~~
"Broad agreement", that really seems to be another assumption, the fact
that the debate has been as long and acrimonious as it has been suggests
that there isn't broad agreement. Also, resorting to "SHOUTING" doesn't win
any favors when it comes to engaging in reasonable discussion om the
technical merits of a proposal.



On Sun, Feb 7, 2016 at 5:04 PM, Corey Haddad <corey3@gmail.com> wrote:

> We don't have any evidence of how fast nodes will upgrade when faced with
> an impending hard fork, but it seems like a very safe assumption that the
> upgrade pace will be significantly faster.  The hard fork case it is:
> "upgrade or be kicked off the network".  In the previous cases it has been,
> "here's the latest and greatest, give it a go!".  Also, there will be
> alerts sent out warning people of the situation, prompting them to take
> action.
>
> It is unclear if this will translate into more or less than 6x the
> adoption speed of previous instances, but the idea that it would be faster
> is solid.  28 days is aggressive, but again, it is only 28 days from when
> the fork triggers.  Compatible software is already available for anyone who
> wants to prepare.
>
> It is also of significance that this proposed fork, and this debate, has
> been going on for many, many months.  If someone proposed a forking concept
> today, wrote the BIP tomorrow, deployed it next week, miners adopted it
> instantly, and 28 days later it was the flag day, those 28 days would be in
> a different context.  There is no surprise here.
>
> On Sun, Feb 7, 2016 at 1:33 PM, Steven Pine via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Is it me or did Gavin ignore Yifu's direct questions? In case you missed
>> it Gavin --
>>
>> ~
>> "We can look at the adoption of the last major Bitcoin core release to
>> guess how long it might take people to upgrade. 0.11.0 was released on 12
>> July, 2015. Twenty eight days later, about 38% of full nodes were
>> running that release. Three months later, about 50% of the network was
>> running that release, and six months later about 66% of the network was
>> running some flavor of 0.11."
>>
>> On what grounds do you think it is reasonable to assume that this update
>> will roll out 6x faster than previous data suggested, as oppose to your own
>> observation of 66% adoption in 6 month. or do you believe 38% node
>> upgrade-coverage (in 28 days ) on the network for a hard fork is good
>> enough?
>>
>> There are no harm in choosing a longer grace period but picking one short
>> as 28 days you risk on alienating the nodes who do not upgrade with the
>> aggressive upgrade timeline you proposed.
>> ~~
>>
>> When Gavin writes "Responding to "28 days is not long enough" :
>>
>> I keep seeing this claim made with no evidence to back it up.  As I said,
>> I surveyed several of the biggest infrastructure providers and the btcd
>> lead developer and they all agree "28 days is plenty of time."
>>
>> For individuals... why would it take somebody longer than 28 days to
>> either download and restart their bitcoind, or to patch and then re-run
>> (the patch can be a one-line change MAX_BLOCK_SIZE from 1000000 to
>> 2000000)?"
>>
>> ~~
>>
>> Isn't Yifu's comment, evidence, the very best sort of evidence, it isn't
>> propositional a priori logic, but empirical evidence that. As for why
>> people take longer, who knows, we simply know from passed experience that
>> it in fact does take longer.
>>
>> It's extremely frustrating to read Gavin's comments, it's hard to believe
>> he is engaging in earnest discussion.
>>
>> On Sun, Feb 7, 2016 at 4:01 PM, Luke Dashjr via bitcoin-dev <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>>> On Sunday, February 07, 2016 2:16:02 PM Gavin Andresen wrote:
>>> > On Sat, Feb 6, 2016 at 3:46 PM, Luke Dashjr via bitcoin-dev <
>>> > bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> > > On Saturday, February 06, 2016 5:25:21 PM Tom Zander via bitcoin-dev
>>> wrote:
>>> > > > If you have a node that is "old" your node will stop getting new
>>> > > > blocks. The node will essentially just say "x-hours behind" with
>>> "x"
>>> > > > getting larger every hour. Funds don't get confirmed. etc.
>>> > >
>>> > > Until someone decides to attack you. Then you'll get 6, 10, maybe
>>> more
>>> > > blocks confirming a large 10000 BTC payment. If you're just a normal
>>> end
>>> > > user (or perhaps an automated system), you'll figure that payment is
>>> good
>>> > > and irreversibly hand over the title to the house.
>>> >
>>> > There will be approximately zero percentage of hash power left on the
>>> > weaker branch of the fork, based on past soft-fork adoption by miners
>>> (they
>>> > upgrade VERY quickly from 75% to over 95%).
>>>
>>> I'm assuming there are literally ZERO miners left on the weaker branch.
>>> The attacker in this scenario simply rents hashing for a few days in
>>> advance
>>> to build his fake chain, then broadcasts the blocks to the unsuspecting
>>> merchant at ~10 block intervals so it looks like everything is working
>>> normal
>>> again. There are lots of mining rental services out there, and miners
>>> quite
>>> often do not care to avoid selling hashrate to the highest bidder
>>> regardless
>>> of what they're mining. 10 blocks worth costs a little more than 250 BTC
>>> -
>>> soon, that will be 125 BTC.
>>>
>>> Luke
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
>>
>>
>> --
>> Steven Pine
>> (510) 517-7075
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>


-- 
Steven Pine
(510) 517-7075

-------------------------------------
Would it be feasible to transmit an entire BIP21 URI as audio? If you were
to encode any extra information (such as amount), it would be useful to
include a checksum for the entire message. This checksum could possibly be
used instead of the checksum in the address.

Trevin

On Aug 8, 2016 3:06 PM, "Justin Newton via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Daniel,
>    Thanks for proposing this.  I think this could have some useful use
> cases as you state.  I was wondering what you would think to adding some
> additional tones to optionally denote an amount (in satoshis?).
>
> (FYI, actual link is here:  https://github.com/Dako300/BIP )
>
> Justin
>
> On Mon, Aug 8, 2016 at 2:22 PM, Daniel Hoffman via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> This is my BIP idea: a fast, robust, and standardized for representing
>> Bitcoin addresses over audio. It takes the binary representation of the
>> Bitcoin address (little endian), chops that up into 4 or 2 bit chunks
>> (depending on type, 2 bit only for low quality audio like american
>> telephone lines), and generates a tone based upon that value. This started
>> because I wanted an easy way to donate to podcasts that I listen to, and
>> having a Shazam-esque app (or a media player with this capability) that
>> gives me an address automatically would be wonderful for both the consumer
>> and producer. Comes with error correction built into the protocol
>>
>> You can see the full specification of the BIP on my GitHub page (
>> https://github.com/Dako300/BIP-0153).
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
>
> --
>
> Justin W. Newton
> Founder/CEO
> Netki, Inc.
>
> justin@netki.com
> +1.818.261.4248
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Thanks for your email Peter!

On Tuesday 20 Sep 2016 17:56:44 Peter Todd wrote:
> On Tue, Sep 20, 2016 at 07:15:45PM +0200, Tom via bitcoin-dev wrote:
> > === Serialization order===
> > 
> > The tokens defined above have to be serialized in a certain order for the
> > transaction to be well-formatted.  Not serializing transactions in the
> > order specified would allow multiple interpretations of the data which
> > can't be allowed.
> 
> If the order of the tokens is fixed, the tokens themselves are redundant
> information when tokens are required; when tokens may be omitted, a simple
> "Some/None" flag to mark whether or not the optional data has been omitted
> is appropriate.

This is addressed in the spec; 
https://github.com/bitcoinclassic/documentation/blob/master/spec/transactionv4.md

The way towards that flexibility is to use a generic concept made popular
various decades ago with the XML format. The idea is that we give each
field a name and this means that new fields can be added or optional fields
can be omitted from individual transactions


> Also, if you're going to break compatibility with all existing software, it
> makes sense to use a format that extends the merkle tree down into the
> transaction inputs and outputs.

Please argue your case.
-------------------------------------


On 02/09/16 22:10, Luke Dashjr wrote:
> On Tuesday, February 09, 2016 10:00:44 PM Matt Corallo via bitcoin-dev wrote:
>> Indeed, we could push for more place by just always having one 0-byte,
>> but I'm not sure the added complexity helps anything? ASICs can never be
>> designed which use more extra-nonce-space than what they can reasonably
>> assume will always be available, so we might as well just set the
>> maximum number of bytes and let ASIC designers know exactly what they
>> have available. Currently blocks start with at least 8 0-bytes. We could
>> just say minimum difficulty is now 6 0-bytes (2**16x harder) and reserve
>> those?
> 
> The extranonce rolling doesn't necessarily need to happen in the ASIC itself. 
> With the current extranonce-in-gentx, an old RasPi 1 can only handle creating 
> work for up to 5 Gh/s with a 500k gentx.


Did you read the footnote on my original email? There is some potential
for optimization if you can brute-force the midstate, which today
requires using the nVersion space as nonce. In order to fix this we need
to add nonce space in the first compression function, so this is an
ideal place. Even ignoring that reducing complexity of mining control
stuff is really nice. If we could go back to just providing block
headers to miners instead of having to provide the entire
transaction-hash-list we could move a ton of complexity back into
Bitcoin Core from mining setups, which have historically been pretty
poorly-reviewed codebases.


> Furthermore, there is a direct correlation between ASIC speeds and difficulty, 
> so increasing the extranonce space dynamically makes a lot of sense.
> 
> I don't see any reason *not* to increase the minimum difficulty at the same 
> time, though.

Meh, my point was less that its a really bad idea and more that it adds
compexity that I dont see much need for.


-------------------------------------
>They both require authentication,

Yeah, but not the same *sort* of authentication. As a trivial example,
you could have ten servers that sign long-term keys for nodes. All
that they need to check is that the node requesting a signature owns
the corresponding IP address. On the other hand, 'evil nodes' is a
subjective quality that is hard to assess automatically.

>and if you intend to connect to potentially evil nodes you aren't securing anything

Bitcoin is designed with the assumption that some of the nodes you
connect to might be evil. Sure, if 100% of the nodes you're connected
to are evil, you're screwed. However, we shouldn't avoid protecting
people from someone on the same coffee-shop network, just because the
same mitigation won't work against a nation-state.

On Tue, Jun 28, 2016 at 5:29 PM, Eric Voskuil via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Your description of the two scenarios reduces to one. They both require
> authentication, and if you intend to connect to potentially evil nodes you
> aren't securing anything with link level security except the knowledge that
> your potentially evil node connection remains so.
>
> e
>
> On Jun 29, 2016, at 12:33 AM, Cameron Garnham <da2ce7@gmail.com> wrote:
>
>
> There are two different topics mixed up here.
>
> 1. Link-level security (secure connection to the node we intended to connect
> to).
>
> 2. Node-level security (aka; don't connect to a 'evil node').
>
> The fist requires link-level encryption and authentication.
>
> The second requires identity authentication.
>
> You described the 'evil node' attack; that indeed needs an identity system
> to stop. However BIP151 doesn't intend to protect against connecting to evil
> Bitcoin Nodes.
>
> It is important not to mixup link-level authentication and node-level
> authentication.
>
> When your client picks random nodes to connect to, you are not considered
> whom in particular runs them. (Rather that you have a good random sample of
> the network).
>
> If you manually add a friends node; at this point you wish to have
> node-level authentication.  However, this may (and probably should) happen
> out-of-band.
>
>
> Sent from my iPhone
>
> On 29 Jun 2016, at 01:07, Eric Voskuil <eric@voskuil.org> wrote:
>
> Hi Cameron, good to hear from you!
>
> On Jun 28, 2016, at 11:40 PM, Cameron Garnham <da2ce7@gmail.com> wrote:
>
> Unauthenticated link level encryption is wonderful! MITM attacks are
> overrated; as they require an active attacker.
>
>
> This is not really the case with Bitcoin. A MITM attack does not require
> that the attacker find a way to inject traffic into the communication
> between nodes. Peers will connect to the attacker directly, or accept
> connections directly from it. Such attacks can be easier than even passive
> attacks.
>
> Stopping passive attacks is the low hanging fruit. This should be taken
> first.
>
> Automated and secure peer authentication in a mesh network is a huge topic.
> One of the unsolved problems in computer science.
>
> A simple 'who is that' by asking for the fingerprint of your peers from your
> other peers is a very simple way to get 'some' authentication.  Semi-trusted
> index nodes also is a low hanging fruit for authentication.
>
>
> It is the implication of widespread authentication that is at issue. Clearly
> there are ways to implement it using a secure side channels.
>
> However, let's first get unauthenticated encryption. Force the attackers to
> use active attacks. (That are thousands times more costly to couduct).
>
> Sent from my iPhone
>
> On 29 Jun 2016, at 00:36, Gregory Maxwell via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> On Tue, Jun 28, 2016 at 9:22 PM, Eric Voskuil via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> An "out of band key check" is not part of BIP151.
>
>
> It has a session ID for this purpose.
>
> It requires a secure channel and is authentication. So BIP151 doesn't
> provide the tools to detect an attack, that requires authentication. A
> general requirement for authentication is the issue I have raised.
>
>
> One might wonder how you ever use a Bitcoin address, or even why we
> might guess these emails from "you" aren't actually coming from the
> NSA.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
Sorry for hijacking the thread again

> As I understand it, you can scan sequentially starting with the genesis
> block (or with a block at around the time when BIP44 was written).  Then
> if you find a new transaction, which requires to generate new addresses,
> you generate them and scan further from that point on.  This way you can
> scan in a single pass if the scanning process calls you back when it
> finds a transaction and allows you to change the set of addresses on the
> fly.

(I think this case if not completely unrealistic):

What would happen, if a user gave out 21 addresses, then address0 had
receive funds in +180 days after generation where address21 had receive
funds immediately (all other addresses never received a tx).

In a scan, address0 would be detected at <address-birthday>+180 days
which would trigger the resize+20 of the address-lookup-window, but, we
would require to go back 180day in order to detect received transaction
of address21 (new lookup-window) in that case.

Or do I misunderstand something?


</jonas>


-------------------------------------
I wonder if this is possible as a soft fork without using segwit?
Increasing the sigop count for a NOP would be a hard fork, but such a
change would be fine with a new segwit version. It might require specific
support in the altcoin, which might be troublesome..
On 11 Feb 2016 20:05, "Tier Nolan via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> There was some discussion on the bitcointalk forums about using CLTV for
> cross chain transfers.
>
> Many altcoins don't support CLTV, so transfers to those coins cannot be
> made secure.
>
> I created a protocol.  It uses on cut and choose to allow commitments to
> publish private keys, but it is clunky and not entirely secure.
>
> I created a BIP draft for an opcode which would allow outputs to be locked
> unless a private key was published that matches a given public key.
>
> https://github.com/TierNolan/bips/blob/cpkv/bip-cprkv.mediawiki
> <https://www.avast.com/sig-email> This email has been sent from a
> virus-free computer protected by Avast.
> www.avast.com <https://www.avast.com/sig-email>
> <#-1229186329_DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
This BIP defines a change in consensus rules regarding to block nVersion, and define a concept of generalized block header to implement a hardfork warning system for full nodes and light nodes.

For better formatting, visit github
https://github.com/jl2012/bips/blob/hfwarning/bip-hfwarning.mediawiki



BIP: ?
Title: Hardfork warning system
Author: Johnson Lau <jl2012@xbt.hk>
Status: Draft
Type: Standard
Created: 2016-12-01

Abstract

This BIP defines a change in consensus rules regarding to block nVersion, and define a concept of generalized block header to implement a hardfork warning system for full nodes and light nodes.

Motivation

Softfork and hardfork are the 2 majors categories of consensus rules change. Generally, softforks make some previously valid blocks invalid, while hardforks make some previously invalid blocks valid. Bitcoin has successfully introduced a number of new functions through softforks. A built-in warning system is also available in many implementations to warn users for the activation of any unknown softforks.

Some features, however, may not be easily introduced with a softfork. Examples include expanding maximum block resources limits, and changing the average block time interval. When such features are implemented with a hardfork, existing full node implementations would consider such blocks as invalid, and may even ban a peer for relaying such blocks. They are effectively blind to such hardfork rule changes, leaving users to unknowingly transact on a system with potentially different token value. On the other hand, light nodes may blindly follow a hardfork with unknown rule changes and lose the right to choose the previous system.

This BIP defines a change in consensus rules regarding to block nVersion, and define a concept of generalized block header to implement a hardfork warning system for full nodes and light nodes.

Definitions

Valid block
A block that satisfies all the consensus rules being enforced by a bitcoin protocol implementation. An implementation may intentionally (e.g. a light node) or unintentionally (e.g. unaware of a softfork) not enforcing any part of the current netwrok rules.

Valid blockchain
A blockchain constituting of only valid blocks.

Best valid blockchain
The valid blockchain with highest total proof-of-work.

Valid blockchain fork
A valid blockchain sharing a common ancestral block with the best valid blockchain, but with less total proof-of-work

Generalized block header
Any serialized hexadecimal data with exactly 80 bytes (byte 0 to byte 79). The bytes 4 to 35 are the double-SHA256 hash of another generalized block header. The bytes 72 to 75 are nBits, the target of this generalized block header encoded in the same way as normal bitcoin block header. The 2 most significant bits of the byte 3 are the hardfork notification bits. The semantics of other data in a generalized block header is not defined in any general way. It should be noted that a normal bitcoin block header is a special case of generalized block header.

Generalized block header chain
A chain of generalized block header. A header chain of valid blocks is a special case of a generalized block header chain.


Specifications


Block nVersion softfork

A softfork is deployed to restrict the valid value of block nVersion. Upon activation, any block with the second highest nVersion bit set becomes invalid (nVersion & 0x40000000)

This softfork will be deployed by "version bits" BIP9 with the name "hfbit" and using bit 2.

For Bitcoin mainnet, the BIP9 starttime will be midnight TBC UTC (Epoch timestamp TBC) and BIP9 timeout will be midnight TBC UTC (Epoch timestamp TBC).

For Bitcoin testnet, the BIP9 starttime will be midnight TBC UTC (Epoch timestamp TBC) and BIP9 timeout will be midnight TBC UTC (Epoch timestamp TBC).

Any bitcoin implementation (full nodes and light nodes) supporting this softfork should also implement a hardfork warning system described below.


Validation of generalized block header

A bitcoin protocol implementation should consider a generalized block header as valid if it satisfies all of the following criteria:

	• It is a descendant of the header of a valid block in a valid blockchain (the best valid blockchain or a valid blockchain fork).
	• It satisfies the proof-of-work requirement: its double-SHA256 value MUST be smaller than its target (encoded as nBits).
	• Its target MUST NOT be greater than the target of its last ancestral valid block by more than 1024 times. An implementation may decide to use a different threshold (or dynamic threshold), depending on its tolerance against potential DoS attacks by generating many low difficulty headers. However, if the value is set too low, a hardfork with lower difficulty may not be detected.[1]
In general, a bitcoin protocol implementation should keep an index of all known generalized block header chains, along with the valid blockchain(s). However, if a generalized block header chain is grown on top of a very old valid block, with total proof-of-work much lower than the current best valid bloackchain, it may be safely discarded.


Hardfork warning system in full nodes

Hardfork with unknown rules
If a generalized block header chain with non-trivial total proof-of-work is emerging, and is not considered as a valid blockchain, a hardfork with unknown rules may be happening.

A wallet implementation should issue a warning to its users and stop processing incoming and outgoing transactions, until further instructions are given. It should not attempt to conduct transactions on or otherwise interpreting any block data of the hardfork with unknown rules.

A mining implementation should issue a warning to its operator. Until further instructions are given, it may either stop mining, or ignore the hardfork with unknown rules. It should not attempt to confirm a generalized block header with unknown rules.

Setting of one or both hardfork notification bits is, as defined by BIP34 and this BIP, a hardfork, and should be considered as an indication of a planned hardfork. If a hardfork with unknown rules is happening without any hardfork notification bits set, it is probably an accidental consensus failure, such as the March 2013 fork due to a block database bug (BIP50), and the July 2015 fork following the BIP66 activation.[2]


Hardfork with multiple valid blockchains
If a valid blockchain fork is emerging with non-trivial total proof-of-work, a consensus disagreement may be happening among different miners.

A wallet implementation should issue a warning to its users and stop processing incoming and outgoing transactions, until further instructions are given.

A mining implementation should issue a warning to its operator. Until further instructions are given, it may either stop mining, or mine on top of the best valid chain by its own standard.

Hardfork warning system in light nodes

Light node (usually wallet implementations) is any bitcoin protocol implementations that intentionally not fully enforcing the network rules. As an important part of the hardfork warning system, a light node should observe the hardfork notification bits in block header, along with any other rules it opts to validate. If any of the hardfork notification bits is set, it should issue a warning to its users and stop processing incoming and outgoing transactions, until further instructions are given. It should not attempt to conduct transactions on or otherwise interpreting any block data of the hardfork blockchain, even if it might be able to decode the block data.


Applications

Hardfork notification bits
There are 2 hardfork notification bits defined in this BIP. The higher bit has been forbidden since BIP34, and the lower bit is disabled by this BIP. For nodes supporting this BIP, the semantics of the 2 bits are the same: a hardfork is happening. For legacy node, however, setting the higher bit would make them fail to follow the hardforking chain. In a soft-hardfork design (described below), the lower notification bit should be used.
The hardfork warning system is able to detect the following types of hardforks:

Soft-hardfork (with the lower hardfork notification bit)
A soft-hardfork is a technique to implement a hardfork by pretending to create blocks with only a zero output value coinbase transaction, and commit the real transaction Merkle root in the coinbase scriptSig field. With the lower hardfork notification bit set, a node following this BIP will consider this as a hardfork and enter the safe mode, while a legacy node not following this BIP will be effectively broken due to seeing the continuously empty blockchain.

Redefining the nTime field
As the warning system does not interpret the nTime field, redefining it through a hardfork would be detectable. For example, overflow may be allowed to overcome the year 2106 problem.

Redefining the Merkle root hash field and changing block content validation rules
The 32-byte Merkle root hash could be redefined, for example, with a different hashing algorithm. Any block resources limitation and transaction validation rules may also be changed. All such hardforks would be detected by the warning system.

Changing average block interval or difficulty reset
Since the warning system is not bound to a particular proof-of-work target adjustment schedule, a hardfork changing the average block interval or resetting the difficulty will be detectable.

Introducing secondary proof-of-work
Introducing secondary proof-of-work (with non-SHA256 algorithm or fixing the block withholding attack against mining pools) may be detectable, as long as the generalized block header format is preserved.

Accidental hardfork
An accidental hardfork may be detectable, if the generalized block headers in both forks are valid but no hardfork notification bit is set.


Limitations

The only function of this system is to inform the users that a hardfork might be happening and prompt for further instructions. It does not guarantee that the hardfork will be successful and not end up with two permanent incompatible forks. This requires broad consensus of the whole community and is not solvable with technical means alone.

The following types of hardfork are not detectable with this warning system:

	• Changing of proof-of-work algorithm
	• Changing the encoding of previous block header hash or nBits
	• A coercive soft-hardfork without setting any hardfork notification bit


Backward compatibility

The softfork described in the BIP would only affect miners. As the disabled nVersion bit is never used in the main network, it is unlikely that any miner would unintentionally find an invalid block due to the new rules.

BIP9 is disabled when any of the hardfork notification bits is set, which may interrupt any ongoing softfork support signalling process. Developers should pay attention to this when desinging a hardfork. For example, they may redefine the counting of signal, or move the signalling bitfield to a different location.

Legacy nodes would not be benefited from this softfork and warning system. However, no additional risks are introduced to legacy node either.


Reference implementation

To be done


Footnotes


	• ^ Please note that a hardfork may have lower difficulty but higher total proof-of-work, such as by decreasing the average block interval.
	• ^ In the March 2013 fork, pre-0.8 full nodes would see that as a hardfork with unknown rules, while light nodes and 0.8.0 full nodes would see that as multiple valid blockchains. In the July 2015 fork, BIP66-complying full nodes would see that as a hardfok with unknown rules, while legacy full nodes would see that as multiple valid blockchains.


Copyright

This document is placed in the public domain.


-------------------------------------
Thanks for getting this started, Luke.

Noticeably absent here is the "default_witness_commitment" key, as
added by the current reference implementation[0].

I assume (please correct me if I'm wrong) that this has been omitted
for the sake of having clients create the commitment themselves as
opposed to having it provided to them.

I don't think that the two approaches (providing the default
commitment for the complete tx set as well as the ability to create it
from chosen transactions) are at odds with each-other, rather it
merely allows for a simpler approach for those who are taking tx's
as-is from bitcoind. It's obviously important for the clients to be
able to chose tx's and create commitments as they desire, but it's
equally important to allow for simpler use-cases.

The issue in particular here is that a non-trivial burden is thrust
upon mining software, increasing the odds of bugs in the process. I'd
like to point out that this is not a theoretical argument. I've
already fixed a handful of bugs relating to serialization or
commitment creation in the mining/pool software that I've worked on
for segwit [1][2][3][4]. Asking them to handle more serialization and
calculation of complex structures needlessly increases the complexity
for zero benefit in the case where the tx's are to be used as-is.

I'll PR this change to the BIP, as I can't really come up with an
argument against. At worst, it can simply be ignored.

[0]: https://github.com/sipa/bitcoin/blob/segwit/src/rpcmining.cpp#L590
[1]: https://github.com/bitcoin/libblkmaker/commit/22f6e42844aa14ed0037ebf12a734f07e63533d7
[2]: https://github.com/bitcoin/libblkmaker/commit/15e2c35bf69c997488e37147cf062dfa925b4912
[3]: https://github.com/bitcoin/libblkmaker/commit/9a5799891e0f3590779b8e5a993a7b306088e2fa
[4]: https://github.com/theuni/ckpool/commit/7d84b1d76b39591cc1c1ef495ebec513cb19a08e

Regards,
Cory

On Sat, Jan 30, 2016 at 1:50 PM, Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> I've completed an initial draft of a BIP for updating getblocktemplate for
> segregated witness here:
>     https://github.com/luke-jr/bips/blob/segwit_gbt/bip-segwit-gbt.mediawiki
>
> Please review and comment (especially with regard to the changes in the
> sigoplimits handling).
>
> (Note: libblkmaker's reference implementation is at this time incompatible
> with the "last output" rule in this BIP.)
>
> Thanks,
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
I think this conversation has gone off the rails and is no longer really
appropriate for the list.

But just to be clear to any readers.  Bitcoin Core absolutely does rely on
the impossibility of a hash collision for maintaining consensus.  This
happens in multiple places in the code but in particular we don't check
BIP30 any more since the only way it could get violated is by a hash
collision.





On Thu, Nov 17, 2016 at 6:22 AM, Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On 11/17/2016 02:22 AM, Tier Nolan via bitcoin-dev wrote:
> > On Thu, Nov 17, 2016 at 12:43 AM, Eric Voskuil <eric@voskuil.org
> > <mailto:eric@voskuil.org>> wrote:
> >
> >     > This means that all future transactions will have different
> txids...
> >     rules do guarantee it.
> >
> >     No, it means that the chance is small, there is a difference.
> >
> > I think we are mostly in agreement then?  It is just terminology.
>
> Sure, if you accept that mostly is not fully - just as unlikely is not
> impossible.
>
> > In terms of discussing the BIP, barring a hash collision, it does make
> > duplicate txids impossible.
>
> That's like saying, as long as we exclude car accidents from
> consideration, car accidents are impossible.
>
> > Given that a hash collision is so unlikely, the qualifier should be
> > added to those making claims that require hash collisions rather than
> > those who assume that they aren't possible.
> >
> > You could have said "However nothing precludes different txs from having
> > the same hash, but it requires a hash collision".
>
> I generally try to avoid speaking in tautologies :)
>
> > Thinking about it, a re-org to before the enforcement height could allow
> > it.  The checkpoints protect against that though.
> >
> >     As such this is not something that a node
> >     can just dismiss.
> >
> > The security of many parts of the system is based on hash collisions not
> > being possible.
>
> This is not the case.
>
> Block hash duplicates within the same chain are invalid as a matter of
> consensus, which is the opposite of assuming impossibility.
>
> Tx hash collisions are explicitly allowed in the case that preceding tx
> with the same hash is unspent. This is also not a reliance on the
> impossibility of hash collision. Core certainly implements this
> distinction:
>
> https://github.com/bitcoin/bitcoin/blob/master/src/main.cpp#L2419-L2426
>
> Address hashes and script hashes can collide without harming the
> security of Bitcoin (although address owner(s) may experience harm).
> Rare in this case is sufficient because of this distinction.
>
> Compact blocks contemplates hash collisions:
>
> https://github.com/bitcoin/bips/blob/master/bip-0152.
> mediawiki#Random_collision_probabilty
>
> Checkpoints aren't part of Bitcoin security, so even the remote
> possibility of two different potential blocks, with the same hash, at
> the same height in the same chain, does not indicate a problem.
>
> There is no case where the security of Bitcoin assumes that hashes never
> collide. Consensus rules have specific handling for both block hash
> collisions and tx hash collisions.
>
> e
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On Thu, Jan 7, 2016 at 5:10 PM, Luke Dashjr <luke@dashjr.org> wrote:

> - BIP 46 is missing from the repository, but apparently self-soft-assigned
> by
> Tier Nolan in
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-April/005545.html
> ; if this was later assigned official, or if he is still
> interested in pursuing this, it seems logical to just keep it at BIP 46.
>

I was never officially assigned any number for this.

Subsequent P2SH changes give the required functionality in an alternative
way.  This renders the BIP obsolete.

I suggest marking the number as nonassignable, in order to prevent
confusion with archive searches.  I assume that new BIP numbers will be
greater than 100 anyway.

As was pointed out at the time, I shouldn't have used a number in the
original git branch before being assigned it officially.

-------------------------------------
On Sunday, 16 October 2016 20:41:34 CEST Jorge Timn wrote:
> You keep insisting on "2 months after activation", but that's not how
> BIP9 works. We could at most change BIP9's initial date, but if those
> who haven't started to work on supporting segwit will keep waiting for
> activation, then changing the initial date won't be of any help to
> them can only delay those who are ready and waiting.

Then don't use BIP9...

Honestly, if the reason for the too-short-for-safety timespan is that you 
want to use BIP9, then please take a step back and realize that SegWit is a 
contriversial soft-fork that needs to be deployed in a way that is extra 
safe because you can't roll the feature back a week after deployment.
All transactions that were made in the mean time turn into everyone-can-
spent transactions.

I stand by the minimum of 2 months. There is no reason to use BIP9 as it was 
coded in an older client. That is an excuse that I don't buy.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
I think the biggest question here would be how would the difficulty retargeting be changed?  Without seeing the algorithm proposal it's difficult to assess the impact that it would have, but my intuition is that this is likely to be problematic.

Probabilistically the network sees surprisingly frequent swings of +/-20% in terms of the block finding rate on any given day, while the statistical noise over a 2016 block period can be more than +/-5%.  Any change would still have to require a fairly significant period of time before there would be a reasonable level of confidence that the hash rate really had fallen as opposed to just seeing statistical noise (http://hashingit.com/analysis/29-lies-damned-lies-and-bitcoin-difficulties and http://hashingit.com/analysis/28-reach-for-the-ear-defenders).

How long would be required to deem that the hash rate had dramatically fallen?  Would such a change be a one-time event or would it be ever-present?

If we were to say that if the hash rate dropped 50% in one day (which could, of course be a 30% real drop and 20% variance) and the difficulty was retargeted to 50% lower then that would have to be matched with a similar rapid retarget if it were to increase by a similar amount.  Failing to do this both ways this would introduce an economic incentive for large miners to suppress the difficulty and gain dramatically larger numbers of block rewards.  The current fixed block count per difficulty change prevents this because the daily losses while suppressing hashing outweigh the potential gains when it's re-added.


Cheers,
Dave


> On 2 Mar 2016, at 14:56, Luke Dashjr via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> We are coming up on the subsidy halving this July, and there have been some 
> concerns raised that a non-trivial number of miners could potentially drop off 
> the network. This would result in a significantly longer block interval, which 
> also means a higher per-block transaction volume, which could cause the block 
> size limit to legitimately be hit much sooner than expected. Furthermore, due 
> to difficulty adjustment being measured exclusively in blocks, the time until 
> it adjusts to compensate would be prolonged.
> 
> For example, if 50% of miners dropped off the network, blocks would be every 
> 20 minutes on average and contain double the transactions they presently do. 
> Even double would be approximately 850-900k, which potentially bumps up 
> against the hard limit when empty blocks are taken into consideration. This 
> situation would continue for a full month if no changes are made. If more 
> miners drop off the network, most of this becomes linearly worse, but due to 
> hitting the block size limit, the backlog would grow indefinitely until the 
> adjustment occurs.
> 
> To alleviate this risk, it seems reasonable to propose a hardfork to the 
> difficulty adjustment algorithm so it can adapt quicker to such a significant 
> drop in mining rate. BtcDrak tells me he has well-tested code for this in his 
> altcoin, which has seen some roller-coaster hashrates, so it may even be 
> possible to have such a proposal ready in time to be deployed alongside SegWit 
> to take effect in time for the upcoming subsidy halving. If this slips, I 
> think it may be reasonable to push for at least code-readiness before July, 
> and possibly roll it into any other hardfork proposed before or around that 
> time.
> 
> I am unaware of any reason this would be controversial, so if anyone has a 
> problem with such a change, please speak up sooner rather than later. Other 
> ideas or concerns are of course welcome as well.
> 
> Thanks,
> 
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
On 11 May 2016 at 16:18, Sergio Demian Lerner via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Jorge Timón said..
> > What do you mean by "embrace" in the context of a patented optimization
> that one miner can prevent the rest from using?
>
> Everyone seems to assume that one ASIC manufacturer will get the advantage
> of AsicBoost while others won't. If a patent license is non-exclusive, then
> all can.
>
>

1. Whatever way you look at it, it will be an extra barrier of entry (cost,
legal hassle, more complex chip design) for any new ASIC manufacturer
trying to enter the market. That counters free competition and thus
decentralization.

2. Why would you want to put yourself in the central spot of the big
decider on who gets access to the technology (and therefore the whole
mining game) and who doesn't. You're not afraid of NSA knocking on your
door to politely hand you their blacklist? You don't think this counters
all the years of hard work that went into Bitcoin exactly to avoid any such
central points of authority?

P.S. I'm not decided yet on being for or against a HF to ban AsicBoost
myself, nor does my opinion count for much. But I think I do see real
problems, like the above.

-------------------------------------
Indeed, anything which uses P2SH is obviously vulnerable if there is an attack on RIPEMD160 which reduces it's security only marginally. While no one thought hard about these attacks when P2SH was designed, we realized later this was not such a good idea to reuse the structure from P2PKH. Hence why this discussion came up.

On January 7, 2016 7:30:11 PM PST, Rusty Russell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>Pieter Wuille via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
>writes:
>> Yes, this is what I worry about. We're constructing a 2-of-2 multisig
>> escrow in a contract. I reveal my public key A, you do a 80-bit
>search for
>> B and C such that H(A and B) = H(B and C). You tell me your keys B,
>and I
>> happily send to H(A and B), which you steal with H(B and C).
>
>FWIW, this attack would effect the current lightning-network
>"deployable
>lightning" design at channel establishment; we reveal our pubkey in the
>opening packet (which is used to redeem a P2SH using normal 2of2).
>
>At least you need to grind before replying (which will presumably time
>out), rather than being able to do it once the channel is open.
>
>We could pre-commit by exchanging hashes of pubkeys first, but
>contracts
>on bitcoin are hard enough to get right that I'm reluctant to add more
>hoops.
>
>Cheers,
>Rusty.
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev@lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
In light of the recent hack: what does everyone think of the idea of
creating a new address type that has a reversal key and settlement layer
that can be used to revoke transactions?

You could specify so that transactions "sent" from these addresses must
receive N confirmations before they can't be revoked, after which the
transaction is "settled" and the coins become redeemable from their
destination output. A settlement phase would also mean that a transaction's
progress was publicly visible so transparent fraud prevention and auditing
would become possible by anyone.

The reason why I bring this up is existing OP codes and TX types don't seem
suitable for a secure clearing mechanism; Nlocktimed TXs won't work for
this since you can't know ahead of time when and where a withdrawal needs
to be made, plus there's still the potential for key mismanagement; Similar
problems with OP_CHECKLOCKTIMEVERIFY apply too – unless you keep a private
key around on the server which would defeat the purpose. The main use case
here, would be specifically to improve centralized exchange security by
making it impossible for a hot wallet to be raided all at once.

Thoughts?

Some existing background:

http://hackingdistributed.com/2016/08/03/how-bitfinex-heist-could-have-been-avoided/
-- Proposed the basic idea for a time-based clearing house but using
blockchains directly, this is a much better idea than my own.

roberts.pm/timechain -- My original paper written in 2015 which proposed a
similar idea for secure wallet design but implemented using time-locked
ECDSA keys. Obviously a blockchain would work better for this.

Other -- if the idea has already been brought up by other people, I
apologize.

-------------------------------------
Why not just have a single 1-of-m multisig transaction, with one key on
each server? Based on which key is used you would know which server is
compromised, and (in my opinion) it wouldn't look nearly as suspicious.

On Thu, Aug 25, 2016 at 11:26 AM Gregory Maxwell via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Thu, Aug 25, 2016 at 2:27 PM, Christian Decker via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > If however
> > he is planning to use it as a foothold to further compromise your
> > company, send spam or similar, he will likely try to avoid these
> > tripwires. [...]
>
> Depends on the value of their activity compared to the value of the coins.
> Spamming doesn't pay much.
>
> Covert tripwires would obviously be better, but if shared tripwires
> allow you to have 100x the funds available it could be a good
> trade-off.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
https://github.com/bitcoin/bips/pull/441

The BIP146 is revised the second time:

1. NULLDUMMY is removed from BIP146 and becomes another softfork that will implement at the same time as segwit https://github.com/bitcoin/bips/pull/440

2. A new rule, namely NULLFAIL, is added to require empty signature(s) when a CHECK(MULTI)SIG returns a FALSE

3. NULLFAIL will be implemented as a policy rule in 0.13.1. However, the softfork won't be deployed in 0.13.1.

As we discovered some undocumented behavior in LOW_S, we may want to deploy the LOW_S softfork in a later release. The newly added NULLFAIL rules should cover all the special cases. https://github.com/bitcoin/bitcoin/pull/8533#issuecomment-243973512

--------
BIP: 146
  Title: Dealing with signature encoding malleability
  Author: Johnson Lau <jl2012@xbt.hk>
          Pieter Wuille <pieter.wuille@gmail.com>
  Status: Draft
  Type: Standards Track
  Created: 2016-08-16

Abstract

This document specifies proposed changes to the Bitcoin transaction validity rules to fix signature malleability related to ECDSA signature encoding.

Motivation

Signature malleability refers to the ability of any relay node on the network to transform the signature in transactions, with no access to the relevant private keys required. For non-segregated witness transactions, signature malleability will change the txid and invalidate any unconfirmed child transactions. Although the txid of segregated witness (BIP141) transactions is not third party malleable, this malleability vector will change the wtxid and may reduce the efficiency of compact block relay (BIP152).

Since the enforcement of Strict DER signatures (BIP66), there are 2 remaining known sources of malleability in ECDSA signatures:

Inherent ECDSA signature malleability: ECDSA signatures are inherently malleable as taking the negative of the number S inside (modulo the curve order) does not invalidate it.
Malleability of failing signature: If a signature failed to validate in OP_CHECKSIG or OP_CHECKMULTISIG, a FALSE would be returned to the stack and the script evaluation would continue. The failing signature may take any value, as long as it follows all the rules described in BIP66.
This document specifies new rules to fix the aforesaid signature malleability.
Specification

To fix signature malleability, the following new rules are applied:

LOW_S

We require that the S value inside ECDSA signatures is at most the curve order divided by 2 (essentially restricting this value to its lower half range). Every signature passed to OP_CHECKSIG[1], OP_CHECKSIGVERIFY, OP_CHECKMULTISIG, or OP_CHECKMULTISIGVERIFY, to which ECDSA verification is applied, MUST use a S value between 0x1 and 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0 (inclusive) with strict DER encoding (see BIP66).

If a signature passing to ECDSA verification does not pass the Low S value check and is not an empty byte array, the entire script evaluates to false immediately.

A high S value in signature could be trivially replaced by S' = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE BAAEDCE6 AF48A03B BFD25E8C D0364141 - S.

NULLFAIL

If an OP_CHECKSIG is trying to return a FALSE value to the stack, we require that the relevant signature must be an empty byte array.

If an OP_CHECKMULTISIG is trying to return a FALSE value to the stack, we require that all signatures passing to this OP_CHECKMULTISIG must be empty byte arrays, even the processing of some signatures might have been skipped due to early termination of the signature verification.

Otherwise, the entire script evaluates to false immediately.

Examples

The following examples combine the LOW_S and NULLFAIL rules.

Notation:

  CO       : curve order = 0xFFFFFFFF FFFFFFFF FFFFFFFF FFFFFFFE BAAEDCE6 AF48A03B BFD25E8C D0364141
  HCO      : half curve order = CO / 2 = 0x7FFFFFFF FFFFFFFF FFFFFFFF FFFFFFFF 5D576E73 57A4501D DFE92F46 681B20A0
  P1, P2   : valid, serialized, public keys
  S1L, S2L : valid low S value signatures using respective keys P1 and P2 (1 ≤ S ≤ HCO)
  S1H, S2H : signatures with high S value (otherwise valid) using respective keys P1 and P2 (HCO < S < CO)
  F        : any BIP66-compliant non-empty byte array but not a valid signature

These scripts will return a TRUE to the stack as before:

  S1L P1 CHECKSIG
  0 S1L S2L 2 P1 P2 2 CHECKMULTISIG

These scripts will return a FALSE to the stack as before:

  0 P1 CHECKSIG
  0 0 0 2 P1 P2 2 CHECKMULTISIG

These previously TRUE scripts will fail immediately under the new rules:

  S1H P1 CHECKSIG
  0 S1H S2L 2 P1 P2 2 CHECKMULTISIG
  0 S1L S2H 2 P1 P2 2 CHECKMULTISIG
  0 S1H S2H 2 P1 P2 2 CHECKMULTISIG
These previously FALSE scripts will fail immediately under the new rules:

  F P1 CHECKSIG
  0 S2L S1L 2 P1 P2 2 CHECKMULTISIG
  0 S1L F   2 P1 P2 2 CHECKMULTISIG
  0 F   S2L 2 P1 P2 2 CHECKMULTISIG
  0 S1L 0   2 P1 P2 2 CHECKMULTISIG
  0 0   S2L 2 P1 P2 2 CHECKMULTISIG
  0 F   0   2 P1 P2 2 CHECKMULTISIG
  0 0   F   2 P1 P2 2 CHECKMULTISIG

Deployment

This BIP will be deployed by "version bits" BIP9. Details TBD.

For Bitcoin mainnet, the BIP9 starttime will be midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout will be midnight TBD UTC (Epoch timestamp TBD).

For Bitcoin testnet, the BIP9 starttime will be midnight TBD UTC (Epoch timestamp TBD) and BIP9 timeout will be midnight TBD UTC (Epoch timestamp TBD).

Compatibility

The reference client has produced LOW_S compatible signatures since v0.9.0, and the LOW_S rule has been enforced as relay policy by the reference client since v0.11.1. As of August 2016, very few transactions violating the requirement are being added to the chain. For all scriptPubKey types in actual use, non-compliant signatures can trivially be converted into compliant ones, so there is no loss of functionality by these requirements.

Scripts with failing OP_CHECKSIG or OP_CHECKMULTISIG rarely happen on the chain. The NULLFAIL rule has been enforced as relay policy by the reference client since v0.13.1.

Users MUST pay extra attention to these new rules when designing exotic scripts.

Implementation

Implementations for the reference client is available at:

https://github.com/bitcoin/bitcoin/blob/35fe0393f216aa6020fc929272118eade5628636/src/script/interpreter.cpp#L185

and

https://github.com/bitcoin/bitcoin/pull/8634

Footnotes

^ Including pay-to-witness-public-key-hash (P2WPKH) described in BIP141
Acknowledgements

This document is extracted from the previous BIP62 proposal which had input from various people.

Copyright

This document is placed in the public domain.

--------

> On August 17, 2016 at 8:43 AM Johnson Lau via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> The BIP146 has been updated to include NULLDUMMY* as part of the softfork:
> 
> https://github.com/bitcoin/bips/pull/435
> 
> NULLDUMMY is a trivial softfork to fix malleability related to the extra stack element consumed by CHECKMULTISIG(VERIFY). It is probably more important than LOW_S since without that an attacker may replace the stack element with any value.

> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
On Wed, Aug 24, 2016 at 11:03 PM, Chris Priest via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> How does your system prevent against insider attacks? How do you know
> the money is stolen by someone who compromised server #4, and not
> stolen by the person who set up server #4? It is my understanding
> these days most attacks are inside jobs.

Working as designed in that case:  You know #4 is compromised, it
doesn't tell you if it was an insider or an outsider, but in both
cases someone unauthorized or without integrity got access to the
key(s).


-------------------------------------
- Payment channels seem clearly inappropriate for things like monthly
subscriptions, the use of nlocktime, etc.

- Merchants cannot send requests to users for future payments, because
users don't run servers that they can connect to.  That's why BIP0070 works
the way it does.

- Need to have an interval for subscriptions, at a minimum, and stored in
the wallet so next months payment can go out on time

- Support for varying currency conversion needs to be baked in to
wallets.   Fortunately, by adding advisory subscription info to the
paymentrequest, this is left up to the wallet to
secure/validate/repeat/convert/etc. as needed for each subscription.

- The UI you describe is nice - but not unique to the solution.




On Wed, Jun 22, 2016 at 12:20 PM, Andy Schroder <info@andyschroder.com>
wrote:

> I understand the need for people to make repeated payments to individuals
> in real life that they know, without the payee every even taking the effort
> to make a formal payment request (say you're just paying a family member of
> friend back for picking something up for you at the store, and you've
> already payed them many times before).
>
> For a subscription, wouldn't it be better to promote payment channels or
> just send another payment request? I've been brainstorming recently about a
> model where service providers could deliver invoices, receipts, and payment
> requests in a standardized and secure way. In addition to having a send,
> receive, and transaction history tab in your bitcoin wallet, you'd also
> have an open payment channels tab (which would include all applications on
> your computer that have an open real time payment channel, such as a wifi
> access point, web browser, voip provider, etc.), as well as a "bills to
> pay" tab. Since everything would be automated and consolidated locally, you
> wouldn't have to deal with logging into a million different websites to get
> the bills and then pay them. If it were this easy, why would you ever want
> to do a recurring payment from a single payment request? I understand why
> you may think you want to given current work flows, but I'm wondering if it
> may be better to just skip over to a completely better way of doing things.
>
>
> Andy Schroder
>
>
> On 06/22/2016 11:30 AM, Erik Aronesty wrote:
>
>> My conclusion at the bottom of that post was to keep BIP 75 the same,
>> don't change a bit, and stick any subscription information (future payment
>> schedule) in the PaymentACK.   Then the wallet then re-initiates an invoice
>> (unattended or attended.. up to the user), after the subscription interval
>> is passed. Subscriptions are pretty important for Bitcoin to be used as a
>> real payment system.
>>
>
>
>

-------------------------------------
TLDR:

  1.7MB effective block size is a better estimate than 1.6MB for p2pkh
  with segwit. 2MB for 2/2 multisig still seems accurate.

  Additional post-segwit soft forked script improvements can improve
  the effective block size for p2pkh txns from 1.7MB to 1.9MB, and for
  2/2 multisig from 2MB to 2.5MB/3MB.

  (To the best of my knowledge, anyway; if I've made a mistake in my
  maths or assumptions, corrections appreciated)

On Tue, Dec 08, 2015 at 02:58:03PM +1000, Anthony Towns via bitcoin-dev wrote:
> So from IRC, this doesn't seem quite right -- capacity is constrained as
>   base_size + witness_size/4 <= 1MB
..
> That would be 1.6MB and 2MB of total actual data if you hit the limits
> with real transactions, so it's more like a 1.8x increase for real
> transactions afaics, even with substantial use of multisig addresses.

I think these numbers are slightly mistaken -- I was only aware of version
1 segwit scripts at the time, and assumed 256 bit hashes would be used
for all segwit transaction, however version 0 segwit txns would be more
efficient for p2pkh, with the same security as bitcoin currently has
(which seems fine).

Also, segwit will make two additional soft-fork improvements possible that
would have a positive effect on transactions per block without requiring
more data per block: ecdsa public key recovery (more space efficient for
*both* multisig and p2pkh) and schnorr signatures (more space efficient
multisig) which might also improve things. I don't know how soon they're
planned to be worked on post segwit's roll out; basic Schnorr signatures
are in the Elements sidechain, but I don't think key recovery has been
implemented anywhere? (Actually, I guess they could both be done already
via softforking OP_NOP opcodes, though segwit makes them slightly
cleaner)

Anyhoo here's some revised figures, working explained in the footnotes.
If I've made mistakes, corrections appreciated, of course.

p2pkh:

  now: 10+146i+34o [0]
  segwit: 10+41i+36o + 0.25*105*i [1]
  ecdsa recovery: 10+41i+33o + 0.25*71*i [2]
  80-bit schnorr: 10+41i+33o + 0.25*71*i (same as ecdsa recovery imo [3])
  128-bit schnorr: 10+41i+43o + 0.25*106*i [4]

(128-bit schnorr provides a not very useful increase in security here)

2-of-2 multisig:

  now: 10+254i+32o [5]
  segwit: 10+43i+43o + 0.25*213*i [6]
  ecdsa recovery: 10+43i+43o + 0.25+187*i [7]
  80-bit schnorr: 10+41i+33o + 0.25*71*i (same as p2pkh)
  128-bit schnorr: 10+41i+43o + 0.25*106*i (same as p2pkh)

(segwit, ecdsa recovery and 128-bit schnorr all provide a beneficial
security increase here, as per the "Time to worry about 80-bit collision
attacks" thread; 80-bit schnorr provides the same security as current
p2sh multisig)

Using the same assumptions in the previous mail, ie that over the long
term number inputs is about the same as number of outputs, these
simplify to:

        p2pkh           2-of-2 msig
now     10+180i         10+286i
segwit  10+104i         10+140i
recov   10+92i          10+133i
sch80   10+92i          10+92i
sch128  10+111i         10+111i

Translating "now" to 100%, the scaling factors work out to be:

i=1, i->inf

        p2pkh           2-of-2 msig
now     100%            100%
segwit  166%-173%       197%-204%
recov   186%-195%       207%-215%
sch80   186%-195%       290%-310%
sch128  157%-162%       244%-257%

So 170% for p2pkh (rather my original estimate of 160%) and 200% for
multisig (same as my original estimate), which can rise via further
soft-forks up to 190% for p2pkh and 250% or 300% for 2-of-2 multisig
(depending on whether you want additional security for 2/2 multisig
beyond what's currently available).

(I'm assuming people are mostly interested in the number of transactions
per block (or tx/second or tx/day); if miners are worried about the
actual data per block (which effects orphan rates) implied by the above,
but don't want to work it out themselves, I could do the maths for that
too pretty easily. Let me know)


If a 2MB hard fork is done first, then the 1/4 discount for segwit could
mean up to 8MB of total data per block -- from what I understand this
is currently infeasible; so I presume that segwit on top of a hardfork
and prior to IBLT/weak blocks would need to have a smaller discount or
no discount applied so as to ensure total data per block remains at 4MB
or less. With no discount for witness data (ie, no "accounting tricks")
those figures look like:

        p2pkh           2-of-2 msig
now     100%            100%
segwit  99%             95%
recov   122%-124%       104%
sch80   122%-124%       191%-198%
sch128  94%-95%         148%-150%

That is, without discounting, segwit comes at a slight cost in
transactions per block, and additional soft forks will only result in
25% gain for p2pkh (via key recovery) and 50%-100% for 2-of-2 multisig
(through the use of schnorr sigs and key recovery, and depending on
whether you want 128 bits of security rather than 80 bits).

(So without the discounting factor, with a 2MB block size, 2MB per block
with segwit and key recovery gives you 25% more p2pkh transactions than
just 2MB per block now; while segwit and schnorr signatures gives you
50%-100% more 2/2 multisig transactions in the same 2MB. Likewise with
1MB or 4MB and no discounting. Discounting has the indirect benefit of
providing a monetary incentive to limit UTXO sizes however)


(2 of 3 multisig for escrow payments would probably be interesting to
work out too; I think ecdsa key recovery combined with 1/4 discounting
would provide a substantial improvement there. I don't think Schnorr
helps at all for that case, unfortunately; and it's probably too small
scale for merkle-ised abstract syntax trees to do any good either)


A caveat: I'm only counting the script data from witnesses here; but it's
possible that additional metadata (such as a length for each witness
signature, or the value of the input, or even some/all of the merkle
hashes) should also be accounted for. I don't think any of them need to
be accounted for segwit as proposed, but I'm not sure. And it might well
be different for a hardforked segwit; there I have no idea at all. I
don't think a byte or two for length would make much difference, at least.

Cheers,
aj

[0] 10 bytes for version (4), input count (1), output count (1) and
    locktime (4); 146 bytes per input consisting of tx hash (32), txout
    index (4), script length (1), scriptsig (signature and pubkey =
    105), CHECKSIG = 25), and sequence number (4); 34 bytes per output
    consisting of value (8), script length (1) and scriptpubkey (DUP
    HASH160 PUSH20 EQVERIFY CHECKSIG = 25).

[1] Same as now, except two extra bytes per output script (segwit push and
    segwit version byte), and moving the 105 bytes of signature script
    directly into the segregated witness

[2] Allowing ECDSA recovery requires an additional soft-fork post segwit
    to change the CHECKSIG operation; this requires bumping the
    segwit script version to 2 or higher and possibly using a different
    opcode, but allows the scriptsig to just be the 70 byte signature,
    without also including the 33 byte pubkey. The 33 byte pubkey is
    automatically calculated from the signature, and verified against
    the hash provided in the scriptpubkey to maintain security, with a
    scriptpubkey like: [PUSH (20 byte pubkey hash) CHECKSIG_RECOVER] (22
    bytes versus 25 bytes), and a scriptsig like [PUSH (70 byte sig)]
    (71 bytes versus 105 bytes).

[3] libsecp256k1 has a function to recover a pubkey from a schnorr
    signature, so I'm assuming that means pubkey recovery with schnorr
    is possible :) -- I haven't actually verified the maths
    https://github.com/bitcoin/secp256k1/blob/master/include/secp256k1_schnorr.h

[4] The witness scriptpubkey is limited to 32 bytes (plus push op and
    version byte for a total of 34 bytes, so 128 bit security requires
    version 1 segwit, and p2sh-style constuction. Hence: 10 bytes
    (version, input and output counts and locktime); 41 base bytes per
    input (tx hash, tx index, script length, and sequence number); 106
    witness bytes per input (sig (70 bytes) plus witness script (PUSH
    schnorr merged pubkey (32 bytes) plus CHECKSCHNORR), plus PUSH ops);
    and 43 bytes per output (value, script length, and 34 bytes for the
    v1-style witness script).

[5] Per input is (32 bytes tx hash, 4 bytes tx index, 4 bytes nsequence,
    1 byte scriptsig length, 143 bytes for actual signature (2x70
    for the sigs, 3 bytes for OP_0 and two OP_PUSH), and 70 bytes for
    the redeemscript (2 pub pub 2 OP_CHECKMULTISIG)) for 254 bytes;
    per output is (8 bytes value, 1 byte length, 23 bytes for HASH160
    [20 byte hash] OP_EQUAL) for 32 bytes.

[6] Per input is (34 bytes tx hash, 4 bytes tx index, 4 bytes nsequence,
    1 byte scriptsig length) for 43 bytes in the base block and (143
    bytes for the actual signature, plus 70 bytes for the redeemscript)
    for 213 bytes of witness data; per output is (8 bytes value, 1 byte
    length, and 34 bytes for version 1 segwit scriptpubkey) for 43
    bytes.

[7] Same as [6], but with key recovery on a MULTISIG op, rather than 33
    bytes per pubkey, this could be reduced to a 20 byte pubkey hash
    per pubkey, for a saving of 26 bytes of witness data.



-------------------------------------
I think the main thing you're missing is that there will always be
transactions available to mine simply because demand for blockspace is
effectively unbounded as fees approach 0. Nodes generally have a
static mempool size and dynamic minrelaytxfee nowadays so as
transactions get mined lower fee transactions get accepted into the
mempool. An individual opting to not send a transaction would not make
the blocks smaller simply because there will always be other
transactions available(it would really only have an effect on the
transaction fees needed to get mined).

On Sun, Dec 11, 2016 at 3:40 PM, t. khan <teekhan42@gmail.com> wrote:
>
> On Sun, Dec 11, 2016 at 3:31 PM, James Hilliard <james.hilliard1@gmail.com>
> wrote:
>>
>> What's most likely to happen is miners will max out the blocks they
>> mine simply to try and get as many transaction fees as possible like
>> they are doing right now(there will be a backlog of transactions at
>> any block size). Having the block size double every year would likely
>> cause major problems and this proposal allows over a 7x increase it
>> seems.
>
>
> Block75 is not exponential scaling. It's true the max theoretical increase
> in the first year would be 7x, but the next year would be a max of 2x, and
> the next could only increase by 50% and so on.
>
> However, to reach the max in the first year: 1) ALL blocks would have to be
> 100% full and 2) transactions would have to increase at the same rate. We'd
> have to be doing 2.1 million transactions a day within a year to make that
> happen, and would therefore need blocks to be that big.
>
> Realistically, max block size will grow (and shrink) at a much slower rate
> ... even more so with SegWit.
>
>>
>>  The main problem with this proposal I think is that users effectively
>>
>> have no way to stop the miners from increasing block size
>> continuously.
>
>
> Yes they could, simply by not sending transactions. Users don't care at all
> about block size. They just want their transactions to be fast and
> relatively cheap.
>
> -t.k.


-------------------------------------
On Fri, Jan 08, 2016 at 07:38:50AM -0500, Gavin Andresen via bitcoin-dev wrote:
> Lets see if I've followed the specifics of the collision attack correctly,
> Ethan (or somebody) please let me know if I'm missing something:
> 
> So attacker is in the middle of establishing a payment channel with
> somebody. Victim gives their public key, attacker creates the innocent
> fund-locking script  '2 V A 2 CHECKMULTISIG' (V is victim's public key, A
> is attacker's) but doesn't give it to the victim yet.

Using Ethan Heilman's procedure, the attacker can create two scripts:

  2 V __A1__ 2 CHECKMULTISIG

  2 V __A2__ 2 CHECKMULTISIG

and find values A1 and A2 which hash the scripts to the same result
with under 3*2**80 work. I think you can do that by setting the next
private key as the result of RIPEMD(SHA256(script with pubkey)), so you
could still spend either. But it doesn't change the script, so it's not
*that* helpful -- you've just got two different keys you can use.

Ah, but you can make the form of the script be a function of your key, so:

  if privkey % 2 == 0:
    script = "2 V %s 2 CHECKMULTISIG" % (pubkey)
  else:
    script = "%s CHECKSIG" % (pubkey)
  hash = ripemd160(sha256(script))

  nextprivkey = hash

Then you have a 50% chance of your cycle giving you a matching hash for
one script with A1 and the other script with A2, and you can find the
cycle with under 3*2**80 work. Doing five attempts should give you ~96%
chance of hitting a usable pair, and should take under 15*2**80 work ~=
2**84 work, with trivial memory use.

Trying that in python with a vastly weakened hash function (namely,
the first five bytes of ripemd160(sha256()), with 40 bits of security
and 3*2**20 work) works as expected -- I got a "useful" collision on my
second try in about 7 seconds, seeding with "grumpycat3" ("grumpycat2"
didn't work) with the result being:

 hexlify(ripemd160(sha256("foo%sbar"%unhexlify("86f9fbac1a")))[:5])
 'ae94d9f908'

 hexlify(ripemd160(sha256("baz%squux"%unhexlify("104fc5093f")))[:5])
 'ae94d9f908'

Cheers,
aj



-------------------------------------
On Wed, Jun 8, 2016 at 11:47 PM, Alfie John via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Hi folks,
>
> Overall I think BIP 151 is a good idea. However unless I'm mistaken, what's to
> prevent someone between peers to suppress the initial 'encinit' message during
> negotiation, causing both to fallback to plaintext?
>
> Peers should negotiate a secure channel from the outset or backout entirely
> with no option of falling back. This can be indicated loudly by the daemon
> listening on an entirely new port.

Reduction to plaintext isn't an interesting attack vector for an
active attacker: they can simply impersonate the remote side.

This is addressed via authentication, where available, which is done
by a separate specification that builds on this one.

Without authentication this only provides protection against passive attackers.


-------------------------------------
This is a specific implementation of the "nuclear option" soft fork (or
"firm-fork").

The problem with any hard-fork (like) change is that there is an incentive
to add as much as possible and then the process gets bogged down.

Since the POW is based on the header 1, you could make header 3
expandable.  This would allow adding new fields later.

This could be used for other block commitments.  This would save having to
make the merkle tree a sum tree immediately.  At a later time, the sum-tree
root could be added. (I think you also need to commit the path to the first
entry in the sum-tree, in order to get compact proofs).  There could be
separate sum-trees for each counter (sigops, block size, tx count, sighash?)

Having a dedicated hard fork and soft fork counter is a good idea.  There
should also be a field for parallel soft forks.  Incrementing the soft fork
counter could set the bitfield soft forks back to zero.  Ideally, each soft
fork would have a yes and no bit.  If > 50% vote No, then it fails adoption.

The effect of this change is that nodes react to hard forks by stalling the
chain.  The hard fork counter means that the new rules could be that nodes
should do that going forward for all new hard forks.

- soft fork (bitfield or count) => warn user that a soft fork has happened
- hard fork count increase => warn user that update is required and don't
process any more blocks

This means that header3 should be kept as simple as possible.

   - 2 bytes: hardfork block version
   - 2 bytes: softfork block version
   - 4 bytes: softfork bitfields
   - 32 byte: hash(header4)

Header4 and everything else in the block could be changed when a hard fork
happens.  The merged mining rules and header3 would be fixed.

I think confirmation counts should be based on even numbers, i.e. 3800 of
4000, but that is an aesthetic issue and doesn't really matter.

A section on recommendations for the different client types would be useful.

If 1000 of the last 2000 blocks are votes for a hard fork, then warn the
user that a hard fork is being considered

If 4000 of the last 4463 blocks are votes for a hard fork, then warn the
user that a hard fork is likely to occur within the next few days

If a hard fork happens:

- shut down transaction processing
- inform the user that a hard fork has happened

Non-upgraded miners could blacklist the hard forking block and keep mining
on their own chain.  Their chain would never reach the threshold to trigger
the hard fork.  It would max out at 4323 blocks of the last 4463.

Ironically, if users did this, it would defeat some of the benefit of using
the hard fork field.

Users should definitely be given the option of accepting or rejecting the
hard fork.  Otherwise, miners can hard-fork at will, which isn't desirable.
<https://www.avast.com/sig-email> This email has been sent from a
virus-free computer protected by Avast.
www.avast.com <https://www.avast.com/sig-email>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

-------------------------------------
On Wednesday 11 May 2016 22:58:48 Gregory Maxwell via bitcoin-dev wrote:
> On Wed, May 11, 2016 at 10:42 PM, Timo Hanke via bitcoin-dev
> 
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > This is what I meant. If existing hardware gets forked-out it will
> > inevitably lead to the creation of an altcoin. Simply because the hardware
> > exists and can't be used for anything else both chains will survive. I was
> > only comparing the situation to a contentious hardfork that does not fork
> > out any hardware. If the latter one is suspected to lead to the permanent
> > existence of two chains then a hardfork that forks out hardware is even
> > more likely to do so (I claim it's guaranteed).
> 
> There are already many altcoins out there, we could not prevent that
> even if we wanted to. New ones are created all the time.

Comparing apples and oranges.

Altcoins have their own genesis block, the example Timo was talking about was 
a fork in the Bitcoin blockchain.

But its good to know you don't mind a fork in the Bitcoin chain, I'll remember 
that.


-------------------------------------
On 08/16/2016 12:22 PM, Luke Dashjr via bitcoin-dev wrote:
> It would be best if the hardware protocol were standardised, so the user 
> doesn't need a plugin of *any* sort... I notice some hardware wallets have 
> begun to implement (or reuse) Trezor's interface, so that would seem a good 
> place to start?

I also agree with this - the user experience would be a lot better
without the need to install custom adapter software, especially for the
desktop case.

There could be two layers to the specification - the raw messages that
need to be passed, and the transport mechanism to pass them (USB HID, QR
code, audio...). For the most common case (USB), both layers could be
defined, and other transports could be added later. This split already
exists in the draft specification, though it's not very clear (URIs
include return URIs that don't make sense for a pipe, for example).

The existing URI scheme, while allowing disambiguate by manufacturer,
provides no way to to enumerate available manufacturers or enabled
wallets. This means that the "driver" would have to include a GUI to
select this. Also, passing return URIs seems rather fragile - are there
any other examples of protocols that use URIs for bidirectional IPC?

Thomas


-------------------------------------
How is this compared to my earlier proposal: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011952.html> ?

In my proposal, only the (pruned) UTXO set, and 32 bytes per archived block, are required for mining. But it is probably more difficult for people to spend an archived output. They need to know the status of other archived outputs from the same block. A full re-scan of the blockchain may be needed to generate the proof but this could be done by a third party archival node.

> 
> 
> 
> ## Implementation
> 
> Our proposed high-performance/low-latency delayed commitment full-node
> implementation needs to store the following data:
> 
> 1) UTXO set
> 
>    Low-latency K:V map of txouts definitely known to be unspent. Similar to
>    existing UTXO implementation, but with the key difference that old,
>    unspent, outputs may be pruned from the UTXO set.
> 
> 
> 2) STXO set
> 
>    Low-latency set of transaction outputs known to have been spent by
>    transactions after the most recent TXO commitment, but created prior to the
>    TXO commitment.
> 
> 
> 3) TXO journal
> 
>    FIFO of outputs that need to be marked as spent in the TXO MMR. Appends
>    must be low-latency; removals can be high-latency.
> 
> 
> 4) TXO MMR list
> 
>    Prunable, ordered list of TXO MMR's, mainly the highest pending commitment,
>    backed by a reference counted, cryptographically hashed object store
>    indexed by digest (similar to how git repos work). High-latency ok. We'll
>    cover this in more in detail later.
> 
> 


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Binaries for bitcoin Core version 0.12.0rc2 are available from:

    https://bitcoin.org/bin/bitcoin-core-0.12.0/test/

Source code can be found on github under the signed tag

    https://github.com/bitcoin/bitcoin/tree/v0.12.0rc2

This is a release candidate for a new major version release, bringing new
features, bug fixes, as well as other improvements.

Preliminary release notes for the release can be found here:

    https://github.com/bitcoin/bitcoin/blob/0.12/doc/release-notes.md

Release candidates are test versions for releases. When no critical problems
are found, this release candidate will be tagged as 0.12.0.

Diff since rc1:
- - #7222 `e25b158` RPC: indicate which transactions are replaceable
- - #7386 `da83ecd` Add option `-permitrbf` to set transaction replacement policy
- - #7290 `b16b5bc` Add missing options help
- - #7387 `f4b2ce8` Get rid of inaccurate ScriptSigArgsExpected
- - #7381 `621bbd8` [walletdb] Fix syntax error in key parser
- - #7327 `b16b5bc` [Wallet] Transaction View: LastMonth calculation fixed
- - #7364 `7726c48` [qt] Windows: Make rpcconsole monospace font larger

Please report bugs using the issue tracker at github:

    https://github.com/bitcoin/bitcoin/issues

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQEcBAEBCgAGBQJWphBHAAoJEHSBCwEjRsmmh0EIALopACCwaCYRt9vl6fadDLxL
JMTyEPsGUaEX82iwuXeAhdReyvZDlC00ACZy6agp7oOuS1ryqOeYAsc33N+WtHE0
iETNyvRZVD1ASopHkdJrRW1a9X63Yvcvk/d6nVbO5auUAG5gPUFLrSTrqpSzR2D4
QtY1ofifXrYdqdQmPFJ5hnWg/Z1rko99sD8Pu3ebD6Dof5zuvJKHkLmXunGGXn/n
GOn8roS5LXEFHwCcL0zgNzfDywt/dhKiUHMKSNPsnz5qEDRg7WPzsNiQA/HVDlcp
v6akQ4ykZ56Lik8cVLi0NRW2dozSDti/XKBfWQHqWjGUJGUOS+lPJaWLv81oMwc=
=WlQm
-----END PGP SIGNATURE-----


-------------------------------------
Hi Lee

Thank you very much for the valuable input.
I'm still processing your feedback....

> 
> *Key Revocation*
> This is probably too complicated, but an additional public key would
> allow for cold-storage key revocation. Spreading the knowledge of such
> an event is always painful, but it could be stored in the blockchain. I
> think this is likely too complicated, but having these long-term keys
> constantly in memory/disk is unfortunate.
> 

Yes. This could be something that could be extended once the BIP is
stable and/or implemented.



>> <code>K_1</code> must be used to only encrypt the payload size of the
>> encrypted message to avoid leaking information by revealing the
>> message size. 
>>
>> <code>K_2</code> must be used in conjunction with poly1305 to build
>> an AEAD.
> 
> Chacha20 is a stream cipher, so only a single encryption key is needed.
> The first 32 bytes of the keystream would be used for the Poly1305 key,
> the next 4 bytes would be used to encrypt the length field, and the
> remaining keystream would be used to encrypt the payload. Poly1305
> would then generate a tag over the length and payload. The receiver
> would generate the same keystream to decrypt the length which
> identifies the length of the message and the MAC offset, then
> authenticate the length and payload, then decypt with the remaining
> keystream.
> 

Right. The AEAD construct I though of is probably called
chacha20-poly1305@openssh.com and specified in
https://github.com/openssh/openssh-portable/blob/05855bf2ce7d5cd0a6db18bc0b4214ed5ef7516d/PROTOCOL.chacha20poly1305#L34

I think this construct has already serval implementations and is widely
used.

I have updated the BIP to mention the chacha20-poly1305@openssh.com
specification.

> Is it safer to define two keys to prevent implementations from screwing
> this up? You have to split the decryption and authentication, so the
> basic modes of libsodium cannot be used for instance. If a custom tag
> generation scheme is being used, then the basic modes are already
> unusable ...
> 
> *Failed Authentication*
> What happens on a failed MAC attempt? Connection closure is the
> easiest way to handle the situation.

Yes. I think closing would make sense.

>> After a successful <code>encinit</code>/<code>encack</code>
>> interaction from both sides, the messages format must use the
>> "encrypted messages structure". Non-encrypted messages from the
>> requesting peer must lead to a connection termination (can be
>> detected by the 4 byte network magic in the unencrypted message
>> structure).
> 
> The magic bytes are at the same offset and size as the encrypted length
> field in the encrypted messages structure. So the magic bytes are not a
> reliable way to identify unencrypted messages, although the probability
> of collision is low.

Yes. This is a good point.
The implementation should probably also accept messages that contain the
4 byte network magic from unencrypted messages (to avoid possible
collisions).
If the message is unencrypted, the length check or the unsuccessful
authentication check will lead to a disconnect.

>> {|class="wikitable"
>> ! Field Size !! Description !! Data type !! Comments
>> |-
>> | 4 || length || uint32_t || Length of ciphertext payload in number
>> of bytes
>> |-
>> | ? || ciphertext payload || ? || One or many ciphertext command &
>> message data
>> |-
>> | 8 || MAC tag || ? || MAC-tag truncated to 8 bytes
>> |}
> 
> Why have a fixed MAC length? I think the MAC length should be inferred
> from the cipher + authentication mode. And the Poly1305 tag is 16 bytes.
> 
> *Unauthenticated Buffering*
> Implementations are unlikely to (i.e. should not) process the payload
> until authentication succeeds. Since the length field is 4 bytes, this
> means an implementation may have to buffer up to 4 GiB of data _per
> connection_ before it can authenticate the length field. If the outter
> length field were reduced to 2 or 3 bytes, the unauthenticated
> buffering requirements drop to 64 KiB and 16 MiB respectively. Inner
> messages already have their own length, so they can span multiple
> encrypted blocks without other changes. This will increase the
> bandwidth requirements when the size of a single message exceeds 64 KiB
> or 16 MiB, since it will require multiple authentication tags for that
> message. I think an additional 16 bytes per 16 MiB seems like a good
> tradeoff.
> 

Good point.
I have mentioned this now in the BIP but I think the BIP should allow
message > 16 MiB.
I leave the max. message length up to the implementation while keeping
the 4 byte length on the protocol level.

> 
>> A responding peer can inform the requesting peer over a re-keying
>> with a <code>encack</code> message containing 33byte of zeros to
>> indicate that all encrypted message following after this
>> <code>encack</code> message will be encrypted with ''the next
>> symmetric cipher key''.
>>
>> The new symmetric cipher key will be calculated by
>> <code>SHA256(SHA256(old_symetric_cipher_key))</code>.
>>
>> Re-Keying interval is a peer policy with a minimum timespan of 600
>> seconds.
> 
> Should the int64_t message count be reset to 0 on a re-key? Or should
> the value reset to zero after 2^63-1? Hopefully the peer re-keys before
> that rollover, or keystream reusage will occur. Unlikely that many
> messages are sent on a single connection though. And presumably this
> only re-keys the senders side? Bi-directional re-keying would be racy.

I just added the RFC4253 recommendation as a must (re-key after every
1GB of data sent or received).


</jonas>


-------------------------------------
What is the difference between downloading a hash and comparing it to a hash vs downloading a hash and then a block and comparing it to a block?

You are talking about breaking a system in order to make it run faster. Using the hash is an non-optimization trade against correctness.

There is no "first seen" rule, there is only valid and invalid. Even the name exposes the error of this thinking as "first" requires order.

Caching invalidity for DOS protection is fine. It should be quite obvious that the blockchain is nothing more than a coach of validity. If it's faster in some cases to store both validity and all invalidity that you are aware of it is fine, you are trading space for time.

But caching information that is neither validity nor invalidity, and using it to validate blocks is a break.

I cannot emphasize this point enough. A technique that provides varied results based on communication history, such as this "rule", is an attack vector. It allows the attacker to place information into your cache and read it back later from another connection. Even optimizing correct results based on communication history exposes the node in this manner. These sort of attacks have been shown to be very effective at deanonymizing hidden nodes.

The p2p protocol actually makes this sort of attack a matter of communication standard via the sharing of address information, but this can be disabled without impacting correctness. Due to such non-optimizations as the first seen "rule" however, a node becomes a candy store of fingerprinting attack vectors.

Bitcoin provides the mechanism to reject cheaply-produced invalid blocks quickly. This is after all the fundamental principle of hash cash - force the attacker to pay to spam attack. By obtaining headers first a node can obtain proof of work and perform correct and fast validation before ever obtaining the block's transactions. This technique is probably no more time-costly than the incorrect technique of checking a cache of hashes (ironically, a "hash cache" is an incorrect "hash cash"), and avoids the extra space of a secondary cache (the blockchain is the primary cache). It also avoids the varied time response that a secondary cache creates.

So once again, premature optimization erupts from the underlying design flaw, and creates more problems than proper design. The p2p network standard didn't have headers first at one point, making correct checks more costly. That is no longer the case. But nevertheless, one cannot trade correctness for time.

The tx pool, like the orphan pool, as I mentioned previously, is an optimization. It is not a part of consensus, so it isn't relevant to a discussion about forks. It is also a design flaw that nodes are expected to hold invalid transactions. It exposes nodes to both DOS and fingerprinting attacks. Proper tx handling implies that a tx connect to a valid block. There is no "header" for a transaction so correctness requires that the tx be downloaded before it can be validated.

e

> On Nov 18, 2016, at 8:43 AM, Johnson Lau <jl2012@xbt.hk> wrote:
> 
> In this case I don’t understand how your implementation won’t be DoS-ed. An attacker could keep sending you inv for the same block / transaction. Since you don’t assume the hash is unique, each time you have to download the block/tx again before you could tell if that is the same one you have already known. Otherwise, you are implementing the “first seen” rule.
> 
> Also, you can’t ban a peer just because you get an invalid tx from him, because he might be referring to a hash-colliding UTXO that you don’t know. In that case you need to request for the parent tx to verify. I wonder if you are really doing that.
> 
>> On 18 Nov 2016, at 11:20, Eric Voskuil <eric@voskuil.org> wrote:
>> 
>> You are suggesting that, since a node implements a denial of service policy that actually denies itself otherwise valid blocks, those blocks are conditionally invalid. And that, since the validity condition is based on order of arrival and therefore independently unverifiable, Bitcoin consensus is broken in the face of a hash collision.
>> 
>> I am aware of two other hash collision scenarios that cause Core to declare blocks invalid based on ordering. The block hash duplicate check (it's not fork-point relative) and signature verification caching. Like the "block banning" issue above, the latter is related to an internal optimization. I would categorize the former as a simple oversight that presumably goes way back.
>> 
>> What then is the consequence of validity that is unverifiable? You believe this means that Bitcoin consensus is broken. This is incorrect. First understand that it is not possible for consensus rules to invalidate blocks based on order of arrival. As such any *implementation* that invalidates blocks based on order of arrival is broken. It is an error to claim that these behaviors are part of consensus, despite being implemented in the satoshi node(s).
>> 
>> Validity must be verifiable independent of the state of other nodes. Consensus is a function of block history and time alone. Time is presumed to be universally consistent. To be a consensus rule all nodes must be able to independently reach the same validity conclusion, given the same set of blocks, independent of order. If this is not the case the behavior is not a consensus rule, it is simply a bug. 
>> 
>> Deviating from such bugs is not a break with consensus, since such non-rules cannot be part of consensus. One node implementation can behave deterministically while others are behaving non-deterministically, with the two nodes remaining consistent from a consensus standpoint (deterministic produces a subset of non-deterministic results). But, unlike arbitrary nodes, deterministic nodes will not cause disruption on the network.
>> 
>> You imply that these determinism bugs are necessary, that there is no fix. This is also incorrect.
>> 
>> The block banning hash collision bug is avoided by not using non-chain/clock state to determine validity. Doing otherwise is clearly a bug. The hash of a block is not the block itself, a logically-correct ban would be to compare the wire serialization of the block as opposed to the hash, or not maintain the feature at all.
>> 
>> The signature verification caching hash collision bug is the same problem, an optimization based on an invalid assumption. A full serialization comparison (true identity), or elimination of the feature resolves the  bug.
>> 
>> The block hash check collision bug is trivially resolved by checking at the fork point as opposed to the tip. This prevents arbitrary (and irrational) invalidity based on conflict with irrelevant blocks that may or may not exist above the fork point.
>> 
>> Libbitcoin is deterministic in all three cases (although the third issue is not made consistent until v3). I am not aware of any other non-determinism in Core, but I don't spend a lot of time there. There is no need to study other implementations to ensure determinism, as that can be verified independently.
>> 
>> Any situation in which a node cannot provide deterministic validation of unordered blocks constitutes a non-consensus bug, as the behavior is not consistently verifiable by others under any conditions. Fixing/preventing these bugs is responsible development behavior, and does not require forks or BIPs, since Bitcoin doesn't inherently contain any such bugs. They are the consequence of incorrect implementation, and in two of the three cases above have resulted from supposed optimizations. But any code that creates non-determinism in exchange for speed, etc. is not an optimization, it's a bug. A node must implement its optimizations in a manner that does not alter consensus.
>> 
>> The BIP30 regression hard fork is not a case of non-determinism. This will produce deterministic results (apart from the impact of unrelated bugs). However the results are both a clear break from previous (and documented) consensus but also produce a very undesirable outcome - destruction of all unspent outputs in the "replaced" transaction for starters. So this is a distinct category, not a determinism bug but a hard fork that produces undesired consequences.
>> 
>> The BIP30 regression hard fork actually enables the various pathological scenarios that you were describing, where no such issues existed in Bitcoin consensus previously. It is now possible to produce a block that mutates another arbitrarily deep block, and forces a reorg all the way back to the mutated block. This was done to save microseconds per block. Despite the improbability of hash collisions, I find this deplorable and the lack of public discussion on the decision concerning.
>> 
>> With respect to the original post, the point at issue is the introduction of another hard fork, with some odd behaviors, but without any justification apart from tidying up the small amount of necessary code. These issues are related in that they are both consensus forks that have been introduced as supposed optimizations, with no public discussion prior to release (or at least merging to master with the presumption of shipping in the latter case). Two of the three hash collision issues above are also related in that they are bugs introduced by a desire to optimize internals.
>> 
>> The engineering lesson here should be clear - watch out for developers bearing optimizations. A trade against correctness is not an optimization, it's a break. Satoshi was clearly a fan of the premature optimization. FindAndDelete is a howler. So this is a tradition in Bitcoin. My intent is not to sling mud but to improve the situation.
>> 
>> It is very possible to produce straightforward and deterministic code that abides consensus and materially outperforms Core, without any of the above optimization breaks, even avoiding the utxo set optimization. Even the tx (memory) and block (orphan) pools are complex store denormalizations implemented as optimizations. Optimizing before producing a clean conceptual model architecture and design is a software development anti-pattern (premature optimization). The proposed fork is a premature optimization. There are much more significant opportunities to better organize code (and improve performance). I cannot support the decision to advance it.
>> 
>> I was unaware Core had regressed BIP30. Given that the behavior is catastrophic and that it introduces the *only* hash-collision consensus misbehavior (unless we consider a deep reorg sans the otherwise necessary proof of work desirable behavior), I strongly recommend it be reverted, with a post-mortem BIP.
>> 
>> Finally I recommend people contemplate the difference between unlikely and impossible. The chance of random collision is very small, but not zero. Colliding hashes is extremely difficult, but not impossible. But Bitcoin does not rely on impossibility for correct behavior. It relies of difficulty. This is a subtle but important distinction that people are missing.
>> 
>> Difficulty is a knowable quantity - a function of computing power.  If hash operations remain difficult, Bitcoin is undeterred. Collisions will have no impact, even if they happen with unexpected frequency (which would still be vanishingly infrequent). If the difficulty of producing a collision is reduced to the point where people cannot rely on addresses (for example), then Bitcoin has a problem, as it has become a leaky ship (and then there's mining). But with the unnecessary problems described above, a single hash collision can be catastrophic. Unlike difficulty, which is known, nobody can know when a single collision will show up. Betting Bitcoin, and potentially the world's money, on the unknowable is poor reasoning, especially given that the cost of not doing so is so very low.
>> 
>> e
>> 
>>> On Nov 17, 2016, at 10:08 AM, Johnson Lau <jl2012@xbt.hk> wrote:
>>> 
>>> The fact that some implementations ban an invalid block hash and some do not, suggests that it’s not a pure p2p protocol issue. A pure p2p split should be unified by a bridge node. However, a bridge node is not helpful in this case. Banning an invalid block hash is an implicit “first seen” consensus rule.
>>> 
>>> jl2012
>>> 
>>>> On 18 Nov 2016, at 01:49, Eric Voskuil <eric@voskuil.org> wrote:
>>>> 
>>>> Actually both possibilities were specifically covered in my description. Sorry if it wasn't clear.
>>>> 
>>>> If you create a new valid block out of an old one it's has potential to cause a reorg. The blocks that previously built on the original are still able to do so but presumably cannot build forever on the *new* block as it has a different tx. But other new blocks can. There is no chain split due to a different interpretation of valid, there are simply two valid competing chains.
>>>> 
>>>> Note that this scenario requires not only block and tx validity with a tx hash collision, but also that the tx be valid within the block. Pretty far to reach to not even get a chain split, but it could produce a deep reorg with a very low chance of success. As I keep telling people, deep reorgs can happen, they are just unlikely, as is this scenario.
>>>> 
>>>> If you create a new invalid block it is discarded by everyone. That does not invalidate the hash of that block. Permanent blocking as you describe it would be a p2p protocol design choice, having nothing to do with consensus. Libbitcoin for example does not ban invalidated hashes at all. It just discards the block and drops the peer.
>>>> 
>>>> e
>>> 
>>> 
> 
> 


-------------------------------------
On Thu, Aug 18, 2016 at 11:35 AM, Jonas Schnelli <dev@jonasschnelli.ch>
wrote:

> I agree that BIP70 is a mess (including the bitcoin:// additions). The
> proposed URI scheme would be completely different.


This reminds me https://xkcd.com/927/

I have some experience with hardware wallet development and its integration
and I know it's a mess. But it is too early to define such rigid standards
yet. Also, TREZOR concept (device as a server and the primary source of
workflow management) goes directly against your proposal of wallet software
as an workflow manager. So it is clear NACK for me.

slush

-------------------------------------
Luke, do you mean to replace the first 4 bytes of the second chunk (bytes
64..67 in 0-based counting) by the XOR of those 4 bytes with the first 4
bytes of the midstate? (I assume you don't care about 12 bytes but rather
those 4 bytes.)

This does not work. All it does is adding another computational step before
you can check for a collision in those 4 bytes. It makes finding a
collision only marginally harder.

On Wed, May 11, 2016 at 7:28 AM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Wednesday, May 11, 2016 12:20:55 PM Sergio Demian Lerner via bitcoin-dev
> wrote:
> > On Tue, May 10, 2016 at 6:43 PM, Sergio Demian Lerner <
> > sergio.d.lerner@gmail.com> wrote:
> > > You can find it here:
> > >
> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-blo
> > > ck-header/
> > >
> > > Basically, the idea is to put in the first 64 bytes a 4 byte hash of
> the
> > > second 64-byte chunk. That design also allows increased nonce space in
> > > the first 64 bytes.
> >
> > My mistake here. I didn't recalled correctly my own idea. The idea is to
> > include in the second 64-byte chunk a 4-byte hash of the first chunk, not
> > the opposite.
>
> What if we XOR bytes 64..76 with the first 12 bytes of the SHA2 midstate?
> Would that work?
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Okay.

I'm not really opposed to this BIP, but I am worried that fighting script
malleability is a battle that can never be won; even leaving one avenue of
malleability open is probably just as bad as having many avenues of
malleability, so it just doesn't seem worthwhile to me.

On Tue, Aug 16, 2016 at 8:18 PM, Gregory Maxwell <greg@xiph.org> wrote:

> On Tue, Aug 16, 2016 at 10:52 PM, Russell O'Connor via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > I see.
> >
> > But is it really necessary to soft fork over this issue?  Why not just
> make
> > it a relay rule?  Miners are already incentivized to modify transactions
> to
> > drop excess witness data and/or prioritize (versions of) transactions
> based
> > on their cost.  If a miner wants to mine a block with excess witness
> data,
> > it is mostly their own loss.
>
> Relay rules are quite fragile-- people build programs or protocols not
> expecting them to be violated, without proper error handling in those
> cases... and then eventually some miner rips them out because they
> simply don't care about them: not enforcing them won't make their
> blocks invalid.
>
> It's my general view that we should avoid blocking things with relay
> rules unless we think that someday they could be made invalid... not
> necessarily that they will, but that it's plausible. Then the
> elimination at the relay level is just the first exploratory step in
> that direction.
>
> One should also consider adversarial behavior by miners.  For example,
> I can mine blocks with mutated witnesses with a keyed mac that chooses
> the mutation. The key is shared by conspirators or customers, and now
> collectively we have a propagation advantage (since we know the
> mutated version before it shows up).  Not the _biggest_ concern, since
> parties doing this could just create their own new transactions to
> selectively propagate; but doing that would require leaving behind fee
> paying public transactions, while using malleability wouldn't.
>

-------------------------------------
I think we would be open to either leaving them in, or doing a separate
BIP.  What do others think?  I’d prefer to keep them together if the
changes are non-controversial just to cut down on #of BIP’s, but thats not
a strong preference.

On Fri, Mar 11, 2016 at 3:54 AM, Andreas Schildbach via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I think it's a bad idea to pollute the original idea of this BIP with
> other extensions. Other extensions should go to separate BIPs,
> especially since methods to clarify the fee have nothing to do with
> secure and authenticated bi-directional BIP70 communication.
>
>
> On 03/10/2016 10:43 PM, James MacWhyte via bitcoin-dev wrote:
> > Hi everyone,
> >
> > Our BIP (officially proposed on March 1) has tentatively been assigned
> > number 75. Also, the title has been changed to "Out of Band Address
> > Exchange using Payment Protocol Encryption" to be more accurate.
> >
> > We thought it would be good to take this opportunity to add some
> > optional fields to the BIP70 paymentDetails message. The new fields are:
> > subtractable fee (give permission to the sender to use some of the
> > requested amount towards the transaction fee), fee per kb (the minimum
> > fee required to be accepted as zeroconf), and replace by fee (whether or
> > not a transaction with the RBF flag will be accepted with zeroconf). I
> > know it doesn't make much sense for merchants to accept RBF with
> > zeroconf, so that last one might be used more to explicitly refuse RBF
> > transactions (and allow the automation of choosing a setting based on
> > who you are transacting with).
> >
> > I see BIP75 as a general modernization of BIP70, so I think it should be
> > fine to include these extensions in the new BIP, even though these
> > fields are not specific to the features we are proposing. Please take a
> > look at the relevant section and let me know if anyone has any concerns:
> >
> https://github.com/techguy613/bips/blob/master/bip-0075.mediawiki#Extending_BIP70_PaymentDetails
> >
> > The BIP70 extensions page in our fork has also been updated.
> >
> > Thanks!
> >
> > James
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev@lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>



-- 

Justin W. Newton
Founder/CEO
Netki, Inc.

justin@netki.com
+1.818.261.4248

-------------------------------------
On 11 May 2016 at 12:36, Henning Kopp <henning.kopp@uni-ulm.de> wrote:

> On Wed, May 11, 2016 at 11:21:10AM +0200, Jannes Faber via bitcoin-dev
> wrote:
> > On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <
> > bitcoin-dev@lists.linuxfoundation.org> wrote:
> >
> > > There is no way to tell from a block if it was mined with AsicBoost or
> > > not. So you don’t know what percentage of the hashrate uses AsicBoost
> at
> > > any point in time. How can you risk forking that percentage out? Note
> that
> > > this would be a GUARANTEED chain fork. Meaning that after you change
> the
> > > block mining algorithm some percentage of hardware will no longer be
> able
> > > to produce valid blocks. That hardware cannot “switch over” to the
> majority
> > > chain even if it wanted to. Hence you are guaranteed to have two
> > > co-existing bitcoin blockchains afterwards.
> > >
> > > Again: this is unlike the hypothetical persistence of two chains after
> a
> > > hardfork that is only contentious but doesn’t change the mining
> algorithm,
> > > the kind of hardfork you are proposing would guarantee the persistence
> of
> > > two chains.
> > >
> >
> > Assuming AsicBoost miners are in the minority, their chain will
> constantly
> > get overtaken. So it will not be one endless hard fork as you claim, but
> > rather AsicBoost blocks will continue to be ignored (orphaned) until they
> > stop making them.
>
> At least until a difficulty adjustment on the AsicBoost chain takes
> place. From that point on, both chains, the AsicBoost one and the
> forked one will grow approximately at the same speed.
>
>
No: you are still assuming AsicBoost miners would reject normal blocks.
They don't now and they would have to specifically code for that as a reply
to AsicBoost being banned. So there won't be two chains at all, only the
main chain with a lot (more than usual) of short (few blocks) forks. Each
forks starts anew, it's not one long fork. Therefore there is no
"difficulty adjustment on the AiscBoost chain".

Now if they do decide to ban non-AsicBoost blocks as a response to being
banned themselves, they're just another altcoin with a different PoW and no
one would have a reason to use them over Bitcoin (apart from maybe selling
those forked coins asap).

You're confused about what "longest" means as well: it's not just the
number of blocks, it's the aggregate difficulty that counts: so AsicBoost
would never become "longer" (more total work) either.

Hope this helps clear things up.

--
Jannes

-------------------------------------
On 2016-01-03 02:46, Marco Falke wrote:
> 2015-12-30 17:27 GMT+01:00  <joe2015@openmailbox.org>:
>> On 2015-12-30 18:33, Marco Falke wrote:
>>> 
>>> This is an interesting approach but I don't see how this is a soft
>>> fork. (Just because something is not a hard fork, doesn't make it a
>>> soft fork by definition)
>>> Softforks don't require any nodes to upgrade. [1]
>>> Nonetheless, as I understand your approach, it requires nodes to
>>> upgrade. Otherwise they are missing all transactions but the coinbase
>>> transactions. Thus, they cannot update their utxoset and are easily
>>> susceptible to double spends...
>>> 
>>> Am I missing something obvious?
>>> 
>>> -- Marco
>>> 
>>> 
>>> [1] https://en.bitcoin.it/wiki/Softfork#Implications
>> 
>> 
>> It just depends how you define "softfork".  In my original write-up I 
>> called
>> it a "generalized" softfork, Peter suggested a "firm" fork, and there 
>> are
>> some suggestions for other names.  Ultimately what you call it is not 
>> very
>> important.
>> 
>> --joe.
> 
> joe, indeed it is not important how you call it, but please, let's not
> call it "soft fork".

This kind of fork (whatever it is called) has all the traditional 
properties of a softfork except meaningful backwards compatibility for 
non-upgraded clients.  So I think it is reasonable to call it a softfork 
with some qualification.

> Besides my initial question about the coinbase
> tx, I was also wondering how non-updated nodes would verify the
> collected fees without the actual txs at hand. (They only have the
> coinbase tx, don't they?)

Yes this appears to be an oversight in my proof-of-concept 
implementation.  The unintended consequence being that all transactions 
would have to be zero-fee...

The simplest fix would be make the new rules add the fees implicitly.  
There are other solutions.

> Moreover, I can't see the benefits over a hard fork. A hard fork is
> much cleaner in regard to code changes. As one of the intends of
> "generalized soft forks" is to force user to update, at least a hard
> fork doesn't lie about the fact. Am I missing any obvious advantages
> of a "generalized soft fork" over a "clean" hard fork?

A "firm soft fork" also does not lie about that fact -- you must 
upgrade.  I don't see it dishonest if it was never claimed otherwise.

I agree that hardforks can be "cleaner".

However the obvious disadvantage of a hardfork is the risk of the 
network splitting between upgraded and non-upgraded clients.  This is 
not a problem if there is 100% consensus behind the hardfork, but I am 
not sure if 100% is realistically achievable for contentious issues such 
as the blocksize limit.

If 100% consensus is never achieved, then the options are:
1. Never upgrade and keep the blocksize limit unchanged forever.
2. Use a firm softfork to resolve the deadlock.
3. Hardfork anyway and split the network.

My argument is simply that 2 is better than 3 and possibly 1.

--joe



-------------------------------------
*One of my biggest fears about using any wallet is the "whoops, cosmic ray
flipped a bit while producing receiving address; SFYL!" possibility. For
high value cold storage, I always generate my addresses on two independent
machines using two different pieces of software. Am I nuts for doing that?*
A randomly flipped bit would be extremely unlikely to yield a valid
address, however, I still think it you are wise to use independent routes
to confirm that your addresses match the keys.  I do the same when I
generating my cold storage key pairs.  I think malicious address
substitution is an under appreciated attack vector.

Regarding this thread in general, would it make sense for this proposal to
include standards for multi-sig wallet interoperability?  A whole spectrum
of attacks would be made less likely - and easy for typical users to guard
against - by using wallets on separate devices AND where the wallet
software was written and provided by different parties.

On Mon, Aug 22, 2016 at 9:50 AM, Moral Agent via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> It would be nice if the detached signer and the normal wallet could both
> verify the correctness of generated addresses before you cause coins to be
> sent there.
>
> e.g. the hardware wallet could give its master public key to Bitcoin Core
> and you can thereafter generate your receiving addresses on Core, with the
> option to have the HW wallet validate them.
>
> One of my biggest fears about using any wallet is the "whoops, cosmic ray
> flipped a bit while producing receiving address; SFYL!" possibility. For
> high value cold storage, I always generate my addresses on two independent
> machines using two different pieces of software. Am I nuts for doing that?
>
> With the above scheme, you are pretty well protected from losing money if
> your HW wallet is defective. You could still lose it if the HW wallet was
> evil of course, but that strikes me as much more likely to be discovered
> quickly.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
If this is just encoding BIP-21 addresses, it is basically an "audio QR
code". In this case, does publishing it as a BIP still make sense? (Not
to imply that it doesn't, but it's something you should consider.)

Please look at existing implementations of audio modems when creating
your design. A lot of this work has been done many times before, so
there is a lot to learn from.

Your selected frequencies are harmonics of each other, meaning nonlinear
distortion will make detection more difficult. The Bell 202 and similar
modem standards chose AFSK frequencies to minimize interference.

Repeating a message multiple times is a very inefficient method of error
recovery. It works, but there may be better techniques, such as trellis
modulation or other convolutional codes.

Defining channel models to simulate your various use cases will help a
lot to determine if you have met your requirements.

- Thomas

P.S. I also briefly considered audio to exchange transactions with a
hardware wallet. Using GNU Radio made the implementation much easier.

On 08/09/2016 04:06 PM, Daniel Hoffman via bitcoin-dev wrote:
> I have updated the GitHub a lot (changed tones to be less chirpy, fixed
> some smalls) and made a couple of samples (see attachment for MP3 and
> FLAC of both tone tables, first 16 then 4). Is this good enough to
> warrant an official BIP number? I haven't built a decoder yet, but it
> seems like the encoder is working properly (looked at Audacity, seems
> like it is working), and some people on reddit want to "allow for
> decoding experiments"
> <https://www.reddit.com/r/btc/comments/4wsn7v/bip_proposal_addresses_over_audio_thoughts/d69m3st>
> 
> What suggestions do you all have for it?
> 
> On Mon, Aug 8, 2016 at 8:50 PM, Daniel Hoffman
> <danielhoffman699@gmail.com <mailto:danielhoffman699@gmail.com>> wrote:
> 
>     It wouldn't be feasible in the vast majority of cases, but I can't
>     think of a reason why it can't be built into the standard.
> 
>     On Mon, Aug 8, 2016 at 5:59 PM, Trevin Hofmann via bitcoin-dev
>     <bitcoin-dev@lists.linuxfoundation.org
>     <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>         Would it be feasible to transmit an entire BIP21 URI as audio?
>         If you were to encode any extra information (such as amount), it
>         would be useful to include a checksum for the entire message.
>         This checksum could possibly be used instead of the checksum in
>         the address.
> 
>         Trevin
> 
> 
>         On Aug 8, 2016 3:06 PM, "Justin Newton via bitcoin-dev"
>         <bitcoin-dev@lists.linuxfoundation.org
>         <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>             Daniel,
>                Thanks for proposing this.  I think this could have some
>             useful use cases as you state.  I was wondering what you
>             would think to adding some additional tones to optionally
>             denote an amount (in satoshis?).
> 
>             (FYI, actual link is here:  https://github.com/Dako300/BIP
>             <https://github.com/Dako300/BIP> )
> 
>             Justin
> 
>             On Mon, Aug 8, 2016 at 2:22 PM, Daniel Hoffman via
>             bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org
>             <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
> 
>                 This is my BIP idea: a fast, robust, and standardized
>                 for representing Bitcoin addresses over audio. It takes
>                 the binary representation of the Bitcoin address (little
>                 endian), chops that up into 4 or 2 bit chunks (depending
>                 on type, 2 bit only for low quality audio like american
>                 telephone lines), and generates a tone based upon that
>                 value. This started because I wanted an easy way to
>                 donate to podcasts that I listen to, and having a
>                 Shazam-esque app (or a media player with this
>                 capability) that gives me an address automatically would
>                 be wonderful for both the consumer and producer. Comes
>                 with error correction built into the protocol
> 
>                 You can see the full specification of the BIP on my
>                 GitHub page (https://github.com/Dako300/BIP-0153
>                 <https://github.com/Dako300/BIP-0153>).
> 
>                 _______________________________________________
>                 bitcoin-dev mailing list
>                 bitcoin-dev@lists.linuxfoundation.org
>                 <mailto:bitcoin-dev@lists.linuxfoundation.org>
>                 https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>                 <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
> 
> 
> 
> 
>             -- 
> 
>             Justin W. Newton
>             Founder/CEO
>             Netki, Inc.
> 
>             justin@netki.com <mailto:justin@netki.com>
>             +1.818.261.4248 <tel:+1.818.261.4248>
> 
> 
> 
>             _______________________________________________
>             bitcoin-dev mailing list
>             bitcoin-dev@lists.linuxfoundation.org
>             <mailto:bitcoin-dev@lists.linuxfoundation.org>
>             https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>             <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
> 
> 
>         _______________________________________________
>         bitcoin-dev mailing list
>         bitcoin-dev@lists.linuxfoundation.org
>         <mailto:bitcoin-dev@lists.linuxfoundation.org>
>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>         <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
> 
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
Hello List,

With SegWit approaching it would make sense to define a common derivation scheme how BIP44 compatible wallets will handle P2(W)SH (and later on P2WPKH) receiving addresses.
I was thinking about starting a BIP for it, but I wanted to get some feedback from other wallets devs first.

In my opinion there are two(?) different options: 

1) Stay with the current Bip44 account, give the user for each public key the option to show it as a P2PKH-Address or a P2SH address and also scan the blockchain for both representation of each public key.
	+) This has the advantage, that the user does not need to decide or have to understand that he needs to migrate to a new account type
	-) The downside is that the wallet has to scan/look for ever twice as much addresses. In the future when we have a P2WPKH, it will be three times as much.
	-) If you have the same xPub/xPriv key in different wallets, you need to be sure both take care for the different address types

2) Define a new derivation path, parallel to Bip44, but a different  'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user choose which account he want to add ("Normal account", "Witness account").  

	m / purpose' / coin_type' / account' / change / address_index

	+) Wallet needs only to take care of 1 address per public key
	+) If you use more than one wallet on the same xPub/xPriv it will work or fail completely. You will notice it immediately that there is something wrong
	-) User has to understand that (s)he needs to migrate to a new account to get the benefits of SegWit
	+) Thus, its easier to make a staged roll-out, only user actively deciding to use SegWit will get it and we can catch bugs earlier.
	
3) other ideas?

My personal favourite is pt2.

Has any Bip44 compliant wallet already done any integration at this point?

Thx,
Daniel/Mycelium



    




-------------------------------------
On Sun, May 15, 2016 at 5:08 AM, Daniel Weigl via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> > 0x40000000 would be the next available to specify witness addresses.
> > This is compatible with existing accounts and wallet layouts.
>
> my main concern here is that
>  -) every Bip<this-bip>-compatible wallet in the future will have to
> implement all (then probably) legacy derivation and tx schemes.
>

I can see the advantage of a segwit only scheme, but we will need to
support old derivations anyway for many decades if not indefinitely. People
are using it to store value for the long term.


>  -) it does not fail in a deterministic way, if I import a seed or
> xPriv/xPub across different capable wallets.
>         It is more visible if one account has [no funds/does not show up]
> at all after an import than if something shows up but you need to make sure
> that the balance is what you might expect.
>

This is certainly a downside. It has to be weighed against the benefit of
being able to upgrade existing wallets in place. Asking users to create a
new wallet, and replace their recovery phrase backups is an even bigger
problem in my estimation.

What do you think of doing both? A new BIP43 purpose number for segwit only
wallets, but that also specifies 0x40000000/1 for the change/receive index
so that the scheme is compatible with other schemes for upgrade existing
wallets in place? There will certainly be wallet developers who decide to
upgrade in place, but we can standardized both how to indicate segwit
chains, independent of segwit only schemes or upgrade schemes, and still
have the advantages of a new segwit only BIP43 purpose number.

Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com/>

-------------------------------------
Ethan Heilman <eth3rs@gmail.com> writes:
>>It's also not clear to me why the HMAC, vs just SHA256(key|cipher-type|mesg).  But that's probably just my crypto ignorance...
>
> SHA256(key|cipher-type|mesg) is an extremely insecure MAC because of
> the length extension property of SHA256.
>
> If I have a tag y = SHA256(key|cipher-type|mesg), I can without
> knowing key or msg compute a value y' such that
> y' = SHA256(key|cipher-type|mesg|any values I want).

Not quite, there's an important subtlety that SHA256 appends the
bitlength, so you can only create:

y' = SHA256(key|cipher-type|mesg|padding|bitlength|any values I want).

But we're not using this for a MAC in BIP151, we're using this to
generate the encryption keys.

Arthur Chen <arthur.chen@btcc.com> said:
> HMAC has proven security property.
> It is still secure even when underlying crypto hashing function has
> collision resistant weakness.
> For example, MD5 is considered completely insecure now, but HMAC-MD5 is
> still considered secure.
> When in doubt, we should always use HMAC for MAC(Message Authentication
> Code) rather than custom construction

Bitcoin already relies on SHA256's robustness, but again, we don't need
a MAC here.

I'm happy to buy "we just copied ssh" if that's the answer, and I can't
see anything wrong with using HMAC here, it just seems odd...

Thanks!
Rusty.


-------------------------------------
I'm curious to hear the answers to the questions Luke asked earlier. I also
read through the documentation and wasn't convinced it was thought out well
enough to actually build something on top of, but there's no reason it
can't get a number as a work-in-progress.

I hope it does continue to get worked on, though. The lack of response or
discussion worries me that it might become an abandoned project.

On Tue, Jul 5, 2016, 18:32 Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Tuesday, July 05, 2016 5:46:36 PM Peter Todd wrote:
> > On Thu, May 26, 2016 at 03:53:04AM +0000, Luke Dashjr via bitcoin-dev
> wrote:
> > > On Thursday, May 26, 2016 2:50:26 AM Nicolas Dorier via bitcoin-dev
> wrote:
> > > >   Author: Flavien Charlon <flavien@charlon.net>
> >
> > What's the status of this BIP? Will it be assigned?
>
> I was waiting for clarification on the Author thing, but Nicholas hasn't
> responded yet. I am unaware of any reason NOT to assign it, and there
> appear
> to be no objections, so let's call it BIP 160.
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Gregory Maxwell <greg@xiph.org> writes:
> On Tue, May 10, 2016 at 5:28 AM, Rusty Russell via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> I used variable-length bit encodings, and used the shortest encoding
>> which is unique to you (including mempool).  It's a little more work,
>> but for an average node transmitting a block with 1300 txs and another
>> ~3000 in the mempool, you expect about 12 bits per transaction.  IOW,
>> about 1/5 of your current size.  Critically, we might be able to fit in
>> two or three TCP packets.
>
> Hm. 12 bits sounds very small even giving those figures. Why failure
> rate were you targeting?

That's a good question; I was assuming a best-case in which we have
mempool set reconciliation (handwave) thus know they are close.  But
there's also an alterior motive: any later more sophisticated approach
will want variable-length IDs, and I'd like Matt to do the work :)

In particular, you can significantly narrow the possibilities for a
block by sending the min-fee-per-kb and a list of "txs in my mempool
which didn't get in" and "txs which did despite not making the
fee-per-kb".  Those turn out to be tiny, and often make set
reconciliation trivial.  That's best done with variable-length IDs.

> (*Not interesting because it mostly reduces exposure to loss and the
> gods of TCP, but since those are the long poles in the latency tent,
> it's best to escape them entirely, see Matt's udp_wip branch.)

I'm not convinced on UDP; it always looks impressive, but then ends up
reimplementing TCP in practice.  We should be well within a TCP window
for these, so it's hard to see where we'd win.

>> I would also avoid the nonce to save recalculating for each node, and
>> instead define an id as:
>
> Doing this would greatly increase the cost of a collision though, as
> it would happen in many places in the network at once over the on the
> network at once, rather than just happening on a single link, thus
> hardly impacting overall propagation.

"Greatly increase"?  I don't see that.

Let's assume an attacker grinds out 10,000 txs with 128 bits of the same
TXID, and gets them all in a block.  They then win the lottery and get a
collision.  Now we have to transmit ~48 bytes more than expected.

> Using the same nonce means you also would not get a recovery gain from
> jointly decoding using compact blocks sent from multiple peers (which
> you'll have anyways in high bandwidth mode).

Not quite true, since if their mempools differ they'll use different
encoding lengths, but yes, you'll get less of this.

> With a nonce a sender does have the option of reusing what they got--
> but the actual encoding cost is negligible, for a 2500 transaction
> block its 27 microseconds (once per block, shared across all peers)
> using Pieter's suggestion of siphash 1-3 instead of the cheaper
> construct in the current draft.
>
> Of course, if you're going to check your whole mempool to reroll the
> nonce, thats another matter-- but that seems wasteful compared to just
> using a table driven size with a known negligible failure rate.

I'm not worried about the sender: The recipient needs to encode all the
mempool.

>> As Peter R points out, we could later enhance receiver to brute force
>> collisions (you could speed that by sending a XOR of all the txids, but
>> really if there are more than a few collisions, give up).
>
> The band between "no collisions" and "infeasible many" is fairly
> narrow.  You can add a small amount more space to the ids and
> immediately be in the no collision zone.

Indeed, I would be adding extra bits in the sender and not implementing
brute force in the receiver.  But I welcome someone else to do so.

Cheers,
Rusty.


-------------------------------------
== Background

OP_PRANDOM is a new op code for Bitcoin that pushes a pseudo-random number
to the top of the stack based on the next N block hashes. The source of the
pseudo-random number is defined as the XOR of the next N block hashes after
confirmation of a transaction containing the OP_PRANDOM encumbered output.
When a transaction containing the op code is redeemed, the transaction
receives a pseudo-random number based on the next N block hashes after
confirmation of the redeeming input. This means that transactions are also
effectively locked until at least N new blocks have been found.


== Rational

Making deterministic, verifiable, and trustless pseudo-random numbers
available for use in the Script language makes it possible to support a
number of new smart contracts. OP_PRANDOM would allow for the simplistic
creation of purely decentralized lotteries without the need for complicated
multi-party computation protocols. Gambling is also another possibility as
contracts can be written based on hashed commitments, with the winner
chosen if a given commitment is closest to the pseudo-random number.
OP_PRANDOM could also be used for cryptographically secure virtual asset
management such as rewards in video games and in other applications.


== Security

Pay-to-script-hash can be used to protect the details of contracts that use
OP_PRANDOM from the prying eyes of miners. However, since there is also a
non-zero risk that a participant in a contract may attempt to bribe a miner
the inclusion of multiple block hashes as a source of randomness is a must.
Every miner would effectively need to be bribed to ensure control over the
results of the random numbers, which is already very unlikely. The risk
approaches zero as N goes up.

There is however another issue: since the random numbers are based on a
changing blockchain, its problematic to use the next immediate block hashes
before the state is “final.” A safe default for accepting the blockchain
state as final would need to be agreed upon beforehand, otherwise you could
have multiple random outputs becoming valid simultaneously on different
forks.

A simple solution is not to reveal any commitments before the chain height
surpasses a certain point but this might not be an issue since only one
version will eventually make it into the final chain anyway -- though it is
something to think about.


== Outro

I'm not sure how secure this is or whether its a good idea so posting it
here for feedback

Thoughts?

-------------------------------------
Dear all,

Bitcoin Unlimited’s market-based solution to the block-size limit is slowly winning support from node operators and miners.  With this increased attention, many people are asking for a better explanation of how Bitcoin Unlimited actually works.  The article linked below describes how Bitcoin Unlimited’s excessive-block logic works from the perspective of a single node. (I’m hoping to do a follow-up article that describe how this “node-scale” behavior facilitates the emergence of a fluid and organic block size limit at the network scale.)

https://medium.com/@peter_r/the-excessive-block-gate-how-a-bitcoin-unlimited-node-deals-with-large-blocks-22a4a5c322d4 <https://medium.com/@peter_r/the-excessive-block-gate-how-a-bitcoin-unlimited-node-deals-with-large-blocks-22a4a5c322d4>

Best regards,
Peter R
-------------------------------------
On Thu, Feb 11, 2016 at 06:03:18PM +1100, Patrick Shirkey via bitcoin-dev wrote:
> This is very useful information but from my experience it is not viable to
> have a full node running full time on a desktop system i.e sharing the
> system with a normal desktop workload.
> 
> With a very powerful "Desktop" machine bitcoin-qt dominates CPU/GPU
> resources. Surely the majority of nodes NOT running open ports are being
> run on desktop systems.  It's likely that the vast majority of the
> "normal/desktop" user base are not going to setup dedicated machines to
> run a full node full time.

I suspect you may be confusing full nodes with mining nodes. The two are
not directly synonymous. When running a full node in non-mining mode,
the CPU load is fairly light and the GPU is not touched at all. There is
a decent amount of RAM / disk used, but I've found that running a full
node on my low-power NAS box to be a nice way to use the extra idle CPU
time in a somewhat useful way (again, not mining). I've also run a full
node on a netbook without any trouble.

> It's likely that the vast majority of full nodes that are not running open
> ports are used occasionally when the user wants to make a transaction or
> "catch up" with the blockchain.
> 
> That creates a divide between those who do have the resources to
> contribute to the system on a full time basis (minority) and those who do
> not (majority).

Bitcoin is extremely tolerant of nodes entering and leaving the network
at will. Even part time nodes help to improve the quality of the network
purely by following the rules when passing on blocks / transactions
(i.e. preventing the propogation of erroneous or invalid data and
checking the proof of work of all present chains)

> Does the power of p2p decentralization lie with the vast majority or the
> "wealthy" resource rich minority?
> 
> How will the move to 2MB hard fork affect the vast majority of nodes?

They will need to upgrade, so yes it will affect every node.

> The rollout affect of the hard fork on the entire bitcoin ecosystem is a
> difficult process to plan in advance. It's not viable to simply rely on
> press releases to encourage users to upgrade their nodes. The debacle with
> Pulse Audio during the mid 2000's should be a lesson for those who seek
> that route.

This has been thoroughly discussed in other threads. Hard forks are not
done on a whim.


--Sean



-------------------------------------
I agree this is an interesting area of transaction malleability to still
consider in the future, and minimization of these areas of malleability
with regards to its impact on the p2p network should be easy to resolve
and (hopefully) well-understood by script writers in the future.

On Tue, Aug 16, 2016 at 12:43:32PM -0700, Peter Todd via bitcoin-dev wrote:
> Having said that, a better approach may be a separate CHECKBOOLVERIFY opcode
> that fails unless the top item on the stack is a minimally encoded true or
> false value, to allow script writers to opt into this behavior; it's not always
> ideal.

I think the biggest value of the proposed BIP behavior is that the cost
is lower for "doing it right" to create script enforcement of OP_TRUE or
OP_FALSE. It is already possible to enforce with 2 bytes pushing OP_TRUE
and then OP_EQUAL. Creating an "OP_CHECKBOOLVERIFY" definitely achieves
the same result, but at a 1-byte (insetad of 2-byte) cost to "do it
right", so there is the same incentive to save on the byte and push
potential DoS costs onto the network -- whereas enforcing OP_TRUE byte
in OP_IF would create costs for those who want to evaluate pushdata, so
that has to be explicitly opt-in from an optimization/convenience
standpoint.

-- 
Joseph Poon


-------------------------------------
If someone uses OP_EQUALVERIFY after OP_COUNT_ACKS then the transaction
probably won't be able to be included at a different height.

On Oct 2, 2016 19:16, "Sergio Demian Lerner" <sergio.d.lerner@gmail.com>
wrote:

> It can be included at another block at a differnt height. It can be
> included anytime during the liveness period which starts 100 blocks later
> than the poll period ends. I'm reading the BIP now and it's true that this
> is not enterily clear. I will try to clarify.
>
>
> On Sun, Oct 2, 2016 at 7:58 PM, Russell O'Connor <roconnor@blockstream.io>
> wrote:
>
>> A related problem is that if this transaction is reorged out during an
>> innocent reorg, one that doesn't involve a double spend, the transaction
>> may never get back in unless it occurs at exactly  the same height, which
>> is not guaranteed.
>>
>> This affects fungabity of coins generated from these transactions.
>>
>> On Oct 2, 2016 18:37, "Sergio Demian Lerner" <sergio.d.lerner@gmail.com>
>> wrote:
>>
>>>
>>>
>>> On Sun, Oct 2, 2016 at 6:46 PM, Russell O'Connor via bitcoin-dev <
>>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>
>>>>
>>>> But I would argue that in this scenario, the only way it
>>>>> would become invalid is the equivalent of a double-spend... and
>>>>> therefore it
>>>>> may be acceptable in relation to this argument.
>>>>>
>>>>
>>>> The values returned by OP_COUNT_ACKS vary in their exact value
>>>> depending on which block this transaction ends up in.  While the proposed
>>>> use of this operation is somewhat less objectionable (although still
>>>> objectionable to me), nothing stops users from using OP_EQUALVERIFY and and
>>>> causing their transaction fluctuate between acceptable and unacceptable,
>>>> with no party doing anything like a double spend.  This is a major problem
>>>> with the proposal.
>>>>
>>>
>>> Transactions that redeem an output containing (or referencing by means
>>> of P2WSH) an OP_COUNT_ACKS are not broadcast by the network. That means
>>> that the network cannot be DoS attacked by flooding with a transaction that
>>> will not verify due to being too late.
>>> The only parties that can include the redeem transaction are the miners
>>> themselves.
>>> Therefore I see no problem that an OP_COUNT_ACKS scriptSig transaction
>>> is invalidated after the liveness times expires.
>>> If there is no expiration, then polls can last forever and the system
>>> fails to provide DoS protection for block validation since active polls can
>>> accumulate forever.
>>>
>>>
>>>
>>>
>

-------------------------------------
Interesting and thx for sharing what a DPL  is.

gm
________________________________
From: Peter Todd via bitcoin-dev<mailto:bitcoin-dev@lists.linuxfoundation.org>
Sent: ‎10/‎13/‎2016 4:51 AM
To: bitcoin-dev@lists.linuxfoundation.org<mailto:bitcoin-dev@lists.linuxfoundation.org>
Cc: defensivepatent@gmail.com<mailto:defensivepatent@gmail.com>
Subject: [bitcoin-dev] Defensive Patent License Offer Notice

Also published at https://petertodd.org/2016/defensive-patent-license-offer,
and Bitcoin txid b4bf94f5c457d080924aa163106d423670373cfe3b10f8ec00742c2234b01b72

    -----BEGIN PGP SIGNED MESSAGE-----
    Hash: SHA256

    I, Peter Todd, hereby declare myself and all technology companies that I
    control as "Defensive" by committing to offer a Defensive Patent License,
    version 1.1, for any of my patents either existing or future, to any DPL User.
    Neither I nor any companies that I control have any patents at this time.

    My contact address is pete@petertodd.org

    -----BEGIN PGP SIGNATURE-----

    iQEcBAEBCAAGBQJX/t11AAoJEGOZARBE6K+yR00H/0xp3oO7FiMvM4pjfoHZPPOa
    m3KjT4RSbFQLa9uniz0u/9bkc5I70CggkY3jtNLtDMbMBTwcMP61ABsvx+5y2gGD
    zE6VZ9DPcHVg/Eup6WSBlQO3HQKuFVz7vXSMuaidG7A+fpkU71SjDpB4M6hdvWnS
    +L9XBQ1GtQe0lSM73s4mld/IvB1giwPN1bOheQ9koYcQjj+B8PWyt2gIUwctxyvA
    7bC+KtCQT4RJPsQHbHx569CDkyIi3dNt0rTjCo5bOeUKrJF7eA3YktYdTJefZ+Rf
    00dbRZMslrg3dW9VWECfC0xC/kn+heStJ7WqJJKqYWo4apm6IiKPZxlwIcVscF0=
    =xrPk
    -----END PGP SIGNATURE-----

# Notes

* On the advice of my lawyer, I'm currently offering only a specific version of
  the DPL. I probably will offer licenses under subsequent versions in the
  future, but I'd prefer to see how the DPL evolves and whether or not the
  license stewards behind it prove trustworthy before committing to doing so.

* The language "all technology companies I control" is there to avoid any
  complications with non-technology companies that I may control in the future,
  e.g. family real-estate holding companies, and the non-profit caving group
  I'm a part of. To my knowledge, I only control one company as of writing, the
  numbered company I do all my consulting through; I consider that company a
  "technology company", and thus the above offer applies to it.

* Equally, if by some stroke of luck I do end up in control of any other
  technology companies - maybe Bill Gate's blockchain smart-contract will
  mysteriously gives me control of Microsoft - then the above offer will apply.

--
https://petertodd.org 'peter'[:-1]@petertodd.org
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev@lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
On Sat, Sep 10, 2016 at 12:42:30AM +0000, Gregory Maxwell via bitcoin-dev wrote:
> The alert system was a centralized facility to allow trusted parties
> to send messages to be displayed in wallet software (and, very early
> on, actually remotely trigger the software to stop transacting).

<snip>

> One of the facilities in the alert system is that you can send a
> maximum sequence alert which cannot be overridden and displays only a
> static key compromise text message and blocks all other alerts. I plan
> to send a triggering alert in the not-distant future (exact time to be
> announced well in advance) feedback on timing would be welcome.
> 
> There are likely a few production systems that automatically shut down
> when there is an alert, so this risks some small one-time disruption
> of those services-- but none worse than if an alert were sent to
> advise about a new system upgrade.
> 
> At some point after that, I would then plan to disclose this private
> key in public, eliminating any further potential of reputation attacks
> and diminishing the risk of misunderstanding the key as some special
> trusted source of authority.

ACK

Good to do this sooner rather than later, as alert propagation on the P2P
network is going to continue to get less reliable as nodes upgrade to software
that has removed alert functionality; better that the final alert key
retirement message is reliably seen by the remaining software out there in a
predictable way than this be something that happens unpredictably.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
Well, here's another idea: we could shorten the tx hashes to about 4 to 6 bytes instead of 32.

Let's say we have a 1 GB mempool with 2M transactions in it. A 4 byte shorthash would have a 0.046% chance of resulting in a collision with another transaction in our mempool, assuming a random distribution of hash values.

Of course, an attacker might construct transactions specifically for collisions. To protect against that, we set up a different salt value for each connection, and for the INV message, we use a 4 to 6 byte salted hash instead of the full thing. In case a peer does have a collision with one salt value, there are still 7 other peers with different salt values. The probability that they all fail is about 2.2e-27 with a 4-byte hash for a single peer. If we have 500,000 full nodes and 1M transactions per 10 minutes, the chance is 1.1e-15 that even one peer misses even one transaction.

This strategy would come with about 12 bytes of additional memory overhead per peer per tx, or maybe a little more. In exchange for that 12 bytes per peer*tx, we would save up to 28 bytes per peer*tx of network bandwidth. In typical conditions (e.g. 100-ish MB mempool, 16 peers, 2 MB blocks, 500 B serialized tx size), that could result in 1.792 MB net traffic saved per block (7.7 GB/month) at the expense of 12 MB of RAM. Overall, this technique might have the ability to reduce INV traffic by 5-8x in the asymptotic case, or maybe 2-3x for a realistic case.

I know short hashes like this have been proposed many times before for block propagation (e.g. by Gavin in his O(1) scaling gist, or in XTB). Has anyone else thought of using them like this in INV messages? Can anyone think of any major problems with the idea?

-------------------------------------
2016-01-27 3:45 GMT+01:00 Warren Togami Jr. <wtogami@gmail.com>:
> On Tue, Jan 26, 2016 at 9:42 AM, Luzius Meisser via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>> Idea: currently, the total amount of fees collected in a block is paid
>> out in full to whoever mined that block. I propose to only pay out,
>> say, 10% of the collected fees, and to add the remaining 90% to the
>> collected fees of the next block. Thus, the payout to the miner
>> constitutes a rolling average of collected fees from the current and
>> past blocks.
>
> [...] Another major issue with mandatory sharing is
> if the miner doesn't want to share, nothing stops them from taking payment
> out-of-band and confirming the transaction with little or no fees visible in
> the block.

While I find the other points you raised debatable, the out-of-band
argument looks strong enough to kill the idea. To work around it, one
would need to create rules about the transactions that can be included
in a block, for example by mandating that all included transactions
must have a fee at least as high as 0.9 times the 5th percentile of
the transactions in the previous 10 blocks. However, having to tell
the miners what fees they are allowed to accept destroys some of the
elegance of the idea. Maybe I should put it to rest for now and see if
a more elegant solution comes to mind later.

> While I don't agree with the rest of your logic, it is agreeable that you
> care about aligning the miner's supply incentives with the global marginal
> cost.  If you believe that is an important goal, you might like the Flex Cap
> approach as presented by Mark Friedenbach at Scaling Bitcoin Hong Kong.
> Under the general idea of the Flex Cap approach block size is no longer
> fixed, it can be bursted higher on a per-block basis if the miner is willing
> to defer a tiny portion of the current block subsidy to pay out to the miner
> of later blocks.
> [...]
> Flex Cap is an area of ongoing research that I strongly believe would
> benefit Bitcoin in the long-term.  For this reason it requires careful study
> and simulations to figure out specifics.

I agree that flex cap is promising. However, for it to be a viable
long-term solution, it must not depend on significant block subsidies
to work as the block subsidy will become less and less relevant over
time.

Picking up your thoughts, I guess this is how flex cap should be done:
1. There is a flexible block cap (e.g. 1 MB). This first MB is free to fill.
2. Miners can buy additional space for an exponentially increasing
fee. For example, the first KiB might cost 200 Satoshis, the second
KiB 400 Satoshis, the tenth KiB 102400 Satoshis etc.
3. The price of the purchased space is subtracted from the collected
fees and added to the reward of the next block.
4. The amount miners are willing to spend on additional space allows
to calculate the marginal costs of a transaction of a miner. For
example, if a miner pays 6000 Satoshis to include a 1 KB transaction
with a fee of 6100 Satoshis, the marginal costs must be below 100
Satoshis, assuming a rational miner. This cost is multiplied by say 50
to account for the costs of decentralization to get a global cost
estimate of 5000 Satoshis per KB.
5. Every 1000 blocks or so, the basic cap is adjusted upwards or
downwards (e.g. by 10%) depending on whether the average fees per KB
were above or below the global cost estimate.

Under such a scheme, prices should get very close to free market
prices. However, ruthless competition can get ugly in markets where
fixed costs dominate. We can currently witness this in the oil
industry. Thus, from an economic point of view, it might be more
advisable to simply let miners vote on block size, as has been
proposed by others. The drawback of voting is that it allows miners to
enforce a cartel among themselves and to charge monopoly prices
instead of competitive prices. However, monopoly prices would already
be much better than having an artificial cap.

Warren, thank you for your thoughts! I appreciate the opportunity to
discuss ideas at such a high level.

--
Luzius Meisser
President of Bitcoin Association Switzerland
MSc in Computer Science and MA in Economics


-------------------------------------
On Thu, Jun 23, 2016 at 02:01:10PM +0200, Pieter Wuille wrote:
> On Thu, Jun 23, 2016 at 1:39 PM, Peter Todd <pete@petertodd.org> wrote:
> > On Thu, Jun 23, 2016 at 01:30:45PM +0200, Pieter Wuille wrote:
> > For the record, I think the idea of the bips repo being a pure publication
> > platform isn't a good one and doesn't match reality; like it or not by
> > accepting bips we're putting a stamp of some kind of approval on them.
> 
> We? I don't feel like I have any authority to say what goes into that
> repository, and neither do you. We just give technical opinion on
> proposals. The fact that it's under the bitcoin organization on github
> is a historical artifact.

That's simply not how the rest of the community perceives bips, and until we
move them elsewhere that's not going to change.

No matter how much we scream that we don't have authority, the fact of the
matter is the bips are located under the github.com/bitcoin namespace, and we
do have editorial control over them.

> > I have zero issues with us exercising editorial control over what's in the bips
> > repo; us doing so doesn't in any way prevent other's from publishing elsewhere.
> 
> Editorial control is inevitable to some extent, but I think that's
> more a matter of process than of opinion. Things like "Was there
> community discussion?", "Is it relevant?", "Is there a reference
> implementation?". I don't think that you objecting for moral reasons
> to an otherwise technically sound idea is a reason for removal of a
> BIP. You are of course free to propose alternatives, or recommend
> against its usage.

Right, so you accept that we'll exert some degree of editorial control; the
question now is what editorial policies should we exert?

My argument is that rejecting BIP75 is something we should do on
ethical/strategic grounds. You may disagree with that, but please don't troll
and call that "advocating censorship"

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
All of this is already implemented in the bitcoind and bitcoin gui.

The theoretic minimum for the prune target would be 0 (just the header
of the current best block) as Bitcoin Core already stores the
chainstate (about 2 GiB) regardless of what you set for -prune=<N>.

In practice, the minimum is 510, so reorgs and small rescans (may not
be implemented in 0.12) are still possible.

The clients won't let you set it below that target:
"Prune configured below the minimum of 550 MiB. Please use a higher number."

Also, keep in mind Bitcoin Core comes with a help message explaining
-prune and other command line options

--Marco

2016-01-25 13:27 GMT+01:00 xor--- via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org>:
> On Monday, January 25, 2016 01:03:17 PM Wladimir J. van der Laan wrote:
>> > If yes, I would highly recommend advertising it in the new release notes -
>> > as said, the disk space reduction is a big deal.
>>
>> Good idea, has been added by Marco Falke in commit fa31133,
>
> Thanks. The RC2 changelog now says:
>
>> To enable block pruning set prune=<N> on the command line or in
>> bitcoin.conf, where N is the number of MiB to allot for raw block & undo
>> data.
>
> From having read the Bitcoin whitepaper quite a few months ago ago, I have the
> very very basic understanding that pruning is meant to:
> - delete old transaction data which merely "moves coins around"
> - instead only store the "origin" (= block where coins were mined) and
> "current location" of the coins, i.e. the unspent transactions. Notably, I
> understood it as "this is as secure as storing everything, since we know where
> the coins were created, and where they are".
>
> So from that point of view, I would assume that there is a "natural" amount of
> megabytes which a fully pruned blockchain consists of: It would be defined by
> the final amount of unspent coins.
> I thereby am confused why it is possible to configure a number of megabytes
> "to allot for raw block & undo data". I would rather expect pruning just to be
> a boolean on/off flag, and the number of megabytes to be an automatically
> computed result from the natural size of the dataset.
> And especially, I fear that I could set N too low, and as a result, it would
> delete "too much". I mean could this result in even security relevant
> transaction data being deleted?
>
> Thus, it would be nice if you could yet once more edit the release notes to:
> - explain why a N must be given
> - what a "safe" value of N is. I.e. how large must N be at least to not delete
> security-relevant stuff?
> - maybe mention if there is a "auto" setting for N to ensure that it choses a
> safe value on its own?
>
> Sorry if my descriptions are from a layman's point of view. I intentionally
> did *not* re-read the Bitcoin whitepaper to have a better understanding:
> I think having a layman's understanding is a good usability test for such
> stuff.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
Thomas,

I like your idea about expanding Bitcoin URI's to include signatures. For
BIP75 store and forward servers we are already thinking the DNS record
would have the user's public key as well as the URL of their store and
forward endpoint, so as soon as that becomes a standard you could use it
just for the public key part. Expanding the Bitcoin URI should be done as
well, for people who want to go the simpler route and not rely on servers.

Erik, Andy, everyone else,

I don't understand why subscriptions would need to be built into the
protocol. With BIP75 the merchant could automatically issue a
PaymentRequest message every X amount of time, and the customer's wallet
would either display the request like normal or be set to pre-authorize
requests from the merchant. If the merchant goes out of business, the
requests would stop coming. This sounds like a UI issue and not a
protocol-level requirement.

If you think I'm wrong, please explain why :)

On Wed, Jun 22, 2016 at 12:35 PM Erik Aronesty via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> - Payment channels seem clearly inappropriate for things like monthly
> subscriptions, the use of nlocktime, etc.
>
> - Merchants cannot send requests to users for future payments, because
> users don't run servers that they can connect to.  That's why BIP0070 works
> the way it does.
>
> - Need to have an interval for subscriptions, at a minimum, and stored in
> the wallet so next months payment can go out on time
>
> - Support for varying currency conversion needs to be baked in to
> wallets.   Fortunately, by adding advisory subscription info to the
> paymentrequest, this is left up to the wallet to
> secure/validate/repeat/convert/etc. as needed for each subscription.
>
> - The UI you describe is nice - but not unique to the solution.
>
>
>
>
> On Wed, Jun 22, 2016 at 12:20 PM, Andy Schroder <info@andyschroder.com>
> wrote:
>
>> I understand the need for people to make repeated payments to individuals
>> in real life that they know, without the payee every even taking the effort
>> to make a formal payment request (say you're just paying a family member of
>> friend back for picking something up for you at the store, and you've
>> already payed them many times before).
>>
>> For a subscription, wouldn't it be better to promote payment channels or
>> just send another payment request? I've been brainstorming recently about a
>> model where service providers could deliver invoices, receipts, and payment
>> requests in a standardized and secure way. In addition to having a send,
>> receive, and transaction history tab in your bitcoin wallet, you'd also
>> have an open payment channels tab (which would include all applications on
>> your computer that have an open real time payment channel, such as a wifi
>> access point, web browser, voip provider, etc.), as well as a "bills to
>> pay" tab. Since everything would be automated and consolidated locally, you
>> wouldn't have to deal with logging into a million different websites to get
>> the bills and then pay them. If it were this easy, why would you ever want
>> to do a recurring payment from a single payment request? I understand why
>> you may think you want to given current work flows, but I'm wondering if it
>> may be better to just skip over to a completely better way of doing things.
>>
>>
>> Andy Schroder
>>
>>
>> On 06/22/2016 11:30 AM, Erik Aronesty wrote:
>>
>>> My conclusion at the bottom of that post was to keep BIP 75 the same,
>>> don't change a bit, and stick any subscription information (future payment
>>> schedule) in the PaymentACK.   Then the wallet then re-initiates an invoice
>>> (unattended or attended.. up to the user), after the subscription interval
>>> is passed. Subscriptions are pretty important for Bitcoin to be used as a
>>> real payment system.
>>>
>>
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------


> On Jun 28, 2016, at 10:14 PM, Peter Todd <pete@petertodd.org> wrote:
> 
>> On Tue, Jun 28, 2016 at 08:35:26PM +0200, Eric Voskuil wrote:
>> Hi Peter,
>> 
>> What in this BIP makes a MITM attack easier (or easy) to detect, or increases the probability of one being detected?
> 
> BIP151 gives users the tools to detect a MITM attack.
> 
> It's kinda like PGP in that way: lots of PGP users don't properly check keys,

PGP requires a secure side channel for transmission of public keys. How does one "check" a key of an anonymous peer? I know you well enough to know you wouldn't trust a PGP key received over an insecure channel.

All you can prove is that you are talking to a peer and that communications in the session remain with that peer. The peer can be the attacker. As Jonas has acknowledged, authentication is required to actually guard against MITM attacks.

> so an attacker won't have a hard time MITM attacking those users. But some
> users do check keys, a labor intensive manual process, but not a process that
> requires any real cryptographic sophistication, let alone writing any code.
> It's very difficult for widescale attackers to distinguish the users who do
> check keys from the ones that don't, so if you MITM attack _any_ user you run
> the risk of running into one of the few that does check, and those users can
> alert everyone else.
> 
> The key thing, is we need to get everyones communications encrypted first: if
> we don't the MITM attacker can intercept 99% of the communications with 0% risk
> of detection, because the non-sophisticated users are trivially distinguishable from the sophisticated users: just find the users with unencrypted
> communications!
> 
> -- 
> https://petertodd.org 'peter'[:-1]@petertodd.org


-------------------------------------
On Sun, Dec 4, 2016 at 7:34 PM, Johnson Lau via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Something not yet done:
> 1. The new merkle root algorithm described in the MMHF BIP
>

Any new merkle algorithm should use a sum tree for partial validation and
fraud proofs.

Is there something special about 216 bits?  I guess at most 448 bits total
means only one round of SHA256.  16 bits for flags would give 216 for each
child.

Even better would be to make the protocol extendable.  Allow blocks to
indicate new trees and legacy nodes would just ignore the extra ones.  If
Bitcoin supported that then the segregated witness tree could have been
added as a easier soft fork.

The sum-tree could be added later as an extra tree.


> 3. Communication with legacy nodes. This version can’t talk to legacy
> nodes through the P2P network, but theoretically they could be linked up
> with a bridge node
>

The bridge would only need to transfer the legacy blocks which are coinbase
only, so very little data.


> 5. Many other interesting hardfork ideas, and softfork ideas that works
> better with a header redesign
>

That is very true.

-------------------------------------
 A new BIP is proposed to prevent excessive O(n^2) SignatureHash operation.

https://github.com/jl2012/bips/blob/sighash/bip-sighash.mediawiki
https://github.com/bitcoin/bitcoin/pull/8755 (Tight estimation)
https://github.com/bitcoin/bitcoin/pull/8756 (Loose estimation)

Two methods of sighash size estimation are proposed, with different tradeoff. The tight estimation is more permissive (disabling less txs) but require disabling of OP_CODESEPARATOR, FindAndDelete, and unusual nHashType. The loose estimation is less permissive (may disable more big and strange txs) but does not depend on further policy/consensus rules. With either type of estimation, normal standard txs (<100kB, P2PK, P2PKH, canonical bare or P2SH multisig) are totally unaffected by this BIP.

  BIP: ?
  Title: Limiting excessive SignatureHash operation
  Author: Johnson Lau <jl2012@xbt.hk>
  Status: Draft
  Type: Standards Track
  Created: 2016-09-21


Abstract

This proposal defines a new type of block-level resources limit, with several (optional) script restrictions, to prevent excessive SignatureHash operation.

Introduction

There are 4 ECDSA signature verification codes in the Bitcoin script system: CHECKSIG, CHECKSIGVERIFY, CHECKMULTISIG, CHECKMULTISIGVERIFY (“sigops”). According to the SIGHASH type, a transaction digest (sighash) is generated with a double SHA256 of a serialized subset of the transaction, with a function called SignatureHash, and the signature is verified against this sighash with a given public key. Due to a design weakness, the amount of data hashing in SignatureHash is proportional to the size of the transaction. Therefore, data hashing grows in O(n2) as the number of sigops in a transaction increases. While a 1 MB block would normally take 2 seconds to verify with an average computer in 2015, a 1 MB transaction with 5569 sigops may take 25 seconds to verify. [1][2][3]

BIP143 fixes this problem by introducing a new SignatureHash algorithm in segregated witness transactions. However, it would not be able and is not intended to fix the problem of pre-segregated witness transactions. This document proposes a new type of block-level resources limit to prevent excessive SignatureHash operation. However, the calculation of sighash is a complicated process involving several consensus-critical procedures, including the use of OP_CODESEPARATOR, the FindAndDelete function, and the interpretation of nHashType. A correct limitation should be made based on the effects of these procedures.

Specification

Transaction hashable size

Transaction hashable size (TxHashableSize) is defined as the size of a transaction, which:

is serialized without witness data (BIP144), and
has scriptSig in all inputs replaced by zero-size script
TxHashableSize is an estimation of the amount of data hashed with a SIGHASH_ALL. Without counting the size of scriptCode and nHashType, it always underestimates the size. However, the difference is negligible since it grows linearly with the number of sigops.
int64_t GetTransactionHashableSize(const CTransaction& tx)
{
    int64_t size = ::GetSerializeSize(tx, SER_NETWORK, PROTOCOL_VERSION | SERIALIZE_TRANSACTION_NO_WITNESS);
    for (unsigned int i = 0; i < tx.vin.size(); i++) {
        int64_t scriptSigSize = tx.vin[i].scriptSig.size();
        size -= scriptSigSize;
        // If the scriptSig size is larger than 252, 2 bytes compactSize encoding is deducted.
        if (scriptSigSize > 252)
            size -= 2;
        /*
         * Theoretically, 4 bytes should be deducted if the scriptSig is larger than 65535 bytes,
         * and 8 bytes should be deducted if it is larger than 4294967295 bytes.
         * However, scriptSig larger than 10000 bytes is invalid so it is not needed.
         */
    }
    return size;
}
SignatureHash equivalent operation

SignatureHash equivalent operation (SigHashOp) is defined as the maximum possible number of times which a sigop would perform SignatureHash at the TxHashableSize. Depends on whether some extra restrictions in script use are enforced, there are 2 ways to estimate the SigHashOp:

Loose estimation: A loose SigHashOp estimation does not depend on any extra script restrictions. It assumes that SignatureHash is performed not more than once for every ECDSA signature passing to a sigop. It looks for all sigops in a transaction (even in an unexecuted conditional branch), in scriptSig, in scriptPubKey of the outputs being spent, and in redeemScript of P2SH transactions. Each OP_CHECKSIG or CHECKSIGVERIFY is counted as 1 SigHashOp. Each OP_CHECKMULTISIG or CHECKMULTISIGVERIFY is counted as 20 SigHashOp, unless all the following conditions are satisfied:

The OP_CHECKMULTISIG or CHECKMULTISIGVERIFY is immediately preceded by a value push as OP_n (1 ≤ n ≤ 16), denoting the number of public keys (n).
The n opcodes preceding the OP_n must be push type (i.e. 0x00 ≤ opcode ≤ 0x60), as the public key(s)
The opcodes preceding the public key(s) is a OP_m, with 1 ≤ m ≤ 16 and m ≤ n, denoting the number of signatures (m)
If all these conditions are met, the OP_CHECKMULTISIG or CHECKMULTISIGVERIFY is counted as m SigHashOp.
SigHashOp of a transaction is the sum of SigHashOp of each of its inputs.

unsigned int CScript::GetSigHashOpCount() const
{
    unsigned int n = 0;
    const_iterator pc = begin();
    std::vector<opcodetype> pushOpcodes;
    while (pc < end())
    {
        opcodetype opcode;
        if (!GetOp(pc, opcode))
            break; // The script is invalid
        if (opcode == OP_CHECKSIG || opcode == OP_CHECKSIGVERIFY)
            n++;
        else if (opcode == OP_CHECKMULTISIG || opcode == OP_CHECKMULTISIGVERIFY) {
            unsigned int nKey = 0;
            unsigned int nSig = 0;
            // We assume a CHECKMULTISIG will hash the transaction for 20 times, unless it is in some canonical form.
            n += 20;
            // The number of keys must be k = 1 to 16 denoted by OP_k
            if (pushOpcodes.size() >= 3 && pushOpcodes.back() >= OP_1 && pushOpcodes.back() <= OP_16) {
                nKey = DecodeOP_N(pushOpcodes.back());
                // All the k + 2 opcodes before the CHECKMULTISIG must be push only
                if (pushOpcodes.size() >= nKey + 2) {
                    opcodetype nSigCode = pushOpcodes.at(pushOpcodes.size() - nKey - 2);
                    // The number of signatures must be k = 1 to 16 denoted by OP_k, and not larger than number of keys
                    if (nSigCode >= OP_1 && nSigCode <= OP_16) {
                        nSig = DecodeOP_N(nSigCode);
                        if (nSig <= nKey)
                            // We use the number of signatures as the SigOpCount of this CHECKMULTISIG
                            n = n - 20 + nSig;
                    }
                }
            }
        }

        if (opcode <= OP_16)
            pushOpcodes.push_back(opcode);
        else
            pushOpcodes.clear();
    }
    return n;
}
Tight estimation: An tight SigHashOp estimation depends on these extra consensus rules for pre-segregated witness scripts:

nHashType is confined to only 6 types: 0x01 for SIGHASH_ALL, 0x02 for SIGHASH_NONE, 0x03 for SIGHASH_SINGLE, 0x81 for SIGHASH_ALL|SIGHASH_ANYONECANPAY, 0x82 for SIGHASH_NONE|SIGHASH_ANYONECANPAY, and 0x83 for SIGHASH_SINGLE|SIGHASH_ANYONECANPAY. A signature with other nHashType is invalid.
Script with OP_CODESEPARATOR, even in an unexecuted conditional branch, is invalid.
Script that involves non-zero FindAndDelete results is invalid.
It also assumes that SignatureHash is performed not more than once in each script for each nHashType.
SigHashOp is counted in the same way as the loose estimation. However, if the SigHashOp for a script is found to be larger than 3, it is counted as only 3 SigHashOp.[4] The SigHashOp of a transaction is the sum of SigHashOp of each of its inputs.

SigHashOp of the generating transaction is defined to be 0.

SignatureHash size

SignatureHash size (SigHashSize) of a transaction is the product of TxHashableSize and SigHashOp.

SigHashSize of a block is the sum of SigHashSize of all transactions in the block.

Consensus and policy limits for SigHashSize

A new consensus rule is enforced to require that SigHashSize of a block MUST NOT be larger than 500,000,000 (500MB). Consequently, SigHashSize of a valid transaction MUST NOT be larger than 500MB.

A new relay and mempool policy is recommended to reject any unconfirmed transaction that has a SigHashSize to Transaction weight ratio larger than 90. This policy limit is equivalent to 36MB SigHashSize for a 100kB non-segregated witness transaction, or 360MB for a full block of such transactions.

Rationale

Static analysis

This proposal employs a static analysis approach to estimate SigHashSize of transactions and blocks. This allows early rejection of violating transactions and blocks without executing the scripts at all. Despite that the size of scriptCode and nHashType are not considered in the estimation, the difference is negligible comparing with size of the main transaction body, since the overheads grows linearly with the number of sigops, which is mostly restricted by the 80,000 sigop limit (BIP141). [5]

Loose SigHashOp estimation

The loose SigHashOp estimation assumes that SignatureHash is performed not more than once for every ECDSA signature passing to a sigop. This assumption is obviously correct for OP_CHECKSIG and CHECKSIGVERIFY since they would never perform SignatureHash more than once, while no SignatureHash would be performed if they happen in an unexecuted conditional branch, or if the signature is an empty vector. This assumption is also correct for OP_CHECKMULTISIG and CHECKMULTISIGVERIFY with appropriate code refactoring. While a signature may be verified against multiple public keys, the sighash for this signature must remain unchanged across the whole operation and therefore could be reused. Therefore, SigHashOp of a OP_CHECKMULTISIG and CHECKMULTISIGVERIFY is equal to the number of signatures.

Tight SigHashOp estimation with extra script restrictions

The tight SigHashOp estimation is based on more assumptions. It assumes that the scriptCode serialized within SignatureHash is a constant value for all sigops in an transaction input. For this assumption to be true, we must disable any process that may modify the scriptCode, which are OP_CODESEPARATOR and FindAndDelete. Transactions involving OP_CODESEPARATOR and FindAndDelete are extremely rare in the main network, and arguably all of those were performed for testing purpose. Removal of these operations would have next to no functional loss, significantly simply the consensus-critical logic, and reduce the risks of unintentional consensus forks. [6]

The tight SigHashOp estimation also assumes that only 6 nHashType are allowed. This is a relay policy in reference implementation since v0.?, and transactions with violating signatures are extremely rare in the main network. However, at consensus level, SigHashOp could be any value from 0 to 255, and a SIGHASH type could be encoded in multiple ways. For example, there are 116 ways to denote SIGHASH_ALL. Since nHashType is serialized inside SignatureHash, the sighash produced by different nHashType are not the same, even if all of them were SIGHASH_ALL.

With all these extra consensus rules implemented, we could be assured that SignatureHash is performed not more than once in each script for each nHashType, due to the invariability of scriptCode. The 6 nHashType limitation further guarantees that each script, with whatever number of sigops, would never perform SignatureHash for more than approximately 3 times of TxHashableSize (excluding some linearly growing overhead), as shown below:

TxHashableSize could be divided into 3 parts: size of inputs, size of outputs, and size of overhead including nVersion, nLockTime, and maybe some CompactSize encoding. The size of overhead grows linearly with the number of sigops, and is negligible.
SIGHASH_ALL would hash all inputs and outputs of the transaction.
SIGHASH_NONE would hash all inputs of the transaction, but no output is hashed.
SIGHASH_SINGLE would hash all inputs of the transaction. It also hashes the scriptPubKey of output with matching index, and all outputs with lower indexes with empty scriptPubKey. With the famous Gauss summation formula, it could be shown that if a transaction has the same number of inputs and outputs, and all inputs use a SIGHASH_SINGLE, the worst case would be hashing approximately 50% of all outputs of the transaction.
nHashType with SIGHASH_ANYONECANPAY would hash only one input, which scales linearly and is negligible. Therefore,
SIGHASH_ALL|SIGHASH_ANYONECANPAY would hash all outputs.
SIGHASH_NONE|SIGHASH_ANYONECANPAY is negligible.
SIGHASH_SINGLE|SIGHASH_ANYONECANPAY would hash approximately 50% of all outputs in the worst case.
By adding up the effects of 6 nHashType, it could be shown that the total amount of data hashed would be equal to 3 times of input size and 3 times of output size. Therefore, in the worst case, a script may perform SignatureHash for up to approximately 3 times of TxHashableSize.
SigHashSize policy limit

A policy limit for SigHashSize to Transaction weight ratio is recommended as 90, which is equivalent to 36MB SigHashSize for a 100kB non-segregated witness transaction. This limit is chosen based on the concept of normal transaction.

A transaction is normal if each SigHashOp consumes at least 70 bytes of space in scriptSig on average. According to BIP66, the maximum size of an ECDSA signature is 73 bytes, which should consume 74 bytes of scriptSig space including the push opcode. Using the low S value, the maximum signature size becomes 72 bytes. It could be shown that with 99.6% of chance, a randomly generated low S signature would be at least 69 bytes (consuming 70 bytes of space in scriptSig).

In actual use, the scriptSig size associated to a SigHashOp is often much more than 70 bytes. For example, pay-to-public-key-hash transactions and OP_CHECKMULTISIG inside P2SH would consume extra scriptSig space with their public keys. In such cases, more than 100 bytes of scriptSig would be consumed by a SigHashOp.

If a transaction is performing many SigHashOp with disproportionately small scriptSig, very likely it employed some strange scripts, such as using OP_DUP to copy a signature.

The size of scriptSig is important in determining the SigHashSize limit, since it is deducted from the transaction size for the TxHashableSize. Comparing 2 transactions of the same size, the one with more SigHashOp may have smaller SigHashSize due to the smaller TxHashableSize. With the 100kB standard transaction size limit, it could be shown that the maximum SigHashSize happens when there are 714 SigHashOp, consuming at least 714 * 70 = 49.98kB of scriptSig. With the resulting TxHashableSize = 100 - 49.98 = 50.02kB, the SigHashSize is 50.02kB * 714 = 35.7MB, which is just below the recommended policy limit.

Despite the limit is determined based on normal transactions, abnormal transactions may still be accepted as long as they are not too big. For example, if the transaction size is 10kB, a transaction may remain standard with the recommended policy limit even if each SigHashOp is associated with only 7 bytes of scriptSig.

SigHashSize consensus limit

The consensus limit of 500MB SigHashSize per block is based on the policy limit of SigHashSize to Transaction weight ratio. It is set above the policy limit, to make sure that a miner enforcing the policy limit would never produce a block violating the consensus rules. The 500MB limit is compromise between avoiding loss of functionality (as it may disable some very big transactions) and the harm of intentional or unintentional sighash attack initiated by a miner.

Deployment

This is a softfork to be deployed with BIP9.

Backward compatibility

Impact of the recommended policy limit

No matter the loose or the tight SigHashOp estimation is employed, this softfork with recommended policy limit should be completely transparent to users of normal standard transactions, including pay-to-public-key, pay-to-public-key-hash, and P2SH m-of-n OP_CHECKMULTISIG with 1 ≤ m ≤ n ≤ 15. A complete scan up to block 430368 showed that the transaction 7b587808a7f6b135ef91011be9b42fcbb0892da50963822e47a5827ced8653ce was the normal standard transaction with highest SigHashSize to weight ratio. With a ratio of 80.1, it is still well below the policy limit of 90. Should this policy had been deployed since genesis block, all normal standard transactions should still have been accepted.

If the loose estimation had been employed, a few abnormal standard transactions would have been rejected by policy, but were still valid by consensus. This is a full list of the affected transactions:

    bea1c2b87fee95a203c5b5d9f3e5d0f472385c34cb5af02d0560aab973169683
    24b16a13c972522241b65fbb83d09d4bc02ceb33487f41d1f2f620b047307179
    53666009e036171b1aee099bc9cd3cb551969a53315410d13ad5390b8b4f3bd0
    ffc178be118bc2f9eaf016d1c942aec18441a6c5ec17c9d92d1da7962f0479f6
    2f1654561297114e434c4aea5ca715e4e3f10be0be8c1c9db2b6f68ea76dae09
    62fc8d091a7c597783981f00b889d72d24ad5e3e224dbe1c2a317aabef89217e
    d939315b180d3d73b5e316eb57a18f8137a3f5943aef21a811660d25f1080a3f
    8a6bfaa78828a81147e4848372d491aa4e9048631982a670ad3a61402a4ec327
    02cc78789cc070125817189ec378daa750355c8b22bbce982ed96aa549facb1f
    b97a16ae2e8ae2a804ed7965373b42055f811653f4628e4bef999145d4b593bc
    c51ffaf08188859669571f897f119b6d39ea48a9334212f554bf4927401b71f3
    324456fe9ec97a380effba0a0205a226e380790b93e7366d39f2a416a44d2a34
These transactions all used a large number of sigops, and obviously were made for testing purpose. However, they would have gone through if the tight estimation had been used.

Impact of the block-level consensus limit

With the block-level consensus limit of 500MB SigHashSize, transactions with SigHashSize above 500MB would also become invalid. Up to block 430368, 49 transactions would have become invalid with this limit (with either loose or tight estimation):

    Transaction ID                                                       SigHashSize
    9c667c64fcbb484b44dcce638f69130bbf1a4dd0fbb4423f58ceff92af4219ec	 2,215,084,200
    9fdbcf0ef9d8d00f66e47917f67cc5d78aec1ac786e2abb8d2facb4e4790aad6	 2,215,076,850
    5d8875ed1707cfee2221741b3144e575aec4e0d6412eeffe1e0fa07335f61311	 1,271,892,772
    cb550c9a1c63498f7ecb7bafc6f915318f16bb54069ff6257b4e069b97b367c8	 1,271,892,772
    14dd70e399f1d88efdb1c1ed799da731e3250d318bfdadc18073092aa7fd02c2	 1,271,892,772
    a684223716324923178a55737db81383c28f055b844d8196c988c70ee7075a9a	 1,271,892,772
    bb41a757f405890fb0f5856228e23b715702d714d59bf2b1feb70d8b2b4e3e08	 1,271,820,375
    5b0a05f12f33d2dc1507e5c18ceea6bb368afc51f00890965efcc3cb4025997d	 1,091,954,040
    bb75a8d10cfbe88bb6aba7b28be497ea83f41767f4ee26217e311c615ea0132f	 1,025,295,000
    5e640a7861695fa660343abde52cfe10b5a97dd8fc6ad3c5e4b2b4bb1c8c3dd9	 1,025,295,000
    dd49dc50b54b4bc1232e4b68cfdd3d349e49d3d7fe817d1041fff6dd583a6eaf	 1,025,230,000
    3d724f03e8bcc9e2e3ea79ebe4c6cffca86d85e510742cd6d3ac29d420787a34	 1,025,210,000
    8bcf8e8d8265922956bda9b651d2a0e993072c9dca306f3a132dcdb95c7cee6e	 1,025,210,000
    54bf51be42ff45cdf8217b07bb233466e18d23fd66483b12449cd9b99c3a0545       995,042,075
    6bb39576292c69016d0e0c1fe7871640aab12dd95874d67c46cf3424822f8dfd	   988,589,147
    d38417fcc27d3422fe05f76f6e658202d7fa394d0c9f5b419fef97610c3c49f1	   923,884,836
    66b614e736c884c1a064f7b0d6a9b0abd97e7bb73ac7e4b1b92b493d558a0711	   902,501,490
    d985c42bcd704aac88b9152aede1cca9bbb6baee55c8577f84c42d600cfec8e4	   898,372,800
    e32477636e47e1da5fb49090a3a87a3b8ff637d069a70cd5b41595da225e65b4	   893,548,487
    bf40393fedc45a1b347957124ef9bb8ae6a44feecee10ef2cc78064fabf8125f	   891,859,369
    1d93bfe18bc05b13169837b6bc868a92da3c87938531d6f3b58eee4b8822ecbf	   888,420,676
    79e30d460594694231f163dd79a69808904819e2f39bf3e31b7ddc4baa030a04	   877,542,875
    4eba5deb2bbf3abf067f524484763287911e8d68fb54fa09e1287cf6cd6d1276	   874,353,609
    c3f2c2df5388b79949c01d66e83d8bc3b9ccd4f85dbd91465a16fb8e21bf8e1b	   869,060,209
    446c0a1d563c93285e93f085192340a82c9aef7a543d41a86b65e215794845ef	   833,655,283
    e0c5e2dc3a39e733cf1bdb1a55bbcb3c2469f283becf2f99a0de771ec48f6278	   802,433,929
    2e7c454cfc348aa220f53b5ba21a55efa3d36353265f085e34053c4efa575fda	   789,067,716
    01d23d32bccc04b8ca5a934be16da08ae6a760ccaad2f62dc2f337eee7643517	   785,833,449
    9f8cc4496cff3216608c2f2177ab360bd2d4f58cae6490d5bc23312cf30e72e0	   775,457,104
    1e700d8ce85b17d713cad1a8cae932d26740e7c8ab09d2201ddfe9d1acb4706c	   757,230,231
    9db4e0838c55ef20c5eff271fc3bf09a404fff68f9cdad7df8eae732500b983d	   756,319,396
    763e13f873afa5f24cd33fc570a178c65e0a79c05c88c147335834fc9e8f837b	   734,988,489
    b8ba939da1babf863746175b59cbfb3b967354f04db41bd13cb11da58e43d2a8	   732,906,849
    f62f2c6a16b5da61eaae36d30d43bb8dd8932cd89b40d83623fa185b671c67f9	   723,659,859
    6e278c0ca05bf8e0317f991dae8a9efa141b5a310a4c18838b4e082e356ef649	   703,394,401
    e3de81a5817a3c825cf44fbf8185e15d446393615568966a6e3fc22cba609c7d	   697,632,336
    b5ca68205e6d55e87bd6163b28467da737227c6cbcc91cb9f6dc7b400163a12b	   665,208,049
    9c972a02db30f9ee91cc02b30733d70d4e2d759b5d3c73b240e5026a8a2640c4	   653,370,601
    02313ac62ca8f03930cdc5d2e437fabc05aea60a31ace18a39678c90b45d32bd	   622,323,625
    e245f6c3c6b02dc81ea1b6694735565cc535f603708783be027d0e6a94ac3bd5	   609,926,656
    1cf52f9ef89fa43bb4f042cbd4f80e9f090061e466cbe14c6b7ba525df0e572e	   607,214,327
    461308024d89ea4231911df4ef24e65e60af2a9204c8282a6b67f4214c1714e7	   606,137,296
    fa5a58f787f569f5b8fab9dadb2447161fac45b36fb6c2c0f548ed0209b60663	   589,853,184
    905df97982a2904d6d1b3dfc272435a24d705f4c7e1fc4052798b9904ad5e597	   546,737,250
    d85ce71f583095a76fb17b5bb2a1cbf369e2a2867ca38103aa310cbb2aaf2921	   546,737,250
    1b604a075075197c82d33555ea48ae27e3d2724bc4c3f31650eff79692971fb7	   531,511,200
    ba31c8833b7417fec9a84536f32fcb52d432acb66d99b9be6f3899686a269b2b	   531,511,200
    92f217ec13ab309240adc0798804b3418666344a5cbfff73fb7be8192dad5261	   509,443,536
    22e861ee83c3d23a4823a3786460119425d8183783068f7ec519646592fac8c2	   506,268,969
Extra consensus rules required by tight SigHashOp estimation

Transactions in the main network, up to block 430368, that would have been affected by the extra consensus rules are listed below:

Transactions with OP_CODESEPARATOR:

    eb3b82c0884e3efa6d8b0be55b4915eb20be124c9766245bcc7f34fdac32bccb
    055707ce7fea7b9776fdc70413f65ceec413d46344424ab01acd5138767db137
    6d36bc17e947ce00bb6f12f8e7a56a1585c5a36188ffa2b05e10b4743273a74b
    bc179baab547b7d7c1d5d8d6f8b0cc6318eaa4b0dd0a093ad6ac7f5a1cb6b3ba
    4d932e00d5e20e31211136651f1665309a11908e438bb4c30799154d26812491
    0157f2eec7bf856d66714856182a146998910dc6fa576bec200a9fa8039459e7
    ddd070541bf2fddaa5e08a9d93126f73211fe15291beb897c762908949420ad9
    d4a27d10404d87ee0b8a05fb700e55f9f83f80a59ebf87af2fbf87e5c9546177
    492cdb3c95c1fe0c597d8dc847adb5459d403ea083f4b5e706300d437c84748f
    b3e977a2c48145255d84e1c82d4ea07522528991d50ead1cf3a783559d9733e3
Transactions with non-zero FindAndDelete results:

    5df1375ffe61ac35ca178ebb0cab9ea26dedbd0e96005dfcee7e379fa513232f
    ded7ff51d89a4e1ec48162aee5a96447214d93dfb3837946af2301a28f65dbea
    307b173ef009b970c1a0dd67166a8ce3e91fc5551b8950d2d17f1fe0eaa07358
Transactions with abnormal nHashType:

    c99c49da4c38af669dea436d3e73780dfdb6c1ecf9958baa52960e8baee30e73
    0ad07700151caa994c0bc3087ad79821adf071978b34b8b3f0838582e45ef305
    7c451f68e15303ab3e28450405cfa70f2c2cc9fa29e92cb2d8ed6ca6edb13645
    a6c116351836d9cc223321ba4b38d68c8f0db53661f8c2229acabbc269c1b2c8
    f5efee46ccfa4191ccd9d9f645e2f5d09bbe195f95ef5608e992d6794cd653cd
    904bda3a7d3e3b8402793334a75fb1ce5a6ff5cf1c2d3bcbd7bd25872d0e8c1e
    8ac76995ce4ac10dd02aa819e7e6535854a2271e44f908570f71bc418ffe3f02
    e218970e8f810be99d60aa66262a1d382bc4b1a26a69af07ac47d622885db1a7
    ba4f9786bb34571bd147448ab3c303ae4228b9c22c89e58cc50e26ff7538bf80
    38df010716e13254fb5fc16065c1cf62ee2aeaed2fad79973f8a76ba91da36da
Reference Implementation

Policy only:

https://github.com/bitcoin/bitcoin/pull/8755 (Tight estimation)
https://github.com/bitcoin/bitcoin/pull/8756 (Loose estimation)
References

^ CVE-2013-2292
^ New Bitcoin vulnerability: A transaction that takes at least 3 minutes to verify
^ The Megatransaction: Why Does It Take 25 Seconds?
^ It should be noted that since sigops may exist in both scriptSig (non-standard and extremely rare) and scriptPubKey, in theory an input may have up to 6 SigHashOp
^ Not totally, since it does not count the sigops inside the scriptPubKey of the outputs being spent, while inappropriately counting sigops in scriptPubKey of the current transaction.
^ https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-November/006878.html
Copyright

This document is placed in the public domain.






-------------------------------------
Interesting, can you provide some historical context around it so I
understand better ?
Actually I know that your relay's protocol (and about what I see in
abstract) was about optimizing propagation time and not bandwidth.

And I agree that bandwidth is what need to be optimized for nodes.
So far there was two other proposal that I know only from name and theory
which is xthin block and ILBT which would also have decreased bandwidth.

Can you quickly describe how does it compares to them ?

-------------------------------------
BitGo also intends to support SegWit transactions as soon as possible.

- Jameson

On Thu, Jan 7, 2016 at 9:17 PM, Matthieu Riou via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Not strictly speaking a wallet but we (BlockCypher) will also go down the
> segwit path as soon as the BIP and branch are mature enough.  All
> transactions built from our APIs should eventually be segwitted (just made
> up a verb).
>
> Thanks,
> Matthieu
> *CTO and Founder, Blockcypher*
> I have been informed that Breadwallet has also committed to supporting
> segwit.
>
> The list now includes Blocktrail, Breadwallet, GreenAddress, GreenBits,
> mSIGNA, and NBitcoin.
>
> ---
> Eric
>
> On January 7, 2016 5:28:18 AM PST, Eric Lombrozo <elombrozo@gmail.com>
> wrote:
>>
>> I am pleased to report that as of December 31, 2015 we have been
>> successfully running a segregated witness testnet, called segnet, and have
>> already implemented rudimentary wallets with support.
>>
>> For source code, please look at sipa's github repo:
>> https://github.com/sipa/bitcoin/tree/segwit
>>
>> And some example signing code at my repo:
>>
>> https://github.com/CodeShark/BitcoinScriptExperiments/blob/master/src/signwitnesstx.cpp
>>
>> Several wallets have already committed to supporting it including mSIGNA,
>> GreenAddress, GreenBits, Blocktrail, and NBitcoin. More wallets are
>> expected to be added to this list soon. If you're a wallet dev and are
>> interested in developing and testing on segnet please contact me.
>>
>> We're right on schedule and are very excited about the fundamental
>> improvements to bitcoin that segwit will enable.
>>
>> ---
>> Eric
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> There is no way to tell from a block if it was mined with AsicBoost or
> not. So you don’t know what percentage of the hashrate uses AsicBoost at
> any point in time. How can you risk forking that percentage out? Note that
> this would be a GUARANTEED chain fork. Meaning that after you change the
> block mining algorithm some percentage of hardware will no longer be able
> to produce valid blocks. That hardware cannot “switch over” to the majority
> chain even if it wanted to. Hence you are guaranteed to have two
> co-existing bitcoin blockchains afterwards.
>
> Again: this is unlike the hypothetical persistence of two chains after a
> hardfork that is only contentious but doesn’t change the mining algorithm,
> the kind of hardfork you are proposing would guarantee the persistence of
> two chains.
>

Assuming AsicBoost miners are in the minority, their chain will constantly
get overtaken. So it will not be one endless hard fork as you claim, but
rather AsicBoost blocks will continue to be ignored (orphaned) until they
stop making them.

That hardware cannot “switch over” to the majority chain even if it wanted
> to.
>

They will in fact continually "switch over" to the majority, they just are
unable to extend that majority chain themselves.

--
Jannes

-------------------------------------
On Wed, Aug 31, 2016 at 07:48:50PM +0000, James MacWhyte wrote:
> >
> > >I've always assumed honeypots were meant to look like regular, yet
> > >poorly-secured, assets.
> >
> > Not at all. Most servers have zero reason to have any Bitcoin's accessible
> > via them, so the presence of BTC privkeys is a gigantic red flag that they
> > are part of a honeypot.
> >
> 
> I was talking about the traditional concept. From Wikipedia: "Generally, a
> honeypot consists of data (for example, in a network site) that appears to
> be a legitimate part of the site but is actually isolated and monitored,
> and that seems to contain information or a resource of value to attackers,
> which are then blocked."
> 
> I would argue there are ways to make it look like it is not a honeypot
> (plenty of bitcoin services have had their hot wallets hacked before, and
> if the intruder only gains access to one server they wouldn't know that all
> the servers have the same honeypot on them). But I was just confirming that
> the proposal is for an obvious honeypot.

Ah, yeah, I think you have a point re: naming - this isn't quite the
traditional honeypot, as we uniquely have the ability to give the attackers a
reward in a way where it's ok for the intruder to know that they've been
detected; with traditional non-monetary honeypots it's quite difficult to come
up with a scenario where it's ok for an intruder to gain something from the
intrusion, so you're forced to use deception instead.

Perhaps a better term for this technique would be a "compromise canary"? Or
"intruder bait"? After all, in wildlife animal research it's common to use bait
as a way of attracting targets to discover that they exist (e.g. w/ wildlife
cameras), even when you have no intention of doing any harm to the animal.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
One side benefit of OP_COUNT_ACKS is that it enables a completely different
use case:

It allow users to pay for any service miners can provide as group for the
common good (e.g. fee payment smoothing over many blocks). For instance,
users could pay miners to jointly buy better Internet service to improve
bandwidth or reduce latency between them.

By sending bitcoins to a script containing OP_COUNT_ACKS requiring 51% of
miners approval and adding a special text tag to such outputs such as
"FOR-MINERS-TO-BUY-X", users can send bitcoins to miners and ask the
majority of them to vote on the proposal, if accepted create a transaction
to redeem those funds. This could help to address the so-called tragedy of
the commons problem that Bitcoin may face in in long-term, by users
crowdfunding mining of the following n blocks.

-------------------------------------
Responding between lines...

On Mon, Oct 24, 2016 at 2:37 PM, Johnson Lau <jl2012@xbt.hk> wrote:

> Some comments and questions
>
> 1. In the BIP you mentioned scriptSig 3 times, but I don't think you are
> really talking about scriptSig. Especially, segwit has aborted the use of
> scriptSig to fix malleability. From the context I guess you mean
> redeemScript (see BIP141)
>

You're right.I will change the naming asap.


>
> 2. It seems that 51% of miners may steal all money from the peg, right?
> But I think this is unavoidable for all 2-way-peg proposals. To make it
> safer you still need notaries.
>

Correct, that's inherently a technical limitation. However, there can be
many deterrents from miners stealing money (legal contracts,
mutual-assured-destruction states). Aslo as you mention, you can combine
OP_COUNT_ACK with notary sigantures as AND/OR or a more complex acknowledge
weight distribution.


>
> 3. Instead of using a OP_NOPx, I suggest you using an unused code such as
> 0xba. OP_NOPx should be reserved for some simple "VERIFY"-type codes that
> does not write to the stack.
>

Ok. I'm not sure, but if everyone agrees to it, I will. Also Segwit
versioning allows to create new opcode multiplexing opcodes, so I was
thinking about adding an "opcode index" to a more generic OP_OPERATE. But
that prevents using all NOP space, but prevents easily counting
OP_ACK_COUNT for checksig block limit.


> 4. I don't think you should simply replace "(witversion == 0)" with
> "((witversion == 0) || (witversion == 1))". There are only 16 available
> versions. It'd be exhausted very soon if we use a version for every new
> opcode. As a testing prototype this is fine, but the actual softfork should
> not waste a witversion this way. We need a better way to coordinate the use
> of new witness version. BIP114 suggests an additional field in the witness
> to indicate the script version (https://github.com/bitcoin/
> bips/blob/master/bip-0114.mediawiki)
>
> Good. But currently that version is not enforced, so this BIP cannot make
use of it. I can use (witversion == 1) but add the BIP114 version field so
that the next BIP can make use of it.



> 5. It seems this is the first BIP in markdown format, not mediawiki (but
> this is allowed by BIP1)
>

> 6. The coinbase space is limited to 100 bytes and is already overloaded by
> many different purposes. I think any additional consensus critical message
> should go to a dummy scriptPubKey like the witness commitment. You may
> consider to  have a new OP_RETURN output like BIP141, with different magic
> bytes. However, please don't make this output mandatory (cf. witness
> commitment output is optional if the block does not have witness tx)
>
> 6a. "..........due to lack of space to include the proper ack tag in a
> block": this shouldn't happen if you use a OP_RETURN output
>
> I'm not sure about this. The fact that the space for acknowledge and
proposal is short has been seen by other developers a benefit and not a
drawback. It prevent hundreds of sidechains to be offered, which might hurt
solo miners. 70 bytes allows for approximately 10 active polls.



> 7.  "It can be the case that two different secondary blockchains specify
> the same transaction candidate, but **at least** one of them will clearly
> be unauthentic."
>
> thnks.

8. Question: is an ack-poll valid only for 1 transaction? When the
> transaction is confirmed, could full nodes prune the corresponding ack-poll
> data? (I think it has to be prunable after spending because ack-poll data
> is effectively UTXO data)
>
> Yes, there is no ack-poll data stored except for the coinbase field cache,
which periodically cleans itself by using a FIFO approach.



> 9. No matter how you design a softfork, "Zero risk of invalidating a
> block" couldn't be true for any softfork. For example, even if a miner does
> not include any txs with OP_COUNT_ACKS, he may still build on top of blocks
> with invalid OP_COUNT_ACKS operations.
>
> I'm not sure. I assumed that transactions redeeming a script using
OP_COUNT_ACKS  would be non-standard, so the the problem you point out
would only happen if the block including the transaction would be mined
specifically for the purpose to defeat subsequent miners. A honest pre-fork
miner would never include a redeemScript that that verifies an
OP_COUNT_ACKS, since that transaction would never be received by the p2p
protocol (it could if the miner accepts non-standard txs by a different
channnel).

But I must check this in the BIP source code if OP_COUNT_ACKS is really
non-standard as I designed it to be.

(Thanks Jonhson Lau for taking the time to analyze the BIP.)

Sergio.



>  ---- On Sun, 02 Oct 2016 23:49:08 +0800 Sergio Demian Lerner via
> bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote ----
>  > Since ScalingBitcoin is close, I think this is a good moment to publish
> our proposal on drivechains. This BIP proposed the drivechain we'd like to
> use in RSK (a.k.a. Rootstock) two-way pegged blockchain and see it
> implemented in Bitcoin. Until that happens, we're using a federated
> approach.
>  > I'm sure that adding risk-less Bitcoin extensibility through
> sidechains/drivechains is what we all want, but it's of maximum importance
> to decide which technology will leads us there.
>  > We hope this work can also be the base of all other new 2-way-pegged
> blockchains that can take Bitcoin the currency to new niches and test new
> use cases, but cannot yet be realized because of current
> limitations/protections.
>  >
>  > The full BIP plus a reference implementation can be found here:
>  >
>  > BIP (draft):
>  > https://github.com/rootstock/bips/blob/master/BIP-R10.md
>  >
>  > Code & Test cases:
>  > https://github.com/rootstock/bitcoin/tree/op-count-acks_devel
>  > (Note: Code is still unaudited)
>  >
>  > As a summary, OP_COUNT_ACKS is a new segwit-based and soft-forked
> opcode that counts acks and nacks tags in coinbase fields, and push the
> resulting totals in the script stack.
>  >
>  > The system was designed with the following properties in mind:
>  >
>  > 1. Interoperability with scripting system
>  > 2. Zero risk of invalidating a block
>  > 3. No additional computation during blockchain management and
> re-organization
>  > 4. No change in Bitcoin security model
>  > 5. Bounded computation of poll results
>  > 6. Strong protection from DoS attacks
>  > 7. Minimum block space consumption
>  > 8. Zero risk of cross-secondary chain invalidation
>  >
>  > Please see the BIP draft for a more-detailed explanation on how we
> achieve these goals.
>  >
>  > I'll be in ScalingBitcoin in less than a week and I'll be available to
> discuss the design rationale, improvements, changes and ideas any of you
> may have.
>  >
>  > Truly yours,
>  > Sergio Demian Lerner
>  > Bitcoiner and RSK co-founder
>  >
>  >  _______________________________________________
>  > bitcoin-dev mailing list
>  > bitcoin-dev@lists.linuxfoundation.org
>  > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>  >
>
>
>

-------------------------------------
Hi all,

The following is a BIP-formatted design spec for compact block relay
designed to limit on wire bytes during block relay. You can find the
latest version of this document at
https://github.com/TheBlueMatt/bips/blob/master/bip-TODO.mediawiki.

There are several TODO items left on the document as indicated.
Additionally, the implementation linked at the bottom of the document
has a few remaining TODO items as well:

 * Only request compact-block-announcement from one or two peers at a
time, as the spec requires.
 * Request new blocks using MSG_CMPCT_BLOCK where appropriate.
 * Fill prefilledtxn with more than just the coinbase, as noted by the
spec, up to 10K in transactions.

Luke (CC'd): Can you assign a BIP number?

Thanks,
Matt

<pre>
  BIP: TODO
  Title: Compact block relay
  Author: Matt Corallo <bip@bluematt.me>
  Status: Draft
  Type: Standards Track
  Created: 2016-04-27
</pre>

==Abstract==

Compact blocks on the wire as a way to save bandwidth for nodes on the
P2P network.

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.

==Motivation==

Historically, the Bitcoin P2P protocol has not been very bandwidth
efficient for block relay. Every transaction in a block is included when
relayed, even though a large number of the transactions in a given block
are already available to nodes before the block is relayed. This causes
moderate inbound bandwidth spikes for nodes when receiving blocks, but
can cause very significant outbound bandwidth spikes for some nodes
which receive a block before their peers. When such spikes occur, buffer
bloat can make consumer-grade internet connections temporarily unusable,
and can delay the relay of blocks to remote peers who may choose to wait
instead of redundantly requesting the same block from other, less
congested, peers.

Thus, decreasing the bandwidth used during block relay is very useful
for many individuals running nodes.

While the goal of this work is explicitly not to reduce block transfer
latency, it does, as a side effect reduce block transfer latencies in
some rather significant ways. Additionally, this work forms a foundation
for future work explicitly targeting low-latency block transfer.

==Specification==

===Intended Protocol Flow===
TODO: Diagrams

The protocol is intended to be used in two ways, depending on the peers
and bandwidth available, as discussed [[#Implementation_Details|later]].
The "high-bandwidth" mode, which nodes may only enable for a few of
their peers, is enabled by setting the first boolean to 1 in a
"sendcmpct" message. In this mode, peers send new block announcements
with the short transaction IDs already, possibly even before fully
validating the block. In some cases no further round-trip is needed, and
the receiver can reconstruct the block and process it as usual
immediately. When some transactions were not available from local
sources (ie mempool), a getblocktxn/blocktxn roundtrip is neccessary,
bringing the best-case latency to the same 1.5*RTT minimum time that
nodes take today, though with significantly less bandwidth usage.

The "low-bandwidth" mode is enabled by setting the first boolean to 0 in
a "sendcmpct" message. In this mode, peers send new block announcements
with the usual inv/headers announcements (as per BIP130, and after fully
validating the block). The receiving peer may then request the block
using a MSG_CMPCT_BLOCK getdata reqeuest, which will receive a response
of the header and short transaction IDs. In some cases no further
round-trip is needed, and the receiver can reconstruct the block and
process it as usual, taking the same 1.5*RTT minimum time that nodes
take today, though with significantly less bandwidth usage. When some
transactions were not available from local sources (ie mempool), a
getblocktxn/blocktxn roundtrip is neccessary, bringing the best-case
latency to 2.5*RTT, again with significantly less bandwidth usage than
today. Because TCP often exhibits worse transfer latency for larger data
sizes (as a multiple of RTT), total latency is expected to be reduced
even when full the 2.5*RTT transfer mechanism is used.

===New data structures===
Several new data structures are added to the P2P network to relay
compact blocks: PrefilledTransaction, HeaderAndShortIDs,
BlockTransactionsRequest, and BlockTransactions. Additionally, we
introduce a new variable-length integer encoding for use in these data
structures.

For the purposes of this section, CompactSize refers to the
variable-length integer encoding used across the existing P2P protocol
to encode array lengths, among other things, in 1, 3, 5 or 9 bytes.

====New VarInt====
TODO: I just copied this out of the src...Something that is
wiki-formatted and more descriptive should be used here isntead.

Variable-length integers: bytes are a MSB base-128 encoding of the number.
The high bit in each byte signifies whether another digit follows. To make
sure the encoding is one-to-one, one is subtracted from all but the last
digit.
Thus, the byte sequence a[] with length len, where all but the last byte
has bit 128 set, encodes the number:

(a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))

Properties:
* Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)
* Every integer has exactly one encoding
* Encoding does not depend on size of original integer type
* No redundancy: every (infinite) byte sequence corresponds to a list
  of encoded integers.

0:         [0x00]  256:        [0x81 0x00]
1:         [0x01]  16383:      [0xFE 0x7F]
127:       [0x7F]  16384:      [0xFF 0x00]
128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]
255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]
2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]

Several uses of New VarInts below are "differentially encoded". For
these, instead of using raw indexes, the number encoded is the
difference between the current index and the previous index, minus one.
For example, a first index of 0 implies a real index of 0, a second
index of 0 thereafter refers to a real index of 1, etc.

====PrefilledTransaction====
A PrefilledTransaction structure is used in HeaderAndShortIDs to provide
a list of a few transactions explicitly.

{|
|Field Name||Type||Size||Encoding||Purpose
|-
|index||New VarInt||1-3 bytes||[[#New_VarInt|New VarInt]],
differentially encoded since the last PrefilledTransaction in a
list||The index into the block at which this transaction is
|-
|tx||Transaction||variable||As encoded in "tx" messages||The transaction
which is in the block at index index.
|}

====HeaderAndShortIDs====
A HeaderAndShortIDs structure is used to relay a block header, the short
transactions IDs used for matching already-available transactions, and a
select few transactions which we expect a peer may be missing.

{|
|Field Name||Type||Size||Encoding||Purpose
|-
|header||Block header||80 bytes||First 80 bytes of the block as defined
by the encoding used by "block" messages||The header of the block being
provided
|-
|nonce||uint64_t||8 bytes||Little Endian||A nonce for use in short
transaction ID calculations
|-
|shortids_length||CompactSize||1, 3, 5, or 9 bytes||As used elsewhere to
encode array lengths||The number of short transaction IDs in shortids
|-
|shortids||List of uint64_ts||8*shortids_length bytes||Little
Endian||The short transaction IDs calculated from the transactions which
were not provided explicitly in prefilledtxn
|-
|prefilledtxn_length||CompactSize||1, 3, 5, or 9 bytes||As used
elsewhere to encode array lengths||The number of prefilled transactions
in prefilledtxn
|-
|prefilledtxn||List of PrefilledTransactions||variable
size*prefilledtxn_length||As defined by PrefilledTransaction definition,
above||Used to provide the coinbase transaction and a select few which
we expect a peer may be missing
|}

====BlockTransactionsRequest====
A BlockTransactionsRequest structure is used to list transaction indexes
in a block being requested.

{|
|Field Name||Type||Size||Encoding||Purpose
|-
|blockhash||Binary blob||32 bytes||The output from a double-SHA256 of
the block header, as used elsewhere||The blockhash of the block which
the transactions being requested are in
|-
|indexes_length||New VarInt||1-3 bytes||As defined in [[#New_VarInt|New
VarInt]]||The number of transactions being requested
|-
|indexes||List of New VarInts||1-3 bytes*indexes_length||As defined in
[[#New_VarInt|New VarInt]], differentially encoded||The indexes of the
transactions being requested in the block
|}

====BlockTransactions====
A BlockTransactions structure is used to provide some of the
transactions in a block, as requested.

{|
|Field Name||Type||Size||Encoding||Purpose
|-
|blockhash||Binary blob||32 bytes||The output from a double-SHA256 of
the block header, as used elsewhere||The blockhash of the block which
the transactions being provided are in
|-
|transactions_length||New VarInt||1-3 bytes||As defined in
[[#New_VarInt|New VarInt]]||The number of transactions provided
|-
|transactions||List of Transactions||variable||As encoded in "tx"
messages||The transactions provided
|}

====Short transaction IDs====
Short transaction IDs are used to represent a transaction without
sending a full 256-bit hash. They are calculated by:
# single-SHA256 hashing the block header with the nonce appended (in
little-endian)
# XORing each 8-byte chunk of the double-SHA256 transaction hash with
each corresponding 8-byte chunk of the hash from the previous step
# Adding each of the XORed 8-byte chunks together (in little-endian)
iteratively to find the short transaction ID

===New messages===
A new inv type (MSG_CMPCT_BLOCK == 4) and several new protocol messages
are added: sendcmpct, cmpctblock, getblocktxn, and blocktxn.

====sendcmpct====
# The sendcmpct message is defined as a message containing a 1-byte
integer followed by a 8-byte integer where pchCommand == "sendcmpct".
# The first integer SHALL be interpreted as a boolean (and MUST have a
value of either 1 or 0)
# The second integer SHALL be interpreted as a little-endian version
number. Nodes sending a sendcmpct message MUST currently set this value
to 1.
# Upon receipt of a "sendcmpct" message with the first and second
integers set to 1, the node SHOULD announce new blocks by sending a
cmpctblock message.
# Upon receipt of a "sendcmpct" message with the first integer set to 0,
the node SHOULD NOT announce new blocks by sending a cmpctblock message,
but SHOULD announce new blocks by sending invs or headers, as defined by
BIP130.
# Upon receipt of a "sendcmpct" message with the second integer set to
something other than 1, nodes SHOULD treat the peer as if they had not
received the message (as it indicates the peer will provide an
unexpected encoding in cmpctblock, and/or other, messages)
# Nodes SHOULD check for a protocol version of >= 70014 before sending
sendcmpct messages.
# Nodes MUST NOT send a request for a MSG_CMPCT_BLOCK object to a peer
before having received a sendcmpct message from that peer.

====MSG_CMPCT_BLOCK====
# getdata messages may now contain requests for MSG_CMPCT_BLOCK objects.
# Upon receipt of a getdata containing a request for a MSG_CMPCT_BLOCK
object with the hash of a block which was recently announced and after
having sent the requesting peer a sendcmpct message, nodes MUST respond
with a cmpctblock message containing appropriate data representing the
block being requested.
# MSG_CMPCT_BLOCK inv objects MUST NOT appear anywhere except for in
getdata messages.

====cmpctblock====
# The cmpctblock message is defined as as a message containing a
serialized HeaderAndShortIDs message and pchCommand == "cmpctblock".
# Upon receipt of a cmpctblock message after sending a sendcmpct
message, nodes SHOULD calculate the short transaction ID for each
unconfirmed transaction they have available (ie in their mempool) and
compare each to each short transaction ID in the cmpctblock message.
# After finding already-available transactions, nodes which do not have
all transactions available to reconstruct the full block SHOULD request
the missing transactions using a getblocktxn message.
# A node MUST NOT send a cmpctblock message unless they are able to
respond to a getblocktxn message which requests every transaction in the
block.
# A node MUST NOT send a cmpctblock message without having validated
that the header properly commits to each transaction in the block, and
properly builds on top of the existing chain with a valid proof-of-work.
A node MAY send a cmpctblock before validating that each transaction in
the block validly spends existing UTXO set entries.

====getblocktxn====
# The getblocktxn message is defined as as a message containing a
serialized BlockTransactionsRequest message and pchCommand == "getblocktxn".
# Upon receipt of a properly-formatted getblocktxnmessage, nodes which
recently provided the sender of such a message a cmpctblock for the
block hash identified in this message MUST respond with an appropriate
blocktxn message. Such a blocktxn message MUST contain exactly and only
each transaction which is present in the appropriate block at the index
specified in the getblocktxn indexes list, in the order requested.

====blocktxn====
# The blocktxn message is defined as as a message containing a
serialized BlockTransactions message and pchCommand == "blocktxn".
# Upon receipt of a properly-formatted requested blocktxn message, nodes
SHOULD attempt to reconstruct the full block by:
## Taking the prefilledtxn transactions from the original cmpctblock and
placing them in the marked positions.
## For each short transaction ID from the original cmpctblock, in order,
find the corresponding transaction either from the blocktxn message or
from other sources and place it in the first available position in the
block.
# Once the block has been reconstructed, it shall be processed as
normal, keeping in mind that short transaction IDs are expected to
occasionally collide, and that nodes MUST NOT be penalized for such
collisions, wherever they appear.

===Implementation Notes===
# For nodes which have sufficient inbound bandwidth, sending a sendcmpct
message with the first integer set to 1 to up to three peers is
RECOMMENDED. If possible, it is RECOMMENDED that those peers be selected
based on their past performance in providing blocks quickly. This will
allow them to receive some blocks in only 0.5*RTT between them and the
sending peer. It will also reduce their block transfer latency in other
cases due to the smaller amount of data transmitted. Nodes MUST NOT send
such sendcmpct messages to all peers, as it encourages wasting outbound
bandwidth across the network.

# All nodes SHOULD send a sendcmpct message to all appropriate peers.
This will reduce their outbound bandwidth usage by allowing their peers
to request compact blocks instead of full blocks.

# Nodes with limited inbound bandwidth SHOULD request blocks using
MSG_CMPCT_BLOCK/getblocktxn requests, when possible. While this
increases worst-case message round-trips, it is expected to reduce
overall transfer latency as TCP is more likely to exhibit poor
throughput on low-bandwidth nodes.

# Nodes sending cmpctblock messages SHOULD make an attempt to not place
too many transactions into prefilledtxn (ie should limit prefilledtxn to
only around 10KB of transactions). When in doubt, nodes SHOULD only
include the coinbase transaction in prefilledtxn.

# Nodes MAY pick one nonce per block they wish to send, and only build a
cmpctblock message once for all peers which they wish to send a given
block to. Nodes SHOULD NOT use the same nonce across multiple different
blocks.

# Nodes MAY impose additional requirements on when they announce new
blocks by sending cmpctblock messages. For example, nodes with limited
outbound bandwidth MAY choose to announce new blocks using inv/header
messages (as per BIP130) to conserve outbound bandwidth.

# Note that the MSG_CMPCT_BLOCK section does not require that nodes
respond to MSG_CMPCT_BLOCK getdata requests for blocks which they did
not recently announce. This allows nodes to calculate cmpctblock
messages at announce-time instead of at request-time. Thus, nodes MUST
NOT request blocks using MSG_CMPCT_BLOCK getdatas unless it is in
response to an inv/headers block announcement (as per BIP130), and MUST
NOT request blocks using MSG_CMPCT_BLOCK getdatas in response to headers
messages which were, themselves, responses to getheaders requests.

# While the current version sends transactions with the same encodings
as is used in tx messages and elsewhere in the protocol, the version
field in sendcmpct is intended to allow this to change in the future.
For this reason, it is recommended that the code used to decode
PrefilledTransaction and BlockTransactions messages be prepared to take
a different transaction encoding, if and when the version field in
sendcmpct changes in a future BIP.

==Justification==

====Protocol design====
There have been many proposals to save wire bytes when relaying blocks.
Many of them have a two-fold goal of reducing block relay time and thus
rely on the use of significant processing power in order to avoid
introducing additional worst-case RTTs. Because this work is not focused
primarily on reducing block relay time, its design is much simpler (ie
does not rely on set reconciliation protocols). Still, in testing at the
time of writing, nodes are able to relay blocks without the extra
getblocktxn/blocktxn RTT around 90% of the time. With a smart
compact-block-announcement policy, it is thus expected that this work
might allow blocks to be relayed between nodes in 0.5*RTT instead of
1.5*RTT at least 75% of the time.

====Use of New VarInts====
Bitcoin has long had a variable-length integer implementation (referred
to as CompactSize in this document), making a second a strange protocol
quirk. However, in this protocol most of our variable-length integers
are between 0 and 2000. For both encodings, small numbers (<100) are
encoded as 1-byte. For numbers over 250, the CompactSize encoding begins
to use 3 bytes instead of 1, whereas the New VarInt encoding uses 2.
Because the primary motivation for this work is to save bytes during
block relay, the extra byte of saving per transaction-difference is
considered worth the extra design complexity.

====Short transaction ID calculation====
The short transaction ID calculation is designed to take absolutely
minimal processing time during block compaction to avoid introducing
serious DoS vulnerabilities such as those introduced by the
bloom-filtering in BIP 37. As such, it is possible for a node to
construct one compact-block representation of a block for relay to
multiple peers. Additionally, only one cryptographic hash (2 SHA rounds)
is used when calculating the short transaction IDs for an entire block.

The XOR-and-add method is used for calculating short transaction IDs
primarily because it is fast and is reasonably able to limit the ability
of an attacker who does not know the block hash or nonce to cause
collisions in short transaction IDs. If an attacker were able to cause
such collisions, filling mempools (and, thus, blocks) with them would
cause poor network propagation of new (or non-attacker, in the case of a
miner) blocks.

The 8-byte nonce in short transaction ID calculation is used to
introduce additional entropy on a per-node level. While the use of 8
bytes is sufficient for an attacker to maliciously cause short
transaction ID collisions in their own block relay, this would have less
of an effect than if such an attacker were relaying headers/invs and not
responding to requests for the full block.

==Backward compatibility==

Older clients remain fully compatible and interoperable after this change.

==Implementation==

https://github.com/TheBlueMatt/bitcoin/tree/udp

==Acknowledgements==

Thanks to Gregory Maxwell for the initial suggestion as well as a lot of
back-and-forth design and significant testing.

==Copyright==

This document is placed in the public domain.



-------------------------------------
On Friday, February 05, 2016 8:51:08 PM Gavin Andresen via bitcoin-dev wrote:
> Blog post on a couple of the constants chosen:
>   http://gavinandresen.ninja/seventyfive-twentyeight

Can you put this in the BIP's Rationale section (which appears to be mis-named 
"Discussion" in the current draft)?

> Signature operations in un-executed branches of a Script are not counted
> OP_CHECKMULTISIG evaluations are counted accurately; if the signature for a
> 1-of-20 OP_CHECKMULTISIG is satisified by the public key nearest the top
> of the execution stack, it is counted as one signature operation. If it is
> satisfied by the public key nearest the bottom of the execution stack, it
> is counted as twenty signature operations. Signature operations involving
> invalidly encoded signatures or public keys are not counted towards the
> limit

These seem like they will break static analysis entirely. That was a noted 
reason for creating BIP 16 to replace BIP 12. Is it no longer a concern? Would 
it make sense to require scripts to commit to the total accurate-sigop count 
to fix this?

> The amount of data hashed to compute signature hashes is limited to
> 1,300,000,000 bytes per block.

The rationale for this wasn't in your blog post. I assume it's based on the 
current theoretical max at 1 MB blocks? Even a high-end PC would probably take 
40-80 seconds just for the hashing, however - maybe a lower limit would be 
best?

> Miners express their support for this BIP by ...

But miners don't get to decide hardforks. How does the economy express their 
support for it? What happens if miners trigger it without consent from the 
economy?

If you are intent on using the version bits to trigger the hardfork, I suggest 
rephrasing this such that miners should only enable the bit when they have 
independently confirmed economic support (this means implementations need a 
config option that defaults to off).

> SPV (simple payment validation) wallets are compatible with this change.

Would prefer if this is corrected to "Light clients" or something. Actual SPV 
wallets do not exist at this time, and would not be compatible with a 
hardfork.

> In the short term, an increase is needed to continue the current economic
> policies with regards to fees and block space, matching market expectations
> and preventing market disruption.

IMO this sentence is the most controversial part of your draft, and it 
wouldn't suffer a loss to remove it (or at least make it subjective).

I would also prefer to see any hardfork:

1. Address at least the simple tasks on the hardfork wishlist (eg, enable some
   disabled opcodes; fix P2SH for N-of->15 multisig; etc).
2. Be deployed as a soft-hardfork so as not to leave old nodes entirely
   insecure.

Luke


-------------------------------------
On 6/30/16, Erik Aronesty via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> I would like to see a PGP-like "web of trust" proposal for both the
> security of the bitcoin network itself /and/ (eventually) of things like
> transmission of bitcoin addresses.
>
> Something where nodes of any kind (full, spv, mobile wallets) can
> /optionally/ accumulate trust over time and are capable of verifying the
> identity of other nodes in that web.
>

Isn't the system already functioning in this way already? Bitcoin
exchanges and block explorers already have a reputation earned by so
many years of serving the community. Their HTTPS certificate acts like
a public/private key, and when your wallet makes a request to their
server, the keys are automatically checked for validity by the
underlying http library.


-------------------------------------
>It's also not clear to me why the HMAC, vs just SHA256(key|cipher-type|mesg).  But that's probably just my crypto ignorance...

SHA256(key|cipher-type|mesg) is an extremely insecure MAC because of
the length extension property of SHA256.

If I have a tag y = SHA256(key|cipher-type|mesg), I can without
knowing key or msg compute a value y' such that
y' = SHA256(key|cipher-type|mesg|any values I want).

Thus, an attacker can trivially forge a tag protected by
SHA256(key|cipher-type|mesg).

For more details see:
https://web.archive.org/web/20141029080820/http://vudang.com/2012/03/md5-length-extension-attack/

On Tue, Jun 28, 2016 at 9:00 PM, Rusty Russell via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Jonas Schnelli <dev@jonasschnelli.ch> writes:
>>> To quote:
>>>
>>>> HMAC_SHA512(key=ecdh_secret|cipher-type,msg="encryption key").
>>>>
>>>>  K_1 must be the left 32bytes of the HMAC_SHA512 hash.
>>>>  K_2 must be the right 32bytes of the HMAC_SHA512 hash.
>>>
>>> This seems a weak reason to introduce SHA512 to the mix.  Can we just
>>> make:
>>>
>>> K_1 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="header encryption key")
>>> K_2 = HMAC_SHA256(key=ecdh_secret|cipher-type,msg="body encryption key")
>>
>> SHA512_HMAC is used by BIP32 [1] and I guess most clients will somehow
>> make use of bip32 features. I though a single SHA512_HMAC operation is
>> cheaper and simpler then two SHA256_HMAC.
>
> Good point; I would argue that mistake has already been made.  But I was
> looking at appropriating your work for lightning inter-node comms, and
> adding another hash algo seemed unnecessarily painful.
>
>> AFAIK, sha256_hmac is also not used by the current p2p & consensus layer.
>> Bitcoin-Core uses it for HTTP RPC auth and Tor control.
>
> It's also not clear to me why the HMAC, vs just
> SHA256(key|cipher-type|mesg).  But that's probably just my crypto
> ignorance...
>
> Thanks!
> Rusty.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------

>     I guess my question didn't get across.
> 
>     Why would you want to make your usecase do connections over the
>     peer2peer
>     (net.cpp) connection at all?
> 
>     Mixing messages that are being sent to everyone and encrypted
>     messages is
>     asking for trouble.
>     Making your private connection out-of-band would work much better.
> 
> 
> I agree doing it out-of-band is the easiest solution for people who need
> this privacy right now, but I do like the idea of adding this feature as
> the number of SPV wallets is going to increase. I think the best way to
> organize things would be to give encrypted messages their own port
> number, similar to how http vs. https works.

I'm not sure if different ports would make sense. I can't see a benefit
(happy if someone can convince me).
How would this affect p2p address management (address relay)? Wouldn't
this require to extend the current address message to support two port
numbers?


> We don't want two networks to develop, separated by which nodes support
> encryption and which don't, so ideally nodes would rebroadcast messages
> they receive on both (encrypted and non-encrypted) channels. This would
> essentially double the required bandwidth of the network, which is
> something to think about.

It can be the same "p2p network". The only difference would be, that
once two peers has negotiated encryption, the whole traffic between
_these two peers_, and _only_ these two pears, would be encrypted (would
_not_ affect traffic to/from other peers).

A simplified example:
1. Peer Alice connects to peer Bob
2. Alice asks Bob: "lets do encrypted communication, here is my session
pubkey"
3. Bob also supports encryption and answers "Yes, let's do this, here is
my session pubkey"
4. Alice tells Bob (encrypted now): "Perfect. Here I prove that I'm
Alice by signing the session ID with my identity pubkey"
5. Bob checks his "authorized-peers" database and look-up Alices pubkey
and verifies the signatures.
6. Bob tells Alice: "Good! I trust you now Alice, here is my identity
pubkey with a signature of our session-ID"
7. Alice looks up Bobs pubkey in her "known-peers" database and verifies
the signature.
8. Alice response to bob: "Perfect. Indeed, you are Bob!"
---
At this point, the communication is encrypted and the identities has
been verified (MITM protection).


(simplified negotiation [only one-way, missing dh explanation, missing
KDF, session-ID, cipher suite nego., missing re-keying, etc.])


</jonas>


-------------------------------------
On Wednesday, June 08, 2016 5:57:36 AM Johnson Lau via bitcoin-dev wrote:
> Why not make it even bigger, e.g. 75 bytes?

I don't see a sufficient answer to this question. Pieter explained why >75 
would be annoying, but 75 seems like it should be fine.

> In any case, since scripts with a 1-byte push followed by a push of >40
> bytes remain anyone-can-spend, we always have the option to redefine them
> with a softfork.

It's not that simple, since this is preventing use of the witness field for 
such scripts. With this limit in place, any such a softfork would suddenly 
require either two different witness commitments, or disabling the previous 
witness transaction format.

Luke


-------------------------------------
As Segregated Witness will be merged soon as a solution for transaction
malleability, especially with multi-party adversarial signatures, there
may be an additional use case/functionality which is helpful for
Lightning Network and possibly other Bitcoin use cases. This requires a
new SIGHASH flag inside Segregated Witness which does not sign the input
txid/index.

Segwit is very helpful in resolving malleability in pretty much every
case which matters. It is especially helpful in having solid and safe
defaults for standard Bitcoin payments; it's very difficult to mess up
if you are writing code in conjunction with the Bitcoin RPC API.

However, it is very useful for LN if there is a certain level of
outsourcibility for transactions without this 3rd party taking on
onerous costs. In LN, there is a dispute resolution period established
to prevent the counterparty from attesting an incorrect channel state
(represented by broadcasting a timelocked transaction). In other words,
if someone in a channel broadcasts an incorrect state, the output can be
redeemed by a 3rd party (but this 3rd party is not a custodian, since
the output goes to the other party in the channel).

Ideally, a 3rd-party can be handed a transaction which can encompass all
prior states in a compact way. For currently-designed Segregated Witness
transactions, this requires storing all previous signatures, which can
become very costly if individuals to thousands of channel state updates
per day. This is very possible, as fees are near-zero, the value in
atomizing all payments to many transactions becomes viable (reducing
transaction/information costs). If individuals are doing tens of
thousands of transactions per day, and one presumes something like
70-bytes of data per Commitment state in the channel, it quickly becomes
infeasible to watch on behalf of many channels without material costs.

This is especially necessary because it is highly desirable to make
keeping track of these channels be very cheap, as it allows for more
participants to be watching on one's behalf (reducing the chance of a
3rd party fail to watch). Further, it may reduce the need to notify the
3rd party for every single channel Commitment state, instead only
providing the most recent one should provide sufficient information for
all prior states (since the signature will apply for any type of
transaction), making the only updated information the revocation
secret/preimage. Without this SIGHASH flag, every single state would
need to be contacted and updated with 3rd parties. With this SIGHASH
flag, one could instead delegate outsourcing when one's client goes
offline with a single message several hundred bytes in size,
encompassing all prior states.

Of course, while running a 24/7 full-node is encouraged, I suspect many
people will not want to do so at the current time, and it needs to be
functional for those who elect to be connected intermittently. This
requires outsourcing or watching on one's behalf.

This would be achieved using a SIGHASH flag, termed SIGHASH_NOINPUT. It
does not include as part of the signature, the outpoint being spent
(txid and index), nor the amount. It however, would include the spent
outpoint's script as part of the signature. Note that this is just a
SIGHASH flag, and the outpoints are still being included as part of the
txins (if they are mutated, the new txids can be updated by the wallet
without resigning). This allows for a signature to apply to anything
with that pubkey (therefore pubkeys with this flag should not be
reused). For safety, this only applies in SegWit transactions, as segwit
provides a sufficient malleability solution, there is no incentive to
improperly use this sighash flag as a roundabout way to resolve
malleability.

This helps with 3rd-party outsourcing for watching the blockchain, as
one can provide a signature (and the most recent hash-chain of
revocation preimages), which encompasses penalty transactions for all
prior states. Functionally, this allows for opt-in wildcard inputs, but
wallets which do not require these transactions do not need to be
concerned with this flag; since they will never be signing with this
flag, they do not need to be concerned with address re-use.

I'm interested in input and in the level of receptiveness to this. If
there is interest, I'll write up a draft BIP in the next couple days.

-- 
Joseph Poon


-------------------------------------
Coupled with the release of segwit in 0.13.1, there are 3 default relay and mining policy rules that may become softfork proposals in the near future.

Generally, users must not assume that a script spendable in pre-segregated witness system would also be spendable as a P2WPKH or P2WSH script. Before large-scale deployment in the production network, developers should test the scripts on testnet with the default relay policy turned on, and with a small amount of money after BIP141 is activated on mainnet.

The rules include:

1. Only compressed public keys are accepted in P2WPKH and P2WSH (See BIP143 for details)
2. The argument of OP_IF/NOTIF in P2WSH must be minimal (see https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-August/013014.html)
3. Signature(s) must be null vector(s) if an OP_CHECKSIG or OP_CHECKMULTISIG is failed (for both pre-segregated witness script and P2WSH. See BIP146)

The BIP141 and 143 are updated with the aforementioned rules: https://github.com/bitcoin/bips/pull/459/files



-------------------------------------
On Mon, Dec 5, 2016 at 7:27 AM, t. khan via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> Put another way: let’s stop thinking about what the max block size should
> be and start thinking about how full we want the average block to be
> regardless of size. Over the last year, we’ve had averages of 75% or
> higher, so aiming for 75% full seems reasonable, hence naming this concept
> ‘Block75’.
>

That's effectively making the blocksize limit completely uncapped and only
preventing spikes, and even in the case of spikes it doesn't differentiate
between 'real' traffic and low value spam attacks. It suffers from the same
fundamental problems as bitcoin unlimited: There are in the end no
transaction fees, and inevitably some miners will want to impose some cap
on block size for practical purposes, resulting in a fork.

Difficulty adjustment works because there's a clear goal of having a
certain rate of making new blocks. Without a target to attempt automatic
adjustment makes no sense.

-------------------------------------
On 5/10/2016 2:43 PM, Sergio Demian Lerner via bitcoin-dev wrote:
>
> If we change the protocol then the message to the ecosystem is that
> ASIC optimizations should be kept secret.

Further to that point, if THIS optimization had been kept secret, nobody
would be talking about doing anything, as with countless other
optimizations.



-------------------------------------
On Tue, Jun 28, 2016 at 9:22 PM, Eric Voskuil via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> An "out of band key check" is not part of BIP151.

It has a session ID for this purpose.

> It requires a secure channel and is authentication. So BIP151 doesn't provide the tools to detect an attack, that requires authentication. A general requirement for authentication is the issue I have raised.

One might wonder how you ever use a Bitcoin address, or even why we
might guess these emails from "you" aren't actually coming from the
NSA.


-------------------------------------
Happy Lunar New Year Everyone!

Gavin,

> I suspect there ARE a significant percentage of un-maintained full
> nodes-- probably 30 to 40%. Losing those nodes will not be a problem, for
> three reasons:


The notion of large set ( 30% to 40% ) of un-maintained full nodes are not
evident on the network. below is data based on a personal snap shot taken
around Dec, 2015. with the following assumptions.
1) nodes running non standard version strings are considered a preference
by the node operator and are not included.
2) nodes below 0.10 are counted as so called "un-maintained" even though
they also can be a choice of the node operator.

Satoshi:0.9.3, 105
Satoshi:0.8.6, 74
Satoshi:0.9.1, 49
Satoshi:0.9.2.1, 42
Satoshi:0.8.5, 39
Satoshi:0.8.1, 35
Satoshi:0.9.5, 14
Satoshi:0.8.3, 12
Satoshi:0.9.4, 10
Satoshi:0.9.99, 10
Satoshi:0.9.0, 5
Satoshi:0.9.2, 5
Satoshi:0.8.0, 4
Satoshi:0.8.99, 1
Satoshi:0.8.4, 1

There are 406 nodes total that falls under the un-maintained category,
which is below 10% of the network.
Luke also have some data here that shows similar results.
http://luke.dashjr.org/programs/bitcoin/files/charts/versions.txt

> The network could shrink by 60% and it would still have plenty of open
> connection slots


I'm afraid we have to agree to disagree if you think dropping support for
60% of the nodes on the network when rolling out an upgrade is the sane
default.

>
> > People are committing to spinning up thousands of supports-2mb-nodes
> during the grace period.


thousands of nodes?! where did you get this figure? who are these people?
*Please* elaborate.

> We could wait a year and pick up maybe 10 or 20% more.


I don't understand this statement at all, please explicate.

-- 
*Yifu Guo*
*"Life is an everlasting self-improvement."*

On Sat, Feb 6, 2016 at 10:37 AM, Gavin Andresen via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Responding to "28 days is not long enough" :
>
> I keep seeing this claim made with no evidence to back it up.  As I said,
> I surveyed several of the biggest infrastructure providers and the btcd
> lead developer and they all agree "28 days is plenty of time."
>
> For individuals... why would it take somebody longer than 28 days to
> either download and restart their bitcoind, or to patch and then re-run
> (the patch can be a one-line change MAX_BLOCK_SIZE from 1000000 to 2000000)?
>
> For the Bitcoin Core project:  I'm well aware of how long it takes to roll
> out new binaries, and 28 days is plenty of time.
>
> I suspect there ARE a significant percentage of un-maintained full nodes--
> probably 30 to 40%. Losing those nodes will not be a problem, for three
> reasons:
> 1) The network could shrink by 60% and it would still have plenty of open
> connection slots
> 2) People are committing to spinning up thousands of supports-2mb-nodes
> during the grace period.
> 3) We could wait a year and pick up maybe 10 or 20% more.
>
> I strongly disagree with the statement that there is no cost to a longer
> grace period. There is broad agreement that a capacity increase is needed
> NOW.
>
> To bring it back to bitcoin-dev territory:  are there any TECHNICAL
> arguments why an upgrade would take a business or individual longer than 28
> days?
>
>
> Responding to Luke's message:
>
> On Sat, Feb 6, 2016 at 1:12 AM, Luke Dashjr via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> > On Friday, February 05, 2016 8:51:08 PM Gavin Andresen via bitcoin-dev
>> wrote:
>> >> Blog post on a couple of the constants chosen:
>> >>   http://gavinandresen.ninja/seventyfive-twentyeight
>> >
>> > Can you put this in the BIP's Rationale section (which appears to be
>> mis-named
>> > "Discussion" in the current draft)?
>>
>
> I'll rename the section and expand it a little. I think standards
> documents like BIPs should be concise, though (written for implementors),
> so I'm not going to recreate the entire blog post there.
>
>
>> >
>> >> Signature operations in un-executed branches of a Script are not
>> counted
>> >> OP_CHECKMULTISIG evaluations are counted accurately; if the signature
>> for a
>> >> 1-of-20 OP_CHECKMULTISIG is satisified by the public key nearest the
>> top
>> >> of the execution stack, it is counted as one signature operation. If
>> it is
>> >> satisfied by the public key nearest the bottom of the execution stack,
>> it
>> >> is counted as twenty signature operations. Signature operations
>> involving
>> >> invalidly encoded signatures or public keys are not counted towards the
>> >> limit
>> >
>> > These seem like they will break static analysis entirely. That was a
>> noted
>> > reason for creating BIP 16 to replace BIP 12. Is it no longer a
>> concern? Would
>> > it make sense to require scripts to commit to the total accurate-sigop
>> count
>> > to fix this?
>>
>
> After implementing static counting and accurate counting... I was wrong.
> Accurate/dynamic counting/limiting is quick and simple and can be
> completely safe (the counting code can be told the limit and can
> "early-out" validation).
>
> I think making scripts commit to a total accurate sigop count is a bad
> idea-- it would make multisignature signing more complicated for zero
> benefit.  E.g. if you're circulating a partially signed transaction to that
> must be signed by 2 of 5 people, you can end up with a transaction that
> requires 2, 3, 4, or 5 signature operations to validate (depending on which
> public keys are used to do the signing).  The first signer might have no
> idea who else would sign and wouldn't know the accurate sigop count.
>
>
>> >
>> >> The amount of data hashed to compute signature hashes is limited to
>> >> 1,300,000,000 bytes per block.
>> >
>> > The rationale for this wasn't in your blog post. I assume it's based on
>> the
>> > current theoretical max at 1 MB blocks? Even a high-end PC would
>> probably take
>> > 40-80 seconds just for the hashing, however - maybe a lower limit would
>> be
>> > best?
>>
>
> It is slightly more hashing than was required to validate block number
> 364,422.
>
> There are a couple of advantages to a very high limit:
>
> 1) When the fork is over, special-case code for dealing with old blocks
> can be eliminated, because all old blocks satisfy the new limit.
>
> 2) More importantly, if the limit is small enough it might get hit by
> standard transactions, then block creation code (CreateNewBlock() /
> getblocktemplate / or some external transaction-assembling software) will
> have to solve an even more complicated bin-packing problem to optimize for
> fees paid.
>
> In practice, the 20,000 sigop limit will always be reached before
> MAX_BLOCK_SIGHASH.
>
>
>
>> >
>> >> Miners express their support for this BIP by ...
>> >
>> > But miners don't get to decide hardforks. How does the economy express
>> their
>> > support for it? What happens if miners trigger it without consent from
>> the
>> > economy?
>>
>
> "The economy" does support this.
>
>
>
>> >
>> > If you are intent on using the version bits to trigger the hardfork, I
>> suggest
>> > rephrasing this such that miners should only enable the bit when they
>> have
>> > independently confirmed economic support (this means implementations
>> need a
>> > config option that defaults to off).
>>
>
> Happy to add words about economic majority.
>
> Classic will not implement a command-line option (the act of running
> Classic is "I opt in"), but happy to add one for a pull request to Core,
> assuming Core would not see such a pull request as having any hostile
> intent.
>
>
> >
>> >> SPV (simple payment validation) wallets are compatible with this
>> change.
>> >
>> > Would prefer if this is corrected to "Light clients" or something.
>> Actual SPV
>> > wallets do not exist at this time, and would not be compatible with a
>> > hardfork.
>>
>
> Is there an explanation of SPV versus "Light Client" written somewhere
> more permanent than a reddit comment or forum post that I can point to?
>
>
>> >
>> >> In the short term, an increase is needed to continue the current
>> economic
>> >> policies with regards to fees and block space, matching market
>> expectations
>> >> and preventing market disruption.
>> >
>> > IMO this sentence is the most controversial part of your draft, and it
>> > wouldn't suffer a loss to remove it (or at least make it subjective).
>>
>
> Happy to remove.
>
>
>> > I would also prefer to see any hardfork:
>> >
>> > 1. Address at least the simple tasks on the hardfork wishlist (eg,
>> enable some
>> >    disabled opcodes; fix P2SH for N-of->15 multisig; etc).
>>
>
> Those would be separate BIPs. (according to BIP 1, smaller is better)
>
> After this 2MB bump, I agree we need to agree on a process for the next
> hard fork to avoid all of the unnecessary drama.
>
> > 2. Be deployed as a soft-hardfork so as not to leave old nodes entirely
>> >    insecure.
>>
>
> I haven't been paying attention to all of the
> "soft-hardfork/hard-softfork/etc" terminology so have no idea what you
> mean. Is THAT written up somewhere?
>
> --
> --
> Gavin Andresen
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Gavin, please don't quote that list on the Classic website. It's horribly
inaccurate and misleading to the general public.

> That testing is happening by the exchange, library, wallet, etc providers
> themselves. There is a list on the Classic home page:
>
> https://bitcoinclassic.com/

I know for a fact that most companies you list there have no intention to
run Classic, much less test it. You should not mix support for 2MB with
support for Classic, or if people say they welcome a fork, to mean they
support Classic.

On Sun, Feb 7, 2016 at 5:11 AM, Peter Todd via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Sat, Feb 06, 2016 at 12:45:14PM -0500, Gavin Andresen via bitcoin-dev
> wrote:
> > On Sat, Feb 6, 2016 at 12:01 PM, Adam Back <adam@cypherspace.org> wrote:
> >
> > >
> > > It would probably be a good idea to have a security considerations
> > > section
> >
> >
> > Containing what?  I'm not aware of any security considerations that are
> any
> > different from any other consensus rules change.
>
> I covered the security considerations unique to hard-forks on my blog:
>
> https://petertodd.org/2016/soft-forks-are-safer-than-hard-forks
>
> > > , also, is there a list of which exchange, library, wallet,
> > > pool, stats server, hardware etc you have tested this change against?
> > >
> >
> > That testing is happening by the exchange, library, wallet, etc providers
> > themselves. There is a list on the Classic home page:
> >
> > https://bitcoinclassic.com/
>
> How do we know any of this testing is actually being performed? I don't
> currently know of any concrete testing actually done.
>
> > > Do you have a rollback plan in the event the hard-fork triggers via
> > > false voting as seemed to be prevalent during XT?  (Or rollback just
> > > as contingency if something unforseen goes wrong).
> > >
> >
> > The only voting in this BIP is done by the miners, and that cannot be
> faked.
>
> Are you unaware of Not Bitcoin XT?
>
> https://bitcointalk.org/index.php?topic=1154520.0
>
> > I can't imagine any even-remotely-likely sequence of events that would
> > require a rollback, can you be more specific about what you are
> imagining?
> > Miners suddenly getting cold feet?
>
> See above.
>
> Also, as the two coins are separate currencies and can easily trade
> against each other in a 75%/25% split, it would be easy for the price to
> diverge and hashing power to move.
>
> In fact, I've been asked multiple times now by exchanges and other
> players in this ecosystem for technical advice on how to split coins
> across the chains effectively (easily done with nLockTime). Notably, the
> exchanges who have asked me this - who hold customer funds on their
> behalf - have informed me that their legal advice was that the
> post-hard-fork coins are legally speaking separate currencies, and
> customers must be given the opportunity to transact in them separately
> if they choose too.  Obviously, with a 75%/25% split, while block times
> on the other chain will be slower, the chain is still quite useful and
> nearly as secure as the main chain against 51% attack; why I personally
> have suggested a 99% threshold:
>
>
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012309.html
>
> (remember that the threshold can always be soft-forked down)
>
> It's also notable that millions of dollars of Bitcoin are voting agsast
> the fork on the proof-of-stake voting site Bitcoinocracy.com While
> obviously not comprehensive, the fact that a relatively obscure site
> like it can achieve participation like that, even without an easy to use
> user friendly interface.
>
> > > How do you plan to monitor and manage security through the hard-fork?
> > >
> >
> > I don't plan to monitor or manage anything; the Bitcoin network is
> > self-monitoring and self-managing. Services like statoshi.info will do
> the
> > monitoring, and miners and people and businesses will manage the network,
> > as they do every day.
>
> Please provide details on exactly how that's going to happen.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
> 000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Hello all,

We're getting ready for Bitcoin Core's 0.13.1 release - the first one
to include segregated witness (BIP 141, 143, 144, 145) for Bitcoin
mainnet, after being extensively tested on testnet and in other
software. Following the BIP9 recommendation [1] to set the versionbits
start time a month in the future and discussion in the last IRC
meeting [2], I propose we set BIP 141's start time to November 15,
2016, 0:00 UTC (unix time 1479168000).

Note that this is just a lower bound on the time when the versionbits
signalling can begin. Activation on the network requires:
(1) This date to pass
(2) A full retarget window of 2016 blocks with 95% signalling the
version bit (bit 1 for BIP141)
(3) A fallow period consisting of another 2016 blocks.

  [1] https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki
  [2] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.2016-10-13-19.04.log.html

Cheers,

-- 
Pieter


-------------------------------------
2016-03-03 16:28 GMT+01:00 Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org>:
> Bitcoin Core already implements this safety limit with the "absurd fee"
> limit of 10000 * the minimum relay fee. This limit is active in both the
> wallet and the sendrawtransaction RPC call. Additionally for the wallet
> there is a user configurable -maxtxfee option to limit fees set by the
> wallet which currently defaults to 0.1 BTC.


It is planned for Bitcoin Core 0.13 to use -maxtxfee for both, the
wallet and the RPC interface (sendrawtransaction). (c.f.
https://github.com/bitcoin/bitcoin/pull/7084)

In regard to the issue, I agree with Jonas. Such large transaction
fees were historically caused by no or insufficient warnings from the
wallet software. And it's the responsibility of the operators to make
the wallet user friendly.

Apart from that, there are legit use cases where one would want to
"pay" a large transaction fee: It may be convenient for the miner to
just collect the fees instead of sending back the change on their own
transactions. Of course making sure to mine the high-fee tx themself.
Moreover, it could increase privacy if another party decides to "wash"
their bitcoins by letting the miner claim the "fee" and then have the
miner send back a fraction of the fee to a fresh address. Though, this
probably works best if a lot of participants are doing this.

Marco


-------------------------------------
A damping-based design would seem like the obvious choice (I can think of a few variations on a theme here, but most are found in the realms of control theory somewhere).  The problem, though, is working working out a timeframe over which to run the derivative calculations.

The problem is the measurement of the hashrate, which is pretty inaccurate at best because even 2016 events isn't really enough (with a completely constant hash rate running indefinitely we'd see difficulty swings of up to +/- 5% even with the current algorithm).  In order to meaningfully react to a major loss of hashing we'd still need to be considering a window of probably 2 weeks.

My other concern is that if we allow quick retargets to lower difficulties then that seems likely to expose the chain to being gamed.  I'd need to think about this some more, but a few scenarios I was thinking about earlier this week appeared to risk making some types of selfish mining strategies quite a lot more profitable.

With all this said though I'll be very surprised if there's a huge drop in the hash rate come July.  The hash rate has jumped up by almost 70% in the last 6 to 7 months and that implies some pretty serious investments by miners who are quite aware of the halving.  My guess is that quite a lot of the baseline 30% has also been replaced in the same cycle.  These same miners were mining with a coin price around $250 last year so in terms of profitability I'm pretty sure that one around $400 won't be a huge concern.

I'm sure that there will be some very public "I'm done with mining" announcements from a few smaller miners come July, but I suspect the bulk of the network will have a relatively small blip and continue on its way.


Cheers,
Dave


> On 8 Mar 2016, at 22:05, Bob McElrath <bob_bitcoin@mcelrath.org> wrote:
> 
> Dave Hudson via bitcoin-dev [bitcoin-dev@lists.linuxfoundation.org] wrote:
>> I think the biggest question here would be how would the difficulty
>> retargeting be changed?  Without seeing the algorithm proposal it's difficult
>> to assess the impact that it would have, but my intuition is that this is
>> likely to be problematic.
> 
> I have no comment on whether this will be *needed* but there's a simple
> algorithm that I haven't seen any coin adopt, that I think needs to be: the
> critically damped harmonic oscillator:
> 
>    http://mathworld.wolfram.com/CriticallyDampedSimpleHarmonicMotion.html
> 
> In dynamical systems one does a derivative expansion.  Here we want to find the
> first and second derivatives (in time) of the hashrate.  These can be determined
> by a method of finite differences, or fancier algorithms which use a quadratic
> or quartic polynomial approximation.  Two derivatives are generally all that is
> needed, and the resulting dynamical system is a damped harmonic oscillator.  
> 
> A damped harmonic oscillator is basically how your car's shock absorbers work.
> The relevant differential equation has two parameters: the oscillation frequency
> and damping factor.  The maximum oscillation frequency is the block rate.  Any
> oscillation faster than the block rate cannot be measured by block times.  The
> damping rate is an exponential decay and for critical damping is twice the
> oscillation frequency.
> 
> So, this is a zero parameter, optimal damping solution for a varying hashrate.
> This is inherently a numeric approximation solution to a differential equation,
> so questions of approximations for the hashrate enter, but that's all.  Weak
> block proposals will be able to get better approximations to the hashrate.
> 
> If solving this problem is deemed desirable, I can put some time into this, or
> direct others as to how to go about it.
> 
> --
> Cheers, Bob McElrath
> 
> "For every complex problem, there is a solution that is simple, neat, and wrong."
>    -- H. L. Mencken 
> 



-------------------------------------
Today according to the stats at https://bitnodes.21.co/nodes/ the top 10 
Bitcoin running node versions are:

1.
_Version Satoshi:0.13.1
_Nodes 2071
_38.97%


2.
_Version Satoshi:0.12.1
_Nodes 1022
_19.23%


3.
Satoshi:0.13.0
_Nodes 604
_11.36%


4.
Bitcoin Unlimited:0.12.1
_Nodes 373
_7.02%


5.
Satoshi:0.11.2
_Nodes 183
_3.44%


6.
Satoshi:0.12.0
_Nodes 131
_2.46%


7. Satoshi:0.13.99
_Nodes 122
_2.30%


8.
Satoshi:0.11.0
_Nodes 87
_1.64%


9.
BTCC:0.13.1
_Nodes 53
_1.00%


10.
Satoshi:0.10.2
_Nodes 52
_0.98%


Other
_Nodes 617
_11.61%


There are 75 different versions of visible nodes on the network.

More than 30% of the nodes running Bitcoin Core are running versions 
older than 0.13.0.

For reasons I am unable to determine a significant number of node 
operators do not upgrade their clients.

I also know newer versions require the same or fewer hardware resources 
to run than the same network requirements as older versions of the 
client.

Older node versions may generate issues because some upgrades will make 
several of the nodes running older protocol versions obsolete and or 
incompatible. There may be other hard to predict behaviors on older 
versions of the client.

In order to avoid such wide fragmentation of "Bitcoin Core” node 
versions and to help there be a more predictable protocol improvement 
process, I consider it worth it to analyze introducing some planned 
obsolescence in each new version. In the last year we had 4 new versions 
so if each version is valid for about 1 year (52560 blocks) this may be 
a reasonable time frame for node operators to upgrade. If a node does 
not upgrade it will stop working instead of participating in the network 
with an outdated protocol version.

These changes may also simplify the developer's jobs in some cases by 
avoiding them having to deal with ancient versions of the client.

Regards
Juan Garavaglia


-------------------------------------
On Thu, Feb 4, 2016 at 5:17 PM, Luke Dashjr <luke@dashjr.org> wrote:
> All review itself ought to remain on the ML.

> the author-chosen forum may only be *in addition
> to* the Bitcoin Wiki?

Ahh, much better.  Thank you.

FWIW, this is the phrase that confused me:
[BIP 2:] If a BIP is not yet completed, reviewers should [...]


-------------------------------------
On Wed, Aug 17, 2016 at 9:24 AM, Jonas Schnelli via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

>
> Conclusion:
> ===========
> * Non of the points convinced me that there is a better alternative to
> the proposed URI scheme interaction (please tell me if I'm stubborn).
>

I'd also agree with this - and it's convenient to test against simulators /
mocks.

-- 
Nicolas Bacca | CTO, Ledger

-------------------------------------
Sergio,

It is critically important to the future of Bitcoin that consensus
code avoid any unnecessary entanglements with patents because "the
free market" allows you and anyone else to make consensus change
proposals that rely on (unknown) patents - but this is something we
should all be working to avoid, as it unnecessarily hinders Bitcoin
development and everyone's ability to deploy. Consensus code must not
be hindered by patents and Bitcoin should retain its permissionless
qualities.

When you proposed the extra nonce space BIP [1], you had already
applied for your ASICBOOST patent [2] without disclosure in the BIP
[1] nor in your Bitcoin Core pull request #5102 [2].

The ASICBOOST patent [2] describes the same process as in the BIP [1]
and proposed code [3] "As we explained in our Provisional Application,
it has been proposed to partition the 4-byte Version field in the
block header (see, Fig. 6) and use, e.g., the high 2-byte portion as
additional nonce range."

Today when you proposed a new sidechain BIP [4], Peter Todd was
(rightly) concerned about the prior lack of disclosure of your patents
related to your prior consensus modification proposal. Hence the
concern is that this might be happening this time as well.

There is no evidence that any of the other filers for the
ASICBOOST-like patents by mining companies other than your own were
going to be using it offensively as those other companies appeared to
understand the decentralization risk of having an advantage enforced
by legal and not technical means.

It's great that you have now committed to looking into the Defensive
Patent License. This seems likely to mitigate some of the patent
concerns. Although it would be a show of good faith if you also agreed
to license ASICBOOST under the DPL.

[1]: BIP: https://github.com/BlockheaderNonce2/bitcoin/wiki
[2]: ASICBOOST PATENT https://www.google.com/patents/WO2015077378A1?cl=en
[3]: Extra nonce pull request: https://github.com/bitcoin/bitcoin/pull/5102
[4]: COUNT_ACKS
[https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-October/013174.html

On Sun, Oct 2, 2016 at 6:13 PM, Sergio Demian Lerner via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Please Peter Todd explain here all what you want to say about a patent of a
> hardware design for an ASIC.
>
> Remember that ASICBoost is not the only patent out there, there are at least
> three similar patents, filed by major Bitcoin ASIC manufacturers in three
> different countries, on similar technologies.
>
> That suggest that the problem is not ASICBoot's: you cannot blame any
> company from doing lawful commerce in a FREE MARKET.
>
> It is a flaw in Bitcoin design that could be corrected if the guidelines I
> posted in [1] had been followed.
>
> [1]
> https://bitslog.wordpress.com/2014/03/18/the-re-design-of-the-bitcoin-block-header/
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
Hi

> 6 - Finally, and most importantly, BIP39 seed phrases do not have a
> version number. Without a version number, how are you going to derive
> addresses from a BIP39 seed phrase, when wallets start to use to new
> derivation methods (such as SegWit, or Schnorr signatures)? Does it mean
> that a BIP39 compatible wallet will have to check addresses from all the
> derivation methods that ever existed in the past, in order to ensure
> that all coins are correctly retrieved? Or will there be users that
> cannot access their coins because their BIP39 seed phrase is too old for
> newer software?

I totally agree with Thomas.

Another thing that I think could be a BIP misdesign:

BIP44 Gap Limits
From the BIP:

----------
  "Address gap limit is currently set to 20. If the software hits 20
unused addresses in a row, it expects there are no used addresses beyond
this point and stops searching the address chain."
----------

* Does that mean, we do a transaction rescan back to the genesis block
for every 20 addresses?
* Or is it a prerequirement to do a wallet recovery after BIP44's to
have access to a full address-indexed blockchain?

Or maybe I'm missing something.

</jonas>


-------------------------------------
When would miners vote no to receive more funds?
Also, why would they spend the funds buying X once they get them?

On Oct 3, 2016 00:58, "Sergio Demian Lerner via bitcoin-dev" <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> One side benefit of OP_COUNT_ACKS is that it enables a completely
> different use case:
>
> It allow users to pay for any service miners can provide as group for the
> common good (e.g. fee payment smoothing over many blocks). For instance,
> users could pay miners to jointly buy better Internet service to improve
> bandwidth or reduce latency between them.
>
> By sending bitcoins to a script containing OP_COUNT_ACKS requiring 51% of
> miners approval and adding a special text tag to such outputs such as
> "FOR-MINERS-TO-BUY-X", users can send bitcoins to miners and ask the
> majority of them to vote on the proposal, if accepted create a transaction
> to redeem those funds. This could help to address the so-called tragedy of
> the commons problem that Bitcoin may face in in long-term, by users
> crowdfunding mining of the following n blocks.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------------------------------
Gavin Andresen 於 2016-02-04 12:36 寫到:
> This BIP is unnecessary, in my opinion.
> 
> I'm going to take issue with items (2) and (3) that are the motivation
> for this BIP:
> 
> " 2. Full nodes and SPV nodes following original consensus rules may
> not be aware of the deployment of a hardfork. They may stick to an
> economic-minority fork and unknowingly accept devalued legacy tokens."
> 
> If a hardfork is deployed by increasing the version number in blocks
> (as is done for soft forks), then there is no risk-- Full and SPV
> nodes should notice that they are seeing up-version blocks and warn
> the user that they are using obsolete software.
> 
> It doesn't matter if the software is obsolete because of hard or soft
> fork, the difference in risks between those two cases will not be
> understood by the typical full node or SPV node user.

Thanks for your comments.

In the case of a softfork, as long as an user waits for a few 
confirmations, the risk of money loss is very low. In the worst case 
they run a full node with SPV security. In the case of a hardfork, the 
consequence of failing to upgrade to the economic majority fork *is* 
fatal, even if an user waits for 1000 confirmations. Not to mention the 
risk of having 2 economically active forks. That's why wallets should 
STOP accepting and sending tx after a hardfork bit is detected and wait 
for users' instructions.

> " 3. In the case which the original consensus rules are also valid
> under the new consensus rules, users following the new chain may
> unexpectedly reorg back to the original chain if it grows faster than
> the new one. People may find their confirmed transactions becoming
> unconfirmed and lose money."
> 
> If a hard or soft fork uses a 'grace period' (as described in BIP 9 or
> BIP 101) then there is essentially no risk that a reorg will happen
> past the triggering block. A block-chain re-org of two thousand or
> more blocks on the main Bitcoin chain is unthinkable-- the economic
> chaos would be massive, and the reaction to such a drastic (and
> extremely unlikely) event would certainly be a hastily imposed
> checkpoint to get everybody back onto the chain that everybody was
> using for economic transactions.

No, the "triggering block" you mentioned is NOT where the hardfork 
starts. Using BIP101 as an example, the hardfork starts when the first 
 >1MB is mined. For people who failed to upgrade, the "grace period" is always zero, which is the moment they realize a hardfork.


> Since I don't agree with the motivations for this BIP, I don't think
> the proposed mechanism (a negative-version-number-block) is necessary.
> And since it would simply add more consensus-level code, I believe the
> keep-it-simple principle applies.
> 
> --
> 
> --
> Gavin Andresen



-------------------------------------
Previously:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011862.html

Here are some talks from Milan:
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/fungibility-overview/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/joinmarket/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/tumblebit/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/mimblewimble/

http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/onion-routing-in-lightning/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/flare-routing-in-lightning/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/unlinkable-outsourced-channel-monitoring/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/lightning/

http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/segwit-lessons-learned/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/schnorr-signatures/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/bip151-peer-encryption/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/coin-selection/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/covenants/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/on-the-security-and-performance-of-proof-of-work-blockchains/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/collective-signing/

http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/fast-difficulty-adjustment/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/jute-braiding/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/client-side-validation/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/breaking-the-chain/

http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/day-1-group-summaries/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/milan/day-2-group-summaries/

These are not always exact transcripts because I am typing while I am
listening, thus there are mistakes including typos and listening
errors, so please keep this discrepancy in mind between what's said
and what's typed.

- Bryan
http://heybryan.org/
1 512 203 0507


-------------------------------------
On Thu, Jan 28, 2016 at 7:51 PM, Peter Todd via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> A few notes on upgrade procedures associated with segregated witnesses:

> While Pieter Wuille's segwit branch(1) doesn't yet implement a fix for
> the above problem, the obvious thing to do is to add a new service bit
> such as NODE_SEGWIT, and/or bump the protocol version, and for outgoing
> peers only connect to peers with segwit support.

Agree, I've merged the changes to switch to a service bit instead.
We'll need further changes to prefer connecting to segwit nodes.

> Segwit isn't going to be the last thing that adds new block data. For
> example, my own prev-block-proof proposal(3) requires that blocks commit
> to another tree, which itself is calculated using a nonce that must be
> passed along with the block data. (U)TXO commitments are another
> possible future example.

> Unfortunately, this means that the next soft-fork upgrade to add
> additional data will have the above relaying problem all over again!
> Even a minimal upgrade adding a new commitment - like my
> prev-block-proof proposal - needs to at least add another nonce for
> future upgrades. In addition to having to upgrade full nodes, this also
> requires systems like the relay network to upgrade, even though they may
> not themselves otherwise need to care about the contents of blocks.

Those are good arguments for making the witness data more extensible.
>
> A more subtle implication of this problem is how do you handle parallel
> upgrades, as proposed by BIP9? Splitting the P2P network into
> non-upgraded nodes, and a much smaller group of upgraded nodes, is bad
> enough when done every once in a awhile. How does this look with more
> frequent upgrades, not necessarily done by teams that are working
> closely with each other?

I don't expect that changes that add more data to be relayed with
blocks will be frequent, though I certainly agree there may be some.

> Proposal: Unvalidated Block Extension Data
> ==========================================

(snip)

This will need a backward-incompatible change to the current segwit
change anyway, so at the risk of more bikeshedding, let me propose
going a bit further:

* The coinbase scriptSig gets a second number push (similar to the
current BIP34 height push), which pushes a number O. O is a byte
offset inside the coinbase transaction (excluding its witness data)
that points to a 32-byte hash H. This is more flexible and more
compact than what we have now (a suggestion by jl2012).
* H is the root of a Merkle tree, whose leaves are the hashes of the
coinbase witness's stack items.
* Item 0 of the coinbase witness stack must be 32 bytes, and must be
equal to the witness tree root.
* No further restrictions on the rest of the stack items; these can be
used for future commitments.

> A significant design consideration is that if arbitrary data can be
> added, it is very likely that miners will make use of that ability for
> non-Bitcoin purposes; we've already run into problems deploying segwit
> itself because of pools using the coinbase space for advertising and
> merge-mining. Avoiding this problem is easiest with a merkelized
> key:value mapping, with the ability to use collision-resistant ID's as
> keys (e.g. UUID).

I agree with the concern, but I don't really understand how this idea solves it.

-- 
Pieter


-------------------------------------
Is this unrelated to Bitcoin Vigil idea published in 2014?

http://www.coindesk.com/bitcoin-vigil-program-guards-against-intrusion-coin-theft/





On Wed, Aug 24, 2016 at 8:42 AM Matthew Roberts via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> Really nice idea. So its like a smart contract that incentivizes
> publication that a server has been hacked? I also really like how the
> funding has been handled -- with all the coins stored in the same address
> and then each server associated with a unique signature. That way, you
> don't have to split up all the coins among every server and reduce the
> incentive for an attacker yet you can still identify which server was
> hacked.
>
> It would be nice if after the attacker broke into the server that they
> were also incentivized to act on the information as soon as possible
> (revealing early on when the server was compromised.) I suppose you could
> split up the coins into different outputs that could optimally be redeemed
> by the owner at different points in the future -- so they're incentivzed to
> act lest their reward decays even more (this is of course, assuming that
> the monetary reward for this is greater than any possible legal
> consequences for the attacker -- it might not be. Thinking about this some
> more: it would also be somewhat hard to deny that this -wasn't- a honeypot
> with such a complex and unique scheme required for transactions, and I for
> one wouldn't like to reveal that I'd hacked a server if I knew it was all a
> calculated ploy. Don't honeypots rely on subtly?)
>
> What about also proving to an attacker that by breaking into a server they
> would be guaranteed a reward? I know that the use-case for this is proof of
> compromise so incentivizing a security audit would kind of fall more into
> an active invitation to audit but couldn't you also make a cryptocurrency
> that allowed coins to be moved based on a service banner existing at a
> given IP address? Attackers could then break into the server, setup a
> service that broadcasts their public key hash, and then spend coins locked
> at this special contract address to that pub key hash which miners would
> check on redemption (putting aside malicious use-cases for now.)
>
>
> On Wed, Aug 24, 2016 at 11:46 AM, Peter Todd via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Bitcoin-based honeypots incentivise intruders into revealing the fact
>> they have
>> broken into a server by allowing them to claim a reward based on secret
>> information obtained during the intrusion. Spending a bitcoin can only be
>> done
>> by publishing data to a public place - the Bitcoin blockchain - allowing
>> detection of the intrusion.
>>
>> The simplest way to achieve this is with one private key per server, with
>> each
>> server associated with one transaction output spendable by that key.
>> However
>> this isn't capital efficient if you have multiple servers to protect: if
>> we
>> have N servers and P bitcoins that we can afford to lose in the
>> compromise, one
>> key per server gives the intruder only N/P incentive.
>>
>> Previously Piete Wuille proposed(1) tree signatures for honeypots, with a
>> single txout protected by a 1-N tree of keys, with each server assigned a
>> specific key. Unfortunately though, tree signatures aren't yet
>> implemented in
>> the Bitcoin protocol.
>>
>> However with a 2-of-2 multisig and the SIGHASH_SINGLE feature we can
>> implement
>> this functionality with the existing Bitcoin protocol using the following
>> script:
>>
>>     2 <honeypot-pubkey> <distriminator-pubkey> 2 CHECKMULTISIG
>>
>> The honeypot secret key is shared among all N servers, and left on them.
>> The
>> distriminator secret key meanwhile is kept secret, however for each
>> server a
>> unique signature is created with SIGHASH_SINGLE, paying a token amount to
>> a
>> notification address. For each individual server a pre-signed signature
>> created
>> with the distriminator secret key is then left on the associated server
>> along
>> with the honeypot secret key.
>>
>> Recall the SIGHASH_SINGLE flag means that the signature only signs a
>> single
>> transaction input and transaction output; the transaction is allowed to
>> have
>> additional inputs and outputs added. This allows the thief to use the
>> honeypot
>> key to construct a claim transaction with an additional output added that
>> pays
>> an address that they own with the rest of the funds.
>>
>> Equally, we could also use SIGHASH_NONE, with the per-server discriminator
>> being the K value used in the pre-signed transaction.
>>
>> Note that Jeff Coleman deserves credit as co-inventor of all the above.
>>
>>
>> Censorship Resistance
>> =====================
>>
>> A potential disadvantage of using non-standard SIGHASH flags is that the
>> transactions involved are somewhat unusual, and may be flagged by
>> risk analysis at exchanges and the like, a threat to the fungibility of
>> the
>> reward.
>>
>> We can improve on the above concept from Todd/Coleman by using a
>> pre-signed
>> standard transaction instead. The pre-signed transaction spends the
>> honeypot
>> txout to two addresses, a per-server canary address, and a change
>> address. The
>> private key associated with the change addres is also left on the server,
>> and
>> the intruder can then spend that change output to finally collect their
>> reward.
>>
>> To any external observer the result looks like two normal transactions
>> created
>> in the process of someone with a standard wallet sending a small amount of
>> funds to an address, followed by sending a larger amount.
>>
>>
>> Doublespending
>> ==============
>>
>> A subtlety in the the two transactions concept is that the intruder
>> doesn't
>> have the necessary private keys to modify the first transaction, which
>> means
>> that the honeypot owner can respond to the compromise by doublespending
>> that
>> transaction, potentially recovering the honeypot while still learning
>> about the
>> compromise. While this is possible with all honeypots, if the first
>> transaction
>> is signed with the opt-in RBF flags, and CPFP-aware transaction
>> replacement is
>> not implemented by miners, the mechanics are particularly disadvantageous
>> to
>> the intruder, as the honeypot owner only needs to increase the first
>> transaction's fee slightly to have a high chance of recovering their
>> funds.
>> With CPFP-aware transaction replacement the intruder could in-turn
>> respond with
>> a high-fee CPFP second transaction, but currently no such implementation
>> is
>> known.
>>
>>
>> Scorched Earth
>> ==============
>>
>> We can use the "scorched earth" concept to improve the credibility of the
>> honeypot reward by making it costly for the honeypot owner to
>> doublespend. Here
>> a second version of the honeypot pre-signed transaction would also be
>> provided
>> which sepnds the entirety of the honeypot output to fees, and additionally
>> spends a second output to fees. An economically rational intruder will
>> publish
>> the first version, which maximizes the funds they get out of the
>> honeypot. If
>> the owner tries to dishonestly doublespend, they can respond by
>> publishing the
>> "scorched earth" transaction, encouraging the honeypot owner's honesty and
>> making CPFP-aware transaction replacement irrelevant.
>>
>> Of course, miner centralization adds complexity to the above: in many
>> instances
>> honeypot owners and/or intruders will be able to recover funds from
>> altruistic
>> miners. Equally, the additional complexity may discourage intruders from
>> making
>> use of the honeypot entirely.
>>
>> Note that as an implementation consideration CHECKSEQUENCEVERIFY can be
>> used to
>> ensure the honeypot output can only be spent with transaction replacement
>> enabled, as CSV requires nSequence to be set in specific ways in any
>> transation
>> spending the output.
>>
>>
>> References
>> ==========
>>
>> 1) https://blockstream.com/2015/08/24/treesignatures/
>>
>> --
>> https://petertodd.org 'peter'[:-1]@petertodd.org
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
Am 13.05.2016 um 15:16 schrieb Daniel Weigl via bitcoin-dev:
> 
> With SegWit approaching it would make sense to define a common derivation scheme how BIP44 compatible wallets will handle P2(W)SH (and later on P2WPKH) receiving addresses.
> I was thinking about starting a BIP for it, but I wanted to get some feedback from other wallets devs first.
>

The discussion so far shows that starting a new BIP is a very good idea.
 Otherwise everyone would do it slightly different.

With P2(W)SH you mean P2WPKH embedded in P2SH, right?  P2WSH is
completely different and used for example for multisig.


> In my opinion there are two(?) different options: 

To summarize, option 1 means one account that supports both non-segwit
and segwit addresses.  With option 2 you have one p2pkh-only account and
one segwit-only account, which are completely separated.

I personally would vote for option 1.  Scanning twice the addresses can
be avoided with Aaron's trick.  The second disadvantage remains:

> 	-) If you have the same xPub/xPriv key in different wallets, you need to be sure both take care for the different address types

A non-segwit wallet would ignore all segwit outputs, which means that
the balance it shows is smaller (and it doesn't show transactions that
spend from previous segwit outputs).  I don't see that this can lead to
losing money except maybe when sweeping the account with a p2pkh-only
wallet and then throwing the xprv away.

Of course, you can also do option 2 and let it appear to the user as if
it was only one account, but what is the advantage over option 1 in that
case?  Also you need two xpubs to watch this joined account.

  Jochen



-------------------------------------
I understand this mailing list is for topics relating to development. Is
there a general users mailing list for bitcoin related things such as
questions that are not necessarily related to dev?

-- 

Cannon
PGP Fingerprint: 2BB5 15CD 66E7 4E28 45DC 6494 A5A2 2879 3F06 E832
Email: cannon@cannon-ciota.info
Bitmessage Address: BM-2cVaTbC8fJ5UDDaBBs4jPQoFNp1PfNhxqU
Ricochet-IM: ricochet:hfddt2csxnsb2mdq


-------------------------------------
I started work on the decoder. It works by comparing each cycle of the sine
wave recorded with a certain frequency (freq_t). The function that does
this currently returns the last sample that falls in that range over all
samples. The longest chain to match up with a certain frequency the best is
interpreted as that frequency (assuming it passes an entrance floor to
separate it from static).  I will probably extrapolate the sine wave to
more samples (320 kbps+), but this works OK just for testing. I am also
prepping the docs per the official BIP spec.

GitHub is https://github.com/Dako300/BIP-0170

Is it okay to host the BIP on my own GitHub, or do I need to host it on a
more official repository?

-------------------------------------
On Mon, Feb 1, 2016 at 5:53 PM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> I've completed an initial draft of a BIP that provides clarifications on
> the
> Status field for BIPs, as well as adding the ability for public comments on
> them, and expanding the list of allowable BIP licenses.
>
>
> https://github.com/luke-jr/bips/blob/bip-biprevised/bip-biprevised.mediawiki
>
> I plan to open discussion of making this BIP an Active status (along with
> BIP
> 123) a month after initial revisions have completed. Please provide any
> objections now, so I can try to address them now and enable consensus to be
> reached.
>


I like the more concrete definitions of the various statuses.

I don't like the definition of "consensus".  I think the definition
described gives too much centralized control to whoever controls the
mailing list and the wiki.

-- 
--
Gavin Andresen

-------------------------------------


On 10/16/2016 4:58 PM, Tom Zander via bitcoin-dev wrote:
> Lets get back to the topic. Having a longer fallow period is a simple way to 
> be safe.  Your comments make me even more scared that safety is not taken 
> into account the way it would.

Can you please explain how having a longer grace period makes it any
safer? Once the fork reaches the LOCKED_IN status, the fork will become
active after the period is over. How does having a longer grace period
make this any safer besides just adding more waiting before it goes
active? You said something about rolling back the changes. There is no
mechanism for roll backs, and the whole point of the soft fork
signalling is such that there is no need to roll back anything because
miners have signaled that they are supporting the fork.


-------------------------------------
Greg Maxwell wrote:

> What are you talking about? You seem profoundly confused here...
> 
> I obtain some txouts. I write a transaction spending them in malleable
> form (e.g. sighash single and an op_return output).. then grind the
> extra output to produce different hashes.  After doing this 2^32 times
> I am likely to find two which share the same initial 8 bytes of txid.

[9 May 16 @ 4:30 PDT]

I’m trying to understand the collision attack that you're explaining to Tom Zander.  

Mathematica is telling me that if I generated 2^32 random transactions, that the chances that the initial 64-bits on one of the pairs of transactions is about 40%.  So I am following you up to this point.  Indeed, there is a good chance that a pair of transactions from a set of 2^32 will have a collision in the first 64 bits.  

But how do you actually find that pair from within your large set?  The only way I can think of is to check if the first 64-bits is equal for every possible pair until I find it.  How many possible pairs are there?  

It is a standard result that there are 

    m! / [n! (m-n)!] 

ways of picking n numbers from a set of m numbers, so there are

    (2^32)! / [2! (2^32 - 2)!] ~ 2^63

possible pairs in a set of 2^32 transactions.  So wouldn’t you have to perform approximately 2^63 comparisons in order to identify which pair of transactions are the two that collide?

Perhaps I made an error or there is a faster way to scan your set to find the collision.  Happy to be corrected…

Best regards,
Peter



-------------------------------------
>Ethan:  your algorithm will find two arbitrary values that collide. That isn't useful as an attack in the context we're talking about here (both of those values will be useless as coin destinations with overwhelming probability).

I'm not sure exactly the properties you want here and determining
these properties is not an easy task, but the case is far worse than
just two random values. For instance: (a). with a small modification
my algorithm can also find collisions containing targeted substrings,
(b). length extension attacks are possible with RIPEMD160.

(a). targeted cycles:

target1 = "str to prepend"
target2 = "str to end with"

seed = {0,1}^160
x = hash(seed)

for i in 2^80:
....x = hash(target1||x||target2)
x_final = x

y = hash(tartget1||x_final||target2)

for j in 2^80:
....if y == x_final:
........print "cycle len: "+j
........break
....y = hash(target1||y||target2)

If a collision is found, the two colliding inputs must both start with
"str to prepend" and end with the phrase "str to end with". As before
this only requires 2^81.5 computations and no real memory. For an
additional 2**80 an adversary has an good change of finding two
different targeted substrings which collide. Consider the case where
the attacker mixes the targeted strings with the hash output:

hash("my name is=0x329482039483204324423"+x[1]+", my favorite number
is="+x) where x[1] is the first bit of x.

(b). length extension attacks

Even if all the adversary can do is create two random values that
collide, you can append substrings to the input and get collisions.
Once you find two random values hash(x) = hash(y), you could use a
length extension attack on RIPEMD-160 to find hash(x||z) = hash(y||z).

Now the bitcoin wiki says:
"The padding scheme is identical to MD4 using Merkle–Damgård
strengthening to prevent length extension attacks."[1]

Which is confusing to me because:

1. MD4 is vulnerable to length extension attacks
2. Merkle–Damgård strengthening does not protect against length
extension: "Indeed, we already pointed out that none of the 64
variants above can withstand the 'extension' attack on the MAC
application, even with the Merkle-Damgard strengthening" [2]
3. RIPEMD-160 is vulnerable to length extension attacks, is Bitcoin
using a non-standard version of RIPEMD-160.

RIPEMD160(SHA256()) does not protect against length extension attacks
on SHA256, but should protect RIPEMD-160 against length extension
attacks as RIPEMD-160 uses 512-bit message blocks. That being said we
should be very careful here. Research has been done that shows that
cascading the same hash function twice is weaker than using HMAC[3]. I
can't find results on cascading RIPEMD160(SHA256()).

RIPEMD160(SHA256()) seems better than RIPEMD160() though, but security
should not rest on the notion that an attacker requires 2**80 memory,
many targeted collision attacks can work without much memory.

[1]: https://en.bitcoin.it/wiki/RIPEMD-160
[2]: "Merkle-Damgard Revisited: How to Construct a Hash Function"
https://www.cs.nyu.edu/~puniya/papers/merkle.pdf
[3]: https://www.cs.nyu.edu/~dodis/ps/h-of-h.pdf

On Thu, Jan 7, 2016 at 4:06 PM, Gavin Andresen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Maybe I'm asking this question on the wrong mailing list:
>
> Matt/Adam: do you have some reason to think that RIPEMD160 will be broken
> before SHA256?
> And do you have some reason to think that they will be so broken that the
> nested hash construction RIPEMD160(SHA256()) will be vulnerable?
>
> Adam: re: "where to stop"  :  I'm suggesting we stop exactly at the current
> status quo, where we use RIPEMD160 for P2SH and P2PKH.
>
> Ethan:  your algorithm will find two arbitrary values that collide. That
> isn't useful as an attack in the context we're talking about here (both of
> those values will be useless as coin destinations with overwhelming
> probability).
>
> Dave: you described a first preimage attack, which is 2**160 cpu time and no
> storage.
>
>
> --
> --
> Gavin Andresen
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
> Instead of a hash function you may use a keyed hash function (HMAC) where
> the key is just the random string. They key needs to be stored in the
> history of the coin to allow for verification.

This is nearly equivalent.  The sole purpose of the blinding factor is
to increase the search space and make preimaging of the output
impossible.  While in many applications HMAC is superior to plain hash
of a concatenation of the input data and the key (key = blinding
factor in our case), its preimage resistance is the same as that of
the hash.


2016-08-09 10:26 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:
>
> Hi Tony,
>
> > > Regarding the blinding factor, I think you could just use HMAC.
> > How exactly?
>
> I am not entirely sure if this works. You wrote:
>
> > There is one technical nuance that I omitted above to avoid distraction.
> >  Unlike regular bitcoin transactions, every output in a private payment
> > must also include a blinding factor, which is just a random string.  When
> > the output is spent, the corresponding spend proof will therefore depend on
> > this blinding factor (remember that spend proof is just a hash of the
> > output).  Without a blinding factor, it would be feasible to pre-image the
> > spend proof and reveal the output being spent as the search space of all
> > possible outputs is rather small.
>
> Instead of a hash function you may use a keyed hash function (HMAC) where
> the key is just the random string. They key needs to be stored in the
> history of the coin to allow for verification.
>
> Best
> Henning
>
> On Mon, Aug 08, 2016 at 07:03:28PM +0300, Tony Churyumoff wrote:
> > Hi Henning,
> >
> > 1. The fees are paid by the enclosing BTC transaction.
> > 2. The hash is encoded into an OP_RETURN.
> >
> > > Regarding the blinding factor, I think you could just use HMAC.
> > How exactly?
> >
> > Tony
> >
> >
> > 2016-08-08 18:47 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:
> >
> > > Hi Tony,
> > >
> > > I see some issues in your protocol.
> > >
> > > 1. How are mining fees handled?
> > >
> > > 2. Assume Alice sends Bob some Coins together with their history and
> > > Bob checks that the history is correct. How does the hash of the txout
> > > find its way into the blockchain?
> > >
> > > Regarding the blinding factor, I think you could just use HMAC.
> > >
> > > All the best
> > > Henning
> > >
> > >
> > > On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via bitcoin-dev
> > > wrote:
> > > > This is a proposal about hiding the entire content of bitcoin
> > > > transactions.  It goes farther than CoinJoin and ring signatures, which
> > > > only obfuscate the transaction graph, and Confidential Transactions,
> > > which
> > > > only hide the amounts.
> > > >
> > > > The central idea of the proposed design is to hide the entire inputs and
> > > > outputs, and publish only the hash of inputs and outputs in the
> > > > blockchain.  The hash can be published as OP_RETURN.  The plaintext of
> > > > inputs and outputs is sent directly to the payee via a private message,
> > > and
> > > > never goes into the blockchain.  The payee then calculates the hash and
> > > > looks it up in the blockchain to verify that the hash was indeed
> > > published
> > > > by the payer.
> > > >
> > > > Since the plaintext of the transaction is not published to the public
> > > > blockchain, all validation work has to be done only by the user who
> > > > receives the payment.
> > > >
> > > > To protect against double-spends, the payer also has to publish another
> > > > hash, which is the hash of the output being spent.  We’ll call this hash
> > > *spend
> > > > proof*.  Since the spend proof depends solely on the output being spent,
> > > > any attempt to spend the same output again will produce exactly the same
> > > > spend proof, and the payee will be able to see that, and will reject the
> > > > payment.  If there are several outputs consumed by the same transaction,
> > > > the payer has to publish several spend proofs.
> > > >
> > > > To prove that the outputs being spent are valid, the payer also has to
> > > send
> > > > the plaintexts of the earlier transaction(s) that produced them, then the
> > > > plaintexts of even earlier transactions that produced the outputs spent
> > > in
> > > > those transactions, and so on, up until the issue (similar to coinbase)
> > > > transactions that created the initial private coins.  Each new owner of
> > > the
> > > > coin will have to store its entire history, and when he spends the coin,
> > > he
> > > > forwards the entire history to the next owner and extends it with his own
> > > > transaction.
> > > >
> > > > If we apply the existing bitcoin design that allows multiple inputs and
> > > > multiple outputs per transaction, the history of ownership transfers
> > > would
> > > > grow exponentially.  Indeed, if we take any regular bitcoin output and
> > > try
> > > > to track its history back to coinbase, our history will branch every time
> > > > we see a transaction that has more than one input (which is not
> > > uncommon).
> > > > After such a transaction (remember, we are traveling back in time), we’ll
> > > > have to track two or more histories, for each respective input.  Those
> > > > histories will branch again, and the total number of history entries
> > > grows
> > > > exponentially.  For example, if every transaction had exactly two inputs,
> > > > the size of history would grow as 2^N where N is the number of steps back
> > > > in history.
> > > >
> > > > To avoid such rapid growth of ownership history (which is not only
> > > > inconvenient to move, but also exposes too much private information about
> > > > previous owners of all the contributing coins), we will require each
> > > > private transaction to have exactly one input (i.e. to consume exactly
> > > one
> > > > previous output).  This means that when we track a coin’s history back in
> > > > time, it will no longer branch.  It will grow linearly with the number of
> > > > transfers of ownership.  If a user wants to combine several inputs, he
> > > will
> > > > have to send them as separate private transactions (technically, several
> > > > OP_RETURNs, which can be included in a single regular bitcoin
> > > transaction).
> > > >
> > > > Thus, we are now forbidding any coin merges but still allowing coin
> > > > splits.  To avoid ultimate splitting into the dust, we will also require
> > > > that all private coins be issued in one of a small number of
> > > > denominations.  Only integer number of “banknotes” can be transferred,
> > > the
> > > > input and output amounts must therefore be divisible by the denomination.
> > > > For example, an input of amount 700, denomination 100, can be split into
> > > > outputs 400 and 300, but not into 450 and 250.  To send a payment, the
> > > > payer has to pick the unspent outputs of the highest denomination first,
> > > > then the second highest, and so on, like we already do when we pay in
> > > cash.
> > > >
> > > > With fixed denominations and one input per transaction, coin histories
> > > > still grow, but only linearly, which should not be a concern in regard to
> > > > scalability given that all relevant computing resources still grow
> > > > exponentially.  The histories need to be stored only by the current owner
> > > > of the coin, not every bitcoin node.  This is a fairer allocation of
> > > > costs.  Regarding privacy, coin histories do expose private transactions
> > > > (or rather parts thereof, since a typical payment will likely consist of
> > > > several transactions due to one-input-per-transaction rule) of past coin
> > > > owners to the future ones, and that exposure grows linearly with time,
> > > but
> > > > it is still much much better than having every transaction immediately on
> > > > the public blockchain.  Also, the value of this information for potential
> > > > adversaries arguably decreases with time.
> > > >
> > > > There is one technical nuance that I omitted above to avoid distraction.
> > > >  Unlike regular bitcoin transactions, every output in a private payment
> > > > must also include a blinding factor, which is just a random string.  When
> > > > the output is spent, the corresponding spend proof will therefore depend
> > > on
> > > > this blinding factor (remember that spend proof is just a hash of the
> > > > output).  Without a blinding factor, it would be feasible to pre-image
> > > the
> > > > spend proof and reveal the output being spent as the search space of all
> > > > possible outputs is rather small.
> > > >
> > > > To issue the new private coin, one can burn regular BTC by sending it to
> > > > one of several unspendable bitcoin addresses, one address per
> > > denomination.
> > > >  Burning BTC would entitle one to an equal amount of the new private
> > > coin,
> > > > let’s call it *black bitcoin*, or *BBC*.
> > > >
> > > > Then BBC would be transferred from user to user by:
> > > > 1. creating a private transaction, which consists of one input and
> > > several
> > > > outputs;
> > > > 2. storing the hash of the transaction and the spend proof of the
> > > consumed
> > > > output into the blockchain in an OP_RETURN (the sender pays the
> > > > corresponding fees in regular BTC)
> > > > 3. sending the transaction, together with the history leading to its
> > > input,
> > > > directly to the payee over a private communication channel.  The first
> > > > entry of the history must be a bitcoin transaction that burned BTC to
> > > issue
> > > > an equal amount of BCC.
> > > >
> > > > To verify the payment, the payee:
> > > > 1. makes sure that the amount of the input matches the sum of outputs,
> > > and
> > > > all are divisible by the denomination
> > > > 2. calculates the hash of the private transaction
> > > > 3. looks up an OP_RETURN that includes this hash and is signed by the
> > > > payee.  If there is more than one, the one that comes in the earlier
> > > block
> > > > prevails.
> > > > 4. calculates the spend proof and makes sure that it is included in the
> > > > same OP_RETURN
> > > > 5. makes sure the same spend proof is not included anywhere in the same
> > > or
> > > > earlier blocks (that is, the coin was not spent before).  Only
> > > transactions
> > > > by the same author are searched.
> > > > 6. repeats the same steps for every entry in the history, except the
> > > first
> > > > entry, which should be a valid burning transaction.
> > > >
> > > > To facilitate exchange of private transaction data, the bitcoin network
> > > > protocol can be extended with a new message type.  Unfortunately, it
> > > lacks
> > > > encryption, hence private payments are really private only when bitcoin
> > > is
> > > > used over tor.
> > > >
> > > > There are a few limitations that ought to be mentioned:
> > > > 1. After user A sends a private payment to user B, user A will know what
> > > > the spend proof is going to be when B decides to spend the coin.
> > > >  Therefore, A will know when the coin was spent by B, but nothing more.
> > > >  Neither the new owner of the coin, nor its future movements will be
> > > known
> > > > to A.
> > > > 2. Over time, larger outputs will likely be split into many smaller
> > > > outputs, whose amounts are not much greater than their denominations.
> > > > You’ll have to combine more inputs to send the same amount.  When you
> > > want
> > > > to send a very large amount that is much greater than the highest
> > > available
> > > > denomination, you’ll have to send a lot of private transactions, your
> > > > bitcoin transaction with so many OP_RETURNs will stand out, and their
> > > > number will roughly indicate the total amount.  This kind of privacy
> > > > leakage, however it applies to a small number of users, is easy to avoid
> > > by
> > > > using multiple addresses and storing a relatively small amount on each
> > > > address.
> > > > 3. Exchanges and large merchants will likely accumulate large coin
> > > > histories.  Although fragmented, far from complete, and likely outdated,
> > > it
> > > > is still something to bear in mind.
> > > >
> > > > No hard or soft fork is required, BBC is just a separate privacy
> > > preserving
> > > > currency on top of bitcoin blockchain, and the same private keys and
> > > > addresses are used for both BBC and the base currency BTC.  Every BCC
> > > > transaction must be enclosed into by a small BTC transaction that stores
> > > > the OP_RETURNs and pays for the fees.
> > > >
> > > > Are there any flaws in this design?
> > > >
> > > > Originally posted to BCT https://bitcointalk.org/index.
> > > php?topic=1574508.0,
> > > > but got no feedback so far, apparently everybody was consumed with
> > > bitfinex
> > > > drama and now mimblewimble.
> > > >
> > > > Tony
> > >
> > > > _______________________________________________
> > > > bitcoin-dev mailing list
> > > > bitcoin-dev@lists.linuxfoundation.org
> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > >
> > >
> > > --
> > > Henning Kopp
> > > Institute of Distributed Systems
> > > Ulm University, Germany
> > >
> > > Office: O27 - 3402
> > > Phone: +49 731 50-24138
> > > Web: http://www.uni-ulm.de/in/vs/~kopp
> > >
>
> --
> Henning Kopp
> Institute of Distributed Systems
> Ulm University, Germany
>
> Office: O27 - 3402
> Phone: +49 731 50-24138
> Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------
This is a bad idea. OP_RETURN attachments are tolerated (not encouraged!) for 
the sake of the network, since the spam cannot be outright stopped. If it 
could be outright stopped, it would not be reasonable to allow OP_RETURN. When 
it comes to the payment protocol, however, changing the current behaviour has 
literally no benefit to the network at all, and the changes proposed herein 
are clearly detrimental since it would both encourage spam, and potentially 
make users unwilling (maybe even unaware) participants in it. For these 
reasons, *I highly advise against publishing or implementing this BIP, even if 
the later mentioned issues are fixed.*

On Tuesday, January 26, 2016 1:02:44 AM Toby Padilla wrote:
> An example might be a merchant that adds the hash of a plain text invoice
> to the checkout transaction. The merchant could construct the
> PaymentRequest with the invoice hash in an OP_RETURN and pass it to the
> customer's wallet. The wallet could then submit the transaction, including
> the invoice hash from the PaymentRequest. The wallet will have encoded a
> proof of purchase to the blockchain without the wallet developer having to
> coordinate with the merchant software or add features beyond this BIP.

Such a "proof" is useless without wallet support. Even if you argue it could 
be implemented later on, it stands to reason that a scammer will simply encode 
garbage if the wallet is not checking the proof-of-purchase upfront. To check 
it, you would also need further protocol extensions which are not included in 
this draft.

> Merchants and Bitcoin application developers benefit from this BIP because
> they can now construct transactions that include OP_RETURN data in a
> keyless environment. Again, prior to this BIP, transactions that used
> OP_RETURN (with zero value) needed to be constructed and executed in the
> same software. By separating the two concerns, this BIP allows merchant
> software to create transactions with OP_RETURN metadata on a server without
> storing public or private Bitcoin keys. This greatly enhances security
> where OP_RETURN applications currently need access to a private key to sign
> transactions.

I don't see how this has any relevance to keys at all...

> ## Specification
> 
> The specification for this BIP is straightforward. BIP70 should be fully
> implemented with two changes:
> 
> 1. Outputs where the script is an OP_RETURN and the value is zero should be
> accepted by the wallet.
> 2. Outputs where the script is an OP_RETURN and the value is greater than
> zero should be rejected.
> 
> This is a change from the BIP70 requirement that all zero value outputs be
> ignored.

This does not appear to be backward nor even forward compatible. Old clients 
will continue to use the previous behaviour and transparently omit any 
commitments. New clients on the other hand will fail to include commitments 
produced by old servers. In other words, it is impossible to produce software 
compatible with both BIP 70 and this draft, and implementing either would 
result in severe consequences.

> As it exists today, BIP70 allows for OP_RETURN data storage at the expense
> of permanently destroyed Bitcoin.

It is better for the spammers to lose burned bitcoins, than have a way to 
avoid them.

Luke


-------------------------------------
> Based on previous crypto analysis result, the actual security of SHA512
> is not significantly higher than SHA256.
> maybe we should consider SHA3?

As far as I know the security of the symmetric cipher key mainly depends
on the PRNG and the ECDH scheme.

The HMAC_SHA512 will be used to "drive" keys from the ECDH shared secret.
HMAC_SHA256 would be sufficient but I have specified SHA512 to allow to
directly derive 512bits which allows to have two 256bit keys with one
HMAC operation (same pattern is used in BIP for the key/chaincode
derivation).

Keccak would be an alternative but we probably don't want to introduce
another new hash type just for the encryption.

</jonas>


-------------------------------------
I am a long-time developer and I have some experience in process groups. I
am going to try to keep this short. If you are interested in pursuing this
idea please reply to me privately so we don't put a burden on the list.

As per Satoshi's paper, the blockchain implements a distributed timestamp
service. It defeats double-spending by establishing a "total order" on
transactions. The "domain" on which the ordering takes place is the entire
coin, the money supply. It's obvious to me that total ordering does not
scale well as a use case, it's not a matter of implementation details or
design. It's the requirement which is a problem. Therefore when I see
mention of the many clever schemes proposed to make Bitcoin scalable I
already know that by using that proposal we are going to give up something.
And in some cases I see lengthy and complex proposals, and just what the
user is giving up is not easy to see.

I think that the user has to give up something in order for electronic cash
to really scale, and that something has to be non-locality. At the moment
Bitcoin doesn't know whether I am buying a laptop from 3,000 miles away or
300. This is a wonderful property, but this property makes it impossible to
partition the users geographically. I think that a simple and effective way
to do this is to partition the address using a hash. A convention could be
adopted whereby there is a well-known partition number for each geographic
location. Most users would use third-party clients and the client could
generate Bitcoin addresses until it hits one in the user's geographical
area.

The partitioning scheme could be hierarchical. For example there could be
partitions at the city, state, and country level. A good way to see how
this works in real life is shopping at Walmart, which is something like
4,000 stores. Walmart could have users pay local addresses, and then move
the money "up" to a regional or country level.

The problem is what to do when an address in partition A wants to pay an
address in partition B. This should be done by processing the transaction
in partition A first, and once the block is made a hash of that block
should be included in some block in partition B. After A has made the block
the coin has left A, it cannot be spent. Once B has made its block the coin
has "arrived" in B and can be spent. It can be seen that some transactions
span a longer distance than others, in that they require two or more
blocks. These transactions take longer to execute, and I think that that is
entirely okay.

Transaction verification benefits because a small merchant can accept
payments from local addresses only. Larger merchants can verify
transactions across two or more partitions.

Some will be concerned about 51% attacks on partitions. I would point
out that nodes could process transactions at random, so that the majority
of the computing power is well-balanced across all partitions.

Regards,
Akiva

-------------------------------------
Gavin Andresen via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> writes:
> How many years until we think a 2^84 attack where the work is an ECDSA
> private->public key derivation will take a reasonable amount of time?

vanitygen can generate keypairs pretty fast (on my CPU it's comparable
with hashing time), and there are ways to make it faster.  Since you can
generate multiple script variations, too, I think hashing is the
bottleneck.

Antminer S7 can do 4.73 Terahash per second for $1.2k.  (Double SHA, but
let's assume RIPEMD160(SHA256()) is the same speed).

766,760,562,123 seconds to do 3*2^80, so you'd need over 200 million
S7s to do it in an hour.[1] If you want to do that for $1M, wait 27
years and hope Moore's Law holds?

Also, a colleague points out you could use this attack against a site
like bitrated.com which publishes one side's pubkey, giving you a much
longer attack window.
     
Cheers,
Rusty.
[1] Weirdly, the bitcoin network is doing this much work every 57
    days, for about $92M.  If that's all the attack costs, it's under
    1M in 10 years.


-------------------------------------

> On 5 Dec 2016, at 04:00, adiabat <rx@awsomnet.org> wrote:
> 
> Interesting stuff! I have some comments, mostly about the header.
> 
> The header of forcenet is mostly described in Luke’s BIP, but I have made some amendments as I implemented it. The format is (size in parentheses; little endian):
> 
> Height (4), BIP9 signalling field (4), hardfork signalling field (3), merge-mining hard fork signalling field (1), prev hash (32), timestamp (4), nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), Hash TMR (32), Hash WMR (32), total tx size (8) , total tx weight (8), total sigops (8), number of tx (4), merkle branches leading to header C (compactSize + 32 bit hashes)
> 
> First, I'd really rather not have variable length fields in the header.  It's so much nicer to just have a fixed size.
> 
> Is having both TMR and WMR really needed?  As segwit would be required with this header type, and the WMR covers a superset of the data that the TMR does, couldn't you get rid of the TMR?  The only disadvantage I can see is that light clients may want a merkle proof of a transaction without having to download the witnesses for that transaction.  This seems pretty minor, especially as once they're convinced of block inclusion they can discard the witness data, and also the tradeoff is that light clients will have to download and store and extra 32 bytes per block, likely offsetting any savings from omitting witness data.
> 

I foresee there will be 2 types of headers under this system: the 80 bytes short header and the variable length full header. Short headers are enough to link everything up. SPV needs the full header only if they are interested in any tx in a block. 

> The other question is that there's a bit that's redundant: height is also committed to in the coinbase tx via bip 34 (speaking of which, if there's a hard-fork, how about reverting bip 34 and committing to the height with coinbase tx nlocktime instead?)

you could omit the transmission of nHeight, as it is implied (saving 4bytes). Storing nHeight of headers is what every full and SPV nodes would do anyway


> 
> Total size / weight / number of txs also feels pretty redundant.  Not a lot of space but it's hard to come up with a use for them.  Number of tx could be useful if you want to send all the leaves of a merkle tree, but you could also do that by committing to the depth of the merkle tree in the header, which is 1 byte.

Yes, I agree with you that these are not particularly useful. Sum tree is more useful but it has other problems (see my other reply)

Related discussion: https://github.com/jl2012/bitcoin/commit/69e613bfb0f777c8dcd2576fe1c2541ee7a17208 <https://github.com/jl2012/bitcoin/commit/69e613bfb0f777c8dcd2576fe1c2541ee7a17208>
> 
> Also how about making timestamp 8 bytes?  2106 is coming up soon :)

No need. See my other reply

> 
> Maybe this is too nit-picky; maybe it's better to put lots of stuff in for testing the forcenet and then take out all the stuff that wasn't used or had issues as it progresses.
> 
> Thanks and looking forward to trying out forcenet!
> 
> -Tadge


-------------------------------------
https://github.com/jl2012/bips/blob/mastopcodes/bip-mastopcodes.mediawiki

This BIP defines the scripting system in Merkelized Abstract Syntax Tree (BIP114). It re-enables some of the previously disabled opcodes, introduces new opcodes, and defines expandable opcodes for future extension.

It will:
	• re-enable CAT, SUBSTR, LEFT, RIGHT, INVERT, AND, OR, XOR, LSHIFT, and RSHIFT;
	• introduce new opcodes: DUPTOALTSTACK, DUPFROMALTSTACK, SWAPSTACK, SWAPCAT, and RESIZE;
	• define expandable opcodes for future softforks of stack manipulating opcodes: EXPAND1 to EXPAND32.


This BIP is based on the BIP114 MAST: https://github.com/bitcoin/bips/blob/master/bip-0114.mediawiki

Reference implementation, including the BIP9 logic and script tests, could be found at https://github.com/jl2012/bitcoin/tree/segwit_mast . This branch is rebased on top of the #7910 segwit PR. However, I have not tested the BIP9 activation.

The implementation of the re-enabled opcode are mostly taken from the Elements Project.

-------------

This BIP does not describe changes in CHECKSIG (e.g. new hash type, Schnorr sig), which I think should be another BIP.

I have also considered more radical changes. For example, make all comparison opcode to be “VERIFY” type, and a script passes if and only if the stack is exactly empty after evaluation.



-------------------------------------
A new BIP, as part of the SW softfork, is pending BIP number assignment:

https://github.com/bitcoin/bips/pull/270

This proposal defines a new transaction digest algorithm for signature 
verification in version 0 and version 1 witness program, in order to 
minimize redundant data hashing in verification (solving the O(n^2) 
issue), and to cover the input value by the signature (a frequently 
requested feature for cold wallet).

jl2012 via bitcoin-dev 於 2015-12-24 09:22 寫到:
> The SW payment address format BIP draft is ready and is pending BIP
> number assignment:
> https://github.com/bitcoin/bips/pull/267
> 
> This is the 3rd BIP for segwit. The 2nd one for Peer Services is being
> prepared by Eric Lombrozo
> 
> Eric Lombrozo via bitcoin-dev 於 2015-12-23 10:22 寫到:
>> I've been working with jl2012 on some SEGWIT BIPs based on earlier
>> discussions Pieter Wuille's implementation. We're considering
>> submitting three separate BIPs:
>> 
>> CONSENSUS BIP: witness structures and how they're committed to blocks,
>> cost metrics and limits, the scripting system (witness programs), and
>> the soft fork mechanism.
>> 
>> PEER SERVICES BIP: relay message structures, witnesstx serialization,
>> and other issues pertaining to the p2p protocol such as IBD,
>> synchronization, tx and block propagation, etc...
>> 
>> APPLICATIONS BIP: scriptPubKey encoding formats and other wallet
>> interoperability concerns.
>> 
>> The Consensus BIP is submitted as a draft and is pending BIP number
>> assignment: https://github.com/bitcoin/bips/pull/265 [1]
>> The other two BIPS will be drafted soon.
>> 
>> ---
>> Eric
>> 
>> Links:
>> ------
>> [1] https://github.com/bitcoin/bips/pull/265
>> 
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-------------------------------------
The Bitcoin Unlimited client needs a services bit to indicate that the node
is capable of communicating thin blocks.  We propose to use bit 4 as AFAIK
bit 3 is already earmarked for Segregated Witness.

Andrew

-------------------------------------
On Thu, Jun 23, 2016 at 1:39 PM, Peter Todd <pete@petertodd.org> wrote:
> On Thu, Jun 23, 2016 at 01:30:45PM +0200, Pieter Wuille wrote:
>> On Jun 23, 2016 12:56, "Peter Todd via bitcoin-dev" <
>> bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>> > In any case, I'd strongly argue that we remove BIP75 from the bips
>> repository,
>> > and boycott wallets that implement it. It's bad strategy for Bitcoin
>> developers
>> > to willingly participate in AML/KYC, just the same way as it's bad for
>> Tor to
>> > add wiretapping functionality, and W3C to support DRM tech. The minor
>> tactical
>> > wins you'll get our of this aren't worth it.
>>
>> I hope you're not seriously suggesting to censor a BIP because you feel it
>> is a bad idea.
>
> For the record, I think the idea of the bips repo being a pure publication
> platform isn't a good one and doesn't match reality; like it or not by
> accepting bips we're putting a stamp of some kind of approval on them.

We? I don't feel like I have any authority to say what goes into that
repository, and neither do you. We just give technical opinion on
proposals. The fact that it's under the bitcoin organization on github
is a historical artifact.

> I have zero issues with us exercising editorial control over what's in the bips
> repo; us doing so doesn't in any way prevent other's from publishing elsewhere.

Editorial control is inevitable to some extent, but I think that's
more a matter of process than of opinion. Things like "Was there
community discussion?", "Is it relevant?", "Is there a reference
implementation?". I don't think that you objecting for moral reasons
to an otherwise technically sound idea is a reason for removal of a
BIP. You are of course free to propose alternatives, or recommend
against its usage.

-- 
Pieter


-------------------------------------
On Wednesday 21 Sep 2016 18:01:30 Gregory Maxwell via bitcoin-dev wrote:
> On Tue, Sep 20, 2016 at 5:15 PM, Tom via bitcoin-dev
> 
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
> > BIP number for my FT spec.
> 
> This document does not appear to be concretely specified enough to
> review or implement from it.
> 
> For example, it does not specify the serialization of "integer"

It refers to the external specification which is linked at the bottom.
In that spec you'll see that "Integer" is the standard var-int that Bitcoin 
has used for years.

> nor does it specify how the
> presence of the optional fields are signaled 

How does one signals an optional field except of in the spec? Thats the job of 
a specification.

> nor the cardinality of
> the inputs or outputs. 

Did you miss this in the 3rd table ?  I suggest clicking on the github bips 
repo link as tables are not easy to read in mediawiki plain format that the 
email contained.

> For clearly variable length elements
> ('bytearray') no mention is made of their length encoding. etc.

Also in the external CMF spec.
 
> Without information like this, I don't see how someone could
> realistically begin reviewing this proposal.

I agree, that would be bad. Luckily that you just missed the link :)
Here it is;
https://github.com/bitcoinclassic/documentation/blob/master/spec/compactmessageformat.md

> The motivation seems unclear to me as well: The scheme is described as
> 'flexible' but it appears to remove flexibility from the existing
> system. The "schema" appears to be hardcoded and never communicated.

Being hardcoded and never communicated is what the current format does to. How 
does that "remove flexibility".

Also read my reply to Peter Todd on why this is flexible.

> If the goal is to simply have {snip}

It is not.

Thanks for asking, I understand that the CMF spec is useful to see as well. 
Hopefully you can now review it properly since I linked to it above.

Cheers!


-------------------------------------
On Wed, Jun 15, 2016 at 5:10 PM, Peter Todd <pete@petertodd.org> wrote:

> On Tue, Jun 14, 2016 at 05:14:23PM -0700, Bram Cohen via bitcoin-dev wrote:
>
> > The fundamental approach to handling the latency problem is to have the
> > utxo commitments trail a bit. Computing utxo commitments takes a certain
> > amount of time, too much to hold up block propagation but still hopefully
> > vastly less than the average amount of time between blocks. Trailing by a
> > single block is probably a bad idea because you sometimes get blocks back
> > to back, but you never get blocks back to back to back to back. Having
> the
> > utxo set be trailing by a fixed amount - five blocks is probably
> excessive
> > - would do a perfectly good job of keeping latency from every becoming an
> > issue. Smaller commitments for the utxos added and removed in each block
> > alone could be added without any significant performance penalty. That
> way
> > all blocks would have sufficient commitments for a completely up to date
> > proofs of inclusion and exclusion. This is not a controversial approach.
>
> Agreed - regardless of approach adding latency to commitment calculations
> of
> all kinds is something I think we all agree can work in principle, although
> obviously it should be a last resort technique when optimization fails.
>

An important point: Adding latency to utxo commitments does not imply
latency to proofs of inclusion and exclusion! If roots of what's added and
deleted in each block are added as well, then a proof of inclusion can be
done by having a proof of inclusion of the trailing utxo set followed by a
proof of exclusion from all the following deletion sets, or a proof of
inclusion in one of the single block addition sets followed by proofs of
exclusion from all the more recent deletion sets. Likewise a proof of
exclusion can be a proof of exclusion from the utxo set followed by proofs
of exclusion from all the more recent addition sets or a single proof of
inclusion in a recent deletion set.

This does make proofs larger (except in the case of recent deletions and
maybe recent additions) and adds complexity, so it shouldn't be done unless
necessary. But validation before block propagation needs to be extremely
fast, so for utxo roots this trick is probably both necessary and
sufficient.

-------------------------------------
On Wed, Jul 20, 2016 at 06:17:39AM +0000, Luke Dashjr wrote:
> On Wednesday, July 20, 2016 5:46:54 AM Peter Todd via bitcoin-dev wrote:
> > On Tue, Jul 19, 2016 at 10:35:39PM -0600, Sean Bowe via bitcoin-dev wrote:
> > > I'm requesting feedback for Hash Time-Locked Contract (HTLC) transactions
> > > in Bitcoin.
> > > 
> > > HTLC transactions allow you to pay for the preimage of a hash. CSV/CLTV
> > > can be used to recover your funds if the other party is not cooperative.
> > > These
> > > 
> > > scripts take the following general form:
> > >     [HASHOP] <digest> OP_EQUAL
> > >     OP_IF
> > >     
> > >         <seller pubkey>
> > >     
> > >     OP_ELSE
> > >     
> > >         <num> [TIMEOUTOP] OP_DROP <buyer pubkey>
> > >     
> > >     OP_ENDIF
> > >     OP_CHECKSIG
> > 
> > Note that because you're hashing the top item on the stack regardless
> > scriptSig's that satisfy HTLC's are malleable: that top stack item can be
> > changed anything in the digest-not-provided case and the script still
> > passes.
> 
> OP_SIZE
> OP_IF
>   [HASHOP] <digest> OP_EQUALVERIFY
>   <seller pubkey>
> OP_ELSE
>   <num> [TIMEOUTOP]
>   <buyer pubkey>
> OP_ENDIF
> OP_CHECKSIG

Ha! That's brilliant; good job.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Sat, Oct 15, 2016 at 1:00 PM, Tom Zander via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> My suggestion (sorry for not explaining it better) was that for BIPS to be a
> public domain (aka CC0) and a CC-BY option and nothing else.

Indeed, we agree that BIPs should be licensed as permissive as
possible. Still, I wonder why you chose otherwise with BIP 134.
(Currently OPL and CC-BY-SA)

> I like you agree with that part, but I see you added two licenses.
> Do you have a good reason to add MIT/BSD to that list? Otherwise I think we
> agree.

Licenses that only require attribution are generally compatible with
each other. I don't think we should pick one and only promote/endorse
this one. Let's just leave the decision to the BIP author.


> Well, it has this sentence;
>
>> This BIP is dual-licensed under the Open Publication License and
>> BSD 2-clause license.
>
> Which is a bit odd in light of the initial email from Luke that suggested we
> drop the Open Publication License and we use the CC ones instead in addition
> to the public domain one.

I am pretty sure this is required to host the current text of BIP 2 in
the repo, as currently BIP 1 still applies and still requires for all
BIPs either OPL or PD, which is one of the reasons I think we should
move forward with BIP 2 or amending BIP 1.


> Marco:
>> looks good and addressed the feedback which was
>> accumulated last year. If there are no objections I'd suggest to move
>> forward with BIP 2 in the next couple of days/weeks.
>
> Thats odd, you just stated you like the public domain (aka CC0) license, yet
> you encourage the BIP2 that states we can no longer use public domain for
> BIPs... Did you read it?
> It says;
>  «Public domain is not universally recognised as a legitimate action, thus
>   it is inadvisable.» [1]

BIP 2 does not forbid you to release your work under PD in
legislations where this is possible. None of the licenses mentioned in
BIP 2 is exclusive, so you can choose as many options as you like. One
of the goals of BIP 2 is to no longer allow PD as the only copyright
option.


>
> Also;
> This list has not seen a lot of traffic, if you want to make sure people keep
> using the BIP process, I think you need to reach out to the rest of the
> community and make sure this has been heard and discussed.
> Moving forward the way it is now will likely deminish the importance of the
> BIP process.
>
> I strongly suggest people make very clear any and all changes that are
> proposed and defend each of them with reasons why you want to change things.
>
>
> 1) if you write as a rationale "In some jurisdictions, public domain is not
> recognised as a legitimate legal action" then you can at least name those
> jurisdictions and explain how they *do* support things like GPL. Burden of
> proof is on the man who wants to change things.
> It looks fishy when lawyers disagree. See the CC wikipedia page;
>  "public domain: cc0 Freeing content globally without restrictions"

Luke is the BIP champion of BIP 2, so please cc him if you have
suggestions on how to improve the process of gathering community
consensus.

Marco


-------------------------------------
Hi,

Arguing about which wiki is better doesn't feel productive to me. Can we
just let BIP authors decide for themselves? Draft-BIP2 already has a
provision for allowing authors to specify a backup wiki of their own
choosing; can we just make that the policy in all cases (and drop the
need for a backup wiki)?

-Dave


-------------------------------------
As I feared, request on feedback for this specific BIP has devolved into a
general debate about the merits of soft-forks versus hard-forks (versus
semi-hard Kosher Free Range forks...).

I've replied to several people privately off-list to not waste people's
time rehashing arguments that have been argued to death in the past.

I do want to briefly address all of the concerns that stem from "what if a
significant fraction of hashpower (e.g. 25%) stick with the 1mb branch of
the chain."

Proof of work cannot be spoofed. If there is very little (a few percent) of
hashpower mining a minority chain, confirmations on that chain take orders
of magnitude longer.  I wrote about why the incentives are extremely strong
for only the stronger branch to survive here:
 http://gavinandresen.ninja/minority-branches

... the debate about whether or not that is correct doesn't belong here in
bitcoin-dev, in my humble opinion.

All of the security concerns I have seen flow from an assumption that
significant hashpower continues on the weaker branch. The BIP that is under
discussion assumes that analysis is correct. I have not seen any evidence
that it is not correct; all experience with previous forks (of both Bitcoin
and altcoins) is that the stronger branch survives and the weaker branch
very quickly dies.


As for the argument that creating and testing a patch for Core would take
longer than 28 days:

The glib answer is "people should just run Classic, then."

A less glib answer is it would be trivial to create a patch for Core that
accepted a more proof-of-work chain with larger blocks, but refused to mine
larger blocks.

That would be a trivial patch that would require very little testing
(extensive testing of 8 and 20mb blocks has already been done), and perhaps
would be the best compromise until we can agree on a permanent solution
that eliminates the arbitrary, contentious limits.

-- 
--
Gavin Andresen

-------------------------------------
Hi,

I'm currently compiling my Master's thesis about Coin Selection and my
presentation proposal to Scaling Bitcoin has been accepted.

For my thesis, I have analyzed the Coin Selection problem, created a
framework to simulate wallet behavior on basis of a sequence of
payments, and have re-implemented multiple coin selection strategies of
prominent Bitcoin wallets (Bitcoin Core, Mycelium, Breadwallet, and
Android Wallet for Bitcoin).

As the Scaling Bitcoin site suggests that research should be made
available to this mailing list, I would like to invite you to have a
look at:

http://murch.one/wp-content/uploads/2016/09/CoinSelection.pdf

The PDF (176 kB) contains a two page description of my on-going work,
including preliminary simulation results, and three figures showing the
simulated wallets' UTXO compositions at the end of the simulation.

I can provide further information as requested, and would welcome any
feedback.

→→ If anyone has another sequence of incoming and outgoing payment
amounts at hand that I could run my simulation on, I'd love to hear
about it.

Regards

Murch



-------------------------------------
I don't think every application of OP_RETURN could be classified as "spam".
I also don't think burning the value is going to dissuade anyone from going
down that route. I don't think lost value is better for anyone.

On Mon, Jan 25, 2016 at 7:12 PM, Luke Dashjr <luke@dashjr.org> wrote:

> On Tuesday, January 26, 2016 3:07:40 AM Toby Padilla wrote:
> > > I don't see any benefit to changing that. It is better that coins are
> > > burned.
> >
> > I think this is our fundamental disagreement. People will burn coins to
> > encode data, why allow this when there's a better alternative?
>
> My point is that there isn't a better alternative. The coins being burned,
> is
> strictly better than it being gratis.
>
> > > You *always* need a key, to redeem inputs... regardless of values.
> >
> > Correct, but with BIP70 that key is in the user's wallet and you can
> > construct transactions on another machine (thus not needing a key during
> > construction). Right now there's no way to do the transaction
> construction
> > on another machine with zero value OP_RETURNs.
>
> This is also a good thing. Spam should not be made easier or cheaper.
>
> Luke
>

-------------------------------------
Jorge, first, thanks again for your work on this.

Without creating and using a public blockchain interface in phase 2, how
will you isolate the database dependency from consensus critical code?
Is it that the interface will exist but you will recommend against its use?

This work presumes that the users of the library reject the argument
that the database implementation is consensus critical code. Faithful
reproduction of stored data is a prerequisite for a validity. But a
common store implementation is only slightly more reasonable for this
library than a common RAM implementation.

e

On 01/12/2016 09:53 AM, Jorge Timón wrote:
> After talking to some people about libconsensus in the last Hong Kong
> conference I realized that my initial plan of exposing one more thing
> at a time would actually probably slow things down.
> 
> There's still a promised pdf with pictures that will be released, and
> actually drafting the UML pictures helped realize that the whole
> explanation could be much simpler if #7091 was merged first as the
> last step in phase 1 (that phase has so many contributors that I will
> probably never get finished documenting it). Matt Corallo's idea of
> exposing VerifyScript() through a C API certainly helped a lot in
> cementing the more-minimal-than-earlier dependencies (thanks to Cory
> Fields among many other people before him) that are not part of the
> incomplete but existing libbitcoinconsensus library.
> 
> Given this success in protecting encapsulation by exposing things in a
> new library, my instinct was to expose more things: VerifyHeader(),
> VerifyTx() and VerifyBlock() [in that order].
> But all those three new functions depend on storage in one way or
> another. That was part of my reasoning to expose VerifyHeader() first,
> because I believe there will be less discussion on a common interface
> for the stored longest chain than for the utxo view (which may depend
> on other transactions spent within the same block).
> In any case, I realized we should finish putting all the consensus
> critical code in the libconsensus lib and then worry about its "final"
> API.
> 
> Therefore I changed the goal of the phase 2 in my libconsensus
> encapsulation planning from "expose VerifyHeader() in the existing
> libconsensus library" to "build all the consensus critical code within
> the existing libconsensus library, even if we don't expose anything
> else". I believe this is much feasible for a single Bitcoin Core
> release cycle and also more of a priority. Other implementations
> experimenting with libconsensus like
> https://github.com/libbitcoin/libbitcoin-consensus will have the
> chance to compare their reimplementations with the future complete
> libbitcoinconsensus without having to worry about the C API, which
> ideally they will help to define.
> 
> I repeat, the goal of phase 2 in my upcoming libconsensus
> encapsulation plan is to fully decouple libconsensus from Bitcoin
> Core.
> In phase 3, we can refine the storage interfaces and focus on a
> quasi-final C API.
> In phase 4, we can refine and take out code that doesn't belong in
> libconsensus like CTxOut::GetDustThreshold() in
> primitives/transaction.h and move all those consensus files to the
> consensus folder before creating a separate sub-repository like for
> libsecp256k1. Note that most of the file-moving work can be in
> parallel to phases 2 and 3 and, in fact, by any new developer that is
> willing to exchange rebase-patience for meaningless github stats (I'll
> do it if nobody else wants, but I'm more than happy to delegate there:
> I have more than enough github meaningless stats already).
> 
> As said, the document with pictures and the update to #6714 are still
> promised, but until they're ready, merging/reviewing #7091, #7287,
> #7310 and #7311 could do a great deal to make later steps in
> libconsensus phase 2 more readable.
> Most reviewers probably don't need to see any "big picture" to tell
> whether certain functions on Bitcoin Core are consensus-critical or
> not, or whether consensus critical code needs to depend on util.o or
> not.
> But I wouldn't be writing to the mailing list without a plan with
> further words nor pictures if I didn't had what I believe is a
> complete implementation of what I just defined as "libconsensus phase
> 2".
> 
> Phase 3 should finish long pending discussions like "should
> libconsensus be C++14 or plain C" which should NOT delay phase 2.
> Phase 4 should be mostly trivial: rename files to the target dir and
> move the remaining unused code out of libconsensus.
> Phase 5 should make Bitcoin Core eat its own dog food and use
> libbitcoinconsensus oonly by its generic C API (I'm sorry if this
> looks too far away for me to even think about detailing it).
> 
> The work in progress branch (but hopefully being finished, nit and
> merged within the 0.12.99 cycle) can be found in:
> https://github.com/jtimon/bitcoin/commits/libconsensus-f2
> 
> Before sipa asks, signing code may make it into a new library but
> SHOULDN'T BE PART OF LIBBITCOINCONSENSUS. Ideally, all exposed
> functions will return true or false and an error string. It is based
> on last-0.12.99 3cd836c1 but by popular demand I can open it as a
> "DEPENDENT-tagged" PR linking to smaller steps and keeping track of
> steps done. Analogous to the about to be replaced (for a simpler and
> more maintainable example of testchain) #6382. If people like
> Wladimir, Cory and Pieter cannot see that I've been able to reduce my
> overall cry-for-review noise thanks to github adoption of emacs'
> org-mode's [ ] vs [X] I can alwways leave those "big picture" branches
> as "private" branches out of the pull request count.
> 
> I expect to publish a phase 3 branch very shortly. But as said I
> expect a lot of discussion on the API part, so I don't expect big
> movements in phase 3 until phase 2 is done (as said phase 4 is
> orthogonal to anything, this time git will say "verified MOVEONLY" for
> us).
> 
> To finish this long mail, if you are new to free software and would
> like to get familiarized with Bitcoin Core development in particular,
> moving one file is a simple task that you can always besure you can do
> right.
> The way I plan to hand this to you, you won't need to convince anyone
> to publicly confirm that your "MOVEONLY" commit being legit, because
> all your remaining work will be to build on one platform (ideally you
> should do a gitian build, but embarrassingly enough for someone
> touching consensus code I just trust travis ) and trust travis (as
> said, that's what I do from my laptop, but I plan to buy my own
> building machine [and maybe outsource it for free in some protocol
> that hasn't been invented, sorry again for the distraction]) and fix
> the includes that have stopped working.
> 
> I intend to create an issue to move all the files in this list one by one:
> 
> https://github.com/bitcoin/bitcoin/pull/7091/files#diff-480477e89f9b6ddafb30c4383dcdd705R250
> 
> But don't hesitate to contact me if are eager for moving some files,
> because I believe we can save a few lines of total diff if we chose
> the order of the movements properly.
> 
> Sorry, I forgot many people read this list again.
> Happy to answer any question.
> 
> Specially about https://github.com/jtimon/bitcoin/commits/libconsensus-f2
> 


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512



On 11 May 2016 21:23:21 GMT-04:00, Russell O'Connor via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>Is the design and manufacturing processes for the most power efficient
>ASICs otherwise patent unencumbered?  If not, why do we care so much
>about
>this one patent over all the others that stand on the road between pen
>and
>paper computation and thermodynamically ideal computation?

If others are found that are significant I think we'd definitely consider fighting them as well.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJXM+Mh
AAoJEGOZARBE6K+yz4MH/RwBknvWv+/sXLcJop59gTgfphMlt2KRRDs37bOm+ptc
7eUK+70K6kT64gNEUqZPnYrdV/u1qMad6bo+5Xb3VYEN9jkaQfw6FnKbVJ2oRVSz
2iDgO+bAe92n72bEJobmMxBpvD8lv+OjCMkWANHT8wr2/toFa2+V7JPipeXkZzvq
E5qxhfCHNgoIS55S3LkgAI1cUFMVeYf5yc0MsSzmU3sO29OPuqEWTOgVeDwKF3GS
aNvMSEJeyZb0D4C7XPfwQmqhH6aWsno/7no/D7qYppgSWaP8JpwPW/ULGzfU9Fr9
WdwgD2bX3zgAA3dcNM1nJ4lkoqCuEm2I0dO6Cj39HjE=
=M5NE
-----END PGP SIGNATURE-----



-------------------------------------
Also, it's important to take note of the motivation behind not banning
duplicate tx hashes outright. Doing so would require that spent tx
hashes are retained forever. A pruning node will have no way of knowing
whether a new tx duplicates the hash of a preceding tx. Any
implementation that does retain such hashes and dismisses new txs on
that basis would fork against pruning nodes.

e

On 11/16/2016 04:43 PM, Eric Voskuil wrote:
>> This means that all future transactions will have different txids...
> rules do guarantee it.
> 
> No, it means that the chance is small, there is a difference.
> 
> If there is an address collision, someone may lose some money. If there
> is a tx hash collision, and implementations handle this differently, it
> will produce a chain split. As such this is not something that a node
> can just dismiss. If they do they are implementing a hard fork.
> 
> e
> 
> On 11/16/2016 04:31 PM, Tier Nolan via bitcoin-dev wrote:
>>
>>
>> On Thu, Nov 17, 2016 at 12:10 AM, Eric Voskuil via bitcoin-dev
>> <bitcoin-dev@lists.linuxfoundation.org
>> <mailto:bitcoin-dev@lists.linuxfoundation.org>> wrote:
>>
>>     Both of these cases resulted from exact duplicate txs, which BIP34 now
>>     precludes. However nothing precludes different txs from having the same
>>     hash.
>>
>>
>> The only way to have two transactions have the same txid is if their
>> parents are identical, since the txids of the parents are included in a
>> transaction.
>>
>> Coinbases have no parents, so it used to be possible for two of them to
>> be identical.
>>
>> Duplicate outputs weren't possible in the database, so the later
>> coinbase transaction effectively overwrote the earlier one.
>>
>> The happened for two coinbases.  That is what the exceptions are for.
>>
>> Neither of the those coinbases were spent before the overwrite
>> happened.  I don't even think those coinbases were spent at all.
>>
>> This means that every activate coinbase transaction has a unique hash
>> and all new coinbases will be unique.
>>
>> This means that all future transactions will have different txids.
>>
>> There might not be an explicit rule that says that txids have to be
>> unique, but barring a break of the hash function, they rules do
>> guarantee it.
> 


-------------------------------------
We introduce several concepts that rework the lightweight Bitcoin
client model in a manner which is secure, efficient and privacy
compatible.

Thea properties of BIP37 SPV [0] are unfortunately not as strong as
originally thought:

     * The expected privacy of the probabilistic nature of bloom
       filters does not exist [1][2], any user with a BIP37 SPV wallet
       should be operating under no expectation of privacy.
       Implementation flaws make this effect significantly worse, the
       behavior meaning that no matter how high the false positive
       rate (up to simply downloading the whole blocks verbatim) the
       intent of the client connection is recoverable.

     * Significant processing load is placed on nodes in the Bitcoin
       network by lightweight clients, a single syncing wallet causes
       (at the time of writing) 80GB of disk reads and a large amount
       of CPU time to be consumed processing this data. This carries
       significant denial of service risk [3], non-distinguishable
       clients can repeatedly request taxing blocks causing
       reprocessing on every request. Processed data is unique to every
       client, and can not be cached or made more efficient while
       staying within specification.

     * Wallet clients can not have strong consistency or security
       expectations, BIP37 merkle paths allow for a wallet to validate
       that an output was spendable at some point in time but does not
       prove that this output is not spent today.

     * Nodes in the network can denial of service attack all BIP37 SPV
       wallet clients by simply returning null filter results for
       requests, the wallet has no way of discerning if it has been
       lied to and may be made simply unaware that any payment has been
       made to them. Many nodes can be queried in a probabilistic manor
       but this increases the already heavy network load with little
       benefit.



We propose a new concept which can work towards addressing these
shortcomings.


A Bloom Filter Digest is deterministically created of every block
encompassing the inputs and outputs of the containing transactions,
the filter parameters being tuned such that the filter is a small
portion of the size of the total block data. To determine if a block
has contents which may be interesting a second bloom filter of all
relevant key material is created. A binary comparison between the two
filters returns true if there is probably matching transactions, and
false if there is certainly no matching transactions. Any matched
blocks can be downloaded in full and processed for transactions which
may be relevant.

The BFD can be used verbatim in replacement of BIP37, where the filter
can be cached between clients without needing to be recomputed. It can
also be used by normal pruned nodes to do re-scans locally of their
wallet without needing to have the block data available to scan, or
without reading the entire block chain from disk.

-

For improved probabilistic security the bloom filters can be presented
to lightweight clients by semi-trusted oracles. A client wallet makes
an assumption that they trust a set, or subset of remote parties
(wallet vendors, services) which all all sign the BFD for each block.
The BFD can be downloaded from a single remote source, and the hash of
the filters compared against others in the trust set. Agreement is a
weak suggestion that the filter has not been tampered with, assuming
that these parties are not conspiring to defraud the client.

The oracles do not learn any additional information about the client
wallet, the client can download the block data from either nodes on
the network, HTTP services, NTTP, or any other out of band
communication method that provides the privacy desired by the client.

-

The security model of the oracle bloom filter can be vastly improved
by instead committing a hash of the BFD inside every block as a soft-
fork consensus rule change. After this, every node in the network would
build the filter and validate that the hash in the block is correct,
then make a conscious choice discard it for space savings or cache the
data to disk.

With a commitment to the filter it becomes impossible to lie to
lightweight clients by omission. Lightweight clients are provided with
a block header, merkle path, and the BFD. Altering the BFD invalidates
the merkle proof, it's validity is a strong indicator that the client
has an unadulterated picture of the UTXO condition without needing to
build one itself. A strong assurance that the hash of the BFD means
that the filters can be downloaded out of band along with the block
data at the leisure of the client, allowing for significantly greater
privacy and taking load away from the P2P Bitcoin network.

Committing the BFD is not a hard forking change, and does not require
alterations to mining software so long as the coinbase transaction
scriptSig is not included in the bloom filter.


[0] https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki
[1] https://eprint.iacr.org/2014/763.pdf
[2] https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/
[3] https://github.com/petertodd/bloom-io-attack


-------------------------------------
On Tue, Feb 02, 2016 at 09:41:35AM -0800, Toby Padilla wrote:
> Then the moderation is being unevenly applied. Luke commented against my
> BIP multiple times right after it was published but it took hours for my
> responses to go through and I had to track people down on IRC to ask about
> it:
> 
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/thread.html

Keep in mind that actual human beings need to hit the approve button on
your posts; quite likely Luke happened to respond when those humans were
available, and you didn't. I personally had to do the exact same thing
the other day with one of my posts.

Moderation is an unfortunate thing to need, but this list is read by
literally hundreds of busy people, many of whome have had to unsubscribe
at various points in the past due to a lack of moderation. I wish we had
a better solution, but that's what we have. We're also not along in
using fairly agressive moderation, for example the
cryptography@metzdowd.com mailing list where Bitcoin was originally
announced uses manual approval moderation on all messages as well;
there's also an unmoderated offshoot of it, cryptography@randombit.net

(and feel free to start an unmoderated version of bitcoin-dev!)

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698

-------------------------------------
On Sunday, January 17, 2016 11:08:08 AM Wladimir J. van der Laan via bitcoin-
dev wrote:
> Preliminary release notes for the release can be found here:
> 
>     https://github.com/bitcoin/bitcoin/blob/0.12/doc/release-notes.md


The part which lists raw Git pull requests says:
> #6057 ac5476e re-enable wallet in autoprune

But the main, handwritten part does not mention this.
Is pruning really finished, i.e. could I safely use it as a wallet "end-user"?

IMHO it would be one of the most interesting feature for users, as it could 
fix the issue of taking >60 GB of disk space.

So if it is finished, please mention that
- it's finished
- how to enable it.


Thanks for your hard work! :)
-------------------------------------
The whole idea of BIP43 (which BIP44 bases on) is that how these BIPs
define balance retrieval never changes. This is to make sure you always
see the same balance on "same BIP" wallets (and same seed of course).

So if you want to add paths, it has to be a new BIP.


On 05/13/2016 03:16 PM, Daniel Weigl via bitcoin-dev wrote:
> Hello List,
> 
> With SegWit approaching it would make sense to define a common derivation scheme how BIP44 compatible wallets will handle P2(W)SH (and later on P2WPKH) receiving addresses.
> I was thinking about starting a BIP for it, but I wanted to get some feedback from other wallets devs first.
> 
> In my opinion there are two(?) different options: 
> 
> 1) Stay with the current Bip44 account, give the user for each public key the option to show it as a P2PKH-Address or a P2SH address and also scan the blockchain for both representation of each public key.
> 	+) This has the advantage, that the user does not need to decide or have to understand that he needs to migrate to a new account type
> 	-) The downside is that the wallet has to scan/look for ever twice as much addresses. In the future when we have a P2WPKH, it will be three times as much.
> 	-) If you have the same xPub/xPriv key in different wallets, you need to be sure both take care for the different address types
> 
> 2) Define a new derivation path, parallel to Bip44, but a different  'purpose' (eg. <BipNumber-of-this-BIP>' instead of 44'). Let the user choose which account he want to add ("Normal account", "Witness account").  
> 
> 	m / purpose' / coin_type' / account' / change / address_index
> 
> 	+) Wallet needs only to take care of 1 address per public key
> 	+) If you use more than one wallet on the same xPub/xPriv it will work or fail completely. You will notice it immediately that there is something wrong
> 	-) User has to understand that (s)he needs to migrate to a new account to get the benefits of SegWit
> 	+) Thus, its easier to make a staged roll-out, only user actively deciding to use SegWit will get it and we can catch bugs earlier.
> 	
> 3) other ideas?
> 
> My personal favourite is pt2.
> 
> Has any Bip44 compliant wallet already done any integration at this point?
> 
> Thx,
> Daniel/Mycelium
> 




-------------------------------------
On Thursday, 24 November 2016 22:39:05 CET Sergio Demian Lerner via bitcoin-
dev wrote:
> Without a detailed analysis, unlimited block size seems a risky change to
> Bitcoin, to me.

What exactly do you think is a ‘change’ in bitcoin here?

The concept of proof-of-work is that the longer a chain, the higher 
probability that that one will be extended for the simple reason that 
another chain will have to show a higher amount of proof of work to ‘win’.

As far as I understand the document from Peter, there is no change there at 
all. Only chains with more POW will win.
Or, to answer your example, miners will prefer to extend the chain with the 
most POW.

The other fact stays the same as well, if you protect from reorgs by 
expecting more confirmations. Nothing changes here either. The common-sense 6 
confirmations for things like exchange-deposits keep having the same 
security.

The basic idea that we have a 3 or 4 deep fork is a huge problem in Bitcoin. 
It hasn’t happened for ages, and we like it that way. The miners like it 
that way too. Its disruptive.
The is a problem that is not created by the ‘excessive block’ concept. It 
does, however, provide a possible solution to this very far-fetched problem.

You should also realize that the policy of a miner is stored in the 
coinbase.

That said, I’m sure there are improvements to be made to the policy that BU 
uses. But since this is a node-local policy, the consensus rules are not 
affected by it.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Sunday, 16 October 2016 12:35:58 CEST Gavin Andresen wrote:
> On Sun, Oct 16, 2016 at 10:58 AM, Tom Zander via bitcoin-dev <
> 
> bitcoin-dev@lists.linuxfoundation.org> wrote:
> > The fallow period sounds waaaay to short. I suggest 2 months at minimum
> > since anyone that wants to be safe needs to upgrade.
> 
> I asked a lot of businesses and individuals how long it would take them to
> upgrade to a new release over the last year or two.
> 
> Nobody said it would take them more than two weeks.

The question you asked them was likely about the block size. The main 
difference is that SPV users do not need to update after BIP109, but they do 
need to have a new wallet when SegWit transactions are being sent to them.

This upgrade affects also end users, not just businesses etc.

Personally, I'd say that 2 months is even too fast.
 
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Wed, 23 Mar 2016 16:24:12 +0100
Jonas Schnelli via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org>
wrote:
> Hi
> 
> I have just PRed a draft version of two BIPs I recently wrote.
> https://github.com/bitcoin/bips/pull/362
> 
> Two BIPs that addresses the problem of decoupling wallets/clients from
> nodes while assuming a user (or a group) know the remote peer.
> 
> Authentication would be necessary to selective allow bloom filtering
> of transactions, encryption or any other node service that might lead
> to fingerprinting or resource attacks. Authentication would also be a
> pre-requirement for certificate free encryption-handshakes that is
> (enough?) resistant to MITM attacks.
> 
> Encryption is highly recommended if you connect a SPV node to a
> trusted node.
> 
> Authentication would allow accessing private p2p extensions from a
> remote SPV peer (example: fee estimation).
> 
> I'm aware of other methods to increase privacy and integrity (tor,
> VPN, stunnel, etc.), however I think authentication and a basic
> communication encryption should be part of the protocol and its setup
> should be complete hassle-free.
> 
> Thanks for your feeback.
> 
> /jonas
> 

- The motivation sections seem weak. Why not use SSH? It would have
  similar setup requirements, and is already a deployed solution. If
  there are additional setup simplicities (compared to SSH),
  consider listing them. And if one of the motivating factors is
  complexity reduction from the various "do everything you could
  possible want" protocols/implementations, then add this to the
  motivation.
- ECDSA and "ec pubkey" are mentioned, but not the specific curve.
- The hash algorithm for ECDSA is not explicitly mentioned.
- There is no way to change the cryptographic primitives being used or
  to update to a new protocol version. Would it be done with a new
  message type `auth2` ... ?
- The following seems to be contradictory:

> If the responding peer could not lookup the requesting peer's
> identity-pubkey in the local authorized-peers database or if the
> responding peer could not verify the signature, the requested auth
> message must be ignored to avoid fingerprinting of peers with
> authentication support.
>
> Responding peers must ignore (banning would lead to fingerprinting)
> the requesting peer after 5 unsuccessfully authentication tries to
> avoid resource attacks.

  If I connect to a peer, send 5 auth messages followed by another type
  of message that gets no response, this could indicate auth support.
  Or is this supposed to say ignore further auth messages, but not
  other types of messages? The wording seems to suggest an ignore-all.
- The pubkey from the requester is sent in cleartext, which can be used
  as an identifier across connections (similar to a MAC, except it can
  be seen across every network hop and correlated across connection
  types). Hiding this will likely require encryption, and the protocol
  will start to look similar to CurveCP. If the additional complexity
  is not worth fixing this issue, a section in the encryption BIP
  should be added to explain the identifier leakage.
- The known-peers has an IP and port section. Should the requester limit
  signatures based on this information? This algorithm or process needs
  to be better defined than the vague paragraph about verifying the
  integrity of the remote peer; if an implementation uses the
  any-one-of server approach the known-peers file becomes more like a
  SSL CA list, which does not seem like the intent. However, the example
  at the bottom says "Requesting peer does a lookup of (F) in
  known-peers database (B)".
- The encryption portion does not mention the pubkey pairs in use for
  ECDH (this needs to be described), so I am assuming the pairs from
  authentication are re-used. This increases the chances of data
  exposure since a single botched k selection (re-use) for ECDSA would
  allow for forged authentications, and the decryption of all
  historical data. Adding a temporary key exchange would add slight
  complexity and one RTT from the requesters perspective, but it
  provides forward-secrecy and protection against ECDSA implementation
  failures.
- Can the responding peer set a different cipher in the `ecinit`
  response, or should/must it be the same?
- What happens if the responding peer does not support the cipher?
  Presumably, a rejection?
- The contents of the IV field are unspecified, and should be
  specified to contain new output from a CSPRNG for each message.
- Should `enc` messages be wrapped in `auth` messages (presumably so
  since there is no MAC)? `encinit` have this restriction, but nothing
  is specified for `enc`.
- Is the context hash unique in each direction? There seems to be one,
  which would be racy - what if the client wanted to pipeline messages?
  Or is the intent a single open request/response style? I think this
  _adds_ a restriction to the Bitcoin protocol.
- Instead of a hash, what about a counter in each direction for the
  `enc` stream? The auth portion verifies integrity, authenticity, and
  completeness of each message (including this counter). Missing
  messages (through TCP injection?) would still detected. Using TCP
  injection to forcefully teardown a connection is possible in both
  designs.

Lee


-------------------------------------
I've updated the language of the BIP. New version:

<pre>
  BIP: TBD
  Title: Best Practices for Heterogeneous Input Script Transactions
  Author: Kristov Atlas <kristov@openbitcoinprivacyproject.org>
  Status: Draft
  Type: Informational
  Created: 2016-02-10
</pre>

==Abstract==

The privacy of Bitcoin users with respect to graph analysis is reduced when
a transaction is created that contains inputs composed from different
scripts. However, creating such transactions is often unavoidable.

This document proposes a set of best practice guidelines which minimize the
adverse privacy consequences of such unavoidable transaction situations
while simultaneously maximising the effectiveness of user protection
protocols.

==Copyright==

This BIP is in the public domain.

==Definitions==

* '''Heterogenous input script transaction (HIT)''': A transaction
containing multiple inputs where not all inputs have identical scripts
(e.g. a transaction spending from more than one Bitcoin address)
* '''Unavoidable heterogenous input script transaction''': An HIT created
as a result of a user’s desire to create a new output with a value larger
than the value of his wallet's largest existing unspent output
* '''Intentional heterogenous input script transaction''': An HIT created
as part of a user protection protocol for reducing uncontrolled disclosure
of personally-identifying information (PII)

==Motivations==

The recommendations in this document are designed to accomplish three goals:

# Maximise the effectiveness of user-protecting protocols: Users may find
that protection protocols are counterproductive if such transactions have a
distinctive fingerprint which renders them ineffective.
# Minimise the adverse consequences of unavoidable heterogenous input
transactions: If unavoidable HITs are indistinguishable from intentional
HITs, a user creating an unavoidable HIT benefits from ambiguity with
respect to graph analysis.
# Limiting the effect on UTXO set growth: To date, non-standardized
intentional HITs tend to increase the network's UTXO set with each
transaction; this standard attempts to minimize this effect by
standardizing unavoidable and intentional HITs to limit UTXO set growth.

In order to achieve these goals, this specification proposes a set of best
practices for heterogenous input script transaction creation. These
practices accommodate all applicable requirements of both intentional and
unavoidable HITs while maximising the effectiveness of both in terms of
preventing uncontrolled disclosure of PII.

In order to achieve this, two forms of HIT are proposed: Standard form and
alternate form.

==Standard form heterogenous input script transaction==

===Rules===

An HIT is Standard form if it adheres to all of the following rules:

# The number of unique output scripts must be equal to the number of unique
inputs scripts (irrespective of the number of inputs and outputs).
# All output scripts must be unique.
# At least one pair of outputs must be of equal value.
# The largest output in the transaction is a member of a set containing at
least two identically-sized outputs.

===Rationale===

The requirement for equal numbers of unique input/output scripts instead of
equal number of inputs/outputs accommodates user-protecting UTXO selection
behavior. Wallets may contain spendable outputs with identical scripts due
to intentional or accidental address reuse, or due to dusting attacks. In
order to minimise the adverse consequences of address reuse, any time a
UTXO is included in a transaction as an input, all UTXOs with the same
spending script should also be included in the transaction.

The requirement that all output scripts are unique prevents address reuse.
Restricting the number of outputs to the number of unique input scripts
prevents this policy from growing the network’s UTXO set. A standard form
HIT transaction will always have a number of inputs greater than or equal
to the number of outputs.

The requirement for at least one pair of outputs in an intentional HIT to
be of equal value results in optimal behavior, and causes intentional HITs
to resemble unavoidable HITs.

==Alternate form heterogenous input script transactions==

The formation of a standard form HIT is not possible in the following cases:

# The HIT is unavoidable, and the user’s wallet contains an insufficient
number or size of UTXOs to create a standard form HIT.
# The user wishes to reduce the number of utxos in their wallet, and does
not have any sets of utxos with identical scripts.

When one of the following cases exist, a compliant implementation may
create an alternate form HIT by constructing a transaction as follows:

===Procedure===

# Find the smallest combination of inputs whose value is at least the value
of the desired spend.
## Add these inputs to the transaction.
## Add a spend output to the transaction.
## Add a change output to the transaction containing the difference between
the current set of inputs and the desired spend.
# Repeat step 1 to create a second spend output and change output.
# Adjust the change outputs as necessary to pay the desired transaction fee.

Clients which create intentional HITs must have the capability to form
alternate form HITs, and must do so for a non-zero fraction of the
transactions they create.

==Non-compliant heterogenous input script transactions==

If a user wishes to create an output that is larger than half the total
size of their spendable outputs, or if their inputs are not distributed in
a manner in which the alternate form procedure can be completed, then the
user can not create a transaction which is compliant with this procedure.

-------------------------------------
On Sat, Feb 06, 2016 at 04:11:58PM -0500, Peter Todd via bitcoin-dev wrote:
> On Sat, Feb 06, 2016 at 12:45:14PM -0500, Gavin Andresen via bitcoin-dev wrote:
> > On Sat, Feb 6, 2016 at 12:01 PM, Adam Back <adam@cypherspace.org> wrote:
> > 
> > >
> > > It would probably be a good idea to have a security considerations
> > > section
> > 
> > 
> > Containing what?  I'm not aware of any security considerations that are any
> > different from any other consensus rules change.
> 
> I covered the security considerations unique to hard-forks on my blog:
> 
> https://petertodd.org/2016/soft-forks-are-safer-than-hard-forks

Oh, and to be 100% clear, I should say those are only *some of* the
unique security considerations - for starters the article is mainly
talking about uncontroversial hard-forks, and doesn't even delve into
economic attacks among other omissions. It's just an introductory
article.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698

-------------------------------------
On Mon, Jan 11, 2016 at 11:57 PM, Tier Nolan <tier.nolan@gmail.com> wrote:
>
>     else:
>         script = "CHECKSIG %s OP_DROP" % (prev_hash, const_pub_key)
>

That should be

script = "%s CHECKSIG %s OP_DROP" % (const_pub_key, prev_hash)

-------------------------------------
The following draft BIP proposes changing the MAX_BLOCK_SIZE consensus rule
to be a function of the median block size over the last 12,960 blocks
(about three months) multiplied by 2 and calculated when a block is
connected to the blockchain.

Motivation:

The purpose of this consensus rule change is to allow the maximum block
size to increase or decrease based on actual network usage. A block size
limit, as discussed here, prevents certain types of denial of service
attacks on the Bitcoin network, however a fixed limit does not allow the
capacity of the network to increase as advancements in scaling are
realized.  By adjusting the limit based on the sizes of blocks in the
recent past, the throughput of the network can adjust to changes in user
demand and scaling related technology advancements while still being
protected from denial of service attacks.


Full proposal here:

https://github.com/bitpay/bips/blob/master/bip-adaptiveblocksize.mediawiki

I look forward to your consideration.

thank you,
-- 
Chris Kleeschulte

-------------------------------------
Hi All,

   Here is a suggestion which is similar to bip-0065, but slightly
different.
In a nutshell I under stand bip-0065 to do this;
Create a transaction adding a lock time, that the recipient user must wait
before they can spend the coins.

My proposal is to do this;
Create an entry in the blocks to lock entire wallet addresses indefinitely,
with a specified unlock period.
Later on create / modify an entry in the blocks to acknowledge the wallet
is being unlocked.
Remove the lock on the wallet after the unlock period has transpired.

  I think it is technically feasible since many wallet addresses are in
each block at the transaction level.  However, it would have huge
implications to the entire Bitcion ecosystem, so it would probably need a
start date at least a year in the future after it was developed.

bip-0065 would not allow the following;
  This would allow users holding coins for long periods to monitor the
blockchain to see if someone else is unlocking their wallets (which may
have been stolen/copied etc), giving them some time to react to a
intrusion.  Perhaps there should also be a re-lock (during unlock) feature.

My original message is attached.

Cheers,
Scott

---------- Forwarded message ----------
From: Scott Morgan <scott@adligo.com>
Date: Tue, Jan 12, 2016 at 3:35 PM
Subject: Wallet Lock, Unlock BIP idea
To: bitcoin-dev@lists.linuxfoundation.org


Hi All,

   It seems to me that one of the large issues with bitcoin is that they
can be stolen like cash.   This issue also culminates with the fact that
most miners probably need to hold their coins for some time to be
profitable due to the large interest in mining.
   I think it may be possible to reduce some of this theft by adding a BIP
to lock and unlock wallets.  Here is the basic idea (probably with some
holes);

   1) Users could 'lock' their wallet specifying a unlock period (i.e. 15
days)
       The information that a particular wallet is locked would get added
to the blocks and confirmed like other transactions.
    2) During transaction creation and mining (to be sure a locked wallet
isn't drained) the top blocks would be checked to see if the wallet is
locked.  Locked wallet transactions would not be confirmed.
    3)  Users would eventually 'unlock' their wallet.
        This would put a unlocking as of date time in the blocks to specify
a wallet is unlocking.  Eventually the wallet would not have any lock or
unlocking entries in the blocks.
    4) The users would wait the unlock period (i.e. 15 days)
    5) The Users could then spend their coins.


   This would also have some other consequences on the bitcoin system,
since anyone could check the transactions to locked wallets to see how many
BTC are being held, or are being unlocked soon.   This could effect the
price of BTC in fiat as supply would change similar to the way mining
changes it.  Also it will slow transaction creation a little and mining a
fair amount.
   Also locking a wallet might incur a fee.

  What are your thoughts, does this idea qualify for a BIP?
  If so, I would appreciate it if someone takes it and runs with it.

Cheers,
Scott

PS A bit about me, I am a Privacy and Java evangelist, so I will not be
doing any work on the main bitcoin core.  I have been doing a little mining
to attempt to help fund my companies (Adligo Inc) open source Java projects
Tests4j and Fabricate and hopefully in the future Taxi, Sanctum and
Intelligence4j.

Donations are always welcome;

http://www.plumfund.com/crowdfunding/adligoorg

-------------------------------------
The alert system was a centralized facility to allow trusted parties
to send messages to be displayed in wallet software (and, very early
on, actually remotely trigger the software to stop transacting).

It has been removed completely in Bitcoin Core after being disabled for a while.

While the system had some potential uses, there were a number of
problems with it.

The alert system was a frequent source of misunderstanding about the
security model and 'effective governance', for example a years ago a
BitcoinJ developer wanted it to be used to control fee levels on the
network and few months back one of Bloq's staff was pushing for a
scheme where "the developers" would use it to remotely change the
difficulty-- apparently with no idea how abhorrent others would find
it.

The system also had a problem of not being scalable to different
software vendors-- it didn't really make sense that core would have
that facility but armory had to do something different (nor would it
really make sense to constantly have to maintain some list of keys in
the node software).

It also had the problem of being unaccountable. No one can tell which
of the key holders created a message. This creates a risk of misuse
with a false origin to attack someone's reputation.

Finally, there is good reason to believe that the key has been
compromised-- It was provided to MTGox by a developer and MTGox's
systems' were compromised and later their CEO's equipment taken by the
Japanese police.

In any case, it's gone now in Core and most other current software--
and I think it's time to fully deactivate it.

I've spent some time going around the internet looking for all
software that contains this key (which included a few altcoins) and
asked them to remove it. I will continue to do that.

One of the facilities in the alert system is that you can send a
maximum sequence alert which cannot be overridden and displays only a
static key compromise text message and blocks all other alerts. I plan
to send a triggering alert in the not-distant future (exact time to be
announced well in advance) feedback on timing would be welcome.

There are likely a few production systems that automatically shut down
when there is an alert, so this risks some small one-time disruption
of those services-- but none worse than if an alert were sent to
advise about a new system upgrade.

At some point after that, I would then plan to disclose this private
key in public, eliminating any further potential of reputation attacks
and diminishing the risk of misunderstanding the key as some special
trusted source of authority.

Cheers,


-------------------------------------

There are two different topics mixed up here.

1. Link-level security (secure connection to the node we intended to connect to).

2. Node-level security (aka; don't connect to a 'evil node').

The fist requires link-level encryption and authentication.

The second requires identity authentication.

You described the 'evil node' attack; that indeed needs an identity system to stop. However BIP151 doesn't intend to protect against connecting to evil Bitcoin Nodes.

It is important not to mixup link-level authentication and node-level authentication.

When your client picks random nodes to connect to, you are not considered whom in particular runs them. (Rather that you have a good random sample of the network).

If you manually add a friends node; at this point you wish to have node-level authentication.  However, this may (and probably should) happen out-of-band.


Sent from my iPhone

> On 29 Jun 2016, at 01:07, Eric Voskuil <eric@voskuil.org> wrote:
> 
> Hi Cameron, good to hear from you!
> 
>> On Jun 28, 2016, at 11:40 PM, Cameron Garnham <da2ce7@gmail.com> wrote:
>> 
>> Unauthenticated link level encryption is wonderful! MITM attacks are overrated; as they require an active attacker.
> 
> This is not really the case with Bitcoin. A MITM attack does not require that the attacker find a way to inject traffic into the communication between nodes. Peers will connect to the attacker directly, or accept connections directly from it. Such attacks can be easier than even passive attacks.
> 
>> Stopping passive attacks is the low hanging fruit. This should be taken first.
>> 
>> Automated and secure peer authentication in a mesh network is a huge topic. One of the unsolved problems in computer science.
>> 
>> A simple 'who is that' by asking for the fingerprint of your peers from your other peers is a very simple way to get 'some' authentication.  Semi-trusted index nodes also is a low hanging fruit for authentication.
> 
> It is the implication of widespread authentication that is at issue. Clearly there are ways to implement it using a secure side channels.
> 
>> However, let's first get unauthenticated encryption. Force the attackers to use active attacks. (That are thousands times more costly to couduct).
>> 
>> Sent from my iPhone
>> 
>>> On 29 Jun 2016, at 00:36, Gregory Maxwell via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> 
>>> On Tue, Jun 28, 2016 at 9:22 PM, Eric Voskuil via bitcoin-dev
>>> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>>> An "out of band key check" is not part of BIP151.
>>> 
>>> It has a session ID for this purpose.
>>> 
>>>> It requires a secure channel and is authentication. So BIP151 doesn't provide the tools to detect an attack, that requires authentication. A general requirement for authentication is the issue I have raised.
>>> 
>>> One might wonder how you ever use a Bitcoin address, or even why we
>>> might guess these emails from "you" aren't actually coming from the
>>> NSA.
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev@lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
On Tue, Jun 21, 2016 at 01:28:48AM +0300, Alex Mizrahi wrote:
> > All practical single-use seals will be associated with some kind of
> > condition,
> > such as a pubkey, or deterministic expression, that needs to be satisfied
> > for
> > the seal to be closed.
> 
> 
> I think it would be useful to classify systems w.r.t. what data is
> available to condition.
> I imagine it might be useful if status of other seals is available.

Useful yes, but actually implementing that often results in systems that are
too tightly coupled to scale well.

> > Secondly, the contents of the proof will be able to
> > commit to new data, such as the transaction spending the output associated
> > with
> > the seal.
> >
> 
> So basically a "condition" returns that "new data", right?
> If it commits to a data in a recognizable way, then it's practically a
> function which yields a tuple (valid, new_data).
> If an oracle doesn't care about data then you can convert it to a predicate
> using a simple projection.
> But from point of view of a client, it is a function which returns a tuple.

What do you mean by "new data"?

The point I'm making is simply that to be useful, when you close a seal you
have to be able to close it over some data, in particular, another seal. That's
the key thing that makes the idea a useful construct for smart contacts, value
transfer/currency systems, etc.

> It might help if you describe a type of the condition function.

I did describe some seal authorization condition functions in my more recent
post; the key thing is you'd have some kind of "checksig" operator that checks
a cryptographic signature.

> Some related work on UTXO-based smart contracts:

<snip>

Thanks for the links! Not at all surprising to me that there's a whole bunch of
projects working along those same lines; it's the obvious way to build this
kind of stuff once you realise that the imperative, stateful, model isn't
viable.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------

> On Jun 30, 2016, at 2:20 PM, Jonas Schnelli <dev@jonasschnelli.ch> wrote:
> 
> 
>> Yes, this is exactly what I meant. The complexity of the proposed construction is comparable to that of Bitcoin itself. This is not itself prohibitive, but it is clearly worthy of consideration.
>> 
>> A question we should ask is whether decentralized anonymous credentials is applicable to the authentication problem posed by BIP151. I propose that it is not.
>> 
>> The core problem posed by BIP151 is a MITM attack. The implied solution (BIP151 + authentication) requires that a peer trusts that another is not an attacker.
> 
> BIP151 would increase the risks for MITM attackers.
> What are the benefits for Mallory of he can't be sure Alice and Bob may
> know that he is intercepting the channel?

It is not clear to me why you believe an attack on privacy by an anonymous peer is detectable.

> MITM is possible today, it would still be possible (though under higher
> costs) with BIP151.
> 
> With BIP151 we would have the basic tool-set to effectively reduce the
> risks of being MITMled.
> 
> IMO we should focus on the risks and benefits of BIP151 and not drag
> this discussion into the realm of authentication. This can and should be
> done once we have proposals for authentication (and I'm sure this will
> be a heated debate).
> 
> The only valid risk I have on my list from you, Eric, is the false sense
> of security.
> 
> My countermeasure for that would be...
> - deploy BIP151 together with the simplest form of authentication
> (know_hosts / authorized_keys file, no TOFU only editable "by hand")
> - make it more clear (in the BIP151 MOTIVATION text) that it won't solve
> the privacy/MITM problem without additional authentication.
> 
> Or could you elaborate again – without stepping into the realm of
> authentication/MITM (which is not part of the BIP or possible already
> today) – why BIP151 would make things worse?
> 
> </jonas>
> 


-------------------------------------
Hi Tony,

> > Regarding the blinding factor, I think you could just use HMAC.
> How exactly?

I am not entirely sure if this works. You wrote:

> There is one technical nuance that I omitted above to avoid distraction.
>  Unlike regular bitcoin transactions, every output in a private payment
> must also include a blinding factor, which is just a random string.  When
> the output is spent, the corresponding spend proof will therefore depend on
> this blinding factor (remember that spend proof is just a hash of the
> output).  Without a blinding factor, it would be feasible to pre-image the
> spend proof and reveal the output being spent as the search space of all
> possible outputs is rather small.

Instead of a hash function you may use a keyed hash function (HMAC) where
the key is just the random string. They key needs to be stored in the
history of the coin to allow for verification.

Best
Henning

On Mon, Aug 08, 2016 at 07:03:28PM +0300, Tony Churyumoff wrote:
> Hi Henning,
> 
> 1. The fees are paid by the enclosing BTC transaction.
> 2. The hash is encoded into an OP_RETURN.
> 
> > Regarding the blinding factor, I think you could just use HMAC.
> How exactly?
> 
> Tony
> 
> 
> 2016-08-08 18:47 GMT+03:00 Henning Kopp <henning.kopp@uni-ulm.de>:
> 
> > Hi Tony,
> >
> > I see some issues in your protocol.
> >
> > 1. How are mining fees handled?
> >
> > 2. Assume Alice sends Bob some Coins together with their history and
> > Bob checks that the history is correct. How does the hash of the txout
> > find its way into the blockchain?
> >
> > Regarding the blinding factor, I think you could just use HMAC.
> >
> > All the best
> > Henning
> >
> >
> > On Mon, Aug 08, 2016 at 06:30:21PM +0300, Tony Churyumoff via bitcoin-dev
> > wrote:
> > > This is a proposal about hiding the entire content of bitcoin
> > > transactions.  It goes farther than CoinJoin and ring signatures, which
> > > only obfuscate the transaction graph, and Confidential Transactions,
> > which
> > > only hide the amounts.
> > >
> > > The central idea of the proposed design is to hide the entire inputs and
> > > outputs, and publish only the hash of inputs and outputs in the
> > > blockchain.  The hash can be published as OP_RETURN.  The plaintext of
> > > inputs and outputs is sent directly to the payee via a private message,
> > and
> > > never goes into the blockchain.  The payee then calculates the hash and
> > > looks it up in the blockchain to verify that the hash was indeed
> > published
> > > by the payer.
> > >
> > > Since the plaintext of the transaction is not published to the public
> > > blockchain, all validation work has to be done only by the user who
> > > receives the payment.
> > >
> > > To protect against double-spends, the payer also has to publish another
> > > hash, which is the hash of the output being spent.  We’ll call this hash
> > *spend
> > > proof*.  Since the spend proof depends solely on the output being spent,
> > > any attempt to spend the same output again will produce exactly the same
> > > spend proof, and the payee will be able to see that, and will reject the
> > > payment.  If there are several outputs consumed by the same transaction,
> > > the payer has to publish several spend proofs.
> > >
> > > To prove that the outputs being spent are valid, the payer also has to
> > send
> > > the plaintexts of the earlier transaction(s) that produced them, then the
> > > plaintexts of even earlier transactions that produced the outputs spent
> > in
> > > those transactions, and so on, up until the issue (similar to coinbase)
> > > transactions that created the initial private coins.  Each new owner of
> > the
> > > coin will have to store its entire history, and when he spends the coin,
> > he
> > > forwards the entire history to the next owner and extends it with his own
> > > transaction.
> > >
> > > If we apply the existing bitcoin design that allows multiple inputs and
> > > multiple outputs per transaction, the history of ownership transfers
> > would
> > > grow exponentially.  Indeed, if we take any regular bitcoin output and
> > try
> > > to track its history back to coinbase, our history will branch every time
> > > we see a transaction that has more than one input (which is not
> > uncommon).
> > > After such a transaction (remember, we are traveling back in time), we’ll
> > > have to track two or more histories, for each respective input.  Those
> > > histories will branch again, and the total number of history entries
> > grows
> > > exponentially.  For example, if every transaction had exactly two inputs,
> > > the size of history would grow as 2^N where N is the number of steps back
> > > in history.
> > >
> > > To avoid such rapid growth of ownership history (which is not only
> > > inconvenient to move, but also exposes too much private information about
> > > previous owners of all the contributing coins), we will require each
> > > private transaction to have exactly one input (i.e. to consume exactly
> > one
> > > previous output).  This means that when we track a coin’s history back in
> > > time, it will no longer branch.  It will grow linearly with the number of
> > > transfers of ownership.  If a user wants to combine several inputs, he
> > will
> > > have to send them as separate private transactions (technically, several
> > > OP_RETURNs, which can be included in a single regular bitcoin
> > transaction).
> > >
> > > Thus, we are now forbidding any coin merges but still allowing coin
> > > splits.  To avoid ultimate splitting into the dust, we will also require
> > > that all private coins be issued in one of a small number of
> > > denominations.  Only integer number of “banknotes” can be transferred,
> > the
> > > input and output amounts must therefore be divisible by the denomination.
> > > For example, an input of amount 700, denomination 100, can be split into
> > > outputs 400 and 300, but not into 450 and 250.  To send a payment, the
> > > payer has to pick the unspent outputs of the highest denomination first,
> > > then the second highest, and so on, like we already do when we pay in
> > cash.
> > >
> > > With fixed denominations and one input per transaction, coin histories
> > > still grow, but only linearly, which should not be a concern in regard to
> > > scalability given that all relevant computing resources still grow
> > > exponentially.  The histories need to be stored only by the current owner
> > > of the coin, not every bitcoin node.  This is a fairer allocation of
> > > costs.  Regarding privacy, coin histories do expose private transactions
> > > (or rather parts thereof, since a typical payment will likely consist of
> > > several transactions due to one-input-per-transaction rule) of past coin
> > > owners to the future ones, and that exposure grows linearly with time,
> > but
> > > it is still much much better than having every transaction immediately on
> > > the public blockchain.  Also, the value of this information for potential
> > > adversaries arguably decreases with time.
> > >
> > > There is one technical nuance that I omitted above to avoid distraction.
> > >  Unlike regular bitcoin transactions, every output in a private payment
> > > must also include a blinding factor, which is just a random string.  When
> > > the output is spent, the corresponding spend proof will therefore depend
> > on
> > > this blinding factor (remember that spend proof is just a hash of the
> > > output).  Without a blinding factor, it would be feasible to pre-image
> > the
> > > spend proof and reveal the output being spent as the search space of all
> > > possible outputs is rather small.
> > >
> > > To issue the new private coin, one can burn regular BTC by sending it to
> > > one of several unspendable bitcoin addresses, one address per
> > denomination.
> > >  Burning BTC would entitle one to an equal amount of the new private
> > coin,
> > > let’s call it *black bitcoin*, or *BBC*.
> > >
> > > Then BBC would be transferred from user to user by:
> > > 1. creating a private transaction, which consists of one input and
> > several
> > > outputs;
> > > 2. storing the hash of the transaction and the spend proof of the
> > consumed
> > > output into the blockchain in an OP_RETURN (the sender pays the
> > > corresponding fees in regular BTC)
> > > 3. sending the transaction, together with the history leading to its
> > input,
> > > directly to the payee over a private communication channel.  The first
> > > entry of the history must be a bitcoin transaction that burned BTC to
> > issue
> > > an equal amount of BCC.
> > >
> > > To verify the payment, the payee:
> > > 1. makes sure that the amount of the input matches the sum of outputs,
> > and
> > > all are divisible by the denomination
> > > 2. calculates the hash of the private transaction
> > > 3. looks up an OP_RETURN that includes this hash and is signed by the
> > > payee.  If there is more than one, the one that comes in the earlier
> > block
> > > prevails.
> > > 4. calculates the spend proof and makes sure that it is included in the
> > > same OP_RETURN
> > > 5. makes sure the same spend proof is not included anywhere in the same
> > or
> > > earlier blocks (that is, the coin was not spent before).  Only
> > transactions
> > > by the same author are searched.
> > > 6. repeats the same steps for every entry in the history, except the
> > first
> > > entry, which should be a valid burning transaction.
> > >
> > > To facilitate exchange of private transaction data, the bitcoin network
> > > protocol can be extended with a new message type.  Unfortunately, it
> > lacks
> > > encryption, hence private payments are really private only when bitcoin
> > is
> > > used over tor.
> > >
> > > There are a few limitations that ought to be mentioned:
> > > 1. After user A sends a private payment to user B, user A will know what
> > > the spend proof is going to be when B decides to spend the coin.
> > >  Therefore, A will know when the coin was spent by B, but nothing more.
> > >  Neither the new owner of the coin, nor its future movements will be
> > known
> > > to A.
> > > 2. Over time, larger outputs will likely be split into many smaller
> > > outputs, whose amounts are not much greater than their denominations.
> > > You’ll have to combine more inputs to send the same amount.  When you
> > want
> > > to send a very large amount that is much greater than the highest
> > available
> > > denomination, you’ll have to send a lot of private transactions, your
> > > bitcoin transaction with so many OP_RETURNs will stand out, and their
> > > number will roughly indicate the total amount.  This kind of privacy
> > > leakage, however it applies to a small number of users, is easy to avoid
> > by
> > > using multiple addresses and storing a relatively small amount on each
> > > address.
> > > 3. Exchanges and large merchants will likely accumulate large coin
> > > histories.  Although fragmented, far from complete, and likely outdated,
> > it
> > > is still something to bear in mind.
> > >
> > > No hard or soft fork is required, BBC is just a separate privacy
> > preserving
> > > currency on top of bitcoin blockchain, and the same private keys and
> > > addresses are used for both BBC and the base currency BTC.  Every BCC
> > > transaction must be enclosed into by a small BTC transaction that stores
> > > the OP_RETURNs and pays for the fees.
> > >
> > > Are there any flaws in this design?
> > >
> > > Originally posted to BCT https://bitcointalk.org/index.
> > php?topic=1574508.0,
> > > but got no feedback so far, apparently everybody was consumed with
> > bitfinex
> > > drama and now mimblewimble.
> > >
> > > Tony
> >
> > > _______________________________________________
> > > bitcoin-dev mailing list
> > > bitcoin-dev@lists.linuxfoundation.org
> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> >
> > --
> > Henning Kopp
> > Institute of Distributed Systems
> > Ulm University, Germany
> >
> > Office: O27 - 3402
> > Phone: +49 731 50-24138
> > Web: http://www.uni-ulm.de/in/vs/~kopp
> >

-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------
Miners in general are naturally incentivized to always mine max size
blocks to maximize transaction fees simply because there is very
little marginal cost to including extra transactions(there will always
be a transaction backlog of some sort available to mine since demand
for block space is effectively unbounded as fees approach 0 and they
can even mine their own transactions without any fees). This proposal
would almost certainly cause runaway block size growth and encourage
much more miner centralization.

On Sat, Dec 10, 2016 at 6:26 PM, t. khan via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> Miners 'gaming' the Block75 system -
> There is no financial incentive for miners to attempt to game the Block75
> system. Even if it were attempted and assuming the goal was to create bigger
> blocks, the maximum possible increase would be 25% over the previous block
> size. And, that size would only last for two weeks before readjusting down.
> It would cost them more in transaction fees to stuff the network than they
> could ever make up. To game the system, they'd have to game it forever with
> no possibility of profit.
>
> Blocks would get too big -
> Eventually, blocks would get too big, but only if bandwidth stopped
> increasing and the cost of disk space stopped decreasing. Otherwise, the
> incremental adjustments made by Block75 (especially in combination with
> SegWit) wouldn't break anyone's connection or result in significantly more
> orphaned blocks.
>
> The frequent and small adjustments made by Block75 have the added benefit of
> being more easily adapted to, both psychologically and technologically, with
> regards to miners/node operators.
>
> -t.k
>
> On Sat, Dec 10, 2016 at 5:44 AM, s7r via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>
>> t. khan via bitcoin-dev wrote:
>> > BIP Proposal - Managing Bitcoin’s block size the same way we do
>> > difficulty (aka Block75)
>> >
>> > The every two-week adjustment of difficulty has proven to be a
>> > reasonably effective and predictable way of managing how quickly blocks
>> > are mined. Bitcoin needs a reasonably effective and predictable way of
>> > managing the maximum block size.
>> >
>> > It’s clear at this point that human beings should not be involved in the
>> > determination of max block size, just as they’re not involved in
>> > deciding the difficulty.
>> >
>> > Instead of setting an arbitrary max block size (1MB, 2MB, 8MB, etc.) or
>> > passing the decision to miners/pool operators, the max block size should
>> > be adjusted every two weeks (2016 blocks) using a system similar to how
>> > difficulty is calculated.
>> >
>> > Put another way: let’s stop thinking about what the max block size
>> > should be and start thinking about how full we want the average block to
>> > be regardless of size. Over the last year, we’ve had averages of 75% or
>> > higher, so aiming for 75% full seems reasonable, hence naming this
>> > concept ‘Block75’.
>> >
>> > The target capacity over 2016 blocks would be 75%. If the last 2016
>> > blocks are more than 75% full, add the difference to the max block size.
>> > Like this:
>> >
>> > MAX_BLOCK_BASE_SIZE = 1000000
>> > TARGET_CAPACITY = 750000
>> > AVERAGE_OVER_CAP = average block size of last 2016 blocks minus
>> > TARGET_CAPACITY
>> >
>> > To check if a block is valid, ≤ (MAX_BLOCK_BASE_SIZE + AVERAGE_OVER_CAP)
>> >
>> > For example, if the last 2016 blocks are 85% full (average block is 850
>> > KB), add 10% to the max block size. The new max block size would be
>> > 1,100 KB until the next 2016 blocks are mined, then reset and
>> > recalculate. The 1,000,000 byte limit that exists currently would
>> > remain, but would effectively be the minimum max block size.
>> >
>> > Another two weeks goes by, the last 2016 blocks are again 85% full, but
>> > now that means they average 935 KB out of the 1,100 KB max block size.
>> > This is 93.5% of the 1,000,000 byte limit, so 18.5% would be added to
>> > that to make the new max block size of 1,185 KB.
>> >
>> > Another two weeks passes. This time, the average block is 1,050 KB. The
>> > new max block size is calculated to 1,300 KB (as blocks were 105% full,
>> > minus the 75% capacity target, so 30% added to max block size).
>> >
>> > Repeat every 2016 blocks, forever.
>> >
>> > If Block75 had been applied at the difficulty adjustment on November
>> > 18th, the max block size would have been 1,080KB, as the average block
>> > during that period was 83% full, so 8% is added to the 1,000KB limit.
>> > The current size, after the December 2nd adjustment would be 1,150K.
>> >
>> > Block75 would allow the max block size to grow (or shrink) in response
>> > to transaction volume, and does so predictably, reasonably quickly, and
>> > in a method that prevents wild swings in block size or transaction fees.
>> > It attempts to keep blocks at 75% total capacity over each two week
>> > period, the same way difficulty tries to keep blocks mined every ten
>> > minutes. It also keeps blocks as small as possible.
>> >
>> > Thoughts?
>> >
>> > -t.k.
>> >
>>
>> I like the idea. It is good wrt growing the max. block size
>> automatically without human action, but the main problem (or question)
>> is not how to grow this number, it is what number can the network
>> handle, considering both miners and users. While disk space requirements
>> might not be a big problem, block propagation time is. The time required
>> for a block to propagate in the network (or at least to all the miners)
>> is directly dependent of its size.  If blocks take too much time to
>> propagate in the network, the orphan rate will increase in unpredictable
>> ways. For example if the internet speed in China is worse than in
>> Europe, and miners in China have more than 50% of the hashing power,
>> blocks mined by European miners might get orphaned.
>>
>> The system as described can also be gamed, by filling the network with
>> transactions. Miners have the monetary interest to include as many
>> transactions as possible in a block in order to collect the fees.
>> Regardless how you think about it, there has to be a maximum block size
>> that the network will allow as a consensus rule. Increasing it
>> dynamically based on transaction volume will reach a point where the
>> number got big enough that it broke things. Bitcoin, because its
>> fundamental design, can scale by using offchain solutions.
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev@lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
Since the root cause of what you are trying to address is the reward
having, I'd suggest considering an adjustment to the having schedule.
Instead of their being a large supply shock every four years, perhaps the
reward could drop every 52,500 blocks (yearly), or even at each difficulty
adjustment, in such a way that the inflation curve is smoothed out.  The
exponential decay rate would be preserved, so overall economic philosophy
would be preserved.

I'm guessing hesitance to this approach would lie in a reluctance to tinker
with Bitcoin's 'economic contract', and slippery slope concerns about might
be the next change (21M?).  However, I think it could actually increase
confidence in the system if the community is able to demonstrate a good
process for making such decisions, and show that we can separate the
meaningful underlying principles, such as the coin limit and overall
inflation rate, from what is more akin to an implementation detail, as I
consider the large-step reward reduction to be.

I'm not too worried about the impact of the having as is, but adjusting the
economic parameter would be a safer and simpler way to address the concerns
than to tinker with the difficulty targeting mechanism, which is at the
heart of Bitcoin's security

On Wed, Mar 2, 2016 at 6:56 AM, Luke Dashjr via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> We are coming up on the subsidy halving this July, and there have been some
> concerns raised that a non-trivial number of miners could potentially drop
> off
> the network. This would result in a significantly longer block interval,
> which
> also means a higher per-block transaction volume, which could cause the
> block
> size limit to legitimately be hit much sooner than expected. Furthermore,
> due
> to difficulty adjustment being measured exclusively in blocks, the time
> until
> it adjusts to compensate would be prolonged.
>
> For example, if 50% of miners dropped off the network, blocks would be
> every
> 20 minutes on average and contain double the transactions they presently
> do.
> Even double would be approximately 850-900k, which potentially bumps up
> against the hard limit when empty blocks are taken into consideration. This
> situation would continue for a full month if no changes are made. If more
> miners drop off the network, most of this becomes linearly worse, but due
> to
> hitting the block size limit, the backlog would grow indefinitely until the
> adjustment occurs.
>
> To alleviate this risk, it seems reasonable to propose a hardfork to the
> difficulty adjustment algorithm so it can adapt quicker to such a
> significant
> drop in mining rate. BtcDrak tells me he has well-tested code for this in
> his
> altcoin, which has seen some roller-coaster hashrates, so it may even be
> possible to have such a proposal ready in time to be deployed alongside
> SegWit
> to take effect in time for the upcoming subsidy halving. If this slips, I
> think it may be reasonable to push for at least code-readiness before July,
> and possibly roll it into any other hardfork proposed before or around that
> time.
>
> I am unaware of any reason this would be controversial, so if anyone has a
> problem with such a change, please speak up sooner rather than later. Other
> ideas or concerns are of course welcome as well.
>
> Thanks,
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Wed, May 11, 2016 at 3:47 AM, Jannes Faber <jannes.faber@gmail.com>
wrote:

> On 11 May 2016 at 12:36, Henning Kopp <henning.kopp@uni-ulm.de> wrote:
>
>> On Wed, May 11, 2016 at 11:21:10AM +0200, Jannes Faber via bitcoin-dev
>> wrote:
>> > On 11 May 2016 at 05:14, Timo Hanke via bitcoin-dev <
>> > bitcoin-dev@lists.linuxfoundation.org> wrote:
>> >
>> > > There is no way to tell from a block if it was mined with AsicBoost or
>> > > not. So you don’t know what percentage of the hashrate uses AsicBoost
>> at
>> > > any point in time. How can you risk forking that percentage out? Note
>> that
>> > > this would be a GUARANTEED chain fork. Meaning that after you change
>> the
>> > > block mining algorithm some percentage of hardware will no longer be
>> able
>> > > to produce valid blocks. That hardware cannot “switch over” to the
>> majority
>> > > chain even if it wanted to. Hence you are guaranteed to have two
>> > > co-existing bitcoin blockchains afterwards.
>> > >
>> > > Again: this is unlike the hypothetical persistence of two chains
>> after a
>> > > hardfork that is only contentious but doesn’t change the mining
>> algorithm,
>> > > the kind of hardfork you are proposing would guarantee the
>> persistence of
>> > > two chains.
>> > >
>> >
>> > Assuming AsicBoost miners are in the minority, their chain will
>> constantly
>> > get overtaken. So it will not be one endless hard fork as you claim, but
>> > rather AsicBoost blocks will continue to be ignored (orphaned) until
>> they
>> > stop making them.
>>
>> At least until a difficulty adjustment on the AsicBoost chain takes
>> place. From that point on, both chains, the AsicBoost one and the
>> forked one will grow approximately at the same speed.
>>
>>
> No: you are still assuming AsicBoost miners would reject normal blocks.
> They don't now and they would have to specifically code for that as a reply
> to AsicBoost being banned. So there won't be two chains at all, only the
> main chain with a lot (more than usual) of short (few blocks) forks. Each
> forks starts anew, it's not one long fork. Therefore there is no
> "difficulty adjustment on the AiscBoost chain".
>
> Now if they do decide to ban non-AsicBoost blocks as a response to being
> banned themselves, they're just another altcoin with a different PoW and no
> one would have a reason to use them over Bitcoin (apart from maybe selling
> those forked coins asap).
>

This is what I meant. If existing hardware gets forked-out it will
inevitably lead to the creation of an altcoin. Simply because the hardware
exists and can't be used for anything else both chains will survive. I was
only comparing the situation to a contentious hardfork that does not fork
out any hardware. If the latter one is suspected to lead to the permanent
existence of two chains then a hardfork that forks out hardware is even
more likely to do so (I claim it's guaranteed).


> You're confused about what "longest" means as well: it's not just the
> number of blocks, it's the aggregate difficulty that counts: so AsicBoost
> would never become "longer" (more total work) either.
>
> Hope this helps clear things up.
>
> --
> Jannes
>

-------------------------------------
BIP Proposal - Managing Bitcoin’s block size the same way we do difficulty
(aka Block75)

The every two-week adjustment of difficulty has proven to be a reasonably
effective and predictable way of managing how quickly blocks are mined.
Bitcoin needs a reasonably effective and predictable way of managing the
maximum block size.

It’s clear at this point that human beings should not be involved in the
determination of max block size, just as they’re not involved in deciding
the difficulty.

Instead of setting an arbitrary max block size (1MB, 2MB, 8MB, etc.) or
passing the decision to miners/pool operators, the max block size should be
adjusted every two weeks (2016 blocks) using a system similar to how
difficulty is calculated.

Put another way: let’s stop thinking about what the max block size should
be and start thinking about how full we want the average block to be
regardless of size. Over the last year, we’ve had averages of 75% or
higher, so aiming for 75% full seems reasonable, hence naming this concept
‘Block75’.

The target capacity over 2016 blocks would be 75%. If the last 2016 blocks
are more than 75% full, add the difference to the max block size. Like this:

MAX_BLOCK_BASE_SIZE = 1000000
TARGET_CAPACITY = 750000
AVERAGE_OVER_CAP = average block size of last 2016 blocks minus
TARGET_CAPACITY

To check if a block is valid, ≤ (MAX_BLOCK_BASE_SIZE + AVERAGE_OVER_CAP)

For example, if the last 2016 blocks are 85% full (average block is 850
KB), add 10% to the max block size. The new max block size would be 1,100
KB until the next 2016 blocks are mined, then reset and recalculate. The
1,000,000 byte limit that exists currently would remain, but would
effectively be the minimum max block size.

Another two weeks goes by, the last 2016 blocks are again 85% full, but now
that means they average 935 KB out of the 1,100 KB max block size. This is
93.5% of the 1,000,000 byte limit, so 18.5% would be added to that to make
the new max block size of 1,185 KB.

Another two weeks passes. This time, the average block is 1,050 KB. The new
max block size is calculated to 1,300 KB (as blocks were 105% full, minus
the 75% capacity target, so 30% added to max block size).

Repeat every 2016 blocks, forever.

If Block75 had been applied at the difficulty adjustment on November 18th,
the max block size would have been 1,080KB, as the average block during
that period was 83% full, so 8% is added to the 1,000KB limit. The current
size, after the December 2nd adjustment would be 1,150K.

Block75 would allow the max block size to grow (or shrink) in response to
transaction volume, and does so predictably, reasonably quickly, and in a
method that prevents wild swings in block size or transaction fees. It
attempts to keep blocks at 75% total capacity over each two week period,
the same way difficulty tries to keep blocks mined every ten minutes. It
also keeps blocks as small as possible.

Thoughts?

-t.k.

-------------------------------------
What's most likely to happen is miners will max out the blocks they
mine simply to try and get as many transaction fees as possible like
they are doing right now(there will be a backlog of transactions at
any block size). Having the block size double every year would likely
cause major problems and this proposal allows over a 7x increase it
seems.

The main problem with this proposal I think is that users effectively
have no way to stop the miners from increasing block size
continuously.

On Sun, Dec 11, 2016 at 1:55 PM, t. khan via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
>
> On Sun, Dec 11, 2016 at 12:11 PM, s7r <s7r@sky-ip.org> wrote:
>>
>>
>> This is an incentive, if few miners agree to create a large conglomerate
>> that will ultimately control the network.
>>
>> You miss something obvious that makes this attack actually free of cost.
>> Nothing will "cost them more in transaction fees". A miner can create
>> thousands of transactions paying to himself, and not broadcast them to
>> the network, but hold them and include them in the blocks he mines. The
>> fees are collected by him because transactions are included in a block
>> that he mined and the left amount is in another wallet of the same
>> person. Repeat this continuously to fill blocks.
>
>
> No, that wasn't overlooked. Miners could indeed stuff their own blocks for
> free, but they can't stuff blocks mined by others for free.
>
> In the hypothetical scenario where there is a single mining pool which mines
> most (if not all) of the blocks, we would have much larger problems than
> their ability to raise the max block size gradually. Even if they were able
> to fill 100% of the blocks for an entire year, the max block size for that
> 2016 block period would be 7.25MB (not accounting for SegWit). After the
> whole year they would have made no extra profit vs doing nothing. And as
> soon as they stopped this scheme, block size would spring back to it's
> natural level.
>
> The good news is, this scenario has never happened and even when we've come
> remotely close (when ASICs first shipped), the situation was temporary. The
> odds of this happening in the future and persisting long enough to have any
> major effect with Block75 are very close to zero.
>
>>
>> Topology and bandwidth speed / hash rate of the network cannot be
>> controlled - if we make assumptions about these it might have terrible
>> consequences.
>>
>> Even if we take in consideration that bandwidth will only grow and disk
>> space will only cost less (which is not something we can safely assume,
>> by the way) the hard limit max. block size cannot grow to unlimited
>> value (even if the growth happens over time). There is also a validation
>> cost in time for each block, for the health of the network any node
>> should be able to download _and_ validate a block, before next block
>> gets mined.
>>
>> You said in another post that a permanent solution is preferred, rather
>> than kicking the can down the road. I fully agree, as well as many
>> others reading this list, but the permanent solution doesn't necessarily
>> have to be increasing the max block size dynamically.
>
>
> Increasing *and* decreasing max block size dynamically. Block75 is
> self-correcting, whereas any solution with hardcoded limits can't correct
> without human intervention and would rely on our ability to predict the
> future (which as you pointed out, we can't do). Therefore, any solution
> that's not dynamic cannot be permanent.
>
> Additionally, the frequent and gradual changes in max block size would allow
> us to see any consequences well in advance (years probably).
>
>>
>> If you think about it the other way around, dynamically growing the max
>> block size is also kicking the can down the road ... just without having
>> to touch it and get dust on the boot ;)
>
>
> Not having to touch it again = permanent solution. ;)
>
> It would be helpful if some others would run the numbers on how Block75
> would adjust the block size over time:
>
> new max block size = 1000kb + (average block size over last 2016 blocks -
> 750kb)
>
> -t.k.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-------------------------------------
On Sunday, 16 October 2016 12:49:47 CEST Douglas Roark via bitcoin-dev 
wrote:
> It's not the website's fault if wallet devs aren't updating their
> statuses. Besides, "WIP" can mean an awful lot of things.

As I said, it would be nice to get an updated version so we can see more 
than 20% readyness of wallets.
The wallet devs not caring enough to update the status should be a worrying 
sign, though.

> A lot of devs have already worked on SegWit support. This has been
> covered. Even if they don't support SegWit, the wallets will probably
> work just fine. (For awhile, Armory did crash when trying to read SegWit

SegWit is probably the most disruptive and most invasive change ever made to 
Bitcoin. We have miners actively saying they don't like it and this makes it 
a contriversial upgrade which means the network may split and other issues.

Your "wallets will probably work just fine" comment is honestly not the 
answer to make people put faith in the way that this is being vetted and 
checked...

> Also, once again, FlexTrans is off-topic. 

Its an alternative and brought up in that vain. Nothing more. Feel free to 
respond to the BIP discussion (134) right on this list if you have any 
opinions on it. They will be on-topic there. No problems have been found so 
far.

Lets get back to the topic. Having a longer fallow period is a simple way to 
be safe.  Your comments make me even more scared that safety is not taken 
into account the way it would.

People are not even acknowledging the damage a contriversial soft fork of 
the scope and magnitude of SegWit can have, and a simple request to extend 
the fallow time for safety is really not a big deal.
SegWit has been in development for 18 months or so, what is a couple of 
extra week??

I would just like to ask people to take the safety of Bitcoin serious. This 
discussion and refusal to extend the safety period is not a good sign.
-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Hi Andrew

> In the release notes for 0.12, it says that we have moved from
> using OpenSSL to libsecp256k1 for signature validation. So what
> else is it being used for that we need to keep it as a dependency?

Openssl was dropped from the consensus layer (ECC) in 0.12, though, it
still used for...

1) ... getting random numbers (randomize the ECC signing context)
2) [wallet only] ... AES256 encryption of private keys
3) [GUI only] ... SSL/X.509 for BIP70 (payment protocol)

Openssl dependency for 1) and 2) could be removed. There are some 
outdated  relevant PRs:

Entropy: https://github.com/bitcoin/bitcoin/pull/5885
AES: https://github.com/bitcoin/bitcoin/pull/5949

I guess for point 3) [BIP70] it makes sense to keep openssl.

/jonas
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJWoIwAAAoJECnUvLZBb1PsWSMP/2VyURcUUmnFodX1UUkkTQSu
KmEMqRe3Ak1v4B5S+7raodYE+7ePONedHrciUgNfj0GBDu7/5Wl3LD0GnFb0//Nl
JEHPzNQB8xhRjhXux17rq+Kf60qjc+uybJQDDs9KfQQYS+hFTUKXX61s7wwY/QAy
6Vi5FxZRThzFUFWFZvG9KbRLWEbBVONnXLaA4pB0o7UnU2wAHkmPP5wyeCJLy3cW
uggeLYh3X6GBF/+IQ0ndO4yFJ09ROXBS7N1VisJy2Z4zTJr0y6rAVVG9XcPtlkMc
SvMULeiB34odvlZMRMFdCYLHCuff30jN2+aEJST/d+lr4IB2ai8veXwt69yya4p2
4UUL5ueOzKWfgcxVT/qDDcVkZJFqrhdHmMaEggelRakQCSdLly+4X7Mdo/Dx/RC2
PYUDQVGGFephTpzBTQ3fpRGtZu2JX45T2RKyF2qcVlzXrRW7SjqzwGWWuutwbbrS
V9cSMMVS7NU90mgCE4e3G2oqi40H8dOzg+opf5ynChEccgJwUlxrfjj4kJbQZRH1
X00tGeVs93MxQes+vacYq7VYX4pzM1kiU3EMNStyAvCzd8FbGxmiv3C1VKhRj3xK
Oo98Yg18OBL2jQCWHza3nOU5jN8AnjlkXNvrqsaGedjVNirlnR6a+qmklNIiY1lE
kBxMbfAhTLPY3ukqtaSh
=4GfM
-----END PGP SIGNATURE-----


-------------------------------------
I received this:

---------- Forwarded message ----------
From: Pieter Wuille <pieter.wuille@gmail.com>
Date: Fri, Apr 22, 2016 at 6:44 PM
Subject: Re: [bitcoin-dev] Proposal to update BIP-32
To: Marek Palatinus <marek@palatinus.cz>
Cc: Bitcoin Dev <bitcoin-dev@lists.linuxfoundation.org>


On Thu, Apr 21, 2016 at 2:08 PM, Marek Palatinus <marek@palatinus.cz> wrote:

> On Wed, Apr 20, 2016 at 6:32 PM, Jochen Hoenicke via bitcoin-dev <
> bitcoin-dev@lists.linuxfoundation.org> wrote:
>
>> Hello Bitcoin Developers,
>>
>> I would like to make a proposal to update BIP-32 in a small way.
>>
>> I think the backward compatibility issues are minimal.  The chance
>> that this affects anyone is less than 10^-30.  Even if it happens, it
>> would only create some additional addresses (that are not seen if the
>> user downgrades).  The main reason for suggesting a change is that we
>> want a similar method for different curves where a collision is much
>> more likely.
>>
>
I think I change like this makes a lot of sense technically, and I wish I
had known how BIP-32 would end up being used inside higher level mechanisms
that automatically increment the position beyond the control of the
application generating them. The inclusion of the requirement was there
because ECDSA is notorious for security problems under biased secret keys,
though it's really only a certificational issue for secp256k1 (due to its
group order being so close to 2^256).

>
>> #QUESTIONS:
>>
>> What is the procedure to update the BIP?  Is it still possible to
>> change the existing BIP-32 even though it is marked as final?  Or
>> should I make a new BIP for this that obsoletes BIP-32?
>>
>
BIPs are not supposed to be updated with new ideas, only
remarks/links/typos/clarifications/..., so that their bumbers can
unambiguously be used to refer to an idea. My suggestion would be to write
a new BIP that overrides parts of BIP32, and then put a note in BIP32 that
a better mechanism is available that is unlikely to change things in
reality for the secp256k1 curve.

I guess


> What algorithm is preferred? (bike-shedding)  My suggestion:
>>
>> ---
>>
>> Change the last step of the private -> private derivation functions to:
>>
>>  . In case parse(I_L) >= n or k_i = 0, the procedure is repeated
>>    at step 2 with
>>     I = HMAC-SHA512(Key = c_par, Data = 0x01 || I_R || ser32(i))
>
>
>> ---
>>
>> I think this suggestion is simple to implement (a bit harder to unit
>> test) and the string to hash with HMAC-SHA512 always has the same
>> length.  I use I_R, since I_L is obviously not very random if I_L >= n.
>> There is a minimal chance that it will lead to an infinite loop if I_R
>> is the same in two consecutive iterations, but that has only a chance
>> of 1 in 2^512 (if the algorithm is used for different curves that make
>> I_L >= n more likely, the chance is still less than 1 in 2^256).  In
>> theory, this loop can be avoided by incrementing i in every iteration,
>> but this would make an implementation error in the "hard to test" path
>> of the program more likely.
>>
>
The chance for failure is a bit higher than that, as it only requires a
failed key (one in 2^128) in the first step, followed by an iteration that
results in the same I_R to cause a cycle. If you take multiple failures
before the cycle starts into account, the combined chance for failure is
p/(1-p)^2 / 2^256 (with p the chance for a random inadmissable key), which
is not much better than 1 in 2^128 for high values of p.

An alternative that always converges is to retry with an appended iteration
count is possible:
{
  I = HMAC-SHA512(Key = c_par, Data = 0x01 ||  || ser32(i)) for the first
iteration
  I = HMAC-SHA512(Key = c_par, Data = 0x01 ||  || ser32(i) || ser32(j)) for
iteration number j, with j > 0
}

Cheers,

-- 
Pieter

-------------------------------------
Dear All,

The Call for Proposals (CFP) for 'Scaling Bitcoin 2016: Retarget' is now
open.

Please see https://scalingbitcoin.org for details.

*Important Dates*

Sept 2nd - Deadline for submissions to the CFP
Sept 23rd - Deadline for applicant acceptance notification

See you in Milan! (October 8th and 9th)

Ciao! :)

p.

-------------------------------------
Hi all

Thanks for the response.


Jochen's points:
===============
Indeed. There are some missing points and I'd like to work this into the
BIP. Thanks for bringing this up.

Along with a support for wallet-creation with a xpub from the signing
device, we might also want to support loading multiple pubkeys into a
keypool from the device (in case someone likes to use hardened
derivation at all levels). I guess this would not be over-complex to
achieve.

Luke's points:
=============

USB / Plugin/Driver problematic
-------------------------------
I don't think it would be wise to set Trezors USB communication
(hardware interface) as "the standard". A) A USB stack/interaction in
wallets should be avoided IMO. B) This approach won't work for some
platforms (like iOS) due to technical and legal restrictions.

In my opinion, each hardware wallet has to provide custom software in
any case. We don't want to standardize how a hardware wallet has to do
backups, recovers, firmware upgrade, etc. and if we agree on that, then
hardware wallets must provide an application (mostly Chrome extensions
today) to implement theses processes.

Also diversity at the hardware interface will reduce centralized risks
for weak security/vulnerabilities.

The proposed URI scheme approach does not require any sorts of
libraries/dependencies. USB HID can be a problem for cross platform
desktop wallets as well as it won't work of one of the major mobile
platform (iOS). USB HID interaction can be restricted or disabled in non
superuser setups where I'm not aware of any restriction on URI-Scheme level.

URI scheme instead of stdio/pipe
--------------------------------
The URI scheme is not ugly. Its a modern way  implemented in almost all
platforms  how applications can interact with each other while not
directly knowing each other. Registering a URI scheme like "bitcoin://"
has some concrete advantages over just piping through stdio.

Also, the stdio/piping approach does not work for mobile platforms
(where the URI scheme works).

The URI scheme does not require any sorts of wallet app level
configuration (where the stdio/pipe approach would require to configure
some details about the used hardware wallet).


Thomase D.'s points:
===================
Standardizing to many layers of the interaction stack (including the
hardware interaction) will very likely result in vendors not sticking to
the standard.

I agree, the URI scheme has some fragility, but at a level where we can
handle it and with the advantage of abstracting the used brand/device
for privacy and security reasons.

> The existing URI scheme, while allowing disambiguate by manufacturer,
provides no way to to enumerate available manufacturers or enabled
wallets.

Most operating systems allow to check if a certain URL-Scheme is
supported (registered), this would allow at least to check for known
major vendors (like trezor, etc.) which should solve most
multi-hardware-wallet use-cases.

The URI return scheme does work fine and with the correct set timeouts
it should result in a neat user experience.
It's the proposed way of application intercommunication in Apple iOS [1]
and Google Android [2].

Conclusion:
===========
* Non of the points convinced me that there is a better alternative to
the proposed URI scheme interaction (please tell me if I'm stubborn).
* Also, we should move the end users UX in the center of the
problems-to-solve (and not overweight the ideal
code-/API-/hardware-interaction-design while ignoring the end user
experience).
* We should try to not over-standardize the interaction with the device
itself to allow flexibility on the hardware wallet vendor side.

[1]
https://developer.apple.com/library/ios/documentation/iPhone/Conceptual/iPhoneOSProgrammingGuide/Inter-AppCommunication/Inter-AppCommunication.html
[2] https://developer.android.com/training/basics/intents/sending.html

</jonas>


-------------------------------------
On Tue, Jan 26, 2016 at 09:44:48AM -0800, Toby Padilla via bitcoin-dev wrote:
> I really don't like the idea of policing other people's use of the
> protocol. If a transaction pays its fee and has a greater than dust value,
> it makes no sense to object to it.

I'll point out that getting a BIP for a feature is *not* a hard
requirement for deployment. I'd encourage you to go write up your BIP
document, give it a non-numerical name for ease of reference, and lobby
wallet vendors to implement it.

While I'll refrain from commenting on whether or not I think the feature
itself is a good idea, I really don't want people to get the impression
that we're gatekeepers for how people choose use Bitcoin.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
000000000000000008320874843f282f554aa2436290642fcfa81e5a01d78698

-------------------------------------
On Thu, Feb 4, 2016 at 5:56 PM, jl2012 via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> No, the "triggering block" you mentioned is NOT where the hardfork starts.
> Using BIP101 as an example, the hardfork starts when the first >1MB is
> mined. For people who failed to upgrade, the "grace period" is always zero,
> which is the moment they realize a hardfork.


Clients have to update in some way to get the benefit of this right?

An SPV client which fully validated the header chain would simply reject
the hard forking header.  Last time I checked, the Bitcoinj SPV wallet
ignored the version bits, and just followed the longest chain.  Is that
still the case?

In fact, does Core enforce the 95% rule for the soft-forks before checking
for long forks?  I am assuming that it happens when checking headers rather
than when checking full blocks.
<https://www.avast.com/sig-email> This email has been sent from a
virus-free computer protected by Avast.
www.avast.com <https://www.avast.com/sig-email>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

-------------------------------------
NACK

Horrible precedent (hardcoding rule changes based on the assumption that large forks indicate a catastrophic failure), extremely poor process (already shipped, now the discussion), and not even a material performance optimization (the checks are avoidable once activated until a sufficiently deep reorg deactivates them).

e

> On Nov 14, 2016, at 10:17 AM, Suhas Daftuar via bitcoin-dev <bitcoin-dev@lists.linuxfoundation.org> wrote:
> 
> Hi,
> 
> Recently Bitcoin Core merged a simplification to the consensus rules surrounding deployment of BIPs 34, 66, and 65 (https://github.com/bitcoin/bitcoin/pull/8391), and though the change is a minor one, I thought it was worth documenting the rationale in a BIP for posterity.
> 
> Here's the abstract:
> 
> Prior soft forks (BIP 34, BIP 65, and BIP 66) were activated via miner signaling in block version numbers. Now that the chain has long since passed the blocks at which those consensus rules have triggered, we can (as a simplification and optimization) replace the trigger mechanism by caching the block heights at which those consensus rules became enforced.
> 
> The full draft can be found here: 
> 
> https://github.com/sdaftuar/bips/blob/buried-deployments/bip-buried-deployments.mediawiki
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------------------------------
I’m not sure if you really understand what you and I am talking. It has nothing to do with BIP30, 34, nor any other BIPs.

Say tx1 is confirmed 3 years ago in block X. An attacker finds a valid tx2 which (tx1 != tx2) and (SHA256(tx1) == SHA256(tx2)). Now he could replace tx1 with tx2 in block X and the block is still perfectly valid. Anyone trying to download the blockchain from the beginning may end up with a different ledger. The consensus is irrevocably broken as soon as tx1 or tx2 is spent.

Or, alternatively, an attacker finds an invalid tx3 which (tx1 != tx3) and (SHA256(tx1) == SHA256(tx3)). Now he could replace tx1 with tx3 in block X. Anyone trying to download the blockchain from the beginning will permanently reject the hash of block X. They will instead accept a fork built on top of block X-1. The chain will be permanently forked.

jl2012


> On 18 Nov 2016, at 01:01, Eric Voskuil <eric@voskuil.org> wrote:
> 
> On 11/17/2016 07:40 AM, Johnson Lau wrote:
>> 
>>> On 17 Nov 2016, at 20:22, Eric Voskuil via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>>> 
>>> Given that hash collisions are unquestionably possible,
>> 
>> Everything you said after this point is irrelevant.
> 
> So... you think hash collisions are not possible, or that it's moot
> because Core has broken its ability to handle them.
> 
> 
>> Having hash collision is **by definition** a consensus failure,
> 
> I suppose if you take fairly recent un-BIPped consensus changes in Core
> to be the definition of consensus, you would be right about that.
> 
> 
>> or a hardfork.
> 
> And those changes could definitely result in a chain split. So right
> about that too.
> 
> 
>> You could replace the already-on-chain tx with the collision and
> create 2 different versions of UTXOs (if the colliding tx is valid), or
> make some nodes to accept a fork with less PoW (if the colliding tx is
> invalid, or making the block invalid, such as being to big).
> 
> 
> Not in accordance with BIP30 and not according to the implementation of
> it that existed in Core until Nov 2015. A tx was only valid as a
> "replacement" if it did not collide with the hash of an existing tx with
> unspent outputs. The collision would have been rejected. And an invalid
> colliding tx would not be accepted in any case (since nodes presumably
> validate blocks and don't rely on checkpoints as a security measure).
> 
> A transaction duplicating the hash of another and taking its place in a
> block would not only have to collide the hash, but it would have to be
> fully valid in the context of the block you are suggesting it is
> substituted into. In that case it's simply a fully valid block. This is
> not just the case of a hash collision, this is the case of a hash
> collision where both transactions are fully valid in the context of the
> same block parent. Even if that unlikely event did occur, it's not a
> hard fork, it's a reorg. The chain that builds on this block will be
> valid to all nodes but necessarily deviates from the other block's valid
> chain. This is true whether the magical block is assembled via compact
> blocks or otherwise.
> 
> Transaction "replacement" is an implementation detail of Core. Once Core
> accepted a replacement of a previously spent transaction it would be
> unable to provide the previous block/spent-tx, but that would be a
> wallet failure and an inability to provide valid historical blocks, not
> a consensus/validation failure. The previously spent outputs no longer
> contribute to validation, unless there is a reorg back to before the
> original tx's block, and at that point it would be moot, since neither
> transaction is on the chain.
> 
> You are referring to the *current* behavior ("replacement" without
> concern for collision). That was an unpublished hard fork, and is the
> very source of the problems you are describing.
> 
>> To put it simply, the Bitcoin protocol is broken. So with no doubt,
> Bitcoin Core and any implementation of the Bitcoin protocol should
> assume SHA256 collision is unquestionably **impossible**.
> 
> I'm not disagreeing with you that it is broken. I'm pointing out that it
> was broken by code that was merged recently - an undocumented hard fork
> that reverted the documented BIP30 behavior that was previously
> implemented correctly, based on the assumption that hash collisions
> cannot occur, for the modest performance boost of not having to check
> for unspent duplicates (sounds sort of familiar).
> 
>> If some refuse to make such assumption, they should have introduced an
> alternative hash algorithm and somehow run it in parallel with SHA256 to
> prevent the consensus failure.
> 
> No hash algorithm can prevent hash collisions, including one that is
> just two running in parallel. A better resolution would be to fix the
> problem.
> 
> There is no need to replace the BIP30 rule. That resolves the TX hash
> collision problem from a consensus standpoint. In order to serve up
> whole blocks in the circumstance requires a more robust store than I
> believe is exists in Core, but that has nothing to do with validity.
> 
> The block hash check and signature validation caching splits caused by
> collision can easily be avoided, and doing so doesn't break with
> consensus. I'm not aware of any other aspects of consensus that are
> effected by an implementation assumption of non-colliding hashes. But in
> any case I'm pretty sure there aren't any that are necessary to consensus.
> 
> e
> 
> 




-------------------------------------
(not sure so sent again after subscribing (one use case added))

Dear Bitcoin developers,

Below is provided a draft BIP proposal for a master mnemonic sentence 
from which other mnemonics sentences can be derived in a deterministic 
non-reversible way (on an offline computer). This would make it much 
easier to split funds into smaller fractions and use those in a 
HD-wallet when appropriate (just by inserting 12 or more words), without 
ever putting the master mnemonic at risk on an online computer. But 
there are many more use cases.

A reference implementation, specifically for use with a Trezor, has been 
generated and can be found at: 
http://thebitcoinecosystem.info/DerivedMnemonics.html

I'm not a professional programmer or cryptographer, so the idea and 
reference implementation will probably need a lot of reviewing but I do 
think Bitcoin needs this extension and the corresponding ease of use and 
improved security model.

In the hope you like the idea,

Regards,
sumBTC


<pre>
   BIP: ???
   Title: Derived mnemonics from a master mnemonic.
   Author: sumBTC <millibitcoins@gmail.com>
   Status: For Discussion
   Type:
   Created: 2016-07-24
</pre>

==Abstract==

This BIP??? uses a master mnemonic sentence, as described in BIP39, for 
the deterministic generation of derived mnemonic sentences. The derived 
mnemonics are of the same format as the master mnemonic but can consist 
of a higher or lower number of words.

Binary seeds can then be generated for derived mnemonics (and master 
mnemonic) as described in BIP39. Each of these seeds can be used to 
generate deterministic wallets using BIP-0032 or similar methods.

==Motivation==

A mnemonic code or sentence is superior for human interaction as 
described in BIP39 and can, for example, be written on paper or even 
memorized. However, once a mnemonic has been used online, even through 
the use of a hardware wallet, the mnemonic could be compromised. This 
should be considered a bad practice from a security standpoint.

We therefore propose the generation of a master mnemonic offline and 
from this generate (also offline) multiple derived mnemonics in a 
deterministic way for online use. The master mnemonic is never used 
online and the master mnemonic cannot be obtained from the derived 
mnemonics. Examples of use cases are described below.

==Generating the master mnemonic==

The master mnemonic is first derived as a standard mnemonic as described 
in BIP39.

==From master mnemonic to derived mnemonics==

 From the master mnemonic a new string is created:

string = MasterMnemonic + " " + Count + " " + Strength;

Here, MasterMnemonic are the space separated words of the master 
mnemonic. Count = 0, 1, 2 denotes the different derived mnemonics of a 
given strength and Strength = numWords / 3 * 32, where numWords is the 
number of words desired for the derived mnemonic and only integer 
arithmetic is used in the calculation (e.g. for numWords = 14, Strength 
= 128). Both Count and Strength are converted to strings.

This string is then hashed using sha512:

hash = sha512(string);

and turned into a byte array:

for (var i=0; i<strength/8; i++) {
   byteArray[i] = (hash[Math.floor((i%64)/4)] >>> ((i%4)*8)) & 0b11111111;
}

This byte array is then used to generate a new mnemonic as shown in the 
reference implementation using the method described in BIP39. The core 
of the new code in the reference manual can be found by jumping to 
"start: new code" in the reference software.

A passphrase for the master mnemonic has the same effect on the derived 
mnemoncis (so must be included).

==Reference Implementation==

The reference implementation generates addresses based on BIP44 for a 24 
word master mnemonic and is available from

http://thebitcoinecosystem.info/DerivedMnemonics.html

or

github (not yet)

==Checking the derived mnemonics using Electrum==

The displayed addresses in each of the reference implementations can be 
easily checked using Electrum in the following manner:

move the directory ~/.electrum to a backup directory.
start Electrum and choose:
Restore a wallet or import keys
Hardware wallet
Restore Electum wallet from device seed words
TREZOR wallet
Insert one of the mnemonics and check that the same addresses are 
generated by Electrum

Check the private keys:
move the directory ~/.electrum to a backup directory.
start Electrum and choose:
Restore a wallet or import keys
Standard wallet
Import one of the private keys and check that the correct address has 
been generated.

Some checks should include a passphrase.

==Examples of Use Cases==

A person with 25 bitcoin splits funds using 5 derived mnemonics and 
sends 5 bitcoins to the first address of each derived mnemonic. He can 
then use a (hardware) HD-wallet and simply insert one of the derived 
mnemonics to put only 5 bitcoins online and at risk at once. All funds 
can be recovered from the master mnemonic.

A person wants to give 10 bitcoin to each of his family members, giving 
each participant a derived mnemonic and sending bitcoin to each of them. 
The donating person can always recover the derived mnemonic if one of 
his family members loses his derived mnemonic.

For his Trezor wallet, someone wants to memorize only a 12 words master 
seed but wants to insert a 24 words derived seed so a key logger on his 
computer has 24! possibilities to check and not 12! (not a possibility 
for the current reference implementation but trivial to add).


-------------------------------------
Hi all,

I am trying to fully grasp confidential transactions.

When a sender creates a confidential transaction and picks the blinding
values correctly, anyone can check that the transaction is valid. It
remains publically verifiable.
But how can the receiver of the transaction check which amount was
sent to him?
I think he needs to learn the blinding factor to reveal the commit
somehow off-chain. Am I correct with this assumption?
If yes, how does this work?

All the best
Henning

-- 
Henning Kopp
Institute of Distributed Systems
Ulm University, Germany

Office: O27 - 3402
Phone: +49 731 50-24138
Web: http://www.uni-ulm.de/in/vs/~kopp


-------------------------------------
> BtcDrak tells me he has well-tested code for this in his altcoin
Could you be more explicit, which altcoin is that?

> I am unaware of any reason this would be controversial
Probably not until you get to the details of any proposal. What is
your exact proposal here? Algorithm? Parameters?
As you likely know a too short time window would be dangerous for
other reasons. Getting to an agreement as to what is reasonable or not
is not necessarily trivial.

Jeremie


2016-03-02 16:14 GMT+01:00 Luke Dashjr via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org>:
> On Wednesday, March 02, 2016 3:05:08 PM Pavel Janík wrote:
>> > the network. This would result in a significantly longer block interval,
>> > which also means a higher per-block transaction volume, which could
>> > cause the block size limit to legitimately be hit much sooner than
>> > expected.
>>
>> If this happens at all (the exchange rate of the coin can accomodate such
>> expectation),
>
> The exchange rate is not significantly influenced by these things.
> Historically, it seems fairly obvious that the difficulty has followed value,
> not value following difficulty.
>
>> the local fee market will develop, fees will raise and complement mined
>> coins, thus bringing more miners back to the game (together with expected
>> higher exchange rate).
>
> Depends on the hashrate drop, and tolerance for higher fees, both of which are
> largely unknown at this time. At least having code prepared for the negative
> scenarios in case of an emergency seems reasonable, even if we don't end up
> needing to deploy it.
>
> Luke
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------

On 10/27/2016 11:38 AM, Andrew via bitcoin-dev wrote:
> I have been reading recently through the history of soft forks
> provided by Bitcoin Core:
> https://bitcoin.stackexchange.com/questions/43538/where-can-i-find-a-record-of-blockchain-soft-forks.
>
> It has led me to think that there is a deceiving notion that soft
> forks do not force Bitcoin users to upgrade software. Yes, it's true
> that the past soft forks still allow old nodes to accept blocks under
> the tighter rules as valid, but what about miners who are still using
> old software? What about users who want to make a transaction using
> the old rules? Those people are no longer able to do those things. And
> if they want to do those things, a hard fork will result.
A soft fork means that a transaction using the old rules will still
work. Look at segwit, you can still make the original style of
transactions with P2PK, P2PKH, and P2SH outputs. There is nothing here
to cause a hard fork.

> Remember what happened when BIP 66 was activated? Luckily, it was
> short lived, but this is just the beginning. If you keep tightening
> the rules, you are building up more and more pressure for a split in
> the network to occur. You can call this split a "hard fork" or just a
> "fork", but it is dangerous either way, and it leads to basically the
> creation of two coins when before we just had one, people instantly
> lose value, and the trust in Bitcoin's store of value dies.
BIP 66's hard fork was not due to the soft fork making transactions
invalid. Rather it was because miners were not properly validating
blocks before building on top of them. That fork was because a
non-upgraded miner created a block invalid under the new rules and
upgraded miners did not check the block and built on top of it. That
invalid block was only invalid because the isSuperMajority mechanism
specified that the new version number must be used otherwise the block
was invalid, and that was what happened: the invalid block had the old
version number. This is not an issue for BIP 9 Versionbits soft forks
because no such rule exists.
>
> Obviously every one can debate about what should be the definition of
> a soft fork, but whatever that is, I think it is unacceptable how
> sloppily the past soft forks have been deployed.
In what way have these forks been sloppily deployed? The fork caused by
previous a soft forks (there was only one that caused that had an actual
chain split issue) was due to the isSuperMajority mechanism which is not
a good mechanism for deployment. It has been superseded by BIP 9
Versionbits. Furthermore, that fork was not due to "sloppy deployment"
by the devs but rather due to greedy miners who were SPV mining.
> I can think of many ways in which we could have these new features
> that the soft forks provided, but without forcing the new rules, and
> simply making them features that can be used on an individual miner or
> transaction signer basis. Is there a document from Bitcoin Core that
> outlines the philosophy of soft forks and why it is acceptable to
> force the tightening of rules and cause such risks? And please give me
> another reason other than "it removes a few if statements from the code".
Can you explain what other risks you think there are with soft forks?
>
> Now that Segregated witness is scheduled to be deployed on November
> 15, we should take a look at this "soft fork" as well. I like the idea
> of Segregated Witness, but from conversations on Reddit and IRC, I see
> people saying that this soft fork will be like the others: requiring a
> hard fork in order to revert it. Is this true?
If you are reading r/btc, you are doing something wrong.

Like with all soft forks, the only way to revert them is by a hard fork.
Soft forks make previously valid things invalid, hard forks make
previously invalid things valid. In order to revert a soft fork which
made something invalid, you need to hard fork to make it valid again.
> I am getting conflicting messages by reading the BIP. It says that if
> all transactions are non-segwit, then a node will validate the block
> as before. But if we pass the threshhold (usually 95 % for 1000
> blocks) will miners mining non-segwit blocks be ignored? This is not
> good... I really think we should make it optional. Miners will have an
> incentive to mine segwit blocks, since it allows for more transactions
> per block, so why force them?
No. This is incorrect. There is no requirement to include the witness
commitment in the coinbase if no transactions in the block contain
witnesses. Because transactions that contain witnesses are considered
non-standard transactions by the old rules, if a miner who did not
upgrade continues to follow those standardness rules, their blocks will
not be invalid and they are not forced to upgrade.
> What if we want to slightly modify the Segwit protocol in the future?
> What if we want to replace segwit with something much different? We
> will be forced to do a hard fork in order to do that.
Not necessarily, it depends on the change. Most changes (such as
sighashing, new opcodes, different scripts, etc.) can be done via
another soft fork because segwit introduces script versioning. A new
script version would be created with the necessary changes and that can
be soft forked in.
>
> Now, we can't go back in time and fix the deployment of the soft
> forks, but I do propose one clean way to fix things: Remove all the
> previously "soft forked" rules for non segwit transactions, and
> require them only for segwit transactions. But make segwit optional!
> In addition to what I talked about above, this may also relieve some
> tensions of people who are not comfortable with segwit and are
> thinking of joining a hard fork like the Bitcoin Unlimited project.
Segwit already is optional. If you don't want to use segwit, you don't
have to. If you don't want to mine segwit blocks, you don't have to.

As for removing all previously soft forked things, then you would be
removing a ton of functionality, various fixes, and potentially break a
large number of transactions that already exists but are not confirmed
yet. This would require a hard fork.

As for what is would be reverted, you would be reverting P2SH (multisig
addresses, you still want those right?), OP_CLTV, OP_CSV, block height
requirement in Coinbase, the value overflow incident, removal of
dangerous opcodes, reduction of block size limit, etc. As you can see, a
lot of functionality and various bug fixes would be lost be reverting
every single soft fork.
>
> Unless people can give me a good explanation as to why we are
> deploying soft forks in such forceful manner,
As I have said throughout this response, soft forks are not deployed in
a forceful manner which forces people to upgrade.
> or Bitcoin Core accepts my proposal, then I will have no choice but to
> create a new client (I'm thinking to call it Bitcoin Authentic), that
> will be just as Bitcoin Core but will always follow the chain with the
> most work regardless of whether soft fork rules are respected, and I
> would put at least CHECKLOCKTIMEVERIFY as mandatory within segwit
> transactions.
Great, go ahead. Honestly, I don't think anyone cares about your
"ultimatum". You are a random person on the internet.
>
> -- 
> PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------------------------------
Pieter, these are in my opinion very reasonable positions. I've made some observations inline.

> On Jun 30, 2016, at 3:03 PM, Pieter Wuille <pieter.wuille@gmail.com> wrote:
> 
> On Thu, Jun 30, 2016 at 11:57 AM, Eric Voskuil via bitcoin-dev
> <bitcoin-dev@lists.linuxfoundation.org> wrote:
>> The proliferation of node identity is my primary concern - this relates to privacy and the security of the network.
> 
> I think this is a reasonable concern.
> 
> However, node identity is already being used widely, and in a very
> inadvisable way:
> * Since forever there have been lists of 'good nodes' to pass in
> addnode= configuration options.
> * Various people run multiple nodes in different geographic locations,
> peering with each other.
> * Various pieces of infrastructure exist that relies on connecting to
> well-behaving nodes (miner relay networks, large players peering
> directly with each other, ...)

Yes, libbitcoin also provides these options on an IP basis.

> * Several lightweight clients support configuring a trusted host to connect to.

I explicitly exclude client-server behavior as I believe the proper resolution is to isolate clients from the P2P protocol. Libbitcoin does this already.

> Perhaps you deplore that fact, but I believe it is inevitable that different pieces of the network will make different choices here. You can't prevent people from create connections along preexisting trust lines. That does not mean that the network as a whole relies on first establishing trust everywhere.

Of course, the network operates just fine without universal trust. My concern is not that it is required, but that it may grow significantly and will have a tendency to gravitate towards more effective registration mechanisms for what is a "good" peer. Even an informal but pervasive web of trust may make it difficult for untrusted parties to connect.

> And I do think there are advantages.
> 
> BIP 151 on its own gives you opportunistic encryption. You're very right to point out that this does not give you protection from active attackers, and that active attacking is relatively easy through sybil attacks. I still prefer my attacker to actually do that over just listening in on my connection.

We agree, and the ease of this attack must be acknowledged. And given that the protection is weak it is not unreasonable to consider the potential downside of creeping node identity.

> And yes, we should also work on improving the privacy nodes and wallets have orthogonal to encryption, but nothing will make everything perfectly private.

I agree, and I doubt this proposal will have much impact on an advanced persistent threat, or even lesser threats. People should understand that there is both a risk and a limited benefit to this proposal.

> BIP 151 plus a simple optional pre-shared-secret authentication extension can improve upon pure IP-based authentication, as well as simplify things like SSL tunnels, and onion addresses purely used as identity. This will still require explicit configuration, but not more than now.

I agree - I consider tunneling the legitimate use case for this proposal. Yet when nodes become closely coupled they are not fully independent. I have a concern with these practices being promoted for general use while at the same time being strongly implemented.

> BIP 151 plus a non-leaking public key authentication scheme (where peers can query "are you the peer with pubkey X?" but don't learn anything if the answer is no) with keys specific to the IP addresses can give a TOFU-like security. Nodes already remember IP addresses they've succesfully interacted with in the past, and ban IP addresses that misbehave. Being able to tell whether a node you connect to is the same as one you've connected to before is a natural extension of this,

With this I disagree. There is no way to know that a node is one you have connected to previously unless that node wants you to know (apart from relying on the IP address). This is of no value in detecting misbehaving nodes that do not want to be detected. Ones that don't care (eg broken nodes) can be sufficiently managed by IP address.

> and does not require establishing any real-world identity beyond what we're already implicitly relying on.
> 
> Perhaps these use cases and their security assumptions should be spelled out more clearly in the BIP.

Absolutely.

> If there is a misunderstanding, it should be clearly stated that BIP 151 is only a building block for further improvements
> 
>> Secondarily I am concerned about users operating under a false assumption about the strength of privacy.
> 
> This is a widespread problem, but it exists far outside the scope of this proposal. The privacy properties of Bitcoin are often
> misrepresented and even used as advertizements. The solution is education, not avoiding improvements because they may be misunderstood.

Yes, let's not make it worse. This is a secondary concern. I remain primarily concerned about growth of node identity in a vain attempt to make transaction submission private in the P2P protocol (and to patch the other client-server features, specifically Bloom filters). As you imply, we cannot stop people from turning Bitcoin into a private network - but let's not facilitate it either.

>> The complexity of the proposed construction is comparable to that of Bitcoin itself.
> 
> I really think this is an exaggeration. It's a diffie-hellman handshake and a stream cipher (both very common constructions), that apply to individual connections. There are no consensus risks nor a
> requirement for coordinated change through the network. The
> cryptographic code can be directly reused from a well-known project
> (OpenSSH), and is very small in size.

I believe you have misinterpreted my comments on distributed anonymous credentials (and the like) as commentary on the construction of BIP151 (and a subsequent auth proposal). As such your observation that it is exaggerated would make sense, but it is not what I intended. Encryption and auth are straightforward. Preventing bad nodes from participating in an anonymous distributed system is not.

e

-------------------------------------
Hi everyone,

Our BIP (officially proposed on March 1) has tentatively been assigned
number 75. Also, the title has been changed to "Out of Band Address
Exchange using Payment Protocol Encryption" to be more accurate.

We thought it would be good to take this opportunity to add some optional
fields to the BIP70 paymentDetails message. The new fields are:
subtractable fee (give permission to the sender to use some of the
requested amount towards the transaction fee), fee per kb (the minimum fee
required to be accepted as zeroconf), and replace by fee (whether or not a
transaction with the RBF flag will be accepted with zeroconf). I know it
doesn't make much sense for merchants to accept RBF with zeroconf, so that
last one might be used more to explicitly refuse RBF transactions (and
allow the automation of choosing a setting based on who you are transacting
with).

I see BIP75 as a general modernization of BIP70, so I think it should be
fine to include these extensions in the new BIP, even though these fields
are not specific to the features we are proposing. Please take a look at
the relevant section and let me know if anyone has any concerns:
https://github.com/techguy613/bips/blob/master/bip-0075.mediawiki#Extending_BIP70_PaymentDetails

The BIP70 extensions page in our fork has also been updated.

Thanks!

James

-------------------------------------
I read the DPL v1.1 and I find it dangerous for Bitcoin users. Current
users may be confident they are protected but in fact they are not, as the
future generations of users can be attacked, making Bitcoin technology
fully proprietary and less valuable.

If you read the DPL v1.1 you will see that companies that join DPL can
enforce their patents against anyone who has chosen not to join the DPL.
(http://defensivepatentlicense.org/content/defensive-patent-license)

So basically most users of Bitcoin could be currently under threat of being
sued by Bitcoin companies and individuals that joined DPL in the same way
they might be under threat by the remaining companies. And even if they
joined DPL, they may be asked to pay royalties for the use of the
inventions prior joining DPL.

DPL changes nothing for most individuals that cannot and will not hire
patent attorneys to advise them on what the DPL benefits are and what
rights they are resigning. Remember that patten attorneys fees may be
prohibitive for individuals in under-developed countries.

Also DPL is revocable by the signers (with only a 180-day notice), so if
Bitcoin Core ends up using ANY DPL covered patent, the company owning the
patent can later force all new Bitcoin users to pay royalties.

Because Bitcoin user base grows all the time with new individuals, the sole
existence of DPL licensed patents in Bitcoin represents a danger to Bitcoin
future almost the same as the existence of non-DPL license patents.

If you're publishing all your ideas and code (public disclosure), you
cannot later go and file a patent in most of the world except the US, where
you have a 1 year grace period. So we need to do something specific to
prevent the publishers filing a US patent.
What we need much more than DPL, we need that every BIP and proposal to the
Bitcoin mailing list contains a note that grants all Bitcoin users a
worldwide, royalty-free, no-charge, non-exclusive, irrevocable license for
the content of the e-mail or BIP.

I'm not a lawyer and this is not an advise of any kind. Please check
yourself the DPL v1.1 and get your own idea. I'm speaking on behalf of
myself, and not any company.
(http://defensivepatentlicense.org/content/defensive-patent-license)

Best regards,
 Sergio.

-------------------------------------
I agree.

Encrypting links in a network without identity doesn't really seem to help
enough for the costs to be justified.

I would like to see a PGP-like "web of trust" proposal for both the
security of the bitcoin network itself /and/ (eventually) of things like
transmission of bitcoin addresses.

Something where nodes of any kind (full, spv, mobile wallets) can
/optionally/ accumulate trust over time and are capable of verifying the
identity of other nodes in that web.

*Then* you can slap an encryption layer on top of it.   Once you have
identity & P2P verified pub keys for nodes, encryption becomes easy.


On Thu, Jun 30, 2016 at 5:57 AM, Eric Voskuil via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Jun 29, 2016, at 3:01 AM, Gregory Maxwell <greg@xiph.org> wrote:
> >
> >> On Tue, Jun 28, 2016 at 11:33 PM, Eric Voskuil <eric@voskuil.org>
> wrote:
> >> I don't follow this comment. The BIP aims quite clearly at "SPV"
> wallets as its justifying scenario.
> >
> > It cites SPV as an example, doesn't mention bloom filters.. and sure--
> sounds like the bip text should make the
>
> "MOTIVATION:
> The Bitcoin network does not encrypt communication between peers today.
> This opens up security issues (eg: traffic manipulation by others) and
> allows for mass surveillance / analysis of bitcoin users. Mostly this is
> negligible because of the nature of Bitcoins trust model, however for SPV
> nodes this can have significant privacy impacts [1] and could reduce the
> censorship-resistance of a peer."
>
> This is not an example, this is the exception that is described as
> "significant" in comparison to the other issues, which are described as
> "negligible".
>
> The Bloom filters messages are of course the unique aspects of the
> protocol as it pertains to "SPV".
>
> The RISKS section declares that the BIP cannot prevent MITM attacks and
> that "identity authentication" will  be defined in a forthcoming BIP.
>
> The obvious implication (accepted by the author) is that authentication is
> required to prevent a MITM attack, and furthermore establishment of
> identity will be required to ensure that the authenticated party is not a
> bad actor.
>
> >>> Without something like BIP151 network participants cannot have privacy
> for the transactions they originate within the protocol against network
> observers.
> >>
> >> And they won't get it with BIP151 either. Being a peer is easier than
> observing the network.
> >
> > Not passively, undetectable, and against thousands of users at once at
> low cost.
>
> This is a straw man, as the BIP does not state that its objective is to
> moderately raise the cost of passive attack against large numbers of users.
>
> It is also a red herring, as passivity is not itself a benefit. It implies
> that the attack is easier and therefore less costly. But a trivial active
> attack may be a larger security problem than a complex passive attack.
> Attacks against privacy under this BIP (and with authentication) can be
> carried out by passively monitoring traffic and operating one or more
> nodes. Operating a node may be considered "active" because the node
> communicates, but technically it is not. In either case the activeness
> itself hardly raises the difficulty, especially for a global (thousands of
> users) passive attacker.
>
> Depending on the attacker, cost may not be an issue at all, so raising it
> can have zero effect. Certainly we are not talking about prohibitive
> (cryptographically hard) cost. Raising the cost *any* amount is not likely
> a reasonable cost-benefit tradeoff.
>
> Privacy attacks would remain entirely undetectable under this proposal,
> and under any additional proposal that required authentication in the
> absence of identity. Only with all users of the network identified as
> "good" would such proposals be effective. Until that point any bad actors
> can become an integral part of the network. I will investigate the question
> of identity in a follow-up to an independent post.
>
> >> If one can observe the encrypted traffic one can certainly use a timing
> attack to determine what the node has sent.
> >
> > Not against Bitcoin Core, transactions are batched and relayed in
> > sorted order.  (obviously there are limits at what this provides;
> > ironically, the lack of link encryption has been used to argue against
> > privacy preserving relay behavior)
>
> It cannot be both impossible ("not against Bitcoin Core") and limited in
> effectiveness ("obviously there are limits").
>
> We should be clear at this point that the transaction-posting security
> provided against a privacy attack, based on the assumption of "good"
> (identified) peers in the first few hops, derives entirely from the ability
> of the good peers to break the timing attack, which is itself "limited".
>
> This is a compound pair of weak assumptions, that to be made stronger will
> require widespread use of identity (not just authentication).
>
> The proliferation of node identity is my primary concern - this relates to
> privacy and the security of the network. Secondarily I am concerned about
> users operating under a false assumption about the strength of privacy.
> Thirdly I am concerned about the risk of vulnerability introduced by the
> integration into the P2P network layer of an totally new network security
> scheme. Fourthly I'm concerned about the cost of the above based on the
> belief that the benefit may not be material and that it may lead to
> increased centralization.
>
> >>> Even if, through some extraordinary effort, their own first hop is
> encrypted, unencrypted later hops would rapidly
> >>> expose significant information about transaction origins in the
> network.
> >>
> >> As will remain the case until all connections are encrypted and
> authenticated, and all participants are known to be good guys. Starting to
> sound like PKI?
> >
> > Huh? The first and subsequent hops obscures the origin and timing.
>
> Described as "limited" in effectiveness, and clearly useful only if these
> hops are not attacker nodes.
>
> So back to my comment on how we maintain a pool of "good" nodes for people
> to connect to, and raising the question of how effective is this strategy
> (which is itself unspecified and so cannot be assumed to even exist in the
> context of the BIP).
>
> >>> Without something like BIP151 authenticated links are not possible, so
> >>> manually curated links (addnode/connect) cannot be counted on to
> provide protection against partitioning sybils.
> >>
> >> If we trust the manual links we don't need/want the other links. In
> fact retaining the other links enables the attack you described above. Of
> course there is no need to worry about Sybil attacks when all of your peers
> are authenticated. But again, let us not ignore the problems of requiring
> all peers on the network be authenticated.
> >
> > Don't need and want them for what?  For _partitioning_ resistance,
> > you are not partitioned if you have one honest connection to the
> > functional network. Additional peers purely reduce your partition
> vulnerability-- so long as an active network attacker isn't
> > intercepting all your connections out.
>
> Don't want them as peers for the purpose of tx relay. As I said this,
> "enables the attack you described above."
>
> > For privacy, you have improve transaction privacy so long as your
> > transaction isn't initially relayed to a malicious peer-- but
> > malicious peers can lie further out because transit nodes obscure the
> > order of message creation.  Bitcoin Core currently relays transactions
> > first and more frequently to outbound and whitelisted peers.
>
> This whitelisting is simply a stand-in for a more formal identity system.
> One doesn't whitelist anonymous peers, one whitelists peers controlled by
> trusted parties. Preferring trusted peers is another aspect of trying to
> break the timing attack. So I would lump this under the same analysis as
> above (batching).
>
> >> Maybe I was insufficiently explicit. By "relies on identity" I meant
> that the BIP is not effective without it. I did not mean to imply that the
> BIP itself implements an identity scheme. I thought this was clear from the
> context.
> >
> > I understood that, but my point was that Bitcoin cannot be used at
> all_unless users have secure communication channels to share addresses.
>
> This is true but not relevant. The parties with whom we transact are not
> in the same space as the nodes with which we connect. The fact that I am
> face-to-face with a counterparty does not help me find a "good" node, nor
> does my ability to PGP email a payment address or to send a stealth address
> in the clear.
>
> But the fact that you raise this point is itself instructive. The solution
> that was devised to resolve the problem of verifying that a counterparty is
> who one thinks it is ended up being based on the use of certificate
> authorities - despite the fact the the BIP did not require this. Some
> people consider this extremely dangerous for Bitcoin, enough so that Peter
> Todd recently proposed scrapping the BIP.
>
> It's not clear to me how the Bitcoin community intends to establish what
> nodes are good nodes. But one thing is certain, any anonymous node may be
> an undetectable attacker.
>
> >> then there is no reason to expect any effective improvement, since
> nodes will necessarily have to connect with anonymous peers.
> >
> > They're not required to _only_ connect with anonymous peers. And
> partition resistance requires that you have any one good link.
>
> As a minimum requirement, it implies that only need only to connect to one
> or more "good" peers. Anonymous peers are gravy for partition resistance,
> yet they are potential attackers for tx tainting. In other words the
> logical topology is to only connect to good peers. That is a problem.
>
> >> Anyone with a node and the ability to monitor traffic should remain
> very effective.
> >
> > Not via passive observation.
>
> See above commentary on the irrelevance of this distinction.
>
> >> Defining an auth implementation is not a hard problem, nor is it the
> concern I have raised.
> >
> > Glad you agree.
>
> I don't get your point here. It seems like you are just trying to
> antagonize.
>
> > We seem to be looping now. Feel free to not implement this proposal,
>
> At this point I think it's fair for me to say that nobody needs your
> permission.
>
> > no one suggests making it mandatory.
>
> Have you ever debated an optional feature proposal?
>
> e
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

-------------------------------------
On Sat, Dec 10, 2016 at 4:23 AM, Daniele Pinna via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> We have models for estimating the probability that a block is orphaned
> given average network bandwidth and block size.
>
> The question is, do we have objective measures of these two quantities?
> Couldn't we target an orphan_rate < max_rate?
>

Models can predict orphan rate given block size and network/hashrate
topology, but you can't control the topology (and things like FIBRE hide
the effect of block size on this as well). The result is that if you're
purely optimizing for minimal orphan rate, you can end up with a single
(conglomerate of) pools producing all the blocks. Such a setup has no
propagation delay at all, and as a result can always achieve 0 orphans.

Cheers,

-- 
Pieter

-------------------------------------
On Sunday, 4 December 2016 21:37:39 CET Hampus Sjberg via bitcoin-dev 
wrote:
> > Also how about making timestamp 8 bytes?  2106 is coming up soon 
> 
> AFAICT this was fixed in this commit:
> https://github.com/jl2012/bitcoin/commit/
fa80b48bb4237b110ceffe11edc14c813
> 0672cd2#diff-499d7ee7998a27095063ed7b4dd7c119R200

That commit hacks around it, a new block header fixes it. Subtle difference.

-- 
Tom Zander
Blog: https://zander.github.io
Vlog: https://vimeo.com/channels/tomscryptochannel


-------------------------------------
On Tue, Jun 28, 2016 at 08:35:26PM +0200, Eric Voskuil wrote:
> Hi Peter,
> 
> What in this BIP makes a MITM attack easier (or easy) to detect, or increases the probability of one being detected?

BIP151 gives users the tools to detect a MITM attack.

It's kinda like PGP in that way: lots of PGP users don't properly check keys,
so an attacker won't have a hard time MITM attacking those users. But some
users do check keys, a labor intensive manual process, but not a process that
requires any real cryptographic sophistication, let alone writing any code.
It's very difficult for widescale attackers to distinguish the users who do
check keys from the ones that don't, so if you MITM attack _any_ user you run
the risk of running into one of the few that does check, and those users can
alert everyone else.

The key thing, is we need to get everyones communications encrypted first: if
we don't the MITM attacker can intercept 99% of the communications with 0% risk
of detection, because the non-sophisticated users are trivially distinguishable
from the sophisticated users: just find the users with unencrypted
communications!

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org

-------------------------------------
On Sun, Oct 16, 2016 at 10:58 PM, Tom Zander via bitcoin-dev <
bitcoin-dev@lists.linuxfoundation.org> wrote:

> On Sunday, 16 October 2016 12:49:47 CEST Douglas Roark via bitcoin-dev
> wrote:
> > It's not the website's fault if wallet devs aren't updating their
> > statuses. Besides, "WIP" can mean an awful lot of things.
>
> As I said, it would be nice to get an updated version so we can see more
> than 20% readyness of wallets.
> The wallet devs not caring enough to update the status should be a worrying
> sign, though.
>

WIP for TREZOR means that we've made that hard part already (firwmare) so
we know it is feasible, yet we didn't spend enough time on finalizing all
the stack like our web wallet because we don't see any actual release date
yet. Considering current battles on BU hashpower, we decided simply sit and
watch (I already have popocorn).


SegWit is probably the most disruptive and most invasive change ever made to
> Bitcoin. We have miners actively saying they don't like it and this makes
> it
> a contriversial upgrade which means the network may split and other issues.
>
>
There're also many wallets which are impatiently waiting for segwit to be
released. Segwit is blessing for hardware wallets for many reasons. I
actually think that rolling out Segwit will increase security, because it
will reduce huge complexity in hardware wallets as it is today.

Slush

-------------------------------------
James,

I share your conviction that miners are the natural gatekeepers of the
maximum block size.

The trouble I see with Block75 is that linear growth won't work forever.
Also, by reading actual and not miners' preferred max blocksize, this
proposal is sensitive to randomness in block timing and tx rate, and so
incentivizes miners to manipulate their block content unnaturally in
either the up or down direction to influence the calculation. 

The EB/AD scheme of Bitcoin Unlimited recognizes implementation of the
max blocksize by miners, who publish their preferred max blocksize. But
it expects forks of unpredictable (probably short) length as network
behavior evolves.

BIP100, which also recognizes miner implementation of the max blocksize,
but has a change support threshold, and like Block75 defines the timing
of max blocksize increases, looks superior to me.


On 12/18/2016 1:53 PM, James MacWhyte via bitcoin-dev wrote:
> Hi All,
>
> I'm coming late to the party. I like the Block75 proposal.
>
> Multiple people have said miners would/could stuff blocks with
> insincere transactions to increase the block size, but it was never
> adequately explained what they would gain from this. If there aren't
> enough legitimate transactions to fill up the block, where do you plan
> to earn extra income once the block is bigger?
>
> Miners would be incentivized to include as many legitimate
> transactions as possible, but if propagation time is as big an issue
> as some of you have said it is, miners would also be incentivized to
> keep their blocks small enough to propagate. So why not give them the
> choice? Once the block size gets too big to propagate effectively,
> miners would be naturally incentivized to limit how much data they put
> in each block, finding the perfect balance.
>
> In my opinion, none of the downsides presented so far have been a good
> argument. Risk of a 51% attack is not unique to this proposal, saying
> "we could also do that with hardcoded limits" doesn't actually point
> out any problem with this proposal, and miners already have the
> ability to add or withhold transactions from their blocks.
>
> We trust our miners to serve us by acting in their own best interests,
> and this proposal simply gives them more options for doing that. If
> anyone can make a strong argument against that would earn top marks in
> a high school debate class, I'd love to hear it!
>
> James
>



-------------------------------------
On Fri, Jan 8, 2016 at 4:50 PM, Gavin Andresen via bitcoin-dev
<bitcoin-dev@lists.linuxfoundation.org> wrote:
> And to fend off the messag that I bet somebody is composing right now:
>
> Yes, I know about a "security first" mindset.  But as I said earlier in the
> thread, there is a tradeoff here between crypto strength and code
> complexity, and "the strength of the crypto is all that matters" is NOT
> security first.

If the crypto code is properly encapsulated, the code complexity costs
of choosing one hashing function over another should be non-existent.
You made the space argument which is valid, but in my opinion code
complexity shouldn't be a valid concern in this discussion.

As a maybe uninteresting anecdote, I proposed the asset IDs in
https://github.com/ElementsProject/elements/tree/alpha-0.10-multi-asset
to do the same ```ripemd160 . sha256``` choice that Mark Friedenbach
had proposed and I had approved for
https://github.com/jtimon/freimarkets/blob/master/doc/freimarkets_specs.org#asset-tags
. More humble than me, he admitted he had made a design mistake much
earlier than me, who (maybe paradoxically) probably had less knowledge
for making crypto choices at the low level. In the end I was convinced
with examples I failed to write down for documentation and can't
remember.

That's not to say I have anything to say in this debate other than
code complexity (which I do feel qualified to talk about) shouldn't be
a concern in this debate. Just want to focus the discussion on what it
should be: security vs space tradeoff.
Since I am admittedly in doubt, I tend to prefer to play safe, but
neither my feelings nor my anecdote are logical arguments and should,
therefore, be ignored for any conclusions in the ```ripemd160 .
sha256``` vs sha256d debate. Just like you non-sequitor "sha256d will
lead to more code complexity", if anything, sha256d should be simpler
than ```ripemd160 . sha256``` (but not simpler enough that it matters
much).


-------------------------------------
IMO the moderate success of BIP70 is caused by its complexity. Since the
amount of data in a BIP70 payment request does not fit in a bitcoin:
URI, an https server is required to serve the requests.

Only large merchants are able to maintain such an infrastructure; (even
Coinbase recently failed at it, they forgot to update their
certificate). For end users that is completely unpractical.

The main benefit of BIP70 is that the payment request is signed by the
requestor; this gives the sender a proof that they are sending to the
right person, and that the person actually requested the payment.

The same benefit can be achieved without the complexity of BIP70, by
extending the Bitcoin URI scheme. The requestor is authenticated using
DNSSEC, and the payment request is signed using an EC private key. A
domain name and an EC signature are short enough to fit in a Bitcoin URI
and to be shared by QR code or SMS text.

 bitcoin:address?amount=xx&message=yyy&name=john.example.com&sig=zzz

The URI scheme is extended with two fields:
 name: DNS name containing a public key or bitcoin address
 sig: signature

That extension is sufficient to provide authenticated requests, without
requiring a https server. The signed data can be serialized from the
URI, and DNSSEC verification succeeds without requesting extra data from
the requestor. The only assumption is that the verifier is able to make
DNS requests.

I am willing to write a BIP if other wallet developers are interested.




Le 20/06/2016 19:33, Erik Aronesty via bitcoin-dev a crit :
> BIP 0070 has been a a moderate success, however, IMO:
> 
> - protocol buffers are inappropriate since ease of use and extensibility is
> desired over the minor gains of efficiency in this protocol.  Not too late
> to support JSON messages as the standard going forward
> 
> - problematic reliance on merchant-supplied https (X509) as the sole form
> of mechant identification.   alternate schemes (dnssec/netki), pgp and
> possibly keybase seem like good ideas.   personally, i like keybase, since
> there is no reliance on the existing domain-name system (you can sell with
> a github id, for example)
> 
> - missing an optional client supplied identification
> 
> - lack of basic subscription support
> 
> *Proposed for subscriptions:*
> 
> - BIP0047 payment codes are recommended instead of wallet addresses when
> establishing subscriptions.  Or, merchants can specify replacement
> addresses in ACK/NACK responses.   UI confirms are *required *when there
> are no replacement addresses or payment codes used.
> 
> - Wallets must confirm and store subscriptions, and are responsible for
> initiating them at the specified interval.
> 
> - Intervals can *only *be from a preset list: weekly, biweekly, or 1,
> 2,3,4,6 or 12 months.   Intervals missed by more than 3 days cause
> suspension until the user re-verifies.
> 
> - Wallets *may *optionally ask the user whether they want to be notified
> and confirm every interval - or not.   Wallets that do not ask *must *notify
> before initiating each payment.   Interval confirmations should begin at *least
> *1 day in advance of the next payment.
> 
> 
> *Proposed in general:*
> - JSON should be used instead of protocol buffers going forward.  Easier to
> use, explain extend.
> 
> - "Extendible" URI-like scheme to support multi-mode identity mechanisms on
> both payment and subscription requests.   Support for keybase://, netki://
> and others as alternates to https://.
> 
> - Support for client as well as merchant multi-mode verification
> 
> - Ideally, the identity verification URI scheme is somewhat
> orthogonal/independent of the payment request itself
> 
> Question:
> 
> Should this be a new BIP?  I know netki's BIP75 is out there - but I think
> it's too specific and too reliant on the domain name system.
> 
> Maybe an identity-protocol-agnostic BIP + solid implementation of a couple
> major protocols without any mention of payment URI's ... just a way of
> sending and receiving identity verified messages in general?
> 
> I would be happy to implement plugins for identity protocols, if anyone
> thinks this is a good idea.
> 
> Does anyone think https:// or keybase, or PGP or netki all by themselves,
> is enough - or is it always better to have an extensible protocol?
> 
> - Erik Aronesty
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev@lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


-------------------------------------
