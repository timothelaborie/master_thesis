{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import skew, kurtosis, shapiro\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "price_col = 'd_ln_price'\n",
    "# price_col = 'd_ln_joules_per_coin'\n",
    "\n",
    "results_df = None\n",
    "\n",
    "data_loc = '../6_merging/monthly_stuff.csv'\n",
    "\n",
    "def get_data(filter1: str, filter2: str, averages=3):\n",
    "\n",
    "    # date,price,hashrate,coins_per_block,efficiency,max_efficiency,speculation,adoption,altcoins,none,posts_count,positive,neutral,negative,optimistic_speculation,pessimistic_speculation\n",
    "    # 2023-10-01,27978.1,4.4106749777492784e+20,6.25,23880028945.454502,,0.877076411960133,0.2043938991771647,0.127359781121751,0.3427030670711156,0.1050545094152626,0.3953488372093023,0.4418604651162791,0.1627906976744186,0.8024068322981367,0.2088509316770186\n",
    "    data = pd.read_csv(data_loc, parse_dates=['date'])\n",
    "\n",
    "\n",
    "    # keep only data after filter\n",
    "    data = data[data['date'] >= filter1]\n",
    "    data = data[data['date'] < filter2]\n",
    "\n",
    "    # time variable\n",
    "    # data['time'] = 2+((data['date'].dt.year - 2010) * 12 + data['date'].dt.month -2)\n",
    "    # data['ln_time'] = np.log(data['time'])\n",
    "    # data['time_squared'] = data['time']**2\n",
    "\n",
    "    # # Create quarterly averages\n",
    "    # quarterly_data = data.resample('Q', on='date').mean()\n",
    "\n",
    "    # Resample the data to quarterly, ending the quarter in October\n",
    "    quarterly_data = data.resample('Q-OCT', on='date').mean()\n",
    "\n",
    "\n",
    "    # Create monthly averages\n",
    "    monthly_data = data.resample('M', on='date').mean()\n",
    "    # Create weekly averages\n",
    "    weekly_data = data.resample('W', on='date').mean()\n",
    "    # Keep daily data\n",
    "    daily_data = data\n",
    "\n",
    "    useful_cols = data.columns[1:]\n",
    "\n",
    "    # for df in [quarterly_data,monthly_data, weekly_data, daily_data]:\n",
    "    # for df in [quarterly_data]:\n",
    "    for df in [quarterly_data,monthly_data]:\n",
    "        df[\"date\"] = df.index\n",
    "        df[\"before_asic\"] = np.zeros(len(df))\n",
    "        df[\"before_gpu\"] = np.zeros(len(df))\n",
    "        for i in range(len(df)):\n",
    "            if df[\"date\"].iloc[i] < pd.Timestamp('2013-07-01'):\n",
    "                df[\"before_asic\"].iloc[i] = 1\n",
    "            if df[\"date\"].iloc[i] < pd.Timestamp('2012-07-01'):\n",
    "                df[\"before_gpu\"].iloc[i] = 1\n",
    "\n",
    "        for column in useful_cols:\n",
    "            # df[f'ln_{column}'] = np.log(df[column])\n",
    "            df[f'd_ln_{column}'] = np.log(df[column]).diff().fillna(method='bfill')\n",
    "            df[f'avg_d_ln_{column}'] = df[f'd_ln_{column}']\n",
    "        df['ln_price'] = np.log(df['price'])\n",
    "            \n",
    "\n",
    "        # to prevent the arima model from cheating the values have to be shifted\n",
    "        # for column in df.columns:\n",
    "        #     if column.startswith('d_') and column != 'd_ln_open_price':\n",
    "        #         df[column] = df[column].shift(-1).fillna(method='ffill')\n",
    "\n",
    "    \n",
    "        # to prevent the arima model from cheating the values are replaced with the average of the last 3\n",
    "        for column in useful_cols:\n",
    "            column2 = f'd_ln_{column}'\n",
    "            if column2 != price_col:\n",
    "                # print(column2)\n",
    "                for i in range(len(df)):\n",
    "                    offset = min(i, averages)\n",
    "                    newval = df[column2].iloc[i-offset:i].mean()\n",
    "                    df['avg_' + column2].iloc[i] = newval if not np.isnan(newval) else 0\n",
    "\n",
    "        # calculate a prediction for joules_per_coin using the average of the last 3 d_ln values of hashrate and efficiency\n",
    "        # coins_per_block is trivial to predict and can be taken from the data\n",
    "\n",
    "        # df['predicted_hashrate'] = np.exp(np.log(df['hashrate'].shift(-1)) + df['avg_d_ln_hashrate'])\n",
    "        # df['predicted_efficiency'] = np.exp(np.log(df['efficiency'].shift(-1)) + df['avg_d_ln_efficiency'])\n",
    "        # df['predicted_coins_per_block'] = df['coins_per_block']  # Assuming this is directly taken from the data\n",
    "        # df['predicted_joules_per_coin'] = df['predicted_hashrate'] / (df['predicted_efficiency'] * df['predicted_coins_per_block'])\n",
    "        # df['d_ln_predicted_joules_per_coin'] = np.log(df['predicted_joules_per_coin']).diff().fillna(method='bfill')\n",
    "\n",
    "        df['predicted_hashrate'] = np.zeros(len(df))\n",
    "        df['predicted_efficiency'] = np.zeros(len(df))\n",
    "        df['predicted_coins_per_block'] = np.zeros(len(df))\n",
    "        df['predicted_joules_per_coin'] = np.zeros(len(df))\n",
    "        df['d_ln_predicted_joules_per_coin'] = np.zeros(len(df))\n",
    "        for i in range(len(df)):\n",
    "            # for the first value, we give the model the real value instead of a prediction\n",
    "            if i == 0:\n",
    "                df['predicted_hashrate'].iloc[i] = df['hashrate'].iloc[i]\n",
    "                df['predicted_efficiency'].iloc[i] = df['efficiency'].iloc[i]\n",
    "                df['predicted_coins_per_block'].iloc[i] = df['coins_per_block'].iloc[i]\n",
    "                df['predicted_joules_per_coin'].iloc[i] = df['hashrate'].iloc[i] / (df['efficiency'].iloc[i] * df['coins_per_block'].iloc[i])\n",
    "            else:\n",
    "                df['predicted_hashrate'].iloc[i] = np.exp(np.log(df['hashrate'].iloc[i-1]) + df['avg_d_ln_hashrate'].iloc[i])\n",
    "                df['predicted_efficiency'].iloc[i] = np.exp(np.log(df['efficiency'].iloc[i-1]) + df['avg_d_ln_efficiency'].iloc[i])\n",
    "                df['predicted_coins_per_block'].iloc[i] = df['coins_per_block'].iloc[i]\n",
    "                df['predicted_joules_per_coin'].iloc[i] = df['predicted_hashrate'].iloc[i] / (df['predicted_efficiency'].iloc[i] * df['predicted_coins_per_block'].iloc[i])\n",
    "\n",
    "        df['d_ln_predicted_joules_per_coin'] = np.log(df['predicted_joules_per_coin']).diff().fillna(method='bfill')\n",
    "\n",
    "        df['real_joules_per_coin'] = df['hashrate'] / (df['efficiency'] * df['coins_per_block'])\n",
    "        df['ln_real_joules_per_coin'] = np.log(df['real_joules_per_coin'])\n",
    "        df['ln_joules_per_coin'] = np.log(df['real_joules_per_coin'])\n",
    "        df['d_ln_joules_per_coin'] = np.log(df['real_joules_per_coin']).diff().fillna(method='bfill')\n",
    "\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "        # to_scale = []\n",
    "        # for column in df.columns:\n",
    "        #     if column.startswith('d_') and column != price_col:\n",
    "        #         to_scale.append(column)\n",
    "        # for column in to_scale:\n",
    "        #     scaler = MinMaxScaler()\n",
    "        #     df[column] = scaler.fit_transform(df[column].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    return quarterly_data,monthly_data, weekly_data, daily_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_model(test_data, predictions, p, q, title='Model Estimation'):\n",
    "    # Normalizing count and d_ln_avg_efficiency\n",
    "    # scaler = MinMaxScaler(feature_range=(-0.15, 0.15))\n",
    "    # normalized_values = scaler.fit_transform(test_data[['count', 'd_ln_avg_efficiency', 'cost', 'open_price', 'd_ln_cost', 'd_ln_joules_per_coin']])\n",
    "    # normalized_count = normalized_values[:, 0]\n",
    "    # normalized_d_ln_avg_efficiency = normalized_values[:, 1]\n",
    "    # normalized_cost = normalized_values[:, 2]\n",
    "    # normalized_open_price = normalized_values[:, 3]\n",
    "    # normalized_d_ln_cost = normalized_values[:, 4]\n",
    "    # normalized_d_ln_joules_per_coin = normalized_values[:, 5]\n",
    "\n",
    "    offset = -9 if price_col == 'ln_open_price' else 0\n",
    "    \n",
    "    # Plotting the predictions against the actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(test_data.index, test_data[price_col]+offset, label=f'Actual {price_col}', color='blue', marker='o')\n",
    "    plt.scatter(test_data.index, predictions+offset, label=f'Predicted {price_col}', color='red', marker='x')\n",
    "    # plt.plot(test_data.index, normalized_count, label='Normalized Count', color='green', linestyle='--')\n",
    "    # plt.plot(test_data.index, normalized_d_ln_joules_per_coin, label='Normalized normalized_d_ln_joules_per_coin', color='green', linestyle='--')\n",
    "    # plt.plot(test_data.index, normalized_d_ln_avg_efficiency, label='Normalized d_ln_avg_efficiency', color='purple', linestyle='-.')\n",
    "    # plt.plot(test_data.index, normalized_d_ln_cost, label='Normalized d_ln_cost', color='purple', linestyle='-.')\n",
    "    # plt.plot(test_data.index, normalized_cost, label='Normalized Cost', color='orange', linestyle=':')\n",
    "    # plt.plot(test_data.index, normalized_open_price, label='Normalized Open Price', color='black', linestyle='-.')\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=1)  # Dashed line at y=0\n",
    "    plt.title(f'{title} Predictions vs Actual, p={p}, q={q}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('diff of ln(open price) and Normalized Values')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.tight_layout()\n",
    "    # make only half the x ticks show\n",
    "    for label in plt.gca().xaxis.get_ticklabels()[::2]:\n",
    "        label.set_visible(False)\n",
    "    plt.savefig('pdfs/arimapreds.pdf', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "def add_constant(train_exog, test_exog):\n",
    "    # train_exog[\"const\"] = 1\n",
    "    # test_exog[\"const\"] = 1\n",
    "    # # Ensure the shapes of train_exog and test_exog match\n",
    "    # print(train_exog.columns)\n",
    "    # print(test_exog.columns)\n",
    "    return train_exog, test_exog\n",
    "\n",
    "def residuals_tests(model_fit):\n",
    "    aic = model_fit.aic\n",
    "    bic = model_fit.bic\n",
    "    \n",
    "    # Test residuals for autocorrelation, normality, and heteroscedasticity\n",
    "    residuals = model_fit.resid\n",
    "    # Autocorrelation tests\n",
    "    ljung_box = sm.stats.acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "    durbin_watson = sm.stats.durbin_watson(residuals)\n",
    "    # Normality tests\n",
    "    jarque_bera = sm.stats.jarque_bera(residuals)\n",
    "    shapiro_wilk = shapiro(residuals)\n",
    "    # Heteroscedasticity test\n",
    "    white_test = [-1,-1]\n",
    "    try:\n",
    "        white_test = sm.stats.het_white(residuals, model_fit.model.exog)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    res = {\n",
    "        'AIC': aic, 'BIC': bic,\n",
    "        'Ljung-Box': ljung_box['lb_pvalue'].values[0], 'Durbin-Watson': durbin_watson,\n",
    "        'Jarque-Bera': jarque_bera[1], 'Shapiro-Wilk': shapiro_wilk[1],\n",
    "        'White Test': white_test[1]\n",
    "    }\n",
    "    for k, v in res.items():\n",
    "        # print(k + \": \" + str(v)[:(5 if k not in ['White Test'] else 9)])\n",
    "        v_ = f\"{v:.4g}\"\n",
    "        print(k + \": \" + v_)\n",
    "        res[k] = v_\n",
    "\n",
    "    res_df = pd.DataFrame(res, index=[0])\n",
    "    print(res_df)\n",
    "    res_df = res_df.drop(columns=['AIC', 'BIC']).T.reset_index()\n",
    "    print(res_df.to_latex(index=False))\n",
    "\n",
    "loss_table = []\n",
    "full_model = None\n",
    "# Adjust the model estimation to use the differenced series\n",
    "def model_estimation(data, p_range, q_range, exog_cols, use_exog, title='Model Estimation', eval_size=8, noprint=False):\n",
    "    global results_df\n",
    "    global loss_table \n",
    "    global full_model\n",
    "\n",
    "    results = []\n",
    "    data_size = len(data)\n",
    "    train_size = data_size - eval_size\n",
    "\n",
    "    loss_table = []\n",
    "\n",
    "    for p in p_range:\n",
    "        for q in q_range:\n",
    "            try:\n",
    "                # Perform rolling cross-validation\n",
    "                rolling_mae = []\n",
    "                rolling_mse = []\n",
    "                mean_baseline_mae = []\n",
    "                mean_baseline_mse = []\n",
    "                prev_value_baseline_mae = []\n",
    "                prev_value_baseline_mse = []\n",
    "                step_size = 1\n",
    "                summary = \"\"\n",
    "                all_predictions = []\n",
    "                for end in range(train_size, data_size, step_size):  # Adjust these values as needed\n",
    "                    # print(f\"p: {p}, q: {q}, end: {end}\")\n",
    "                    rolling_train = data.iloc[0:end]\n",
    "                    rolling_test = data.iloc[end:end + step_size]  # Predict one step ahead\n",
    "                    rolling_train_exog, rolling_test_exog = add_constant(rolling_train[exog_cols], rolling_test[exog_cols])\n",
    "                    \n",
    "                    # Baseline 1: Mean Prediction\n",
    "                    mean_pred = np.mean(rolling_train[price_col])\n",
    "                    mean_baseline_mae.append(mean_absolute_error(rolling_test[price_col], [mean_pred]*step_size))\n",
    "                    mean_baseline_mse.append(mean_squared_error(rolling_test[price_col], [mean_pred]*step_size))\n",
    "                    \n",
    "                    # Baseline 2: Previous Value Prediction\n",
    "                    prev_value_pred = rolling_train[price_col].iloc[-1]\n",
    "                    prev_value_baseline_mae.append(mean_absolute_error(rolling_test[price_col], [prev_value_pred]*step_size))\n",
    "                    prev_value_baseline_mse.append(mean_squared_error(rolling_test[price_col], [prev_value_pred]*step_size))\n",
    "\n",
    "\n",
    "                    # ARIMA Model\n",
    "                    if use_exog:\n",
    "                        model = ARIMA(rolling_train[price_col], order=(p, 0, q), exog=rolling_train_exog)\n",
    "                    else:\n",
    "                        model = ARIMA(rolling_train[price_col], order=(p, 0, q))\n",
    "                    model_fit = model.fit()\n",
    "                    \n",
    "                    summary = model_fit.summary()\n",
    "                    if use_exog:\n",
    "                        predictions = model_fit.forecast(steps=step_size, exog=rolling_test_exog)\n",
    "                    else:\n",
    "                        predictions = model_fit.forecast(steps=step_size)\n",
    "                    mae = mean_absolute_error(rolling_test[price_col], predictions)\n",
    "                    mse = mean_squared_error(rolling_test[price_col], predictions)\n",
    "                    rolling_mae.append(mae)\n",
    "                    rolling_mse.append(mse)\n",
    "                    all_predictions.extend(predictions)\n",
    "                \n",
    "                # print(\"rolling_mse\")\n",
    "                # print(rolling_mse)\n",
    "                avg_mae = np.mean(rolling_mae)\n",
    "                avg_mse = np.mean(rolling_mse)\n",
    "                avg_mean_baseline_mae = np.mean(mean_baseline_mae)\n",
    "                avg_mean_baseline_mse = np.mean(mean_baseline_mse)\n",
    "                avg_prev_value_baseline_mae = np.mean(prev_value_baseline_mae)\n",
    "                avg_prev_value_baseline_mse = np.mean(prev_value_baseline_mse)\n",
    "                results.append({'p': p, 'q': q, 'MAE': avg_mae, 'Mean Baseline MAE': avg_mean_baseline_mae, 'Prev Value Baseline MAE': avg_prev_value_baseline_mae, 'MSE': avg_mse, 'Mean Baseline MSE': avg_mean_baseline_mse, 'Prev Value Baseline MSE': avg_prev_value_baseline_mse,'Summary': summary,'all_predictions':all_predictions})\n",
    "\n",
    "                loss_table.append({\"p\":p,\"q\":q,\"MSE\":avg_mse,\"MAE\":avg_mae})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with ARIMA({p},0,{q}): {e}\")\n",
    "                if noprint:\n",
    "                    return 9999\n",
    "\n",
    "    if noprint:\n",
    "        return results[0]['MSE']\n",
    "\n",
    "    # sort results and print/plot only the top 3\n",
    "    results = sorted(results, key=lambda x: x['MSE'])\n",
    "    print(f\"Mean Baseline MSE: {results[0]['Mean Baseline MSE']}\")\n",
    "    print(f\"Previous Value Baseline MSE: {results[0]['Prev Value Baseline MSE']}\")\n",
    "    for i in range(3):\n",
    "        if i >= len(results):\n",
    "            break\n",
    "        p = results[i]['p']\n",
    "        q = results[i]['q']\n",
    "        \n",
    "        # Fit the model on the entire training dataset\n",
    "        model_fit = None\n",
    "        if use_exog:\n",
    "            train_exog, _ = add_constant(data[exog_cols], [])\n",
    "            train_exog = np.array(train_exog)\n",
    "            model = ARIMA(data[price_col], order=(p, 0, q), exog=train_exog)\n",
    "            model_fit = model.fit()\n",
    "        else:\n",
    "            model = ARIMA(data[price_col], order=(p, 0, q))\n",
    "            model_fit = model.fit()\n",
    "\n",
    "        full_model = model_fit\n",
    "\n",
    "        print(f\"Top {i+1} Model: ARIMA({p},0,{q})\")\n",
    "        residuals_tests(model_fit)\n",
    "        print(\"MSE:\",results[i]['MSE'])\n",
    "        print(results[i]['Summary'])\n",
    "\n",
    "        train_data = data.iloc[0:train_size]\n",
    "        test_data = data.iloc[train_size:]\n",
    "\n",
    "        all_predictions = results[i]['all_predictions']\n",
    "        all_predictions_with_dates = pd.Series(all_predictions, index=test_data.index)\n",
    "\n",
    "        all_predictions_with_dates_df = pd.DataFrame(all_predictions_with_dates, columns=['predicted_d_ln_price'])\n",
    "        # all_predictions_with_dates_df.to_csv(f'predictions_{title}_{p}_{q}.csv')\n",
    "\n",
    "        # plot_model(test_data, predictions, p, q, title=f\"{title} - Top {i+1} Model\")\n",
    "        plot_model(test_data, all_predictions_with_dates, p, q, title=f\"{title} - Top {i+1} Model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exploratory_analysis(df):\n",
    "    # Select only numeric columns for distribution, skewness, and kurtosis\n",
    "    # numeric_df = df.select_dtypes(include=[np.number])\n",
    "    cols = [price_col,'d_ln_joules_per_coin'] #'d_ln_avg_efficiency', \n",
    "    numeric_df = df[cols]\n",
    "    \n",
    "    num_cols = len(numeric_df.columns)\n",
    "    num_rows = (num_cols + 1) // 2  # Ensure enough rows to accommodate all columns\n",
    "    print(\"Distribution Plot:\")\n",
    "    numeric_df.hist(bins=20, figsize=(14, num_rows * 3), layout=(num_rows, 2))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pdfs/valuedistribution.pdf', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Summary Statistics:\")\n",
    "    summary = numeric_df.describe().T\n",
    "    summary['std'] = numeric_df.std()\n",
    "    summary['skewness'] = numeric_df.skew()\n",
    "    summary['kurtosis'] = numeric_df.kurtosis()\n",
    "    # rename count to \"amount of values\"\n",
    "    summary.rename(columns={'count': 'amount of values'}, inplace=True)\n",
    "    # print(summary)\n",
    "\n",
    "    for col in cols:\n",
    "        if col in numeric_df.columns:\n",
    "            print(f\"ACF and PACF Plots for {col}:\")\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "            sm.graphics.tsa.plot_acf(numeric_df[col].dropna(), lags=10, ax=axes[0])\n",
    "            sm.graphics.tsa.plot_pacf(numeric_df[col].dropna(), lags=10, ax=axes[1])\n",
    "            plt.savefig('pdfs/autocorrelation.pdf', format='pdf')\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def check_stationarity(series):\n",
    "    from statsmodels.tsa.stattools import adfuller, kpss, zivot_andrews\n",
    "    from arch.unitroot import DFGLS\n",
    "    \n",
    "    # Augmented Dickey-Fuller Test\n",
    "    adf_result = adfuller(series.dropna())\n",
    "    print('Augmented Dickey-Fuller Test:')\n",
    "    print('ADF Statistic: %f' % adf_result[0])\n",
    "    print('p-value: %f' % adf_result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in adf_result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "    print('\\n')\n",
    "    \n",
    "    # KPSS Test\n",
    "    kpss_result = kpss(series.dropna(), regression='c')\n",
    "    print('KPSS Test:')\n",
    "    print('KPSS Statistic: %f' % kpss_result[0])\n",
    "    print('p-value: %f' % kpss_result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "    print('\\n')\n",
    "    \n",
    "    # Phillips-Perron Test\n",
    "    # pp_result = phillips_perron(series.dropna())\n",
    "    # print('Phillips-Perron Test:')\n",
    "    # print('PP Statistic: %f' % pp_result.stat)\n",
    "    # print('p-value: %f' % pp_result.pvalue)\n",
    "    # print('Critical Values:')\n",
    "    # for key, value in pp_result.critical_values.items():\n",
    "    #     print('\\t%s: %.3f' % (key, value))\n",
    "    # print('\\n')\n",
    "    \n",
    "    # Zivot-Andrews Test\n",
    "    # za_result = zivot_andrews(series.dropna(), maxlag=12)\n",
    "    # print('Zivot-Andrews Test:')\n",
    "    # print('ZA Statistic: %f' % za_result[0])\n",
    "    # print('p-value: %f' % za_result[1])\n",
    "    # print('Critical Values:')\n",
    "    # for key, value in za_result[2].items():\n",
    "    #     print('\\t%s: %.3f' % (key, value))\n",
    "    # print('\\n')\n",
    "    \n",
    "    # Elliott-Rothenberg-Stock (ERS) Test\n",
    "    ers_result = DFGLS(series.dropna())\n",
    "    print('Elliott-Rothenberg-Stock (ERS) Test:')\n",
    "    print('ERS Statistic: %f' % ers_result.stat)\n",
    "    print('p-value: %f' % ers_result.pvalue)\n",
    "    print('Critical Values:')\n",
    "    for key, value in ers_result.critical_values.items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import coint\n",
    "def perform_cointegration_test(df, column1, column2):\n",
    "    \"\"\"\n",
    "    Perform the co-integration test on two specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - column1: The name of the first column to test.\n",
    "    - column2: The name of the second column to test.\n",
    "\n",
    "    Returns:\n",
    "    - coint_t: The t-statistic of the test.\n",
    "    - p_value: The p-value of the test.\n",
    "    - crit_value: Critical values for the test.\n",
    "    \"\"\"\n",
    "    score, p_value, crit_value = coint(df[column1], df[column2])\n",
    "    print(f\"Co-integration test results for {column1} and {column2}:\")\n",
    "    print(f\"t-statistic: {score}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "    print(f\"Critical values: {crit_value}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"The series are likely co-integrated.\")\n",
    "    else:\n",
    "        print(\"The series are not likely co-integrated.\")\n",
    "    print(\"\")\n",
    "    # return score, p_value, crit_value\n",
    "\n",
    "\n",
    "def cross_correlation(series1, series2, lag=10, plot=True):\n",
    "    \"\"\"\n",
    "    Calculate and plot cross-correlation between two series.\n",
    "    \n",
    "    Parameters:\n",
    "    - series1: First time series.\n",
    "    - series2: Second time series.\n",
    "    - lag: Number of lags to calculate cross-correlation for.\n",
    "    - plot: Whether to plot the cross-correlation values.\n",
    "    \n",
    "    Returns:\n",
    "    - Cross-correlation values.\n",
    "    \"\"\"\n",
    "    cc_values = [series1.corr(series2.shift(l)) for l in range(-lag, lag+1)]\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.stem(range(-lag, lag+1), cc_values)#, use_line_collection=True)\n",
    "        plt.xlabel('Lag')\n",
    "        plt.ylabel('Cross-correlation')\n",
    "        plt.title('Cross-correlation between series')\n",
    "        plt.show()\n",
    "    \n",
    "    return cc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quarterly_data, monthly_data, weekly_data, daily_data = get_data('2010-12-31', '9999-01-01',2)\n",
    "quarterly_data, monthly_data, weekly_data, daily_data = get_data('2011-08-15', '9999-01-01',2)\n",
    "# quarterly_data, monthly_data, weekly_data, daily_data = get_data('2013-07-01', '9999-01-01',2)\n",
    "# quarterly_data, monthly_data, weekly_data, daily_data = get_data('2005-12-31', '9999-01-01',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_data.tail(15)\n",
    "\n",
    "# quarterly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in quarterly_data.columns:\n",
    "#     if col.startswith('avg'):\n",
    "#         print(f\"# '{col}',\")s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # find best start date\n",
    "# dates = pd.read_csv(data_loc, parse_dates=['date']).resample('Q', on='date').mean().index\n",
    "# dates = dates[:33]\n",
    "# best_mse = 999999\n",
    "# best_date = None\n",
    "# best_averaging = None\n",
    "# for date in dates:\n",
    "#     print(date)\n",
    "#     for averaging in [2,3]:\n",
    "#         print(averaging)\n",
    "#         quarterly_data, monthly_data, weekly_data, daily_data = get_data(date, '9999-01-01',averaging)\n",
    "#         all_mse = []\n",
    "#         for cols in [[],['d_ln_joules_per_coin'],['avg_d_ln_pessimistic_speculation']]:\n",
    "#             mse = model_estimation(quarterly_data, range(1, 2, 1), range(1, 2, 1), cols, True, title='Quarterly Data', eval_size=13, noprint=True)\n",
    "#             all_mse.append(mse)\n",
    "#         print(np.mean(all_mse))\n",
    "#         if np.mean(all_mse) < best_mse:\n",
    "#             best_mse = np.mean(all_mse)\n",
    "#             best_date = date\n",
    "#             best_averaging = averaging\n",
    "\n",
    "# print(\"\")\n",
    "# print(best_date)\n",
    "# print(best_mse)\n",
    "# print(best_averaging)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quaterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_cols = [\n",
    "'before_asic',\n",
    "# 'before_gpu',\n",
    "\n",
    "# 'd_ln_price',\n",
    "\n",
    "# 'avg_efficiency',\n",
    "# 'avg_d_ln_max_efficiency',\n",
    "# 'avg_d_ln_count',\n",
    "# 'avg_d_ln_cost_per_coin',\n",
    "# 'avg_d_ln_watts',\n",
    "# 'avg_d_ln_joules_per_dollar_earned',\n",
    "# 'avg_d_ln_optimistic_speculation',\n",
    "\n",
    "# 'd_ln_predicted_joules_per_coin',\n",
    "'d_ln_joules_per_coin',\n",
    "# 'ln_joules_per_coin',\n",
    "\n",
    "# 'avg_d_ln_adoption',\n",
    "\n",
    "# 'avg_d_ln_hashrate',\n",
    "# 'avg_d_ln_efficiency',\n",
    "# 'd_ln_efficiency',\n",
    "\n",
    "# 'd_ln_pessimistic_speculation',\n",
    "\n",
    "# 'avg_d_ln_pessimistic_speculation',\n",
    "# 'avg_d_ln_speculation',\n",
    "# 'avg_d_ln_posts_count',\n",
    "# 'avg_d_ln_positive',\n",
    "# 'avg_d_ln_neutral',\n",
    "# 'avg_d_ln_negative',\n",
    "\n",
    "# 'time',\n",
    "# 'ln_time',\n",
    "# 'time_squared',\n",
    "]\n",
    "# Baseline 3: no exog\n",
    "# use_exog = False\n",
    "use_exog = True\n",
    "\n",
    "print(\"Quarterly Data Model Estimation:\")\n",
    "# model_estimation(quarterly_data, range(1, 5, 1), range(1, 5, 1), exog_cols, use_exog, title='Quarterly Data', eval_size=13)\n",
    "model_estimation(quarterly_data, range(1, 2, 1), range(1, 2, 1), exog_cols, use_exog, title='Quarterly Data', eval_size=13)\n",
    "# model_estimation(monthly_data, range(1, 2, 1), range(1, 2, 1), exog_cols, use_exog, title='Monthly Data', eval_size=13)\n",
    "# model_estimation(monthly_data, range(1, 5, 1), range(1, 5, 1), exog_cols, use_exog, title='Monthly Data', eval_size=13)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price model: run predictions on the full data\n",
    "full_exog, _ = add_constant(quarterly_data[exog_cols], [])\n",
    "predictions_full_data = full_model.predict(start=0, end=len(quarterly_data)-1, exog=full_exog)\n",
    "predictions_full_data = pd.DataFrame(predictions_full_data)\n",
    "# rename predicted_mean to predicted_d_ln_price\n",
    "predictions_full_data.rename(columns={'predicted_mean': 'predicted_d_ln_price'}, inplace=True)\n",
    "# predictions_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_full_data.to_csv(f'predictions_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_table_df = pd.DataFrame(loss_table)\n",
    "loss_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to latex with rrrr replaced by cccc and caption \"Loss table.\" and \\centering\n",
    "print(loss_table_df.to_latex(index=False, float_format=\"%.4f\", caption=\"Loss table.\").replace(\"rrrr\", \"cccc\").replace(\"\\\\begin{table}\",\"\\\\begin{table}\\n\\\\centering\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain long term forecast\n",
    "# exog = quarterly_data[cols]\n",
    "# model = ARIMA(quarterly_data[price_col], order=(1, 0, 1), exog=exog).fit()\n",
    "# # repeat the average of the last 10 rows for 10 years\n",
    "# exog2 = pd.DataFrame()\n",
    "# for column in cols:\n",
    "#     exog2[column] = [quarterly_data[column].iloc[-10:].mean()]*40\n",
    "# # print(exog2)\n",
    "# preds = model.forecast(steps=40, exog=exog2)\n",
    "# # print(preds)\n",
    "\n",
    "# # plot the predicted price\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# dates = pd.date_range(start=quarterly_data.index[-1], end='2033-11-01', freq='Q')\n",
    "\n",
    "# last_price = quarterly_data[\"price\"].iloc[-1]\n",
    "# ln_prices = np.log(last_price) + preds.cumsum()\n",
    "# prices = np.exp(ln_prices)\n",
    "# log10_prices = np.log10(prices)\n",
    "\n",
    "# plt.plot(dates, log10_prices, label='Predicted Price', color='red')\n",
    "\n",
    "\n",
    "\n",
    "# obtain long term forecast with koomey's law\n",
    "# last_price = quarterly_data[\"price\"].iloc[-1]\n",
    "# time_to_double = 2.6\n",
    "# koomey_preds = []\n",
    "# for i in range(40):\n",
    "#     koomey_preds.append(last_price * 2**((i/4)/time_to_double))\n",
    "# dates = pd.date_range(start=quarterly_data.index[-1], end='2033-11-01', freq='Q')\n",
    "# plt.plot(dates, np.log10(koomey_preds), label='Koomey Prediction', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create plot with old data and prediction\n",
    "# # concat the x axis\n",
    "# old_x = quarterly_data.index\n",
    "# new_x = dates\n",
    "# all_x = np.concatenate([old_x,new_x])\n",
    "# # concat the y axis\n",
    "# old_y = np.log10(quarterly_data[\"price\"])\n",
    "# new_y = np.log10(koomey_preds)\n",
    "# all_y = np.concatenate([old_y,new_y])\n",
    "\n",
    "# plt.plot(all_x, all_y, label='Price', color='blue')\n",
    "\n",
    "# # Get current axes\n",
    "# ax = plt.gca()\n",
    "\n",
    "# # Extend the limits of x and y axes\n",
    "# ax.set_xlim([min(all_x), max(all_x) + 9999999999999999])  # Extend x-axis to the right\n",
    "# ax.set_ylim([min(all_y), max(all_y) + 1])  # Extend y-axis upwards\n",
    "\n",
    "# # Add labels and title if necessary\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Log10 Price')\n",
    "# plt.title('Price predictions')\n",
    "\n",
    "# # Show legend\n",
    "# plt.legend()\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first = koomey_preds[0]\n",
    "# last = koomey_preds[-1]\n",
    "\n",
    "# print(\"percentage increase:\",100*(last-first)/first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quarterly_data = monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stationarity check for ln_price:\")\n",
    "check_stationarity(quarterly_data[\"ln_price\"])\n",
    "# print(f\"Stationarity check for d_ln_price:\")\n",
    "# check_stationarity(quarterly_data[\"d_ln_price\"])\n",
    "print(\"\\nStationarity check for ln_real_joules_per_coin:\")\n",
    "check_stationarity(quarterly_data['ln_real_joules_per_coin'])\n",
    "# print(\"\\nStationarity check for d_ln_joules_per_coin:\")\n",
    "# check_stationarity(quarterly_data['d_ln_joules_per_coin'])\n",
    "# print(\"\\nStationarity check for avg_d_ln_pessimistic_speculation:\")\n",
    "# check_stationarity(quarterly_data['avg_d_ln_pessimistic_speculation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define the data\n",
    "# data = {\n",
    "#     \"Time Series\": [\"Logarithmized Bitcoin Price\", \"Logarithmized Joules per Coin\"],\n",
    "#     \"KPSS Statistic\": [0.996111, 0.999804],\n",
    "#     \"p-value\": [0.010000, 0.010000],\n",
    "#     \"Critical Values\": [\"10%: 0.347, 5%: 0.463, 2.5%: 0.574, 1%: 0.739\", \"10%: 0.347, 5%: 0.463, 2.5%: 0.574, 1%: 0.739\"]\n",
    "# }\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = exploratory_analysis(quarterly_data)\n",
    "print(summary.to_latex(float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_cointegration_test(quarterly_data, 'd_ln_price', 'd_ln_joules_per_coin')\n",
    "perform_cointegration_test(quarterly_data, 'ln_price', 'ln_joules_per_coin')\n",
    "# perform_cointegration_test(monthly_data, 'd_ln_price', 'd_ln_joules_per_coin')\n",
    "# perform_cointegration_test(monthly_data, 'ln_price', 'ln_joules_per_coin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_correlation_values = cross_correlation(quarterly_data['d_ln_avg_efficiency'], quarterly_data[price_col], lag=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_correlation_values = cross_correlation(quarterly_data['d_ln_joules_per_coin'], quarterly_data['d_ln_price'], lag=10)\n",
    "cross_correlation_values = cross_correlation(quarterly_data['d_ln_price'], quarterly_data['d_ln_joules_per_coin'], lag=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_correlation_values = cross_correlation(quarterly_data['d_ln_efficiency'], quarterly_data['d_ln_price'], lag=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ln_real_joules_per_coin\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(quarterly_data.index, quarterly_data['ln_real_joules_per_coin'], label='ln_real_joules_per_coin', color='blue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
