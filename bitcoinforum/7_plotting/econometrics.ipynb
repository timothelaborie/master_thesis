{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import skew, kurtosis, shapiro\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "\n",
    "price_col = 'd_ln_price'\n",
    "\n",
    "results_df = None\n",
    "\n",
    "\n",
    "\n",
    "def get_data(filter1: str, filter2: str, averages=3):\n",
    "\n",
    "    # date,price,hashrate,coins_per_block,efficiency,max_efficiency,speculation,adoption,altcoins,none,posts_count,positive,neutral,negative,optimistic_speculation,pessimistic_speculation\n",
    "    # 2023-10-01,27978.1,4.4106749777492784e+20,6.25,23880028945.454502,,0.877076411960133,0.2043938991771647,0.127359781121751,0.3427030670711156,0.1050545094152626,0.3953488372093023,0.4418604651162791,0.1627906976744186,0.8024068322981367,0.2088509316770186\n",
    "    data = pd.read_csv('../6_calculating_costs/monthly_stuff.csv', parse_dates=['date'])\n",
    "\n",
    "\n",
    "    # keep only data after filter\n",
    "    data = data[data['date'] >= filter1]\n",
    "    data = data[data['date'] < filter2]\n",
    "\n",
    "    # time variable: (year-2011)*12+month\n",
    "    # data['time'] = (data['date'].dt.year - 2011) * 12 + data['date'].dt.month\n",
    "\n",
    "    # data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Create quarterly averages\n",
    "    quarterly_data = data.resample('Q', on='date').mean()\n",
    "    # Create monthly averages\n",
    "    monthly_data = data.resample('M', on='date').mean()\n",
    "    # Create weekly averages\n",
    "    weekly_data = data.resample('W', on='date').mean()\n",
    "    # Keep daily data\n",
    "    daily_data = data\n",
    "\n",
    "    useful_cols = data.columns[1:]\n",
    "\n",
    "    for df in [quarterly_data,monthly_data, weekly_data, daily_data]:\n",
    "        for column in useful_cols:\n",
    "            # df[f'ln_{column}'] = np.log(df[column])\n",
    "            df[f'd_ln_{column}'] = np.log(df[column]).diff().fillna(method='bfill')\n",
    "            df[f'avg_d_ln_{column}'] = df[f'd_ln_{column}']\n",
    "            \n",
    "\n",
    "        # to prevent the arima model from cheating the values have to be shifted\n",
    "        # for column in df.columns:\n",
    "        #     if column.startswith('d_') and column != 'd_ln_open_price':\n",
    "        #         df[column] = df[column].shift(-1).fillna(method='ffill')\n",
    "\n",
    "    \n",
    "        # to prevent the arima model from cheating the values are replaced with the average of the last 3\n",
    "        for column in useful_cols:\n",
    "            column2 = f'd_ln_{column}'\n",
    "            if column2 != price_col:\n",
    "                # print(column2)\n",
    "                for i in range(len(df)):\n",
    "                    offset = min(i, averages)\n",
    "                    newval = df[column2].iloc[i-offset:i].mean()\n",
    "                    df['avg_' + column2].iloc[i] = newval if not np.isnan(newval) else 0\n",
    "\n",
    "        # calculate a prediction for joules_per_coin using the average of the last 3 d_ln values of hashrate and efficiency\n",
    "        # coins_per_block is trivial to predict and can be taken from the data\n",
    "\n",
    "        # df['predicted_hashrate'] = np.exp(np.log(df['hashrate'].shift(-1)) + df['avg_d_ln_hashrate'])\n",
    "        # df['predicted_efficiency'] = np.exp(np.log(df['efficiency'].shift(-1)) + df['avg_d_ln_efficiency'])\n",
    "        # df['predicted_coins_per_block'] = df['coins_per_block']  # Assuming this is directly taken from the data\n",
    "        # df['predicted_joules_per_coin'] = df['predicted_hashrate'] / (df['predicted_efficiency'] * df['predicted_coins_per_block'])\n",
    "        # df['d_ln_predicted_joules_per_coin'] = np.log(df['predicted_joules_per_coin']).diff().fillna(method='bfill')\n",
    "\n",
    "        df['predicted_hashrate'] = np.zeros(len(df))\n",
    "        df['predicted_efficiency'] = np.zeros(len(df))\n",
    "        df['predicted_coins_per_block'] = np.zeros(len(df))\n",
    "        df['predicted_joules_per_coin'] = np.zeros(len(df))\n",
    "        df['d_ln_predicted_joules_per_coin'] = np.zeros(len(df))\n",
    "        for i in range(len(df)):\n",
    "            # for the first value, we give the model the real value instead of a prediction\n",
    "            if i == 0:\n",
    "                df['predicted_hashrate'].iloc[i] = df['hashrate'].iloc[i]\n",
    "                df['predicted_efficiency'].iloc[i] = df['efficiency'].iloc[i]\n",
    "                df['predicted_coins_per_block'].iloc[i] = df['coins_per_block'].iloc[i]\n",
    "                df['predicted_joules_per_coin'].iloc[i] = df['hashrate'].iloc[i] / (df['efficiency'].iloc[i] * df['coins_per_block'].iloc[i])\n",
    "            else:\n",
    "                df['predicted_hashrate'].iloc[i] = np.exp(np.log(df['hashrate'].iloc[i-1]) + df['avg_d_ln_hashrate'].iloc[i])\n",
    "                df['predicted_efficiency'].iloc[i] = np.exp(np.log(df['efficiency'].iloc[i-1]) + df['avg_d_ln_efficiency'].iloc[i])\n",
    "                df['predicted_coins_per_block'].iloc[i] = df['coins_per_block'].iloc[i]\n",
    "                df['predicted_joules_per_coin'].iloc[i] = df['predicted_hashrate'].iloc[i] / (df['predicted_efficiency'].iloc[i] * df['predicted_coins_per_block'].iloc[i])\n",
    "\n",
    "        df['d_ln_predicted_joules_per_coin'] = np.log(df['predicted_joules_per_coin']).diff().fillna(method='bfill')\n",
    "\n",
    "        df['real_joules_per_coin'] = df['hashrate'] / (df['efficiency'] * df['coins_per_block'])\n",
    "        df['d_ln_real_joules_per_coin'] = np.log(df['real_joules_per_coin']).diff().fillna(method='bfill')\n",
    "\n",
    "\n",
    "    return quarterly_data,monthly_data, weekly_data, daily_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_model(test_data, predictions, p, q, title='Model Estimation'):\n",
    "    # Normalizing count and d_ln_avg_efficiency\n",
    "    # scaler = MinMaxScaler(feature_range=(-0.15, 0.15))\n",
    "    # normalized_values = scaler.fit_transform(test_data[['count', 'd_ln_avg_efficiency', 'cost', 'open_price', 'd_ln_cost', 'd_ln_joules_per_coin']])\n",
    "    # normalized_count = normalized_values[:, 0]\n",
    "    # normalized_d_ln_avg_efficiency = normalized_values[:, 1]\n",
    "    # normalized_cost = normalized_values[:, 2]\n",
    "    # normalized_open_price = normalized_values[:, 3]\n",
    "    # normalized_d_ln_cost = normalized_values[:, 4]\n",
    "    # normalized_d_ln_joules_per_coin = normalized_values[:, 5]\n",
    "\n",
    "    offset = -9 if price_col == 'ln_open_price' else 0\n",
    "    \n",
    "    # Plotting the predictions against the actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(test_data.index, test_data[price_col]+offset, label=f'Actual {price_col}', color='blue', marker='o')\n",
    "    plt.scatter(test_data.index, predictions+offset, label=f'Predicted {price_col}', color='red', marker='x')\n",
    "    # plt.plot(test_data.index, normalized_count, label='Normalized Count', color='green', linestyle='--')\n",
    "    # plt.plot(test_data.index, normalized_d_ln_joules_per_coin, label='Normalized normalized_d_ln_joules_per_coin', color='green', linestyle='--')\n",
    "    # plt.plot(test_data.index, normalized_d_ln_avg_efficiency, label='Normalized d_ln_avg_efficiency', color='purple', linestyle='-.')\n",
    "    # plt.plot(test_data.index, normalized_d_ln_cost, label='Normalized d_ln_cost', color='purple', linestyle='-.')\n",
    "    # plt.plot(test_data.index, normalized_cost, label='Normalized Cost', color='orange', linestyle=':')\n",
    "    # plt.plot(test_data.index, normalized_open_price, label='Normalized Open Price', color='black', linestyle='-.')\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=1)  # Dashed line at y=0\n",
    "    plt.title(f'{title} Predictions vs Actual, p={p}, q={q}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('diff of ln(open price) and Normalized Values')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def add_constant(train_exog, test_exog):\n",
    "    # train_exog[\"const\"] = 1\n",
    "    # test_exog[\"const\"] = 1\n",
    "    # # Ensure the shapes of train_exog and test_exog match\n",
    "    # print(train_exog.columns)\n",
    "    # print(test_exog.columns)\n",
    "    return train_exog, test_exog\n",
    "\n",
    "def get_stats(model_fit, predictions, test_data):\n",
    "    mae = mean_absolute_error(test_data[price_col], predictions)\n",
    "    mse = mean_squared_error(test_data[price_col], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    aic = model_fit.aic\n",
    "    bic = model_fit.bic\n",
    "    \n",
    "    # Test residuals for autocorrelation, normality, and heteroscedasticity\n",
    "    residuals = model_fit.resid\n",
    "    # Autocorrelation tests\n",
    "    ljung_box = sm.stats.acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "    durbin_watson = sm.stats.durbin_watson(residuals)\n",
    "    # Normality tests\n",
    "    jarque_bera = sm.stats.jarque_bera(residuals)\n",
    "    shapiro_wilk = shapiro(residuals)\n",
    "    # Heteroscedasticity test\n",
    "    white_test = [-1,-1]\n",
    "    try:\n",
    "        white_test = sm.stats.het_white(residuals, model_fit.model.exog)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'MAE':mae,\n",
    "        'MSE': mse, 'RMSE': rmse, 'AIC': aic, 'BIC': bic,\n",
    "        'Ljung-Box': ljung_box['lb_pvalue'].values[0], 'Durbin-Watson': durbin_watson,\n",
    "        'Jarque-Bera': jarque_bera[1], 'Shapiro-Wilk': shapiro_wilk[1],\n",
    "        'White Test': white_test[1]\n",
    "    }\n",
    "\n",
    "# Adjust the model estimation to use the differenced series\n",
    "def model_estimation(data, p_range, q_range, cols, use_exog, title='Model Estimation', eval_size=8):\n",
    "    global results_df\n",
    "    \n",
    "\n",
    "\n",
    "    results = []\n",
    "    data_size = len(data)\n",
    "    train_size = data_size - eval_size\n",
    "\n",
    "\n",
    "\n",
    "    for p in p_range:\n",
    "        for q in q_range:\n",
    "            try:\n",
    "                # Perform rolling cross-validation\n",
    "                rolling_mae = []\n",
    "                rolling_mse = []\n",
    "                mean_baseline_mae = []\n",
    "                mean_baseline_mse = []\n",
    "                prev_value_baseline_mae = []\n",
    "                prev_value_baseline_mse = []\n",
    "                step_size = 1\n",
    "                summary = \"\"\n",
    "                for end in range(train_size, data_size, step_size):  # Adjust these values as needed\n",
    "                    # print(f\"p: {p}, q: {q}, end: {end}\")\n",
    "                    rolling_train = data.iloc[0:end]\n",
    "                    rolling_test = data.iloc[end:end + step_size]  # Predict one step ahead\n",
    "                    rolling_train_exog, rolling_test_exog = add_constant(rolling_train[cols], rolling_test[cols])\n",
    "                    \n",
    "                    # Baseline 1: Mean Prediction\n",
    "                    mean_pred = np.mean(rolling_train[price_col])\n",
    "                    mean_baseline_mae.append(mean_absolute_error(rolling_test[price_col], [mean_pred]*step_size))\n",
    "                    mean_baseline_mse.append(mean_squared_error(rolling_test[price_col], [mean_pred]*step_size))\n",
    "                    \n",
    "                    # Baseline 2: Previous Value Prediction\n",
    "                    prev_value_pred = rolling_train[price_col].iloc[-1]\n",
    "                    prev_value_baseline_mae.append(mean_absolute_error(rolling_test[price_col], [prev_value_pred]*step_size))\n",
    "                    prev_value_baseline_mse.append(mean_squared_error(rolling_test[price_col], [prev_value_pred]*step_size))\n",
    "\n",
    "\n",
    "                    # ARIMA Model\n",
    "                    if use_exog:\n",
    "                        model = ARIMA(rolling_train[price_col], order=(p, 0, q), exog=rolling_train_exog)\n",
    "                    else:\n",
    "                        model = ARIMA(rolling_train[price_col], order=(p, 0, q))\n",
    "                    model_fit = model.fit()\n",
    "                    \n",
    "                    summary = model_fit.summary()\n",
    "                    if use_exog:\n",
    "                        predictions = model_fit.forecast(steps=step_size, exog=rolling_test_exog)\n",
    "                    else:\n",
    "                        predictions = model_fit.forecast(steps=step_size)\n",
    "                    mae = mean_absolute_error(rolling_test[price_col], predictions)\n",
    "                    mse = mean_squared_error(rolling_test[price_col], predictions)\n",
    "                    rolling_mae.append(mae)\n",
    "                    rolling_mse.append(mse)\n",
    "                \n",
    "                # print(\"rolling_mse\")\n",
    "                # print(rolling_mse)\n",
    "                avg_mae = np.mean(rolling_mae)\n",
    "                avg_mse = np.mean(rolling_mse)\n",
    "                avg_mean_baseline_mae = np.mean(mean_baseline_mae)\n",
    "                avg_mean_baseline_mse = np.mean(mean_baseline_mse)\n",
    "                avg_prev_value_baseline_mae = np.mean(prev_value_baseline_mae)\n",
    "                avg_prev_value_baseline_mse = np.mean(prev_value_baseline_mse)\n",
    "                results.append({'p': p, 'q': q, 'MAE': avg_mae, 'Mean Baseline MAE': avg_mean_baseline_mae, 'Prev Value Baseline MAE': avg_prev_value_baseline_mae, 'MSE': avg_mse, 'Mean Baseline MSE': avg_mean_baseline_mse, 'Prev Value Baseline MSE': avg_prev_value_baseline_mse,'Summary': summary,})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with ARIMA({p},0,{q}): {e}\")\n",
    "\n",
    "    # sort results and print/plot only the top 3\n",
    "    results = sorted(results, key=lambda x: x['MSE'])\n",
    "    print(f\"Mean Baseline MSE: {results[0]['Mean Baseline MSE']}\")\n",
    "    print(f\"Previous Value Baseline MSE: {results[0]['Prev Value Baseline MSE']}\")\n",
    "    for i in range(3):\n",
    "        p = results[i]['p']\n",
    "        q = results[i]['q']\n",
    "        \n",
    "        # Fit the model on the entire training dataset\n",
    "        train_data = data.iloc[0:train_size]\n",
    "        test_data = data.iloc[train_size:]\n",
    "        if use_exog:\n",
    "            train_exog, test_exog = add_constant(train_data[cols], test_data[cols])\n",
    "            train_exog = np.array(train_exog)\n",
    "            # print(train_exog.shape)\n",
    "            # train_exog[-1][0] = 5\n",
    "            model = ARIMA(train_data[price_col], order=(p, 0, q), exog=train_exog)\n",
    "            model_fit = model.fit()\n",
    "            predictions = model_fit.forecast(steps=len(test_data), exog=test_exog)\n",
    "        else:\n",
    "            model = ARIMA(train_data[price_col], order=(p, 0, q))\n",
    "            model_fit = model.fit()\n",
    "            predictions = model_fit.forecast(steps=len(test_data))\n",
    "        print(f\"Top {i+1} Model: ARIMA({p},0,{q})\")\n",
    "        # print(get_stats(model_fit, predictions, test_data))\n",
    "        print(\"MSE:\",results[i]['MSE'])\n",
    "        print(results[i]['Summary'])        \n",
    "              \n",
    "        plot_model(test_data, predictions, p, q, title=f\"{title} - Top {i+1} Model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exploratory_analysis(df):\n",
    "    # Select only numeric columns for distribution, skewness, and kurtosis\n",
    "    # numeric_df = df.select_dtypes(include=[np.number])\n",
    "    cols = [price_col,'cost'] #'d_ln_avg_efficiency', \n",
    "    numeric_df = df[cols]\n",
    "    \n",
    "    num_cols = len(numeric_df.columns)\n",
    "    num_rows = (num_cols + 1) // 2  # Ensure enough rows to accommodate all columns\n",
    "    print(\"Distribution Plot:\")\n",
    "    numeric_df.hist(bins=20, figsize=(14, num_rows * 3), layout=(num_rows, 2))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Summary Statistics:\")\n",
    "    summary = numeric_df.describe().T\n",
    "    summary['std'] = numeric_df.std()\n",
    "    summary['skewness'] = numeric_df.skew()\n",
    "    summary['kurtosis'] = numeric_df.kurtosis()\n",
    "    # print(summary)\n",
    "\n",
    "    for col in cols:\n",
    "        if col in numeric_df.columns:\n",
    "            print(f\"ACF and PACF Plots for {col}:\")\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "            sm.graphics.tsa.plot_acf(numeric_df[col].dropna(), lags=10, ax=axes[0])\n",
    "            sm.graphics.tsa.plot_pacf(numeric_df[col].dropna(), lags=10, ax=axes[1])\n",
    "            plt.show()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def check_stationarity(series):\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    result = adfuller(series.dropna())\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    # if result[1] > 0.05:\n",
    "    #     print(\"Series is not stationary\")\n",
    "    # else:\n",
    "    #     print(\"Series is stationary\")\n",
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import coint\n",
    "def perform_cointegration_test(df, column1, column2):\n",
    "    \"\"\"\n",
    "    Perform the co-integration test on two specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - column1: The name of the first column to test.\n",
    "    - column2: The name of the second column to test.\n",
    "\n",
    "    Returns:\n",
    "    - coint_t: The t-statistic of the test.\n",
    "    - p_value: The p-value of the test.\n",
    "    - crit_value: Critical values for the test.\n",
    "    \"\"\"\n",
    "    score, p_value, crit_value = coint(df[column1], df[column2])\n",
    "    print(f\"Co-integration test results for {column1} and {column2}:\")\n",
    "    print(f\"t-statistic: {score}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "    print(f\"Critical values: {crit_value}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"The series are likely co-integrated.\")\n",
    "    else:\n",
    "        print(\"The series are not likely co-integrated.\")\n",
    "    print(\"\")\n",
    "    # return score, p_value, crit_value\n",
    "\n",
    "\n",
    "def cross_correlation(series1, series2, lag=10, plot=True):\n",
    "    \"\"\"\n",
    "    Calculate and plot cross-correlation between two series.\n",
    "    \n",
    "    Parameters:\n",
    "    - series1: First time series.\n",
    "    - series2: Second time series.\n",
    "    - lag: Number of lags to calculate cross-correlation for.\n",
    "    - plot: Whether to plot the cross-correlation values.\n",
    "    \n",
    "    Returns:\n",
    "    - Cross-correlation values.\n",
    "    \"\"\"\n",
    "    cc_values = [series1.corr(series2.shift(l)) for l in range(-lag, lag+1)]\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.stem(range(-lag, lag+1), cc_values)#, use_line_collection=True)\n",
    "        plt.xlabel('Lag')\n",
    "        plt.ylabel('Cross-correlation')\n",
    "        plt.title('Cross-correlation between series')\n",
    "        plt.show()\n",
    "    \n",
    "    return cc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_data, monthly_data, weekly_data, daily_data = get_data('2014-01-01', '9999-01-01',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quarterly_data.tail(10)\n",
    "# quarterly_data.head(10)\n",
    "# quarterly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in quarterly_data.columns:\n",
    "#     if col.startswith('avg'):\n",
    "#         print(f\"# '{col}',\")s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quaterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "# 'avg_efficiency',\n",
    "# 'avg_d_ln_max_efficiency',\n",
    "# 'avg_d_ln_price',\n",
    "# 'avg_d_ln_count',\n",
    "# 'avg_d_ln_cost_per_coin',\n",
    "# 'avg_d_ln_watts',\n",
    "# 'avg_d_ln_joules_per_dollar_earned',\n",
    "# 'avg_d_ln_optimistic_speculation',\n",
    "\n",
    "# 'd_ln_predicted_joules_per_coin',\n",
    "# 'd_ln_real_joules_per_coin',\n",
    "\n",
    "# 'avg_d_ln_adoption',\n",
    "\n",
    "# 'avg_d_ln_hashrate',\n",
    "# 'avg_d_ln_efficiency',\n",
    "'avg_d_ln_pessimistic_speculation',\n",
    "# 'avg_d_ln_speculation',\n",
    "# 'avg_d_ln_posts_count',\n",
    "# 'avg_d_ln_positive',\n",
    "# 'avg_d_ln_neutral',\n",
    "# 'avg_d_ln_negative',\n",
    "]\n",
    "# Baseline 3: no exog\n",
    "# use_exog = False\n",
    "use_exog = True\n",
    "\n",
    "print(\"Quarterly Data Model Estimation:\")\n",
    "# model_estimation(quarterly_data, range(1, 3, 1), range(1, 3, 1), title='Quarterly Data', eval_size=8)\n",
    "model_estimation(quarterly_data, range(1, 2, 1), range(1, 2, 1), cols, use_exog, title='Quarterly Data', eval_size=8)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stationarity check for d_ln_open_price:\")\n",
    "check_stationarity(quarterly_data['d_ln_open_price'])\n",
    "print(f\"\\nStationarity check for ln_open_price:\")\n",
    "check_stationarity(quarterly_data['ln_open_price'])\n",
    "print(\"\\nStationarity check for d_ln_avg_efficiency:\")\n",
    "check_stationarity(quarterly_data['d_ln_avg_efficiency'])\n",
    "print(\"\\nStationarity check for cost:\")\n",
    "check_stationarity(quarterly_data['cost'])\n",
    "print(\"\\nStationarity check for d_cost:\")\n",
    "check_stationarity(quarterly_data['d_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_analysis(quarterly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_cointegration_test(quarterly_data, price_col, 'd_ln_avg_efficiency')\n",
    "perform_cointegration_test(quarterly_data, price_col, 'd_cost')\n",
    "perform_cointegration_test(quarterly_data, price_col, 'd_ln_cost')\n",
    "perform_cointegration_test(quarterly_data, price_col, 'd_ln_open_price')\n",
    "perform_cointegration_test(quarterly_data, price_col, 'd_ln_watts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_correlation_values = cross_correlation(quarterly_data['d_ln_avg_efficiency'], quarterly_data[price_col], lag=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_correlation_values = cross_correlation(quarterly_data['d_ln_cost'], quarterly_data['d_ln_open_price'], lag=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_correlation_values = cross_correlation(quarterly_data['cost'], quarterly_data['ln_open_price'], lag=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Monthly Data Model Estimation:\")\n",
    "model_estimation(monthly_data, range(1, 6, 1), range(1, 6, 1), title='Monthly Data', eval_size=12)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_analysis(monthly_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weekly Data Model Estimation:\")\n",
    "model_estimation(weekly_data, range(1, 6, 1), range(1, 6, 1), title='Weekly Data', eval_size=55)\n",
    "# model_estimation(weekly_data, range(1, 4, 1), range(1, 3, 1), title='Weekly Data', ratio=0.8)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_analysis(weekly_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Daily Data Model Estimation:\")\n",
    "# # model_estimation(daily_data, range(1, 6, 1), range(1, 6, 1), title='Daily Data', ratio=0.8)\n",
    "# model_estimation(daily_data, range(1, 3, 1), range(1, 3, 1), title='Daily Data', ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_analysis(daily_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
